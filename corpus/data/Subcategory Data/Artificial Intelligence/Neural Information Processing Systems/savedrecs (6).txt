PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
C	Hiranandani, G; Boodaghians, S; Mehta, R; Koyejo, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hiranandani, Gaurush; Boodaghians, Shant; Mehta, Ruta; Koyejo, Oluwasanmi			Multiclass Performance Metric Elicitation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Metric Elicitation is a principled framework for selecting the performance metric that best reflects implicit user preferences. However, available strategies have so far been limited to binary classification. In this paper, we propose novel strategies for eliciting multiclass classification performance metrics using only relative preference feedback. We also show that the strategies are robust to both finite sample and feedback noise.	[Hiranandani, Gaurush; Boodaghians, Shant; Mehta, Ruta; Koyejo, Oluwasanmi] Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Hiranandani, G (corresponding author), Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.	gaurush2@illinois.edu; boodagh2@illinois.edu; rutameht@illinois.edu; sanmi@illinois.edu			NSF [CCF 1750436]	NSF(National Science Foundation (NSF))	Gaurush Hiranandani and Oluwasanmi Koyejo thank Microsoft Azure for providing computing credits. Shant Boodaghians and Ruta Mehta acknowledge the support of NSF via CCF 1750436.	Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Dmitriev P., 2016, CIKM; Duarte MF, 2004, J PARALLEL DISTR COM, V64, P826, DOI 10.1016/j.jpdc.2004.03.020; Herbrich R, 2000, ADV NEUR IN, P115; Jamieson Kevin G, 2011, ADV NEURAL INFORM PR, P2240; Joachims T., 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]; Kaariainen M, 2006, LECT NOTES ARTIF INT, V4264, P63; Kane DM, 2017, ANN IEEE SYMP FOUND, P355, DOI 10.1109/FOCS.2017.40; Kim B, 2017, ARXIV PREPRINT ARXIV; Koyejo Oluwasanmi O, 2015, NIPS, V28, P3321; Narasimhan H, 2015, PR MACH LEARN RES, V37, P2398; Narasimhan Harikrishna, 2018, INT C ART INT STAT, P1646; Peyrard Maxime, 2017, P WORKSH NEW FRONT S, P74, DOI DOI 10.18653/V1/W17-4510; Qian B., 2013, P 23 INT JOINT C ART, P1614; Qian L, 2015, PROC VLDB ENDOW, V8, P1322, DOI 10.14778/2809974.2809992; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Settles B., 2009, ACTIVE LEARNING LIT; Siebert JP., 1987, VEHICLE RECOGNITION; Tamburrelli G, 2014, LECT NOTES COMPUT SC, V8636, P184, DOI 10.1007/978-3-319-09940-8_13; Yang ST, 2014, STAT APPL GENET MOL, V13, P477, DOI 10.1515/sagmb-2013-0053; Zhou ZH, 2010, COMPUT INTELL-US, V26, P232, DOI 10.1111/j.1467-8640.2010.00358.x	24	3	3	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901003
C	Jordan, M; Lewis, J; Dimakis, AG		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jordan, Matt; Lewis, Justin; Dimakis, Alexandros G.			Provable Certificates for Adversarial Examples: Fitting a Ball in the Union of Polytopes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a novel method for computing exact pointwise robustness of deep neural networks for all convex l(p) norms. Our algorithm, GeoCert, finds the largest l(p) ball centered at an input point x(0), within which the output class of a given neural network with ReLU nonlinearities remains unchanged. We relate the problem of computing pointwise robustness of these networks to that of computing the maximum norm ball with a fixed center that can be contained in a non-convex polytope. This is a challenging problem in general, however we show that there exists an efficient algorithm to compute this for polyhedral complices. Further we show that piecewise linear neural networks partition the input space into a polyhedral complex. Our algorithm has the ability to almost immediately output a nontrivial lower bound to the pointwise robustness which is iteratively improved until it ultimately becomes tight. We empirically show that our approach generates distance lower bounds that are tighter compared to prior work, under moderate time constraints.	[Jordan, Matt; Lewis, Justin; Dimakis, Alexandros G.] Univ Texas Austin, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Jordan, M (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	mjordan@cs.utexas.edu; justin941ewis@utexas.edu; dimakis@austin.utexas.edu			NSF [1618689, DMS 1723052, CCF 1763702, AF 1901292]	NSF(National Science Foundation (NSF))	This research has been supported by NSF Grants 1618689, DMS 1723052, CCF 1763702, AF 1901292 and research gifts by Google, Western Digital and NVIDIA.	Adusumalli M, 2018, ROUT INT HANDB, P121; Bastani Osbert, 2016, MEASURING NEURAL NET; BOTKIN ND, 1994, APPL MATH OPT, V29, P211, DOI 10.1007/BF01204183; Boyd S, 2004, CONVEX OPTIMIZATION; Cheng Chih-Hong, 2017, MAXIMUM RESILIENCE A; Croce F., 2018, ARXIV181007481; Ehlers Ruediger, 2017, FORMAL VERIFICATION; Fischetti M, 2018, CONSTRAINTS, V23, P296, DOI 10.1007/s10601-018-9285-6; Hein M., 2017, FORMAL GUARANTEES RO; Katz G., 2017, RELUPLEX EFFICIENT S; Kingma D.P, P 3 INT C LEARNING R; Kolter J Zico, 2017, PROVABLE DEFENSES AD; LLC Gurobi Optimization, 2019, GUROBI OPTIMIZER REF; Lomuscio A., 2017, CORR; Mirman M., 2018, P 35 INT C MACH LEAR, V80, P3578; Raghunathan A., 2018, CERTIFIED DEFENSES A; Raghunathan Aditi, 2018, INT C LEARN REPR; Salman Hadi, 2019, CORR; Singla Sahil, 2019, ARXIV190201235; Szegedy C., 2013, P INT C LEARN REPR; Tjeng V., 2017, EVALUATING ROBUSTNES; Tsuzuku Y, 2018, LIPSCHITZ MARGIN TRA; Virmaux A., 2018, ADV NEURAL INFORM PR, V31, P3835; Wang SQ, 2018, PROCEEDINGS OF THE 27TH USENIX SECURITY SYMPOSIUM, P1599; Wang Shiqi, 2018, CORR; Zhang Huan, 2018, ARXIV180409699; Ziegler G., 1995, GRADUATE TEXTS MATH	27	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905070
C	Joseph, M; Kulkarni, J; Mao, JM; Wu, ZS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Joseph, Matthew; Kulkarni, Janardhan; Mao, Jieming; Wu, Zhiwei Steven			Locally Private Gaussian Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study a basic private estimation problem: each of n users draws a single i.i.d. sample from an unknown Gaussian distribution N(mu, sigma(2)), and the goal is to estimate mu while guaranteeing local differential privacy for each user. As minimizing the number of rounds of interaction is important in the local setting, we provide adaptive two-round solutions and nonadaptive one-round solutions to this problem. We match these upper bounds with an information-theoretic lower bound showing that our accuracy guarantees are tight up to logarithmic factors for all sequentially interactive locally private protocols.	[Joseph, Matthew] Univ Penn, Philadelphia, PA 19104 USA; [Joseph, Matthew; Kulkarni, Janardhan] Microsoft Res Redmond, Redmond, WA USA; [Mao, Jieming] Google Res New York, New York, NY USA; [Wu, Zhiwei Steven] Univ Minnesota, Minneapolis, MN 55455 USA; [Mao, Jieming] Univ Penn, Warren Ctr, Philadelphia, PA 19104 USA; [Wu, Zhiwei Steven] Microsoft Res New York, New York, NY USA	University of Pennsylvania; Microsoft; Google Incorporated; University of Minnesota System; University of Minnesota Twin Cities; University of Pennsylvania	Joseph, M (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.	majos@cis.upenn.edu; jakul@microsoft.com; maojm@google.com; zsw@umn.edu		Wu, Steven/0000-0002-8125-8227				Abowd John M., 2016, CENSUS SCI ADVISORY; [Anonymous], 2018, ARXIV180202638; Avent Brendan, 2017, USENIX SEC S; Beimel Amos, 2008, INT CRYPT C CRYPTO; Bittau Andrea, 2017, S OP SYST PRINC SOSP; Braverman Mark, 2016, S THEOR COMP STOC; Daniely Amit, 2019, NEURAL INFORM PROCES; DifferentialDifferential Privacy Team Apple, 2017, LEARNING PRIVACY SCA; Ding Bolin, 2017, NEURAL INFORM PROCES; Duchi John, 2019, C LEARN THEOR COLT; Duchi John C, 2013, FDN COMPUTER SCI FOC; Dwork C., 2006, THEOR CRYPT C TCC; Dwork C., 2014, FDN TRENDS THEORETIC; Erlingsson U., 2014, C COMP COMM SEC CCS; Gaboardi Marco, 2019, ARTIFICIAL INTELLIGE; Joseph Matthew, 2019, FDN COMPUTER SCI FOC; Joseph Matthew, 2019, ARXIV181108382; Kamath Gautam, 2019, C LEARN THEOR COLT; Karwa Vishesh, 2018, INN THEOR COMP SCI C; Kasiviswanathan S. P., 2011, SIAM J COMPUTING; Kuo Yu-Hsuan, 2018, INT C VER LARG DAT V; Raginsky M, 2016, IEEE T INFORM THEORY, V62, P3355, DOI 10.1109/TIT.2016.2549542; Smith Adam, 2017, S SEC PRIV SP; Vadhan S, 2017, INFORM SEC CRYPT TEX, P347, DOI 10.1007/978-3-319-57048-8_7	24	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303002
C	Kanagawa, M; Hennig, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kanagawa, Motonobu; Hennig, Philipp			Convergence Guarantees for Adaptive Bayesian Quadrature Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Adaptive Bayesian quadrature (ABQ) is a powerful approach to numerical integration that empirically compares favorably with Monte Carlo integration on problems of medium dimensionality (where non-adaptive quadrature is not competitive). Its key ingredient is an acquisition function that changes as a function of previously collected values of the integrand. While this adaptivity appears to be empirically powerful, it complicates analysis. Consequently, there are no theoretical guarantees so far for this class of methods. In this work, for a broad class of adaptive Bayesian quadrature methods, we prove consistency, deriving non-tight but informative convergence rates. To do so we introduce a new concept we call weak adaptivity. Our results identify a large and flexible class of adaptive Bayesian quadrature rules as consistent, within which practitioners can develop empirically efficient methods.	[Kanagawa, Motonobu] EURECOM, Sophia Antipolis, France; [Kanagawa, Motonobu; Hennig, Philipp] Univ Tubingen, Tubingen, Germany; [Kanagawa, Motonobu; Hennig, Philipp] Max Planck Inst Intelligent Syst, Tubingen, Germany	IMT - Institut Mines-Telecom; EURECOM; Eberhard Karls University of Tubingen; Max Planck Society	Kanagawa, M (corresponding author), EURECOM, Sophia Antipolis, France.; Kanagawa, M (corresponding author), Univ Tubingen, Tubingen, Germany.; Kanagawa, M (corresponding author), Max Planck Inst Intelligent Syst, Tubingen, Germany.	motonobu.kanagawa@eurecom.fr; philipp.hennig@uni-tuebingen.de		Kanagawa, Motonobu/0000-0002-3948-8053	European Research Council through ERC StG Action [757275/PANAMA]; DFG Cluster of Excellence "Machine Learning -New Perspectives for Science" [EXC 2064/1, 390727645]; German Federal Ministry of Education and Research (BMBF) through the Tubingen AI Center [FKZ: 01IS18039A, 01IS18039B]; Ministry of Science, Research and Arts of the State of Baden-Wurttemberg	European Research Council through ERC StG Action(European Research Council (ERC)); DFG Cluster of Excellence "Machine Learning -New Perspectives for Science"(German Research Foundation (DFG)); German Federal Ministry of Education and Research (BMBF) through the Tubingen AI Center(Federal Ministry of Education & Research (BMBF)); Ministry of Science, Research and Arts of the State of Baden-Wurttemberg	We would like to express our gratitude to the anonymous reviewers for their constructive feedback. We also thank Alexandra Gessner, Hans Kersting, Tim Sullivan and GeorgeWynne for their comments and for fruitful discussions. The authors gratefully acknowledge financial supports by the European Research Council through ERC StG Action 757275/PANAMA, by the DFG Cluster of Excellence "Machine Learning -New Perspectives for Science", EXC 2064/1, project number 390727645, by the German Federal Ministry of Education and Research (BMBF) through the Tubingen AI Center (FKZ: 01IS18039A, 01IS18039B), and by the Ministry of Science, Research and Arts of the State of Baden-Wurttemberg.	Acerbi, 2018, ADV NEURAL INFORM PR, P8213; Acerbi L., 2019, P MACHINE LEARNING R, V96, P1; Agapiou S, 2017, STAT SCI, V32, P405, DOI 10.1214/17-STS611; [Anonymous], 2002, LEARNING KERNELS; Bach F., 2017, J MACH LEARN RES, V18, p1 629 681; Bach F., 2012, P 29 INT C INT C MAC, P1355; Berlinet A., 2004, REPRODUCING KERNEL H, DOI [10.1007/978-1-4419-9096-9, DOI 10.1007/978-1-4419-9096-9]; Briol F.-X., 2015, ADV NEURAL INFORM PR, P1162; Briol FX, 2019, STAT SCI, V34, P1, DOI 10.1214/18-STS660; Chai H. R., 2019, P MACHINE LEARNING R, P2751; Chatterjee S, 2018, ANN APPL PROBAB, V28, P1099, DOI 10.1214/17-AAP1326; Chen WY, 2018, P 35 INT C MACH LEAR, P844; Chen Yutian, 2010, UAI; Dick J., 2013, ACTA NUMER, V22, P133; Gunter T., 2014, ADV NEURAL INFORM PR, P2789; Husz Ferenc, 2012, P 28 C UNCERTAINTY A, P377; KANAGAWA M, 2016, ADV NEURAL INFORM PR, V29, P3288; Kanagawa M., 2019, FDN COMPUTATIONAL MA; Kanagawa M., 2018, 180702582V1 ARXIV; Liu JS., 2001, MONTE CARLO STRATEGI, DOI DOI 10.1007/978-0-387-76371-2; Novak E, 1996, J COMPLEXITY, V12, P199, DOI 10.1006/jcom.1996.0015; NOVAK E, 1995, J APPROX THEORY, V80, P390, DOI 10.1006/jath.1995.1025; NOVAK E, 1995, J APPROX THEORY, V82, P123, DOI 10.1006/jath.1995.1071; Novak E, 2016, SPRINGER P MATH STAT, V163, P161, DOI 10.1007/978-3-319-33507-0_6; Oates CJ, 2019, BERNOULLI, V25, P1141, DOI 10.3150/17-BEJ1016; OHAGAN A, 1991, J STAT PLAN INFER, V29, P245, DOI 10.1016/0378-3758(91)90002-V; Osborne M., 2012, ADV NEURAL INF PROCE, V25, P1; Osborne Michael A, 2012, ARTIF INTELL, P832; Plaskota L, 2009, J FIX POINT THEORY A, V6, P227, DOI 10.1007/s11784-009-0121-x; Rasmussen CE, 2003, ADV NEURAL INF PROCE, P505; Rasmussen E, 2006, GAUSSIAN PROCESSES M; Robert C, 2004, MONTE CARLO STAT MET, DOI DOI 10.1007/978-1-4757-4145-2; Santin G, 2017, DOLOMIT RES NOTES AP, V10, P68; Sarkka S., 2018, P INT C NEUR INF PRO, P5882; Steinwart I., 2008, SUPPORT VECTOR MACHI; Wendland H., 2005, SCATTERED DATA APPRO; Wenliang L., 2019, P MACHINE LEARNING R, V97, P6737; XI X, 2018, P 35 INT C MACH LEAR, V80, P5373	40	3	3	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306026
C	Karami, M; Sohl-Dickstein, J; Schuurmans, D; Dinh, L; Duckworth, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Karami, Mahdi; Sohl-Dickstein, Jascha; Schuurmans, Dale; Dinh, Laurent; Duckworth, Daniel			Invertible Convolutional Flow	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SYMMETRIC CONVOLUTION	Normalizing flows can be used to construct high quality generative probabilistic models, but training and sample generation require repeated evaluation of Jacobian determinants and function inverses. To make such computations feasible, current approaches employ highly constrained architectures that produce diagonal, triangular, or low rank Jacobian matrices. As an alternative, we investigate a set of novel normalizing flows based on the circular and symmetric convolutions. We show that these transforms admit efficient Jacobian determinant computation and inverse mapping (deconvolution) in O(N logN) time. Additionally, element-wise multiplication, widely used in normalizing flow architectures, can be combined with these transforms to increase modeling flexibility. We further propose an analytic approach to designing nonlinear elementwise bijectors that induce special properties in the intermediate layers, by implicitly introducing specific regularizers in the loss. We show that these transforms allow more effective normalizing flow models to be developed for generative image models.	[Karami, Mahdi] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada; [Sohl-Dickstein, Jascha; Schuurmans, Dale; Dinh, Laurent; Duckworth, Daniel] Google Brain, Mountain View, CA USA	University of Alberta; Google Incorporated	Karami, M (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.	karami1@ualberta.ca						Ba J, 2014, ADV NEURAL INFORM PR; Cheng Y, 2015, IEEE I CONF COMP VIS, P2857, DOI 10.1109/ICCV.2015.327; Cinkir Z., 2011, ARXIV E PRINTS; Cortes C., 1998, MNIST DATABASE HANDW; Dinh L., 2016, ARXIV160508803CSSTAT; Dinh L., 2014, ARXIV; Foltz TM, 1998, J OPT SOC AM A, V15, P2827, DOI 10.1364/JOSAA.15.002827; Germain M, 2015, PR MACH LEARN RES, V37, P881; Gonzalez R C, 1992, DIGITAL IMAGE PROCES; Grathwohl W., 2019, P INT C LEARN REPR; Gray RM, 2006, FOUND TRENDS COMMUN, V2, DOI 10.1561/0100000006; Ho Jonathan, 2019, FLOW IMPROVING FLOW; Hoogeboom E., 2019, ARXIV190111137; Huang CW, 2018, PR MACH LEARN RES, V80; Karami M., 2018, WORKSH BAYES DEEP LE; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2018, PROC NEURIPS; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Luxburg U. V., 2016, ADV NEURAL INFORM PR, V29, P4790; Martinsson PG, 2005, COMPUT MATH APPL, V50, P741, DOI 10.1016/j.camwa.2005.03.011; MARTUCCI SA, 1994, IEEE T SIGNAL PROCES, V42, P1038, DOI 10.1109/78.295213; Moczulski M., 2015, ACDC STRUCTURED EFFI; Monahan J. F., 2011, NUMERICAL METHODS ST; Papamakarios George, 2017, ADV NEURAL INFORM PR, P2338; Rezende D., 2015, ICML, P1530; Sonderby Casper Kaae, 2015, ADV NEURAL INFORM PR, P3738; Van den Berg R., 2018, ARXIV180305649; Zheng Guoqing, 2017, ARXIV171102255	28	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305061
C	Kavis, A; Levy, KY; Bach, F; Cevher, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kavis, Ali; Levy, Kfir Y.; Bach, Francis; Cevher, Volkan			UniXGrad: A Universal, Adaptive Algorithm with Optimal Guarantees for Constrained Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a novel adaptive, accelerated algorithm for the stochastic constrained convex optimization setting. Our method, which is inspired by the Mirror-Prox method, simultaneously achieves the optimal rates for smooth/non-smooth problems with either deterministic/stochastic first-order oracles. This is done without any prior knowledge of the smoothness nor the noise properties of the problem. To the best of our knowledge, this is the first adaptive, unified algorithm that achieves the optimal rates in the constrained setting. We demonstrate the practical performance of our framework through extensive numerical experiments.	[Kavis, Ali; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Levy, Kfir Y.] Technion, Haifa, Israel; [Bach, Francis] INRIA, Rocquencourt, France	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Inria	Kavis, A (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	ali.kavis@epfl.ch; kfirylevy@technion.ac.il; francis.bach@inria.fr; volkan.cevher@epfl.ch		Levy, Kfir Yehuda/0000-0003-1236-2626	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [725594]; Swiss National Science Foundation (SNSF) [200021_178865/1]	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC)); Swiss National Science Foundation (SNSF)(Swiss National Science Foundation (SNSF))	AK and VC are supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement no 725594 -time-data) and the Swiss National Science Foundation (SNSF) under grant number 200021_178865/1.	Bach F., 2019, ARXIV190201637; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Cohen M.B., 2018, ARXIV180512591; Cutkosky A., 2019, INT C MACH LEARN ICM; Deng Q., 2018, ARXIV181000553; Diakonikolas J., 2017, ARXIV170604680; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Kingma D.P, P 3 INT C LEARNING R; Koepelevich, 1976, EKONOMIKA MATEMATICH, V12, P747; Levy K., 2017, ADV NEURAL INFORM PR, P1612; Levy K. Y., 2018, NEURAL INFORM PROCES; Liu W, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON AUTOMATION AND LOGISTICS ( ICAL 2009), VOLS 1-3, P778, DOI 10.1109/ICAL.2009.5262817; Luo ZQ, 2007, SIAM J OPTIMIZ, V18, P1, DOI 10.1137/050642691; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nesterov Y., 2004, APPL OPTIM; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Yu. E., 1983, Doklady Akademii Nauk SSSR, V269, P543; Rakhlin S., 2013, ADV NEURAL INFORM PR, P3066; Reddi Sashank J., 2018, INT C LEARN REPR; Xiao L, 2010, J MACH LEARN RES, V11, P2543; Zhu Z, 2014, CORR	23	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306028
C	Khan, ME; Immer, A; Abedi, E; Korzepa, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Khan, Mohammad Emtiyaz; Immer, Alexander; Abedi, Ehsan; Korzepa, Maciej			Approximate Inference Turns Deep Networks into Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep neural networks (DNN) and Gaussian processes (GP) are two powerful models with several theoretical connections relating them, but the relationship between their training methods is not well understood. In this paper, we show that certain Gaussian posterior approximations for Bayesian DNNs are equivalent to GP posteriors. This enables us to relate solutions and iterations of a deep-learning algorithm to GP inference. As a result, we can obtain a GP kernel and a nonlinear feature map while training a DNN. Surprisingly, the resulting kernel is the neural tangent kernel. We show kernels obtained on real datasets and demonstrate the use of the GP marginal likelihood to tune hyperparameters of DNNs. Our work aims to facilitate further research on combining DNNs and GPs in practical settings.	[Khan, Mohammad Emtiyaz] RIKEN Ctr AI Project, Tokyo, Japan; [Immer, Alexander; Abedi, Ehsan] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Korzepa, Maciej] Tech Univ Denmark, Lyngby, Denmark	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Technical University of Denmark	Khan, ME (corresponding author), RIKEN Ctr AI Project, Tokyo, Japan.	emtiyaz.khan@riken.jp; alexander.immer@epfl.ch; ehsan.abedi@epfl.ch; mjko@dtu.dk						[Anonymous], 6 INT C LEARN REPR I; Bishop C.M, 2006, PATTERN RECOGN; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Bradshaw J., 2017, ARXIV PREPRINT ARXIV; Cho Y., 2009, NIPS, P342; Garriga-Alonso Adria, 2019, INT C LEARN REPR; Hazan Tamir, 2015, ABS150805133 CORR; Jacot Arthur, 2018, ADV NEURAL INFORM PR, P8571; Khan M, 2012, THESIS; Kingma D.P, P 3 INT C LEARNING R; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee J., 2019, ARXIV190206720; Martens J., 2014, ABS14121193 CORR; Matthews Alexander G. de G., 2018, P 7 INT C LEARN REPR, V3; Neal RM., 1996, BAYESIAN LEARNING NE, P29; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Osawa Kazuki, 2019, ADV NEURAL INFORM PR; Snelson Edward, 2006, ADV NEURAL INFORM PR, V3; Tieleman Tijmen, 2012, LECT 65 RMSPROP DIVI, V4; Vuppalapati C, 2018, INT CONF MACH LEARN, P161; Williams Christopher KI, 2006, GAUSSIAN PROCESSES M, V2; Yang Greg, 2019, INT C LEARN REPR	23	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303012
C	Lan, J; Liu, R; Zhou, H; Yosinski, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lan, Janice; Liu, Rosanne; Zhou, Hattie; Yosinski, Jason			LCA: Loss Change Allocation for Neural Network Training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Neural networks enjoy widespread use, but many aspects of their training, representation, and operation are poorly understood. In particular, our view into the training process is limited, with a single scalar loss being the most common viewport into this high-dimensional, dynamic process. We propose a new window into training called Loss Change Allocation (LCA), in which credit for changes to the network loss is conservatively partitioned to the parameters. This measurement is accomplished by decomposing the components of an approximate path integral along the training trajectory using a Runge-Kutta integrator. This rich view shows which parameters are responsible for decreasing or increasing the loss during training, or which parameters "help" or "hurt" the network's learning, respectively. LCA may be summed over training iterations and/or over neurons, channels, or layers for increasingly coarse views. This new measurement device produces several insights into training. (1) We find that barely over 50% of parameters help during any given iteration. (2) Some entire layers hurt overall, moving on average against the training gradient, a phenomenon we hypothesize may be due to phase lag in an oscillatory training process. (3) Finally, increments in learning proceed in a synchronized manner across layers, often peaking on identical iterations.	[Lan, Janice; Liu, Rosanne; Zhou, Hattie; Yosinski, Jason] Uber AI, San Francisco, CA 94103 USA		Lan, J (corresponding author), Uber AI, San Francisco, CA 94103 USA.	janlan@uber.com; rosanne@uber.com; hattie@uber.com; yosinski@uber.com		Yosinski, Jason/0000-0002-4701-0199				Achille Alessandro, 2017, ABS171108856 CORR; Alain G, 2016, UNDERSTANDING INTERM; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Goodfellow IJ, 2014, 3 INT C LEARN REPR I; Gotmare A., 2019, INT C LEARN REPR; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Hoffer Elad, 2018, ABS180104540 CORR; Jastrzebski Stanislaw, 2019, INT C LEARN REPR ICL; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Keskar Nitish Shirish, 2016, LARGE BATCH TRAINING, P4; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Kutta Wilhelm, 1901, BEITRAG NAHERUNGWEIS; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Chunyuan, 2018, INT C LEARN REPR APR; Li H, 2018, ADV NEUR IN, V31; Nguyen Q, 2017, PR MACH LEARN RES, V70; Raghu M., 2017, SVCCA SINGULAR VECTO; Runge C., 1895, MATH ANN, V46, P167, DOI DOI 10.1007/BF01446807; Safran I, 2016, PR MACH LEARN RES, V48; Shwartz-Ziv Ravid, 2017, ARXIV170300810; Soudry D., 2016, ABS160508361 CORR; Springenberg J.T., 2014, ARXIV14126806; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Weisstein EricW, 2003, SIMPSONS RULE; Xing C., 2018, ARXIV180208770; Yosinski J., 2015, ICML DEEP LEARN WORK; Zenke Friedemann, 2017, ABS170304200 CORR; Zhang Chiyuan, 2019, ARXIV190201996	36	3	3	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303059
C	Laue, S; Mitterreiter, M; Giesen, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Laue, Soeren; Mitterreiter, Matthias; Giesen, Joachim			GENO - GENeric Optimization for Classical Machine Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CONVERGENCE CONDITIONS; NONNEGATIVE MATRIX; ASCENT METHODS; ALGORITHMS; BFGS	Although optimization is the longstanding algorithmic backbone of machine learning, new models still require the time-consuming implementation of new solvers. As a result, there are thousands of implementations of optimization algorithms for machine learning problems. A natural question is, if it is always necessary to implement a new solver, or if there is one algorithm that is sufficient for most models. Common belief suggests that such a one-algorithm-fits-all approach cannot work, because this algorithm cannot exploit model specific structure and thus cannot be efficient and robust on a wide variety of problems. Here, we challenge this common belief. We have designed and implemented the optimization framework GENO (GENeric Optimization) that combines a modeling language with a generic solver. GENO generates a solver from the declarative specification of an optimization problem class. The framework is flexible enough to encompass most of the classical machine learning problems. We show on a wide variety of classical but also some recently suggested problems that the automatically generated solvers are (1) as efficient as well-engineered specialized solvers, (2) more efficient by a decent margin than recent state-of-the-art solvers, and (3) orders of magnitude more efficient than classical modeling language plus solver approaches.	[Laue, Soeren; Mitterreiter, Matthias; Giesen, Joachim] Friedrich Schiller Univ Jena, Jena, Germany; [Laue, Soeren] Data Assessment Solut GmbH, Hannover, Germany	Friedrich Schiller University of Jena	Laue, S (corresponding author), Friedrich Schiller Univ Jena, Jena, Germany.; Laue, S (corresponding author), Data Assessment Solut GmbH, Hannover, Germany.	soeren.laue@uni-jena.de; matthias.mitterreiter@uni-jena.de; joachim.giesen@uni-jena.de			Deutsche Forschungsgemeinschaft (DFG) [LA 2971/1-1]	Deutsche Forschungsgemeinschaft (DFG)(German Research Foundation (DFG))	Soren Laue has been funded by Deutsche Forschungsgemeinschaft (DFG) under grant LA 2971/1-1.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99; Agrawal A., 2018, J CONTROL DECIS, V5, P42, DOI [10.1080/23307706.2017.1397554, DOI 10.1080/23307706.2017.1397554]; Banjac Goran, 2017, P 56 IEEE C DEC CONT, P1906, DOI [10.1109/CDC.2017.8263928, DOI 10.1109/CDC.2017.8263928]; Birgin EG, 2014, FUND ALGORITHMS, P1, DOI 10.1137/1.9781611973365; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; BROOKE A, 1992, SCI PRESS SERIES; BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Cichocki A, 2009, IEICE T FUND ELECTR, VE92A, P708, DOI 10.1587/transfun.E92.A.708; COX DR, 1958, J R STAT SOC B, V20, P215; CVX Research Inc, 2018, CVX MATLAB SOFTWARE; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Defazio A, 2016, ADV NEUR IN, V29; Diamond S, 2016, J MACH LEARN RES, V17; Ding C, 2005, SIAM PROC S, P606; Domahidi Alexander, 2013, 2013 European Control Conference (ECC), P3071; Dunning I, 2017, SIAM REV, V59, P295, DOI 10.1137/15M1020575; Fourer R, 2003, AMPL MODELING LANGUA; Frank E., 2016, WEKA WORKBENCH ONLIN, V4; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Gabay D., 1976, Computers & Mathematics with Applications, V2, P17, DOI 10.1016/0898-1221(76)90003-1; Giselsson P, 2017, IEEE T AUTOMAT CONTR, V62, P532, DOI 10.1109/TAC.2016.2564160; GLOWINSKI R, 1975, REV FR AUTOMAT INFOR, V9, P41; GOFFIN JL, 1977, MATH PROGRAM, V13, P329, DOI 10.1007/BF01584346; Grant MC, 2008, LECT NOTES CONTR INF, V371, P95, DOI 10.1007/978-1-84800-155-8_7; Hestenes M. R., 1969, Journal of Optimization Theory and Applications, V4, P303, DOI 10.1007/BF00927673; Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289; Huang H. Y., 1970, Journal of Optimization Theory and Applications, V5, P405, DOI 10.1007/BF00927440; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Kim J, 2008, IEEE DATA MINING, P353, DOI 10.1109/ICDM.2008.149; Kuang D, 2015, J GLOBAL OPTIM, V62, P545, DOI 10.1007/s10898-014-0247-2; Laue S., 2018, ADV NEURAL INFORM PR; Laue Soren, 2020, C ART INT AAAI; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Morales JL, 2011, ACM T MATH SOFTWARE, V38, DOI 10.1145/2049662.2049669; Meng XR, 2016, J MACH LEARN RES, V17; MORE JJ, 1994, ACM T MATH SOFTWARE, V20, P286, DOI 10.1145/192115.192132; NAZARETH L, 1979, SIAM J NUMER ANAL, V16, P794, DOI 10.1137/0716059; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Yurii, 1983, DOKLADY AN USSR TRAN, V269; O'Donoghue B, 2016, J OPTIMIZ THEORY APP, V169, P1042, DOI 10.1007/s10957-016-0892-3; Paszke Adam, 2017, NIPS AUT WORKSH, DOI DOI 10.1017/CBO9781107707221.009; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Polson Nicholas G., 2015, PROXIMAL ALGORITHMS; Powell M. J. D., 1969, MATH PROGRAM, V14, P224; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; WOLFE P, 1971, SIAM REV, V13, P185, DOI 10.1137/1013035; WOLFE P, 1969, SIAM REV, V11, P226, DOI 10.1137/1011036; Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236; Zhu ZH, 2018, ADV NEUR IN, V31; Zhuang Y, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1103, DOI 10.1145/3269206.3271687	57	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302021
C	Letarte, G; Germain, P; Guedj, B; Laviolette, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Letarte, Gael; Germain, Pascal; Guedj, Benjamin; Laviolette, Francois			Dichotomize and Generalize: PAC-Bayesian Binary Activated Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present a comprehensive study of multilayer neural networks with binary activation, relying on the PAC-Bayesian theory. Our contributions are twofold: (i) we develop an end-to-end framework to train a binary activated deep neural network, (ii) we provide nonvacuous PAC-Bayesian generalization bounds for binary activated deep neural networks. Our results are obtained by minimizing the expected loss of an architecture-dependent aggregation of binary activated deep neural networks. Our analysis inherently overcomes the fact that binary activation function is non-differentiable. The performance of our approach is assessed on a thorough numerical experiment protocol on real-life datasets.	[Letarte, Gael; Laviolette, Francois] Univ Laval, Quebec City, PQ, Canada; [Germain, Pascal; Guedj, Benjamin] INRIA, Rocquencourt, France; [Guedj, Benjamin] UCL, London, England	Laval University; Inria; University of London; University College London	Letarte, G (corresponding author), Univ Laval, Quebec City, PQ, Canada.	gael.letarte.1@ulaval.ca; pascal.germain@inria.fr; benjamin.guedj@inria.fr; francois.laviolette@ift.ulaval.ca		Germain, Pascal/0000-0003-3998-9533; Guedj, Benjamin/0000-0003-1237-7430	French Project APRIORI [ANR-18-CE23-0015]; NSERC; Intact Financial Corporation	French Project APRIORI; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Intact Financial Corporation	We would like to thank Mario Marchand for the insight leading to the Theorem 3, Gabriel Dube and Jean-Samuel Leboeuf for their input on the theoretical aspects, Frederik Paradis for his help with the implementation, and Robert Gower for his insightful comments. This work was supported in part by the French Project APRIORI ANR-18-CE23-0015, in part by NSERC and in part by Intact Financial Corporation. We gratefully acknowledge the support of NVIDIA Corporation with the donation of Titan Xp GPUs used for this research.	Ambroladze A., 2006, NIPS; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Bengio Yoshua, 2013, ESTIMATING PROPAGATI, P4; Catoni O, 2004, STAT LEARNING THEORY; Catoni O., 2007, PAC BAYESIAN SUPERVI, V56; Catoni Olivier., 2003, PAC BAYESIAN APPROAC; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Dziugaite G. K., 2017, UAI; Germain Pascal, 2009, INT C MACH LEARN; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Guedj Benjamin, 2019, PRIMER PAC BAYESIAN; Hubara I, 2018, J MACH LEARN RES, V18; Hubara I, 2016, ADV NEUR IN, V29; Kingma D.P, P 3 INT C LEARNING R; Knoblauch Jeremias, 2019, GEN VARIATIONAL INTE; Lacasse Alexandre, 2010, THESIS; LANGFORD D, 2005, JMLR, V6, DOI DOI 10.1186/1471-2202-6-8; Langford John, 2002, NIPS; Langford John, 2001, NIPS, P809; Maurer A., 2004, CORR; McAllester David, 1999, MACHINE LEARNING, V37; McAllester David, 2003, MACHINE LEARNING, V51; Neyshabur Behnam, 2018, INT C LEARN REPR; Paradis F., 2018, POUTYNE KERAS LIKE F; Parrado-Hernandez Emilio, 2012, JMLR, V13; Paszke Adam, 2017, NIPS AUT WORKSH, DOI DOI 10.1017/CBO9781107707221.009; Seeger M., 2002, JMLR, V3; Shawe-Taylor J., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P2, DOI 10.1145/267460.267466; Soudry D., 2014, PROC 27 INT C NEURAL, P963, DOI DOI 10.5555/2968826.2968934; Valiant L. G., 1984, Communications of the ACM, V27, P1134, DOI 10.1145/1968.1972; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Yin Mingzhang, 2019, ICLR POSTER; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhou Wenda, 2019, ICLR	34	3	3	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306083
C	Li, C; Deng, C; Gao, SQ; Xie, D; Liu, W		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Chao; Deng, Cheng; Gao, Shangqian; Xie, De; Liu, Wei			Cross-Modal Learning with Adversarial Samples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					With the rapid developments of deep neural networks, numerous deep cross-modal analysis methods have been presented and are being applied in widespread real-world applications, including healthcare and safety-critical environments. However, the recent studies on robustness and stability of deep neural networks show that a microscopic modification, known as adversarial sample, which is even imperceptible to humans, can easily fool a well-performed deep neural network and brings a new obstacle to deep cross-modal correlation exploring. In this paper, we propose a novel Cross-Modal correlation Learning with Adversarial samples, namely CMLA, which for the first time presents the existence of adversarial samples in cross-modal data. Moreover, we provide a simple yet effective adversarial sample learning method, where inter- and intra- modality similarity regularizations across different modalities are simultaneously integrated into the learning of adversarial samples. Finally, our proposed CMLA is demonstrated to be highly effective in cross-modal hashing based retrieval. Extensive experiments on two cross-modal benchmark datasets show that the adversarial examples produced by our CMLA are efficient in fooling a target deep cross-modal hashing network. On the other hand, such adversarial examples can significantly strengthen the robustness of the target network by conducting an adversarial training.	[Li, Chao; Deng, Cheng; Xie, De] Xidian Univ, Sch Elect Engn, Xian, Shaanxi, Peoples R China; [Li, Chao; Gao, Shangqian] Univ Pittsburgh, Elect & Comp Engn, Pittsburgh, PA USA; [Liu, Wei] Tencent AI Lab, Bellevue, WA 98004 USA	Xidian University; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Deng, C (corresponding author), Xidian Univ, Sch Elect Engn, Xian, Shaanxi, Peoples R China.; Liu, W (corresponding author), Tencent AI Lab, Bellevue, WA 98004 USA.	chaolee.xd@gmail.com; chdeng.xd@gmail.com; shg84@pitt.edu; xiede.xd@gmail.com; wl2223@columbia.edu	Gao, Shangqian/AAE-5993-2022	Deng, Cheng/0000-0003-2620-3247; LI, CHAO/0000-0002-0609-9510; Liu, Wei/0000-0002-3865-8145	National Natural Science Foundation of China [61572388]; National Key Research and Development Program of China [2017YFE0104100]; Key R&D Program-The Key Industry Innovation Chain of Shaanxi [2018ZDXM-GY-176]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key Research and Development Program of China; Key R&D Program-The Key Industry Innovation Chain of Shaanxi	This work was partially supported by the National Natural Science Foundation of China 61572388, the National Key Research and Development Program of China (2017YFE0104100), and the Key R&D Program-The Key Industry Innovation Chain of Shaanxi under Grant 2018ZDXM-GY-176.	Abadi M, 2015, P 12 USENIX S OPERAT; Ateniese G., 2013, ARXIV13064447; Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928; Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286; Cao Y, 2018, LECT NOTES COMPUT SC, V11205, P207, DOI 10.1007/978-3-030-01246-5_13; Cao Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1445, DOI 10.1145/2939672.2939812; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Chen S.-T., 2018, P JOINT EUR C MACH L, P52; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Chua Tat-Seng, 2009, P ACM INT C IM VID R, P1, DOI DOI 10.1145/1646396.1646452; Deng C., 2019, IEEE T NEURAL NETWOR; Deng C, 2019, IEEE T IMAGE PROCESS, V28, P4032, DOI 10.1109/TIP.2019.2903661; Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921; Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G., 2015, ARXIV150302531; Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645; Huiskes Mark J, 2008, P 1 ACM INT C MULTIM, P39, DOI DOI 10.1145/1460096.1460104; JIANG QY, 2017, CVPR, P3270, DOI DOI 10.1109/CVPR.2017.348; Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13; Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446; Li CZ, 2019, AAAI CONF ARTIF INTE, P996; Li YQ, 2018, IEEE T PATTERN ANAL, V40, P1526, DOI 10.1109/TPAMI.2017.2710186; Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011; Liong VE, 2017, IEEE I CONF COMP VIS, P4097, DOI 10.1109/ICCV.2017.439; Liu W., 2014, ADV NEURAL INFORM PR, V4, P3419; Liu W, 2016, IEEE MULTIMEDIA, V23, P75, DOI 10.1109/MMUL.2016.39; Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912; Liu W, 2011, SER INF MANAGE SCI, V10, P1; Liu Wei, 2012, ICML, P467; Madry Aleksander, 2017, ARXIV; Metzen J. H., 2017, 5 INT C LEARNING REP, DOI DOI 10.1109/ICCV.2017.300; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Papernot Nicolas, 2016, ARXIV1605072722; Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598; Sun MY, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P793, DOI 10.1145/3219819.3219909; Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326; Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976; Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153; Xu X., 2017, ARXIV PREPRINT ARXIV; Yang EK, 2019, PROC CVPR IEEE, P2941, DOI 10.1109/CVPR.2019.00306; Yang E, 2018, IEEE T NEUR NET LEAR, V29, P5292, DOI 10.1109/TNNLS.2018.2793863; Yang EK, 2017, AAAI CONF ARTIF INTE, P1618; Yichen Shen, 2019, 2019 24th OptoElectronics and Communications Conference (OECC) and 2019 International Conference on Photonics in Switching and Computing (PSC), DOI 10.23919/PS.2019.8817791	49	3	3	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902042
C	Li, FS; Bowling, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Fushan; Bowling, Michael			Ease-of-Teaching and Language Structure from Emergent Communication	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				EVOLUTION	Artificial agents have been shown to learn to communicate when needed to complete a cooperative task. Some level of language structure (e.g., compositionality) has been found in the learned communication protocols. This observed structure is often the result of specific environmental pressures during training. By introducing new agents periodically to replace old ones, sequentially and within a population, we explore such a new pressure - ease of teaching - and show its impact on the structure of the resulting language.	[Li, Fushan; Bowling, Michael] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada	University of Alberta	Li, FS (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.	fushan@ualberta.ca; mbowling@ualberta.ca			NSERC; CIFAR through the Alberta Machine Intelligence Institute (Amii)	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); CIFAR through the Alberta Machine Intelligence Institute (Amii)	The authors would like to thank Angeliki Lazaridou, Jakob Foerster, Fei Fang for useful discussions. We would like to thank Yash Satsangi for comments on earlier versions of this paper. We would also like to thank Dale Schuurmans, Vadim Bulitko, and anonymous reviewers for valuable suggestions. This work was supported by NSERC, as well as CIFAR through the Alberta Machine Intelligence Institute (Amii).	Andreas Jacob, 2017, P 2017 C EMP METH NA, P2893; Bogin B., 2018, ARXIV180900549; Brighton H, 2006, ARTIF LIFE, V12, P229, DOI 10.1162/106454606776073323; Cao K, 2018, INT C LEARN REPR; de Freitas N, 2018, INT C LEARN REPR; Evtimova K, 2018, INT C LEARN REPR; Foerster JN, 2016, ADV NEUR IN, V29; Grover A., 2018, INT C MACH LEARN, P1797; Havrylov S, 2017, ADV NEUR IN, V30; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jaderberg M, 2019, SCIENCE, V364, P859, DOI 10.1126/science.aau6249; Jaques N, 2019, PR MACH LEARN RES, V97; Kingma D.P, P 3 INT C LEARNING R; Kirby S., 2008, P NATL ACAD SCI; Kirby S., 1998, LEARNING BOTTLENECKS; Kirby S, 2015, COGNITION, V141, P87, DOI 10.1016/j.cognition.2015.03.016; Kirby S, 2014, CURR OPIN NEUROBIOL, V28, P108, DOI 10.1016/j.conb.2014.07.014; Kottur Satwik, 2017, P 2017 C EMP METH NA, P2962, DOI DOI 10.18653/V1/D17-1321.URL; Lazaridou Angeliki, 2018, 6 INT C LEARN REPR I; Lazaridou Angeliki, 2016, ARXIV161207182; Lee Jooyoung, 2018, INT C LEARN REPR; Lowe R., 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295385; Lowe R, 2019, AAMAS '19: PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P693; Mnih V, 2016, PR MACH LEARN RES, V48; Mordatch I, 2018, AAAI CONF ARTIF INTE, P1495; Smith K, 2003, ADV COMPLEX SYST, V6, P537, DOI 10.1142/S0219525903001055; Smith K, 2003, ARTIF LIFE, V9, P371, DOI 10.1162/106454603322694825; Smith K, 2003, LECT NOTES ARTIF INT, V2801, P507; Sukhbaatar S., 2016, ADV NEURAL INF PROCE, V29, P2244, DOI DOI 10.5555/3157096.3157348; Vogt P, 2005, ARTIF INTELL, V167, P206, DOI 10.1016/j.artint.2005.04.010; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	31	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907050
C	Li, JCB; Qu, SH; Li, XJ; Szurley, J; Kolter, JZ; Metze, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Juncheng B.; Qu, Shuhui; Li, Xinjian; Szurley, Joseph; Kolter, J. Zico; Metze, Florian			Adversarial Music: Real World Audio Adversary Against Wake-word Detection System	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Voice Assistants (VAs) such as Amazon Alexa or Google Assistant rely on wake-word detection to respond to people's commands, which could potentially be vulnerable to audio adversarial examples. In this work, we target our attack on the wake-word detection system, jamming the model with some inconspicuous background music to deactivate the VAs while our audio adversary is present. We implemented an emulated wake-word detection system of Amazon Alexa based on recent publications. We validated our models against the real Alexa in terms of wake-word detection accuracy. Then we computed our audio adversaries with consideration of expectation over transform and we implemented our audio adversary with a differentiable synthesizer. Next we verified our audio adversaries digitally on hundreds of samples of utterances collected from the real world. Our experiments show that we can effectively reduce the recognition F1 score of our emulated model from 93.4% to 11.0%. Finally, we tested our audio adversary over the air, and verified it works effectively against Alexa, reducing its F1 score from 92.5% to 11.0%.(1) We also verified that non-adversarial music does not disable Alexa as effectively as our music at the same sound level. To the best of our knowledge, this is the first real-world adversarial attack against a commercial grade VA wake-word detection system.	[Li, Juncheng B.; Li, Xinjian; Kolter, J. Zico; Metze, Florian] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Szurley, Joseph; Kolter, J. Zico] Bosch Ctr Artificial Intelligence, Renningen, Germany; [Qu, Shuhui] Stanford Univ, Stanford, CA 94305 USA	Carnegie Mellon University; Stanford University	Li, JCB (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	junchenl@cs.cmu.edu; shuhuiq@stanford.edu; xinjianl@cs.cmu.edu; jszurley@bosch.com; zkolter@cs.cmu.edu; fmetze@cs.cmu.edu						ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599; Athalye A, 2018, PR MACH LEARN RES, V80; Carlini N, 2016, PROCEEDINGS OF THE 25TH USENIX SECURITY SYMPOSIUM, P513; Carlini Nicholas, 2018, ARXIV180101944; Cieri C., 2004, L REC, V4, P69; Cisse M., 2017, ARXIV170408847; Ebrahimi J, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P31; Elsayed Gamaleldin F, 2018, NIPS; Godfrey J. J., 1992, ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech and Signal Processing (Cat. No.92CH3103-9), P517, DOI 10.1109/ICASSP.1992.225858; Guo JX, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5489; Hoffmeister Bjbrn, 2019, ALEXA PITTSBURGHS SC; Ilyas Andrew, 2018, PRIOR CONVICTIONS BL; Ito K, 2017, LJ SPEECH DATASET; Iyyer M., 2018, P NAACL; JAFFE DA, 1983, COMPUT MUSIC J, V7, P56, DOI 10.2307/3680063; Kumatani K, 2017, 2017 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU), P252; Kurakin Alexey, 2016, WORKSHOP TRACK P, DOI DOI 10.48550/ARXIV.1607.02533; Li J. B., 2019, ARXIV190400759; Mitchell J. L., 2004, J ELECTRON IMAGING, V13, P399; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Naik Aakanksha, 2018, ARXIV180600692; Nguyen A., 2015, IEEE C COMP VIS PATT; Panchapagesan S, 2016, INTERSPEECH, P760, DOI 10.21437/Interspeech.2016-1485; Papernot N, 2016, IEEE MILIT COMMUN C, P49, DOI 10.1109/MILCOM.2016.7795300; Qin Y., 2019, ARXIV E PRINTS; Reddy Sravana, 2016, P 1 WORKSH NLP COMP, P17; Rousseau A, 2012, LREC 2012 - EIGHTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P125; Scheibler R, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P351; Schonherr L., 2018, ARXIV PREPRINT ARXIV; Smith Julius Orion, 1983, 14 CCRMA STANF U DEP; Szegedy Christian, 2013, INTRIGUING PROPERTIE, DOI 10.1364/BOE.8.000579; Wong E, 2018, PR MACH LEARN RES, V80; Woodhouse J, 2004, ACTA ACUST UNITED AC, V90, P945; Wu MH, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5494; Zhang GM, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P103, DOI 10.1145/3133956.3134052	35	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903053
C	Li, P; Li, XY; Zhang, CH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Ping; Li, Xiaoyun; Zhang, Cun-Hui			Re-randomized Densification for One Permutation Hashing and Bin-wise Consistent Weighted Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Jaccard similarity is widely used as a distance measure in many machine learning and search applications. Typically, hashing methods are essential for the use of Jaccard similarity to be practical in large-scale settings. For hashing binary (0/1) data, the idea of one permutation hashing (OPH) with densification significantly accelerates traditional minwise hashing algorithms while providing unbiased and accurate estimates. In this paper, we propose a "re-randomization" strategy in the process of densification and we show that it achieves the smallest variance among existing densification schemes. The success of this idea inspires us to generalize one permutation hashing to weighted (non-binary) data, resulting in the so-called "bin-wise consistent weighted sampling (BCWS)" algorithm. We analyze the behavior of BCWS and compare it with a recent alternative. Experiments on a range of datasets and tasks confirm the effectiveness of proposed methods. We expect that BCWS will be adopted in practice for training kernel machines and fast similarity search.	[Li, Ping] Baidu Res, Cognit Comp Lab, Bellevue, WA 98004 USA; [Li, Xiaoyun; Zhang, Cun-Hui] Rutgers State Univ, Dept Stat, Piscataway, NJ 08854 USA; [Li, Xiaoyun; Zhang, Cun-Hui] Baidu Res, Bellevue, WA USA	Baidu; Rutgers State University New Brunswick; Baidu	Li, P (corresponding author), Baidu Res, Cognit Comp Lab, Bellevue, WA 98004 USA.	liping11@baidu.com; xiaoyun.li@rutgers.edu; cunhui@stat.rutgers.edu						Bendersky M, 2009, P 2 ACM INT C WEB SE, P262; Bengio Y., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556; Bottou L., 2007, LARGE SCALE KERNEL M; Broder A. Z., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P327, DOI 10.1145/276698.276781; Broder AZ, 1997, COMPUT NETWORKS ISDN, V29, P1157, DOI 10.1016/S0169-7552(97)00031-7; Buehrer Gregory, 2008, P 2008 INT C WEB SEA, P95, DOI DOI 10.1145/1341531.1341547; Charikar M.S., 2002, P 34 ANN ACM S THEOR, V34, P380, DOI DOI 10.1145/509907.509965; Cherkasova L, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1087; Chierichetti F, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P219; Dourisboure Y, 2009, ACM T WEB, V3, DOI 10.1145/1513876.1513879; Fetterly D., 2003, PROCEEDING WWW 2003, P669, DOI DOI 10.1145/775152.775246; Forman George, 2009, Operating Systems Review, V43, P84, DOI 10.1145/1496909.1496926; Gollapudi S., 2006, P 15 ACM INT C INF K, P475; Gollapudi S, 2009, P 18 INT C WORLD WID, P381; Ioffe S., 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P246, DOI 10.1109/ICDM.2010.80; Jindal N., 2008, P 2008 INT C WEB SEA, P219, DOI DOI 10.1145/1341531.1341560; Kalpakis K, 2008, COMPUT COMMUN, V31, P1979, DOI 10.1016/j.comcom.2008.01.001; Kleinberg J., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P14, DOI 10.1109/SFFCS.1999.814572; Li P., 2012, NIPS; Li P., 2005, HLT EMNLP, P708; Li P, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P1053, DOI 10.1145/3038912.3052679; Li P, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P315, DOI 10.1145/3097983.3098081; Li P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P665, DOI 10.1145/2783258.2783406; Li Ping, 2018, ARXIV180502830; Manasse Mark, 2010, MSRTR201073; Najork M, 2009, P 2 ACM INT C WEB SE, P242; Pandey Sandeep, 2009, P 18 INT C WORLD WID, P441; Ping Lu, 2010, Proceedings 2010 Second WRI Global Congress on Intelligent Systems (GCIS 2010), P302, DOI 10.1109/GCIS.2010.57; Shrivastava Anshumali, 2017, P 34 INT C MACH LEAR, P3154; Shrivastava  Anshumali, 2016, ADV NEURAL INFORM PR, P1498; Shrivastava Anshumali, 2014, P 31 INT C MACH LEAR; Urvoy T, 2008, ACM T WEB, V2, DOI 10.1145/1326561.1326564	32	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907057
C	Linzner, D; Schmidt, M; Koeppl, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Linzner, Dominik; Schmidt, Michael; Koeppl, Heinz			Scalable Structure Learning of Continuous-Time Bayesian Networks from Incomplete Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				EXPRESSION	Continuous-time Bayesian Networks (CTBNs) represent a compact yet powerful framework for understanding multivariate time-series data. Given complete data, parameters and structure can be estimated efficiently in closed-form. However, if data is incomplete, the latent states of the CTBN have to be estimated by laboriously simulating the intractable dynamics of the assumed CTBN. This is a problem, especially for structure learning tasks, where this has to be done for each element of a super-exponentially growing set of possible structures. In order to circumvent this notorious bottleneck, we develop a novel gradient-based approach to structure learning. Instead of sampling and scoring all possible structures individually, we assume the generator of the CTBN to be composed as a mixture of generators stemming from different structures. In this framework, structure learning can be performed via a gradient-based optimization of mixture weights. We combine this approach with a new variational method that allows for a closed-form calculation of this mixture marginal likelihood. We show the scalability of our method by learning structures of previously inaccessible sizes from synthetic and real-world data.	[Linzner, Dominik; Schmidt, Michael; Koeppl, Heinz] Tech Univ Darmstadt, Dept Elect Engn & Informat Technol, Darmstadt, Germany; [Koeppl, Heinz] Tech Univ Darmstadt, Dept Biol, Darmstadt, Germany	Technical University of Darmstadt; Technical University of Darmstadt	Linzner, D (corresponding author), Tech Univ Darmstadt, Dept Elect Engn & Informat Technol, Darmstadt, Germany.	dominik.linzner@bcs.tu-darmstadt.de; michael.schmidt@bcs.tu-darmstadt.de; heinz.koeppl@bcs.tu-darmstadt.de			European Union [826121]; European Research Council (ERC) within the CONSYN project [773196]; Hessian research priority programme LOEWE within the project CompuGene	European Union(European Commission); European Research Council (ERC) within the CONSYN project(European Research Council (ERC)); Hessian research priority programme LOEWE within the project CompuGene	We thank the anonymous reviewers for helpful comments on the previous version of this manuscript. Dominik Linzner and Michael Schmidt are funded by the European Union's Horizon 2020 research and innovation programme (iPC-Pediatric Cure, No. 826121). Heinz Koeppl acknowledges support by the European Research Council (ERC) within the CONSYN project, No. 773196, and by the Hessian research priority programme LOEWE within the project CompuGene.	Acerbi E, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/s12859-014-0387-x; Battistin Claudia, 2017, Current Opinion in Systems Biology, V1, P122, DOI 10.1016/j.coisb.2016.12.010; Cantone I, 2009, CELL, V137, P172, DOI 10.1016/j.cell.2009.01.055; Cohn I, 2010, J MACH LEARN RES, V11, P2745; El-Hay T., 2010, P 27 INT C MACH LEAR, P343; El-Hay T., 2011, P 22 C UNC ART INT; Fan Yu, 2008, AI MATH; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Friedman N., 2010, PROBABILISTIC GRAPHI; GLAUBER RJ, 1963, J MATH PHYS, V4, P294, DOI 10.1063/1.1703954; Koller D., 2005, P 21 C UNC ART INT U, P421; Linzner D., 2018, ADV NEURAL INFORM PR, P7880; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Nandy P, 2018, ANN STAT, V46, P3151, DOI 10.1214/17-AOS1654; Nodelman U., 1995, P 18 C UNC ART INT, P378; Nodelman U., 2003, P 19 C UNC ART INT, P451; Opper M., 2008, ADV NEURAL INFORM PR, P1105; Penfold CA, 2011, INTERFACE FOCUS, V1, P857, DOI 10.1098/rsfs.2011.0053; Rao Vinayak, 2012, J MACHINE LEARNING R, V14, P3295; Schadt EE, 2005, NAT GENET, V37, P710, DOI 10.1038/ng1589; Spirtes P., 2000, CAUSATION PREDICTION; Studer L, 2016, AAAI CONF ARTIF INTE, P2051; Zou CL, 2009, BMC BIOINFORMATICS, V10, DOI 10.1186/1471-2105-10-122	23	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303070
C	Lorberbom, G; Gane, A; Jaakkola, T; Hazan, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lorberbom, Guy; Gane, Andreea; Jaakkola, Tommi; Hazan, Tamir			Direct Optimization through arg max for Discrete Variational Auto-Encoder	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Reparameterization of variational auto-encoders with continuous random variables is an effective method for reducing the variance of their gradient estimates. In the discrete case, one can perform reparametrization using the Gumbel-Max trick, but the resulting objective relies on an arg max operation and is non-differentiable. In contrast to previous works which resort to softmax-based relaxations, we propose to optimize it directly by applying the direct loss minimization approach. Our proposal extends naturally to structured discrete latent variable models when evaluating the arg max operation is tractable. We demonstrate empirically the effectiveness of the direct loss minimization technique in variational autoencoders with both unstructured and structured discrete latent variables.	[Lorberbom, Guy; Hazan, Tamir] Technion, Haifa, Israel; [Gane, Andreea; Jaakkola, Tommi] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Lorberbom, G (corresponding author), Technion, Haifa, Israel.							Andriyash E., 2018, ARXIV181000116; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Corro C., 2019, INT C LEARN REPR; Djolonga J, 2017, ADV NEUR IN, V30; Eslami SM, 2016, NEURIPS, V1; Goumas Georgios, 2008, ARXIV151105176, P283; Grathwohl Will, 2018, ICLR; Hazan Tamir, 2010, NEURIPS; Hu ZT, 2017, PR MACH LEARN RES, V70; Jang E., 2016, ARXIV; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kotz S, 2000, EXTREME VALUE DISTRI; Kusner MJ, 2017, PR MACH LEARN RES, V70; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Lawson D, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5799; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Maddison Chris J, 2017, ICLR; Mena G, 2018, INT C LEARN REPR ICL; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Mordatch I, 2018, AAAI CONF ARTIF INTE, P1495; Ranganath R, 2014, JMLR WORKSH CONF PRO, V33, P814; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Roeder G, 2017, ADV NEUR IN, V30; Rolfe J. T., 2016, ARXIV160902200; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Schuster-Bockler Benjamin, 2007, Curr Protoc Bioinformatics, VAppendix 3, p3A, DOI [10.1109/MASSP.1986.1165342, 10.1002/0471250953.bia03as18]; Shen Dinghan, 2018, ARXIV180505361; Song Y, 2016, PR MACH LEARN RES, V48; Tucker G, 2017, ADV NEUR IN, V30; Vahdat A., 2018, ADV NEURAL INFORM PR, V31, P1864; Vahdat Arash, 2018, ARXIV180204920; WILLIAMS N, 1992, DESIGN, P5; Xiao H., 2017, FASHION MNIST NOVEL; Yin M., 2019, INT C LEARN REPR; Yogatama Dani, 2016, ARXIV161109100	41	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306023
C	Louizos, C; Shi, XH; Schutte, K; Welling, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Louizos, Christos; Shi, Xiahan; Schutte, Klamer; Welling, Max			The Functional Neural Process	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MODELS	We present a new family of exchangeable stochastic processes, the Functional Neural Processes (FNPs). FNPs model distributions over functions by learning a graph of dependencies on top of latent representations of the points in the given dataset. In doing so, they define a Bayesian model without explicitly positing a prior distribution over latent global parameters; they instead adopt priors over the relational structure of the given dataset, a task that is much simpler. We show how we can learn such models from data, demonstrate that they are scalable to large datasets through mini-batch optimization and describe how we can make predictions for new points via their posterior predictive distribution. We experimentally evaluate FNPs on the tasks of toy regression and image classification and show that, when compared to baselines that employ global latent parameters, they offer both competitive predictions as well as more robust uncertainty estimates.	[Louizos, Christos; Welling, Max] Univ Amsterdam, Amsterdam, Netherlands; [Louizos, Christos; Schutte, Klamer] TNO Intelligent Imaging, Delft, Netherlands; [Shi, Xiahan] UvA Bosch Delta Lab, Bosch Ctr Artificial Intelligence, Amsterdam, Netherlands; [Welling, Max] Qualcomm, San Diego, CA USA	University of Amsterdam; Qualcomm	Louizos, C (corresponding author), Univ Amsterdam, Amsterdam, Netherlands.; Louizos, C (corresponding author), TNO Intelligent Imaging, Delft, Netherlands.	c.louizos@uva.nl; xiahan.shi@de.bosch.com; klamer.schutter@tno.nl; m.welling@uva.nl						Atanov Andrei, 2018, ARXIV181006943; Bae Juhan, 2018, ARXIV181112565; Bartunov Sergey, 2018, INT C ART INT STAT, P670; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Cai D, 2016, ELECTRON J STAT, V10, P3490, DOI 10.1214/16-EJS1185; Chen Xi, 2016, ARXIV161102731; Choi Hyunsun, 2018, WAIC WHY GENERATIVE; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cutajar K, 2017, PR MACH LEARN RES, V70; Damianou Andreas, 2013, ARTIF INTELL, P207, DOI DOI 10.1002/NME.1296; Datta A, 2016, J AM STAT ASSOC, V111, P800, DOI 10.1080/01621459.2015.1044091; Falorsi L, 2019, PR MACH LEARN RES, V89; Gal Y, 2016, PR MACH LEARN RES, V48; Garnelo M, 2018, ARXIV180701622; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Graves Alex, 2018, ARXIV180402476; Hafner D., 2018, ARXIV180709289; Hensman J, 2018, J MACH LEARN RES, V18, P1; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1861; Jang E., 2016, ARXIV; Kim Hyunjik, 2019, P INT C LEARN REPR; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Klenke A., 2013, PROBABILITY THEORY C; Koch G., 2015, ICML DEEP LEARNING W; Lee J., 2019, ARXIV190206720; Louizos C, 2017, ADV NEUR IN, V30; Louizos C, 2017, PR MACH LEARN RES, V70; Louizos C, 2016, PR MACH LEARN RES, V48; Ma Chao, 2018, ARXIV180602390; MACKAY DJC, 1995, NETWORK-COMP NEURAL, V6, P469, DOI 10.1088/0954-898X/6/3/011; Maddison Chris J, 2016, ARXIV161100712; Mattos Cesar Lincoln C, 2015, ARXIV151106644; Nalisnick Eric, 2018, INT C LEARN REPR; Neal R. M., 2012, BAYESIAN LEARNING NE; Novak Roman, 2018, BAYESIAN DEEP CONVOL; Orbanz P, 2015, IEEE T PATTERN ANAL, V37, P437, DOI 10.1109/TPAMI.2014.2334607; Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Shaked M, 2007, SPRINGER SER STAT, P3; Shi J., 2017, ARXIV170510119; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Snelson E., 2006, ADV NEURAL INFORM PR, V18, P1259; Sun S., 2019, INT C LEARN REPR; Sung Flood, 2018, LEARNING COMP RELATI; Tang Da, 2019, ARXIV190505335; Titsias M. K., 2009, ARTIF INTELL STAT, V3; van der Wilk M, 2017, ADV NEUR IN, V30; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wilson AG, 2016, ADV NEUR IN, V29; Wilson AG, 2016, JMLR WORKSH CONF PRO, V51, P370; Zhang G., 2017, ARXIV171202390	56	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900035
C	Lu, XY; Van Roy, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lu, Xiuyuan; Van Roy, Benjamin			Information-Theoretic Confidence Bounds for Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				REGRET BOUNDS	We integrate information-theoretic concepts into the design and analysis of optimistic algorithms and Thompson sampling. By making a connection between information-theoretic quantities and confidence bounds, we obtain results that relate the per-period performance of the agent with its information gain about the environment, thus explicitly characterizing the exploration-exploitation tradeoff. The resulting cumulative regret bound depends on the agent's uncertainty over the environment and quantifies the value of prior information. We show applicability of this approach to several environments, including linear bandits, tabular MDPs, and factored MDPs. These examples demonstrate the potential of a general information-theoretic approach for the design and analysis of reinforcement learning algorithms.	[Lu, Xiuyuan; Van Roy, Benjamin] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Lu, XY (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	lxy@stanford.edu; bvr@stanford.edu			Charles and Katherine Lin Graduate Fellowship; Dantzig-Lieberman Operations Research Fellowship; Boeing	Charles and Katherine Lin Graduate Fellowship; Dantzig-Lieberman Operations Research Fellowship; Boeing	This work was generously supported by the Charles and Katherine Lin Graduate Fellowship, the Dantzig-Lieberman Operations Research Fellowship, and a research grant from Boeing. We would also like to thank Ayfer Ozgur and Daniel Russo for helpful discussions.	Agrawal S., 2012, P 21 ANN C LEARN THE; Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Boutilier C, 2000, ARTIF INTELL, V121, P49, DOI 10.1016/S0004-3702(00)00033-3; Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Dani V, 2008, P C LEARN THEOR COLT, P355; Ghahramani Z, 1998, LECT NOTES ARTIF INT, V1387, P168, DOI 10.1007/BFb0053999; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Kauffmann Emilie, 2012, INT C ALG LEARN THEO; Kaufmann E., 2012, C ART INT STAT AISTA; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Osband I., 2014, ADV NEURAL INFORM PR, V27, P604; Osband Ian, 2013, ADV NEURAL INFORM PR; Russo D, 2016, J MACH LEARN RES, V17; Srinivas N, 2012, IEEE T INFORM THEORY, V58, P3250, DOI 10.1109/TIT.2011.2182033; Szita I., 2009, P 26 INT C MACH LEAR, P1001; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285	18	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302046
C	Maheswaranathan, N; Williams, AH; Golub, MD; Ganguli, S; Sussillo, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Maheswaranathan, Niru; Williams, Alex H.; Golub, Matthew D.; Ganguli, Surya; Sussillo, David			Universality and individuality in neural dynamics across large populations of recurrent networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MODELS; COMPUTATION; CHAOS	Task-based modeling with recurrent neural networks (RNNs) has emerged as a popular way to infer the computational function of different brain regions. These models are quantitatively assessed by comparing the low-dimensional neural representations of the model with the brain, for example using canonical correlation analysis (CCA). However, the nature of the detailed neurobiological inferences one can draw from such efforts remains elusive. For example, to what extent does training neural networks to solve common tasks uniquely determine the network dynamics, independent of modeling architectural choices? Or alternatively, are the learned dynamics highly sensitive to different model choices? Knowing the answer to these questions has strong implications for whether and how we should use task-based RNN modeling to understand brain dynamics. To address these foundational questions, we study populations of thousands of networks, with commonly used RNN architectures, trained to solve neuroscientifically motivated tasks and characterize their nonlinear dynamics. We find the geometry of the RNN representations can be highly sensitive to different network architectures, yielding a cautionary tale for measures of similarity that rely on representational geometry, such as CCA. Moreover, we find that while the geometry of neural dynamics can vary greatly across architectures, the underlying computational scaffold-the topological structure of fixed points, transitions between them, limit cycles, and linearized dynamics-often appears universal across all architectures.	[Maheswaranathan, Niru; Sussillo, David] Google Inc, Google Brain, Mountain View, CA 94043 USA; [Williams, Alex H.; Golub, Matthew D.; Ganguli, Surya] Stanford Univ, Stanford, CA 94305 USA; [Ganguli, Surya] Google Brain, Mountain View, CA USA	Google Incorporated; Stanford University; Google Incorporated	Sussillo, D (corresponding author), Google Inc, Google Brain, Mountain View, CA 94043 USA.	nirum@google.com; ahwillia@stanford.edu; mgolub@stanford.edu; sganguli@stanford.edu; sussillo@google.com			Stanford Neurosciences Institute; Office of Naval Research [N00014-18-1-2158]	Stanford Neurosciences Institute; Office of Naval Research(Office of Naval Research)	The authors would like to thank Jeffrey Pennington, Maithra Raghu, Jascha Sohl-Dickstein, and Larry Abbott for helpful feedback and discussions. MDG was supported by the Stanford Neurosciences Institute, the Office ohf Naval Researc Grant #N00014-18-1-2158.	Banino A, 2018, NATURE, V557, P429, DOI 10.1038/s41586-018-0102-6; Barak O, 2017, CURR OPIN NEUROBIOL, V46, P1, DOI 10.1016/j.conb.2017.06.003; Barak O, 2013, PROG NEUROBIOL, V103, P214, DOI 10.1016/j.pneurobio.2013.02.002; Barrett DGT, 2019, CURR OPIN NEUROBIOL, V55, P55, DOI 10.1016/j.conb.2019.01.007; Bengio Y., 2014, ARXIV14061078; Collins J., 2017, ICLR; Cueva C. J, 2018, ARXIV180307770; De Vries SEJ., 2018, LARGE SCALE STANDARD, DOI [10.1101/359513, DOI 10.1101/359513]; Destexhe A, 2009, BIOL CYBERN, V101, P1, DOI 10.1007/s00422-009-0328-3; Doya Kenji, 1993, DEP BIOL UCSD TECH R; Feigenbaum M. J., 2017, UNIVERSALITY CHAOS, V2, P49; Golub M., 2018, J OPEN SOURCE SOFTW, V3, P1003, DOI [10.21105/joss.01003, DOI 10.21105/JOSS.01003]; Guan Melody, 2018, ICML, P4095; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Jordan Ian D., 2019, CORR; Kanitscheider I, 2017, ADV NEUR IN, V30; Kell AJE, 2018, NEURON, V98, P630, DOI 10.1016/j.neuron.2018.03.044; Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915; Kornblith S, 2019, PR MACH LEARN RES, V97; Kriegeskorte N, 2013, TRENDS COGN SCI, V17, P401, DOI 10.1016/j.tics.2013.06.007; Le Q.V., 2015, ABS150400941 CORR; Maheswaranathan N, 2018, BIORXIV340943, DOI [10.1101/340943v1, DOI 10.1101/340943, 10.1101/340943]; Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742; Mastrogiuseppe F, 2018, NEURON, V99, P609, DOI 10.1016/j.neuron.2018.07.003; McIntosh LT, 2016, ADV NEUR IN, V29; Merity Stephen, 2018, INT C LEARN REPR; Morcos Ari S., 2018, NEURIPS; Orhan AE, 2019, NAT NEUROSCI, V22, P275, DOI 10.1038/s41593-018-0314-y; Pandarinath C, 2018, NAT METHODS, V15, P805, DOI 10.1038/s41592-018-0109-9; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; Poole B, 2016, ADV NEUR IN, V29; Raghu M, 2017, PR MACH LEARN RES, V70; Raghu Maithra, 2017, ADV NEURAL INFORM PR, P6076; Rajalingham R, 2018, J NEUROSCI, V38, P7255, DOI 10.1523/JNEUROSCI.0388-18.2018; Rajan K, 2016, NEURON, V90, P128, DOI 10.1016/j.neuron.2016.02.009; Recanatesi S., 2018, BIORXIV, DOI [10.1101/471987, DOI 10.1101/471987]; Remington ED, 2018, NEURON, V98, P1005, DOI 10.1016/j.neuron.2018.05.020; Rivkind A, 2017, PHYS REV LETT, V118, DOI 10.1103/PhysRevLett.118.258101; Saxe AM, 2019, P NATL ACAD SCI USA, V116, P11537, DOI 10.1073/pnas.1820226116; Seung HS, 1996, P NATL ACAD SCI USA, V93, P13339, DOI 10.1073/pnas.93.23.13339; Sireci SG, 2003, J EDUC MEAS, V40, P277, DOI 10.1111/j.1745-3984.2003.tb01108.x; SOMPOLINSKY H, 1988, PHYS REV LETT, V61, P259, DOI 10.1103/PhysRevLett.61.259; Stanley H.E., 1971, INTRO PHASE TRANSITI; Sussillo D, 2015, NAT NEUROSCI, V18, P1025, DOI 10.1038/nn.4042; Sussillo D, 2014, CURR OPIN NEUROBIOL, V25, P156, DOI 10.1016/j.conb.2014.01.008; Sussillo D, 2013, NEURAL COMPUT, V25, P626, DOI 10.1162/NECO_a_00409; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Wang J, 2018, NAT NEUROSCI, V21, P102, DOI 10.1038/s41593-017-0028-6; Wisdom S, 2016, ADV NEUR IN, V29; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111; Yang GR, 2019, NAT NEUROSCI, V22, P297, DOI 10.1038/s41593-018-0310-2; Zoph B., 2017, P1	57	3	3	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907030
C	Mannelli, SS; Biroli, G; Cammarota, C; Krzakala, F; Zdeborova, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mannelli, Stefano Sarao; Biroli, Giulio; Cammarota, Chiara; Krzakala, Florent; Zdeborova, Lenka			Who is Afraid of Big Bad Minima? Analysis of Gradient-Flow in a Spiked Matrix-Tensor Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SPIN-GLASS MODEL; DYNAMICS	Gradient-based algorithms are effective for many machine learning tasks, but despite ample recent effort and some progress, it often remains unclear why they work in practice in optimising high-dimensional non-convex functions and why they find good minima instead of being trapped in spurious ones. Here we present a quantitative theory explaining this behaviour in a spiked matrix-tensor model. Our framework is based on the Kac-Rice analysis of stationary points and a closed-form analysis of gradient-flow originating from statistical physics. We show that there is a well defined region of parameters where the gradient-flow algorithm finds a good global minimum despite the presence of exponentially many spurious local minima. We show that this is achieved by surfing on saddles that have strong negative direction towards the global minima, a phenomenon that is connected to a BBP-type threshold in the Hessian describing the critical points of the landscapes.	[Mannelli, Stefano Sarao; Zdeborova, Lenka] CNRS, Inst Phys Theor, Paris, France; [Mannelli, Stefano Sarao; Zdeborova, Lenka] CEA, Paris, France; [Mannelli, Stefano Sarao; Zdeborova, Lenka] Univ Paris Saclay, Paris, France; [Biroli, Giulio; Krzakala, Florent] Univ Paris Diderot, Sorbonne Univ, Univ PSL,,Ecole Normale Super ENS, Sorbonne Paris Cite,Lab Phys,CNRS, Paris, France; [Cammarota, Chiara] Kings Coll London, Dept Math, London WC2R 2LS, England	CEA; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Cite; CEA; UDICE-French Research Universities; Universite Paris Saclay; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Sorbonne Universite; Universite Paris Cite; University of London; King's College London	Mannelli, SS (corresponding author), CNRS, Inst Phys Theor, Paris, France.; Mannelli, SS (corresponding author), CEA, Paris, France.; Mannelli, SS (corresponding author), Univ Paris Saclay, Paris, France.		Cammarota, Chiara/C-5038-2014	Cammarota, Chiara/0000-0003-3443-6905	National Science Foundation [NSF PHY-1748958]; ERC [714608-SMiLe]; European Union [823748]; French National Research Agency (ANR); Simons Foundation [454935]	National Science Foundation(National Science Foundation (NSF)); ERC(European Research Council (ERC)European Commission); European Union(European Commission); French National Research Agency (ANR)(French National Research Agency (ANR)); Simons Foundation	We thank Pierfrancesco Urbani for proof checking of the draft and many related discussions. We would also like to thank the Kavli Institute for Theoretical Physics (KITP) for welcoming us during part of this research, with the support of the National Science Foundation under Grant No. NSF PHY-1748958 We acknowledge funding from the ERC under the European Union's Horizon 2020 Research and Innovation Programme Grant Agreement 714608-SMiLe; from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement CoSP No 823748; from the French National Research Agency (ANR) grant PAIL; and from the Simons Foundation (#454935, Giulio Biroli).	Adler RJ., 2009, RANDOM FIELDS GEOMET; Anandkumar Anima, 2016, ARXIV161009322; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; [Anonymous], 2017, ADV NEURAL INF PROCE; [Anonymous], 2016, ADV NEURAL INFORM PR; Arous Gerard Ben, 2017, ARXIV171105424; Auffinger A, 2013, COMMUN PUR APPL MATH, V66, P165, DOI 10.1002/cpa.21422; Ben Arous G, 2006, PROBAB THEORY REL, V136, P619, DOI 10.1007/s00440-005-0491-y; Bhojanapalli S., 2016, ADV NEURAL INFORM PR, P3873; Bouchaud J.-P., 1998, SPIN GLASSES RANDOM, P161, DOI [10.1142/97898128194370006, DOI 10.1142/97898128194370006, DOI 10.1142/3517]; CRISANTI A, 1992, Z PHYS B CON MAT, V87, P341, DOI 10.1007/BF01309287; CRISANTI A, 1993, Z PHYS B CON MAT, V92, P257, DOI 10.1007/BF01312184; Cugliandolo L. F., 2003, SLOW RELAXATIONS NON, P367, DOI [10.1007/978-3-540-44835-8, DOI 10.1007/978-3-540-44835-8, DOI 10.1007/978-3-540-44835-8_]; CUGLIANDOLO LF, 1993, PHYS REV LETT, V71, P173, DOI 10.1103/PhysRevLett.71.173; CUGLIANDOLO LF, 1994, J PHYS A-MATH GEN, V27, P5749, DOI 10.1088/0305-4470/27/17/011; CUGLIANDOLO LF, 1995, PHILOS MAG B, V71, P501, DOI 10.1080/01418639508238541; Deshpande Y, 2014, IEEE INT SYMP INFO, P2197, DOI 10.1109/ISIT.2014.6875223; Du SS, 2018, PR MACH LEARN RES, V80; Freeman C Daniel, 2017, ARXIV161101540; Fyodorov YV, 2004, PHYS REV LETT, V92, DOI 10.1103/PhysRevLett.92.240601; Ge R, 2017, PR MACH LEARN RES, V70; GROSS DJ, 1984, NUCL PHYS B, V240, P431, DOI 10.1016/0550-3213(84)90237-2; Johnstone IM, 2009, J AM STAT ASSOC, V104, P682, DOI 10.1198/jasa.2009.0121; Kac M., 1943, B AM MATH SOC, V49, P314, DOI [DOI 10.1090/S0002-9904-1943-07912-8, 10.1090/S0002-9904-1943-07912-8]; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Ling Shuyang, 2018, ARXIV180911083; Lu H., 2017, ARXIV PREPRINT ARXIV; Mannelli S. S., 2019, P MACHINE LEARNING R, V97, P4333; Mannelli S. S., 2018, ARXIV181209066; Rice SO, 1944, BELL SYST TECH J, V23, P282, DOI 10.1002/j.1538-7305.1944.tb00874.x; Richard E., 2014, P ADV NEUR INF PROC, P2897; Ros V, 2019, PHYS REV X, V9, DOI 10.1103/PhysRevX.9.011003; Sagun L., 2014, ARXIV14126615; Soudry D., 2016, ARXIV PREPRINT ARXIV; Zamfir Pompiliu Manuel, 2008, ARXIV08063519	38	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900029
C	Meister, M; Sarlos, T; Woodruff, DP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Meister, Michela; Sarlos, Tamas; Woodruff, David P.			Tight Dimensionality Reduction for Sketching Low Degree Polynomial Kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We revisit the classic randomized sketch of a tensor product of q vectors x(i) is an element of R-n. The i-th coordinate (Sx)(i) of the sketch is equal to Pi(q)(j=1) (u(i,j), x(j))/ root m, where u(i,j) are independent random sign vectors. Kar and Karnick (JMLR, 2012) show that if the sketching dimension m = Omega(epsilon(-2) C(Omega)(2)log(1/delta)), where C-Omega is a certain property of the point set Omega one wants to sketch, then with probability 1 - delta, parallel to Sx parallel to(2) = (1 +/- epsilon)parallel to x parallel to(2) for all x epsilon Omega. However, in their analysis C-Omega(2) can be as large as Theta(n(2q)), even for a set Omega of O(1) vectors x. We give a new analysis of this sketch, providing nearly optimal bounds. Namely, we show an upper bound of m = Theta(epsilon(-2) log(1/delta) + epsilon(-1) logq(n/delta)), which by composing with CountSketch, can be improved to Theta(epsilon(-2) log(1/delta epsilon) + epsilon(-1) log(q) (1/(delta epsilon)). For the important case of q = 2 and delta = 1/poly(n), this shows that m = Omega(epsilon(-2) log(n) + epsilon(-1) log(2)(n)), demonstrating that the epsilon(-2) and log(2)(n) terms do not multiply each other. We also show a nearly matching lower bound of m = Omega(epsilon(-2) log(1/delta)) + epsilon(-1) log(q) (1/(delta))). In a number of applications, one has vertical bar Omega vertical bar = poly(n) and in this case our bounds are optimal up to a constant factor. This is the first high probability sketch for tensor products that has optimal sketch size and can be implemented in m . Sigma(q)(i=1) nnz(x(i)) time, where nnz(x(i)) is the number of non -zero entries of x(i). Lastly, we empirically compare our sketch to other sketches for tensor products, and give a novel application to compressing neural networks.	[Meister, Michela] Cornell Univ, Ithaca, NY 14850 USA; [Meister, Michela; Sarlos, Tamas; Woodruff, David P.] Google Res, Mountain View, CA 94043 USA; [Woodruff, David P.] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA; [Woodruff, David P.] Simons Inst Theory Comp, Berkeley, CA USA	Cornell University; Google Incorporated; Carnegie Mellon University	Meister, M (corresponding author), Cornell Univ, Ithaca, NY 14850 USA.	meister.michela@gmail.com; stamas@google.com; dwoodruf@cs.cmu.edu						Abadi M, 2015, P 12 USENIX S OPERAT; Ahle Thomas D, 2020, SODA; Ahle Thomas D., 2019, CORR; Andoni Alexandr, 2015, ADV NEURAL INFORM PR, P1225; Arora S, 2018, PR MACH LEARN RES, V80; Arriaga R. I., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P616, DOI 10.1109/SFFCS.1999.814637; Blum A, 2006, LECT NOTES COMPUT SC, V3940, P52; Braverman Vladimir, 2010, 27 INT S THEOR ASP C; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Charikar M, 2002, LECT NOTES COMPUT SC, V2380, P693; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Cohen Michael B, 2018, 1 S SIMPL ALG SOSA S; Cotter A., 2011, ARXIV110947603; Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073; Dheeru D., 2019, UCI MACHINE LEARNING; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Freksen C., 2018, ADV NEURAL INFORM PR, P5394; Har-Peled S., 2012, THEORY COMPUT, V8, P321, DOI DOI 10.4086/TOC.2012.V008A014; Indyk P, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P737; Jayram TS, 2013, ACM T ALGORITHMS, V9, DOI 10.1145/2483699.2483706; Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400; Kane Daniel, 2011, Approximation, Randomization, and Combinatorial Optimization Algorithms and Techniques. Proceedings 14th International Workshop, APPROX 2011 and 15th International Workshop, RANDOM 2011, P628, DOI 10.1007/978-3-642-22935-0_53; Kane DM, 2014, J ACM, V61, DOI 10.1145/2559902; Kapralov Michael, 2019, CORR; Kar P., 2012, ARTIF INTELL, P583; Larsen KG, 2017, ANN IEEE SYMP FOUND, P633, DOI 10.1109/FOCS.2017.64; LeCun Y., 2010, MNIST HANDWRITTEN DI; Muthukrishnan S, 2005, FOUND TRENDS THEOR C, V1, P1, DOI 10.1561/0400000002; Pham N, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P239, DOI 10.1145/2487575.2487591; Pagh R., 2012, P 3 INN THEOR COMP S, P442; Tao Terence, 2010, MATH 254A NOTES 1 CO; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060	35	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901014
C	Mitrovic, M; Kazemi, E; Feldman, M; Krause, A; Karbasi, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mitrovic, Marko; Kazemi, Ehsan; Feldman, Moran; Krause, Andreas; Karbasi, Amin			Adaptive Sequence Submodularity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In many machine learning applications, one needs to interactively select a sequence of items (e.g., recommending movies based on a user's feedback) or make sequential decisions in a certain order (e.g., guiding an agent through a series of states). Not only do sequences already pose a dauntingly large search space, but we must also take into account past observations, as well as the uncertainty of future outcomes. Without further structure, finding an optimal sequence is notoriously challenging, if not completely intractable. In this paper, we view the problem of adaptive and sequential decision making through the lens of submodularity and propose an adaptive greedy policy with strong theoretical guarantees. Additionally, to demonstrate the practical utility of our results, we run experiments on Amazon product recommendation and Wikipedia link prediction tasks.	[Mitrovic, Marko; Kazemi, Ehsan; Karbasi, Amin] Yale Univ, New Haven, CT 06520 USA; [Feldman, Moran] Univ Haifa, Haifa, Israel; [Krause, Andreas] Swiss Fed Inst Technol, Zurich, Switzerland	Yale University; University of Haifa; Swiss Federal Institutes of Technology Domain; ETH Zurich	Mitrovic, M (corresponding author), Yale Univ, New Haven, CT 06520 USA.	marko.mitrovic@yale.edu; ehsan.kazemi@yale.edu; moranfe@openu.ac.il; krausea@ethz.ch; amin.karbasi@yale.edu	Gurfil, Pini/AAA-4072-2020; Kazemi, Ehsan/G-7735-2015	Krause, Andreas/0000-0001-7260-9673; Kazemi, Ehsan/0000-0001-8427-054X	NSF [IIS-1845032]; ONR [N00014-19-1-2406]; AFOSR [FA9550-18-1-0160]; ISF [1357/16]; ERC StG SCADAPT	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); ISF(Israel Science Foundation); ERC StG SCADAPT	This work was partially supported by NSF (IIS-1845032), ONR (N00014-19-1-2406), AFOSR (FA9550-18-1-0160), ISF (1357/16), and ERC StG SCADAPT.	Alaei S., 2010, ARXIV PREPRINT ARXIV; [Anonymous], 2010, PROC 16 ACM SIGKDD I, DOI DOI 10.1145/1835804.1835933; Bach F, 2015, ARXIV151100394; Badanidiyuru A, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P671, DOI 10.1145/2623330.2623637; Bai WR, 2018, ADV NEUR IN, V31; Balkanski E, 2018, ACM S THEORY COMPUT, P1138, DOI 10.1145/3188745.3188752; Barbosa R, 2015, PR MACH LEARN RES, V37, P1236; Bhaskara A, 2010, ACM S THEORY COMPUT, P201; Bogunovic I., 2017, INT C MACH LEARN ICM; Buchbinder N., 2015, P ACM SIAM S DISCRET, P1202; Chakrabarti Amit, 2014, Integer Programming and Combinatorial Optimization. 17th International Conference, IPCO 2014. Proceedings: LNCS 8494, P210, DOI 10.1007/978-3-319-07557-0_18; Chen Y., 2013, INT C MACHINE LEARNI, P160; Das A, 2011, P 28 INT C INT C MAC; Elenberg E. R., 2016, CORR; Elenberg Ethan, 2018, ADV NEURAL INFORM PR; Elenberg ER, 2017, ADV NEUR IN, V30; Ene A, 2019, P 30 ANN ACM SIAM S, P274; FELDMAN M, 2018, ADV NEURAL INFORM PR, P735; Fujii K, 2019, PR MACH LEARN RES, V97; Gabillon Victor, 2013, NIPS; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; Gomes R., 2010, P 27 INT C MACHINE L, P391; Gotovos A, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1996; Hassani S. H., 2017, NIPS, P5843; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kazemi E, 2019, PR MACH LEARN RES, V97; Kazemi E, 2018, PR MACH LEARN RES, V80; Kempe D., 2003, ACM SIGKDD INT C KNO, P137, DOI DOI 10.1145/956750.956769; Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.2307/2332226; Khanna R, 2017, PR MACH LEARN RES, V54, P1560; Kirchhoff K., 2014, P 2014 C EMPIRICAL M, P131, DOI [10.3115/v1/D14-1014, DOI 10.3115/V1/D14-1014]; Kortsarz G., 1993, Proceedings. 34th Annual Symposium on Foundations of Computer Science (Cat. No.93CH3368-8), P692, DOI 10.1109/SFCS.1993.366818; Krause A., 2005, P 21 C UNCERTAINTY A, P324, DOI DOI 10.5555/3020336.3020377; Krause A, 2008, J MACH LEARN RES, V9, P235; Krause A, 2008, J MACH LEARN RES, V9, P2761; Kumar R., 2013, PROC SEG ANN M, P1, DOI [10.1145/2809814, DOI 10.1145/2809814]; Li P, 2017, ADV NEUR IN, V30; Lin Hui, 2011, P 49 ANN M ASS COMP, P510; Manurangsi P, 2017, ACM S THEORY COMPUT, P954, DOI 10.1145/3055399.3055412; McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755; Mirrokni V, 2015, ACM S THEORY COMPUT, P153, DOI 10.1145/2746539.2746624; Mitrovic M, 2018, PR MACH LEARN RES, V80; Mitrovic M, 2018, PR MACH LEARN RES, V84; Norouzi-Fard A, 2018, PR MACH LEARN RES, V80; Settles B., 2012, SYNTH LECT ARTIF INT, V6, P1; Singer Y, 2019, P 30 ANN ACM SIAM S, P283; Singla A, 2014, PR MACH LEARN RES, V32, P154; Staib M., 2017, P 34 INT C MACH LEAR, P3230; Staib Matthew, 2018, CORR; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Tschiatschek S, 2017, AAAI CONF ARTIF INTE, P2667; Tzoumas V, 2017, IEEE DECIS CONTR P; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; West R., 2009, INT JOINT C ART INT; WOLSEY LA, 1982, COMBINATORICA, V2, P385, DOI 10.1007/BF02579435; Yue Y., 2011, ADV NEURAL INFORM PR; Zadimoghaddam M, 2019, P 30 ANN ACM SIAM S, P255, DOI DOI 10.1137/1.9781611975482.17; Zhang ZL, 2016, IEEE T AUTOMAT CONTR, V61, P601, DOI 10.1109/TAC.2015.2440566	60	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305036
C	Nachum, O; Chow, Y; Dai, B; Li, LH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Nachum, Ofir; Chow, Yinlam; Dai, Bo; Li, Lihong			DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				RATIO	In many real-world reinforcement learning applications, access to the environment is limited to a fixed dataset, instead of direct (online) interaction with the environment. When using this data for either evaluation or training of a new policy, accurate estimates of discounted stationary distribution ratios- correction terms which quantify the likelihood that the new policy will experience a certain state-action pair normalized by the probability with which the state-action pair appears in the dataset - can improve accuracy and performance. In this work, we propose an algorithm, DualDICE, for estimating these quantities. In contrast to previous approaches, our algorithm is agnostic to knowledge of the behavior policy (or policies) used to generate the dataset. Furthermore, it eschews any direct use of importance weights, thus avoiding potential optimization instabilities endemic of previous methods. In addition to providing theoretical guarantees, we present an empirical study of our algorithm applied to off-policy policy evaluation and find that our algorithm significantly improves accuracy compared to existing techniques.(1)	[Nachum, Ofir; Chow, Yinlam; Dai, Bo; Li, Lihong] Google Res, Mountain View, CA 94043 USA	Google Incorporated	Nachum, O (corresponding author), Google Res, Mountain View, CA 94043 USA.	ofirnachum@google.com; yinlamchow@google.com; bodai@google.com; lihong@google.com						Bellman R. E., 2003, DYNAMIC PROGRAMMING; Bickel Steffen, 2007, P 24 INT C MACH LEAR, DOI DOI 10.1145/1273496.1273507; Brockman G., 2016, OPENAI GYM; Cao LL, 2018, MINERAL MET MAT SER, P353, DOI 10.1007/978-3-319-72138-5_36; Cheng KF, 2004, BERNOULLI, V10, P583, DOI 10.3150/bj/1093265631; Dai B., 2016, ARXIV160704579; Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639; Dudik Miroslav, 2011, ICML; Farajtabar M., 2018, ARXIV180203493; Fonteneau R, 2013, ANN OPER RES, V208, P383, DOI 10.1007/s10479-012-1248-5; Gelada Carles, 2018, AAAI; Gretton A, 2009, NEURAL INF PROCESS S, P131; Hallak Assaf, 2017, P 34 INT C MACH LEAR, V70, P1372; Hastings W.K., 1970, MONTE CARLO SAMPLING; Jiang N, 2016, PR MACH LEARN RES, V48; Kanamori T, 2009, J MACH LEARN RES, V10, P1391; Li Jiwei, 2016, P 2016 C EMP METH NA; Li LJ, 2011, METAGENOMICS OF THE HUMAN BODY, P297, DOI 10.1007/978-1-4419-7089-3_14; Li LH, 2015, JMLR WORKSH CONF PRO, V38, P608; Liu Yao, 2019, P 35 C UNC ART INT; Mahmood A, 2014, 2014 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P2700, DOI 10.1109/ICACCI.2014.6968323; Mandel T, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1077; Mnih V., 2013, ARXIV PREPRINT ARXIV; Murphy SA, 2001, J AM STAT ASSOC, V96, P1410, DOI 10.1198/016214501753382327; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Paduraru C., 2013, THESIS; Precup D., 2001, P 18 INT C MACH LEAR, P417; PRECUP D, 2001, P 18 INT C MACH LEAR, P417; Precup D., 2000, INT C MACH LEARN, P759; Precup D., 2000, P 17 INT C MACH LEAR; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Qin J, 1998, BIOMETRIKA, V85, P619, DOI 10.1093/biomet/85.3.619; Rockafellar R.T., 2015, CONVEX ANAL; Rockafellar R.T., 2009, VARIATIONAL ANAL, V317; Soman S, 2015, 2015 INTERNATIONAL CONFERENCE ON EMERGING RESEARCH IN ELECTRONICS, COMPUTER SCIENCE AND TECHNOLOGY (ICERECT), P298, DOI 10.1109/ERECT.2015.7499030; Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613; Sugiyama M, 2008, ANN I STAT MATH, V60, P699, DOI 10.1007/s10463-008-0197-x; Sutton R. S., 2008, P 24 C UNC ART INT, P528; Sutton R. S., INTRO REINFORCEMENT, V135; Sutton R. S., 2016, J MACHINE LEARNING R, V17, P2603; Swaminathan Adith, 2017, ADV NEURAL INFORM PR; Thomas P., 2016, INT C MACH LEARN, P2139; Wang Y. -X., 2017, P 34 INT C MACH LEAR, P3589	46	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302033
C	Nadjahi, K; Durmus, A; Simsekli, U; Badeau, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Nadjahi, Kimia; Durmus, Alain; Simsekli, Umut; Badeau, Roland			Asymptotic Guarantees for Learning Generative Models with the Sliced-Wasserstein Distance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Minimum expected distance estimation (MEDE) algorithms have been widely used for probabilistic models with intractable likelihood functions and they have become increasingly popular due to their use in implicit generative modeling (e.g. Wasserstein generative adversarial networks, Wasserstein autoencoders). Emerging from computational optimal transport, the Sliced-Wasserstein (SW) distance has become a popular choice in MEDE thanks to its simplicity and computational benefits. While several studies have reported empirical success on generative modeling with SW, the theoretical properties of such estimators have not yet been established. In this study, we investigate the asymptotic properties of estimators that are obtained by minimizing SW. We first show that convergence in SW implies weak convergence of probability measures in general Wasserstein spaces. Then we show that estimators obtained by minimizing SW (and also an approximate version of SW) are asymptotically consistent. We finally prove a central limit theorem, which characterizes the asymptotic distribution of the estimators and establish a convergence rate of root n, where n denotes the number of observed data points. We illustrate the validity of our theory on both synthetic data and neural networks.	[Nadjahi, Kimia; Simsekli, Umut; Badeau, Roland] Telecom Paris, Inst Polytech Paris, LTCI, Paris, France; [Durmus, Alain] Univ Paris Saclay, CNRS, ENS Cachan, CMLA, Paris, France; [Simsekli, Umut] Univ Oxford, Dept Stat, Oxford, England	IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Cite; Universite Paris Saclay; University of Oxford	Nadjahi, K (corresponding author), Telecom Paris, Inst Polytech Paris, LTCI, Paris, France.	kimia.nadjahi@telecom-paris.fr; alain.durmus@cmla.ens-cachan.fr; umut.simsekli@telecom-paris.fr; roland.badeau@telecom-paris.fr			French National Research Agency (ANR) [ANR-16-CE23-0014]; industrial chair Machine Learning for Big Data from Telecom ParisTech; Polish National Science Center [NCN UMO-2018/31/B/ST1/00253]	French National Research Agency (ANR)(French National Research Agency (ANR)); industrial chair Machine Learning for Big Data from Telecom ParisTech; Polish National Science Center	The authors are grateful to Pierre Jacob for his valuable comments on an earlier version of this manuscript. This work is partly supported by the French National Research Agency (ANR) as a part of the FBIMATRIX project (ANR-16-CE23-0014) and by the industrial chair Machine Learning for Big Data from Telecom ParisTech. Alain Durmus acknowledges support from Polish National Science Center grant: NCN UMO-2018/31/B/ST1/00253.	Adler J., 2018, ADV NEURAL INFORM PR; Arjovsky M, 2017, PR MACH LEARN RES, V70; Basu A., 2011, CHAPMAN HALL CRC MON; Bernton E., 2019, INFORM INFERENCE J I; Billingsley P., 1999, CONVERGE PROBAB MEAS, Vsecond, DOI 10.1002/9780470316962; Bonneel N, 2015, J MATH IMAGING VIS, V51, P22, DOI 10.1007/s10851-014-0506-3; Bonnotte N., 2013, UNIDIMENSIONAL EVOLU; Bousquet O., 2017, TECH REP; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Dede S, 2009, STOCH PROC APPL, V119, P3494, DOI 10.1016/j.spa.2009.06.006; Del Barrio E, 1999, ANN PROBAB, V27, P1009, DOI 10.1214/aop/1022677394; Deshpande I., 2019, IEEE C COMP VIS PATT; Deshpande I, 2018, PROC CVPR IEEE, P3483, DOI 10.1109/CVPR.2018.00367; Ge nevay A., 2017, ARXIV170600292; Genevay A, 2017, ARXIV170601807; Kingma D.P, P 3 INT C LEARNING R; Kolouri S, 2019, INT C LEARN REPR; Kolouri S, 2019, ADV NEUR IN, V32; Leglaive S, 2017, INT CONF ACOUST SPEE, P576, DOI 10.1109/ICASSP.2017.7952221; Liu S, 2017, ADV NEUR IN, V30; Liutkus A, 2019, PR MACH LEARN RES, V97; Mandelbrot B.B., 2013, FRACTALS SCALING FIN, Ve; Nolan JP, 2013, COMPUTATION STAT, V28, P2067, DOI 10.1007/s00180-013-0396-7; Patrini G., 2018, ARXIV181001118; Paty FP, 2019, PR MACH LEARN RES, V97; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Rabin J, 2012, LECT NOTES COMPUT SC, V6667, P435, DOI 10.1007/978-3-642-24785-9_37; rachev s.t., 1998, PROB APPL S, VII, DOI 10.1007/b98894; SAMORODNITSKY G, 1994, STOCHASTIC MODELING; Simsekli U, 2015, IEEE SIGNAL PROC LET, V22, P2289, DOI 10.1109/LSP.2015.2477535; Tolstikhin Ilya, 2017, ARXIV171101558; Villani C, 2008, GRUNDLEHREN MATH WIS; WOLFOWITZ J, 1957, ANN MATH STAT, V28, P75, DOI 10.1214/aoms/1177707038; Wu JQ, 2019, PROC CVPR IEEE, P3708, DOI 10.1109/CVPR.2019.00383	34	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300023
C	Najafi, A; Maeda, S; Koyama, M; Miyato, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Najafi, Amir; Maeda, Shin-ichi; Koyama, Masanori; Miyato, Takeru			Robustness to Adversarial Perturbations in Learning from Incomplete Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					What is the role of unlabeled data in an inference problem, when the presumed underlying distribution is adversarially perturbed? To provide a concrete answer to this question, this paper unifies two major learning frameworks: Semi-Supervised Learning (SSL) and Distributionally Robust Learning (DRL). We develop a generalization theory for our framework based on a number of novel complexity measures, such as an adversarial extension of Rademacher complexity and its semi-supervised analogue. Moreover, our analysis is able to quantify the role of unlabeled data in the generalization under a more general condition compared to the existing theoretical works in SSL. Based on our framework, we also present a hybrid of DRL and EM algorithms that has a guaranteed convergence rate. When implemented with deep neural networks, our method shows a comparable performance to those of the state-of-the-art on a number of real-world benchmark datasets.	[Najafi, Amir] Sharif Univ Technol, Dept Comp Engn, Tehran, Iran; [Maeda, Shin-ichi; Koyama, Masanori; Miyato, Takeru] Preferred Networks Inc, Tokyo, Japan	Sharif University of Technology	Najafi, A (corresponding author), Sharif Univ Technol, Dept Comp Engn, Tehran, Iran.	najafy@ce.sharif.edu; ichi@preferred.jp; masomatics@preferred.jp; miyato@preferred.jp		Najafi, Amir/0000-0002-6680-0110				Amini MR, 2002, FR ART INT, V77, P390; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; [Anonymous], 2018, IEEE T PAMI; Balsubramani A., 2015, ADV NEURAL INFORM PR, P1351; Ben-Tal A, 2013, MANAGE SCI, V59, P341, DOI 10.1287/mnsc.1120.1641; BLANCHET J, 2017, ARXIV170208848; Carmon Yair, 2019, ADV NEURAL INFORM PR; Cranko Z., 2019, INT C MACH LEARN, P1406; Cullina D., 2018, ARXIV180601471; Dai Z., 2017, P ADV NEURAL INFORM, P6510; DUCHI J, 2016, ARXIV161003425; Dugoua JJ, 2009, J OBSTET GYNAECOL CA, V31, P542, DOI 10.1016/S1701-2163(16)34218-9; Esfahani P. M., 2017, MATH PROGRAM, V171, P1; Fiala M, 2002, INT C PATT RECOG, P27, DOI 10.1109/ICPR.2002.1047392; Goodfellow I. J., 2015, P ICLR; Grandvalet Y., 2005, CAP, P529; Hu WH, 2018, PR MACH LEARN RES, V80; Kim E, 2013, INT CONF ADV COMMUN, P1002; Krizhevsky A., 2009, CITESEER TECH REP; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Loog M, 2016, IEEE T PATTERN ANAL, V38, P462, DOI 10.1109/TPAMI.2015.2452921; Madry Aleksander, 2018, ICLR; Mohri M., 2018, FDN MACHINE LEARNING; Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Raghunathan A., 2019, ICML WORKSH ID UND D; Rigollet P, 2007, J MACH LEARN RES, V8, P1369; Schmidt Ludwig, 2018, ADV NEURAL INFORM PR, V31, P5014; Sinha A., 2018, ICLR; Staib M, 2017, NIPS MACHINE LEARN C; Stanforth Robert, 2019, ARXIV190513725; Szegedy C., 2014, 2014 INT C LEARNING, DOI DOI 10.48550/ARXIV.1312.6199; Wang YS, 2019, PR MACH LEARN RES, V97; Yan Y, 2016, AAAI CONF ARTIF INTE, P2244; Zhai R., 2019, ARXIV190600555; Zhu X., 2009, ADV NEURAL INFORM PR, P1513; Zhu Xiaojin., 2006, COMPUTER SCI U WISCO, V2, P4	38	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305052
C	Novotny, D; Graham, B; Reizenstein, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Novotny, David; Graham, Benjamin; Reizenstein, Jeremy			PerspectiveNet: A Scene-consistent Image Generator for New View Synthesis in Real Indoor Environments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Given a set of a reference RGBD views of an indoor environment, and a new viewpoint, our goal is to predict the view from that location. Prior work on new-view generation has predominantly focused on significantly constrained scenarios, typically involving artificially rendered views of isolated CAD models. Here we tackle a much more challenging version of the problem. We devise an approach that exploits known geometric properties of the scene (per-frame camera extrinsics and depth) in order to warp reference views into the new ones. The defects in the generated views are handled by a novel RGBD inpainting network, PerspectiveNet, that is fine-tuned for a given scene in order to obtain images that are geometrically consistent with all the views in the scene camera system. Experiments conducted on the ScanNet and SceneNet datasets reveal performance superior to strong baselines.	[Novotny, David; Graham, Benjamin; Reizenstein, Jeremy] Facebook AI Res, London, England	Facebook Inc	Novotny, D (corresponding author), Facebook AI Res, London, England.	dnovotny@fb.com; benjamingraham@fb.com; reizenstein@fb.com						Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036; Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972; Bloesch M, 2018, PROC CVPR IEEE, P2560, DOI 10.1109/CVPR.2018.00271; Chang A., 2015, TECHNICAL REPORT; Charbonnier P, 1997, IEEE T IMAGE PROCESS, V6, P298, DOI 10.1109/83.551699; Chen T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508378; Dai Angela, 2017, P COMP VIS PATT REC; Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170; Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595; FU YL, 2015, ADV NEURAL INFORM PR, P1099; Gatys LeonA., 2015, ARXIV, DOI 10.1167/16.12.326; Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Horry Y., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P225, DOI 10.1145/258734.258854; Isola Phillip, 2017, COMP VIS PATT REC CV; Ji Dinghuang, 2017, P IEEE C COMP VIS PA, P2155; Jin DK, 2017, COMPUT VIS PATT REC, P151, DOI 10.1016/B978-0-08-101291-8.00007-9; Kar A., 2017, P NIPS, P365; Kingma D.P, P 3 INT C LEARNING R; Kulkarni TD, 2015, ADV NEUR IN, V28; Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu Guilin, 2018, PARTIAL CONVOLUTION; Liu Guilin, 2018, P EUR C COMP VIS ECC, P85; Liu ZH, 2016, 9TH INTERNATIONAL CONFERENCE ON MICROWAVE AND MILLIMETER WAVE TECHNOLOGY PROCEEDINGS, VOL. 1, (ICMMT 2016), P286, DOI 10.1109/ICMMT.2016.7761750; McCormac J., 2017, SCENENET RGB D CAN 5; Meshry Moustafa, 2019, P IEEE C COMP VIS PA, P6878; Novotny David, 2018, IEEE T PATTERN ANAL, P6; Park Eunbyung, 2017, P IEEE C COMP VIS PA, P3500; Simonyan K., 2014, ICLR; Sitzmann Vincent, 2018, ABS181201024 CORR; Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20; Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596; Tulsiani Shubham, 2018, ECCV; Ulyanov Dmitry, 2017, DEEP IMAGE PRIOR; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhou T., 2017, P IEEE C COMP VIS PA, P1851; Zhu J.-Y., 2017, COMP VIS ICCV 2017 I; Zhu Z, 2014, I NAVIG SAT DIV INT, P2174	46	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307060
C	Oh, MH; Iyengar, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Oh, Min-hwan; Iyengar, Garud			Thompson Sampling for Multinomial Logit Contextual Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DYNAMIC ASSORTMENT; CHOICE MODEL; OPTIMIZATION	We consider a dynamic assortment selection problem where the goal is to offer a sequence of assortments that maximizes the expected cumulative revenue, or alternatively, minimize the expected regret. The feedback here is the item that the user picks from the assortment. The distinguishing feature in this work is that this feedback is given by a multinomial logit choice model. The utility of each item is a dynamic function of contextual information of both the item and the user. We refer to this problem as the multinomial logit contextual bandit. We propose two Thompson sampling algorithms for this multinomial logit contextual bandit. Our first algorithm maintains a posterior distribution of the unknown parameter and establishes (O) over tilde (d root T)(1) Bayesian regret over T rounds with d dimensional context vector. The second algorithm approximates the posterior by a Gaussian distribution and uses a new optimistic sampling procedure to address the issues that arise in worst-case regret analysis. This algorithm achieves (O) over tilde (d(3/2) root T) worst-case(frequentist) regret bound. The numerical experiments show that the practical performance of both methods is in line with the theoretical guarantees.	[Oh, Min-hwan; Iyengar, Garud] Columbia Univ, New York, NY 10027 USA	Columbia University	Oh, MH (corresponding author), Columbia Univ, New York, NY 10027 USA.	m.oh@columbia.edu; garud@ieor.columbia.edu						Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Abe N, 1999, MACHINE LEARNING, PROCEEDINGS, P3; Abeille M, 2017, ELECTRON J STAT, V11, P5165, DOI 10.1214/17-EJS1341SI; Agrawal S., 2017, ARXIV170603880; Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; Andrieu C, 2003, MACH LEARN, V50, P5, DOI 10.1023/A:1020281327116; [Anonymous], 2013, ASSORTMENT PLANNING; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Bernstein Fernando, 2018, MANAGEMENT SCI; Caro F, 2007, MANAGE SCI, V53, P276, DOI 10.1287/mnsc.1060.0613; Chen X., 2018, ARXIV181013069; Chen Xi, 2017, ARXIV170906109; Cheung W.C., 2017, THOMPSON SAMPLING ON; Cheung Wang Chi, 2017, ARXIV170400108; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Dani V, 2008, P C LEARN THEOR COLT, P355; Davis JM, 2014, OPER RES, V62, P250, DOI 10.1287/opre.2014.1256; Desir A., 2014, NEAR OPTIMAL ALGORIT; Filippi S., 2010, NIPS, P586; Goldenshluger A., 2013, STOCHASTIC SYSTEMS, V3, P230, DOI [10.1287/11-SSY032, DOI 10.1287/11-SSY032]; Kallus Nathan, 2016, ARXIV161005604; Kveton B, 2015, PR MACH LEARN RES, V37, P767; Lattimore Tor, 2019, BANDIT ALGORITHMS; Lehmann E.L., 2006, THEORY POINT ESTIMAT; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Li Shuai, 2019, INT C MACH LEARN, P3856; Luce R Duncan, 2012, INDIVIDUAL CHOICE BE; Mahapatra PP, 2017, IEEE I C COMP INT CO, P75; McFadden Daniel, 1978, TRANSPORTATION RES R; Plackett R. L., 1975, Applied Statistics, V24, P193, DOI 10.2307/2346567; Rusmevichientong P, 2010, OPER RES, V58, P1666, DOI 10.1287/opre.1100.0866; Rusmevichientong P, 2010, MATH OPER RES, V35, P395, DOI 10.1287/moor.1100.0446; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Saure D, 2013, M&SOM-MANUF SERV OP, V15, P387, DOI 10.1287/msom.2013.0429; Stegun I. A., 1965, HDB MATH FUNCTIONS F, V55; Strens, 2000, P 17 INT C MACH LEAR, P943; Talluri K, 2004, MANAGE SCI, V50, P15, DOI 10.1287/mnsc.1030.0147; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Wen Z, 2015, PR MACH LEARN RES, V37, P1113; Xu HH, 2017, 2017 IEEE 2ND INTERNATIONAL CONFERENCE ON BIG DATA ANALYSIS (ICBDA), P207	43	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303017
C	Pouget-Abadie, J; Aydin, K; Schudy, W; Brodersen, K; Mirrokni, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Pouget-Abadie, Jean; Aydin, Kevin; Schudy, Warren; Brodersen, Kay; Mirrokni, Vahab			Variance Reduction in Bipartite Experiments through Correlation Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CAUSAL INFERENCE; SCHEME	Causal inference in randomized experiments typically assumes that the units of randomization and the units of analysis are one and the same. In some applications, however, these two roles are played by distinct entities linked by a bipartite graph. The key challenge in such bipartite settings is how to avoid interference bias, which would typically arise if we simply randomized the treatment at the level of analysis units. One effective way of minimizing interference bias in standard experiments is through cluster randomization, but this design has not been studied in the bipartite setting where conventional clustering schemes can lead to poorly powered experiments. This paper introduces a novel clustering objective and a corresponding algorithm that partitions a bipartite graph so as to maximize the statistical power of a bipartite experiment on that graph. Whereas previous work relied on balanced partitioning, our formulation suggests the use of a correlation clustering objective. We use a publicly-available graph of Amazon user-item reviews to validate our solution and illustrate how it substantially increases the statistical power in bipartite experiments.	[Pouget-Abadie, Jean; Schudy, Warren; Mirrokni, Vahab] Google Res, New York, NY 10011 USA; [Aydin, Kevin] Google Res, Mountain View, CA 94043 USA; [Brodersen, Kay] Google, Zurich, Switzerland	Google Incorporated; Google Incorporated; Google Incorporated	Pouget-Abadie, J (corresponding author), Google Res, New York, NY 10011 USA.	jeanpa@google.com; kaydin@google.com; wschudy@google.com; kbrodersen@google.com; mirrokni@google.com						Ailon N, 2008, J ACM, V55, DOI 10.1145/1411509.1411513; [Anonymous], 2018, DETERMINANTS ONLINE; [Anonymous], 2013, INT C MACHINE LEARNI; Aydin K, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P387, DOI 10.1145/2835776.2835829; Bakshy E, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P283, DOI 10.1145/2566486.2567967; Bansal N, 2002, ANN IEEE SYMP FOUND, P238, DOI 10.1109/SFCS.2002.1181947; Basse G, 2018, J AM STAT ASSOC, V113, P41, DOI 10.1080/01621459.2017.1323641; Charikar M, 2003, ANN IEEE SYMP FOUND, P524, DOI 10.1109/SFCS.2003.1238225; Chawla S, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P19, DOI 10.1145/2940716.2940757; Delling Daniel, 2012, P 14 M ALG ENG EXP A, P30, DOI [10.1137/1.9781611972924.3, DOI 10.1137/1.9781611972924.3]; Demaine ED, 2006, THEOR COMPUT SCI, V361, P172, DOI 10.1016/j.tcs.2006.05.008; Eckles D, 2017, J CAUSAL INFERENCE, V5, DOI 10.1515/jci-2015-0021; Elsner Micha, 2009, P WORKSH INT LIN PRO, P19; Galagate D., 2016, THESIS U MARYLAND CO; Gilotte A, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P198, DOI 10.1145/3159652.3159687; Gui H, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P399, DOI 10.1145/2736277.2741081; He RN, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P507, DOI 10.1145/2872427.2883037; Hirano K., 2004, APPL BAYESIAN MODELI, P73, DOI DOI 10.1002/0470090456.CH7; Hong GL, 2005, EDUC EVAL POLICY AN, V27, P205, DOI 10.3102/01623737027003205; Hudgens MG, 2008, J AM STAT ASSOC, V103, P832, DOI 10.1198/016214508000000292; Imai K., 2004, J AM STAT ASSOC, V99, P467; Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751; Ioffe S., 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P246, DOI 10.1109/ICDM.2010.80; Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997; Karypis G, 1998, J PARALLEL DISTR COM, V48, P96, DOI 10.1006/jpdc.1997.1404; Kluve J, 2012, J R STAT SOC A STAT, V175, P587, DOI 10.1111/j.1467-985X.2011.01000.x; Manski C. F., 2013, ECONOMET J, V16, P1; McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755; Moodie EEM, 2012, STAT METHODS MED RES, V21, P149, DOI 10.1177/0962280209340213; Pokhiko Victoria, 2019, ARXIV190200482; Pouget-Abadie J, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2090, DOI 10.1145/3219819.3220067; Raudenbush SW, 1997, PSYCHOL METHODS, V2, P173, DOI 10.1037/1082-989X.2.2.173; Rubin DB, 2005, J AM STAT ASSOC, V100, P469; Saveski M, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1027, DOI 10.1145/3097983.3098192; Shakya HB, 2017, BMJ OPEN, V7, DOI 10.1136/bmjopen-2016-012996; Stanton I, 2012, KDD, P1222, DOI [10.1145/2339530.2339722, DOI 10.1145/2339530.2339722]; Tchetgen EJT, 2012, STAT METHODS MED RES, V21, P55, DOI 10.1177/0962280210386779; Tsourakakis CE, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P333, DOI 10.1145/2556195.2556213; Ugander J., 2013, PROC 6 ACM INT C WEB, P507; Ugander J, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P329; Zigler C. M., 2018, ARXIV180708660	41	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905001
C	Putzky, P; Welling, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Putzky, Patrick; Welling, Max			Invert to Learn to Invert	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SIGNAL RECOVERY	Iterative learning to infer approaches have become popular solvers for inverse problems. However, their memory requirements during training grow linearly with model depth, limiting in practice model expressiveness. In this work, we propose an iterative inverse model with constant memory that relies on invertible networks to avoid storing intermediate activations. As a result, the proposed approach allows us to train models with 400 layers on 3D volumes in an MRI image reconstruction task. In experiments on a public data set, we demonstrate that these deeper, and thus more expressive, networks perform state-of-the-art image reconstruction.	[Putzky, Patrick; Welling, Max] Univ Amsterdam, Amlab, Amsterdam, Netherlands; [Putzky, Patrick] Max Planck Inst Intelligent Syst MPI IS, Tubingen, Germany; [Welling, Max] Canadian Inst Adv Res CIFAR, Toronto, ON, Canada	University of Amsterdam; Canadian Institute for Advanced Research (CIFAR)	Putzky, P (corresponding author), Univ Amsterdam, Amlab, Amsterdam, Netherlands.; Putzky, P (corresponding author), Max Planck Inst Intelligent Syst MPI IS, Tubingen, Germany.	patrick.putzky@googlemail.com; welling.max@googlemail.com			Netherlands Organisation for Scientific Research (NWO); Netherlands Institute for Radio Astronomy (ASTRON) through the big bang, big data grant	Netherlands Organisation for Scientific Research (NWO)(Netherlands Organization for Scientific Research (NWO)); Netherlands Institute for Radio Astronomy (ASTRON) through the big bang, big data grant	Patrick Putzky is supported by the Netherlands Organisation for Scientific Research (NWO) and the Netherlands Institute for Radio Astronomy (ASTRON) through the big bang, big data grant.	Adler J, 2017, INVERSE PROBL, V33, DOI 10.1088/1361-6420/aa9581; [Anonymous], 2010, P INT C MACH LEARN; Ardizzone Lynton, 2019, INT C LEARN REPR; Behrmann J., 2018, INVERTIBLE RESIDUAL; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Chen YJ, 2015, PROC CVPR IEEE, P5261, DOI 10.1109/CVPR.2015.7299163; Dauphin YN, 2017, PR MACH LEARN RES, V70; Dinh L., 2017, 5 INT C LEARN REPR; Dinh L., 2014, ARXIV; Gomez Aidan N, 2017, ADV NEURAL INFORM PR, P2214; Gulehre C, 2014, NETWORKS SEQUENCE MO; Hammernik K, 2018, MAGN RESON MED, V79, P3055, DOI 10.1002/mrm.26977; Hoogeboom Emiel, 2019, EMERGING CONVOLUTION; Jacobsen Jorn-Henrik, 2018, INT C LEARN REPR; Kingma D. P., 2018, ADV NEURAL INFORM PR, P10215; Lonning Kai, 2019, MED IMAGE ANAL; Lonning Kai, 1 MED IM DEEP LEARN; Morningstar W. R., 2018, ARXIV180800011; Morningstar W. R, 2019, ARXIV190101359; Putzky P., 2017, ARXIV PREPRINT ARXIV; PUTZKY P, 2019, I RIM APPL FASTMRI C; Schlemper J, 2017, LECT NOTES COMPUT SC, V10265, P647, DOI 10.1007/978-3-319-59050-9_51; Schmidt U, 2016, IEEE T PATTERN ANAL, V38, P677, DOI 10.1109/TPAMI.2015.2441053; Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349; Tomczak Jakub M., 2016, IMPROVING VARIATIONA; WANG F, 2016, ADV NEURAL INFORM PR, P865; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Zbontar Jure, 2018, ARXIV181108839; Zheng Shuai, 2015, INT C COMP VIS ICCV, P16	31	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300041
C	Russo, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Russo, Daniel			Worst-Case Regret Bounds for Exploration via Randomized Value Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper studies a recent proposal to use randomized value functions to drive exploration in reinforcement learning. These randomized value functions are generated by injecting random noise into the training data, making the approach compatible with many popular methods for estimating parameterized value functions. By providing a worst-case regret bound for tabular finite-horizon Markov decision processes, we show that planning with respect to these randomized value functions can induce provably efficient exploration.	[Russo, Daniel] Columbia Univ, New York, NY 10027 USA	Columbia University	Russo, D (corresponding author), Columbia Univ, New York, NY 10027 USA.	djr2174@gsb.columbia.edu						Abeille M, 2017, ELECTRON J STAT, V11, P5165, DOI 10.1214/17-EJS1341SI; AGRAWAL S, 2017, ADV NEURAL INFORM PR, P1184; Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; Asmuth J., 2009, P 25 C UNC ART INT, P19; Azar MG, 2017, PR MACH LEARN RES, V70; Azizzadenesheli K, 2018, 2018 INFORMATION THEORY AND APPLICATIONS WORKSHOP (ITA); Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Boucheron S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Dann C, 2017, ADV NEUR IN, V30; Fortunato M., 2018, INT C LEARN REPR, P1; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Kakade Sham M., 2003, THESIS; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; Klimov O, 2019, INT C LEARNING REPRE; Lagoudakis M. G., 2003, J MACHINE LEARNING, P1107, DOI DOI 10.1162/JMLR.2003.4.6.1107; Lu XY, 2017, ADV NEUR IN, V30; Osband I, 2019, J MACH LEARN RES, V20; Osband I, 2018, ADV NEUR IN, V31; Osband I, 2016, PR MACH LEARN RES, V48; Strehl A.L., 2006, ICML, P881, DOI [10.1145/1143844.1143955, DOI 10.1145/1143844.1143955]; Strehl AL, 2009, J MACH LEARN RES, V10, P2413; Strens, 2000, P 17 INT C MACH LEAR, P943; Szepesvdri C., 2018, BANDIT ALGORITHMS; Touati Ahmed, P 35 C UNC ART INT; Tziortziotis Nikolaos, 2019, ARXIV190403535; Van Roy, 2013, ADV NEURAL INFORM PR, P3003; Weissman Tsachy, 2003, INEQUALITIES 11 DEVI	30	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906013
C	Saha, A; Gopalan, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Saha, Aadirupa; Gopalan, Aditya			Combinatorial Bandits with Relative Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider combinatorial online learning with subset choices when only relative feedback information from subsets is available, instead of bandit or semi-bandit feedback which is absolute. Specifically, we study two regret minimisation problems over subsets of a finite ground set [n], with subset-wise relative preference information feedback according to the Multinomial logit choice model. In the first setting, the learner can play subsets of size bounded by a maximum size and receives top-m rank-ordered feedback, while in the second setting the learner can play subsets of a fixed size k with a full subset ranking observed as feedback. For both settings, we devise instance-dependent and order-optimal regret algorithms with regret O(n/m ln T) and O(n/k ln T), respectively. We derive fundamental limits on the regret performance of online learning with subset-wise preferences, proving the tightness of our regret guarantees. Our results also show the value of eliciting more general top-m rank-ordered feedback over single winner feedback (m = 1). Our theoretical results are corroborated with empirical evaluations.	[Saha, Aadirupa; Gopalan, Aditya] Indian Inst Sci, Bangalore, Karnataka, India	Indian Institute of Science (IISC) - Bangalore	Saha, A (corresponding author), Indian Inst Sci, Bangalore, Karnataka, India.	aadirupa@iisc.ac.in; aditya@iisc.ac.in			Qualcomm Innovation Fellowship 2019; Dept. of Telecommunications, Government of India; ACM-India/IARCS Travel Grants	Qualcomm Innovation Fellowship 2019; Dept. of Telecommunications, Government of India; ACM-India/IARCS Travel Grants	The authors are grateful to the anonymous reviewers for valuable feedback. This work is supported by a Qualcomm Innovation Fellowship 2019, and the Indigenous 5G Test Bed project grant from the Dept. of Telecommunications, Government of India. Aadirupa Saha thanks Arun Rajkumar for the valuable discussions, and the Tata Trusts and ACM-India/IARCS Travel Grants for travel support.	Agrawal S., 2012, C LEARN THEOR, P39; Agrawal S, 2019, OPER RES, V67, P1453, DOI 10.1287/opre.2018.1832; Agrawal S, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P599; Agrawal Shipra, 2017, MACHINE LEARNING RES, V65, P1; Ailon N, 2014, PR MACH LEARN RES, V32, P856; ALWIN DF, 1985, PUBLIC OPIN QUART, V49, P535, DOI 10.1086/268949; [Anonymous], 2016, ADV NEURAL INFORM PR, P649; Azari H, 2012, NIPS 12, P126; Bartok Gabor, 2011, P INT C COMP LEARN T, P133; Ben-Akiva M., 1994, MARKET LETT, V5, P335; Benson AR, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P963; Brost B., 2016, ABS160806253 CORR; Busa-Fekete R, 2014, LECT NOTES ARTIF INT, V8776, P18, DOI 10.1007/978-3-319-11662-4_3; Busa-Fekete Robert, 2014, P 31 INT C MACH LEAR, V32; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Chen S, 2013, IMBALANCED LEARNING: FOUNDATIONS, ALGORITHMS, AND APPLICATIONS, P151; Chen XF, 2013, KEY ENG MATER, V538, P193, DOI 10.4028/www.scientific.net/KEM.538.193; Chen X, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2504; Chen YX, 2015, PR MACH LEARN RES, V37, P371; Combes R., 2015, P 28 INT C NEUR INF, P2116; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Garivier A, 2019, MATH OPER RES, V44, P377, DOI 10.1287/moor.2017.0928; Graepel Thore, 2006, GAME DEV MAGAZ, V25, P34; Hajek S., 2014, ADV NEURAL INFORM PR, P1475; HENSHER DA, 1994, TRANSPORTATION, V21, P107, DOI 10.1007/BF01098788; Hofmann Katja, 2013, SIGIR Forum, V47; Jang M, 2017, P 31 IT C NEUR INF P, P1685; Katariya S, 2016, PR MACH LEARN RES, V48; Kaufmann E, 2016, J MACH LEARN RES, V17; Khetan A, 2016, J MACH LEARN RES, V17; Komiyama Junpei, 2015, C LEARN THEOR, V40, P1141; Kveton B, 2015, JMLR WORKSH CONF PRO, V38, P535; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Popescu PG, 2016, ELECTRON J DIFFER EQ; Radlinski F., 2008, P 17 ACM C INF KNOWL, P43, DOI DOI 10.1145/1458082.1458092; Ren W., 2018, ARXIV180602970; Saha A, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P805; Saha Aadirupa, 2019, ALGORITHMIC LEARNING, P700; Soufiani HA, 2014, PR MACH LEARN RES, V32; Sui YN, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Szorenyi B., 2015, ADV NEURAL INFORM PR, V28, P604; Urvoy T., 2013, INT C MACH LEARN, V28, P91; Yue Y., 2009, ICML, P1201; Yue Y., 2011, P 28 INT C MACH LEAR, P241; Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028; Zoghi M, 2014, PR MACH LEARN RES, V32, P10; Zoghi Masrour, 2013, ARXIV13123393	47	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301003
C	Schein, A; Linderman, SW; Zhou, MY; Blei, DM; Wallach, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Schein, Aaron; Linderman, Scott W.; Zhou, Mingyuan; Blei, David M.; Wallach, Hanna			Poisson-Randomized Gamma Dynamical Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper presents the Poisson-randomized gamma dynamical system (PRGDS), a model for sequentially observed count tensors that encodes a strong inductive bias toward sparsity and burstiness. The PRGDS is based on a new motif in Bayesian latent variable modeling, an alternating chain of discrete Poisson and continuous gamma latent states that is analytically convenient and computationally tractable. This motif yields closed-form complete conditionals for all variables by way of the Bessel distribution and a novel discrete distribution that we call the shifted confluent hypergeometric distribution. We draw connections to closely related models and compare the PRGDS to these models in studies of real-world count data sets of text, international events, and neural spike trains. We find that a sparse variant of the PRGDS, which allows the continuous gamma latent states to take values of exactly zero, often obtains better predictive performance than other models and is uniquely capable of inferring latent structures that are highly localized in time.	[Schein, Aaron] Columbia Univ, Data Sci Inst, New York, NY 10027 USA; [Linderman, Scott W.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Zhou, Mingyuan] Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA; [Blei, David M.] Columbia Univ, Dept Stat, New York, NY 10027 USA; [Wallach, Hanna] Microsoft Res, New York, NY USA	Columbia University; Stanford University; University of Texas System; University of Texas Austin; Columbia University; Microsoft	Schein, A (corresponding author), Columbia Univ, Data Sci Inst, New York, NY 10027 USA.		Zhou, Mingyuan/AAE-8717-2021	Linderman, Scott/0000-0002-3878-9073	Simons Collaboration on the Global Brain [SCGB 418011]; NSF [IIS-1812699, CCF-1740833]; ONR [N00014-17-1-2131, N00014-15-1-2209]; NIH [1U01MH115727-01]; DARPA [SD2 FA8750-18-C-0130]; IBM; 2Sigma; Amazon; NVIDIA; Simons Foundation	Simons Collaboration on the Global Brain; NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); IBM(International Business Machines (IBM)); 2Sigma; Amazon; NVIDIA; Simons Foundation	We thank Saurabh Vyas, Alex Williams, and Krishna Shenoy for kindly providing us with the macaque monkey motor cortex data set and their corresponding preprocessing code. SWL was supported by the Simons Collaboration on the Global Brain (SCGB 418011). MZ was supported by NSF IIS-1812699. DMB was supported by ONR N00014-17-1-2131, ONR N00014-15-1-2209, NIH 1U01MH115727-01, NSF CCF-1740833, DARPA SD2 FA8750-18-C-0130, IBM, 2Sigma, Amazon, NVIDIA, and the Simons Foundation.	Abramowitz M., 1965, HDB MATH FUNCTIONS F; Acharya A., 2015, AISTATS; [Anonymous], 2008, THESIS U CAMBRIDGE; [Anonymous], 2014, ARXIV14098276; Bhattacharya A, 2012, J AM STAT ASSOC, V107, P362, DOI 10.1080/01621459.2011.646934; Blundell C, 2012, NIPS; Boschee Elizabeth, 2015, HARVARD DATAVERSE, V12, DOI 10.7910/DVN/28075; Brandt PT, 2012, POLIT ANAL, V20, P292, DOI 10.1093/pan/mps001; Canny J., 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P122, DOI 10.1145/1008992.1009016; Cemgil AT, 2007, LECT NOTES COMPUT SC, V4666, P697; Cemgil Ali Taylan, 2009, Comput Intell Neurosci, P785152, DOI 10.1155/2009/785152; Charlin L., 2015, RECSYS ACM, P155; Devroye L, 2002, STAT PROBABIL LETT, V57, P249, DOI 10.1016/S0167-7152(02)00055-X; Dunson DB, 2005, BIOSTATISTICS, V6, P11, DOI 10.1093/biostatistics/kxh025; Erikson RS, 2014, POLIT ANAL, V22, P457, DOI 10.1093/pan/mpt051; Fevotte C, 2013, INT CONF ACOUST SPEE, P3158, DOI 10.1109/ICASSP.2013.6638240; Gelman A, 2014, STAT COMPUT, V24, P997, DOI 10.1007/s11222-013-9416-2; Ghahramani Z, 1999, ADV NEUR IN, V11, P431; Gong CY, 2017, ADV NEUR IN, V30; Gopalan PK, 2013, P NATL ACAD SCI USA, V110, P14534, DOI 10.1073/pnas.1221839110; Green DP, 2001, INT ORGAN, V55, P441, DOI 10.1162/00208180151140630; Guo DD, 2018, ADV NEUR IN, V31; Harshman R.A., 1970, MULTIMODAL FACTOR AN; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; Hoff PD, 2004, POLIT ANAL, V12, P160, DOI 10.1093/pan/mph012; Hoff PD, 2016, BAYESIAN ANAL, V11, P627, DOI 10.1214/14-BA934; Jerfel G, 2017, PR MACH LEARN RES, V54, P738; Kalman R.E., 1961, J BASIC ENG-T ASME, V83, P95, DOI [10.1115/1.3658902, DOI 10.1115/1.3658902]; King G, 2001, INT ORGAN, V55, P497, DOI 10.1162/00208180151140667; Kleinberg J, 2003, DATA MIN KNOWL DISC, V7, P373, DOI 10.1023/A:1024940629314; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Kunihama T, 2013, J AM STAT ASSOC, V108, P1324, DOI 10.1080/01621459.2013.823866; Leetaru K, 2013, ISA ANN CONVENTION, V2, P1; Linderman SW, 2014, PR MACH LEARN RES, V32, P1413; Macke Jakob H, 2012, ADV NEURAL INFORM PR, V24, P1350; Makarov RN, 2010, MONTE CARLO METHODS, V16, P283, DOI 10.1515/MCMA.2010.010; NELDER JA, 1972, J R STAT SOC SER A-G, V135, P370, DOI 10.2307/2344614; NeurIPS corpus, UCI MACHINE LEARNING; Paninski L, 2010, J COMPUT NEUROSCI, V29, P107, DOI 10.1007/s10827-009-0179-x; Poast P, 2010, POLIT ANAL, V18, P403, DOI 10.1093/pan/mpq024; Ranganath R, 2015, JMLR WORKSH CONF PRO, V38, P762; Schein A., 2016, ADV NEURAL INFORM PR, P5006; Schein A, 2016, PR MACH LEARN RES, V48; Schein A, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1045, DOI 10.1145/2783258.2783414; Schein Aaron, 2019, THESIS; Schrodt Philip A, 1995, FOREIGN POLICY ANAL; SIMMA A., 2010, PROC UAI 10, P546; Smith AC, 2003, NEURAL COMPUT, V15, P965, DOI 10.1162/089976603765202622; Steel Robert GD, 1953, RELATION POISSON MUL; Stewart Brandon, 2014, TECHNICAL REPORT; Titsias Michalis, 2008, ADV NEURAL INFORM PR; Vyas S, 2018, NEURON, V97, P1177, DOI 10.1016/j.neuron.2018.01.040; Wallach H., 2009, P 26 ANN INT C MACHI, V382, P1105, DOI DOI 10.1145/1553374.1553515; Williams AH, 2018, NEURON, V98, P1099, DOI 10.1016/j.neuron.2018.05.015; Yang SK, 2018, PR MACH LEARN RES, V80; Yuan L, 2000, ANN I STAT MATH, V52, P438, DOI 10.1023/A:1004152916478; Zhou M, 2012, ADV NEURAL INFORM PR; Zhou MY, 2015, ADV NEUR IN, V28; Zhou MY, 2016, J MACH LEARN RES, V17, P1; Zhou Mingyuan, 2012, AISTATS, P1462	62	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300071
C	Schlegel, M; Chung, W; Graves, D; Qian, J; White, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Schlegel, Matthew; Chung, Wesley; Graves, Daniel; Qian, Jian; White, Martha			Importance Resampling for Off-policy Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Importance sampling (IS) is a common reweighting strategy for off-policy prediction in reinforcement learning. While it is consistent and unbiased, it can result in high variance updates to the weights for the value function. In this work, we explore a resampling strategy as an alternative to reweighting. We propose Importance Resampling (IR) for off-policy prediction, which resamples experience from a replay buffer and applies standard on-policy updates. The approach avoids using importance sampling ratios in the update, instead correcting the distribution before the update. We characterize the bias and consistency of IR, particularly compared to Weighted IS (WIS). We demonstrate in several microworlds that IR has improved sample efficiency and lower variance updates, as compared to IS and several variance-reduced IS strategies, including variants of WIS and V-trace which clips IS ratios. We also provide a demonstration showing IR improves over IS for learning a value function from images in a racing car simulator.	[Schlegel, Matthew; Chung, Wesley; Qian, Jian; White, Martha] Univ Alberta, Edmonton, AB T6G 2M7, Canada; [Graves, Daniel] Huawei, Shenzhen, Peoples R China	University of Alberta; Huawei Technologies	Schlegel, M (corresponding author), Univ Alberta, Edmonton, AB T6G 2M7, Canada.	mkschleg@ualberta.ca; wchung@ualberta.ca; daniel.graves@huawei.com; jq1@ulberta.ca; whitem@ulberta.ca	White, Martha/AAF-7066-2020	White, Martha/0000-0002-5356-2950	Huawei; University of Alberta; Alberta Machine Intelligence Institute; NSERC; IVADO	Huawei(Huawei Technologies); University of Alberta(University of Alberta); Alberta Machine Intelligence Institute; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); IVADO	We would like to thank Huawei for their support, and especially for allowing a portion of this work to be completed during Matthews internship in the summer of 2018. We also would like to acknowledge University of Alberta, Alberta Machine Intelligence Institute, IVADO, and NSERC for their continued funding and support, as well as Compute Canada (www.computecanada.ca) for the computing resources used for this work.	Andradottir Sigrun, 1995, OPERATIONS RES; Arulampalam M S, 2002, IEEE T SIGNAL PROCES; Bojarski Mariusz, 2016, arXiv; Espeholt Lasse, 2018, ARXIV180201561; Gordon N J, 1993, IET; Hallak Assaf, 2017, ARXIV170207121; Jaderberg Max, 2017, INT C REPR LEARN; KAHN H, 1953, J OPER RES SOC AM, V1, P263, DOI 10.1287/opre.1.5.263; Kong Augustine, 1994, J AM STAT ASS; Levin D.A., 2017, MARKOV CHAINS MIXING, V107, DOI DOI 10.1090/MBK/107; Lin Long-Ji, 1992, MACHINE LEARNING; Liu Q, 2018, ADV NEURAL INFORM PR, V31; Long-Ji Lin, 1993, THESIS; Lopez Victoria, 2013, INFORM SCI; Mahmood A R, 2015, C UNC ART INT; Mahmood Ashique Rupam, 2017, ARXIV150901240V2; Martino L, 2017, INT CONF DIGIT SIG; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Modayil J, 2014, ADAPT BEHAV, V22, P146, DOI 10.1177/1059712313511648; Peng J., 1993, ADAPT BEHAV, V1, P437; Precup D., 2001, P 18 INT C MACH LEAR, P417; Precup D., 2000, P 17 INT C MACH LEAR; Rubin D.B., 1988, BAYESIAN STAT; Rubinstein RY, 2016, SIMULATION MONTE CAR, DOI DOI 10.1002/9781118631980; Schaul T., 2013, P 23 INT JOINT C ART, P1656; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Schaul Tom, 2015, ARXIV151105952CS; Silver David, 2017, AAAI C ART INT; Skare Oivind, 2003, SCANDINAVIAN J STAT; Smith AFM, 1992, BAYESIAN STAT TEARS; SUTTON R S, 2011, 10 INT C AUTONOMOUS, P761, DOI DOI 10.5555/2031678.2031726; Sutton R. S., 2005, ADV NEURAL INFORM PR; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Thomas Philip, 2016, AAAI C ART INT; Thomas Philip S, 2017, AAAI C ART INT	39	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301075
C	Sessa, PG; Bogunovic, I; Kamgarpour, M; Krause, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sessa, Pier Giuseppe; Bogunovic, Ilija; Kamgarpour, Maryam; Krause, Andreas			No-Regret Learning in Unknown Games with Correlated Payoffs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider the problem of learning to play a repeated multi-agent game with an unknown reward function. Single player online learning algorithms attain strong regret bounds when provided with full information feedback, which unfortunately is unavailable in many real-world scenarios. Bandit feedback alone, i.e., observing outcomes only for the selected action, yields substantially worse performance. In this paper, we consider a natural model where, besides a noisy measurement of the obtained reward, the player can also observe the opponents' actions. This feedback model, together with a regularity assumption on the reward function, allows us to exploit the correlations among different game outcomes by means of Gaussian processes (GPs). We propose a novel confidence-bound based bandit algorithm GP-MW, which utilizes the GP model for the reward function and runs a multiplicative weight (MW) method. We obtain novel kernel-dependent regret bounds that are comparable to the known bounds in the full information setting, while substantially improving upon the existing bandit results. We experimentally demonstrate the effectiveness of GP-MW in random matrix games, as well as real-world problems of traffic routing and movie recommendation. In our experiments, GP-MW consistently outperforms several baselines, while its performance is often comparable to methods that have access to full information feedback.	[Sessa, Pier Giuseppe; Bogunovic, Ilija; Kamgarpour, Maryam; Krause, Andreas] Swiss Fed Inst Technol, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Sessa, PG (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	sessap@ethz.ch; ilijab@ethz.ch; maryamk@ethz.ch; krausea@ethz.ch		Krause, Andreas/0000-0001-7260-9673	Swiss National Science Foundation [SNSF 200021_172781]; European Union's Horizon 2020 ERC [815943]	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission); European Union's Horizon 2020 ERC	This work was gratefully supported by Swiss National Science Foundation, under the grant SNSF 200021_172781, and by the European Union's Horizon 2020 ERC grant 815943.	Abbasi-Yadkori Y., 2012, THESIS U ALBERTA; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Bogunovic Ilija, 2018, NEURAL INFORM PROCES; Bogunovic Ilija, 2016, NEURAL INFORM PROCES; Bubeck S, 2017, ACM S THEORY COMPUT, P72, DOI 10.1145/3055399.3055403; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chapman AC, 2013, SIAM J CONTROL OPTIM, V51, P3154, DOI 10.1137/120893501; Chowdhury Sayak Ray, 2017, INT C MACH LEARN ICM; Fainmesser IP, 2012, AM ECON J-MICROECON, V4, P32, DOI 10.1257/mic.4.1.32; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Hazan Elad, 2017, INT C MACH LEARN ICM; Jackson M. O., 2015, HDB GAME THEORY EC A, V4, P95; Krause Andreas, 2011, NEURAL INFORM PROCES; Leblanc L. J., 1975, Transportation Science, V9, P183, DOI 10.1287/trsc.9.3.183; Maillard OA, 2010, LECT NOTES ARTIF INT, V6322, P305, DOI 10.1007/978-3-642-15883-4_20; Monderer D, 1996, GAME ECON BEHAV, V14, P124, DOI 10.1006/game.1996.0044; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Skyrms B, 2009, UNDERST COMPLEX SYST, P231, DOI 10.1007/978-3-642-01284-6_11; Srinivas N., 2010, INT C MACH LEARN ICM; Syrgkanis Vasilis, 2015, NEURAL INFORM PROCES; Zinkevich M., 2003, INT C MACH LEARN	24	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905029
C	Shen, YY; Sanghavi, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shen, Yanyao; Sanghavi, Sujay			Iterative Least Trimmed Squares for Mixed Linear Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Given a linear regression setting, Iterative Least Trimmed Squares (ILTS) involves alternating between (a) selecting the subset of samples with lowest current loss, and (b) re-fitting the linear model only on that subset. Both steps are very fast and simple. In this paper we analyze ILTS in the setting of mixed linear regression with corruptions (MLR-C). We first establish deterministic conditions (on the features etc.) under which the ILTS iterate converges linearly to the closest mixture component. We also evaluate it for the widely studied setting of isotropic Gaussian features, and establish that we match or better existing results in terms of sample complexity. We then provide a global algorithm that uses ILTS as a subroutine, to fully solve mixed linear regressions with corruptions. Finally, we provide an ODE analysis for a gradient-descent variant of ILTS that has optimal time complexity. Our results provide initial theoretical evidence that iteratively fitting to the best subset of samples - a potentially widely applicable idea - can provably provide state-of-the-art performance in bad training data settings.	[Shen, Yanyao; Sanghavi, Sujay] Univ Texas Austin, ECE Dept, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Shen, YY (corresponding author), Univ Texas Austin, ECE Dept, Austin, TX 78712 USA.	shenyanyao@utexas.edu; sanghavi@mail.utexas.edu			NSF [1302435, 1564000]	NSF(National Science Foundation (NSF))	We would like to acknowledge NSF grants 1302435 and 1564000 for supporting this research.	Balakrishnan S, 2017, ANN STAT, V45, P77, DOI 10.1214/16-AOS1435; Boucheron Stephane, 2012, ELECTR COMMUN, V17; Chen W, 2013, UKSIM INT CONF COMP, P774, DOI 10.1109/UKSim.2013.86; DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001; Diakonikolas I, 2018, ARXIV180302815; Diakonikolas I, 2017, PR MACH LEARN RES, V70; Karmalkar S., 2018, 2 S SIMPL ALG SOSA 2; Klivans A., 2018, C LEARNING THEORY, P1420; Klusowski JM, 2019, IEEE T INFORM THEORY, V65, P3515, DOI 10.1109/TIT.2019.2891628; Kwon Jeongyeol, 2018, ARXIV181005752; Liu Liu, 2018, ARXIV180511643; Moitra A, 2010, ANN IEEE SYMP FOUND, P93, DOI 10.1109/FOCS.2010.15; Mount DM, 2014, ALGORITHMICA, V69, P148, DOI 10.1007/s00453-012-9721-8; Prasad A., 2018, ARXIV180206485; ROUSSEEUW PJ, 1984, J AM STAT ASSOC, V79, P871, DOI 10.2307/2288718; Sedghi H, 2016, JMLR WORKSH CONF PRO, V51, P1223; Shen YY, 2019, PR MACH LEARN RES, V97; Suggala AS, 2018, ADV NEUR IN, V31; Vershynin R., 2010, ARXIV10113027; Xu, 2017, C LEARN THEOR, P1849; Yang E, 2018, ELECTRON J STAT, V12, P3519, DOI 10.1214/18-EJS1470; Yi XY, 2014, PR MACH LEARN RES, V32, P613; Yi Xinyang, 2016, ARXIV160805749; Yin Dong, 2018, IEEE T INFORM THEORY; Zhong K., 2016, ADV NEURAL INFORM PR, P2190	29	3	3	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306012
C	Shi, WJ; Song, SJ; Wu, H; Hsu, YC; Wu, C; Huang, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shi, Wenjie; Song, Shiji; Wu, Hui; Hsu, Ya-Chu; Wu, Cheng; Huang, Gao			Regularized Anderson Acceleration for Off-Policy Deep Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Model-free deep reinforcement learning (RL) algorithms have been widely used for a range of complex control tasks. However, slow convergence and sample inefficiency remain challenging problems in RL, especially when handling continuous and high-dimensional state spaces. To tackle this problem, we propose a general acceleration method for model-free, off-policy deep RL algorithms by drawing the idea underlying regularized Anderson acceleration (RAA), which is an effective approach to accelerating the solving of fixed point problems with perturbations. Specifically, we first explain how policy iteration can be applied directly with Anderson acceleration. Then we extend RAA to the case of deep RL by introducing a regularization term to control the impact of perturbation induced by function approximation errors. We further propose two strategies, i.e., progressive update and adaptive restart, to enhance the performance. The effectiveness of our method is evaluated on a variety of benchmark tasks, including Atari 2600 and MuJoCo. Experimental results show that our approach substantially improves both the learning speed and final performance of state-of-the-art deep RL algorithms.	[Huang, Gao] Tsinghua Univ, Dept Automat, Beijing, Peoples R China; Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing, Peoples R China	Tsinghua University	Huang, G (corresponding author), Tsinghua Univ, Dept Automat, Beijing, Peoples R China.	shiwj16@mails.tsinghua.edu.cn; shijis@tsinghua.edu.cn; wuhui14@mails.tsinghua.edu.cn; xuyz17@mails.tsinghua.edu.cn; wuc@tsinghua.edu.cn; gaohuang@tsinghua.edu.cn			Beijing Academy of Artificial Intelligence (BAAI) [BAAI2019QN0106]; Tencent AI Lab Rhino-Bird Focused Research Program [JR201914]; National Science Foundation of China (NSFC) [41427806]	Beijing Academy of Artificial Intelligence (BAAI); Tencent AI Lab Rhino-Bird Focused Research Program; National Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	Gao Huang is supported in part by Beijing Academy of Artificial Intelligence (BAAI) under grant BAAI2019QN0106 and Tencent AI Lab Rhino-Bird Focused Research Program under grant JR201914. This research is supported by the National Science Foundation of China (NSFC) under grant 41427806.	[Anonymous], 2016, P INT C LEARN REPR; [Anonymous], 2017, P INT C LEARN REPR; Anschel O, 2017, PR MACH LEARN RES, V70; Bertsekas Dimitri P, 1996, NEURODYNAMIC PROGRAM, V5; Buckman Jacob, 2018, ARXIV180701675; Chebotar Yevgen, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3381, DOI 10.1109/ICRA.2017.7989384; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Fujimoto S, 2018, PR MACH LEARN RES, V80; Geist M, 2018, 2018 4 EUR WORKSH RE; Granas A, 2013, FIXED POINT THEORY; Greensmith E, 2004, J MACH LEARN RES, V5, P1471; Haarnoja T, 2018, PR MACH LEARN RES, V80; Henderson N. C., 2019, J COMPUTATIONAL GRAP, P1; Lillicrap T. P., 2016, P 33 INT C MACH LEAR; Lin L.-J., 1993, CARNEGIE MELLON U PI; Mandel T, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1077; Mirowski P, 2017, P INT C LEARN REPR; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nachum O., 2018, NEURIPS; Scieur D., 2016, ADV NEURAL INFORM PR, P712; Shi W., 2019, P INT JOINT C ART IN, P3425; Shi W., 2018, IEEE T NEURAL NETWOR, P3534; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Toth A, 2015, SIAM J NUMER ANAL, V53, P805, DOI 10.1137/130919398; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; Wang ZY, 2016, PR MACH LEARN RES, V48; Wang Ziyu, 2017, P INT C LEARN REPR; Williams Grady, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1714, DOI 10.1109/ICRA.2017.7989202; Xie G Z, 2018, ARXIV180506753	32	3	3	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901082
C	Shih, A; Van den Broeck, G; Beame, P; Amarilli, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shih, Andy; Van den Broeck, Guy; Beame, Paul; Amarilli, Antoine			Smoothing Structured Decomposable Circuits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INFERENCE	We study the task of smoothing a circuit, i.e., ensuring that all children of a E-gate mention the same variables. Circuits serve as the building blocks of state-of-the-art inference algorithms on discrete probabilistic graphical models and probabilistic programs. They are also important for discrete density estimation algorithms. Many of these tasks require the input circuit to be smooth. However, smoothing has not been studied in its own right yet, and only a trivial quadratic algorithm is known. This paper studies efficient smoothing for structured decomposable circuits. We propose a near-linear time algorithm for this task and explore lower bounds for smoothing decomposable circuits, using existing results on range-sum queries. Further, for the important case of All-Marginals, we show a more efficient linear-time algorithm. We validate experimentally the performance of our methods.	[Shih, Andy; Van den Broeck, Guy] Univ Calif Los Angeles, Los Angeles, CA 90024 USA; [Beame, Paul] Univ Washington, Seattle, WA 98195 USA; [Amarilli, Antoine] Telecom Paris, LTCI, IP Paris, Paris, France	University of California System; University of California Los Angeles; University of Washington; University of Washington Seattle; IMT - Institut Mines-Telecom; Institut Polytechnique de Paris	Shih, A (corresponding author), Univ Calif Los Angeles, Los Angeles, CA 90024 USA.	andyshih@cs.ucla.edu; guyvdb@cs.ucla.edu; beame@cs.washington.edu; antoine.amarilli@telecom-paris.fr		Van den Broeck, Guy/0000-0003-3434-2503	NSF [IIS-1657613, IIS-1633857, CCF-1837129]; DARPA XAI grant [N66001-17-2-4032]; NEC Research	NSF(National Science Foundation (NSF)); DARPA XAI grant; NEC Research	This work is partially supported by NSF grants #IIS-1657613, #IIS-1633857, #CCF-1837129, DARPA XAI grant #N66001-17-2-4032, NEC Research, and gifts from Intel and Facebook Research. We thank Louis Jachiet for the helpful discussion of Theorem 4.7.	Bekker J., 2015, NIPS; Bellodi E, 2013, INTELL DATA ANAL, V17, P343, DOI 10.3233/IDA-130582; Bova Simone, 2014, ABS14111995 CORR; BRYANT RE, 1986, IEEE T COMPUT, V35, P677, DOI 10.1109/TC.1986.1676819; Chavira M, 2008, ARTIF INTELL, V172, P772, DOI 10.1016/j.artint.2007.11.002; Chazelle Bernard, 1989, S COMP GEOM; Choi A., 2017, ICML; Darwiche A., 2001, J APPL NONCLASSICAL, V11, P11; Darwiche Adnan, 2011, IJCAI; Darwiche Adnan, 2017, DAGSTUHL REPORTS, V7, P62; Fierens D, 2015, THEOR PRACT LOG PROG, V15, P358, DOI 10.1017/S1471068414000076; Forouzan Sholeh, 2015, UC IRVINE; Friedman Tal, 2018, NEURIPS; Friesen Abram L., 2016, ICML; Garg Akshat, DIFFERENCE ARRAY; Gens R., 2013, ICML; Kimmig Angelika, 2016, INT J APPL LOGIC NOV; Kisa D., 2014, KR; Liang Y., 2017, UAI; Liang Y, 2017, IEEE INT C NETW SENS, P174, DOI 10.1109/ICNSC.2017.8000087; Mei Jun, 2018, AAAI; Oztok Umut, 2015, IJCAI; Peharz R, 2017, IEEE T PATTERN ANAL, V39, P2030, DOI 10.1109/TPAMI.2016.2618381; Poon H., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P689, DOI 10.1109/ICCVW.2011.6130310; Rooshenas A, 2014, ICML; Sang T., 2005, AAAI; Shen Yanyao, 2016, NIPS; TARJAN RE, 1975, J ACM, V22, P215, DOI 10.1145/321879.321884; Van den Broeck G., 2014, KR; Vergari A., 2015, ECML PKDD; Xu J., 2018, ICML; Yao Andrew Chi-Chih, 1982, STOC; [No title captured]	33	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903009
C	Shin, R; Allamanis, M; Brockschmidt, M; Polozov, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shin, Richard; Allamanis, Miltiadis; Brockschmidt, Marc; Polozov, Oleksandr			Program Synthesis and Semantic Parsing with Learned Code Idioms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Program synthesis of general-purpose source code from natural language specifications is challenging due to the need to reason about high-level patterns in the target program and low-level implementation details at the same time. In this work, we present PATOIS, a system that allows a neural program synthesizer to explicitly interleave high-level and low-level reasoning at every generation step. It accomplishes this by automatically mining common code idioms from a given corpus, incorporating them into the underlying language for neural synthesis, and training a tree-based neural synthesizer to use these idioms during code generation. We evaluate PATOIS on two complex semantic parsing datasets and show that using learned code idioms improves the synthesizer's accuracy.	[Shin, Richard] Univ Calif Berkeley, Berkeley, CA 94701 USA; [Allamanis, Miltiadis; Brockschmidt, Marc; Polozov, Oleksandr] Microsoft Res, Redmond, WA USA	University of California System; University of California Berkeley; Microsoft	Shin, R (corresponding author), Univ Calif Berkeley, Berkeley, CA 94701 USA.	ricshin@berkeley.edu; miallama@microsoft.com; mabrocks@microsoft.com; polozov@microsoft.com						Aggarwal C. C., 2014, FREQUENTPATTERN MINI; Allamanis M., 2018, IEEE TRANSACTIONSON; Allamanis M, 2014, 22ND ACM SIGSOFT INTERNATIONAL SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING (FSE 2014), P472, DOI 10.1145/2635868.2635901; [Anonymous], 2010, BAYESIAN NONPARAMETR; Balog Matej, 2017, P INT C LEARN REPR O; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Bosnjak M, 2017, PR MACH LEARN RES, V70; Brockschmidt M., 2019, P 7 INT C LEARN REPR; Cohn T, 2010, J MACH LEARN RES, V11, P3053; Devlin J., 2017, P 34 INT C MACH LEAR; Devlin Jacob, 2017, ADV NEURAL INFORM PR, P2080; Dong L, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P33; Dong Li, 2018, P 56 ANN M ASS COMP; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Ellis Kevin, 2018, ADV NEURAL INFORM PR, P7805; Gulwani S, 2017, FOUND TRENDS PROGRAM, V4, P1, DOI 10.1561/2500000010; Gulwani S, 2011, POPL 11: PROCEEDINGS OF THE 38TH ANNUAL ACM SIGPLAN-SIGACT SYMPOSIUM ON PRINCIPLES OF PROGRAMMING LANGUAGES, P317, DOI 10.1145/1926385.1926423; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Iyer S., 2019, EMNLP; Kalyan A., 2018, ICLR; Kingma D.P, P 3 INT C LEARNING R; Li Yujia, 2016, P INT C LEARN REPR I, P2; Liang P., 2010, HUMAN LANGUAGE TECHN, P573; Liang P, 2016, COMMUN ACM, V59, P68, DOI 10.1145/2866568; Ling W, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P599; Murali V, 2018, P 6 INT C LEARN REPR; Neelakantan A., 2017, P 5 INT C LEARN REPR; Polozov O, 2015, ACM SIGPLAN NOTICES, V50, P107, DOI [10.1145/2858965.2814310, 10.1145/2814270.2814310]; Post M., 2009, P ACL IJCNLP 2009 C, P45; See A, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1073, DOI 10.18653/v1/P17-1099; SETHURAMAN J, 1994, STAT SINICA, V4, P639; Shaw Peter, 2018, P 2018 C N AM CHAPT, P464, DOI DOI 10.18653/V1/N18-2074; Shin R., 2019, ENCODING DATABASE SC; Sun Z., 2019, AAAI; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Yin P., 2018, P 56 ANN M ASS COMP; Yin PC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P440, DOI 10.18653/v1/P17-1041; Yin PC, 2018, IEEE WORK CONF MIN S, P476, DOI 10.1145/3196398.3196408; Yu Tao, 2018, P 2018 C EMP METH NA, P3911, DOI DOI 10.18653/V1/D18-1425; Zeiler Matthew D, 2012, ARXIV12125701	41	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902045
C	Simchowitz, M; Jamieson, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Simchowitz, Max; Jamieson, Kevin			Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper establishes that optimistic algorithms attain gap-dependent and non-asymptotic logarithmic regret for episodic MDPs. In contrast to prior work, our bounds do not suffer a dependence on diameter-like quantities or ergodicity, and smoothly interpolate between the gap dependent logarithmic-regret, and the O(root HSAT)-minimax rate. The key technique in our analysis is a novel "clipped" regret decomposition which applies to a broad family of recent optimistic algorithms for episodic MDPs.	[Simchowitz, Max] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Jamieson, Kevin] Univ Washington, Seattle, WA 98195 USA	University of California System; University of California Berkeley; University of Washington; University of Washington Seattle	Simchowitz, M (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	msimchow@berkeley.edu; jamieson@cs.washington.edu						Azar MG, 2017, PR MACH LEARN RES, V70; Bartlett RA, 2009, 2009 ICSE WORKSHOP ON SOFTWARE ENGINEERING FOR COMPUTATIONAL SCIENCE AND ENGINEERING, P35, DOI 10.1109/SECSE.2009.5069160; Burnetas AN, 1997, MATH OPER RES, V22, P222, DOI 10.1287/moor.22.1.222; Dann C., 2017, ADV NEURAL INFORM PR, P5713; Dann Christopher, 2018, ARXIV181103056; Garivier A., 2018, MATH OPERATIONS RES; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Jin C., 2018, ADV NEURAL INFORM PR; Ok J., 2018, ADV NEURAL INFORM PR, P8888; Osband I., 2016, ARXIV160802732; TEWARI A, 2008, ADV NEURAL INFORM PR, P1505; Tewari Ambuj, 2007, REINFORCEMENT LEARNI; Zanette Andrea, 2019, ARXIV190100210	14	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301018
C	Song, Z; Woodruff, DP; Zhong, PL		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Song, Zhao; Woodruff, David P.; Zhong, Peilin			Towards a Zero-One Law for Column Subset Selection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ERROR MATRIX APPROXIMATION	There are a number of approximation algorithms for NP-hard versions of low rank approximation, such as finding a rank-k matrix B minimizing the sum of absolute values of differences to a given n-by-n matrix A, min(rank-k) (B) parallel to A - B parallel to(1), or more generally finding a rank-k matrix B which minimizes the sum of p-th powers of absolute values of differences, min(rank-k) (B) parallel to A - B parallel to(p)(p). Many of these algorithms are linear time columns subset selection algorithms, returning a subset of poly(k log n) columns whose cost is no more than a poly(k) factor larger than the cost of the best rank-k matrix. The above error measures are special cases of the following general entrywise low rank approximation problem: given an arbitrary function g : R -> R->= 0, find a rank-k matrix B which minimizes parallel to A - B parallel to(g) = Sigma(i,j) g(A(i,j) - B-i,B-j). A natural question is which functions g admit efficient approximation algorithms? Indeed, this is a central question of recent work studying generalized low rank models. In this work we give approximation algorithms for every function g which is approximately monotone and satisfies an approximate triangle inequality, and we show both of these conditions are necessary. Further, our algorithm is efficient if the function g admits an efficient approximate regression algorithm. Our approximation algorithms handle functions which are not even scale-invariant, such as the Huber loss function, which we show have very different structural properties than l(p)-norms, e.g., one can show the lack of scale-invariance causes any column subset selection algorithm to provably require a root log n factor larger number of columns than l(p)-norms, nevertheless we design the first efficient column subset selection algorithms for such error measures.	[Song, Zhao] Univ Washington, Seattle, WA 98195 USA; [Woodruff, David P.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Zhong, Peilin] Columbia Univ, New York, NY 10027 USA	University of Washington; University of Washington Seattle; Carnegie Mellon University; Columbia University	Song, Z (corresponding author), Univ Washington, Seattle, WA 98195 USA.	magic.linuxkde@gmail.com; dwoodruf@cs.cmu.edu; pz2225@columbia.edu			Office of Naval Research (ONR) [N00014-18-1-2562]; NSF [CCF-1703925, CCF-1421161, CCF-1714818, CCF-1617955, CCF-1740833]; Simons Foundation [491119]; Google Research Award; Google Ph.D. fellowship	Office of Naval Research (ONR)(Office of Naval Research); NSF(National Science Foundation (NSF)); Simons Foundation; Google Research Award(Google Incorporated); Google Ph.D. fellowship(Google Incorporated)	David P. Woodruff was supported in part by Office of Naval Research (ONR) grant N00014-18-1-2562. Part of this work was done while he was visiting the Simons Institute for the Theory of Computing. Peilin Zhong was supported in part by NSF grants (CCF-1703925, CCF-1421161, CCF-1714818, CCF-1617955 and CCF-1740833), Simons Foundation (#491119 to Alexandr Andoni), Google Research Award and a Google Ph.D. fellowship. Part of this work was done while Zhao Song and Peilin Zhong were interns at IBM Research -Almaden and while Zhao Song was visiting the Simons Institute for the Theory of Computing.	Alon Noga, 2009, ALGEBRAIC METHODS CO; Ban Frank, 2019, SODA; Bartlett P., 2018, INT C MACH LEARN, P520; Berman P, 2002, SIAM PROC S, P514; Bourgain J, 2015, ACM S THEORY COMPUT, P499, DOI 10.1145/2746539.2746541; Boutsidis C, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P353, DOI 10.1145/2591796.2591819; Boutsidis C, 2011, ANN IEEE SYMP FOUND, P305, DOI 10.1109/FOCS.2011.21; Boutsidis C, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P968; Bringmann Karl, 2017, ADV NEURAL INFORM PR, P6651; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chierichetti Flavio, 2017, ICML; Clarkson K. L., 2019, P INT C MACH LEARN, P1262; Clarkson KL, 2015, ANN IEEE SYMP FOUND, P310, DOI 10.1109/FOCS.2015.27; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Clarkson Kenneth L, 2015, P 26 ANN ACM SIAM S, P921; Cohen Michael B, 2016, P 27 ANN ACM SIAM S, P278; COPPERSMITH D, 1990, J SYMB COMPUT, V9, P251, DOI 10.1016/S0747-7171(08)80013-2; Deshpande Amit, 2009, ABS09121403 CORR; Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X; Drineas P, 2006, LECT NOTES COMPUT SC, V4168, P304; Drineas P, 2006, LECT NOTES COMPUT SC, V4110, P316; Farahat AK, 2013, IEEE DATA MINING, P171, DOI 10.1109/ICDM.2013.155; Goel Surbhi, 2018, ICML; Hampel FR., 2011, WILEY SERIES PROBABI; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; KOENKER R, 1978, ECONOMETRICA, V46, P33, DOI 10.2307/1913643; Legall Francois, 2014, P 39 INT S SYMBOLIC, P296, DOI [DOI 10.1145/2608628.2608664, 10.1145/2608628.2608664]; Meng XR, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P91; Nelson J, 2013, ANN IEEE SYMP FOUND, P117, DOI 10.1109/FOCS.2013.21; Razenshteyn I., 2016, P 48 ANN S THEOR COM; Sarlos T, 2006, ANN IEEE SYMP FOUND, P143; Song Z., 2019, SODA 19 P 30 ANN ACM; Song Z., 2017, P 49 ANN S THEOR COM; Song Zhao, 2019, ARXIV191001788; STRASSEN V, 1969, NUMER MATH, V13, P354, DOI 10.1007/BF02165411; Udell M, 2016, FOUND TRENDS MACH LE, V9, P2, DOI 10.1561/2200000055; Wang YN, 2015, JMLR WORKSH CONF PRO, V38, P1033; Williams VV, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P887; Woodruff D., 2013, C LEARN THEOR, P546; Yang Jiyan, 2014, SIAM J SCI COMPUTING, V36; Yelp, 2014, YELP DAT; Zhang ZY, 1997, IMAGE VISION COMPUT, V15, P59, DOI 10.1016/S0262-8856(96)01112-2; Zhong K, 2017, PR MACH LEARN RES, V70	44	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306016
C	Srinivasa, RS; Lee, K; Junge, M; Romberg, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Srinivasa, Rakshith S.; Lee, Kiryung; Junge, Marius; Romberg, Justin			Decentralized sketching of low-rank matrices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				OPTIMIZATION	We address a low-rank matrix recovery problem where each column of a rank-r matrix X is an element of R-d1xd2 is compressed beyond the point of individual recovery to R-L with L << d(1). Leveraging the joint structure among the columns, we propose a method to recover the matrix to within an epsilon relative error in the Frobenius norm from a total of O(r(d(1) +d(2)) log(6) (d(1) +d(2))/epsilon(2)) observations. This guarantee holds uniformly for all incoherent matrices of rank r. In our method, we propose to use a novel matrix norm called the mixed-norm along with the maximum l(2)-norm of the columns to design a new convex relaxation for low-rank recovery that is tailored to our observation model. We also show that the proposed mixed-norm, the standard nuclear norm, and the max-norm are particular instances of convex regularization of low-rankness via tensor norms. Finally, we provide a scalable ADMM algorithm for the mixed-norm-based method and demonstrate its empirical performance via large-scale simulations.	[Srinivasa, Rakshith S.; Romberg, Justin] Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30318 USA; [Lee, Kiryung] Ohio State Univ, Dept Elect & Comp Engn, Columbus, OH 43210 USA; [Junge, Marius] Univ Illinois Urbana Champagne, Dept Math, Urbana, IL 61801 USA	University System of Georgia; Georgia Institute of Technology; University System of Ohio; Ohio State University; University of Illinois System; University of Illinois Urbana-Champaign	Srinivasa, RS (corresponding author), Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30318 USA.	rsrinivasa6@gatech.edu; lee.8763@osu.edu; mjunge@illinois.edu; jrom@ece.gatech.edu			NSF [CCF-1718771]; NSF DMS [18-00872]; C-BRIC, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program - DARPA	NSF(National Science Foundation (NSF)); NSF DMS(National Science Foundation (NSF)); C-BRIC, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program - DARPA	This work was supported in part NSF CCF-1718771, NSF DMS 18-00872 and in part by C-BRIC, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.	Azizyan M., 2015, ARXIV150600898; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Cai TT, 2016, ELECTRON J STAT, V10, P1493, DOI 10.1214/16-EJS1147; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Davenport MA, 2016, IEEE J-STSP, V10, P608, DOI 10.1109/JSTSP.2016.2539100; Gross D, 2011, IEEE T INFORM THEORY, V57, P1548, DOI 10.1109/TIT.2011.2104999; Jameson G. J. O., 1987, SUMMING NUCL NORMS B, V8; Krahmer F, 2014, COMMUN PUR APPL MATH, V67, P1877, DOI 10.1002/cpa.21504; Ledoux M., 2013, PROBABILITY BANACH S, P86; Lee K., 2019, P 13INT C SAMPL THEO; Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850; Pisier G., 1999, VOLUME CONVEX BODIES, V94; Pourkamali-Anaraki F, 2014, PR MACH LEARN RES, V32, P1341; Qi HC, 2012, IEEE IMAGE PROC, P937, DOI 10.1109/ICIP.2012.6467015; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Ryan R.A, 2013, INTRO TENSOR PRODUCT; Spencer R., IET SEM DAT AN DER I; Srebro N., 2005, P ADV NEURAL INFORM; Sturm JF, 1999, OPTIM METHOD SOFTW, V11-2, P625, DOI 10.1080/10556789908805766; Toh K, 2010, PACIFIC J OPTIMIZATI; Tropp J.A., 2019, ARXIV190208651	23	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901070
C	Stephenson, C; Feather, J; Padhy, S; Elibol, O; Tang, HL; McDermott, J; Chung, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Stephenson, Cory; Feather, Jenelle; Padhy, Suchismita; Elibol, Oguz; Tang, Hanlin; McDermott, Josh; Chung, SueYeon			Untangling in Invariant Speech Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Encouraged by the success of deep neural networks on a variety of visual tasks, much theoretical and experimental work has been aimed at understanding and interpreting how vision networks operate. Meanwhile, deep neural networks have also achieved impressive performance in audio processing applications, both as sub-components of larger systems and as complete end-to-end systems by themselves. Despite their empirical successes, comparatively little is understood about how these audio models accomplish these tasks. In this work, we employ a recently developed statistical mechanical theory that connects geometric properties of network representations and the separability of classes to probe how information is untangled within neural networks trained to recognize speech. We observe that speaker-specific nuisance variations are discarded by the network's hierarchy, whereas task-relevant properties such as words and phonemes are untangled in later layers. Higher level concepts such as parts-of-speech and context dependence also emerge in the later layers of the network. Finally, we find that the deep representations carry out significant temporal untangling by efficiently extracting task-relevant features at each time step of the computation. Taken together, these findings shed light on how deep auditory models process time dependent input signals to achieve invariant speech recognition, and show how different concepts emerge through the layers of the network.	[Stephenson, Cory; Padhy, Suchismita; Elibol, Oguz; Tang, Hanlin] Intel AI Lab, Santa Clara, CA 95054 USA; [Feather, Jenelle; McDermott, Josh; Chung, SueYeon] MIT, Cambridge, MA 02139 USA; [McDermott, Josh] Ctr Brains Minds & Machines, Cambridge, MA USA; [Chung, SueYeon] Columbia Univ, New York, NY 10027 USA	Massachusetts Institute of Technology (MIT); Columbia University	Stephenson, C (corresponding author), Intel AI Lab, Santa Clara, CA 95054 USA.	cory.stephenson@intel.com; jfeather@mit.edu; suchismita.padhy@intel.com; oguz.h.elibol@intel.com; hanlin.tang@intel.com; jhm@mit.edu; sueyeon@mit.edu	Chung, SueYeon/ABI-7940-2020		NSF [BCS1634050]; DOE CSGF Fellowship; Intel Corporate Research Grant; NSF NeuroNex [DBI-1707398]; Gatsby Charitable Foundation	NSF(National Science Foundation (NSF)); DOE CSGF Fellowship(United States Department of Energy (DOE)); Intel Corporate Research Grant; NSF NeuroNex; Gatsby Charitable Foundation	We thank Yonatan Belinkov, Haim Sompolinsky, Larry Abbott, Tyler Lee, Anthony Ndirango, Gokce Keskin, and Ting Gong for helpful discussions. This work was funded by NSF grant BCS1634050 to J.H.M. and a DOE CSGF Fellowship to J.J.E S.C acknowledges support by Intel Corporate Research Grant, NSF NeuroNex Award DBI-1707398, and The Gatsby Charitable Foundation.	Advani M, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03014; Amodei D, 2016, PR MACH LEARN RES, V48; Barrett DGT, 2019, CURR OPIN NEUROBIOL, V55, P55, DOI 10.1016/j.conb.2019.01.007; Belinkov Y, 2017, ADV NEUR IN, V30; Belinkov Yonatan, 2018, THESIS; Chung S, 2018, PHYS REV X, V8, DOI 10.1103/PhysRevX.8.031003; Chung S, 2016, PHYS REV E, V93, DOI 10.1103/PhysRevE.93.060301; Cohen Uri, 2020, NATURE COMMUNICATION, DOI [10.1101/644658., DOI 10.1101/644658]; COVER TM, 1965, IEEE TRANS ELECTRON, VEC14, P326, DOI 10.1109/PGEC.1965.264137; DiCarlo JJ, 2007, TRENDS COGN SCI, V11, P333, DOI 10.1016/j.tics.2007.06.010; Elloumi Zied, 2018, ARXIV180808573; Gao P, 2017, BIORXIV; Gao PR, 2015, CURR OPIN NEUROBIOL, V32, P148, DOI 10.1016/j.conb.2015.04.003; Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261; Giryes R, 2016, IEEE T SIGNAL PROCES, V64, P3444, DOI 10.1109/TSP.2016.2546221; Goris Robbe LT, 2019, NAT NEUROSCI, P1; Graves A., 2006, P 23 INT C MACH LEAR, P369; Hnaff Olivier J, 2016, GEODESICS LEARNED RE; Kell AJE, 2018, NEURON, V98, P630, DOI 10.1016/j.neuron.2018.03.044; Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915; Kohn Arne, 2016, P 10 INT C LANG RES; Krug Andreas, 2018, NEURON ACTIVATION PR; Li C, 2017, ARXIV PREPRINT ARXIV; Li Yujia, 2016, P INT C LEARN REPR I, P2; Litwin-Kumar A, 2017, NEURON, V93, P1153, DOI 10.1016/j.neuron.2017.01.030; Nagamine T, 2016, INTERSPEECH, P803, DOI 10.21437/Interspeech.2016-1406; Nagamine T, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1912; Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964; PAUL DB, 1992, SPEECH AND NATURAL LANGUAGE, P357; Poole B, 2016, ADV NEUR IN, V29; Raghu Maithra, 2017, ADV NEURAL INFORM PR, P6076; Senior A, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P604, DOI 10.1109/ASRU.2015.7404851; Sharpee TO, 2011, CURR OPIN NEUROBIOL, V21, P761, DOI 10.1016/j.conb.2011.05.027; Wang S, 2017, INTERSPEECH, P1497, DOI 10.21437/Interspeech.2017-1125; Wang YH, 2017, INTERSPEECH, P3822, DOI 10.21437/Interspeech.2017-877	37	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906009
C	Sun, H; Li, ZZ; Liu, XT; Lin, DH; Zhou, BL		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sun, Hao; Li, Zhizhong; Liu, Xiaotong; Lin, Dahua; Zhou, Bolei			Policy Continuation with Hindsight Inverse Dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DENSITY	Solving goal-oriented tasks is an important but challenging problem in reinforcement learning (RL). For such tasks, the rewards are often sparse, making it difficult to learn a policy effectively. To tackle this difficulty, we propose a new approach called Policy Continuation with Hindsight Inverse Dynamics (PCHID). This approach learns from Hindsight Inverse Dynamics based on Hindsight Experience Replay. Enabling the learning process in a self-imitated manner and thus can be trained with supervised learning. This work also extends it to multi-step settings with Policy Continuation. The proposed method is general, which can work in isolation or be combined with other on-policy and off-policy algorithms. On two multi-goal tasks GridWorld and FetchReach, PCHID significantly improves the sample efficiency as well as the final performance(1).	[Sun, Hao; Li, Zhizhong; Lin, Dahua; Zhou, Bolei] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Liu, Xiaotong] Peking Univ, Beijing, Peoples R China	Chinese University of Hong Kong; Peking University	Sun, H (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.				SenseTime Group (CUHK) [7051699]; CUHK direct fund [4055098]	SenseTime Group (CUHK); CUHK direct fund	We acknowledge discussions with Yuhang Song and Chuheng Zhang. This work was partially supported by SenseTime Group (CUHK Agreement No.7051699) and CUHK direct fund (No.4055098).	Alili A, 2005, STOCH MODELS, V21, P967, DOI 10.1080/15326340500294702; [Anonymous], 2018, ARXIV181012894; [Anonymous], ARXIV170506366; [Anonymous], 2018, ARXIV180209464; BLAKE IF, 1973, IEEE T INFORM THEORY, V19, P295, DOI 10.1109/TIT.1973.1055016; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Burda Y., 2018, CORR; Duan Y, 2016, INT C MACH LEARN, P1329; Florensa C., 2017, ARXIV170705300; Hecht-Nielsen R., 1987, IEEE First International Conference on Neural Networks, P11; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; JORDAN MI, 1992, COGNITIVE SCI, V16, P307, DOI 10.1207/s15516709cog1603_1; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kakade S, 2002, ADV NEUR IN, V14, P1531; KURKOVA V, 1992, NEURAL NETWORKS, V5, P501, DOI 10.1016/0893-6080(92)90012-8; Levy A., 2019, P INT C LEARN REPR; Lillicrap TP, 2016, 4 INT C LEARN REPR; Mirowski Piotr, 2017, INT C LEARN REPR; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nair A, 2018, IEEE INT CONF ROBOT, P6292; Oh J., 2018, ARXIV180605635; Pathak Deepak, 2017, IEEE C COMP VIS PATT; Rauber Paulo, 2017, ARXIV171106006; RICCIARDI LM, 1988, J APPL PROBAB, V25, P43, DOI 10.2307/3214232; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Schulman J., 2017, ABS170706347 CORR; Shelhamer Evan, 2016, ARXIV161207307; Silver D., 2014, ICML; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Tamar Aviv, 2016, ADV NEURAL INFORM PR, P2154, DOI DOI 10.5555/3171837.3171991; THOMAS MU, 1975, J APPL PROBAB, V12, P600, DOI 10.2307/3212877; Wu Yuxin, 2017, INT C LEARN REPR	35	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901085
C	Tao, C; Blanco, SA; Peng, J; Zhou, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tao, Chao; Blanco, Saul A.; Peng, Jian; Zhou, Yuan			Thresholding Bandit with Optimal Aggregate Regret	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALLOCATION	We consider the thresholding bandit problem, whose goal is to find arms of mean rewards above a given threshold theta, with a fixed budget of T trials. We introduce LSA, a new, simple and anytime algorithm that aims to minimize the aggregate regret (or the expected number of mis-classified arms). We prove that our algorithm is instance-wise asymptotically optimal. We also provide comprehensive empirical results to demonstrate the algorithm's superior performance over existing algorithms under a variety of different scenarios.	[Tao, Chao; Blanco, Saul A.; Zhou, Yuan] Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA; [Peng, Jian] Univ Illinois, Dept Comp Sci, Champaign, IL USA; [Zhou, Yuan] Univ Illinois, Dept ISE, Champaign, IL USA	Indiana University System; Indiana University Bloomington; University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign	Tao, C (corresponding author), Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA.							AGRAWAL R, 1995, ADV APPL PROBAB, V27, P1054, DOI 10.2307/1427934; Audibert J.-Y., 2009, C LEARN THEOR COLT; Audibert Jean-Yves, 2010, P 23 ANN C LEARN THE, p2010a; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Carpentier A., 2016, C LEARN THEOR, P590; Chen X, 2015, J MACH LEARN RES, V16, P1; Garivier A., 2016, C LEARN THEOR, P998; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Jamieson K., 2014, C LEARN THEOR, P423; Janson S, 2018, STAT PROBABIL LETT, V135, P1, DOI 10.1016/j.spl.2017.11.017; Karnin Zohar, 2013, P 30 INT C INT C MAC, P1238; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lattimore Tor, 2018, PREPRINT; Lazaric A., 2012, ADV NEURAL INFORM PR, V25, P3212; Locatelli Andrea, INT C MACH LEARN ICM, P1690; Matouek Jir, 2006, UNDERSTANDINGAND USI; Steinwart I, 2005, J MACH LEARN RES, V6, P211; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Wang RG, 2017, DESTECH TRANS SOC, P534	20	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903031
C	Thomas, C; Kovashka, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Thomas, Christopher; Kovashka, Adriana			Predicting the Politics of an Image Using Webly Supervised Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The news media shape public opinion, and often, the visual bias they contain is evident for human observers. This bias can be inferred from how different media sources portray different subjects or topics. In this paper, we model visual political bias in contemporary media sources at scale, using webly supervised data. We collect a dataset of over one million unique images and associated news articles from left- and right-leaning news sources, and develop a method to predict the image's political leaning. This problem is particularly challenging because of the enormous intra-class visual and semantic diversity of our data. We propose a two-stage method to tackle this problem. In the first stage, the model is forced to learn relevant visual concepts that, when joined with document embeddings computed from articles paired with the images, enable the model to predict bias. In the second stage, we remove the requirement of the text domain and train a visual classifier from the features of the former model. We show this two-stage approach facilitates learning and outperforms several strong baselines. We also present extensive qualitative results demonstrating the nuances of the data.	[Thomas, Christopher; Kovashka, Adriana] Univ Pittsburgh, Dept Comp Sci, Pittsburgh, PA 15213 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Thomas, C (corresponding author), Univ Pittsburgh, Dept Comp Sci, Pittsburgh, PA 15213 USA.	chris@cs.pitt.edu; kovashka@cs.pitt.edu			National Science Foundation [1566270]; NVIDIA hardware grant	National Science Foundation(National Science Foundation (NSF)); NVIDIA hardware grant	This material is based upon work supported by the National Science Foundation under Grant Number 1566270. It was also supported by an NVIDIA hardware grant. We thank the reviewers for their constructive feedback.	Anderson P, 2018, P IEEE C COMP VIS PA; Angermeyer M. C., 2001, INT J LAW PSYCHIAT; [Anonymous], NY TIMES; Baumer Eric, 2015, P 2015 C N AM CHAPT, P1472, DOI DOI 10.3115/V1/N15-1171; Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097; Chen T.-H., 2017, P IEEE INT C COMP VI; Chen XL, 2015, IEEE I CONF COMP VIS, P1431, DOI 10.1109/ICCV.2015.168; Cinbis RG, 2017, IEEE T PATTERN ANAL, V39, P189, DOI 10.1109/TPAMI.2016.2535231; Cohen R., 2013, 7 INT ASS ADV ART IN; Colleoni E, 2014, J COMMUN, V64, P317, DOI 10.1111/jcom.12084; Conover M. D., 2011, Proceedings of the 2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and IEEE Third International Conference on Social Computing (PASSAT/SocialCom 2011), P192, DOI 10.1109/PASSAT/SocialCom.2011.34; Cucchiara R., 2018, BRIT MACH VIS C BMVC; Dai B., 2017, P IEEE INT C COMP VI; Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597; Donahue Jeffrey, 2015, P IEEE C COMP VIS PA; Edsall, 2012, STUDIES CONSERVATIVE; ehrek Radim., 2010, P LREC 2010 WORKSH N, P45; Eisenschtat Aviv, 2017, P IEEE C COMP VIS PA; Elliott D, 2017, P 8 INT JOINT C NAT, V1, P130; Fei-Fei L, 2005, PROC CVPR IEEE, P524; Garbe W., SYMSPELL; Gilens M, 1996, PUBLIC OPIN QUART, V60, P515, DOI 10.1086/297771; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Gomez Lluis, 2017, P IEEE C COMP VIS PA, P2; Hihn J, 2016, AEROSP CONF PROC; Hoffman J, 2016, PROC CVPR IEEE, P826, DOI 10.1109/CVPR.2016.96; Hussain Z., 2017, P IEEE C COMP VIS PA; Jiang L, 2015, P AAAI C ART INT AAA, V2, P6; Joo J., 2014, P IEEE C COMP VIS PA; Joo J., 2015, P IEEE INT C COMP VI; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; King DE, 2009, J MACH LEARN RES, V10, P1755; Kingma D.P., 2015, INT C LEARN REPR, P1; Kovashka, 2018, P BRIT MACH VIS C BM; Lambert J., 2018, P IEEE C COMP VIS PA; Lee YJ, 2013, IEEE I CONF COMP VIS, P1857, DOI 10.1109/ICCV.2013.233; Li HZ, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P291, DOI 10.1145/3206025.3206039; Li Y, 2017, INT J COMPUT VISION, V121, P344, DOI 10.1007/s11263-016-0945-y; Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107; Liu JJ, 2017, CHIN CONTR CONF, P2432, DOI 10.23919/ChiCC.2017.8027723; Lu J, 2018, PR MACH LEARN RES, V80; Luo, 2017, INT C SOC INF; M. Pedersoli, 2017, P IEEE INT C COMP VI; Malkov Y. A., 2016, IEEE T PATTERN ANAL; Morin F., 2005, PROC INT WORKSHOP AR, P246; Motiian S, 2016, PROC CVPR IEEE, P1496, DOI 10.1109/CVPR.2016.166; Munoz CL, 2017, J POLITICAL MARKETIN, V16, P290, DOI 10.1080/15377857.2017.1334254; Oquab M, 2015, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2015.7298668; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Peck T., BIG POLITICAL DATA; Peng YL, 2018, J COMMUN, V68, P920, DOI 10.1093/joc/jqy041; Pennacchiotti M., 2011, 5 INT ASS ADV ART IN; Pentina A, 2015, PROC CVPR IEEE, P5492, DOI 10.1109/CVPR.2015.7299188; Peters ME, 2013, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'13 COMPANION), P89; Philo, 2013, J SOCIAL POLITICAL P, V1, P321, DOI [DOI 10.5964/JSPP.V1I1.96, 10.5964/jspp.v1i1.96]; Philo G, 2008, JOURNALISM STUD, V9, P535, DOI 10.1080/14616700802114217; Quoc Le, 2014, P 31 INT C MACHINE L, V32, P1188; Recasens M., 2013, P 51 ANN M ASS COMP, V1, P1650; Richard A., 2017, P IEEE C COMP VIS PA, P754; Schill Dan., 2012, REV COMMUNICATION, V12, P118, DOI [DOI 10.1080/15358593.2011.653504, 10.1080/15358593.2011.653504]; Schreiber D, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0052970; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sharma A, 2012, WOODHEAD PUBL FOOD S, P73; Sharmanska V, 2013, IEEE I CONF COMP VIS, P825, DOI 10.1109/ICCV.2013.107; Sicre R, 2017, PROC CVPR IEEE, P3116, DOI 10.1109/CVPR.2017.332; Sivic J, 2005, IEEE I CONF COMP VIS, P370; Vapnik V, 2015, J MACH LEARN RES, V16, P2023; Venugopalan S., 2017, P IEEE C COMP VIS PA; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Volkova S, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P186; Wang Y., 2016, ICWSM, P723; Wei Y., 2018, P ECCV, P434; Wong FMF, 2016, IEEE T KNOWL DATA EN, V28, P2158, DOI 10.1109/TKDE.2016.2553667; Ye K., 2019, P IEEE INT C COMP VI; Ye Keren, 2019, IEEE T PATTERN ANAL; Zamir AR, 2017, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2017.196; Zhang Y, 2017, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2017.223; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhou F, 2010, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2010.5539966	79	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303060
C	van Hasselt, H; Hessel, M; Aslanides, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		van Hasselt, Hado; Hessel, Matteo; Aslanides, John			When to use parametric models in reinforcement learning?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PREDICTIVE CONTROL	We examine the question of when and how parametric models are most useful in reinforcement learning. In particular, we look at commonalities and differences between parametric models and experience replay. Replay-based learning algorithms share important traits with model-based approaches, including the ability to plan: to use more computation without additional data to improve predictions and behaviour. We discuss when to expect benefits from either approach, and interpret prior work in this context. We hypothesise that, under suitable conditions, replay-based algorithms should be competitive to or better than model-based algorithms if the model is used only to generate fictional transitions from observed states for an update rule that is otherwise model-free. We validated this hypothesis on Atari 2600 video games. The replay-based algorithm attained state-of-the-art data efficiency, improving over prior results with parametric models. Additionally, we discuss different ways to use models. We show that it can be better to plan backward than to planforward when using models to perform credit assignment (e.g., to directly learn a value or policy), even though the latter seems more common. Finally, we argue and demonstrate that it can be beneficial to plan forward for immediate behaviour,rather than for credit assignment.	[van Hasselt, Hado; Hessel, Matteo; Aslanides, John] DeepMind, London, England		van Hasselt, H (corresponding author), DeepMind, London, England.	hado@google.com; mtthss@google.com; jaslanides@google.com						Baird L., 1995, P 12 INT C MACH LEAR, P30; Bellemare MG, 2017, PR MACH LEARN RES, V70; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Boyan JA, 1999, MACHINE LEARNING, PROCEEDINGS, P49; Efroni Y., 2018, ADV NEURAL INFORM PR, P5238; Fortunato Meire, 2017, ARXIV170402798; Harutyunyan A, 2016, LECT NOTES ARTIF INT, V9925, P305, DOI 10.1007/978-3-319-46379-7_21; Hasselt H, 2010, ADV NEURAL INFORM PR, V23, P2613, DOI DOI 10.5555/2997046.2997187; Hessel M., 2018, OPENREVIEW; Hessel M, 2018, AAAI CONF ARTIF INTE, P3215; Holland G. Z., 2018, CORR; John Christopher, 1989, THESIS; Kaelbling L. P., 2010, WORKSH 24 AAA C ART; Kaiser L., 2019, ARXIV150300185; LIN LJ, 1992, MACH LEARN, V8, P293, DOI 10.1007/BF00992699; Lowrey K, 2019, INT C LEARN REPR; Mayne DQ, 2014, AUTOMATICA, V50, P2967, DOI 10.1016/j.automatica.2014.10.128; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Morari M, 1999, COMPUT CHEM ENG, V23, P667, DOI 10.1016/S0098-1354(98)00301-9; Pan YC, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4794; Parr R., 2008, P 25 INT C MACH LEAR, P752; Precup D., 2000, P 17 INT C MACH LEAR, P766; RICHALET J, 1978, AUTOMATICA, V14, P413, DOI 10.1016/0005-1098(78)90001-8; Riedmiller M, 2005, LECT NOTES ARTIF INT, V3720, P317, DOI 10.1007/11564096_32; Schaul T., 2016, ABS151105952 CORR; Silver D, 2017, PR MACH LEARN RES, V70; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton R. S., 1995, P WORKSH VAL FUNCT A, P85; Sutton R. S., 2013, P 7 INT C MACH LEARN, P216; Sutton R. S., 2009, P INT C MACH LEARN I, P125, DOI DOI 10.1145/1553374.1553501; Sutton R. S., 2008, ADV NEURAL INFORM PR, V21, P1609; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Sutton RS, 1998, P 15 INT C MACH LEAR, V98, P556; Sutton RS., 2016, J MACH LEARN RES, V17, P2603; Tsitsiklis JN, 1997, IEEE T AUTOMAT CONTR, V42, P674, DOI 10.1109/9.580874; Van Hasselt H., 2015, ARXIV150906461; van Hasselt H., 2016, CORR; van Hasselt H., 2014, UNCERTAINTYIN ARTIFI; van Hasselt H, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P330; van Seijen H, 2015, PR MACH LEARN RES, V37, P2314; Wagener N., 2019, AXXIV190208967; Wan Y., 2019, CORR; Wang ZY, 2016, PR MACH LEARN RES, V48; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Weber T., 2017, CORR; Williams R. J., 1993, NUCCS9311	49	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906005
C	Veitch, V; Wang, YX; Blei, DM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Veitch, Victor; Wang, Yixin; Blei, David M.			Using Embeddings to Correct for Unobserved Confounding in Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider causal inference in the presence of unobserved confounding. We study the case where a proxy is available for the unobserved confounding in the form of a network connecting the units. For example, the link structure of a social network carries information about its members. We show how to effectively use the proxy to do causal inference. The main idea is to reduce the causal estimation problem to a semi-supervised prediction of both the treatments and outcomes. Networks admit high-quality embedding models that can be used for this semi-supervised prediction. We show that the method yields valid inferences under suitable (weak) conditions on the quality of the predictive model. We validate the method with experiments on a semi-synthetic social network dataset.	[Veitch, Victor; Wang, Yixin; Blei, David M.] Columbia Univ, Dept Stat, New York, NY 10027 USA; [Blei, David M.] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA	Columbia University; Columbia University	Veitch, V (corresponding author), Columbia Univ, Dept Stat, New York, NY 10027 USA.		Wang, Yixin/ABF-4336-2021	Wang, Yixin/0000-0002-6617-4842				Airoldi Edoardo Maria, 2008, J MACHINE LEARNING R, V2008; Bickel P. J., 2000, INDIAN J STAT SERI A; Chamberlain BP, 2017, CORR MLG WORKSH 2017; Chernozhukov V., 2017, AM EC REV, V5; Chernozhukov Victor, 2017, ECONOMETRICS J; Cole S. R., 2008, AM J EPIDEMIOLOGY; Crane H., 2015, ARXIV150908185; Ding P., 2017, BIOMETRIKA, V2; Durrett R, 2006, RANDOM GRAPH DYNAMIC; Gopalan PK, 2013, P NATL ACAD SCI USA, V110, P14534, DOI 10.1073/pnas.1221839110; Hamilton WL, 2017, ADV NEUR IN, V30; Hamilton WL, 2017, REPRESENTATION LEARN; Hoff P., 2002, J AM STAT ASS, V460; Kang J. D. Y., 2007, STAT SCI, V4; Kuroki M., 1999, J JAPAN STAT SOC, V2; Leskovec J, 2014, SNAP DATASETS STANFO; Louizos C, 2017, ADV NEUR IN, V30; Miao W., 2018, BIOMETRIKA, V4; Middleton J. A., 2016, POLITICAL ANAL, V3; Newman M., 2010, NETWORKS INTRO, DOI [DOI 10.1093/ACPROF:OSO/9780199206650.001.0001, 10.1162/artl_r_00062., 10.1162/artl_r_00062]; Ogburn E. L., 2017, ANN APPL STAT, V2; Ogburn EL, 2017, ARXIV; Ogburn EL, 2018, COMPUT SOC SCI, P47, DOI 10.1007/978-3-319-77332-2_3; Orbanz P., 2015, PATTERN ANAL MACHINE, V2; Pearl J., 2012, ARXIV12033504; Perozzi B., 2014, P 20 INT C KNOWL DIS; Robins J. M., 2000, J AM STAT ASS, V450; Robins J. M., 2000, ASA P SECT BAYES STA; Robins J. M., 1994, J AM STAT ASS, V427; Rose S, 2011, SPRINGER SER STAT, P3, DOI 10.1007/978-1-4419-9782-1; ROSENBAUM PR, 1983, BIOMETRIKA, V70, P41, DOI 10.1093/biomet/70.1.41; Shalizi C., 2016, ARXIV160706565; Shalizi C. R., 2011, SOCIOL METHODS RES, V2; Takac L, 2012, P INT SCI C INT WORK; TAKAHASHI N, 2014, BIOMETRIKA, V2, DOI DOI 10.1186/2050-7771-2-6; Tchetgen E.J.T., 2017, ARXIV PREPRINT ARXIV; Veitch V., 2015, ARXIV151203099; Veitch V, 2019, PR MACH LEARN RES, V89; Veitch Victor, 2019, ARXIV190512741; Wang Y., 1987, J AM STAT ASS, V397; Yang ZJ, 2016, INT C ADV MECH SYST, P33, DOI 10.1109/ICAMechS.2016.7813417	41	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905044
C	Verma, A; Le, HM; Yue, YS; Chaudhuri, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Verma, Abhinav; Le, Hoang M.; Yue, Yisong; Chaudhuri, Swarat			Imitation-Projected Programmatic Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the problem of programmatic reinforcement learning, in which policies are represented as short programs in a symbolic language. Programmatic policies can be more interpretable, generalizable, and amenable to formal verification than neural policies; however, designing rigorous learning approaches for such policies remains a challenge. Our approach to this challenge-a meta-algorithm called PROPEL-is based on three insights. First, we view our learning task as optimization in policy space, modulo the constraint that the desired policy has a programmatic representation, and solve this optimization problem using a form of mirror descent that takes a gradient step into the unconstrained policy space and then projects back onto the constrained space. Second, we view the unconstrained policy space as mixing neural and programmatic representations, which enables employing state-of-the-art deep policy gradient approaches. Third, we cast the projection step as program synthesis via imitation learning, and exploit contemporary combinatorial methods for this task. We present theoretical convergence results for PROPEL and empirically evaluate the approach in three continuous control domains. The experiments show that PROPEL can significantly outperform state-of-the-art approaches for learning programmatic policies.	[Verma, Abhinav; Chaudhuri, Swarat] Rice Univ, Houston, TX 77251 USA; [Le, Hoang M.; Yue, Yisong] CALTECH, Pasadena, CA 91125 USA	Rice University; California Institute of Technology	Verma, A (corresponding author), Rice Univ, Houston, TX 77251 USA.	averma@rice.edu; hmle@caltech.edu; yyue@caltech.edu; swarat@rice.edu	Verma, Abhinav/ABE-3591-2021		United States Air Force [FA8750-19-C-0092]; NSF [CCF-1704883, 1645832]; Okawa Foundation; Intel; Raytheon; PIMCO	United States Air Force(United States Department of Defense); NSF(National Science Foundation (NSF)); Okawa Foundation; Intel(Intel Corporation); Raytheon; PIMCO	This work was supported in part by United States Air Force Contract # FA8750-19-C-0092, NSF Award # 1645832, NSF Award # CCF-1704883, the Okawa Foundation, Raytheon, PIMCO, and Intel.	Achiam J, 2017, PR MACH LEARN RES, V70; Alshiekh M, 2018, AAAI CONF ARTIF INTE, P2669; Alur R, 2017, LECT NOTES COMPUT SC, V10205, P319, DOI 10.1007/978-3-662-54577-5_18; Alur R, 2013, 2013 FORMAL METHODS IN COMPUTER-AIDED DESIGN (FMCAD), P1, DOI 10.3233/978-1-61499-495-4-1; Ang KH, 2005, IEEE T CONTR SYST T, V13, P559, DOI 10.1109/TCST.2005.847331; ASTROM KJ, 1984, AUTOMATICA, V20, P645, DOI 10.1016/0005-1098(84)90014-1; BALOG M, 2017, P 5 INT C LEARN REPR; Bastani O, 2018, ADV NEUR IN, V31; Bauschke H. H., 2011, CONVEX ANAL MONOTONE, V408; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Bellman R., Q APPL MATH, V14, P11; Berkenkamp Felix, 2017, ADV NEURAL INFORM PR, P908; Bloem R, 2009, LECT NOTES COMPUT SC, V5643, P140, DOI 10.1007/978-3-642-02658-4_14; Boots, 2019, INT C ART INT STAT A; Chang KW, 2015, PR MACH LEARN RES, V37, P2058; Chaudhuri S, 2014, ACM SIGPLAN NOTICES, V49, P207, DOI 10.1145/2535838.2535859; Cheng CA, 2019, PR MACH LEARN RES, V97; Cheng Ching-An, 2019, UNCERTAINTY ARTIFICI; Cheng R, 2019, PR MACH LEARN RES, V97; Dalal G., 2018, ARXIV180108757; Devlin J, 2017, PR MACH LEARN RES, V70; Du Tao, 2018, ACM T GRAPHIC, V37, p[213, 16]; Duan Y, 2016, PR MACH LEARN RES, V48; Duchi J. C., 2010, COLT, P14; Ellis K, 2018, ADV NEUR IN, V31; Feser JK, 2015, ACM SIGPLAN NOTICES, V50, P229, DOI [10.1145/2737924.2737977, 10.1145/2813885.2737977]; Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385; Gulwani S, 2017, FOUND TRENDS PROGRAM, V4, P1, DOI 10.1561/2500000010; Henderson P, 2018, AAAI CONF ARTIF INTE, P3207; JHA S, 2010, P 2010 ACMIEEE 32 IN, P215, DOI DOI 10.1145/1806799.1806833; Kakade Sham M., 2003, THESIS; Konda VR, 2000, ADV NEUR IN, V12, P1008; Le HM, 2016, PR MACH LEARN RES, V48; Le HM, 2019, PR MACH LEARN RES, V97; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Mahadevan S., 2012, C UNC ART INT, P564; Montgomery W, 2016, ADV NEUR IN, V29; Murali V., 2018, INT C LEARN REPR; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Parisotto Emilio, 2016, ARXIV161101855; Polozov O, 2015, ACM SIGPLAN NOTICES, V50, P107, DOI [10.1145/2858965.2814310, 10.1145/2814270.2814310]; Raychev V, 2016, ACM SIGPLAN NOTICES, V51, P761, DOI 10.1145/2914770.2837671; Ross S., 2014, ROSS; Ross St<prime>ephane, 2011, AISTATS; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Si X., 2019, INT C LEARN REPR; Solar-Lezama A, 2006, ACM SIGPLAN NOTICES, V41, P404, DOI 10.1145/1168917.1168907; Sun W., 2018, PROC 32 INT C NEURAL, P7059; Sun Wen, 2017, INT C MACH LEARN ICM; Sun Wen, 2018, INT C LEARN REPR ICL; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Thomas P. S., 2013, ADV NEURAL INFORM PR, V26, P2337; Valkov Lazar, 2018, ADV NEURAL INFORM PR, P8687; Yadav D, 2018, 2018 5TH IEEE UTTAR PRADESH SECTION INTERNATIONAL CONFERENCE ON ELECTRICAL, ELECTRONICS AND COMPUTER ENGINEERING (UPCON), P502; Zhu He, 2019, ACM C PROGR LANG DES	58	3	3	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907041
C	Wang, Q; Li, YR; Xiong, JC; Zhang, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Qing; Li, Yingru; Xiong, Jiechao; Zhang, Tong			Divergence-Augmented Policy Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In deep reinforcement learning, policy optimization methods need to deal with issues such as function approximation and the reuse of off-policy data. Standard policy gradient methods do not handle off-policy data well, leading to premature convergence and instability. This paper introduces a method to stabilize policy optimization when off-policy data are reused. The idea is to include a Bregman divergence between the behavior policy that generates the data and the current policy to ensure small and safe policy updates with off-policy data. The Bregman divergence is calculated between the state distributions of two policies, instead of only on the action probabilities, leading to a divergence augmentation formulation. Empirical experiments on Atari games show that in the data-scarce scenario where the reuse of off-policy data becomes necessary, our method can achieve better performance than other state-of-the-art deep reinforcement learning algorithms.	[Wang, Qing] Huya AI, Guangzhou, Peoples R China; [Li, Yingru] Chinese Univ Hong Kong, Shenzhen, Peoples R China; [Wang, Qing; Xiong, Jiechao] Tencent AI Lab, Shenzhen, Peoples R China; [Zhang, Tong] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China	Chinese University of Hong Kong, Shenzhen; Tencent; Hong Kong University of Science & Technology	Wang, Q (corresponding author), Huya AI, Guangzhou, Peoples R China.							Abadi M, 2016, P 12 USENIX S OPERAT; Abdolmaleki Abbas, 2018, INT C LEARN REPR; Achiam J., 2017, ARXIV170510528; [Anonymous], 2018, ARXIV180201561; [Anonymous], 2016, ARXIV161101224; Asmussen S., 2003, APPL PROBABILITY QUE, P3; Audibert J.-Y., 2009, P 22 ANN C LEARN THE, P217; Audibert J-Y, 2011, P 24 ANN C LEARN THE, P107; Ba J., 2017, P 3 INT C LEARN REPR; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Belousov B., 2017, ARXIV E PRINTS; Bregman L. M., 1967, COMP MATH MATH PHYS+, V7, P200, DOI DOI 10.1016/0041-5553(67)90040-7; Brockman G., 2016, OPENAI GYM; Bubeck S., 2015, FDN TRENDS MACHINE L; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cesa-Bianchi N., 2017, ADV NEURAL INFORM PR; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chow Y., 2018, INT C MACH LEARN, P978; Csiszar Imre, 2011, INFORM THEORY CODING, VSecond; Fox R., 2015, ARXIV PREPRINT ARXIV; Haarnoja T., 2018, P 35 INT C MACH LEAR; Haarnoja T., 2017, ARXIV170208165; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Horgan D., 2018, P ICLR; Kakade S, 2002, ADV NEUR IN, V14, P1531; Kakade S., 2002, P 19 INT C MACH LEAR; Kappen HJ, 2005, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2005/11/P11011; KIM C, 2018, ADV NEURAL INFORM PR, P344; Lee K., 2019, TSALLIS REINFORCEMEN; Li YL, 2016, 2016 9TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2016), P1995; Liu Q., 2018, ADV NEURAL INFORM PR, P5356; McMahan B., 2011, JMLR WORKSHOP C P, V15, P525; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Munos R, 2016, P 30 INT C NEUR INF; Nachum O, 2017, NIPS, P2775; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Oh J, 2018, PR MACH LEARN RES, V80; Peters J, 2010, AAAI CONF ARTIF INTE, P1607; ROCKAFELLAR RT, 1976, SIAM J CONTROL, V14, P877, DOI 10.1137/0314056; Schulman J., 2017, ABS170706347 CORR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Schulman John, 2017, EQUIVALENCE POLICY; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton, 2017, INTRO REINFORCEMENT; Sutton R. S., 2016, J MACHINE LEARNING R, V17, P2603; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Todorov E., 2007, ADV NEURAL INFORM PR, V19, DOI 10.7551/mitpress/7503.003.0176; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; Wang Q., 2018, ADV NEURAL INFORM PR, V31, P6291; Xu Z., 2018, 32 C NEUR INF PROC S, P2396; Ziebart B. D., 2008, AAAI, V8, P1433; Zimin A., 2013, ADV NEURAL INFORM PR, V26	56	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306014
C	Wang, W; Dang, Z; Hu, YL; Fua, P; Salzmann, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Wei; Dang, Zheng; Hu, Yinlin; Fua, Pascal; Salzmann, Mathieu			Backpropagation-Friendly Eigendecomposition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DECOMPOSITION	Eigendecomposition (ED) is widely used in deep networks. However, the backpropagation of its results tends to be numerically unstable, whether using ED directly or approximating it with the Power Iteration method, particularly when dealing with large matrices. While this can be mitigated by partitioning the data in small and arbitrary groups, doing so has no theoretical basis and makes its impossible to exploit the power of ED to the full. In this paper, we introduce a numerically stable and differentiable approach to leveraging eigenvectors in deep networks. It can handle large matrices without requiring to split them. We demonstrate the better robustness of our approach over standard ED and PI for ZCA whitening, an alternative to batch normalization, and for PCA denoising, which we introduce as a new normalization strategy for deep networks, aiming to further denoise the network's features.	[Wang, Wei; Hu, Yinlin; Fua, Pascal; Salzmann, Mathieu] Ecole Polytech Fed Lausanne, CVLab, CH-1015 Lausanne, Switzerland; [Dang, Zheng] Xi An Jiao Tong Univ, Xian, Peoples R China	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Xi'an Jiaotong University	Wang, W (corresponding author), Ecole Polytech Fed Lausanne, CVLab, CH-1015 Lausanne, Switzerland.	wei.wang@epfl.ch; dangzheng713@stu.xjtu.edu.cn; yinlin.hu@epfl.ch; pascal.fua@epfl.ch; mathieu.salzmann@epfl.ch	Wang, Wei/AAK-5521-2021	Wang, Wei/0000-0002-5477-1017				Babu Y., 2012, SIGNAL IMAGE PROCESS, V3; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Burden R., 1989, NUMERICAL ANAL, V4th; Dang Zheng, 2018, ECCV; Desjardins G, 2015, NIPS; Huang Lei, 2018, CVPR; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Ionescu Catalin, 2015, CVPR; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kessy A, 2018, AM STAT, V72, P309, DOI 10.1080/00031305.2016.1277159; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; Miyato Takeru, 2018, ICLR; Nakatsukasa Y, 2013, SIAM J SCI COMPUT, V35, pA1325, DOI 10.1137/120876605; Papadopoulo T, 2000, LECT NOTES COMPUT SC, V1842, P554; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Ranftl Rene, 2018, ECCV; Siarohin A., 2018, ICLR; Suwajanakorn S., 2018, NIPS; Ye M, 2017, IEEE I CONF COMP VIS, P5152, DOI 10.1109/ICCV.2017.550; Yi K.M., 2018, CVPR; Zanfir Andrei, 2018, CVPR	21	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303018
C	Wei, CL; Ma, TY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wei, Colin; Ma, Tengyu			Data-dependent Sample Complexity of Deep Neural Networks via Lipschitz Augmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Existing Rademacher complexity bounds for neural networks rely only on norm control of the weight matrices and depend exponentially on depth via a product of the matrix norms. Lower bounds show that this exponential dependence on depth is unavoidable when no additional properties of the training data are considered. We suspect that this conundrum comes from the fact that these bounds depend on the training data only through the margin. In practice, many data-dependent techniques such as Batchnorm improve the generalization performance. For feedforward neural nets as well as RNNs, we obtain tighter Rademacher complexity bounds by considering additional data-dependent properties of the network: the norms of the hidden layers of the network, and the norms of the Jacobians of each layer with respect to all previous layers. Our bounds scale polynomially in depth when these empirical quantities are small, as is usually the case in practice. To obtain these bounds, we develop general tools for augmenting a sequence of functions to make their composition Lipschitz and then covering the augmented functions. Inspired by our theory, we directly regularize the network's Jacobians during training and empirically demonstrate that this improves test performance.	[Wei, Colin; Ma, Tengyu] Stanford Univ, Comp Sci Dept, Stanford, CA 94305 USA	Stanford University	Wei, CL (corresponding author), Stanford Univ, Comp Sci Dept, Stanford, CA 94305 USA.	colinwei@stanford.edu; tengyuma@stanford.edu			NSF Graduate Research Fellowship; Toyota Research Institute (TRI)	NSF Graduate Research Fellowship(National Science Foundation (NSF)); Toyota Research Institute (TRI)	CW was supported by a NSF Graduate Research Fellowship. Toyota Research Institute (TRI) provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity.	[Anonymous], 2017, ADV NEURAL INFORM PR; [Anonymous], 2017, ARXIV171206541; Arora Sanjeev, 2018, ARXIV180205296; Ba Jimmy Lei, 2016, ARXIV160706450; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; BARTLETT P. L., 2017, ADV NEURAL INFORM PR, V30, P6240; BAUER FL, 1974, SIAM J NUMER ANAL, V11, P87, DOI 10.1137/0711010; Brutzkus Alon, 2017, ARXIV PREPRINT ARXIV; Chen M., 2019, GEN BOUNDS FAMILY RE; DUDLEY R.M, 1967, J FUNCT ANAL, V1, P290, DOI DOI 10.1016/0022-1236(67)90017-1; Dziugaite G. K., 2018, ADV NEURAL INFORM PR, P8430; Dziugaite Gintare Karolina, 2017, ARXIV170311008; Dziugaite Gintare Karolina, 2017, ARXIV171209376; Gunasekar S., 2017, ADV NEURAL INFORM PR, P6151; Gunasekar S, 2018, ARXIV180208246; Gunasekar S., 2018, ARXIV180600468; Hardt M, 2015, ARXIV150901240; Ioffe S, 2015, ARXIV 1502 03167, V32, P448; Ji Z., 2018, ARXIV180307300; Keskar N.S., 2016, ABS160904836; Klambauer Gnter, 2017, SELF NORMALIZING NEU; Koiran P., 1997, Computational Learning Theory. Third European Conference, EuroCOLT '97. Proceedings, P223; Krueger David, 2015, ARXIV PREPRINT ARXIV; Li Y., 2018, C LEARN THEOR, V75, P2; Littwin Etai, 2018, ADV NEURAL INFORM PR, P2115; Nagarajan V., 2019, ARXIV190513344, p2019a; Neyshabur B., 2017, ADV NEURAL INFORM PR, P5947; Neyshabur B., 2018, ARXIV180512076; Neyshabur B., 2015, ARXIV151106747; Neyshabur Behnam, 2017, ARXIV170709564; Novak R., 2018, P INT C LEARN REPR, P1; Sokolic J, 2017, IEEE T SIGNAL PROCES, V65, P4265, DOI 10.1109/TSP.2017.2708039; Soudry D, 2018, J MACH LEARN RES, V19; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Unterthiner T, 2015, COMPUTER SCI, DOI DOI 10.48550/ARXIV.1511.07289; Wager Stefan, 2013, ADV NEURAL INFORM PR, P351; Wei Colin, 2018, ARXIV181005369; Wikipedia contributors, 2019, CHAIN RUL WIK; Wu Y., 2018, ARXIV180308494; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]; Zhang Chiyuan, 2016, ARXIV161103530; Zhang H., 2019, ARXIV190104684; Zhang Jiong, 2018, ARXIV180309327	44	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901036
C	Wu, SS; Dimakis, AG; Sanghavi, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wu, Shanshan; Dimakis, Alexandros G.; Sanghavi, Sujay			Learning Distributions Generated by One-Layer ReLU Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider the problem of estimating the parameters of a d-dimensional rectified Gaussian distribution from i.i.d. samples. A rectified Gaussian distribution is defined by passing a standard Gaussian distribution through a one -layer ReLU neural network. We give a simple algorithm to estimate the parameters (i.e., the weight matrix and bias vector of the ReLU neural network) up to an error 611VVlIF using 0(11E2) samples and (d2 62) time (log factors are ignored for simplicity). This implies that we can estimate the distribution up to in total variation distance using O(, 2d2/ 2) samples, where n is the condition number of the covariance matrix. Our only assumption is that the bias vector is non -negative. Without this non -negativity assumption, we show that estimating the bias vector within any error requires the number of samples at least exponential in the infinity norm of the bias vector. Our algorithm is based on the key observation that vector norms and pairwise angles can be estimated separately. We use a recent result on learning from truncated samples. We also prove two sample complexity lower bounds: C2(1/6) samples are required to estimate the parameters up to error 6, while C2(d162) samples are necessary to estimate the distribution up to E in total variation distance. The first lower bound implies that our algorithm is optimal for parameter estimation. Finally, we show an interesting connection between learning a two -layer generative model and non -negative matrix factorization. Experimental results are provided to support our analysis.	[Wu, Shanshan; Dimakis, Alexandros G.; Sanghavi, Sujay] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Wu, SS (corresponding author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.	shanshan@utexas.edu; dimakis@austin.utexas.edu; sanghavi@mail.utexas.edu			NSF [1302435, 1564000, 1618689, DMS 1723052, CCF 1763702, AF 1901292]	NSF(National Science Foundation (NSF))	This research has been supported by NSF Grants 1302435, 1564000, and 1618689, DMS 1723052, CCF 1763702, AF 1901292 and research gifts by Google, Western Digital and NVIDIA.	Arjovsky M, 2017, PR MACH LEARN RES, V70; Arora S, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P145; Ashtiani H, 2018, ADV NEUR IN, V31; Cho Youngmin, 2009, P ADV NEUR INF PROC, V22, P342; Daskalakis C, 2018, ANN IEEE SYMP FOUND, P639, DOI 10.1109/FOCS.2018.00067; Devroye L., 2018, ARXIV181008693; Duchi John, 2019, LECT NOTES STAT, V377; Gao W, 2019, 22 INT C ART INT STA, P1950; Ge Rong, 2019, INT C LEARN REPR; Goel S, 2018, PR MACH LEARN RES, V80; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grathwohl Will, 2018, ARXIV181001367; Harva M, 2007, SIGNAL PROCESS, V87, P509, DOI 10.1016/j.sigpro.2006.06.006; Kingma D. P., 2013, AUTO ENCODING VARIAT; Mazumdar Arya, 2019, P 57 ANN ALL C COMM; Nguyen Thanh V, 2018, ARXIV180600572; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Valiant L. G., 1984, Communications of the ACM, V27, P1134, DOI 10.1145/1968.1972; van den Oord A, 2016, PR MACH LEARN RES, V48; Wainwright MJ, 2019, CA ST PR MA, P1, DOI 10.1017/9781108627771; Williamson D. P., 2011, DESIGN APPROXIMATION, DOI DOI 10.1017/CBO9780511921735; Zhong K, 2017, PR MACH LEARN RES, V70	28	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308016
C	Xiao, CJ; Mei, JC; Huang, RT; Schuurmans, D; Muller, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xiao, Chenjun; Mei, Jincheng; Huang, Ruitong; Schuurmans, Dale; Mueller, Martin			Maximum Entropy Monte-Carlo Planning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GAME; GO	We develop a new algorithm for online planning in large scale sequential decision problems that improves upon the worst case efficiency of UCT. The idea is to augment Monte-Carlo Tree Search (MCTS) with maximum entropy policy optimization, evaluating each search node by softmax values back-propagated from simulation. To establish the effectiveness of this approach, we first investigate the single-step decision problem, stochastic softmax bandits, and show that softmax values can be estimated at an optimal convergence rate in terms of mean squared error. We then extend this approach to general sequential decision making by developing a general MCTS algorithm, Maximum Entropy for Tree Search (MENTS). We prove that the probability of MENTS failing to identify the best decision at the root decays exponentially, which fundamentally improves the polynomial convergence rate of UCT. Our experimental results also demonstrate that MENTS is more sample efficient than UCT in both synthetic problems and Atari 2600 games.	[Xiao, Chenjun; Mei, Jincheng; Schuurmans, Dale; Mueller, Martin] Univ Alberta, Edmonton, AB, Canada; [Huang, Ruitong] Borealis AI, Toronto, ON, Canada	University of Alberta	Xiao, CJ (corresponding author), Univ Alberta, Edmonton, AB, Canada.	fchenjun@ualberta.com; jmei2@ualberta.com; ruitong.huang@borealisai.com; daes@ualberta.com; mmueller@ualberta.com			NSERC, the Natural Sciences and Engineering Research Council of Canada; AMII, the Alberta Machine Intelligence Institute	NSERC, the Natural Sciences and Engineering Research Council of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)); AMII, the Alberta Machine Intelligence Institute	The authors wish to thank Csaba Szepesvari for useful discussions, and the anonymous reviewers for their valuable advice. Part of the work is performed when the first two authors were interns at BorealisAl. This research was supported by NSERC, the Natural Sciences and Engineering Research Council of Canada, and AMII, the Alberta Machine Intelligence Institute.	Cazenave T, 2015, IEEE T COMP INTEL AI, V7, P102, DOI 10.1109/TCIAIG.2014.2317737; Coquelin P.-A., 2007, BANDIT ALGORIT UNPUB; Haarnoja T, 2017, PR MACH LEARN RES, V70; Haarnoja T, 2018, PR MACH LEARN RES, V80; Karnin Zohar, 2013, P 30 INT C INT C MAC, P1238; Kearns M, 2002, MACH LEARN, V49, P193, DOI 10.1023/A:1017932429737; Khandelwal P, 2016, PR MACH LEARN RES, V48; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Smith Stephen JJ, 1994, AAAI; Sutton Richard S, 1998, INTRO REINFORCEMENT, V135; Szepesvdri C., 2018, BANDIT ALGORITHMS; TOLPIN D, 2012, 26 AAAI C ART INT; Xiao CJ, 2018, AAAI CONF ARTIF INTE, P1455	17	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901018
C	Xie, YQ; Xu, ZW; Kankanhalli, MS; Meel, KS; Soh, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xie, Yaqi; Xu, Ziwei; Kankanhalli, Mohan S.; Meel, Kuldeep S.; Soh, Harold			Embedding Symbolic Knowledge into Deep Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this work, we aim to leverage prior symbolic knowledge to improve the performance of deep models. We propose a graph embedding network that projects propositional formulae (and assignments) onto a manifold via an augmented Graph Convolutional Network (GCN). To generate semantically-faithful embeddings, we develop techniques to recognize node heterogeneity, and semantic regularization that incorporate structural constraints into the embedding. Experiments show that our approach improves the performance of models trained to perform entailment checking and visual relation prediction. Interestingly, we observe a connection between the tractability of the propositional theory representation and the ease of embedding. Future exploration of this connection may elucidate the relationship between knowledge compilation and vector representation learning.	[Xie, Yaqi; Xu, Ziwei; Kankanhalli, Mohan S.; Meel, Kuldeep S.; Soh, Harold] Natl Univ Singapore, Sch Comp, Singapore, Singapore	National University of Singapore	Xie, YQ (corresponding author), Natl Univ Singapore, Sch Comp, Singapore, Singapore.	yaqixie@comp.nus.edu.sg; ziwei-xu@comp.nus.edu.sg; mohan@comp.nus.edu.sg; meel@comp.nus.edu.sg; harold@comp.nus.edu.sg			MOE Tier 1 Grant; National Research Foundation Singapore under its Al Singapore Programme [AISG-RP-2018-005]; National Research Foundation, Prime Minister's Office, Singapore under its Strategic Capability Research Centres Funding Initiative	MOE Tier 1 Grant(Ministry of Education, Singapore); National Research Foundation Singapore under its Al Singapore Programme; National Research Foundation, Prime Minister's Office, Singapore under its Strategic Capability Research Centres Funding Initiative(National Research Foundation, Singapore)	This work was supported in part by a MOE Tier 1 Grant to Harold Soh and by the National Research Foundation Singapore under its Al Singapore Programme [AISG-RP-2018-005]. It was also supported by the National Research Foundation, Prime Minister's Office, Singapore under its Strategic Capability Research Centres Funding Initiative.	Allamanis M., 2017, ICML; Ansari AF, 2019, AAAI CONF ARTIF INTE, P3175; Besold TR, 2017, ARXIV171103902; Darwiche A, 2001, J ACM, V48, P608, DOI 10.1145/502090.502091; Darwiche A, 2002, J ARTIF INTELL RES, V17, P229, DOI 10.1613/jair.989; Darwiche A., 2001, J APPL NONCLASSICAL, V11; Darwiche A., 2004, ECAI; Defferrard M., 2016, NIPS; Demeester Thomas, 2016, EMNLP; Donadello I., 2017, IJCAL; Evans Richard, 2018, ICLR; Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005; Hu Z., 2016, ABS160306318 CORR; Ignatiev A, 2018, LECT NOTES COMPUT SC, V10929, P428, DOI 10.1007/978-3-319-94144-8_26; Kim Yoon, 2016, AAAL; Kingma D. P., 2015, ABS14126980 CORR; Kipf Thomas N., 2017, INT C LEARNING REPRE; Le P., 2015, SEM NAACL HLT; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Liang Y., 2019, AAAL; Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; Pennington Jeffrey, 2014, EMNLP; Rockt <spacing diaeresis>aschel T., 2015, HLT NAACL; Serafini L., 2016, NESY HLAI; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Soh H, 2015, J HUM-ROBOT INTERACT, V4, P76, DOI 10.5898/JHRI.4.3.Soh; Stewart R., 2017, AAAI; Tai K. S., 2015, ACL; Taunyazov T, 2019, IEEE INT CONF ROBOT, P4269, DOI 10.1109/ICRA.2019.8793967; Xu J., 2018, ICML; Zhu X., 2015, ICML	32	3	3	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304026
C	Zhang, J; Yu, HF; Dhillon, IS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Jiong; Yu, Hsiang-Fu; Dhillon, Inderjit S.			AutoAssist: A Framework to Accelerate Training of Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep Neural Networks (DNNs) have yielded superior performance in many contemporary applications. However, the gradient computation in a deep model with millions of instances leads to a lengthy training process even with modern GPU/TPU hardware acceleration. In this paper, we propose AutoAssist, a simple framework to accelerate training of a deep neural network. Typically, as the training procedure evolves, the amount of improvement by a stochastic gradient update varies dynamically with the choice of instances in the mini-batch. In AutoAssist, we utilize this fact and design an instance shrinking operation that is used to filter out instances with relatively low marginal improvement to the current model; thus the computationally intensive gradient computations are performed on informative instances as much as possible. Specifically, we train a very lightweight Assistant model jointly with the original deep network, which we refer to as the Boss. The Assistant model is designed to gauge the importance of a given instance with respect to the current Boss model such that the shrinking operation can be applied in the batch generator. With careful design, we train the Boss and Assistant in a non-blocking and asynchronous fashion such that overhead is minimal. To demonstrate the effectiveness of AutoAssist, we conduct experiments on two contemporary applications: image classification using ResNets with varied number of layers, and neural machine translation using LSTMs, ConvS2S and Transformer models. For each application, we verify that AutoAssist leads to significant reduction in training time; in particular, 30% to 40% of the total operation count can be reduced which leads to faster convergence and a corresponding decrease in training time.	[Zhang, Jiong; Dhillon, Inderjit S.] Univ Texas Austin, Austin, TX 78712 USA; [Yu, Hsiang-Fu; Dhillon, Inderjit S.] Amazon, Seattle, WA USA	University of Texas System; University of Texas Austin; Amazon.com	Zhang, J (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	zhangjiong724@utexas.edu; rofu.yu@gmail.com; inderjit@cs.utexas.edu			NSF [IIS-1546452 CCF-1564000]; AWS Cloud Credits for Research program	NSF(National Science Foundation (NSF)); AWS Cloud Credits for Research program	This research was supported by NSF grants IIS-1546452 CCF-1564000 and AWS Cloud Credits for Research program.	Arvin AM, 2009, LIVE VARIOLA VIRUS: CONSIDERATIONS FOR CONTINUING RESEARCH, P9; Bengio, 2015, ARXIV151106481; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chang H. -S., 2017, ADV NEURAL INFORM PR, V30, P1002; Collobert R., 2008, P 25 ICML, V25, P160, DOI DOI 10.1145/1390156.1390177; Fan Y., 2018, ARXIV PREPRINT ARXIV; Gehring J., 2017, P ICML; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hsieh C.J., 2008, P 25 INT C MACHINE L, P408, DOI DOI 10.1145/1390156.1390208; Jiang L., 2017, ARXIV PREPRINT ARXIV; Jiang L, 2015, AAAI CONF ARTIF INTE, P2694; Joachims, 1998, ADV KERNEL METHODS S, P28; Katharopoulos A., 2017, ARXIV170600043; Kim T.H., 2018, ARXIV180100904; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kumar A, 2010, ASIA PACIF MICROWAVE, P1189; Langkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008; LeCun Y., 2010, MNIST HANDWRITTEN DI; Li H., 2017, P INT JOINT C ART IN; Loshchilov I., 2015, ARXIV151106343; Needell D, 2016, MATH PROGRAM, V155, P549, DOI 10.1007/s10107-015-0864-7; Robertson S, 2004, J DOC, V60, P503, DOI 10.1108/00220410410560582; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Tsvetkov Y, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P130; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wiseman Sam, 2016, ARXIV160602960; Zhaoguang Pan, 2015, 2015 IEEE Power & Energy Society General Meeting, P1, DOI 10.1109/PESGM.2015.7285868	31	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306005
C	Zhang, R; Tong, HH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Rui; Tong, Hanghang			Robust Principal Component Analysis with Adaptive Neighbors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				FACE REPRESENTATION; 2-DIMENSIONAL PCA; 2DPCA	Suppose certain data points are overly contaminated, then the existing principal component analysis (PCA) methods are frequently incapable of filtering out and eliminating the excessively polluted ones, which potentially lead to the functional degeneration of the corresponding models. To tackle the issue, we propose a general framework namely robust weight learning with adaptive neighbors (RWL-AN), via which adaptive weight vector is automatically obtained with both robustness and sparse neighbors. More significantly, the degree of the sparsity is steerable such that only exact k well-fitting samples with least reconstruction errors are activated during the optimization, while the residual samples, i.e., the extreme noised ones are eliminated for the global robustness. Additionally, the framework is further applied to PCA problem to demonstrate the superiority and effectiveness of the proposed RWL-AN model.	[Zhang, Rui] Arizona State Univ, Tempe, AZ USA; [Tong, Hanghang] Univ Illinois, Urbana, IL 61801 USA	Arizona State University; Arizona State University-Tempe; University of Illinois System; University of Illinois Urbana-Champaign	Tong, HH (corresponding author), Univ Illinois, Urbana, IL 61801 USA.	ruizhang8633@gmail.com; htong@illinois.edu			NSF [IIS-1651203, IIS-1715385, 2017-ST-061-QA0001]	NSF(National Science Foundation (NSF))	This work is supported by NSF (IIS-1651203, IIS-1715385), and DHS (2017-ST-061-QA0001).	[Anonymous], 1994, PARAMETERISATION STO; Graham D.B., 1998, NATO ASI SERIES F CO, V163, P446; Jolliffe IT, 2002, ENCY STATIST BEHAV S, DOI [10.1007/0-387-22440-8_13, 10.1007/b98835]; Li XL, 2010, IEEE T SYST MAN CY B, V40, P1170, DOI 10.1109/TSMCB.2009.2035629; Luo Minnan, 2016, P 25 INT JOINT C ART; Nie F., 2011, PROC INT JOINT C ART, P1433; Nie FP, 2016, AAAI CONF ARTIF INTE, P1962; Nie FP, 2014, PR MACH LEARN RES, V32, P1062; Papadimitriou Christos H, 1998, IEEE T ACOUSTICS SPE, V32, P1258; Philips I., 1998, IMAGE VISION COMPUT, P295; Song K, 2017, AAAI CONF ARTIF INTE, P2555; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105; Wang HX, 2013, NEURAL NETWORKS, V46, P190, DOI 10.1016/j.neunet.2013.06.002; Wang R, 2015, IEEE T CYBERNETICS, V45, P1108, DOI 10.1109/TCYB.2014.2341575; WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9; Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097; Ye J., 2004, MACHINE LEARN, V61, P167; Yun Q, 2015, I C INTELL COMPUT TE, P871, DOI 10.1109/ICICTA.2015.221; Zhang DQ, 2005, NEUROCOMPUTING, V69, P224, DOI 10.1016/j.neucom.2005.06.004; Zhang R, 2017, INT CONF ACOUST SPEE, P6065, DOI 10.1109/ICASSP.2017.7953321	20	3	3	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307002
C	Zhang, R; Liu, XW; Wang, YY; Wang, LW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Rui (Ray); Liu, Xingwu; Wang, Yuyi; Wang, Liwei			McDiarmid-Type Inequalities for Graph-Dependent Variables and Stability Bounds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A crucial assumption in most statistical learning theory is that samples are independently and identically distributed (i.i.d.). However, for many real applications, the i.i.d. assumption does not hold. We consider learning problems in which examples are dependent and their dependency relation is characterized by a graph. To establish algorithm-dependent generalization theory for learning with non-i.i.d. data, we first prove novel McDiarmid-type concentration inequalities for Lipschitz functions of graph-dependent random variables. We show that concentration relies on the forest complexity of the graph, which characterizes the strength of the dependency. We demonstrate that for many types of dependent data, the forest complexity is small and thus implies good concentration. Based on our new inequalities, we establish stability bounds for learning graph-dependent data.	[Zhang, Rui (Ray)] Monash Univ, Sch Math, Clayton, Vic, Australia; [Liu, Xingwu] Chinese Acad Sci, Univ Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China; [Wang, Yuyi] Swiss Fed Inst Technol, Zurich, Switzerland; [Wang, Yuyi] X Order Lab, Beijing, Peoples R China; [Wang, Liwei] Peking Univ, Sch EECS, MOE, Ctr Data Sci,Key Lab Machine Percept, Beijing, Peoples R China	Monash University; Chinese Academy of Sciences; Institute of Computing Technology, CAS; University of Chinese Academy of Sciences, CAS; Swiss Federal Institutes of Technology Domain; ETH Zurich; Peking University	Liu, XW (corresponding author), Chinese Acad Sci, Univ Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.	rui.zhang@monash.edu; liuxingwu@ict.ac.cn; yuyiwang@920gmail.com; wangl@cis.pku.edu.cn	Liu, Xingwu/HGA-4674-2022	Zhang, Rui/0000-0002-0748-7398	National Key Research and Development Program of China [2016YFB1000201]; National Natural Science Foundation of China [61420106013]; State Key Laboratory of Computer Architecture Open Fund [CARCH3410]; Youth Innovation Promotion Association of Chinese Academy of Sciences	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); State Key Laboratory of Computer Architecture Open Fund; Youth Innovation Promotion Association of Chinese Academy of Sciences	Rui (Ray) Zhang would like to thank Nick Wormald for valuable comments on an early version of this paper. Yuyi Wang would like to thank Ondfej Kuelka for very helpful discussions. Liwei Wang would like to thank Yunchang Yang for very helpful discussions. Xingwu Liu's work is partially supported by the National Key Research and Development Program of China (Grant No. 2016YFB1000201), the National Natural Science Foundation of China (61420106013), State Key Laboratory of Computer Architecture Open Fund (CARCH3410), and Youth Innovation Promotion Association of Chinese Academy of Sciences.	Amini M.-R., 2015, LEARNING PARTIALLY L; [Anonymous], 2014, ICML; Anselin L, 2013, SPATIAL ECONOMETRICS, V4; BALDI P, 1989, ANN PROBAB, V17, P1646, DOI 10.1214/aop/1176991178; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Chatterjee Sourav, 2005, TMATH0507526 ARXIV; CHEN LHY, 1978, PROBABILITY THEORY R, V43, P223; Chen LHY, 2005, LECT NOTES SER INST, V4, P1; Dagan Yuval, 2019, C LEARN THEOR, P914; Dehling H., 2002, EMPIRICAL PROCESS TE, P3; DEVROYE LP, 1979, IEEE T INFORM THEORY, V25, P601, DOI 10.1109/TIT.1979.1056087; DIANANDA PH, 1953, P CAMB PHILOS SOC, V49, P239, DOI 10.1017/S0305004100028334; DOBRUSCH.PL, 1968, THEOR PROBAB APPL+, V13, P197, DOI 10.1137/1113026; Dousse Jehanne, 2016, ARXIV161005082; Erdos P., 1975, INFINITE FINITE SETS, V10, P609; Feray V, 2018, ELECTRON J PROBAB, V23, DOI 10.1214/18-EJP222; Hang H, 2014, J MULTIVARIATE ANAL, V127, P184, DOI 10.1016/j.jmva.2014.02.012; He FC, 2016, NEUROCOMPUTING, V171, P1556, DOI 10.1016/j.neucom.2015.07.088; HOEFFDING W, 1948, DUKE MATH J, V15, P773, DOI 10.1215/S0012-7094-48-01568-3; Ibragimov I. A., 1962, THEOR PROBAB APPL, V7, P349; Janson S, 2004, RANDOM STRUCT ALGOR, V24, P234, DOI 10.1002/rsa.20008; Janson Svante, 1988, EXPONENTIAL BOUND PR; Kirichenko A, 2015, J MACH LEARN RES, V16, P2909; Kontorovich A, 2017, IMA VOL MATH APPL, V161, P183, DOI 10.1007/978-1-4939-7005-6_6; Kontorovich Leonid, 2007, MEASURE CONCENTRATIO; Kontorovich L, 2008, ANN PROBAB, V36, P2126, DOI 10.1214/07-AOP384; Kulske C, 2003, COMMUN MATH PHYS, V239, P29, DOI 10.1007/s00220-003-0841-5; Kutin S, 2002, P 18 C UNC ART INT, P275; Lampert Christoph H, 2018, ARXIV1811O1404; Lozano A.C., 2006, ADV NEURAL INFORM PR, P819; McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148, DOI DOI 10.1017/CBO9781107359949.008; Meir R, 2000, MACH LEARN, V39, P5, DOI 10.1023/A:1007602715810; Mohri M, 2008, ADV NEURAL INFORM PR, P1025; Mohri M., 2009, ADV NEURAL INFORM PR, V21, P1097; Mohri M, 2010, J MACH LEARN RES, V11, P789; Mou W., 2017, ARXIV170705947; Mou WL, 2018, PR MACH LEARN RES, V80; Ralaivola L, 2015, PR MACH LEARN RES, V37, P2436; Ralaivola L, 2010, J MACH LEARN RES, V11, P1927; ROGERS WH, 1978, ANN STAT, V6, P506, DOI 10.1214/aos/1176344196; ROSENBLATT M, 1956, P NATL ACAD SCI USA, V42, P43, DOI 10.1073/pnas.42.1.43; Rucinski A., 2011, RANDOM GRAPHS, V45; SEN PK, 1968, ANN MATH STAT, V39, P1724, DOI 10.1214/aoms/1177698155; Steinwart I., 2009, ADV NEURAL INF PROCE, V22, P1768; Usunier Nicolas, 2006, PROC 18 INT C ADV NE, P1369; Volkonskii V.A., 1959, THEOR PROBABILITY AP, V4, P178, DOI [10.1137/1104015, DOI 10.1137/1104015]; Wang Y, 2017, INT C ALG LEARN THEO, P641; Wang YX, 2018, AAAI CONF ARTIF INTE, P5561; Yi Hao, 2018, ADV NEURAL INFORM PR, P646; Zhang Rui (Ray), 2019, ARXIV190902330	53	3	3	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902051
C	Zhao, YR; Gao, XT; Bates, D; Mullins, R; Xu, CZ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhao, Yiren; Gao, Xitong; Bates, Daniel; Mullins, Robert; Xu, Cheng-Zhong			Focused Quantization for Sparse CNNs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep convolutional neural networks (CNNs) are powerful tools for a wide range of vision tasks, but the enormous amount of memory and compute resources required by CNNs pose a challenge in deploying them on constrained devices. Existing compression techniques, while excelling at reducing model sizes, struggle to be computationally friendly. In this paper, we attend to the statistical properties of sparse CNNs and present focused quantization, a novel quantization strategy based on power-of-two values, which exploits the weight distributions after fine-grained pruning. The proposed method dynamically discovers the most effective numerical representation for weights in layers with varying sparsities, significantly reducing model sizes. Multiplications in quantized CNNs are replaced with much cheaper bit-shift operations for efficient inference. Coupled with lossless encoding, we built a compression pipeline that provides CNNs with high compression ratios (CR), low computation cost and minimal loss in accuracy. In ResNet-50, we achieved a 18.08x CR with only 0.24% loss in top-5 accuracy, outperforming existing compression methods. We fully compressed a ResNet-18 and found that it is not only higher in CR and top-5 accuracy, but also more hardware efficient as it requires fewer logic gates to implement when compared to other state-of-the-art quantization methods assuming the same throughput.	[Zhao, Yiren; Bates, Daniel; Mullins, Robert] Univ Cambridge, Cambridge, England; [Gao, Xitong] Shenzhen Inst Adv Technol, Shenzhen, Peoples R China; [Xu, Cheng-Zhong] Univ Macau, Zhuhai, Peoples R China	University of Cambridge; Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS; University of Macau	Zhao, YR (corresponding author), Univ Cambridge, Cambridge, England.; Gao, XT (corresponding author), Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.	yiren.zhao@cl.cam.ac.uk; xt.gao@siat.ac.cn	Gao, Xitong/HHR-8568-2022; XU, CHENGZHONG/AAX-1707-2020	Gao, Xitong/0000-0002-2063-2051; XU, CHENGZHONG/0000-0001-9480-0356	National Key R&D Program of China [2018YFB1004804]; National Natural Science Foundation of China [61806192]; EPSRC	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work is supported in part by the National Key R&D Program of China (No. 2018YFB1004804), the National Natural Science Foundation of China (No. 61806192). We thank EPSRC for providing Yiren Zhao his doctoral scholarship.	DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dubey Abhimanyu, 2018, P EUR C COMP VIS ECC; Gao X., 2019, ICLR 2019; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Guo YW, 2016, ADV NEUR IN, V29; Han S., 2016, P 4 INT C LEARN REPR, P1; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306; Howard A. G., 2017, MOBILENETS EFFICIENT; Hubara I, 2016, ADV NEUR IN, V29; Krizhevsky A., 2014, CIFAR 10 CIFAR 100 D; Leng C, 2018, AAAI CONF ARTIF INTE, P3466; Lin Xiaofan, 2017, NEURIPS, V1, P2; Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681; Miyashita Daisuke, 2016, ARXIV160301025; Nikolic Milos, 2019, 2019 IEEE INT S PERF; Polino A., 2018, ARXIV180205668; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Tung F, 2018, PROC CVPR IEEE, P7873, DOI 10.1109/CVPR.2018.00821; Zhang DQ, 2018, LECT NOTES COMPUT SC, V11212, P373, DOI 10.1007/978-3-030-01237-3_23; Zhao YR, 2018, PROCEEDINGS OF THE 2018 INTERNATIONAL WORKSHOP ON EMBEDDED AND MOBILE DEEP LEARNING (EMDL '18), P25, DOI 10.1145/3212725.3212726; Zhao Yiren, 2019, 2019 INT C FIELD PRO; Zhou A, 2017, INCREMENTAL NETWORK; Zhu Chenzhuo, 2017, ICLR	27	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305056
C	Acharya, J; Bhattacharyya, A; Daskalakis, C; Kandasamy, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Acharya, Jayadev; Bhattacharyya, Arnab; Daskalakis, Constantinos; Kandasamy, Saravanan			Learning and Testing Causal Models with Interventions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DISTRIBUTIONS; SELECTION; CIRCUITS	We consider testing and learning problems on causal Bayesian networks as defined by Pearl [Pea09]. Given a causal Bayesian network M on a graph with n discrete variables and bounded in-degree and bounded "confounded components", we show that O(log n) interventions on an unknown causal Bayesian network X on the same graph, and O(n/epsilon(2)) samples per intervention, suffice to efficiently distinguish whether X = M or whether there exists some intervention under which X and M are farther than epsilon in total variation distance. We also obtain sample/time/intervention efficient algorithms for: (i) testing the identity of two unknown causal Bayesian networks on the same graph; and (ii) learning a causal Bayesian network on a given graph. Although our algorithms are non-adaptive, we show that adaptivity does not help in general: Omega(log n) interventions are necessary for testing the identity of two unknown causal Bayesian networks on the same graph, even adaptively. Our algorithms are enabled by a new subadditivity inequality for the squared Hellinger distance between two causal Bayesian networks.	[Acharya, Jayadev] Cornell Univ, Sch ECE, Ithaca, NY 14853 USA; [Bhattacharyya, Arnab] Natl Univ Singapore, Singapore, Singapore; [Bhattacharyya, Arnab] Indian Inst Sci, Bengaluru, Karnataka, India; [Daskalakis, Constantinos] MIT, EECS, Cambridge, MA 02139 USA; [Kandasamy, Saravanan] Tata Inst Fundamental Res, STCS, Mumbai, Maharashtra, India	Cornell University; National University of Singapore; Indian Institute of Science (IISC) - Bangalore; Massachusetts Institute of Technology (MIT); Tata Institute of Fundamental Research (TIFR)	Acharya, J (corresponding author), Cornell Univ, Sch ECE, Ithaca, NY 14853 USA.	acharya@cornell.edu; arnabb@iisc.ac.in; costis@csail.mit.edu; saravan.tuty@gmail.com			Google India	Google India(Google Incorporated)	We would like to thank Vasant Honavar who told us about the problems considered here and for several helpful discussions that were essential for us to complete this work. We acknowledge the support of Google India and NeurIPS in the form of an International Travel Grant, which enabled Saravanan Kandasamy to attend the conference.	Abbeel P, 2006, J MACH LEARN RES, V7, P1743; Acharya Jayadev, 2015, ADV NEURAL INFORM PR, P3591; Ali R Ayesha, 2005, 21 C UNC ART INT UAI; ALON N, 1992, RANDOM STRUCT ALGOR, V3, P289, DOI 10.1002/rsa.3240030308; Alon N., 2004, PROBABILISTIC METHOD; Angluin D, 2008, MACH LEARN, V72, P113, DOI 10.1007/s10994-008-5048-8; Angluin D, 2009, J COMPUT SYST SCI, V75, P60, DOI 10.1016/j.jcss.2008.07.004; [Anonymous], 2016, ABS160607384 CORR; Bareinboim E., 2013, P 16 INT C ART INT S, V31, P135; Bareinboim E., 2012, P 26 AAAI C ARTIFICI, V26, P698; Batu T, 2000, ANN IEEE SYMP FOUND, P259, DOI 10.1109/SFCS.2000.892113; Bhattacharyya A., 2011, P ITCS, P239; BOLLEN KA, 1992, SOCIOL METHOD RES, V21, P123, DOI 10.1177/0049124192021002001; Bresler G, 2015, ACM S THEORY COMPUT, P771, DOI 10.1145/2746539.2746631; Canonne Clement L., 2015, ELECT C COMPUTATIONA, V22; Canonne Clement L., 2017, P 30 C LEARN THEOR C, P370; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Cover TM, 2006, ELEMENTS INFORM THEO; Daskalakis C., 2017, ADV NEURAL INFORM PR; Daskalakis C, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2747; Daskalakis Constantinos, 2018, P 29 ANN ACM SIAM S; Daskalakis Constantinos, 2017, P MACHINE LEARNING R, V65, P1; Devroye L., 2012, COMBINATORIAL METHOD; Dixit A, 2016, CELL, V167, P1853, DOI 10.1016/j.cell.2016.11.038; Eberhardt F, 2007, THESIS CARNEGIE MELL, P93; Eberhardt F., 2005, P 21 C UNC ART INT, P178; Even G., 1992, Proceedings of the Twenty-Fourth Annual ACM Symposium on the Theory of Computing, P10, DOI 10.1145/129712.129714; Fisher R. A, 1925, STAT METHODS RES WOR; Glymour C., 1999, COMPUTATION CAUSATIO; Goldreich O., 2017, INTRO PROPERTY TESTI; Goldreich Oded, 2000, ELECT C COMPUT COMPL, V20; Haavelmo T, 1943, ECONOMETRICA, V11, P1, DOI 10.2307/1905714; Hauser A, 2012, J MACH LEARN RES, V13, P2409; Hauser Alain, 2012, P 6 EUR WORKSH PROB, V119; Hoyer P. O., 2009, ADV NEURAL INFORM PR, V21, P689; Hyttinen A, 2013, J MACH LEARN RES, V14, P3041; Janzing D., 2010, JMLR WORKSHOP C P, V6, P1; Janzing D, 2012, ARTIF INTELL, V182, P1, DOI 10.1016/j.artint.2012.01.002; Kang C., 2006, P 17 ANN C UNC ART I, P233; Klivans A., 2017, ARXIV170606274; Kocaoglu M., 2017, ADV NEURAL INFORM PR, P7021, DOI [10.5555/3295222.3295445, DOI 10.5555/3295222.3295445]; Kocaoglu M, 2017, PR MACH LEARN RES, V70; Koller D., 2009, PROBABILISTIC GRAPHI; Lee S, 2013, P 27 AAAI C ART INT; Lehmann E. L., 2006, SPRINGER TEXTS STAT; Macosko EZ, 2015, CELL, V161, P1202, DOI 10.1016/j.cell.2015.05.002; Meganck Stijn, 2006, P 3 EUR WORKSH PROB; Moser RA, 2010, J ACM, V57, DOI 10.1145/1667053.1667060; Moser RA, 2009, ACM S THEORY COMPUT, P343; NAOR J, 1990, PROCEEDINGS OF THE TWENTY SECOND ANNUAL ACM SYMPOSIUM ON THEORY OF COMPUTING, P213, DOI 10.1145/100216.100244; Neapolitan R.E, 2004, INT J DATA MIN BIOIN, V38, DOI DOI 10.1016/J.JBI.2010.03.005; Pearl J, 1995, BIOMETRIKA, V82, P669, DOI 10.1093/biomet/82.4.669; Pearl J., 1995, STUD LOG FDN MATH, V134, P789, DOI DOI 10.1016/S0049-237X(06)80074-1; Pearl J, 2009, CAUSALITY, DOI 10.1017/CBO9780511803161; PEARL J., 2011, P 25 AAAI C ART INT, P247; Peters J, 2011, IEEE T PATTERN ANAL, V33, P2436, DOI 10.1109/TPAMI.2011.71; Reyzin Lev, 2009, ACTIVE LEARNING INTE; Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809; Scheines Richard, 2008, CAUSAL STRUCTURE SEA; Sen R., 2017, ADV NEURAL INFORM PR, V30, P2955; Shanmugam K., 2015, ADV NEURAL INFORM PR, P3195; Shpitser I, 2008, J MACH LEARN RES, V9, P1941; SPIEGELHALTER DJ, 1993, STAT SCI, V8, P219, DOI 10.1214/ss/1177010888; Spirtes P., 1999, ALGORITHM CAUSAL INF; Spirtes P., 2000, CAUSATION PREDICTION; Srivastava Piyush, 2016, UNCERTAINTY ARTIFICI; Tian J., 2002, STUDIES CAUSAL REASO; Tian J., 2002, P 18 C UNC ART INT, P519; Verma T., 1992, P 8 C UNC ART INT, P323; Verma T., 1990, MACH INTELL PATTERN, V9, P69, DOI [10.1016/B978-0-444-88650-7.50011-1, DOI 10.1016/B978-0-444-88650-7.50011-1, 10.1016/B978-0-444-88650-7.50011-1.5, DOI 10.1016/B978-0-444-88650-7.50011-1.5]; Vuffray M., 2016, ADV NEURAL INFORM PR, P2595; Wang Yuhao, 2017, ADV NEURAL INFORM PR, P5822; Wright S, 1920, J AGRIC RES, V20, P0557; Yang Karren, 2018, ARXIV180206310; Zhang JJ, 2008, ARTIF INTELL, V172, P1873, DOI 10.1016/j.artint.2008.08.001; Zhang K., 2012, ARXIV12023775	78	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													14	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004005
C	Adler, J; Lunz, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Adler, Jonas; Lunz, Sebastian			Banach Wasserstein GAN	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Wasserstein Generative Adversarial Networks (WGANs) can be used to generate realistic samples from complicated image distributions. The Wasserstein metric used in WGANs is based on a notion of distance between individual images, which induces a notion of distance between probability distributions of images. So far the community has considered l(2) as the underlying distance. We generalize the theory of WGAN with gradient penalty to Banach spaces, allowing practitioners to select the features to emphasize in the generator. We further discuss the effect of some particular choices of underlying norms, focusing on Sobolev norms. Finally, we demonstrate a boost in performance for an appropriate choice of norm on CIFAR-10 and CelebA.	[Adler, Jonas] KTH Royal Inst Technol, Dept Math, Res & Phys, Elekta, Stockholm, Sweden; [Lunz, Sebastian] Univ Cambridge, Dept Appl Math & Theoret Phys, Cambridge, England	Elekta; Royal Institute of Technology; University of Cambridge	Adler, J (corresponding author), KTH Royal Inst Technol, Dept Math, Res & Phys, Elekta, Stockholm, Sweden.	jonasadl@kth.se; lunz@math.cam.ac.uk			Swedish Foundation of Strategic Research [AM13-0049, ID14-0055]; Elekta; EPSRC [EP/L016516/1]; Cambridge Centre for Analysis; Confab Capital Institute for the Mathematics of Information	Swedish Foundation of Strategic Research(Swedish Foundation for Strategic Research); Elekta; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Cambridge Centre for Analysis; Confab Capital Institute for the Mathematics of Information	The work by J.A. was supported by the Swedish Foundation of Strategic Research grants AM13-0049, ID14-0055 and Elekta. The work by S.L. was supported by the EPSRC grant EP/L016516/1 for the University of Cambridge Centre for Doctoral Training, and the Cambridge Centre for Analysis. We also acknowledge the support of the Confab Capital Institute for the Mathematics of Information.	[Anonymous], 2016, P INT C LEARNING REP; Arjovsky M., 2017, INT C MACH LEARN ICM; Arjovsky Martin, 2017, INT C LEARN REPR ICL; Brezis H., 2010, FUNCTIONAL ANAL SOBO; Chan T. F., 2005, IMAGE PROCESSING ANA, V94; Deza M. M., 2009, ENCY DISTANCES; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS, P1; Heusel M., 2017, 31 C NEUR INF PROC S, P6626; Karras Tero, 2018, INT C LEARN REPR ICL; Kingma DP, 2015, INT C LEARN REPR ICL; Liu Shuang, 2017, APPROXIMATION CONVER; Loshchilov Ilya, 2017, INT C LEARN REPR ICL; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Miyato T., 2018, 6 INT C LEARN REPR I; Petzka H., 2018, P INT C LEARN REPR; Seward Calvin, 2018, FIRST ORDER GENERATI; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wei Xiang, 2018, P INT C LEARN REPR, P3; Zhao Junbo Jake, 2017, INT C LEARN REPR ICL	21	3	3	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001031
C	Ambrogioni, L; Guclu, U; Gucluturk, Y; Hinne, M; Maris, E; van Gerven, MAJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ambrogioni, Luca; Guclu, Umut; Gucluturk, Yagmur; Hinne, Max; Maris, Eric; van Gerven, Marcel A. J.			Wasserstein Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					This paper introduces Wasserstein variational inference, a new form of approximate Bayesian inference based on optimal transport theory. Wasserstein variational inference uses a new family of divergences that includes both f-divergences and the Wasserstein distance as special cases. The gradients of the Wasserstein variational loss are obtained by backpropagating through the Sinkhorn iterations. This technique results in a very stable likelihood-free training method that can be used with implicit distributions and probabilistic programs. Using the Wasserstein variational inference framework, we introduce several new forms of autoencoders and test their robustness and performance against existing variational autoencoding techniques.	[Ambrogioni, Luca; Guclu, Umut; Gucluturk, Yagmur; Maris, Eric; van Gerven, Marcel A. J.] Radboud Univ Nijmegen, Nijmegen, Netherlands; [Hinne, Max] Univ Amsterdam, Amsterdam, Netherlands	Radboud University Nijmegen; University of Amsterdam	Ambrogioni, L (corresponding author), Radboud Univ Nijmegen, Nijmegen, Netherlands.	l.ambrogioni@donders.ru.nl; u.guclu@donders.ru.nl; y.gucluturk@donders.ru.nl; m.hinne@uva.nl; e.maris@donders.ru.nl; m.vangerven@donders.ru.nl	Hinne, Max/ABD-1486-2021; Güçlü, Umut/AAX-6105-2020	Hinne, Max/0000-0002-9279-6725; Güçlü, Umut/0000-0003-4753-159X				Ambrogioni  L., 2018, ARXIV180511542; Arjovsky Mart<prime>in, 2017, P 5 INT C LEARN REPR; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bamler R, 2017, ADV NEUR IN, V30; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Burago  D., 2001, GRADUATE STUDIEMAT, V33; Cuturi M, 2018, ARXIV180300567; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Dai Jifeng, 2016, ADV NEURAL INFORM PR, P379, DOI DOI 10.1016/J.JPOWSOUR.2007.02.075; Dieng A. B., 2017, ADV NEURAL INFORM PR, P2732; Dumoulin Vincent, 2017, ICLR; Fouskakis D, 2002, INT STAT REV, V70, P315, DOI 10.2307/1403861; Genevay A, 2018, PR MACH LEARN RES, V84; Gulrajani I, 2017, P NIPS 2017; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Huszar F., 2017, ARXIV170208235; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kucukelbir A, 2017, J MACH LEARN RES, V18, P1; Makhzani A., 2015, ARXIV151105644; Mescheder L, 2017, PR MACH LEARN RES, V70; Montavon  G., 2016, ADV NEURAL INFORM PR; Ranganath R., 2014, ARTIFICIAL INTELLIGE, P814; Ranganath R, 2016, ADV NEUR IN, V29; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; SINKHORN R, 1967, PAC J MATH, V21, P343, DOI 10.2140/pjm.1967.21.343; Tolstikhin Ilya, 2018, ICLR 2018; Tran D, 2017, ADV NEUR IN, V30; Tran Dustin, 2016, ARXIV161009787; Villani C., 2003, TOPICS OPTIMAL TRANS, V58; Weed J, 2017, ARXIV170700087; Zhang C., 2017, ARXIV171105597	32	3	3	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302048
C	Bafna, M; Murtagh, J; Vyas, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bafna, Mitali; Murtagh, Jack; Vyas, Nikhil			Thwarting Adversarial Examples: An L-0-Robust Sparse Fourier Transform	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We give a new algorithm for approximating the Discrete Fourier transform of an approximately sparse signal that has been corrupted by worst-case L-0 noise, namely a bounded number of coordinates of the signal have been corrupted arbitrarily. Our techniques generalize to a wide range of linear transformations that are used in data analysis such as the Discrete Cosine and Sine transforms, the Hadamard transform, and their high-dimensional analogs. We use our algorithm to successfully defend against well known L-0 adversaries in the setting of image classification. We give experimental results on the Jacobian-based Saliency Map Attack (JSMA) and the Carlini Wagner (CW) L-0 attack on the MNIST and Fashion-MNIST datasets as well as the Adversarial Patch on the ImageNet dataset.	[Bafna, Mitali; Murtagh, Jack] Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA; [Vyas, Nikhil] MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA	Harvard University; Massachusetts Institute of Technology (MIT)	Bafna, M (corresponding author), Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.	mitalibafna@g.harvard.edu; jmurtagh@g.harvard.edu; nikhilv@mit.edu			NSF [CCF 1715187, CNS-1565387, CCF-1552651]; Akamai Presidential Fellowship	NSF(National Science Foundation (NSF)); Akamai Presidential Fellowship	Mitali Bafna was supported by NSF Grant CCF 1715187. Jack Murtagh was supported by NSF grant CNS-1565387. Nikhil Vyas was supported by an Akamai Presidential Fellowship and NSF Grant CCF-1552651. We would like to thank Yaron Singer and Adam Breuer for helpful feedback and encouragement in the early stages of this work. We also want to thank Thibaut Horel for valuable comments on the manuscript. Thanks also to the reviewers for helpful remarks.	Backurs A, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2215; Baraniuk RG, 2010, IEEE T INFORM THEORY, V56, P1982, DOI 10.1109/TIT.2010.2040894; Blumensath Thomas, 2008, CORR; Brown T. B., 2017, CORR; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Carlini N., 2016, CORR; Evtimov Ivan., 2017, ABS170708945 CORR; Goodfellow I. J., 2015, P ICLR; Hassanieh H., 2012, P 44 ANN ACM S THEOR, P563; Hassanieh H., 2012, P 23 ANN ACM SIAM S, P1183; He K., 2015, CORR; Hegde C, 2015, IEEE T INFORM THEORY, V61, P5129, DOI 10.1109/TIT.2015.2457939; Hegde C, 2014, LECT NOTES COMPUT SC, V8572, P588; Indyk P., 2014, P 25 ACM SIAM S DISC, P480; Madry Aleksander, 2017, CORR; Moosavi-Dezfooli S.-M., 2015, CVPR; NEEDELL D, 2008, ARXIV08032392; Papernot N., 2015, CORR; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392; Szegedy C., 2013, CORR; Tramer F., 2017, ARXIV170507204; Xiao H., 2017, CORR; Yann L., 1998, MNIST DATABASE HANDW, P1	25	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004061
C	Bravo, M; Leslie, D; Mertikopoulos, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bravo, Mario; Leslie, David; Mertikopoulos, Panayotis			Bandit Learning in Concave N-Person Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				OPTIMIZATION; DYNAMICS	This paper examines the long-run behavior of learning with bandit feedback in non-cooperative concave games. The bandit framework accounts for extremely low-information environments where the agents may not even know they are playing a game; as such, the agents' most sensible choice in this setting would be to employ a no-regret learning algorithm. In general, this does not mean that the players' behavior stabilizes in the long run: no-regret learning may lead to cycles, even with perfect gradient information. However, if a standard monotonicity condition is satisfied, our analysis shows that no-regret learning based on mirror descent with bandit feedback converges to Nash equilibrium with probability 1. We also derive an upper bound for the convergence rate of the process that nearly matches the best attainable rate for single-agent bandit stochastic optimization.	[Bravo, Mario] Univ Santiago Chile, Dept Matemat & Ciencia Computac, Santiago, Chile; [Leslie, David] Univ Lancaster, Lancaster, England; [Leslie, David] PROWLER Io, Cambridge, England; [Mertikopoulos, Panayotis] Univ Grenoble Alpes, CNRS, INRIA, Grenoble INP,LIG, F-38000 Grenoble, France	Universidad de Santiago de Chile; Lancaster University; Inria; UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS)	Bravo, M (corresponding author), Univ Santiago Chile, Dept Matemat & Ciencia Computac, Santiago, Chile.	mario.bravo.g@usach.cl; d.leslie@lancaster.ac.uk; panayotis.mertikopoulos@imag.fr		Bravo, Mario/0000-0002-4312-9600	FONDECYT [11151003]; Huawei HIRP flagship grant ULTRON; French National Research Agency (ANR) grant ORACLESS [ANR-16-CE33-0004-01]; ECOS project [C15E03]	FONDECYT(Comision Nacional de Investigacion Cientifica y Tecnologica (CONICYT)CONICYT FONDECYT); Huawei HIRP flagship grant ULTRON(Huawei Technologies); French National Research Agency (ANR) grant ORACLESS(French National Research Agency (ANR)); ECOS project	M. Bravo gratefully acknowledges the support provided by FONDECYT grant 11151003. P. Mertikopoulos was partially supported by the Huawei HIRP flagship grant ULTRON, and the French National Research Agency (ANR) grant ORACLESS (ANR-16-CE33-0004-01). Part of this work was carried out with financial support by the ECOS project C15E03.	Agarwal Alekh, 2010, COLT 10 P 23 ANN C L; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; AUER P, 1995, P 36 ANN S FDN COMP; Bauschke HH, 2017, CONVEX ANAL MONOTONE, DOI 10.1007/978-3-319-48311-5; Benaim M, 1999, LECT NOTES MATH, V1709, P1; Bervoets S., 2018, LEARNING MINIMAL INF; Chen G, 1993, SIAM J OPTIMIZ, V3, P538, DOI 10.1137/0803026; Cohen Johanne, 2017, NIPS 17 P 31 INT C N; D'Oro S, 2015, IEEE T WIREL COMMUN, V14, P6536, DOI 10.1109/TWC.2015.2456063; DEBREU G, 1952, P NATL ACAD SCI USA, V38, P886, DOI 10.1073/pnas.38.10.886; Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385; Foster D. J., 2016, ADV NEURAL INFORM PR, P4727; Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Kleinberg Robert D., 2004, NIPS 04 P 18 ANN C N; Mertikopoulos P, 2017, IEEE T SIGNAL PROCES, V65, P2277, DOI 10.1109/TSP.2017.2656847; Mertikopoulos P, 2016, IEEE J SEL AREA COMM, V34, P743, DOI 10.1109/JSAC.2016.2544600; Mertikopoulos Panayotis, 2018, MATH PROGRAM; Mertikopoulos Panayotis, 2018, SODA 18 P 29 ANN ACM; Mertikopoulos Panayotis, 2018, OPTIMISTIC MIRROR DE; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x; Orda A., 1993, IEEE ACM T NETWORK, V1, P614; Palaiopanos Gerasimos, 2017, ADV NEURAL INFORM PR, P5872; Perkins S, 2014, J ECON THEORY, V152, P179, DOI 10.1016/j.jet.2014.04.008; Perkins S, 2017, IEEE T AUTOMAT CONTR, V62, P379, DOI 10.1109/TAC.2015.2511930; ROSEN JB, 1965, ECONOMETRICA, V33, P520, DOI 10.2307/1911749; Scutari G, 2010, IEEE SIGNAL PROC MAG, V27, P35, DOI 10.1109/MSP.2010.936021; Shamir O., 2013, P COLT; Sorin S, 2016, J DYN GAMES, V3, P101, DOI 10.3934/jdg.2016005; Spall JC, 1997, AUTOMATICA, V33, P109, DOI 10.1016/S0005-1098(96)00149-5; Syrgkanis Vasilis, 2015, ADV NEURAL INFORM PR, P2989; Viossat Y, 2013, J ECON THEORY, V148, P825, DOI 10.1016/j.jet.2012.07.003; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	35	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000019
C	Carratino, L; Rudi, A; Rosasco, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Carratino, Luigi; Rudi, Alessandro; Rosasco, Lorenzo			Learning with SGD and Random Features	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				APPROXIMATION	Sketching and stochastic gradient methods are arguably the most common techniques to derive efficient large scale learning algorithms. In this paper, we investigate their application in the context of nonparametric statistical learning. More precisely, we study the estimator defined by stochastic gradient with mini batches and random features. The latter can be seen as form of nonlinear sketching and used to define approximate kernel methods. The considered estimator is not explicitly penalized/constrained and regularization is implicit. Indeed, our study highlights how different parameters, such as number of features, iterations, step-size and mini-batch size control the learning properties of the solutions. We do this by deriving optimal finite sample bounds, under standard assumptions. The obtained results are corroborated and illustrated by numerical experiments.	[Carratino, Luigi; Rosasco, Lorenzo] Univ Genoa, Genoa, Italy; [Rudi, Alessandro] Ecole Normale Super, INRIA, Sierra Project Team, Paris, France; [Rosasco, Lorenzo] IIT, LCSL, Genoa, Italy; [Rosasco, Lorenzo] MIT, Cambridge, MA 02139 USA	University of Genoa; Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Istituto Italiano di Tecnologia - IIT; Massachusetts Institute of Technology (MIT)	Carratino, L (corresponding author), Univ Genoa, Genoa, Italy.	luigi.carratino@dibris.unige.it		Carratino, Luigi/0000-0001-9947-9944	Center for Brains, Minds and Machines (CBMM); NSF STC [CCF-1231216]; Italian Institute of Technology; AFOSR [FA9550-17-1-0390, BAA-AFRL-AFOSR-2016-0007]; EU [MADS -DLV-777826]; European Research Council [SEQUOIA 724063]	Center for Brains, Minds and Machines (CBMM); NSF STC(National Science Foundation (NSF)); Italian Institute of Technology(Istituto Italiano di Tecnologia - IIT); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); EU(European Commission); European Research Council(European Research Council (ERC)European Commission)	This material is based upon work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216, and the Italian Institute of Technology. We gratefully acknowledge the support of NVIDIA Corporation for the donation of the Titan Xp GPUs and the Tesla k40 GPU used for this research. L. R. acknowledges the support of the AFOSR projects FA9550-17-1-0390 and BAA-AFRL-AFOSR-2016-0007 (European Office of Aerospace Research and Development), and the EU H2020-MSCA-RISE project NoMADS -DLV-777826. A. R. acknowledges the support of the European Research Council (grant SEQUOIA 724063).	Agarwal A., 2012, P ADV NEUR INF PROC, P1538, DOI DOI 10.1109/CISS.2014.6814157; Alaoui A., 2015, P 28 INT C NEURAL IN, P775; [Anonymous], 2013, P INT C MACH LEARN; [Anonymous], 2015, ADV NEURAL INFORM PR; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Avron H., 2013, ADV NEURAL INFORM PR, P2994; BACH F, 2017, J MACHINE LEARNING R, V18; Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Camoriano R, 2016, JMLR WORKSH CONF PRO, V51, P1403; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Cho Y., 2009, NIPS, P342; Ciliberto C., 2017, ADV NEURAL INFORM PR, P1986; Ciliberto Carlo, 2018, ARXIV180602402; Ciliberto Carlo, 2016, ADV NEURAL INFORM PR, V29, P4412; Cucker F, 2002, B AM MATH SOC, V39, P1; Dai B., 2014, NIPS; Dekel O, 2012, J MACH LEARN RES, V13, P165; Devroye L, 2013, PROBABILISTIC THEORY, V31; Dieuleveut A., 2017, J MACHINE LEARNING R, V18, P3520; Dieuleveut A, 2016, ANN STAT, V44, P1363, DOI 10.1214/15-AOS1391; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Hamid R, 2014, PR MACH LEARN RES, V32, P19; Lin J., 2017, ARXIV170700577; Lin J., 2018, APPL COMPUTATIONAL H, DOI 10.1016/j.acha.2018.09.009.; LIN JH, 2017, J MACHINE LEARNING R, V18; Lin Junhong, 2017, ARXIV171007797; Orabona F, 2014, ADV NEUR IN, V27; Osokin Anton, 2017, ADV NEURAL INFORM PR, P302; Pillaud-Vivien L., 2018, P MACH LEARN RES, V75, P250; Pillaud-Vivien Loucas, 2018, ADV NEURAL INFORM PR, P8114; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Rudi A., 2015, ADV NEURAL INFORM PR, V28, P1657, DOI DOI 10.5555/2969239.2969424; Rudi A., 2018, ADV NEURAL INFORM PR, P5677; Rudi A., 2017, ADV NEURAL INFORM PR, V30, P3891; Rudi A., 2017, ADV NEURAL INFORM PR, V30, P3215; Rudi Alessandro, 2013, ADV NEURAL INFORM PR, P2067; Scholkopf B., 2001, LEARNING KERNELS SUP; Shamir O., 2013, P INT C MACH LEARN A, P71; Smale S, 2003, ANAL APPL, V1, P17, DOI 10.1142/S0219530503000089; Smola Alex J, 2000, SPARSE GREEDY MATRIX; Sriperumbudur Bharath, 2015, ADV NEURAL INFORM PR, P1144; Steinwart I., 2008, SUPPORT VECTOR MACHI; Steinwart I., 2009, P C LEARN THEOR COLT; Tu Stephen, 2016, ARXIV160205310; Williams CKI, 2001, ADV NEUR IN, V13, P682; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060; Yang Yun, 2015, ARXIV150106195; Yu Felix X, 2016, ADV NEURAL INFORM PR, V29, P1975	53	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004072
C	Cheng, Y; Diakonikolas, I; Kane, DM; Stewart, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Cheng, Yu; Diakonikolas, Ilias; Kane, Daniel M.; Stewart, Alistair			Robust Learning of Fixed-Structure Bayesian Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MODELS; SUM	We investigate the problem of learning Bayesian networks in a robust model where an epsilon-fraction of the samples are adversarially corrupted. In this work, we study the fully observable discrete case where the structure of the network is given. Even in this basic setting, previous learning algorithms either run in exponential time or lose dimension-dependent factors in their error guarantees. We provide the first computationally efficient robust learning algorithm for this problem with dimension-independent error guarantees. Our algorithm has near-optimal sample complexity, runs in polynomial time, and achieves error that scales nearly-linearly with the fraction of adversarially corrupted samples. Finally, we show on both synthetic and semi-synthetic data that our algorithm performs well in practice.	[Cheng, Yu] Duke Univ, Dept Comp Sci, Durham, NC 27708 USA; [Diakonikolas, Ilias; Stewart, Alistair] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA; [Kane, Daniel M.] Univ Calif San Diego, Dept Comp Sci & Engn, La Jolla, CA 92093 USA	Duke University; University of Southern California; University of California System; University of California San Diego	Cheng, Y (corresponding author), Duke Univ, Dept Comp Sci, Durham, NC 27708 USA.	yucheng@cs.duke.edu; ilias.diakonikolas@gmail.com; dakane@ucsd.edu; stewart.al@gmail.com		Cheng, Yu/0000-0002-0019-2570	NSF [CCF-1527084, CCF-1535972, CCF-1637397, CCF-1704656, IIS-1447554]; NSF CAREER Award [CCF-1750140, CCF-1652862, CCF-1553288]; Sloan Research Fellowship	NSF(National Science Foundation (NSF)); NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Sloan Research Fellowship(Alfred P. Sloan Foundation)	We are grateful to Daniel Hsu for suggesting the model of Bayes nets, and for pointing us to [Das97]. Yu Cheng is supported in part by NSF CCF-1527084, CCF-1535972, CCF-1637397, CCF-1704656, IIS-1447554, and NSF CAREER Award CCF-1750140. Ilias Diakonikolas is supported by NSF CAREER Award CCF-1652862 and a Sloan Research Fellowship. Daniel Kane is supported by NSF CAREER Award CCF-1553288 and a Sloan Research Fellowship.	Abbeel P, 2006, J MACH LEARN RES, V7, P1743; Acharya J., 2016, P 33 INT C MACH LEAR, P2878; Acharya J, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1278; Anandkumar A., 2012, P NIPS, P1061; [Anonymous], [No title captured]; [Anonymous], [No title captured]; Balakrishnan S., 2017, C LEARN THEOR PMLR, P169; Beinlich IA, 1989, ALARM MONITORING SYS; BERNHOLT T, 2006, TECHNICAL REPORT; Bresler G, 2015, ACM S THEORY COMPUT, P771, DOI 10.1145/2746539.2746631; Bresler G, 2013, SIAM J COMPUT, V42, P563, DOI 10.1137/100796029; Bresler Guy, 2014, NIPS, P2852; Canonne Clement L., 2017, P 30 C LEARN THEOR C, P370; Chan SO, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P604, DOI 10.1145/2591796.2591848; Chan SO, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1380; Chan Siu-on, 2014, ADV NEURAL INFORM PR, P1844; Chen MJ, 2016, ELECTRON J STAT, V10, P3752, DOI 10.1214/16-EJS1216; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Daly R, 2011, KNOWL ENG REV, V26, P99, DOI 10.1017/S0269888910000251; Dasgupta S, 1997, MACH LEARN, V29, P165, DOI 10.1023/A:1007417612269; Daskalakis Constantinos, 2014, THEORY COMPUTING, V10, P535; Diakonikolas I., 2018, CORR; Diakonikolas I., 2018, P 29 ACM SIAM S DISC; Diakonikolas I, 2018, ARXIV180302815; Diakonikolas I., 2016, P 57 IEEE S FDN COMP; Diakonikolas I, 2018, ACM S THEORY COMPUT, P1061, DOI 10.1145/3188745.3188754; Diakonikolas I, 2018, ACM S THEORY COMPUT, P1047, DOI 10.1145/3188745.3188758; Diakonikolas I, 2017, PR MACH LEARN RES, V70; Diakonikolas I, 2017, ANN IEEE SYMP FOUND, P73, DOI 10.1109/FOCS.2017.16; Diakonikolas Ilias, 2018, P 31 C LEARNING THEO, P819; Hampel FR., 2011, WILEY SERIES PROBABI; Hardt M., 2013, C LEARN THEOR, P354; Hopkins SB, 2018, ACM S THEORY COMPUT, P1021, DOI 10.1145/3188745.3188748; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Huber PJ, 2009, WILEY SERIES PROBABI; Johnson D. S., 1978, Theoretical Computer Science, V6, P93, DOI 10.1016/0304-3975(78)90006-3; Klivans A., 2018, C LEARNING THEORY, P1420; Koller D., 2009, PROBABILISTIC GRAPHI; Kothari PK, 2018, ACM S THEORY COMPUT, P1035, DOI 10.1145/3188745.3188970; Lai K. A., 2016, P 57 IEEE S FDN COMP; Liu L., 2018, CORR; Neapolitan R. E., 2003, LEARNING BAYESIAN NE; Nielsen T.D., 2007, COMPSTAT, V2nd, DOI [DOI 10.1007/978-0-387-68282-2, DOI 10.1007/978-3-642-46890-2_1]; Prasad A., 2018, ARXIV180206485; Tukey J., 1975, P INT C MATH, V2; Wainwright M. J., 2006, ADV NEURAL INFORM PR, P1465; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001	48	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004080
C	Cohen-Addad, V; Kanade, V; Mallmann-Trenn, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Cohen-Addad, Vincent; Kanade, Varun; Mallmann-Trenn, Frederik			Clustering Redemption-Beyond the Impossibility of Kleinberg's Axioms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Kleinberg [20] stated three axioms that any clustering procedure should satisfy and showed there is no clustering procedure that simultaneously satisfies all three. One of these, called the consistency axiom, requires that when the data is modified in a helpful way, i.e. if points in the same cluster are made more similar and those in different ones made less similar, the algorithm should output the same clustering. To circumvent this impossibility result, research has focused on considering clustering procedures that have a clustering quality measure (or a cost) and showing that a modification of Kleinberg's axioms that takes cost into account lead to feasible clustering procedures. In this work, we take a different approach, based on the observation that the consistency axiom fails to be satisfied when the "correct" number of clusters changes. We modify this axiom by making use of cost functions to determine the correct number of clusters, and require that consistency holds only if the number of clusters remains unchanged. We show that single linkage satisfies the modified axioms, and if the input is well-clusterable, some popular procedures such as k-means also satisfy the axioms, taking a step towards explaining the success of these objective functions for guiding the design of algorithms.	[Cohen-Addad, Vincent] UPMC Univ Paris 06, Sorbonne Univ, CNRS, LIP6, Paris, France; [Kanade, Varun] Univ Oxford, Oxford, England; [Mallmann-Trenn, Frederik] MIT, Cambridge, MA 02139 USA	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite; University of Oxford; Massachusetts Institute of Technology (MIT)	Cohen-Addad, V (corresponding author), UPMC Univ Paris 06, Sorbonne Univ, CNRS, LIP6, Paris, France.	vincent.cohen-addad@lip6.fr; varunk@cs.ox.ac.uk; mallmann@mit.edu	Kanade, Varun/AAS-3434-2020	Kanade, Varun/0000-0002-2300-4819	Alan Turing Institute through the EPSRC [EP/N510129/1]; NSF [CCF-1461559, CCF-0939370, CCF-1810758]	Alan Turing Institute through the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); NSF(National Science Foundation (NSF))	This work was supported in part by the Alan Turing Institute through the EPSRC grant EP/N510129/1.; This work was supported in part by NSF Award Numbers CCF-1461559, CCF-0939370, and CCF-1810758.	Ackerman M., 2010, COLT, P270; Ackerman M., 2010, ADV NEURAL INFORM PR, V23, P10; Ackerman  M., 2012, THEORETICAL FDN CLUS; Ackerman Margareta, 2013, ARTIF INTELL, P66; Ackerman Margareta, 2012, AAAI; Alon N, 1997, SIAM J COMPUT, V26, P1733, DOI 10.1137/S0097539794270248; Alon N, 1998, RANDOM STRUCT ALGOR, V13, P457, DOI 10.1002/(SICI)1098-2418(199810/12)13:3/4<457::AID-RSA14>3.0.CO;2-W; Angelidakis H, 2017, ACM S THEORY COMPUT, P438, DOI 10.1145/3055399.3055487; Awasthi Pranjal, 2012, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques. Proceedings 15th International Workshop, APPROX 2012 and 16th International Workshop, RANDOM 2012, P37, DOI 10.1007/978-3-642-32512-0_4; Awasthi P, 2012, INFORM PROCESS LETT, V112, P49, DOI 10.1016/j.ipl.2011.10.006; Balcan MF, 2016, SIAM J COMPUT, V45, P102, DOI 10.1137/140981575; Ben-David S., 2009, ADV NEURAL INFORM PR, V21, P121; Ben-David  S., 2015, ARXIV150100437; Ben-David S, 2014, PR MACH LEARN RES, V32, P280; Bilu Y, 2012, COMB PROBAB COMPUT, V21, P643, DOI 10.1017/S0963548312000193; Daniely  A., 2012, ARXIV12054891; Dudoit S, 2002, GENOME BIOL, V3; Dutta  A., 2017, ARXIV171201241; Kleinberg J., 2002, P ADV NEUR INF PROC; Kumar A, 2010, ANN IEEE SYMP FOUND, P299, DOI 10.1109/FOCS.2010.35; McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929; Meila M., 2005, PROC INT C MACHINE L, P577, DOI DOI 10.1145/1102351.1102424; Ostrovsky R, 2006, ANN IEEE SYMP FOUND, P165; Puzicha J, 2000, PATTERN RECOGN, V33, P617, DOI 10.1016/S0031-3203(99)00076-X; Thorndike RL., 1953, PSYCHOMETRIKA, V18, P267, DOI [10.1007/BF02289263, DOI 10.1007/BF02289263]; Tibshirani R, 2001, J ROY STAT SOC B, V63, P411, DOI 10.1111/1467-9868.00293; van Laarhoven T, 2014, J MACH LEARN RES, V15, P193; Weisstein E. W., TREE MATHWORLD WOLFR; Williams  A., 2015, IS CLUSTERING MATH I; Zadeh R.B., 2009, P 25 C UNCERTAINTY A, P639	30	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003011
C	Cutkosky, A; Busa-Fekete, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Cutkosky, Ashok; Busa-Fekete, Robert			Distributed Stochastic Optimization via Adaptive SGD	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Stochastic convex optimization algorithms are the most popular way to train machine learning models on large-scale data. Scaling up the training process of these models is crucial, but the most popular algorithm, Stochastic Gradient Descent (SGD), is a serial method that is surprisingly hard to parallelize. In this paper, we propose an efficient distributed stochastic optimization method by combining adaptivity with variance reduction techniques. Our analysis yields a linear speedup in the number of machines, constant memory footprint, and only a logarithmic number of communication rounds. Critically, our approach is a black-box reduction that parallelizes any serial online learning algorithm, streamlining prior analysis and allowing us to leverage the significant progress that has been made in designing adaptive algorithms. In particular, we achieve optimal convergence rates without any prior knowledge of smoothness parameters, yielding a more robust algorithm that reduces the need for hyperparameter tuning. We implement our algorithm in the Spark distributed framework and exhibit dramatic performance gains on large-scale logistic regression problems.	[Cutkosky, Ashok] Stanford Univ, Stanford, CA 94305 USA; [Busa-Fekete, Robert] Yahoo Res, New York, NY USA; [Cutkosky, Ashok] Google, Mountain View, CA 94043 USA	Stanford University; Google Incorporated	Cutkosky, A (corresponding author), Stanford Univ, Stanford, CA 94305 USA.; Cutkosky, A (corresponding author), Google, Mountain View, CA 94043 USA.	cutkosky@google.com; busafekete@oath.com		Cutkosky, Ashok/0000-0002-3822-3029				Agarwal A, 2014, J MACH LEARN RES, V15, P1111; [Anonymous], 2016, ARXIV160306861; Beygelzimer Alina, 2018, ALGORITHMIC NOTIFICA; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Chen-Guang He, 2010, Proceedings 2010 IEEE Youth Conference on Information, Computing and Telecommunications (YC-ICT 2010), P351, DOI 10.1109/YCICT.2010.5713117; Cotter A., 2011, P NEURIPS GRAN SPAIN; Cutkosky A., 2017, C LEARN THEOR COLT, P643; Dekel O, 2012, J MACH LEARN RES, V13, P165; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Frostig R., 2015, P C LEARNING THEORY, P728; Harikandeh R., 2015, PROC NEURAL INF PROC, P2251; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Lei LH, 2017, ADV NEUR IN, V30; Lei Lihua, 2016, ARXIV160903261; Orabona F, 2014, ADV NEUR IN, V27; Orabona Francesco, 2016, ARXIV160101974; Ross S., 2013, ARXIV13056646; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Shamir Ohad, 2013, CORR; Wang J., 2017, CORR; Zhang Yuchen, 2015, CORR	22	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301086
C	Davidson, I; Gourru, A; Ravi, SS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Davidson, Ian; Gourru, Antoine; Ravi, S. S.			The Cluster Description Problem - Complexity Results, Formulations and Approximations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Consider the situation where you are given an existing k - way clustering pi. A challenge for explainable AI is to find a compact and distinct explanation of each cluster which in this paper is assumed to use instance-level descriptors/tags from a common dictionary. Since the descriptors/tags were not given to the clustering method, this is not a semi-supervised learning situation. We show that the feasibility problem of testing whether any distinct description (not necessarily the most compact) exists is generally intractable for just two clusters. This means that unless P = NP, there cannot exist an efficient algorithm for the cluster description problem. Hence, we explore ILP formulations for smaller problems and a relaxed but restricted setting that leads to a polynomial time algorithm for larger problems. We explore several extensions to the basic setting such as the ability to ignore some instances and composition constraints on the descriptions of the clusters. We show our formulation's usefulness on Twitter data where the communities were found using social connectivity (i.e. follower relation) but the explanation of the communities is based on behavioral properties of the nodes (i.e. hashtag usage) not available to the clustering method.	[Davidson, Ian] Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA; [Gourru, Antoine] Univ Lyon, ERIC, Lyon 2, Lyon, France; [Ravi, S. S.] Univ Virginia, Biocomplex Inst, Charlottesville, VA 22903 USA; [Davidson, Ian] Coll Lyon, Lyon, France; [Ravi, S. S.] SUNY Albany, Dept Comp Sci, Albany, NY 12222 USA	University of California System; University of California Davis; University of Virginia; State University of New York (SUNY) System; State University of New York (SUNY) Albany	Davidson, I (corresponding author), Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA.; Davidson, I (corresponding author), Coll Lyon, Lyon, France.	davidson@cs.ucdavis.edu; antoine.gourru@univ-lyon2.fr; ssravi0@gmail.com			Deep Graph Models of Functional Networks Grant [ONR-N000141812485]; Functional Network Discovery NSF Grant [IIS-1422218]; NSF DIBBS Grant [ACI-1443054]; NSF BIG DATA Grant [IIS-1633028]; NSF EAGER Grant [CMMI-1745207]	Deep Graph Models of Functional Networks Grant; Functional Network Discovery NSF Grant; NSF DIBBS Grant(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF BIG DATA Grant; NSF EAGER Grant(National Science Foundation (NSF))	Ian Davidson was an Institute of Advanced Studies Fellow at the Collegium de Lyon at the time of writing and was also supported by Deep Graph Models of Functional Networks Grant ONR-N000141812485 and Functional Network Discovery NSF Grant IIS-1422218. S. S. Ravi was supported in part by NSF DIBBS Grant ACI-1443054, NSF BIG DATA Grant IIS-1633028 and NSF EAGER Grant CMMI-1745207. The twitter data was provided by the ERIC lab at the University of Lyon 2 and was prepared by one of the authors (Antoine Gourru). Thanks to Yue Wu (UC Davis) for writing the MATLAB code available at www.cs.ucdavis.edu/<SUP>similar to</SUP>davidson/description-clustering/nips18_code.	Basu S, 2009, CH CRC DATA MIN KNOW, P1; Blondel VD, 2008, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2008/10/P10008; Chabert M, 2017, LECT NOTES COMPUT SC, V10416, P460, DOI 10.1007/978-3-319-66158-2_30; Chattopadhyay R., 2013, PROC 30 INT C MACH L, P253; Davidson I, 2007, DATA MIN KNOWL DISC, V14, P25, DOI 10.1007/s10618-006-0053-7; Davidson I, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1034; Ester M., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P226; Fisher D. H., 1987, Machine Learning, V2, P139, DOI 10.1007/BF00114265; Garey M.R., 1979, COMPUTERS INTRACTABI; GENNARI JH, 1989, ARTIF INTELL, V40, P11, DOI 10.1016/0004-3702(89)90046-5; Gilpin S, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P113; Guns T, 2013, IEEE T KNOWL DATA EN, V25, P402, DOI 10.1109/TKDE.2011.204; Jha S, 2005, LECT NOTES COMPUT SC, V3679, P397; Kotthoff  L., 2015, P 14 INT WORKSH CONS, P1; Langley P., 1996, ELEMENTS MACHINE LEA; Metivier Jean-Philippe, 2012, Advances in Intelligent Data Analysis XI. Proceedings 11th International Symposium, IDA 2012, P207, DOI 10.1007/978-3-642-34156-4_20; Mueller M, 2010, LECT NOTES ARTIF INT, V6332, P159, DOI 10.1007/978-3-642-16184-1_12; Ouali A., 2016, IJCAI 16, P647; QIAN B, 2010, P 24 AAAI C ART INT, P569; Walker PB, 2017, IEEE ACM T COMPUT BI, V14, P534, DOI 10.1109/TCBB.2016.2591549; Wang XS, 2013, PROC EUR S-STATE DEV, P234, DOI 10.1109/ESSDERC.2013.6818862	22	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000067
C	Dong, HY; Liang, XD; Gong, K; Lai, HJ; Zhu, J; Yin, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dong, Haoye; Liang, Xiaodan; Gong, Ke; Lai, Hanjiang; Zhu, Jia; Yin, Jian			Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Despite remarkable advances in image synthesis research, existing works often fail in manipulating images under the context of large geometric transformations. Synthesizing person images conditioned on arbitrary poses is one of the most representative examples where the generation quality largely relies on the capability of identifying and modeling arbitrary transformations on different body parts. Current generative models are often built on local convolutions and overlook the key challenges (e.g. heavy occlusions, different views or dramatic appearance changes) when distinct geometric changes happen for each part, caused by arbitrary pose manipulations. This paper aims to resolve these challenges induced by geometric variability and spatial displacements via a new Soft-Gated Warping Generative Adversarial Network (Warping-GAN), which is composed of two stages: 1) it first synthesizes a target part segmentation map given a target pose, which depicts the region-level spatial layouts for guiding image synthesis with higher-level structure constraints; 2) the Warping-GAN equipped with a soft-gated warping-block learns feature-level mapping to render textures from the original image into the generated segmentation map. Warping-GAN is capable of controlling different transformation degrees given distinct target poses. Moreover, the proposed warping-block is lightweight and flexible enough to be injected into any networks. Human perceptual studies and quantitative evaluations demonstrate the superiority of our WarpingGAN that significantly outperforms all existing methods on two large datasets.	[Dong, Haoye; Gong, Ke; Lai, Hanjiang; Yin, Jian] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou, Guangdong, Peoples R China; [Dong, Haoye; Gong, Ke; Yin, Jian] Guangdong Key Lab Big Data Anal & Proc, Guangzhou 510006, Guangdong, Peoples R China; [Liang, Xiaodan] Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou, Guangdong, Peoples R China; [Zhu, Jia] South China Normal Univ, Sch Comp Sci, Guangzhou, Guangdong, Peoples R China	Sun Yat Sen University; Sun Yat Sen University; South China Normal University	Liang, XD (corresponding author), Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou, Guangdong, Peoples R China.	donghy7@mail2.sysu.edu.cn; xdliang328@gmail.com; kegong936@gmail.com; laihanj3@mail.sysu.edu.cn; jzhu@m.scun.edu.cn; issjyin@mail.sysu.edu.cn	Gong, Ke/AAI-8783-2020		National Natural Science Foundation of China [61472453, U1401256, U1501252, U1611264, U1711261, U1711262, 61602530]; National Natural Science Foundation of China (NSFC) [61836012]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	This work is supported by the National Natural Science Foundation of China (61472453, U1401256, U1501252, U1611264, U1711261, U1711262, 61602530), and National Natural Science Foundation of China (NSFC) under Grant No. 61836012.	[Anonymous], 2017, INT C MACH LEARN ICM; [Anonymous], 2017, HIGH RESOLUTION IMAG; [Anonymous], 2017, NIPS; [Anonymous], 2017, ICCV; Balakrishnan Guha, 2018, CVPR; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Cao KD, 2018, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2018.00544; Cao Z., 2017, P IEEE C COMP VIS PA; Choi Y., 2018, CVPR; Deng ZJ, 2017, ADV NEUR IN, V30; Dong P, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P489, DOI 10.1109/ICIP.2002.1039014; Esser P., 2018, CVPR; Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Han X., 2017, ARXIV171108447; Hu Z., 2018, NIPS; Isola P., 2017, CVPR; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kingma D.P, P 3 INT C LEARNING R; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Liang X., 2018, P ECCV, P558; Liang X, 2017, INT J ADV ROBOT SYST, V14, DOI 10.1177/1729881417724179; Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124; Ma Liqian, 2018, CVPR; Mirza M., 2014, ARXIV PREPRINT ARXIV; Pumarola A., 2018, CVPR; Radford A., 2016, ICLR; Rocco I., 2017, CVPR; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Salimans Tim, 2016, ADV NEURAL INFORM PR; Siarohin A., 2018, CVPR; Wang B., 2018, ECCV; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Yan Y., 2017, ACM MM; Yi Zili, 2017, DUALGAN UNSUPERVISED; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133	36	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300044
C	Fang, C; Li, CJ; Lin, ZC; Zhang, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Fang, Cong; Li, Chris Junchi; Lin, Zhouchen; Zhang, Tong			SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we propose a new technique named Stochastic Path-Integrated Differential EstimatoR (SPIDER), which can be used to track many deterministic quantities of interests with significantly reduced computational cost. Combining SPIDER with the method of normalized gradient descent, we propose SPIDER-SFO that solve non-convex stochastic optimization problems using stochastic gradients only. We provide a few error-bound results on its convergence rates. Specially, we prove that the SPIDER-SFO algorithm achieves a gradient computation cost of O (min(n(1/2)epsilon(-2), epsilon(-3))) to find an epsilon-approximate first-order stationary point. In addition, we prove that SPIDER-SFO nearly matches the algorithmic lower bound for finding stationary point under the gradient Lipschitz assumption in the finite-sum setting. Our SPIDER technique can be further applied to find an (epsilon, O(epsilon(0.5)))-approximate second-order stationary point at a gradient computation cost of (O) over tilde (min(n(1/2)epsilon(-2) + epsilon(-2.5), epsilon(-3))).	[Fang, Cong; Lin, Zhouchen] Peking Univ, Sch EECS, Key Lab Machine Intelligence MoE, Beijing, Peoples R China; [Li, Chris Junchi; Zhang, Tong] Tencent AI Lab, Bellevue, WA USA	Peking University	Lin, ZC (corresponding author), Peking Univ, Sch EECS, Key Lab Machine Intelligence MoE, Beijing, Peoples R China.	fangcong@pku.edu.cn; junchi.li.duke@gmail.com; zlin@pku.edu.cn; tongzhang@tongzhang-ml.org	LI, chris/HDO-6232-2022; Zhang, Tong/HGC-1090-2022		National Basic Research Program of China (973 Program) [2015CB352502]; National Natural Science Foundation (NSF) of China [61625301, 61731018]; Microsoft Research Asia	National Basic Research Program of China (973 Program)(National Basic Research Program of China); National Natural Science Foundation (NSF) of China(National Natural Science Foundation of China (NSFC)); Microsoft Research Asia(Microsoft)	The authors would like to thank Jeffrey Z. HaoChen for his help on the numerical experiments, thank an anonymous reviewer to point out a mistake in the original proof of Theorem 1 and thank Zeyuan Allen-Zhu and Quanquan Gu for relevant discussions and pointing out references Zhou et al. [39, 40], also Jianqiao Wangni for pointing out references Nguyen et al. [28, 29], and Zebang Shen, Ruoyu Sun, Haishan Ye, Pan Zhou for very helpful discussions and comments. Zhouchen Lin is supported by National Basic Research Program of China (973 Program, grant no. 2015CB352502), National Natural Science Foundation (NSF) of China (grant nos. 61625301 and 61731018), and Microsoft Research Asia.	Agarwal N, 2017, ACM S THEORY COMPUT, P1195, DOI 10.1145/3055399.3055464; Allen-Zhu Z., 2018, ADV NEURAL INFORM PR; Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Bubeck S., 2015, FDN TRENDS MACHINE L; Carmon Y., 2016, SIAM J OPTIMIZATION; Carmon Y, 2017, PR MACH LEARN RES, V70; Cauchy A. L., 1847, COMP REND SCI PARIS, V25, P536; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Durrett R., 2010, CAMBRIDGE SERIES STA, V4th; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Harchaoui, 2018, P INT C ART INT STAT, P613; Hazan E., 2015, ADV NEURAL INFORM PR, P1594; Hinder O., 2017, ARXIV171011606; Jain P, 2017, FOUND TRENDS MACH LE, V10, P142, DOI 10.1561/2200000058; Jin C., 2018, C LEARN THEOR COLT 2; Jin C, 2017, PR MACH LEARN RES, V70; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Lee J. D., 2016, C LEARN THEOR, P1246; Lei LH, 2017, ADV NEUR IN, V30; Levy K. Y., 2016, ARXIV161104831; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Nguyen L. M., 2017, ARXIV170507261; Nguyen LM, 2017, PR MACH LEARN RES, V70; Reddi SJ, 2018, PR MACH LEARN RES, V84; Reddi SJ, 2016, PR MACH LEARN RES, V48; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Tripuraneni N., 2018, ADV NEURAL INFORM PR; Woodworth Blake, 2017, ARXIV170903594; Xu Yi, 2017, ARXIV171101944; Zhou D., 2018, ARXIV180608782; Zhou D., 2018, ADV NEURAL INFORM PR	39	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300064
C	Farahmand, AM		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Farahmand, Amir-massoud			Iterative Value-Aware Model Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				BOUNDS	This paper introduces a model-based reinforcement learning (MBRL) framework that incorporates the underlying decision problem in learning the transition model of the environment. This is in contrast with conventional approaches to MBRL that learn the model of the environment, for example by finding the maximum likelihood estimate, without taking into account the decision problem. Value-Aware Model Learning (VAML) framework argues that this might not be a good idea, especially if the true model of the environment does not belong to the model class from which we are estimating the model. The original VAML framework, however, may result in an optimization problem that is difficult to solve. This paper introduces a new MBRL class of algorithms, called Iterative VAML, that benefits from the structure of how the planning is performed (i.e., through approximate value iteration) to devise a simpler optimization problem. The paper theoretically analyzes Iterative VAML and provides finite sample error upper bound guarantee for it.	[Farahmand, Amir-massoud] Vector Inst, Toronto, ON, Canada; [Farahmand, Amir-massoud] MERL, Cambridge, MA 02139 USA		Farahmand, AM (corresponding author), Vector Inst, Toronto, ON, Canada.; Farahmand, AM (corresponding author), MERL, Cambridge, MA 02139 USA.	farahmand@vectorinstitute.ai						Antos A, 2008, MACH LEARN, V71, P89, DOI 10.1007/s10994-007-5038-2; Asadi Kavosh, 2018, FAIM WORKSH PRED GEN; Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282; Culotta A., 2010, ADV NEURAL INFORM PR, V23, P568; Doukhan P., 1994, LECT NOTES STAT; Ernst D, 2005, J MACH LEARN RES, V6, P503; Farahmand A. - m., 2012, ADV NEURAL INFORM PR, P1349; Farahmand A. - m., 2011, THESIS; Farahmand AM, 2009, P AMER CONTR CONF, P725, DOI 10.1109/ACC.2009.5160611; Farahmand AM, 2016, AAAI CONF ARTIF INTE, P3123; Farahmand AM, 2017, PR MACH LEARN RES, V54, P1486; Farahmand AM, 2016, J MACH LEARN RES, V17; Farahmand AM, 2012, J STAT PLAN INFER, V142, P493, DOI 10.1016/j.jspi.2011.08.007; Farahmand Amir-massoud, 2017, AM CONTR C ACC; Farahmand Amir-massoud, 2016, 13 EUR WORKSH REINF; Farquhar Gregory, 2018, INT C LEARN REPR ICL; Gine<prime> E., 2015, MATH FDN INFINITE DI; Gordon Geoffrey, 1995, INT C MACH LEARN ICM; Gyorfi L., 2002, DISTRIBUTION FREE TH; Huang DA, 2015, AAAI CONF ARTIF INTE, P2673; Joseph J, 2013, IEEE INT CONF ROBOT, P939, DOI 10.1109/ICRA.2013.6630686; Lagoudakis M. G., 2003, J MACHINE LEARNING, P1107, DOI DOI 10.1162/JMLR.2003.4.6.1107; Lazaric A, 2012, J MACH LEARN RES, V13, P3041; Lee WS, 2008, IEEE T INFORM THEORY, V54, P4395, DOI 10.1109/TIT.2008.928242; Lee WS, 1998, IEEE T INFORM THEORY, V44, P1974, DOI 10.1109/18.705577; Mann TA, 2015, J ARTIF INTELL RES, V53, P375, DOI 10.1613/jair.4676; Meir R, 2000, MACH LEARN, V39, P5, DOI 10.1023/A:1007602715810; Mendelson S, 2008, IEEE T INFORM THEORY, V54, P3797, DOI 10.1109/TIT.2008.926323; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mohri M., 2009, ADV NEURAL INFORM PR, V21, P1097; Mohri M, 2010, J MACH LEARN RES, V11, P789; Munos R, 2008, J MACH LEARN RES, V9, P815; Munos R, 2007, SIAM J CONTROL OPTIM, V46, P541, DOI 10.1137/040614384; Oh J, 2017, ADV NEUR IN, V30; Scherrer Bruno, 2012, P 29 INT C MACH LEAR; Silver D, 2017, PR MACH LEARN RES, V70; Steinwart I., 2009, ADV NEURAL INF PROCE, V22, P1768; Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216; Szepesv?ri, 2010, SYNTHESIS LECT ARTIF, V4, DOI [10.2200/S00268ED1V01Y201005AIM009, DOI 10.2200/S00268ED1V01Y201005AIM009]; Szepesvari Csaba, 2004, P 21 INT C MACH LEAR; Van de Geer S., 2000, APPL EMPIRICAL PROCE; Yang YH, 1999, ANN STAT, V27, P1564; Zhou DX, 2003, IEEE T INFORM THEORY, V49, P1743, DOI 10.1109/TIT.2003.813564	48	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003061
C	Foster, DJ; Krishnamurthy, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Foster, Dylan J.; Krishnamurthy, Akshay			Contextual bandits with surrogate losses: Margin bounds and efficient algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We use surrogate losses to obtain several new regret bounds and new algorithms for contextual bandit learning. Using the ramp loss, we derive new margin-based regret bounds in terms of standard sequential complexity measures of a benchmark class of real-valued regression functions. Using the hinge loss, we derive an efficient algorithm with a root dT-type mistake bound against benchmark policies induced by d-dimensional regressors. Under realizability assumptions, our results also yield classical regret bounds.	[Foster, Dylan J.] Cornell Univ, Ithaca, NY 14853 USA; [Krishnamurthy, Akshay] Microsoft Res, Nyc, NY USA	Cornell University; Microsoft	Foster, DJ (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	djfoster@cs.cornell.edu; akshay@cs.umass.edu			NDSEG PhD fellowship; Facebook PhD fellowship	NDSEG PhD fellowship; Facebook PhD fellowship(Facebook Inc)	We thank Haipeng Luo, Karthik Sridharan, Chen-Yu Wei, and Chicheng Zhang for several helpful discussions. D.F. acknowledges the support of the NDSEG PhD fellowship and Facebook PhD fellowship.	AbbasiYadkori  Y., 2011, ADV NEURAL INFORM PR; Agarwal A., 2016, ARXIV160603966; Agarwal Alekh, 2012, ARTIFICIAL INTELLIGE; Agarwal Alekh, 2014, INT C MACH LEARN; Auer P., 2002, SIAM J COMPUTING; Bartlett P., 2006, J AM STAT ASS; Beygelzimer Alina, 2017, INT C MACH LEARN; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Bubeck Sebastien, 2018, DISCRETE COMPUTATION; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Cesa-Bianchi Nicolo, 2017, C LEARN THEOR; Cheng X., 2018, ARXIV180501648; Chu Wei, 2011, INT C ART INT STAT; Dalalyan A. S., 2017, ARXIV171000095; Daniely Amit, 2013, C LEARN THEOR; Duchi John C., 2012, SIAM J OPTIMIZATION; Foster Dylan J., 2018, C LEARN THEOR; Foster Dylan J., 2015, ADV NEURAL INFORM PR; Freund Y., 1997, J COMPUTER SYSTEM SC; Hazan Elad, 2011, ADV NEURAL INFORM PR; Hazan Elad, 2007, C LEARN THEOR; Kakade S.M., 2009, ADV NEURAL INFORM PR; Kakade Sham M., 2012, J MACHINE LEARNING R; Kakade Sham M., 2008, INT C MACH LEARN; Langford John, 2008, ADV NEURAL INFORM PR; Lovasz Laszlo, 2007, RANDOM STRUCTURES AL; Lykouris Thodoris, 2018, C LEARN THEOR; Narayanan Hariharan, 2017, J MACHINE LEARNING R; Neu Gergely, 2013, INT C ALG LEARN THEO; Pisier Gilles, 1975, ISRAEL J MATH; Raginsky Maxim, 2017, C LEARN THEOR; Rakhlin A., 2015, ARXIV150106598; Rakhlin A., 2016, INT C MACH LEARN; Rakhlin Alexander, 2009, C LEARN THEOR; Rakhlin Alexander, 2015, PROBABILITY THEORY R; Rakhlin Alexander, 2017, C LEARN THEOR; Rakhlin Alexander, 2015, J MACHINE LEARNING R; Rakhlin Alexander, 2010, ADV NEURAL INFORM PR; Schapire Robert E., 2012, BOOSTING FDN ALGORIT; Slivkins Aleksandrs, 2011, C LEARN THEOR; Srebro Nathan, 2011, ADV NEURAL INFORM PR; Stephane Boucheron GL., 2005, ESAIM-PROBAB STAT, V7, P251, DOI [10.1051/ps, DOI 10.1051/PS]; Syrgkanis Vasilis, 2016, ADV NEURAL INFORM PR; Syrgkanis Vasilis, 2016, INT C MACH LEARN; Szepesvari Csaba, 2013, INT C MACH LEARN; Tewari Ambuj, 2017, MOBILE HLTH; Tong Zhang, 2004, J MACHINE LEARNING R	48	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302062
C	Haug, L; Tschiatschek, S; Singla, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Haug, Luis; Tschiatschek, Sebastian; Singla, Adish			Teaching Inverse Reinforcement Learners via Features and Demonstrations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Learning near-optimal behaviour from an expert's demonstrations typically relies on the assumption that the learner knows the features that the true reward function depends on. In this paper, we study the problem of learning from demonstrations in the setting where this is not the case, i.e., where there is a mismatch between the worldviews of the learner and the expert. We introduce a natural quantity, the teaching risk, which measures the potential suboptimality of policies that look optimal to the learner in this setting. We show that bounds on the teaching risk guarantee that the learner is able to find a near-optimal policy using standard algorithms based on inverse reinforcement learning. Based on these findings, we suggest a teaching scheme in which the expert can decrease the teaching risk by updating the learner's worldview, and thus ultimately enable her to find a near-optimal policy.	[Haug, Luis] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland; [Tschiatschek, Sebastian] Microsoft Res, Cambridge, England; [Singla, Adish] Max Planck Inst Software Syst, Saarbrucken, Germany	Swiss Federal Institutes of Technology Domain; ETH Zurich; Microsoft; Max Planck Society	Haug, L (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	lhaug@inf.ethz.ch; setschia@microsoft.com; adishs@mpi-sws.org	Singla, Adish/ABG-8960-2021	Singla, Adish/0000-0001-9922-0668; Tschiatschek, Sebastian/0000-0002-2592-0108				Abbeel P., 2004, ICML; Aodha O. M., 2018, CVPR; Aodha Oisin Mac, 2018, CORR; Brown Daniel S., 2018, CORR; Cakmak M., 2012, 26 AAAI C ART INT; Cakmak M, 2014, ARTIF INTELL, V217, P198, DOI 10.1016/j.artint.2014.08.005; Chen Y., 2018, NEURAL INFORM PROCES; Haug L., 2018, CORR; Kamalaruban P., 2018, LEARN INSTR WORKSH N; Liu WY, 2017, PR MACH LEARN RES, V70; Mayer M., 2017, DARTS DAGSTUHL ARTIF, V3; Mei SK, 2015, AAAI CONF ARTIF INTE, P2871; Patil K. R., 2014, NIPS, P2465; Rafferty AN, 2016, COGNITIVE SCI, V40, P1290, DOI 10.1111/cogs.12290; Sermanet P, 2018, IEEE INT CONF ROBOT, P1134; Singla A., 2013, NIPS WORKSH DAT DRIV; Singla A, 2014, PR MACH LEARN RES, V32, P154; Stadie B. C., 2017, CORR; Yeo Teresa, 2019, AAAI; Zhu XJ, 2015, AAAI CONF ARTIF INTE, P4083; Zhu Xiaojin, 2018, CORR; Ziebart B. D., 2008, AAAI, V8, P1433	22	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003006
C	He, LF; Chen, K; Xu, WW; Zhou, JY; Wang, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		He, Lifang; Chen, Kun; Xu, Wanwan; Zhou, Jiayu; Wang, Fei			Boosted Sparse and Low-Rank Tensor Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				REGULARIZATION; SELECTION	We propose a sparse and low-rank tensor regression model to relate a univariate outcome to a feature tensor, in which each unit-rank tensor from the CP decomposition of the coefficient tensor is assumed to be sparse. This structure is both parsimonious and highly interpretable, as it implies that the outcome is related to the features through a few distinct pathways, each of which may only involve subsets of feature dimensions. We take a divide-and-conquer strategy to simplify the task into a set of sparse unit-rank tensor regression problems. To make the computation efficient and scalable, for the unit-rank tensor regression, we propose a stagewise estimation procedure to efficiently trace out its entire solution path. We show that as the step size goes to zero, the stagewise solution paths converge exactly to those of the corresponding regularized regression. The superior performance of our approach is demonstrated on various real-world and synthetic examples.	[He, Lifang; Wang, Fei] Weill Cornell Med, New York, NY 10065 USA; [Chen, Kun; Xu, Wanwan] Univ Connecticut, Storrs, CT 06269 USA; [Zhou, Jiayu] Michigan State Univ, E Lansing, MI 48824 USA	Cornell University; University of Connecticut; Michigan State University	Chen, K (corresponding author), Univ Connecticut, Storrs, CT 06269 USA.	lifanghescut@gmail.com; kun.chen@uconn.edu; wanwan.xu@uconn.edu; dearjiayu@gmail.com; few2001@med.cornell.edu	He, Lifang/D-8175-2016	He, Lifang/0000-0001-7810-9071; Chen, Kun/0000-0003-3579-5467	NSF [IIS-1716432, IIS-1750326, IIS-1718798, DMS-1613295, IIS-1749940, IIS-1615597]; ONR [N00014-18-1-2585, N00014-17-1-2265]; Michael J. Fox Foundation [14858]; Michael J. Fox Foundation for Parkinson's Research; Abbvie; Avid; Biogen; Bristol-Mayers Squibb; Covance; GE; Genentech; GlaxoSmithKline; Lilly; Lundbeck; Merk; Meso Scale Discovery; Pfizer; Piramal; Roche; Sanofi; Servier; TEVA; UCB; Golub Capital;  [1R01AI130460]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); Michael J. Fox Foundation; Michael J. Fox Foundation for Parkinson's Research; Abbvie(AbbVie); Avid; Biogen(Biogen); Bristol-Mayers Squibb; Covance; GE; Genentech(Roche HoldingGenentech); GlaxoSmithKline(GlaxoSmithKline); Lilly(Eli Lilly); Lundbeck(Lundbeck Corporation); Merk; Meso Scale Discovery; Pfizer(Pfizer); Piramal; Roche(Roche Holding); Sanofi; Servier(Servier); TEVA(Teva Pharmaceutical Industries); UCB(UCB Pharma SA); Golub Capital; 	This work is supported by NSF No. IIS-1716432 (Wang), IIS-1750326 (Wang), IIS-1718798 (Chen), DMS-1613295 (Chen), IIS-1749940 (Zhou), IIS-1615597 (Zhou), ONR N00014-18-1-2585 (Wang), and N00014-17-1-2265 (Zhou), and Michael J. Fox Foundation grant number 14858 (Wang). Lifang He's research is supported in part by 1R01AI130460. Data used in the preparation of this article were obtained from the Parkinson's Progression Markers Initiative (PPMI) database (http://www.ppmi-info.org/data).For up-to-date information on the study, visit http://www.ppmi-info.org.PPMI - a public-private partnership -is funded by the Michael J. Fox Foundation for Parkinson's Research and funding partners, including Abbvie, Avid, Biogen, Bristol-Mayers Squibb, Covance, GE, Genentech, GlaxoSmithKline, Lilly, Lundbeck, Merk, Meso Scale Discovery, Pfizer, Piramal, Roche, Sanofi, Servier, TEVA, UCB and Golub Capital.	Bengua JA, 2017, IEEE T IMAGE PROCESS, V26, P2466, DOI 10.1109/TIP.2017.2672439; Chen K, 2012, J R STAT SOC B, V74, P203, DOI 10.1111/j.1467-9868.2011.01002.x; da Silva A. P., 2015, ARXIV150805273; Friedman J., 2009, ELEMENTS STAT LEARNI, DOI 10.1007/978-0-387-84858-7; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Guo WW, 2012, IEEE T IMAGE PROCESS, V21, P816, DOI 10.1109/TIP.2011.2165291; Kampa K, 2014, J GLOBAL OPTIM, V59, P439, DOI 10.1007/s10898-013-0134-2; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Minasian A, 2014, IEEE T WIREL COMMUN, V13, P6118, DOI 10.1109/TWC.2014.2320977; Mishra A, 2017, J COMPUT GRAPH STAT, V26, P814, DOI 10.1080/10618600.2017.1340891; Phan AH, 2015, IEEE T SIGNAL PROCES, V63, P5924, DOI 10.1109/TSP.2015.2458785; Signoretto M, 2014, MACH LEARN, V94, P303, DOI 10.1007/s10994-013-5366-3; Song XN, 2017, AAAI CONF ARTIF INTE, P2562; Su Y, 2012, IEEE T SYST MAN CY B, V42, P1560, DOI 10.1109/TSMCB.2012.2195171; Tan X., 2012, P INT C INT SCI INT, P573; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Vaughan G, 2017, BIOMETRICS, V73, P1332, DOI 10.1111/biom.12669; Wang F, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P145, DOI 10.1145/2623330.2623755; YU R., 2016, INT C MACH LEARN LEA, P373; Zhao P, 2007, J MACH LEARN RES, V8, P2701; Zhou H, 2013, J AM STAT ASSOC, V108, P540, DOI 10.1080/01621459.2013.776499; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	22	3	3	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301004
C	Huang, J; Wu, F; Precup, D; Cai, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Huang, Jessie; Wu, Fa; Precup, Doina; Cai, Yang			Learning safe policies with expert guidance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ROBUST; OPTIMIZATION	We propose a framework for ensuring safe behavior of a reinforcement learning agent when the reward function may be difficult to specify. In order to do this, we rely on the existence of demonstrations from expert policies, and we provide a theoretical framework for the agent to optimize in the space of rewards consistent with its existing knowledge. We propose two methods to solve the resulting optimization: an exact ellipsoid-based method and a method in the spirit of the "follow-the-perturbed-leader" algorithm. Our experiments demonstrate the behavior of our algorithm in both discrete and continuous problems. The trained agent safely avoids states with potential negative effects while imitating the behavior of the expert in the other states.	[Huang, Jessie; Wu, Fa; Precup, Doina; Cai, Yang] McGill Univ, Sch Comp Sci, Montreal, PQ, Canada; [Wu, Fa] Zhejiang Demet Med Technol, Taizhou, Peoples R China	McGill University	Huang, J (corresponding author), McGill Univ, Sch Comp Sci, Montreal, PQ, Canada.	jiexi.huang@mcgill.ca; fa.wu2@mcgill.ca; dprecup@cs.mcgill.ca; cai@cs.mcgill.ca		Cai, Yang/0000-0002-5426-1324	Open Philanthropy Fund; NSERC [RGPIN2015-06127]; FRQNT [2017-NC-198956]	Open Philanthropy Fund; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); FRQNT	Doina Precup and Jessie Huang gratefully acknowledge funding from Open Philanthropy Fund and NSERC which made this research possible. Yang Cai and Fa Wu thank the NSERC for its support through the Discovery grant RGPIN2015-06127 and FRQNT for its support through the grant 2017-NC-198956.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Amin Kareem, 2017, NEURAL INFORM PROCES, P1813; Amodei D., 2016, CONCRETE PROBLEMS AI; Ben-Tal A, 2015, OPER RES, V63, P628, DOI 10.1287/opre.2015.1374; Boyd S., 2007, LECT NOTES STANFORD; Brockman G., 2016, OPENAI GYM; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; GROTSCHEL M, 1981, COMBINATORICA, V1, P169, DOI 10.1007/BF02579273; Hadfield-Menell D., 2017, ADV NEURAL INFORM PR, P6768; Hutter M, 2005, J MACH LEARN RES, V6, P639; Iyengar GN, 2005, MATH OPER RES, V30, P257, DOI 10.1287/moor.1040.0129; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; KARP RM, 1982, SIAM J COMPUT, V11, P620, DOI 10.1137/0211053; Khachiyan L. G., 1979, SOV MATH DOKL, V20, p[1, 191]; Lim SH, 2013, ADV NEURAL INFORM PR, V26, P701; Morimoto J, 2005, NEURAL COMPUT, V17, P335, DOI 10.1162/0899766053011528; Nilim A, 2005, OPER RES, V53, P780, DOI 10.1287/opre.1050.0216; Syed U., 2008, P 25 INT C MACH LEAR, P1032, DOI [DOI 10.1145/1390156.1390286, 10.1145/1390156.1390286]; Syed U., 2008, ADV NEURAL INFORM PR, P1449; Yang Cai, 2013, 24 ANN ACM SIAM S DI	21	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003064
C	Hughes, E; Leibo, JZ; Phillips, M; Tuyls, K; Duenez-Guzman, E; Castaneda, AG; Dunning, I; Zhu, TN; McKee, K; Koster, R; Roff, H; Graepel, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hughes, Edward; Leibo, Joel Z.; Phillips, Matthew; Tuyls, Karl; Duenez-Guzman, Edgar; Castaneda, Antonio Garcia; Dunning, Iain; Zhu, Tina; McKee, Kevin; Koster, Raphael; Roff, Heather; Graepel, Thore			Inequity aversion improves cooperation in intertemporal social dilemmas	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				FAIRNESS; REINFORCEMENT; PREFERENCES; INFORMATION; PUNISHMENT	Groups of humans are often able to find ways to cooperate with one another in complex, temporally extended social dilemmas. Models based on behavioral economics are only able to explain this phenomenon for unrealistic stateless matrix games. Recently, multi-agent reinforcement learning has been applied to generalize social dilemma problems to temporally and spatially extended Markov games. However, this has not yet generated an agent that learns to cooperate in social dilemmas as humans do. A key insight is that many, but not all, human individuals have inequity averse social preferences. This promotes a particular resolution of the matrix game social dilemma wherein inequity-averse individuals are personally pro-social and punish defectors. Here we extend this idea to Markov games and show that it promotes cooperation in several types of sequential social dilemma, via a profitable interaction with policy learnability. In particular, we find that inequity aversion improves temporal credit assignment for the important class of intertemporal social dilemmas. These results help explain how large-scale cooperation may emerge and persist.	[Hughes, Edward; Leibo, Joel Z.; Phillips, Matthew; Tuyls, Karl; Duenez-Guzman, Edgar; Castaneda, Antonio Garcia; Dunning, Iain; Zhu, Tina; McKee, Kevin; Koster, Raphael; Roff, Heather; Graepel, Thore] DeepMind, London, England		Hughes, E (corresponding author), DeepMind, London, England.	edwardhughes@google.com; jzl@google.com; matthew.phillips.12@ucl.ac.uk; karltuyls@google.com; duenez@google.com; antoniogc@google.com; idunning@google.com; tinazhu@google.com; kevinrmckee@google.com; rkoster@google.com; hroff@google.com; thore@google.com						[Anonymous], 2017, ARXIV170701068; Bellemare C, 2008, ECONOMETRICA, V76, P815, DOI 10.1111/j.1468-0262.2008.00860.x; Bicchieri C, 2010, J BEHAV DECIS MAKING, V23, P161, DOI 10.1002/bdm.648; Blake R. P., 2015, ONTOGENY FAIRNESS 7, V528, P11; Brosnan SF, 2014, SCIENCE, V346, P314, DOI 10.1126/science.1251776; Camerer C.F., 2011, BEHAV GAME THEORY EX; Charness G, 2002, Q J ECON, V117, P817, DOI 10.1162/003355302760193904; Chentanez, 2005, ADV NEURAL INFORM PR, P1281, DOI DOI 10.21236/ADA440280; Cote E. M. d., 2006, P 5 INT JOINT C AUT, P783, DOI [DOI 10.1145/1160633.1160770, 10.1145/1160633.1160770]; de Jong S, 2011, AUTON AGENT MULTI-AG, V22, P103, DOI 10.1007/s10458-010-9122-9; Dietz T, 2003, SCIENCE, V302, P1907, DOI 10.1126/science.1091015; Eckel C, 2010, J ECON BEHAV ORGAN, V73, P109, DOI 10.1016/j.jebo.2009.03.026; Engelmann D, 2004, AM ECON REV, V94, P857, DOI 10.1257/0002828042002741; Falk A, 2006, GAME ECON BEHAV, V54, P293, DOI 10.1016/j.geb.2005.03.001; Fehr E, 2002, NATURE, V415, P137, DOI 10.1038/415137a; Fehr E, 1999, Q J ECON, V114, P817, DOI 10.1162/003355399556151; Fehr E, 2000, AM ECON REV, V90, P980, DOI 10.1257/aer.90.4.980; Fehr E, 2007, ANNU REV SOCIOL, V33, P43, DOI 10.1146/annurev.soc.33.040406.131812; Foerster J. N, 2017, ARXIV170904326; FREY BS, 1995, J INST THEOR ECON, V151, P286; Gibbons Robert, 1992, PRIMER GAME THEORY; GRICE GR, 1948, J EXP PSYCHOL, V38, P1, DOI 10.1037/h0061016; Gurerk O, 2006, SCIENCE, V312, P108, DOI 10.1126/science.1123633; HARDIN G, 1968, SCIENCE, V162, P1243, DOI 10.1126/science.162.3859.1243; Hart H. L. A., 1955, PHILOS REV, V64, P175, DOI 10.2307/2182586; Henrich J, 2010, SCIENCE, V327, P1480, DOI 10.1126/science.1182238; Hoppe EI, 2013, REV ECON STUD, V80, P1516, DOI 10.1093/restud/rdt010; Janssen M., 2010, ECOLOGY SOC, V15; Janssen MA, 2013, ECOL SOC, V18, DOI 10.5751/ES-05664-180404; Janssen MA, 2010, SCIENCE, V328, P613, DOI 10.1126/science.1183532; Kearns M.J., 2000, P 13THANNUAL C COMPU, P142; Kleiman-Weiner M., 2016, COGSCI; KLOSKO G, 1987, ETHICS, V97, P353, DOI 10.1086/292843; Kollock P, 1998, ANNU REV SOCIOL, V24, P183, DOI 10.1146/annurev.soc.24.1.183; Leibo J.Z., 2017, P 16 INT C AUT AG MU; Littman ML, 1994, ICML 1994, P157; LOEWENSTEIN GF, 1989, J PERS SOC PSYCHOL, V57, P426, DOI 10.1037/0022-3514.57.3.426; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; OLIVER P, 1980, AM J SOCIOL, V85, P1356, DOI 10.1086/227168; Olson M., 1965, LOGIC COLLECTIVE ACT; Ostrom E, 1998, AM POLIT SCI REV, V92, P1, DOI 10.2307/2585925; OSTROM E, 1992, AM POLIT SCI REV, V86, P404, DOI 10.2307/1964229; Ostrom E., 1992, CRAFTING I SELF GOVE; Perolat Julien, 2017, ADV NEURAL INFORM PR; Peysakhovich A., 2017, ARXIV170902865; Peysakhovich A., 2017, CORR; Rand DG, 2013, TRENDS COGN SCI, V17, P413, DOI 10.1016/j.tics.2013.06.003; RAWLS J, 1958, PHILOS REV, V67, P164, DOI 10.2307/2182612; Rousseau J-J., 1755, DISCOURSE ORIGIN INE; Sandholm TW, 1996, BIOSYSTEMS, V37, P147, DOI 10.1016/0303-2647(95)01551-5; SCHELLING TC, 1973, J CONFLICT RESOLUT, V17, P381, DOI 10.1177/002200277301700302; Shapley L. S., 1953, P NATL ACAD SCI US; Skinner B.F, 1938, BEHAV ORGANISMS EXPT; Sunehag P., 2017, CORR; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Verbeeck K, 2002, LECT NOTES ARTIF INT, V2564, P81; Walsh W. E., 2002, AAAI 02 WORKSH GAM T, P109; Wellman M. P., 2006, AAAI, P1552; YAMAGISHI T, 1986, J PERS SOC PSYCHOL, V51, P110, DOI 10.1037/0022-3514.51.1.110	59	3	3	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303033
C	Ishikawa, I; Fujii, K; Ikeda, M; Hashimoto, Y; Kawahara, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ishikawa, Isao; Fujii, Keisuke; Ikeda, Masahiro; Hashimoto, Yuka; Kawahara, Yoshinobu			Metric on Nonlinear Dynamical Systems with Perron-Frobenius Operators	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PATTERNS; KERNELS	The development of a metric for structural data is a long-term problem in pattern recognition and machine learning. In this paper, we develop a general metric for comparing nonlinear dynamical systems that is defined with Perron-Frobenius operators in reproducing kernel Hilbert spaces. Our metric includes the existing fundamental metrics for dynamical systems, which are basically defined with principal angles between some appropriately-chosen subspaces, as its special cases. We also describe the estimation of our metric from finite data. We empirically illustrate our metric with an example of rotation dynamics in a unit disk in a complex plane, and evaluate the performance with real-world time-series data.	[Ishikawa, Isao; Fujii, Keisuke; Ikeda, Masahiro; Hashimoto, Yuka; Kawahara, Yoshinobu] RIKEN Ctr Adv Intelligence Project, Wako, Saitama, Japan; [Ishikawa, Isao; Ikeda, Masahiro; Hashimoto, Yuka] Keio Univ, Sch Fundamental Sci & Technol, Tokyo, Japan; [Kawahara, Yoshinobu] Osaka Univ, Inst Sci & Ind Res, Suita, Osaka, Japan	RIKEN; Keio University; Osaka University	Ishikawa, I (corresponding author), RIKEN Ctr Adv Intelligence Project, Wako, Saitama, Japan.; Ishikawa, I (corresponding author), Keio Univ, Sch Fundamental Sci & Technol, Tokyo, Japan.	isao.ishikawa@riken.jp; keisuke.fujii.zh@riken.jp; masahiro.ikeda@riken.jp; yukahashimoto@keio.jp; ykawahara@sanken.osaka-u.ac.jp	Kawahara, Yoshinobu/J-2462-2014; KAWAHARA, Yoshinobu/AAM-7540-2020; Fujii, Keisuke/AAJ-1330-2020	Kawahara, Yoshinobu/0000-0001-7789-4709; KAWAHARA, Yoshinobu/0000-0001-7789-4709; Fujii, Keisuke/0000-0001-5487-4297				Banach S., 1995, THEORIE OPERATIONS L; Bao QF, 2006, J MATH ANAL APPL, V323, P481, DOI 10.1016/j.jmaa.2005.10.064; Berger E, 2015, ADV ROBOTICS, V29, P331, DOI 10.1080/01691864.2014.981292; Brunton BW, 2016, J NEUROSCI METH, V258, P1, DOI 10.1016/j.jneumeth.2015.10.010; Chaudhry R., 2014, P 52 IEEE C DEC CONT, P5377; Chen Y, 2015, UCR TIME SERIES CLAS; De Cock K, 2002, SYST CONTROL LETT, V46, P265, DOI 10.1016/S0167-6911(02)00135-4; Fujii K, 2017, LECT NOTES ARTIF INT, V10536, P127, DOI 10.1007/978-3-319-71273-4_11; Kawahara Y., 2016, ADV NEURAL INFORM PR, V29, P911; Koopman BO, 1931, P NATL ACAD SCI USA, V17, P315, DOI 10.1073/pnas.17.5.315; Kutz JN, 2016, SIAM J APPL DYN SYST, V15, P713, DOI 10.1137/15M1023543; Martin RJ, 2000, IEEE T SIGNAL PROCES, V48, P1164, DOI 10.1109/78.827549; Mezic I, 2005, NONLINEAR DYNAM, V41, P309, DOI 10.1007/s11071-005-2824-x; Mezic I, 2004, PHYSICA D, V197, P101, DOI 10.1016/j.physd.2004.06.015; Mezic I., 2016, COMP DYNAMICS DISSIP, P454; Proctor JL, 2016, SIAM J APPL DYN SYST, V15, P142, DOI 10.1137/15M1013857; Proctor JL, 2015, INT HEALTH, V7, P139, DOI 10.1093/inthealth/ihv009; Rowley CW, 2009, J FLUID MECH, V641, P115, DOI 10.1017/S0022112009992059; Semenov EM, 2010, J FUNCT ANAL, V259, P1517, DOI 10.1016/j.jfa.2010.05.011; SUCHESTON L, 1967, AM MATH MON, V74, P308, DOI 10.2307/2316038; SUCHESTON L, 1964, MATH Z, V86, P327, DOI 10.1007/BF01110407; Takeishi N, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2814; Takeishi N, 2017, PHYS REV E, V96, DOI 10.1103/PhysRevE.96.033310; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vishwanathan SVN, 2007, INT J COMPUT VISION, V73, P95, DOI 10.1007/s11263-006-9352-0; Williams MO, 2015, J NONLINEAR SCI, V25, P1307, DOI 10.1007/s00332-015-9258-5	26	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302084
C	Jin, C; Liu, LT; Ge, R; Jordan, MI		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jin, Chi; Liu, Lydia T.; Ge, Rong; Jordan, Michael I.			On the Local Minima of the Empirical Risk	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				OPTIMIZATION	Population risk is always of primary interest in machine learning; however, learning algorithms only have access to the empirical risk. Even for applications with nonconvex nonsmooth losses (such as modern deep networks), the population risk is generally significantly more well-behaved from an optimization point of view than the empirical risk. In particular, sampling can create many spurious local minima. We consider a general framework which aims to optimize a smooth nonconvex function F (population risk) given only access to an approximation f (empirical risk) that is pointwise close to F (i.e., parallel to F - f parallel to(infinity) <= nu). Our objective is to find the 6-approximate local minima of the underlying function F while avoiding the shallow local minima-arising because of the tolerance nu-which exist only in f. We propose a simple algorithm based on stochastic gradient descent (SGD) on a smoothed version of f that is guaranteed to achieve our goal as long as nu <= O(is an element of(1.5)/d). We also provide an almost matching lower bound showing that our algorithm achieves optimal error tolerance nu among all algorithms making a polynomial number of queries of f. As a concrete example, we show that our results can be directly used to give sample complexities for learning a ReLU unit.	[Jin, Chi; Liu, Lydia T.; Jordan, Michael I.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Ge, Rong] Duke Univ, Durham, NC 27706 USA	University of California System; University of California Berkeley; Duke University	Jin, C (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	chijin@cs.berkeley.edu; lydiatliu@cs.berkeley.edu; rongge@cs.duke.edu; jordan@cs.berkeley.edu	Jordan, Michael I/C-5253-2013	Jordan, Michael/0000-0001-8935-817X				Agarwal Alekh, 2010, P 23 ANN C LEARN THE; Agarwal N, 2017, ACM S THEORY COMPUT, P1195, DOI 10.1145/3055399.3055464; Anandkumar A., 2016, 29 ANN C LEARNING TH, P81; [Anonymous], 2015, ADV NEURAL INF PROCE; Auer P, 1996, ADV NEUR IN, V8, P316; Bartlett Peter L., 2003, J MACH LEARN RES, V3; Belloni A., 2015, C LEARN THEOR, P240; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Brutzkus A, 2017, PR MACH LEARN RES, V70; Carmon Yair, 2016, ARXIV161100756; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Dinh L., 2017, ARXIV170304933; Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256; Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385; Ge R., 2015, P 28 C LEARN THEOR C; Jin C, 2018, COMMUNICATION; Jin C, 2017, PR MACH LEARN RES, V70; Jin Chi, 2017, CORR; Keskar N.S., 2016, ABS160904836; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; Kleinberg Robert, 2018, CORR; Loh P.-L., 2013, ADV NEURAL INFORM PR, P476; Mei S., 2016, ARXIV160706534; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Nesterov Y, 2004, INTRO LECT CONVEX PR; Rechenberg I., 1973, EVOLUTIONSSTRATEGIE; Risteski Andrej, 2016, ADV NEURAL INFORM PR, P4745; Shamir Ohad, 2013, P 26 ANN C LEARN THE, V30; Sinha A., 2018, ICLR; Steinhardt J., 2017, ADV NEURAL INFORM PR; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Zhang Y., 2017, C LEARN THEOR PMLR, P1980	32	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304087
C	Kim, S; Lin, S; Jeon, S; Min, D; Sohn, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kim, Seungryong; Lin, Stephen; Jeon, Sangryul; Min, Dongbo; Sohn, Kwanghoon			Recurrent Transformer Networks for Semantic Correspondence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present recurrent transformer networks (RTNs) for obtaining dense correspondences between semantically similar images. Our networks accomplish this through an iterative process of estimating spatial transformations between the input images and using these transformations to generate aligned convolutional activations. By directly estimating the transformations between an image pair, rather than employing spatial transformer networks to independently normalize each individual image, we show that greater accuracy can be achieved. This process is conducted in a recursive manner to refine both the transformation estimates and the feature representations. In addition, a technique is presented for weakly-supervised training of RTNs that is based on a proposed classification loss. With RTNs, state-of-the-art performance is attained on several benchmarks for semantic correspondence.	[Kim, Seungryong; Jeon, Sangryul; Sohn, Kwanghoon] Yonsei Univ, Seoul, South Korea; [Lin, Stephen] Microsoft Res, Beijing, Peoples R China; [Min, Dongbo] Ewha Womans Univ, Seoul, South Korea	Yonsei University; Microsoft; Ewha Womans University	Sohn, K (corresponding author), Yonsei Univ, Seoul, South Korea.	srkim89@yonsei.ac.kr; stevelin@microsoft.com; cheonjsr@yonsei.ac.kr; dbmin@ewha.ac.kr; khsohn@yonsei.ac.kr	Jeon, Sangryul/GSN-6176-2022		Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) - Ministry of Science and ICT [NRF-2017M3C4A7069370]	Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) - Ministry of Science and ICT	This research was supported by Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT (NRF-2017M3C4A7069370).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2016, CVPR; Barnes Connelly, 2010, EUR C COMP VIS; Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302; Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303; Bristow H., 2015, P ICCV; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; HaCohen Y, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964965; Ham B., 2017, IEEE T PAMI; Ham B., 2016, CVPR; Han K., 2017, ICCV; Han YC, 2017, PROCEEDINGS OF THE 2017 IEEE VIS ARTS PROGRAM (VISAP); Hariharan Bharath, 2011, P IEEE C COMP VIS PA; Hassner T., 2012, CVPR; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hur J., 2015, P CVPR; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Kim J., 2013, CVPR; Kim S., 2018, IEEE T PAMI; Kim S., 2017, CVPR; Kim Seungryong, 2018, IEEE T PATTERN ANAL; Liao J., 2017, P SIGGRAPH; Lin C.-H., 2017, CVPR; Lin Yen-Liang, 2014, EUR C COMP VIS ECCV; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Long J.L., 2014, P C NEUR INF PROC SY, V27, P1601; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Novotny D, 2017, PROC CVPR IEEE, P2867, DOI 10.1109/CVPR.2017.306; Qiu W., 2014, P WACV; Rocco I., 2018, CVPR; Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rubinstein M., 2013, CVPR; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Taniai Tatsunori, 2016, P IEEE C COMP VIS PA; Thewlis James, 2017, ADV NEURAL INFORM PR, V3, P8; Ufer N., 2017, CVPR; Yang F., 2017, CVPR; Yang H., 2014, P CVPR; Yi K. M., 2016, CVPR; Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999; Zhou T., 2016, P CVPR; Zhou TH, 2015, PROC CVPR IEEE, P1191, DOI 10.1109/CVPR.2015.7298723	45	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000061
C	Lee, K; Choi, S; Oh, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lee, Kyungjae; Choi, Sungjoon; Oh, Songhwai			Maximum Causal Tsallis Entropy Imitation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we propose a novel maximum causal Tsallis entropy (MCTE) framework for imitation learning which can efficiently learn a sparse multi-modal policy distribution from demonstrations. We provide the full mathematical analysis of the proposed framework. First, the optimal solution of an MCTE problem is shown to be a sparsemax distribution, whose supporting set can be adjusted. The proposed method has advantages over a softmax distribution in that it can exclude unnecessary actions by assigning zero probability. Second, we prove that an MCTE problem is equivalent to robust Bayes estimation in the sense of the Brier score. Third, we propose a maximum causal Tsallis entropy imitation learning (MCTEIL) algorithm with a sparse mixture density network (sparse MDN) by modeling mixture weights using a sparsemax distribution. In particular, we show that the causal Tsallis entropy of an MDN encourages exploration and efficient mixture utilization while Shannon entropy is less effective.	[Lee, Kyungjae; Oh, Songhwai] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea; [Lee, Kyungjae; Oh, Songhwai] Seoul Natl Univ, ASRI, Seoul, South Korea; [Choi, Sungjoon] Kakao Brain, Jeju, South Korea	Seoul National University (SNU); Seoul National University (SNU)	Lee, K (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.; Lee, K (corresponding author), Seoul Natl Univ, ASRI, Seoul, South Korea.	kyungjae.lee@rllab.snu.ac.kr; sam.choi@kakaobrain.com; songhwai@snu.ac.kr	Choi, Sungjoon/AAL-6430-2020		Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Science and ICT [NRF-2017R1A2B2006136]; Brain Korea 21 Plus Project	Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Science and ICT(National Research Foundation of KoreaMinistry of Science, ICT & Future Planning, Republic of Korea); Brain Korea 21 Plus Project	This work was supported in part by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT (NRF-2017R1A2B2006136) and by the Brain Korea 21 Plus Project in 2018.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Bloem M, 2014, IEEE DECIS CONTR P, P4911, DOI 10.1109/CDC.2014.7040156; Brier G. W., 1950, MON WEATHER REV, V78, P1, DOI [10.1175/1520-0493(1950)0782.0.co;2, DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2]; Choi J., 2013, P 23 INT JOINT C ART; Choi J, 2015, IEEE T CYBERNETICS, V45, P793, DOI 10.1109/TCYB.2014.2336867; Chow Y., 2018, INT C MACH LEARN, P978; Grunwald PD, 2004, ANN STAT, V32, P1367, DOI 10.1214/009053604000000553; Haarnoja T, 2017, PR MACH LEARN RES, V70; Hausman K, 2017, ADV NEUR IN, V30; Heess N., 2012, JMLR WORKSHOP C P EW, V24, P43; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Lee K., 2018, IEEE ROBOT AUTOM LET, V3, P1466, DOI [10.1109/LRA.2018.2800085, DOI 10.1109/LRA.2018.2800085]; Levine S., 2011, C NEURAL INFORM PROC, V24, P19; Li Yuxi, 2017, ARXIV170107274; Martins AFT, 2016, PR MACH LEARN RES, V48; MILLAR PW, 1983, LECT NOTES MATH, V976, P75; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Ramachandran Deepak, 2007, P 20 INT JOINT C ART; RATLIFF N, 2006, P 23 INT C MACH LEAR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Syed U., 2008, P 25 INT C MACH LEAR, P1032, DOI [DOI 10.1145/1390156.1390286, 10.1145/1390156.1390286]; Syed U., 2007, ADV NEURAL INFORM PR; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Vamplew P, 2017, NEUROCOMPUTING, V263, P74, DOI 10.1016/j.neucom.2016.09.141; Wang ZY, 2017, ADV NEUR IN, V30; Wulfmeier M., 2015, ARXIV150704888; Zheng J., 2014, P 28 AAAI C ART INT; Ziebart B. D., 2008, AAAI, V8, P1433; Ziebart B. D., 2010, THESIS	29	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304042
C	Lee, S; Bareinboim, E		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lee, Sanghack; Bareinboim, Elias			Structural Causal Bandits: Where to Intervene?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MULTIARMED BANDIT	We study the problem of identifying the best action in a sequential decision-making setting when the reward distributions of the arms exhibit a non-trivial dependence structure, which is governed by the underlying causal model of the domain where the agent is deployed. In this setting, playing an arm corresponds to intervening on a set of variables and setting them to specific values. In this paper, we show that whenever the underlying causal model is not taken into account during the decision-making process, the standard strategies of simultaneously intervening on all variables or on all the subsets of the variables may, in general, lead to suboptimal policies, regardless of the number of interventions performed by the agent in the environment. We formally acknowledge this phenomenon and investigate structural properties implied by the underlying causal model, which lead to a complete characterization of the relationships between the arms' distributions. We leverage this characterization to build a new algorithm that takes as input a causal structure and finds a minimal, sound, and complete set of qualified arms that an agent should play to maximize its expected reward. We empirically demonstrate that the new strategy learns an optimal policy and leads to orders of magnitude faster convergence rates when compared with its causal-insensitive counterparts.	[Lee, Sanghack; Bareinboim, Elias] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus	Lee, S (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.	lee2995@purdue.edu; eb@purdue.edu			IBM Research; Adobe Research; NSF [IIS-1704352, IIS-1750807]	IBM Research(International Business Machines (IBM)); Adobe Research; NSF(National Science Foundation (NSF))	This research is supported in parts by grants from IBM Research, Adobe Research, NSF IIS-1704352, and IIS-1750807 (CAREER).	Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bareinboim E., 2015, ADV NEURAL INFORM PR, V28, P1342; Bareinboim E, 2016, P NATL ACAD SCI USA, V113, P7345, DOI 10.1073/pnas.1510507113; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Combes R, 2014, PR MACH LEARN RES, V32; Dani V, 2008, P C LEARN THEOR COLT, P355; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Forney A, 2017, PR MACH LEARN RES, V70; Garivier A., 2011, P 24 ANN C LEARNING, V24, P359; Kaufmann Emilie, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P199, DOI 10.1007/978-3-642-34106-9_18; Kocaoglu M., 2017, ADV NEURAL INFORM PR, P7021, DOI [10.5555/3295222.3295445, DOI 10.5555/3295222.3295445]; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lattimore  Finnian, ADV NEURAL INFORM PR, V29, P1181; Lee  Sanghack, 2018, R36 PURD U DEP COMP; Magureanu S., 2014, P C LEARN THEOR COLT, P975; Ortega PA, 2014, COMPLEX ADAPT SYST M, V2, DOI 10.1186/2194-3206-2-2; Pearl J, 1995, BIOMETRIKA, V82, P669, DOI 10.1093/biomet/82.4.669; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Sen R, 2017, PR MACH LEARN RES, V70; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Szepesvari C, 2010, ALGORITHMS REINFORCE, V4; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Tian J, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P567; Zhang JZ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1340	28	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302057
C	Li, CX; Welling, M; Zhu, J; Zhang, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Chongxuan; Welling, Max; Zhu, Jun; Zhang, Bo			Graphical Generative Adversarial Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose Graphical Generative Adversarial Networks (Graphical-GAN) to model structured data. Graphical-GAN conjoins the power of Bayesian networks on compactly representing the dependency structures among random variables and that of generative adversarial networks on learning expressive dependency functions. We introduce a structured recognition model to infer the posterior distribution of latent variables given observations. We generalize the Expectation Propagation (EP) algorithm to learn the generative model and recognition model jointly. Finally, we present two important instances of Graphical-GAN, i.e. Gaussian Mixture GAN (GMGAN) and State Space GAN (SSGAN), which can successfully learn the discrete and temporal structures on visual datasets, respectively.	[Li, Chongxuan; Zhu, Jun; Zhang, Bo] Tsinghua Univ, State Key Lab Intell Tech & Sys, Dept Comp Sci & Technol, Inst Artificial Intelligence,BNRist Ctr,THBI Lab, Beijing, Peoples R China; [Welling, Max] Univ Amsterdam, Amsterdam, Netherlands; [Welling, Max] Canadian Inst Adv Res CIFAR, Toronto, ON, Canada	Tsinghua University; University of Amsterdam; Canadian Institute for Advanced Research (CIFAR)	Zhu, J (corresponding author), Tsinghua Univ, State Key Lab Intell Tech & Sys, Dept Comp Sci & Technol, Inst Artificial Intelligence,BNRist Ctr,THBI Lab, Beijing, Peoples R China.	licx14@mails.tsinghua.edu.cn; M.Welling@uva.nl; dcszj@mail.tsinghua.edu.cn; dcszb@mail.tsinghua.edu.cn			National Key Research and Development Program of China [2017YFA0700900]; National NSF of China [61620106010, 61621136008, 61332007]; MIIT Grant of Int. Man. Comp. Stan [2016ZXFB00001]; Youth Top-notch Talent Support Program; Tsinghua Tiangong Institute for Intelligent Computing; NVIDIA NVAIL Program; Project from Siemens; China Scholarship Council	National Key Research and Development Program of China; National NSF of China(National Natural Science Foundation of China (NSFC)); MIIT Grant of Int. Man. Comp. Stan; Youth Top-notch Talent Support Program; Tsinghua Tiangong Institute for Intelligent Computing; NVIDIA NVAIL Program; Project from Siemens; China Scholarship Council(China Scholarship Council)	The work was supported by the National Key Research and Development Program of China (No. 2017YFA0700900), the National NSF of China (Nos. 61620106010, 61621136008, 61332007), the MIIT Grant of Int. Man. Comp. Stan (No. 2016ZXFB00001), the Youth Top-notch Talent Support Program, Tsinghua Tiangong Institute for Intelligent Computing, the NVIDIA NVAIL Program and a Project from Siemens. This work was done when C. Li visited the university of Amsterdam. During this period, he was supported by China Scholarship Council.	Abadi M, 2015, P 12 USENIX S OPERAT; Arjovsky M, 2017, PR MACH LEARN RES, V70; Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Dilokthanakul Nat, 2016, ARXIV161102648; Donahue J., 2016, ARXIV160509782; Dumoulin Vincent, 2016, ARXIV E PRINTS; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Huszar F., 2017, ARXIV170208235; Jang E., 2016, ARXIV; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kalchbrenner N, 2016, ARXIV161000527; Karaletsos T., 2016, ARXIV161205048; Kingma D. P., 2013, AUTO ENCODING VARIAT; Koller D., 2009, PROBABILISTIC GRAPHI; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Yingzhen, 2015, ADV NEURAL INFORM PR, V28, P2323, DOI DOI 10.17863/CAM.21346; Lin  Wu, 2018, ARXIV180305589; Liu M.-Y., 2017, ARXIV170704993; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Makhzani A., 2015, ARXIV151105644; Mathieu Michael, 2015, ARXIV151105440; Mescheder L, 2017, PR MACH LEARN RES, V70; Minka, 2005, DIVERGENCE MEASURES; Minka T.P., 2001, P 17 C UNC ART INT, P362; Mohamed Shakir, 2016, ARXIV161003483; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Nowozin S, 2016, ADV NEUR IN, V29; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Saatci Y., 2017, ADV NEURAL INFORM PR, P3622; Saito  Masaki, 2016, ARXIV161106624; Salimans T, 2016, ADV NEUR IN, V29; Springenberg Jost Tobias, 2015, ARXIV151106390; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Stuhlmuller Andreas, 2013, ADV NEURAL INFORM PR, P3048; Tolstikhin  Ilya, 2017, ARXIV1711015582; Tran D, 2017, ADV NEUR IN, V30; Villegas Ruben, 2017, ICLR, DOI DOI 10.48550/ARXIV.1706.08033; Warde-Farley David, 2016, IMPROVING GENERATIVE; Xue Tianfan, 2016, ADV NEURAL INFORM PR, P2	46	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000056
C	Li, P; He, N; Milenkovic, O		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Pan; He, Niao; Milenkovic, Olgica			Quadratic Decomposable Submodular Function Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce a new convex optimization problem, termed quadratic decomposable submodular function minimization. The problem is closely related to decomposable submodular function minimization and arises in many learning on graphs and hypergraphs settings, such as graph-based semi-supervised learning and PageRank. We approach the problem via a new dual strategy and describe an objective that may be optimized via random coordinate descent (RCD) methods and projections onto cones. We also establish the linear convergence rate of the RCD algorithm and develop efficient projection algorithms with provable performance guarantees. Numerical experiments in semi-supervised learning on hypergraphs confirm the efficiency of the proposed algorithm and demonstrate the significant improvements in prediction accuracy with respect to state-of-the-art methods.(1)	[Li, Pan; He, Niao; Milenkovic, Olgica] UIUC, Champaign, IL 61820 USA	University of Illinois System; University of Illinois Urbana-Champaign	Li, P (corresponding author), UIUC, Champaign, IL 61820 USA.	panli2@illinois.edu; niaohe@illinois.edu; milenkov@illinois.edu			NIH [1u01 CA198943A]; NSF [CCF 15-27636]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF))	The authors gratefully acknowledge many useful suggestions by the reviewers. This work was supported in part by the NIH grant 1u01 CA198943A and the NSF grant CCF 15-27636.	[Anonymous], 2017, P NEURAL INFORM PROC; Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039; Chakrabarty D., 2014, ADV NEURAL INFORM PR, V1, P802, DOI 10.48550/arXiv.1411.0095; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chambolle A, 2009, INT J COMPUT VISION, V84, P288, DOI 10.1007/s11263-009-0238-9; Chan THH, 2018, J ACM, V65, DOI 10.1145/3178123; Chan T-H Hubert, 2017, ARXIV171101560; Ene A, 2017, ADV NEURAL INFORM PR, P2874, DOI [10.5555/3294996.3295047, DOI 10.5555/3294996.3295047]; Ene A, 2015, PR MACH LEARN RES, V37, P787; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Fujishige S, 2011, PAC J OPTIM, V7, P3; Gammerman A., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P148; Gleich DF, 2015, SIAM J MATRIX ANAL A, V36, P1507, DOI 10.1137/140985160; Hein M., 2013, P ADV NEUR INF PROC, V26; Jegelka S., 2013, ADV NEURAL INFO PROC, P1313; Joachims T., 2003, P 20 INT C MACH LEAR, P290, DOI DOI 10.1145/2612669.2612699; Johnson R, 2007, J MACH LEARN RES, V8, P1489; Kumar S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1150, DOI 10.1109/ICCV.2003.1238478; Li P., 2018, ADV NEURAL INFORM PR; Li Pan, 2018, P INT C MACH LEARN; Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10; Nishihara R., 2014, ADV NEURAL INFO PROC, P640; Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412; Page L., 1999, TECH REP; Shor N. Z., 2012, MINIMIZATION METHODS, V3; Stobbe P., 2010, ADV NEURAL INFO PROC, P2208; Yoshida  Y., 2017, ARXIV170808781; Zhang CZ, 2017, PR MACH LEARN RES, V70; Zhou D, 2006, P 2006 C ADV NEURAL, V19, DOI 10.7551/mitpress/7503.003.0205; Zhou DY, 2004, ADV NEUR IN, V16, P321; Zhu X., 2003, P ICML WORKSH CONT L, V3; Zhu Xiaojin., 2003, P ICLR, P912	33	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301008
C	Lim, CH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lim, Cong Han			An Efficient Pruning Algorithm for Robust Isotonic Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CONVEX-FUNCTIONS SUBJECT	We study a generalization of the classic isotonic regression problem where we allow separable nonconvex objective functions, focusing on the case where the functions are estimators used in robust regression. One can solve this problem to within epsilon-accuracy (of the global minimum) in O(n/epsilon) using a simple dynamic program, and the complexity of this approach is independent of the underlying functions. We introduce an algorithm that combines techniques from the convex case with branch-and-bound ideas that is able to exploit the shape of the functions. Our algorithm achieves the best known bounds for both the convex case (O(n log( 1/epsilon ))) and the general nonconvex case. Experiments show that this algorithm can perform much faster than the dynamic programming approach on robust estimators, especially as the desired accuracy increases.	[Lim, Cong Han] Georgia Tech, Sch Ind Syst & Engn, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Lim, CH (corresponding author), Georgia Tech, Sch Ind Syst & Engn, Atlanta, GA 30332 USA.	clim31@gatech.edu			NSF [CMMI-1634597]; DIMACS/Simons Collaboration on Bridging Continuous and Discrete Optimization through NSF [CCF-1740425]; NSF Award at UW-Madison [IIS-1447449]	NSF(National Science Foundation (NSF)); DIMACS/Simons Collaboration on Bridging Continuous and Discrete Optimization through NSF; NSF Award at UW-Madison(National Science Foundation (NSF))	The author would like to thank Alberto Del Pia and Silvia Di Gregorio for initial discussion that lead to this work. The author was partially supported by NSF Award CMMI-1634597, NSF Award IIS-1447449 at UW-Madison. Part of the work was completed while visiting the Simons Institute for the Theory of Computing (partially supported by the DIMACS/Simons Collaboration on Bridging Continuous and Discrete Optimization through NSF Award CCF-1740425).	Ahuja RK, 2001, OPER RES, V49, P784, DOI 10.1287/opre.49.5.784.10601; AYER M, 1955, ANN MATH STAT, V26, P641, DOI 10.1214/aoms/1177728423; Bach F, 2018, ADV NEURAL INFORM PR, V31, P1; Bach F., 2015, ARXIV151100394CSMATH; Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039; Best MJ, 2000, SIAM J OPTIMIZ, V10, P658, DOI 10.1137/S1052623497314970; Bogdan M., 2013, ARXIV13101969, P1; BRUNK HD, 1955, ANN MATH STAT, V26, P607, DOI 10.1214/aoms/1177728420; Burdakov O, 2017, J OPTIMIZ THEORY APP, V172, P929, DOI 10.1007/s10957-017-1060-0; Felzenszwalb P.F., 2012, THEORY COMPUT, V8, P415, DOI DOI 10.4086/TOC.2012.V008A019; Gunasekar Suriya, 2016, ADV NEURAL INFORM PR, P1370; Hampel F. R., 2011, ROBUST STAT APPROACH, V196; Hochbaum DS, 2017, SIAM J OPTIMIZ, V27, P2563, DOI 10.1137/15M1024081; Hochbaum DS, 2001, J ACM, V48, P686, DOI 10.1145/502090.502093; Huber P., 2004, WILEY SERIES PROBABI; Kalai A. T., 2009, C LEARN THEOR; Kolmogorov V, 2016, SIAM J IMAGING SCI, V9, P605, DOI 10.1137/15M1010257; Lim CH, 2016, JMLR WORKSH CONF PRO, V51, P1205; STOUT QF, 2014, FASTEST ISOTONIC REG; Suehiro Daiki, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P260, DOI 10.1007/978-3-642-34106-9_22; Sysoev O, 2016, SMOOTHED MONOTONIC R; Tibshirani RJ, 2011, TECHNOMETRICS, V53, P54, DOI 10.1198/TECH.2010.10111; Yasutake S, 2011, LECT NOTES COMPUT SC, V7074, P534, DOI 10.1007/978-3-642-25591-5_55; Zeng X., 2014, ARXIV14094271	24	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300021
C	Locatello, F; Dresdner, G; Khanna, R; Valera, I; Ratsch, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Locatello, Francesco; Dresdner, Gideon; Khanna, Rajiv; Valera, Isabel; Ratsch, Gunnar			Boosting Black Box Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Approximating a probability density in a tractable manner is a central task in Bayesian statistics. Variational Inference (VI) is a popular technique that achieves tractability by choosing a relatively simple variational approximation. Borrowing ideas from the classic boosting framework, recent approaches attempt to boost VI by replacing the selection of a single density with an iteratively constructed mixture of densities. In order to guarantee convergence, previous works impose stringent assumptions that require significant effort for practitioners. Specifically, they require a custom implementation of the greedy step (called the LMO) for every probabilistic model with respect to an unnatural variational family of truncated distributions. Our work fixes these issues with novel theoretical and algorithmic insights. On the theoretical side, we show that boosting VI satisfies a relaxed smoothness assumption which is sufficient for the convergence of the functional Frank-Wolfe (FW) algorithm. Furthermore, we rephrase the LMO problem and propose to maximize the Residual ELBO (RELBO) which replaces the standard ELBO optimization in VI. These theoretical enhancements allow for black box implementation of the boosting subroutine. Finally, we present a stopping criterion drawn from the duality gap in the classic FW analyses and exhaustive experiments to illustrate the usefulness of our theoretical and algorithmic contributions.	[Locatello, Francesco; Valera, Isabel] Max Planck Inst Intelligent Syst, Stuttgart, Germany; [Locatello, Francesco; Dresdner, Gideon; Ratsch, Gunnar] Swiss Fed Inst Technol, Dept Comp Sci, Univ Str 6, CH-8092 Zurich, Switzerland; [Khanna, Rajiv] Univ Texas Austin, Austin, TX 78712 USA	Max Planck Society; Swiss Federal Institutes of Technology Domain; ETH Zurich; University of Texas System; University of Texas Austin	Locatello, F (corresponding author), Max Planck Inst Intelligent Syst, Stuttgart, Germany.; Locatello, F (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Univ Str 6, CH-8092 Zurich, Switzerland.		Khanna, Rajiv/GPK-2566-2022; Locatello, Francesco/GQY-6025-2022	Khanna, Rajiv/0000-0003-1314-3126; 	Max-Planck ETH Center for Learning Systems; ETH core grant; NSF [IIS 1421729]	Max-Planck ETH Center for Learning Systems; ETH core grant; NSF(National Science Foundation (NSF))	FL is partially supported by the Max-Planck ETH Center for Learning Systems. FL, GD are partially supported by an ETH core grant (to GR). RK is supported by NSF Grant IIS 1421729. We thank Matthias Huser for providing the preprocessed eICU dataset. We also thank David Blei and Anant Raj for helpful discussions.	Blei DM, 2016, ARXIV160100670; Christopher M. B., 2016, PATTERN RECOGN; Fortuin Vincent, 2018, ARXIV180602199; Goldberger AL, 2000, CIRCULATION, V101, pE215, DOI 10.1161/01.CIR.101.23.e215; Guo F., 2016, ARXIV161105559; Jaakkola Tommi S., 1998, IMPROVING MEAN FIELD; Jaggi M., 2011, CONVEX OPTIMIZATION; Jaggi Martin, 2013, ICML 2013 P 30 INT C; Jerfel Ghassen, 2017, BOOSTED STOCHASTIC B; Krishnan Rahul G., 2015, BARRIER FRANK WOLFE, P532; Lacoste-Julien S, 2015, ADV NEURAL INFORM PR, V28, P496; Lacoste-Julien S., 2016, ARXIV PREPRINT ARXIV; Locatello Francesco, 2018, P 21 INT C ART INT S, V84, P464; Locatello Francesco, 2017, P INT C ART INT STAT; Meir R., 2003, Advanced Lectures on Machine Learning. Machine Learning Summer School 2002. Revised Lectures. (Lecture Notes in Artificial Intelligence Vol.2600), P118; Miller Andrew C, 2016, ARXIV161106585; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; Ranganath R, 2014, JMLR WORKSH CONF PRO, V33, P814; Rezende D., 2015, ICML, P1530; Saeedi A, 2017, J MACH LEARN RES, V18, P1; Salakhutdinov R., 2007, ADV NEURAL INF PROCE, V20, P1257; Salimans T., 2015, ICML; Saxena Siddhartha, 2017, ABS170702510 CORR; Tolstikhin Ilya O., 2017, ARXIV170102386; Tran Dustin, 2016, ARXIV161009787	25	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303040
C	Luo, HP; Wei, CY; Zheng, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Luo, Haipeng; Wei, Chen-Yu; Zheng, Kai			Efficient Online Portfolio with Logarithmic Regret	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHMS	We study the decades-old problem of online portfolio management and propose the first algorithm with logarithmic regret that is not based on Cover's Universal Portfolio algorithm and admits much faster implementation. Specifically Universal Portfolio enjoys optimal regret O(N In T) for N financial instruments over T rounds, but requires log-concave sampling and has a large polynomial running time. Our algorithm, on the other hand, ensures a slightly larger but still logarithmic regret of O(N-2 (ln T)(4)), and is based on the well-studied Online Mirror Descent framework with a novel regularizer that can be implemented via standard optimization methods in time O (TN2.5) per round. The regret of all other existing works is either polynomial in T or has a potentially unbounded factor such as the inverse of the smallest price relative.	[Luo, Haipeng; Wei, Chen-Yu] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA; [Zheng, Kai] Peking Univ, Beijing Inst Big Data Res, Ctr Data Sci, Key Lab Machine Percept,MOE,Sch EECS, Beijing, Peoples R China	University of Southern California; Peking University	Luo, HP (corresponding author), Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.	haipeng1@usc.edu; chenyu.wei@usc.edu; zhengk92@pku.edu.cn			China Scholarship Council; NSF [1755781]	China Scholarship Council(China Scholarship Council); NSF(National Science Foundation (NSF))	The authors would like to thank Tim van Erven for introducing the problem and to thank Tim van Erven, Dirk van der Hoeven, and Wouter Koolen for helpful discussions throughout the projects, especially on the FTRL approach. The work was done while KZ visited the University of Southern California. KZ gratefully acknowledges financial support from China Scholarship Council. HL and CYW are grateful for the support of NSF Grant #1755781.	Abernethy JD, 2012, IEEE T INFORM THEORY, V58, P4164, DOI 10.1109/TIT.2012.2192096; Agarwal A., 2006, P 23 INT C MACHINE L, P9, DOI 10.1145/1143844.1143846; Agarwal Alekh, 2017, C LEARN THEOR, P12; Agarwal Amit, 2005, EL C COMP COMPL; [Anonymous], 2016, FDN TRENDS IN OPTIMI; Boyd S, 2004, CONVEX OPTIMIZATION; BUBECK S, 2015, ARXIV150702564; Bubeck Sebastien, 2018, INT C ALG LEARN THEO; Cover T.M., 1991, MATH FINANC, V1, P1, DOI DOI 10.1111/J.1467-9965.1991.TB00002.X; Cover TM, 1996, AN S FDN CO, P534, DOI 10.1109/SFCS.1996.548512; Foster D. J., 2016, ADV NEURAL INFORM PR, P4734; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Helmbold DP, 1998, MATH FINANC, V8, P325, DOI 10.1111/1467-9965.00058; Kalai A., 2002, J MACHINE LEARNING R, V3, P423; Lovasz L, 2006, ANN IEEE SYMP FOUND, P57; Narayanan H., 2010, ADV NEURAL INFORM PR, P1777; Nesterov Y., 1994, INTERIOR POINT POLYN, V13; Orseau L., 2017, INT C ALGORITHMIC LE, P372; van Erven Tim, 2018, COMMUNICATION; Wei Chen-Yu, 2018, C LEARN THEOR; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	22	3	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002075
C	Lyddon, S; Walker, S; Holmes, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lyddon, Simon; Walker, Stephen; Holmes, Chris			Nonparametric learning from Bayesian models with randomized objective functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				INFERENCE; LIKELIHOOD	Bayesian learning is built on an assumption that the model space contains a true reflection of the data generating mechanism. This assumption is problematic, particularly in complex data environments. Here we present a Bayesian nonparametric approach to learning that makes use of statistical models, but does not assume that the model is true. Our approach has provably better properties than using a parametric model and admits a Monte Carlo sampling scheme that can afford massive scalability on modern computer architectures. The model-based aspect of learning is particularly attractive for regularizing nonparametric inference when the sample size is small, and also for correcting approximate approaches such as variational Bayes (VB). We demonstrate the approach on a number of examples including VB classifiers and Bayesian random forests.	[Lyddon, Simon; Holmes, Chris] Univ Oxford, Dept Stat, Oxford, England; [Walker, Stephen] Univ Texas Austin, Dept Math, Austin, TX 78712 USA	University of Oxford; University of Texas System; University of Texas Austin	Lyddon, S (corresponding author), Univ Oxford, Dept Stat, Oxford, England.	lyddon@stats.ox.ac.uk; s.g.walker@math.utexas.edu; cholmes@stats.ox.ac.uk			EPSRC OxWaSP CDT [EP/L016710/1]; MRC; Alan Turing Institute; Li Ka Shing foundation	EPSRC OxWaSP CDT(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); MRC(UK Research & Innovation (UKRI)Medical Research Council UK (MRC)); Alan Turing Institute; Li Ka Shing foundation	SL is funded by the EPSRC OxWaSP CDT, through EP/L016710/1. CH gratefully acknowledges support for this research from the MRC, The Alan Turing Institute, and the Li Ka Shing foundation.	AKAIKE H, 1981, J ECONOMETRICS, V16, P3, DOI 10.1016/0304-4076(81)90071-3; ANTONIAK CE, 1974, ANN STAT, V2, P1152, DOI 10.1214/aos/1176342871; Bernardo J. M., 2006, BAYESIAN THEORY; Bishop C.M., 2014, ANTIMICROB AGENTS CH, V58, P7250; Bissiri PG, 2016, J R STAT SOC B, V78, P1103, DOI 10.1111/rssb.12158; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Burnham K.P., 2003, MODEL SELECTION MULT; Chamberlain G, 2003, J BUS ECON STAT, V21, P12, DOI 10.1198/073500102288618711; Dheeru D., 2019, UCI MACHINE LEARNING; Fernandez-Delgado M, 2014, J MACH LEARN RES, V15, P3133; Fushiki T, 2005, BERNOULLI, V11, P747, DOI 10.3150/bj/1126126768; Fushiki T, 2010, J STAT PLAN INFER, V140, P65, DOI 10.1016/j.jspi.2009.06.007; Hjort N.L., 2010, BAYESIAN NONPARAMETR, V28, DOI 10.1017/CBO9780511802478; Huber PJ, 1967, P 5 BERKELEY S MATH, P221; Jaakkola Tommi S., 1997, INT WORKSH ART INT S; Kucukelbir  Alp, 2015, ADV NEURAL INFORM PR, V1, P1; LO AY, 1984, ANN STAT, V12, P351, DOI 10.1214/aos/1176346412; Lyddon S. P., 2018, 170907616 ARXIV, P1; Muller UK, 2013, ECONOMETRICA, V81, P1805, DOI 10.3982/ECTA9097; NEWTON MA, 1994, J R STAT SOC B, V56, P3; Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001; Robert C. P., 2005, SPRINGER TEXTS STAT; Robert C. P., 2007, BAYESIAN CHOICE; SETHURAMAN J, 1994, STAT SINICA, V4, P639; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Walker SG, 2013, J STAT PLAN INFER, V143, P1621, DOI 10.1016/j.jspi.2013.05.013	27	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302011
C	Mao, JM; Leme, RP; Schneider, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mao, Jieming; Leme, Renato Paes; Schneider, Jon			Contextual Pricing for Lipschitz Buyers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				BANDITS; REGRET	We investigate the problem of learning a Lipschitz function from binary feedback. In this problem, a learner is trying to learn a Lipschitz function f : [0, 1](d) -> [0, 1] over the course of T rounds. On round t, an adversary provides the learner with an input x(t), the learner submits a guess y(t) for f(x(t)), and learns whether y(t) > f(x(t)) or y(t) <= f (x(t)). The learner's goal is to minimize their total loss Sigma(t) l(f(x(t)), y(t)) (for some loss function l). The problem is motivated by contextual dynamic pricing, where a firm must sell a stream of differentiated products to a collection of buyers with non-linear valuations for the items and observes only whether the item was sold or not at the posted price. For the symmetric loss l(f (x(t)), y(t)) = vertical bar f(x(t)) - y(t)vertical bar, we provide an algorithm for this problem achieving total loss O (log T) when d = 1 and O(T-(d 1)/d when d > 1, and show that both bounds are tight (up to a factor of root log T). For the pricing loss function l(f(x(t)), y(t)) = f(x(t)) - y(t)1{y(t) <= f(x(t)) g we show a regret bound of O(Td/(d+1)) and show that this bound is tight. We present improved bounds in the special case of a population of linear buyers.	[Mao, Jieming] Univ Penn, Philadelphia, PA 19104 USA; [Leme, Renato Paes; Schneider, Jon] Google Res, New York, NY USA	University of Pennsylvania; Google Incorporated	Mao, JM (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.	jiemingm@seas.upenn.edu; renatoppl@google.com; jschnei@google.com						AGRAWAL R, 1995, SIAM J CONTROL OPTIM, V33, P1926, DOI 10.1137/S0363012992237273; Amin K., 2014, ADV NEURAL INFORM PR, P622; Babaioff Moshe, 2015, ACM Transactions on Economics and Computation, V3, DOI 10.1145/2559152; Badanidiyuru A, 2018, J ACM, V65, DOI 10.1145/3164539; Besbes O, 2009, OPER RES, V57, P1407, DOI 10.1287/opre.1080.0640; Broder J, 2012, OPER RES, V60, P965, DOI 10.1287/opre.1120.1057; Cesa-Bianchi Nicolo, 2017, C LEARNING THEORY, P465; Chen YW, 2013, OPER RES, V61, P612, DOI 10.1287/opre.2013.1166; Cohen MC, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P817, DOI 10.1145/2940716.2940728; den Boer AV, 2015, OPER RES, V63, P965, DOI 10.1287/opre.2015.1397; Hazan E, 2007, LECT NOTES COMPUT SC, V4539, P499, DOI 10.1007/978-3-540-72927-3_36; Javanmard A., 2016, ARXIV160907574; Javanmard  Adel, 2017, J MACHINE LEARNING R, V18, P1714; Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232; Kleinberg R, 2005, P 17 INT C NEUR INF, V17, P697; Kleinberg R, 2010, PROC APPL MATH, V135, P827; Kleinberg R, 2008, ACM S THEORY COMPUT, P681; Leme Renato Paes, 2018, ABS180403195 CORR; Lobel I., 2017, OPER RES; Qiang S., 2016, DYNAMIC PRICING DEMA; SLIVKINS A, 2011, ADV NEURAL INFORM PR, P1602; Wang ZZ, 2014, OPER RES, V62, P318, DOI 10.1287/opre.2013.1245	24	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000017
C	Marino, J; Cvitkovic, M; Yue, YS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Marino, Joseph; Cvitkovic, Milan; Yue, Yisong			A General Method for Amortizing Variational Filtering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce the variational filtering EM algorithm, a simple, general-purpose method for performing variational inference in dynamical latent variable models using information from only past and present variables, i.e. filtering. The algorithm is derived from the variational objective in the filtering setting and consists of an optimization procedure at each time step. By performing each inference optimization procedure with an iterative amortized inference model, we obtain a computationally efficient implementation of the algorithm, which we call amortized variational filtering. We present experiments demonstrating that this general-purpose method improves performance across several deep dynamical latent variable models.	[Marino, Joseph; Cvitkovic, Milan; Yue, Yisong] CALTECH, Pasadena, CA 91125 USA	California Institute of Technology	Marino, J (corresponding author), CALTECH, Pasadena, CA 91125 USA.	jmarino@caltech.edu; mcvitkovic@caltech.edu; yyue@caltech.edu			JPL [PDF 1584398]; NSF [1564330, 1637598]	JPL; NSF(National Science Foundation (NSF))	We would like to thank Matteo Ruggero Ronchi for helpful discussions. This work was supported by the following grants: JPL PDF 1584398, NSF 1564330, and NSF 1637598.	Andrychowicz M, 2016, ADV NEUR IN, V29; Babaeizadeh Mohammad, 2018, INT C LEARN REPR, V2, P7; Bayer J., 2014, LEARNING STOCHASTIC; Boulanger-Lewandowski N., 2012, P 29 INT C MACH LEAR, P1159, DOI DOI 10.32604/CSSE.2021.014030; Chung J., 2014, ARXIV14123555; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; Denton E, 2018, PR MACH LEARN RES, V80; Friston KJ, 2005, PHILOS T R SOC B, V360, P815, DOI 10.1098/rstb.2005.1622; Garofolo J.S., 1993, DARPA TIMIT ACOUSTIC; Gemici M., 2017, ARXIV PREPRINT ARXIV; Gershman Samuel, 2014, COGNITIVE SCI SOC, V36; Gregor K, 2014, PR MACH LEARN RES, V32, P1242; He JW, 2018, LECT NOTES COMPUT SC, V11209, P466, DOI 10.1007/978-3-030-01228-1_28; Henaff Mikael, 2017, ARXIV171104994; Hinton, 2016, ARXIV PREPRINT ARXIV; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jordan MI, 1998, NATO ADV SCI I D-BEH, V89, P105; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Karl M., 2017, PROC INT C LEARN REP, P1; Kingma D.P, P 3 INT C LEARNING R; Le Tuan Anh, 2018, ICLR; Li Y., 2018, ARXIV; Lotter W., 2017, ICLR, DOI [DOI 10.48550/ARXIV.1605.08104, 10.48550/arXiv.1605.08104]; Marino J, 2018, PR MACH LEARN RES, V80; Naesseth CA, 2018, PR MACH LEARN RES, V84; Ranganath R., 2014, ARTIFICIAL INTELLIGE, P814; Rao Rajesh PN, 1999, NATURE NEUROSCIENCE, V2; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Sarkka S., 2013, BAYESIAN FILTERING S; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Sonderby CK, 2016, ADV NEUR IN, V29; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Walker J, 2016, LECT NOTES COMPUT SC, V9911, P835, DOI 10.1007/978-3-319-46478-7_51; Xue Tianfan, 2016, ADV NEURAL INFORM PR, P2	43	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002041
C	Mendez, JA; Shivkumar, S; Eaton, E		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mendez, Jorge A.; Shivkumar, Shashank; Eaton, Eric			Lifelong Inverse Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Methods for learning from demonstration (LfD) have shown success in acquiring behavior policies by imitating a user. However, even for a single task, LfD may require numerous demonstrations. For versatile agents that must learn many tasks via demonstration, this process would substantially burden the user if each task were learned in isolation. To address this challenge, we introduce the novel problem of lifelong learning from demonstration, which allows the agent to continually build upon knowledge learned from previously demonstrated tasks to accelerate the learning of new tasks, reducing the amount of demonstrations required. As one solution to this problem, we propose the first lifelong learning approach to inverse reinforcement learning, which learns consecutive tasks via demonstration, continually transferring knowledge between tasks to improve performance.	[Mendez, Jorge A.; Shivkumar, Shashank; Eaton, Eric] Univ Penn, Dept Comp & Informat Sci, Philadelphia, PA 19104 USA	University of Pennsylvania	Mendez, JA (corresponding author), Univ Penn, Dept Comp & Informat Sci, Philadelphia, PA 19104 USA.	mendezme@seas.upenn.edu; shashs@seas.upenn.edu; eeaton@seas.upenn.edu	Mendez, Jorge/AAS-5542-2021		AFRL [FA8750-16-1-0109]; DARPA [FA8750-18-2-0117]	AFRL(United States Department of DefenseUS Air Force Research Laboratory); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This research was partly supported by AFRL grant #FA8750-16-1-0109 and DARPA agreement #FA8750-18-2-0117. We would like to thank the anonymous reviewers for their helpful feedback.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Ammar HB, 2014, PR MACH LEARN RES, V32, P1206; Ammar Haitham Bou, 2013, P 11 EUR WORKSH MULT; Ammar Haitham Bou, 2013, P 2013 EUR C MACH LE; Ammar Haitham Bou, 2011, P AD LEARN AG WORKSH; Ammar Haitham Bou, 2015, P 29 C ART INT AAAI; Babes Monica, 2011, ICML; Boularias A., 2011, PROC 14 INT C ARTIF, P182; Chen Z., 2016, SYNTHESIS LECT ARTIF, V10; Choi J., 2012, ADV NEURAL INFORM PR; Dimitrakakis Christos, 2011, P 9 EUR WORKSH REINF; Fachantidis Anestis, 2015, ADAPTIVE BEHAV; Fachantidis Anestis, 2011, P 9 EUR WORKSH REINF; Finn Chelsea, 2017, P 5 INT C LEARN REPR; Konidaris George, 2012, J MACHINE LEARNING R; Kumar A, 2012, P 29 INT C MACH LEAR; Levine S., 2011, C NEURAL INFORM PROC, V24, P19; Levine S., 2012, INT C MACHINE LEARNI, P475; Luo Yong, 2016, P 25 INT JOINT C ART; Luo Yong, 2017, P 26 INT JOINT C ART; MacGlashan James, 2016, BROWN UMBC REINFORCE; Mangin Olivier, 2013, FEATURE LEARNING MUL; Melo Francisco S., 2010, P 2010 EUR C MACH LE; Neu Gergely, 2007, P 23 C UNC ART INT U; Qiao Qifeng, 2011, P 2011 AM CONTR C AC; Ramachandran Deepak, 2007, P 20 INT JOINT C ART; RATLIFF N, 2006, P 23 INT C MACH LEAR; Rothkopf Constantin A., 2011, P 2011 EUR C MACH LE; Ruvolo, 2015, P 24 INT JOINT C ART; Ruvolo P., 2013, P 30 INT C MACHINE L, P507; Silver David, 2009, P 14 INT S ROB RES I; Sorg Jonathan, 2009, P 8 INT C AUT AG MUL; Syed U., 2007, ADV NEURAL INFORM PR; Tanwani Ajay Kumar, 2013, P 2013 INT C INT ROB; Taylor M.E., 2007, P INT C MACH LEARN, P879, DOI 10.1145/1273496.1273607; Taylor Matthew E., 2007, P 6 INT C AUT AG MUL; Taylor Matthew E., 2008, P 7 INT C AUT AG MUL; Wulfmeier M., 2015, ARXIV150704888; Ziebart Brian D., 2008, P 23 C ART INT AAAI	43	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304051
C	Menon, AK; Williamson, RC		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Menon, Aditya Krishna; Williamson, Robert C.			A loss framework for calibrated anomaly detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CLASSIFICATION; SUPPORT; RATES; RISK; CONVERGENCE; CONSISTENCY	Given samples from a distribution, anomaly detection is the problem of determining if a given point lies in a low-density region. This paper concerns calibrated anomaly detection, which is the practically relevant extension where we additionally wish to produce a confidence score for a point being anomalous. Building on a classification framework for standard anomaly detection, we show how minimisation of a suitable proper loss produces density estimates only for anomalous instances. These are shown to naturally relate to the pinball loss, which provides implicit quantile control. Finally, leveraging a result from point processes, we show how to efficiently optimise a special case of the objective with kernelised scores. Our framework is shown to incorporate a close relative of the one-class SVM as a special case.	[Menon, Aditya Krishna; Williamson, Robert C.] Australian Natl Univ, Canberra, ACT, Australia; [Menon, Aditya Krishna] Google Res, New York, NY USA	Australian National University; Google Incorporated	Menon, AK (corresponding author), Australian Natl Univ, Canberra, ACT, Australia.; Menon, AK (corresponding author), Google Res, New York, NY USA.	aditya.menon@anu.edu.au; bob.williamson@anu.edu.au			Australian Research Council	Australian Research Council(Australian Research Council)	This work was supported by the Australian Research Council. It was motivated in part from a discussion with Zahra Ghafoori, whom we thank.	Agarwal  Shivani, 2013, C LEARN THEOR, P338; Audibert JY, 2007, ANN STAT, V35, P608, DOI 10.1214/009053606000001217; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Breunig MM, 2000, SIGMOD REC, V29, P93, DOI 10.1145/335191.335388; Buja A., 2005, LOSS FUNCTIONS BINAR; Cheema P, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1813, DOI 10.1145/2983323.2983359; Chen YT, 2013, INT CONF ACOUST SPEE, P3567, DOI 10.1109/ICASSP.2013.6638322; Clemencon S., 2013, ARTIF INTELL, P659; Ehm W, 2016, J R STAT SOC B, V78, P505, DOI 10.1111/rssb.12154; Ertekin S, 2011, J MACH LEARN RES, V12, P2905; Fan W, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P123, DOI 10.1109/ICDM.2001.989509; Flaxman S, 2017, PR MACH LEARN RES, V54, P270; Gao J, 2006, IEEE DATA MINING, P212; Goix  Nicolas, 2015, INT C ART INT STAT; Hand DJ, 2003, AM STAT, V57, P124, DOI 10.1198/0003130031423; Hastie T, 2009, ELEMENTS STAT LEARNI; Kanamori T, 2009, J MACH LEARN RES, V10, P1391; Kriegel HP, 2011, P 2011 SIAM INT C DA, P13, DOI DOI 10.1137/1.9781611972818.2; Mccullagh P, 2006, ADV APPL PROBAB, V38, P873, DOI 10.1239/aap/1165414583; Menon AK, 2016, PR MACH LEARN RES, V48; Pimentel MAF, 2014, SIGNAL PROCESS, V99, P215, DOI 10.1016/j.sigpro.2013.12.026; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Reid MD, 2011, J MACH LEARN RES, V12, P731; Reid MD, 2010, J MACH LEARN RES, V11, P2387; Ripley BD., 1996; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; Scott CD, 2006, J MACH LEARN RES, V7, P665; Sotiris VA, 2010, IEEE T RELIAB, V59, P277, DOI 10.1109/TR.2010.2048740; Steinwart I, 2005, J MACH LEARN RES, V6, P211; Steinwart I., 2014, JMLR WORKSH C P, V35, P482; STONE CJ, 1980, ANN STAT, V8, P1348, DOI 10.1214/aos/1176345206; Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49; Tsybakov AB, 1997, ANN STAT, V25, P948, DOI 10.1214/aos/1069362732; Vert R, 2006, J MACH LEARN RES, V7, P817; Walder CJ, 2017, PR MACH LEARN RES, V70; Zhang T, 2004, ANN STAT, V32, P56; Zhao M., 2009, ADV NEURAL INFORM PR, P2250	39	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301047
C	Ocko, SA; Lindsey, J; Ganguli, S; Deny, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ocko, Samuel A.; Lindsey, Jack; Ganguli, Surya; Deny, Stephane			The emergence of multiple retinal cell types through efficient coding of natural movies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MIDGET GANGLION-CELLS; RECEPTIVE-FIELDS; INFORMATION; STATISTICS; MORPHOLOGY; PARASOL; IMAGES; ENERGY	One of the most striking aspects of early visual processing in the retina is the immediate parcellation of visual information into multiple parallel pathways, formed by different retinal ganglion cell types each tiling the entire visual field. Existing theories of efficient coding have been unable to account for the functional advantages of such cell-type diversity in encoding natural scenes. Here we go beyond previous theories to analyze how a simple linear retinal encoding model with different convolutional cell types efficiently encodes naturalistic spatiotemporal movies given a fixed firing rate budget. We find that optimizing the receptive fields and cell densities of two cell types makes them match the properties of the two main cell types in the primate retina, midget and parasol cells, in terms of spatial and temporal sensitivity, cell spacing, and their relative ratio. Moreover, our theory gives a precise account of how the ratio of midget to parasol cells decreases with retinal eccentricity. Also, we train a nonlinear encoding model with a rectifying nonlinearity to efficiently encode naturalistic movies, and again find emergent receptive fields resembling those of midget and parasol cells that are now further subdivided into ON and OFF types. Thus our work provides a theoretical justification, based on the efficient coding of natural movies, for the existence of the four most dominant cell types in the primate retina that together comprise 70% of all ganglion cells.	[Ocko, Samuel A.; Lindsey, Jack; Deny, Stephane] Dept Appl Phys, Stanford, CA 94305 USA; [Ganguli, Surya] Google Brain, Mountain View, CA USA	Google Incorporated	Ocko, SA; Deny, S (corresponding author), Dept Appl Phys, Stanford, CA 94305 USA.	samocko@gmail.com; stephane.deny.pro@gmail.com			Karel Urbanek Postdoctoral fellowship; NIH Brain Initiative [U01-NS094288]; Simons Foundations; Burroughs-Wellcome Foundation; McKnight Foundation; James S. McDonnell Foundation; Office of Naval Research	Karel Urbanek Postdoctoral fellowship; NIH Brain Initiative; Simons Foundations; Burroughs-Wellcome Foundation(Burroughs Wellcome Fund); McKnight Foundation; James S. McDonnell Foundation; Office of Naval Research(Office of Naval Research)	We thank Alexandra Kling and E.J. Chichilnisky for useful discussions, and for providing us with receptive field visualizations of real midget and parasol cells. We thank Gabriel Mel for a helpful insight about the two-cell proof. We thank the Karel Urbanek Postdoctoral fellowship (S.O) and the NIH Brain Initiative U01-NS094288 (S.D), and the Burroughs-Wellcome, McKnight, James S. McDonnell, and Simons Foundations, and the Office of Naval Research (S.G) for support.	[Anonymous], 2019, INT C LEARN RE UNPUB; ATICK JJ, 1992, NEURAL COMPUT, V4, P196, DOI 10.1162/neco.1992.4.2.196; Atick JJ, 1990, NEURAL COMPUT, V2, P308, DOI 10.1162/neco.1990.2.3.308; Balasubramanian V, 2002, NETWORK-COMP NEURAL, V13, P531, DOI 10.1088/0954-898X/13/4/306; Barlow H, 1961, POSSIBLE PRINCIPLES; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Chalk M, 2018, P NATL ACAD SCI USA, V115, P186, DOI 10.1073/pnas.1711114115; Cover TM, 2006, ELEMENTS INFORM THEO; CRONER LJ, 1993, P NATL ACAD SCI USA, V90, P8128, DOI 10.1073/pnas.90.17.8128; Dacey D. M., 1994, CIBA F SYMP, V184; Dacey D, 2004, COGNITIVE NEUROSCIENCES III, THIRD EDITION, P281; DACEY DM, 1994, CIBA F SYMP, V184, P12; DACEY DM, 1992, P NATL ACAD SCI USA, V89, P9666, DOI 10.1073/pnas.89.20.9666; DACEY DM, 1993, J NEUROSCI, V13, P5334, DOI 10.1523/JNEUROSCI.13-12-05334.1993; DACEY DM, 1994, CIBA F SYMP, V184, P28; Deny S, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-02159-y; DeVries SH, 1997, J NEUROPHYSIOL, V78, P2048, DOI 10.1152/jn.1997.78.4.2048; Doi E, 2012, J NEUROSCI, V32, P16256, DOI 10.1523/JNEUROSCI.4036-12.2012; DONG DW, 1995, NETWORK-COMP NEURAL, V6, P345, DOI 10.1088/0954-898X/6/3/003; Dong DW, 2001, MOTION VISION, P371; Dong DW, 1997, ADV NEUR IN, V9, P859; FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379; Field GD, 2007, J NEUROSCI, V27, P13261, DOI 10.1523/JNEUROSCI.3437-07.2007; Field GD, 2010, NATURE, V467, P673, DOI 10.1038/nature09424; Gauthier JL, 2009, J NEUROSCI, V29, P4675, DOI 10.1523/JNEUROSCI.5294-08.2009; Geffen MN, 2007, PLOS BIOL, V5, P640, DOI 10.1371/journal.pbio.0050065; Gjorgjieva J, 2014, J NEUROSCI, V34, P12127, DOI 10.1523/JNEUROSCI.1032-14.2014; Gollisch T, 2010, NEURON, V65, P150, DOI 10.1016/j.neuron.2009.12.009; Izenman A. J., 1975, Journal of Multivariate Analysis, V5, P248, DOI 10.1016/0047-259X(75)90042-1; KAPLAN E, 1986, P NATL ACAD SCI USA, V83, P2755, DOI 10.1073/pnas.83.8.2755; Karklin Yan, 2011, Adv Neural Inf Process Syst, V24, P999; Kastner DB, 2015, P NATL ACAD SCI USA, V112, P2533, DOI 10.1073/pnas.1418092112; Laughlin SB, 2001, CURR OPIN NEUROBIOL, V11, P475, DOI 10.1016/S0959-4388(00)00237-3; LEE BB, 1994, VISION RES, V34, P3081, DOI 10.1016/0042-6989(94)90074-4; Lettvin Jerome Y., 1968, MIND BIOL APPROACHES, P233; Li Z, 2014, UNDERSTANDING VISION; MERIGAN WH, 1991, J NEUROSCI, V11, P3422; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Palmer SE, 2015, P NATL ACAD SCI USA, V112, P6908, DOI 10.1073/pnas.1506855112; Ratliff CP, 2010, P NATL ACAD SCI USA, V107, P17368, DOI 10.1073/pnas.1005846107; Tikidji-Hamburyan A, 2015, NAT NEUROSCI, V18, P66, DOI 10.1038/nn.3891; VANHATEREN JH, 1993, VISION RES, V33, P257, DOI 10.1016/0042-6989(93)90163-Q; VANHATEREN JH, 1992, J COMP PHYSIOL A, V171, P157, DOI 10.1007/BF00188924; Vincent BT, 2003, VISION RES, V43, P1283, DOI 10.1016/S0042-6989(03)00096-8; Wassle H, 2004, NAT REV NEUROSCI, V5, P747, DOI 10.1038/nrn1497; Zhang YF, 2012, P NATL ACAD SCI USA, V109, pE2391, DOI 10.1073/pnas.1211547109	46	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003090
C	Pillaud-Vivien, L; Rudi, A; Bach, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Pillaud-Vivien, Loucas; Rudi, Alessandro; Bach, Francis			Statistical Optimality of Stochastic Gradient Descent on Hard Learning Problems through Multiple Passes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				APPROXIMATION; RATES	We consider stochastic gradient descent (SGD) for least-squares regression with potentially several passes over the data. While several passes have been widely reported to perform practically better in terms of predictive performance on unseen data, the existing theoretical analysis of SGD suggests that a single pass is statistically optimal. While this is true for low-dimensional easy problems, we show that for hard problems, multiple passes lead to statistically optimal predictions while single pass does not; we also show that in these hard models, the optimal number of passes over the data increases with sample size. In order to define the notion of hardness and show that our predictive performances are optimal, we consider potentially infinite-dimensional models and notions typically associated to kernel methods, namely, the decay of eigenvalues of the covariance matrix of the features and the complexity of the optimal predictor as measured through the covariance matrix. We illustrate our results on synthetic experiments with non-linear kernel methods and on a classical benchmark with a linear model.	[Pillaud-Vivien, Loucas; Rudi, Alessandro; Bach, Francis] PSL Res Univ, Ecole Normale Super, INRIA, Paris, France	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Pillaud-Vivien, L (corresponding author), PSL Res Univ, Ecole Normale Super, INRIA, Paris, France.	loucas.pillaud-vivien@inria.fr; alessandro.rudi@inria.fr; francis.bach@inria.fr			European Research Council [SEQUOIA 724063]	European Research Council(European Research Council (ERC)European Commission)	We acknowledge support from the European Research Council (grant SEQUOIA 724063). We also thank Raphael Berthier and Yann Labbe for their enlightening advices on this project.	Adams R. A., 2003, SOBOLEV SPACES, VSecond; Aguech R, 2000, SIAM J CONTROL OPTIM, V39, P872, DOI 10.1137/S0363012998333852; [Anonymous], 2015, ADV NEURAL INFORM PR; BACH F, 2017, J MACHINE LEARNING R, V18; Bach F., 2013, ADV NEURAL INFORM PR, V26, P773; Blanchard G, 2016, ANAL APPL, V14, P763, DOI 10.1142/S0219530516400017; Blanchard Gilles, 2017, FDN COMPUTATIONAL MA, P1; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Carratino Luigi, 2018, ADV NEURAL INFORM PR, P10213; Ciliberto C., 2017, ADV NEURAL INFORM PR, P1986; Ciliberto Carlo, 2018, ARXIV180602402; Ciliberto Carlo, 2016, ADV NEURAL INFORM PR, V29, P4412; Defazio Aaron, 2014, ADV NEURAL INFORM PR; Dieuleveut A., 2017, J MACHINE LEARNING R, V18, P3520; Dieuleveut A, 2016, ANN STAT, V44, P1363, DOI 10.1214/15-AOS1391; Fischer S., 2017, SOBOLEV NORM LEARNIN; Gurbuzbalaban Mert, 2015, 151008560 ARXIV; Hardt Moritz, 2016, INT C MACH LEARN; Johoson R., 2013, ADV NEURAL INFORM PR; Lin J., 2018, APPL COMPUTATIONAL H, DOI 10.1016/j.acha.2018.09.009.; LIN JH, 2017, J MACHINE LEARNING R, V18; Lo Gerfo L, 2008, NEURAL COMPUT, V20, P1873, DOI 10.1162/neco.2008.05-07-517; Osokin Anton, 2017, ADV NEURAL INFORM PR, P302; Pillaud-Vivien L., 2018, P MACH LEARN RES, V75, P250; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Roux N., 2012, ADV NEURAL INFORM PR, V25; Rudi A., 2017, ADV NEURAL INFORM PR, V30, P3888; Rudi A., 2017, ADV NEURAL INFORM PR, V30, P3215; Rudi Alessandro, 2013, ADV NEURAL INFORM PR, P2067; Shamir O, 2016, ADV NEUR IN, V29; Steinwart Ingo, 2009, P COLT; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; Wahba G., 1990, SPLINE MODELS OBSERV; Wendland H., 2004, SCATTERED DATA APPRO, V17; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2; Ying YM, 2008, FOUND COMPUT MATH, V8, P561, DOI 10.1007/s10208-006-0237-y; Zhang C., 2017, P INT C LEARN REPR I	41	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002064
C	Rahimzamani, A; Asnani, H; Viswanath, P; Kannan, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Rahimzamani, Arman; Asnani, Himanshu; Viswanath, Pramod; Kannan, Sreeram			Estimators for Multivariate Information Measures in General Probability Spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				FEATURE-SELECTION; DIRECTED INFORMATION; ENTROPY ESTIMATORS; FUNCTIONALS; INFERENCE	Information theoretic quantities play an important role in various settings in machine learning, including causality testing, structure inference in graphical models, time-series problems, feature selection as well as in providing privacy guarantees. A key quantity of interest is the mutual information and generalizations thereof, including conditional mutual information, multivariate mutual information, total correlation and directed information. While the aforementioned information quantities are well defined in arbitrary probability spaces, existing estimators add or subtract entropies (we term them Sigma H methods). These methods work only in purely discrete space or purely continuous case since entropy (or differential entropy) is well defined only in that regime. In this paper, we define a general graph divergence measure (GDM), as a measure of incompatibility between the observed distribution and a given graphical model structure. This generalizes the aforementioned information measures and we construct a novel estimator via a coupling trick that directly estimates these multivariate information measures using the Radon-Nikodym derivative. These estimators are proven to be consistent in a general setting which includes several cases where the existing estimators fail, thus providing the only known estimators for the following settings: (1) the data has some discrete and some continuous valued components (2) some (or all) of the components themselves are discrete-continuous mixtures (3) the data is real-valued but does not have a joint density on the entire space, rather is supported on a low-dimensional manifold. We show that our proposed estimators significantly outperform known estimators on synthetic and real datasets.	[Rahimzamani, Arman; Asnani, Himanshu; Kannan, Sreeram] Univ Washington, Dept ECE, Seattle, WA 98195 USA; [Viswanath, Pramod] Univ Illinois, Dept ECE, Champaign, IL USA	University of Washington; University of Washington Seattle; University of Illinois System; University of Illinois Urbana-Champaign	Rahimzamani, A (corresponding author), Univ Washington, Dept ECE, Seattle, WA 98195 USA.	armanrz@uw.edu; asnani@uw.edu; pramodv@illinois.edu; ksreeram@uw.edu			NSF [1651236, 1703403]; NIH [5R01HG008164]	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This work was partially supported by NSF grants 1651236, 1703403 and NIH grant 5R01HG008164. The authors also would like to thank Yihan Jiang for presenting our work at the NeurIPS conference.	Amblard PO, 2011, J COMPUT NEUROSCI, V30, P7, DOI 10.1007/s10827-010-0231-x; Beirlant J., 1997, INT J MATH STAT SCI, V6, P17; BERNARDO JM, 1976, ROY STAT SOC C-APP, V25, P315, DOI 10.2307/2347257; Brown G., 2009, JMLR WORKSH C P AIST, V5, P49; Chan C., 2016, IEEE T MOL BIOL MULT, V2, P64, DOI DOI 10.1109/TMBMC.2016.2630054; Chan C, 2015, P IEEE, V103, P1883, DOI 10.1109/JPROC.2015.2458316; Cuff P., 2016, P 2016 ACM SIGSAC C, P43, DOI DOI 10.1145/2976749.2978308; DAWID AP, 1979, J ROY STAT SOC B MET, V41, P1; Evans LC., 1992, MEASURE THEORY FINE; Frenzel S, 2007, PHYS REV LETT, V99, DOI 10.1103/PhysRevLett.99.204101; Gao W., 2018, P 27 INT C COMPUTER, P1; Gao Weihao, 2017, NIPS, P5988; Gao Weihao, 2016, P ADV NEUR INF PROC, P2460; Han YJ, 2015, IEEE INT SYMP INFO, P1372, DOI 10.1109/ISIT.2015.7282680; Hlavackova-Schindler K, 2007, PHYS REP, V441, P1, DOI 10.1016/j.physrep.2006.12.004; Hyvarinen A., 2000, NEURAL NETWORKS, V4-5, P411; Jiao J., 2017, ARXIV171108824; Jiao JT, 2015, IEEE T INFORM THEORY, V61, P2835, DOI [10.1109/TIT.2015.2412945, 10.1109/tit.2015.2412945]; Kozachenko L. F., 1987, Problems of Information Transmission, V23, P95; Kraskov A, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066138; Lee I, 2010, NEURAL COMPUT, V22, P2208, DOI 10.1162/neco.2010.02-09-972; Lee J, 2013, PATTERN RECOGN LETT, V34, P349, DOI 10.1016/j.patrec.2012.10.005; Lesniewicz Marek, 2014, Przeglad Elektrotechniczny, V90, P42; Liu Serena, 2016, F1000Res, V5, DOI 10.12688/f1000research.7223.1; Massey J., 1990, P INT S INF THEOR AP, P303; McGill WJ., 1954, PSYCHOMETRIKA, V19, P97, DOI 10.1007/BF02289159; Meyer PE, 2008, IEEE J-STSP, V2, P261, DOI 10.1109/JSTSP.2008.923858; Miller EG, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P297; Nemenman I, 2002, ADV NEUR IN, V14, P471; Noshad M., 2018, ARXIV180100398; Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272; Permuter HH, 2011, IEEE T INFORM THEORY, V57, P3248, DOI 10.1109/TIT.2011.2136270; Qiu XJ, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0049271; Quinn CJ, 2015, IEEE T INFORM THEORY, V61, P6887, DOI 10.1109/TIT.2015.2478440; Rahimzamani A, 2016, ANN ALLERTON CONF, P156, DOI 10.1109/ALLERTON.2016.7852224; Rahimzamani A, 2017, ANN ALLERTON CONF, P1228; Runge, 2017, ARXIV PREPRINT ARXIV; Singh H., 2003, American Journal of Mathematical and Management Sciences, V23, P301; Sricharan K, 2012, IEEE T INFORM THEORY, V58, P4135, DOI 10.1109/TIT.2012.2195549; Sun J, 2015, SIAM J APPL DYN SYST, V14, P73, DOI 10.1137/140956166; Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW); Vejmelka M, 2008, PHYS REV E, V77, DOI 10.1103/PhysRevE.77.026214; Vergara J. R., 2010, J APPL COMPUTER SCI, V2; Vergara JR, 2014, NEURAL COMPUT APPL, V24, P175, DOI 10.1007/s00521-013-1368-0; WATANABE S, 1960, IBM J RES DEV, V4, P66, DOI 10.1147/rd.41.0066; Whittaker J., 2009, GRAPHICAL MODELS APP; Wieczorkowski R, 1999, COMMUN STAT-SIMUL C, V28, P541, DOI 10.1080/03610919908813564; Wu Y., LECT NOTES INFORM TH; Wu YH, 2016, IEEE T INFORM THEORY, V62, P3702, DOI 10.1109/TIT.2016.2548468; Zhang K., 2012, ARXIV12023775	57	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003024
C	Reeb, D; Doerr, A; Gerwinn, S; Rakitsch, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Reeb, David; Doerr, Andreas; Gerwinn, Sebastian; Rakitsch, Barbara			Learning Gaussian Processes by Minimizing PAC-Bayesian Generalization Bounds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Gaussian Processes (GPs) are a generic modelling tool for supervised learning. While they have been successfully applied on large datasets, their use in safety-critical applications is hindered by the lack of good performance guarantees. To this end, we propose a method to learn GPs and their sparse approximations by directly optimizing a PAC-Bayesian bound on their generalization performance, instead of maximizing the marginal likelihood. Besides its theoretical appeal, we find in our evaluation that our learning method is robust and yields significantly better generalization guarantees than other common GP approaches on several regression benchmark datasets.	[Reeb, David; Doerr, Andreas; Gerwinn, Sebastian; Rakitsch, Barbara] Bosch Ctr Artificial Intelligence, Robert Bosch Campus 1, D-71272 Renningen, Germany		Reeb, D (corresponding author), Bosch Ctr Artificial Intelligence, Robert Bosch Campus 1, D-71272 Renningen, Germany.	david.reeb@de.bosch.com; andreas.doerr3@de.bosch.com; sebastian.gerwinn@de.bosch.com; barbara.rakitsch@de.bosch.com						Abadi M, 2015, P 12 USENIX S OPERAT; Ambroladze A., 2007, NIPS; Bauer M., 2016, NIPS; Begin L., 2016, AISTATS; Bui TD, 2017, J MACH LEARN RES, V18; Catoni, 2007, IMS LECT NOTES MONOG, V56, P1, DOI [10.1214/074921707000000391, DOI 10.1214/074921707000000391]; Cheng C.-A., 2017, NIPS; Cherkassky V, 1997, IEEE Trans Neural Netw, V8, P1564, DOI 10.1109/TNN.1997.641482; Dziugaite G. K., 2017, UAI; Germain P., 2016, NIPS; Germain Pascal, 2009, INT C MACH LEARN; Hensman J, 2015, JMLR WORKSH CONF PRO, V38, P351; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694; Langford John, 2002, NIPS; Matthews A. G. de G, 2016, AISTATS; Matthews AGD, 2017, J MACH LEARN RES, V18, P1; Maurer Andreas, 2004, CS0411099 ARXIV; McAllester D., 1999, COLT; McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Seeger M, 2003, J MACH LEARN RES, V3, P233, DOI 10.1162/153244303765208386; Seeger M., 2003, AISTATS; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Shalev-Shwartz S, 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Sheth R., 2017, NIPS; Snelson E., 2005, ADV NEURAL INFORM PR, V18, P1257; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414	29	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303034
C	Rolinek, M; Martius, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Rolinek, Michal; Martius, Georg			L4: Practical loss-based stepsize adaptation for deep learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose a stepsize adaptation scheme for stochastic gradient descent. It operates directly with the loss function and rescales the gradient in order to make fixed predicted progress on the loss. We demonstrate its capabilities by conclusively improving the performance of Adam and Momentum optimizers. The enhanced optimizers with default hyperparameters consistently outperform their constant stepsize counterparts, even the best ones, without a measurable increase in computational cost. The performance is validated on multiple architectures including dense nets, CNNs, ResNets, and the recurrent Differential Neural Computer on classical datasets MNIST, fashion MNIST, CIFAR10 and others.	[Rolinek, Michal; Martius, Georg] Max Planck Inst Intelligent Syst, Tubingen, Germany	Max Planck Society	Rolinek, M (corresponding author), Max Planck Inst Intelligent Syst, Tubingen, Germany.	michal.rolinek@tuebingen.mpg.de; georg.martius@tuebingen.mpg.de						Andrychowicz M, 2016, ADV NEUR IN, V29; [Anonymous], 2014, NEURAL TURING MACHIN; Baydin Atilim Gunes, 2017, ABS170304782 CORR; Byrd RH, 2016, SIAM J OPTIMIZ, V26, P1008, DOI 10.1137/140954362; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Kingma D.P., 2015, INT C LEARN REPR, P1; Krizhevsky Alex, 2009, CIFAR10; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li K., 2016, ABS160601885 CORR; Mahsereci M, 2015, ADV NEURAL INFORM PR, P181; Meier Franziska, 2017, ONLINE LEARNING MEMO; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Polyak B. T, 1987, TRANSLATIONS SERIES; Recht Benjamin, 2017, REFLECTIONS RANDOM K; Recht Benjamin, 2017, GRADIENT DESCENT DOE; Schaul T., 2013, INT C MACHINE LEARNI, P343; Sukhbaatar S., 2015, P 28 INT C NEURAL IN, V28, P2440; TensorFlow GitHub Repository, 2016, TENS IMPL RESNETS; Tieleman Tijmen, 2012, LECT 65 RMSPROP DIVI, V4; Wu Y., 2018, INT C LEARN REPR; Xiao H., 2017, ARXIV 170807747	23	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001001
C	Roy, NA; Bak, JH; Akrami, A; Brody, CD; Pillow, JW		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Roy, Nicholas A.; Bak, Ji Hyun; Akrami, Athena; Brody, Carlos D.; Pillow, Jonathan W.			Efficient inference for time-varying behavior during learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				HISTORY	The process of learning new behaviors over time is a problem of great interest in both neuroscience and artificial intelligence. However, most standard analyses of animal training data either treat behavior as fixed or track only coarse performance statistics (e.g., accuracy, bias), providing limited insight into the evolution of the policies governing behavior. To overcome these limitations, we propose a dynamic psychophysical model that efficiently tracks trial-to-trial changes in behavior over the course of training. Our model consists of a dynamic logistic regression model, parametrized by a set of time-varying weights that express dependence on sensory stimuli as well as task-irrelevant covariates, such as stimulus, choice, and answer history. Our implementation scales to large behavioral datasets, allowing us to infer 500K parameters (e.g., 10 weights over 50K trials) in minutes on a desktop computer. We optimize hyperparameters governing how rapidly each weight evolves over time using the decoupled Laplace approximation, an efficient method for maximizing marginal likelihood in non-conjugate models. To illustrate performance, we apply our method to psychophysical data from both rats and human subjects learning a delayed sensory discrimination task. The model successfully tracks the psychophysical weights of rats over the course of training, capturing day-to-day and trial-to-trial fluctuations that underlie changes in performance, choice bias, and dependencies on task history. Finally, we investigate why rats frequently make mistakes on easy trials, and suggest that apparent lapses can be explained by sub-optimal weighting of known task covariates.	[Roy, Nicholas A.; Akrami, Athena; Brody, Carlos D.; Pillow, Jonathan W.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA; [Bak, Ji Hyun] Korea Inst Adv Study, Seoul, South Korea; [Akrami, Athena; Brody, Carlos D.] Howard Hughes Med Inst, Chevy Chase, MD USA; [Brody, Carlos D.] Princeton Univ, Dept Mol Biol, Princeton, NJ 08544 USA; [Pillow, Jonathan W.] Princeton Univ, Dept Psychol, Princeton, NJ 08544 USA; [Akrami, Athena] UCL, Sainsbury Wellcome Ctr, London, England	Princeton University; Korea Institute for Advanced Study (KIAS); Howard Hughes Medical Institute; Princeton University; Princeton University; University of London; University College London	Roy, NA (corresponding author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.	nroy@princeton.edu; jhbak@kias.re.kr; athena.akrami@ucl.ac.uk; brody@princeton.edu; pillow@princeton.edu		Bak, Ji Hyun/0000-0002-5700-7823	Simons Foundation [SCGB AWD1004351, AWD543027]; NIH [R01EY017366, R01NS104899]; U19 NIH-NINDS BRAIN Initiative Award [NS104648-01]	Simons Foundation; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); U19 NIH-NINDS BRAIN Initiative Award	This work was supported by grants from the Simons Foundation (SCGB AWD1004351 and AWD543027), the NIH (R01EY017366, R01NS104899) and a U19 NIH-NINDS BRAIN Initiative Award (NS104648-01).	Abrahamyan A, 2016, P NATL ACAD SCI USA, V113, pE3548, DOI 10.1073/pnas.1518786113; Ahmadian Y, 2011, NEURAL COMPUT, V23, P46, DOI 10.1162/NECO_a_00059; Akrami A, 2018, NATURE, V554, P368, DOI 10.1038/nature25510; [Anonymous], 2006, NUMERICAL OPTIMIZATI, P135; Bak J. H., 2016, ADV NEURAL INFORM PR, V29, P1947; Bak JH, 2018, J VISION, V18, DOI 10.1167/18.12.4; Busse L, 2011, J NEUROSCI, V31, P11351, DOI 10.1523/JNEUROSCI.6689-10.2011; Erlich JC, 2015, ELIFE, V4, DOI 10.7554/eLife.05457; Gershman Samuel J, 2018, BIORXIV; Gold JI, 2013, PROG NEUROBIOL, V103, P98, DOI 10.1016/j.pneurobio.2012.05.008; Hwang EJ, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01356-z; Jones E., 2001, SCIPY OPEN SOURCE SC; Krakauer JW, 2017, NEURON, V93, P480, DOI 10.1016/j.neuron.2016.12.041; Paninski L, 2010, J COMPUT NEUROSCI, V29, P107, DOI 10.1007/s10827-009-0179-x; Prerau MJ, 2009, J NEUROPHYSIOL, V102, P3060, DOI 10.1152/jn.91251.2008; Prins N, 2012, J VISION, V12, DOI 10.1167/12.6.25; Romo R, 2003, NAT REV NEUROSCI, V4, P203, DOI 10.1038/nrn1058; Roy Nicholas A, 2018, PSYTRACK OPEN SOURCE; Sahani M, 2002, ADV NEURAL INFORM PR, V15, P317; Scott BB, 2015, ELIFE, V4, DOI 10.7554/eLife.11308; Smith AC, 2004, J NEUROSCI, V24, P447, DOI 10.1523/JNEUROSCI.2908-03.2004; Suzuki Wendy A, 2005, Behav Cogn Neurosci Rev, V4, P67, DOI 10.1177/1534582305280030; Wu A., 2017, ADV NEURAL INFORM PR, V30, P3499	23	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000022
C	Shah, V; Blanchet, J; Johari, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shah, Virag; Blanchet, Jose; Johari, Ramesh			Bandit Learning with Positive Externalities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MARKOV BRANCHING-PROCESSES	In many platforms, user arrivals exhibit a self-reinforcing behavior: future user arrivals are likely to have preferences similar to users who were satisfied in the past. In other words, arrivals exhibit positive externalities. We study multiarmed bandit (MAB) problems with positive externalities. We show that the self-reinforcing preferences may lead standard benchmark algorithms such as UCB to exhibit linear regret. We develop a new algorithm, Balanced Exploration (BE), which explores arms carefully to avoid suboptimal convergence of arrivals before sufficient evidence is gathered. We also introduce an adaptive variant of BE which successively eliminates suboptimal arms. We analyze their asymptotic regret, and establish optimality by showing that no algorithm can perform better.	[Shah, Virag; Blanchet, Jose; Johari, Ramesh] Stanford Univ, Management Sci & Engn, Stanford, CA 94305 USA	Stanford University	Shah, V (corresponding author), Stanford Univ, Management Sci & Engn, Stanford, CA 94305 USA.	virag@stanford.edu; jblanche@stanford.edu; rjohari@stanford.edu						Acemoglu D, 2011, REV ECON STUD, V78, P1201, DOI 10.1093/restud/rdr004; Adamic LA, 2005, INT WORKSHOP LINK DI, P36, DOI DOI 10.1145/1134271.1134277; Anderson C., 2004, WIRED MAGAZINE, V12, P170, DOI DOI 10.3359/OZ0912041; ATHREYA KB, 1968, ANN MATH STAT, V39, P1801, DOI 10.1214/aoms/1177698013; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; BIKHCHANDANI S, 1992, J POLIT ECON, V100, P992, DOI 10.1086/261849; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Chakrabarti  Soumen, 2005, P 16 ANN ACM SIAM S; Dembo A., 1998, LARGE DEVIATIONS TEC, V2th; Janson S, 2004, STOCH PROC APPL, V110, P177, DOI 10.1016/j.spa.2003.12.002; KATZ ML, 1994, J ECON PERSPECT, V8, P93, DOI 10.1257/jep.8.2.93; KUSTER P, 1983, Z WAHRSCHEINLICHKEIT, V64, P475, DOI 10.1007/BF00534952; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Langford J., 2008, ADV NEURAL INFORM PR, P817; Mas-Colell A., 1995, MICROECONOMIC THEORY; Ratkiewicz J, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.158701; Shapiro C., 1998, INFORM RULES STRATEG; Shy O, 2011, REV IND ORGAN, V38, P119, DOI 10.1007/s11151-011-9288-6; Slivkins  Aleksandrs, 2011, P 24 ANN C LEARN THE; Smith L, 2000, ECONOMETRICA, V68, P371, DOI 10.1111/1468-0262.00113; Williams D., 1991, PROBABILITY MARTINGA; Yule GU, 1925, PHILOS T R SOC LON B, V213, P21, DOI 10.1098/rstb.1925.0002	25	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304089
C	Shen, YL; Chen, JS; Huang, PS; Guo, YQ; Gao, JF		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shen, Yelong; Chen, Jianshu; Huang, Po-Sen; Guo, Yuqing; Gao, Jianfeng			M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				GAME; GO	Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines.	[Shen, Yelong; Chen, Jianshu] Tencent AI Lab, Bellevue, WA 98004 USA; [Shen, Yelong; Chen, Jianshu; Huang, Po-Sen; Guo, Yuqing; Gao, Jianfeng] Microsoft Res, Redmond, WA USA; [Huang, Po-Sen] DeepMind, London, England	Microsoft	Shen, YL (corresponding author), Tencent AI Lab, Bellevue, WA 98004 USA.	yelongshen@tencent.com; jianshuchen@tencent.com; posenhuang@google.com; yuqguo@microsoft.com; jfgao@microsoft.com						Bello I., 2016, ARXIV PREPRINT ARXIV; Bordes A., 2013, ADV NEURAL INFORM PR; Chen J., 2017, ADV NEURAL INFORM PR, V30, P4984; Cho Kyunghyun, 2014, P 2014 C EMP METH NA, P1724; Das R, 2018, P ICLR; Dettmers Tim, 2018, P AAAI FEBR; Gao J., 2018, ARXIV180908267; Gardner M., 2014, P 2014 C EMP METH NA, P397; Gu Shixiang, 2016, P ICLR; Guu Kelvin, 2015, P EMNLP; HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136; Hausknecht Matthew J., 2015, ABS150706527 CORR; He J., 2016, P 54 ANN M ASS COMP, V1; Kakade S, 2002, ADV NEUR IN, V14, P1531; Lin Y., 2015, AAAI; Lin Yankai, 2015, EMNLP; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Neelakantan Arvind, 2015, P AAAI; Nickel M., 2011, INT C INT C MACH LEA, P809, DOI DOI 10.5555/3104482.3104584; Ore Oystein, 1990, GRAPHS THEIR USES, V34; Rosin C, 2011, ANN MATH ARTIF INTEL, V61, P203, DOI 10.1007/s10472-011-9258-6; Shen Y., 2017, P 2 WORKSHOP REPRESE, P57; Shen Yelong, 2018, ICLR WORKSH; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Silver David, 2017, P ICML; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Toutanova K., 2015, CVSC; Toutanova Kristina, 2016, P ACL; Trouillon T, 2016, PR MACH LEARN RES, V48; Trouillon Theo, 2017, J MACHINE LEARNING R; Vinyals O., 2015, ADV NEURAL INFORM PR, P2692; Weber Theophane, 2017, P NIPS; Wierstra D, 2010, LOG J IGPL, V18, P620, DOI 10.1093/jigpal/jzp049; Williams R.J., 1992, REINFORCEMENT LEARNI, V173, P5, DOI [10.1007/978-1-4615-3618-5, DOI 10.1007/978-1-4615-3618-5]; Wu Y., 2017, NIPS 2017, P5279; Yang B., 2015, ICLR; Yang F., 2017, PROC C EMPIRICAL MET, P575; Yang Fan, 2017, NIPS, P2316	40	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001034
C	Somani, R; Gupta, C; Jain, P; Netrapalli, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Somani, Raghav; Gupta, Chirag; Jain, Prateek; Netrapalli, Praneeth			Support Recovery for Orthogonal Matching Pursuit: Upper and Lower bounds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				INCOHERENCE; SPARSITY	We study the problem of sparse regression where the goal is to learn a sparse vector that best optimizes a given objective function. Under the assumption that the objective function satisfies restricted strong convexity (RSC), we analyze orthogonal matching pursuit (OMP), a greedy algorithm that is used heavily in applications, and obtain a support recovery result as well as a tight generalization error bound for the OMP estimator. Further, we show a lower bound for OMP, demonstrating that both our results on support recovery and generalization error are tight up to logarithmic factors. To the best of our knowledge, these are the first such tight upper and lower bounds for any sparse regression algorithm under the RSC assumption.	[Somani, Raghav; Gupta, Chirag; Jain, Prateek; Netrapalli, Praneeth] Microsoft Res, Bengaluru, India; [Gupta, Chirag] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Somani, R (corresponding author), Microsoft Res, Bengaluru, India.	t-rasom@microsoft.com; chiragg@andrew.cmu.edu; prajain@microsoft.com; praneeth@microsoft.com						Agarwal A, 2012, ANN STAT, V40, P2452, DOI 10.1214/12-AOS1032; Blumensath T., 2008, 2008 16 EUR SIGN PRO, P1; Boucheron  Stephane, 2012, ARXIV12077209; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Candes E, 2007, INVERSE PROBL, V23, P969, DOI 10.1088/0266-5611/23/3/008; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; Foucart S., MATH INTRO COMPRESSI, V1; Gupta C, 2017, PR MACH LEARN RES, V70; Hsu  D., 2011, TAIL INEQUALITY QUAD; Jalali A., 2011, ADV NEURAL INF PROCE, V24, P1935; Jordan M.I., 2014, C LEARN THEOR, P921; Kar P., 2014, ADV NEURAL INFORM PR, P685; Loh PL, 2017, ANN STAT, V45, P2455, DOI 10.1214/16-AOS1530; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; Negahban S, 2012, J MACH LEARN RES, V13, P1665; PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465; Prelic A, 2006, BIOINFORMATICS, V22, P1122, DOI 10.1093/bioinformatics/btl060; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; Shen J, 2017, PR MACH LEARN RES, V70; Shen  Jie, 2017, ADV NEURAL INFORM PR, P3127; Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018; Yuan X., 2016, P 30 ANN C NEUR INF, P3558; Zhang T, 2011, IEEE T INFORM THEORY, V57, P6215, DOI 10.1109/TIT.2011.2162263; Zhang T, 2011, IEEE T INFORM THEORY, V57, P4689, DOI 10.1109/TIT.2011.2146690; Zhang T, 2009, J MACH LEARN RES, V10, P555; Zhang YC, 2017, ELECTRON J STAT, V11, P752, DOI 10.1214/17-EJS1233	28	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005040
C	Srinivasan, S; Downey, C; Boots, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Srinivasan, Siddarth; Downey, Carlton; Boots, Byron			Learning and Inference in Hilbert Space with Quantum Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Quantum Graphical Models (QGMs) generalize classical graphical models by adopting the formalism for reasoning about uncertainty from quantum mechanics. Unlike classical graphical models, QGMs represent uncertainty with density matrices in complex Hilbert spaces. Hilbert space embeddings (HSEs) also generalize Bayesian inference in Hilbert spaces. We investigate the link between QGMs and HSEs and show that the sum rule and Bayes rule for QGMs are equivalent to the kernel sum rule in HSEs and a special case of Nadaraya-Watson kernel regression, respectively. We show that these operations can be kernelized, and use these insights to propose a Hilbert Space Embedding of Hidden Quantum Markov Models (HSE-HQMM) to model dynamics. We present experimental results showing that HSE-HQMMs are competitive with state-of-the-art models like LSTMs and PSRNNs on several datasets, while also providing a nonparametric method for maintaining a probability distribution over continuous-valued features.	[Srinivasan, Siddarth; Boots, Byron] Georgia Tech, Coll Comp, Atlanta, GA 30332 USA; [Downey, Carlton] Carnegie Mellon Univ, Dept Machine Learning, Pittsburgh, PA 15213 USA	University System of Georgia; Georgia Institute of Technology; Carnegie Mellon University	Srinivasan, S (corresponding author), Georgia Tech, Coll Comp, Atlanta, GA 30332 USA.	sidsrini@gatech.edu; cmdowney@cs.cmu.edu; bboots@cc.gatech.edu						Boots B., 2013, ABS13096819 CORR; Boots B., 2012, NIPS WORKSH SPECTR A; Chen-Hsiang Yeang, 2010, 2010 Ninth International Conference on Machine Learning and Applications (ICMLA 2010), P155, DOI 10.1109/ICMLA.2010.30; Downey C, 2017, ADV NEUR IN, V30; Hefny A., 2015, ADV NEURAL INFORM PR, V28, P1963; Leifer MS, 2008, ANN PHYS-NEW YORK, V323, P1899, DOI 10.1016/j.aop.2007.10.001; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Monras A., 2010, ARXIV10022337; Nadaraya E.A., 1964, THEOR PROBAB APPL+, V9, P141, DOI [DOI 10.1137/1109020, 10.1137/1109020]; Nielsen M.A., 2000, QUANTUM COMPUTATION; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Schuld M, 2018, ARXIV180307128; Song L., 2010, P 27 INT C MACH LEAR, P991; Song L, 2013, IEEE SIGNAL PROC MAG, V30, P98, DOI 10.1109/MSP.2013.2252713; Song LD, 2009, PROCEEDINGS OF THE FIBER SOCIETY 2009 SPRING CONFERENCE, VOLS I AND II, P961; Srinivasan S., 2018, P 21 INT C ART INT S; Wasserman L, 2006, ALL NONPARAMETRIC ST; Watson G.S., 1964, SANKHYA SER A, V26, P359, DOI DOI 10.2307/25049340	19	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004085
C	Tay, Y; Tuan, LA; Hui, SC; Su, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tay, Yi; Luu Anh Tuan; Siu Cheung Hui; Su, Jian			Densely Connected Attention Propagation for Reading Comprehension	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose DECAPROP (Densely Connected Attention Propagation), a new densely connected neural architecture for reading comprehension (RC). There are two distinct characteristics of our model. Firstly, our model densely connects all pairwise layers of the network, modeling relationships between passage and query across all hierarchical levels. Secondly, the dense connectors in our network are learned via attention instead of standard residual skip-connectors. To this end, we propose novel Bidirectional Attention Connectors (BAC) for efficiently forging connections throughout the network. We conduct extensive experiments on four challenging RC benchmarks. Our proposed approach achieves state-of-the-art results on all four, outperforming existing baselines by up to 2.6% - 14.2% in absolute F1 score.	[Tay, Yi; Siu Cheung Hui] Nanyang Technol Univ, Singapore, Singapore; [Luu Anh Tuan; Su, Jian] Inst Infocomm Res, Singapore, Singapore	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Agency for Science Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R)	Tay, Y (corresponding author), Nanyang Technol Univ, Singapore, Singapore.	ytay017@e.ntu.edu.sg; at.luu@i2r.a-star.edu.sg; asschui@ntu.edu.sg; sujian@i2r.a-star.edu.sg	Luu, Anh Tuan/AAG-3582-2021	Hui, Siu Cheung/0000-0001-5397-4472	Baidu I2R Research Centre	Baidu I2R Research Centre	This paper is partially supported by Baidu I<SUP>2</SUP>R Research Centre, a joint laboratory between Baidu and A-Star I<SUP>2</SUP>R. The authors would like to thank the anonymous reviewers of NeuRIPS 2018 for their valuable time and feedback!	Abadi M, 2015, P 12 USENIX S OPERAT; [Anonymous], ARXIV180200889; Buck Christian, 2017, ARXIV170507830; Chen DQ, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1870, DOI 10.18653/v1/P17-1171; Cui Yiming, 2016, ARXIV160704423; Dhingra Bhuwan, 2017, QUASAR DATASETS QUES; Dunn Matthew, 2017, ARXIV170405179; Eiband M, 2018, IUI 2018: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P211, DOI 10.1145/3172944.3172961; Gal Yarin, 2016, ADV NEURAL INFORM PR, P1019, DOI DOI 10.5555/3157096.3157211; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu Minghao, 2018, ARXIV180807644; Hu Minghao, 2017, ARXIV170502798; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Joshi M, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1601, DOI 10.18653/v1/P17-1147; Kadlec Rudolf, 2016, ARXIV160301547; Ke NR, 2018, PR MACH LEARN RES, V80; Kingma D.P, P 3 INT C LEARNING R; Kocisky Tomas, 2017, ARXIV171207040; Kundu S., 2018, QUESTION FOCUSED MUL; Lai G., 2017, EMNLP, P785, DOI [10.18653/v1/D17-1082, DOI 10.18653/V1/D17-1082]; McCann Bryan, 2017, ARXIV170800107; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Radford Alec, IMPROVING LANGUAGE U; RAJPURKAR P, 2016, P 2016 C EMP METH NA, V2016, P2383, DOI DOI 10.18653/V1/D16-1264; Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127; Seo Minjoon, 2016, ARXIV161101603; Shen YL, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1047, DOI 10.1145/3097983.3098177; Srivastava Rupesh Kumar, 2015, ABS150500387 CORR; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tay Yi, 2018, P 2018 C EMP METH NA, P4492, DOI DOI 10.18653/V1/D18-1479; Tay Yi, 2017, ARXIV180100102; Tay Yi, 2018, ARXIV180309074; Trischler Adam, 2016, ACL; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang Shuohang, 2017, ARXIV170900023; Wang Shuohang, 2016, ARXIV160807905; Wang Shuohang, 2017, ARXIV171105116; Wang WH, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P189, DOI 10.18653/v1/P17-1018; Weissenborn D., 2017, READING TWICE NATURA; Weissenborn Dirk, 2017, CONLL, DOI DOI 10.18653/V1/K17-1028; Xiong Caiming, 2016, ABS161101604 CORR; Xiong Caiming, 2017, ARXIV171100106; Yu Adams Wei, 2018, ARXIV180409541; Zeiler Matthew D, 2012, ARXIV12125701	45	3	4	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304088
C	Teng, M; Wood, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Teng, Michael; Wood, Frank			Bayesian Distributed Stochastic Gradient Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce Bayesian distributed stochastic gradient descent (BDSGD), a high-throughput algorithm for training deep neural networks on parallel computing clusters. This algorithm uses amortized inference in a deep generative model to perform joint posterior predictive inference of mini-batch gradient computation times in a compute cluster specific manner. Specifically, our algorithm mitigates the straggler effect in synchronous, gradient-based optimization by choosing an optimal cutoff beyond which mini-batch gradient messages from slow workers are ignored. The principle novel contribution and finding of this work goes beyond this by demonstrating that using the predicted run-times from a generative model of cluster worker performance improves over the static-cutoff prior art, leading to higher gradient computation throughput on large compute clusters. In our experiments we show that eagerly discarding the mini-batch gradient computations of stragglers not only increases throughput but sometimes also increases the overall rate of convergence as a function of wall-clock time by virtue of eliminating idleness.	[Teng, Michael] Univ Oxford, Dept Engn Sci, Oxford, England; [Wood, Frank] Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada	University of Oxford; University of British Columbia	Teng, M (corresponding author), Univ Oxford, Dept Engn Sci, Oxford, England.	mteng@robots.ox.ac.uk; fwood@cs.ubc.ca			DARPA D3M [FA8750-17-2-0093]; NERSC Big Data Center under Intel BDC; Office of Science of the U.S. Department of Energy [DE-AC02-05CH11231]; Intel	DARPA D3M; NERSC Big Data Center under Intel BDC; Office of Science of the U.S. Department of Energy(United States Department of Energy (DOE)); Intel(Intel Corporation)	Michael Teng is supported under DARPA D3M, under Cooperative Agreement FA8750-17-2-0093 and partially supported by the NERSC Big Data Center under Intel BDC. This research used resources of the National Energy Research Scientific Computing Center (NERSC), a DOE Office of Science User Facility supported by the Of fi ce of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. We acknowledge Intel for their funding support and we thank members of the UBC PLAI group for helpful discussions.	Ba J., 2017, P 3 INT C LEARN REPR; Chen J., 2016, ABS160400981 CORR; Codreanu Valeriu, 2017, ARXIV171104291; Dean J., 2012, NIPS 12, V1, P1223; Doucet A, 2001, STAT ENG IN, P3; Ergen T., 2017, IEEE T NEURAL NETWOR; Hoffer E., 2017, ADV NEURAL INF PROCE; Kingma DP, 2 INT C LEARN REPR I, P1; Krishnan RG, 2017, AAAI CONF ARTIF INTE, P2101; Le T. A., 2017, 20 INT C ART INT STA; Masters D., 2018, ARXIV180407612; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Rezende Danilo Jimenez, 2015, ARXIV150505770; Ritchie D., 2016, ARXIV161005735; ROYSTON JP, 1982, J R STAT SOC C-APPL, V31, P161; Smith Samuel L, 2017, ARXIV171100489; Streeter M, 2014, ADV NEURAL INFORM PR, P2915; You Y, 2017, ABS170905011 CORR; You Y., 2017, LARGE BATCH TRAINING; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]; Zhang S., 2015, NEURAL INFORM PROCES, P685	21	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000084
C	Teymur, O; Calderhead, B; Lie, HC; Sullivan, TJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Teymur, Onur; Calderhead, Ben; Lie, Han Cheng; Sullivan, T. J.			Implicit Probabilistic Integrators for ODEs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce a family of implicit probabilistic integrators for initial value problems (IVPs), taking as a starting point the multistep Adams-Moulton method. The implicit construction allows for dynamic feedback from the forthcoming time-step, in contrast to previous probabilistic integrators, all of which are based on explicit methods. We begin with a concise survey of the rapidly-expanding field of probabilistic ODE solvers. We then introduce our method, which builds on and adapts the work of Conrad et al. (2016) and Teymur et al. (2016), and provide a rigorous proof of its well-definedness and convergence. We discuss the problem of the calibration of such integrators and suggest one approach. We give an illustrative example highlighting the effect of the use of probabilistic integrators-including our new method-in the setting of parameter inference within an inverse problem.	[Teymur, Onur; Calderhead, Ben] Imperial Coll London, Dept Math, London, England; [Lie, Han Cheng; Sullivan, T. J.] Free Univ Berlin, Inst Math, Berlin, Germany; [Lie, Han Cheng; Sullivan, T. J.] Zuse Inst Berlin, Berlin, Germany	Imperial College London; Free University of Berlin; Zuse Institute Berlin	Teymur, O (corresponding author), Imperial Coll London, Dept Math, London, England.	o@teymur.uk		Lie, Han Cheng/0000-0002-6905-9903	Freie Universitat Berlin within the Excellence Initiative of the German Research Foundation (DFG); DFG [CRC 1114]; National Science Foundation (NSF) [DMS-1127914]	Freie Universitat Berlin within the Excellence Initiative of the German Research Foundation (DFG)(German Research Foundation (DFG)); DFG(German Research Foundation (DFG)); National Science Foundation (NSF)(National Science Foundation (NSF)National Research Foundation of Korea)	HCL and TJS are partially supported by the Freie Universitat Berlin within the Excellence Initiative of the German Research Foundation (DFG). This work was partially supported by the DFG through grant CRC 1114 Scaling Cascades in Complex Systems, and by the National Science Foundation (NSF) under grant DMS-1127914 to the Statistical and Applied Mathematical Sciences Institute's QMC Working Group II Probabilistic Numerics.	Abdulle A., 2018, ARXIV180101340; Butcher J C, 2008, NUMERICAL METHODS OR; Calderhead B., 2009, ADV NEURAL INFORM PR, V21, P217; Chkrebtii OA, 2016, BAYESIAN ANAL, V11, P1239, DOI 10.1214/16-BA1017; Conrad P., 2016, STAT COMPUT, P1; Dondelinger F, 2013, JMLR WORKSHOP C P, V31, P216; Haario H, 2001, BERNOULLI, V7, P223, DOI 10.2307/3318737; Hennig P, 2014, JMLR WORKSH CONF PRO, V33, P347; Hennig P, 2015, P ROY SOC A-MATH PHY, V471, DOI 10.1098/rspa.2015.0142; Kersting H., 2016, UNCERTAINTY ARTIFICI, V32; Lie H. C., 2017, ARXIV170303680; Lie H. C., 2018, SIAM ASA J UNCERTAIN; Macdonald B, 2015, PR MACH LEARN RES, V37, P1539; OEHLERT GW, 1992, AM STAT, V46, P27, DOI 10.2307/2684406; Press W.H., 2007, NUMERICAL RECIPES; Ramsay JO, 2007, J ROY STAT SOC B, V69, P741, DOI 10.1111/j.1467-9868.2007.00610.x; Sarkka S., 2013, BAYESIAN FILTERING S; Schober M., 2018, STAT COMPUT, P1; Schober M, 2014, ADV NEURAL INFORM PR, V27, P739; SKILLING J, 1993, PHYSICS AND PROBABILITY, P207, DOI 10.1017/CBO9780511524448.020; Skilling J., 1991, FUNDAMENTAL THEORIES, P23; Teymur O., 2016, ADV NEURAL INFORM PR, V29, P4314; Wang YL, 2014, PR MACH LEARN RES, V32, P1485	24	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001077
C	Thu, NP; Li, C; Balaban, S; Yang, YL		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Thu Nguyen-Phuoc; Li, Chuan; Balaban, Stephen; Yang, Yong-Liang			RenderNet: A deep convolutional network for differentiable rendering from 3D shapes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Traditional computer graphics rendering pipelines are designed for procedurally generating 2D images from 3D shapes with high performance. The non-differentiability due to discrete operations (such as visibility computation) makes it hard to explicitly correlate rendering parameters and the resulting image, posing a significant challenge for inverse rendering tasks. Recent work on differentiable rendering achieves differentiability either by designing surrogate gradients for non-differentiable operations or via an approximate but differentiable renderer. These methods, however, are still limited when it comes to handling occlusion, and restricted to particular rendering effects. We present RenderNet, a differentiable rendering convolutional network with a novel projection unit that can render 2D images from 3D shapes. Spatial occlusion and shading calculation are automatically encoded in the network. Our experiments show that RenderNet can successfully learn to implement different shaders, and can be used in inverse rendering tasks to estimate shape, pose, lighting and texture from a single image.	[Thu Nguyen-Phuoc; Yang, Yong-Liang] Univ Bath, Bath, Avon, England; [Li, Chuan; Balaban, Stephen] Lambda Labs, San Francisco, CA USA	University of Bath	Thu, NP (corresponding author), Univ Bath, Bath, Avon, England.	T.Nguyen.Phuoc@bath.ac.uk; c@lambdalabs.com; s@lambdalabs.com; Y.Yang@cs.bath.ac.uk	lin, chuan/HHD-2571-2022		European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant [665992]; UK's EPSRC Centre for Doctoral Training in Digital Entertainment (CDE) [EP/L016540/1]; CAMERA, the RCUK Centre for the Analysis of Motion, Entertainment Research and Applications [EP/M023281/1]; Lambda Labs	European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant; UK's EPSRC Centre for Doctoral Training in Digital Entertainment (CDE)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); CAMERA, the RCUK Centre for the Analysis of Motion, Entertainment Research and Applications; Lambda Labs	We thank Christian Richardt for helpful discussions. We thank Lucas Theis for helpful discussions and feedback on the manuscript. This work was supported in part by the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 665992, the UK's EPSRC Centre for Doctoral Training in Digital Entertainment (CDE), EP/L016540/1, and CAMERA, the RCUK Centre for the Analysis of Motion, Entertainment Research and Applications, EP/M023281/1. We also received GPU support from Lambda Labs.	Barron Jonathan T, 2015, TPAMI; Chang A. X., 2015, ABS151203012 CORR; Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168; DeCarlo D, 2003, ACM T GRAPHIC, V22, P848, DOI 10.1145/882262.882354; Dosovitskiy A, 2017, IEEE T PATTERN ANAL, V39, P692, DOI 10.1109/TPAMI.2016.2567384; Genova Kyle, 2018, IEEE C COMP VIS PATT; Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gwak JunYoung, 2017, 3D VISION 3DV; Henderson Paul, 2018, BRIT MACH VIS C BMVC; Horn B. K., 1974, COMPUT VISION GRAPH, V3, P277, DOI DOI 10.1016/0146-664X(74)90022-7; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jimenez Rezende D., 2016, ADV NEURAL INFORM PR, P4996; Kato Hiroharu, 2018, IEEE CVPR; Kingma DP, 2015, INT C LEARN REPR ICL; Kulkarni TD, 2015, ADV NEUR IN, V28; Kundu Abhijit, 2018, CVPR; Lin M., 2014, ICLR; Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11; Miller G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P319, DOI 10.1145/192161.192244; Nalbach O, 2017, COMPUT GRAPH FORUM, V36, P65, DOI 10.1111/cgf.13225; Nooruddin FS, 2003, IEEE T VIS COMPUT GR, V9, P191, DOI 10.1109/TVCG.2003.1196006; Park E, 2017, PROC CVPR IEEE, P702, DOI 10.1109/CVPR.2017.82; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; Rematas K, 2017, IEEE T PATTERN ANAL, V39, P1576, DOI 10.1109/TPAMI.2016.2601093; Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Su Hao, 2014, ABS14120003 CORR; Taniai T, 2018, PR MACH LEARN RES, V80; Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230; Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30; Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608; Winnemoller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018; Wu J., 2017, NIPS; Yang Jimei, 2015, NIPS; Zhu R, 2017, IEEE I CONF COMP VIS, P57, DOI 10.1109/ICCV.2017.16	39	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002044
C	Vahdat, A; Andriyash, E; Macready, WG		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Vahdat, Arash; Andriyash, Evgeny; Macready, William G.			DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Boltzmann machines are powerful distributions that have been shown to be an effective prior over binary latent variables in variational autoencoders (VAEs). However, previous methods for training discrete VAEs have used the evidence lower bound and not the tighter importance-weighted bound. We propose two approaches for relaxing Boltzmann machines to continuous distributions that permit training with importance-weighted bounds. These relaxations are based on generalized overlapping transformations and the Gaussian integral trick. Experiments on the MNIST and OMNIGLOT datasets show that these relaxations outperform previous discrete VAEs with Boltzmann priors. An implementation which reproduces these results is available at https: //github.com/QuadrantAI/dvae.	[Vahdat, Arash; Andriyash, Evgeny; Macready, William G.] D Wave Syst Inc, Quadrant Ai, Burnaby, BC, Canada		Vahdat, A (corresponding author), D Wave Syst Inc, Quadrant Ai, Burnaby, BC, Canada.	arash@quadrant.a; evgeny@quadrant.ai; bill@quadrant.ai						[Anonymous], 2016, AUTOMATIC CHEM DESIG; [Anonymous], 2014, INT C MACH LEARN; [Anonymous], 2014, INT C MACH LEARN; Bengio Yoshua, 2013, ARXIV; Bornschein J., 2017, ADV NEURAL INFORM PR, P3923; Burda Yuri, 2016, INT C LEARN REPR; GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552; Govindarajulu Zakkula, 1966, SCANDINAVIAN ACTUARI, V1966, P132; Grathwohl Will, 2018, INT C LEARN REPR; Graves Alex, 2016, ARXIV160705690; GREGOR K, 2014, INT C MACH LEARN; Gu Shixiang, 2016, ICLR; Hertz J., 1991, INTRO THEORY NEURAL, DOI DOI 10.1201/9780429499661; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; HUBBARD J, 1959, PHYS REV LETT, V3, P77, DOI 10.1103/PhysRevLett.3.77; Hukushima K, 2003, AIP CONF PROC, V690, P200, DOI 10.1063/1.1632130; Jang E., 2017, ICLR; Kingma D.P., 2014, INT C LEARN REPR ICL, Patent No. [1312.6114, 13126114]; Kingma D. P, 2014, ADV NEURAL INFORM PR; Kusner M. J., 2017, INT C MACH LEARN; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Larochelle H., 2008, INT C MACH LEARN ICM; Le Roux Nicolas, 2008, NEURAL COMPUTATION; Li Y., 2016, ADV NEURAL INFORM PR, P1073; Maaloe L., 2017, ARXIV170400637; Maddison Chris J., 2017, 5 INT C LEARN REPR; Mnih A, 2016, PR MACH LEARN RES, V48; Murali V, 2018, INT C LEARN REPR; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Nguyen T. D., 2017, UAI; Pu Y., 2017, ADV NEURAL INFORM PR; Raiko Tapani, 2014, ARXIV14062989; Roberts Adam, 2018, INT C MACH LEARN; Rolfe J. T., 2017, INT C LEARN REPR ICL; Salakhutdinov R., 2009, ARTIFICIAL INTELLIGE; Salakhutdinov R., 2007, P 24 INT C MACHINE L, P791, DOI 10.1145/1273496.1273596; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Salimans T, 2015, PR MACH LEARN RES, V37, P1218; Sallans B, 2004, J MACH LEARN RES, V5, P1063; Tieleman T., 2008, P 25 INT C MACHINE L, P1064, DOI DOI 10.1145/1390156.1390290; Titsias MK, 2015, ADV NEUR IN, V28; Tokui Seiya, 2017, INT C MACH LEARN, P3414; Tucker G., 2017, ADV NEURAL INFORM PR, P2624; Vahdat Arash, 2018, INT C MACH LEARN ICM; Welling M, 2002, LECT NOTES COMPUT SC, V2415, P351; Williams R.J., 1992, REINFORCEMENT LEARNI, V173, P5, DOI [10.1007/978-1-4615-3618-5, DOI 10.1007/978-1-4615-3618-5]; Zhang Y, 2012, ADV NEURAL INFORM PR, V25, P3194	47	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301082
C	Vertes, E; Sahani, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Vertes, Eszter; Sahani, Maneesh			Flexible and accurate inference and learning for deep generative models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce a new approach to learning in hierarchical latent-variable generative models called the "distributed distributional code Helmholtz machine", which emphasises flexibility and accuracy in the inferential process. Like the original Helmholtz machine and later variational autoencoder algorithms (but unlike adversarial methods) our approach learns an explicit inference or "recognition" model to approximate the posterior distribution over the latent variables. Unlike these earlier methods, it employs a posterior representation that is not limited to a narrow tractable parametrised form (nor is it represented by samples). To train the generative and recognition models we develop an extended wake-sleep algorithm inspired by the original Helmholtz machine. This makes it possible to learn hierarchical latent models with both discrete and continuous variables, where an accurate posterior representation is essential. We demonstrate that the new algorithm outperforms current state-of-the-art methods on synthetic, natural image patch and the MNIST data sets.	[Vertes, Eszter; Sahani, Maneesh] UCL, Gatsby Computat Neurosci Unit, London W1T 4JG, England	University of London; University College London	Vertes, E (corresponding author), UCL, Gatsby Computat Neurosci Unit, London W1T 4JG, England.	eszter@gatsby.ucl.ac.uk; maneesh@gatsby.ucl.ac.uk			Gatsby Charitable Foundation	Gatsby Charitable Foundation	This work was funded by the Gatsby Charitable Foundation.	Bounliphone W., 2015, ARXIV151104581; Burda Yuri, 2015, ICLR; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; Goodman N., 2014, P ANN M COGN SCI SOC, V36; Gretton A, 2012, J MACH LEARN RES, V13, P723; Grunewalder S., 2012, P 29 INT C MACH LEAR, P1823; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Jitkrittum W, 2016, ARXIV160506796; King DB, 2015, ACS SYM SER, V1214, P1; Kingma D. P., 2014, P INT C LEARN REPR; Minka T, 2005, MICROSOFT RES; Mnih A, 2016, PR MACH LEARN RES, V48; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Rahimi A, 2008, ANN ALLERTON CONF, P555, DOI 10.1109/ALLERTON.2008.4797607; Ranganath R, 2015, JMLR WORKSH CONF PRO, V38, P762; Rezende D., 2015, ICML, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Sahani M, 2003, NEURAL COMPUT, V15, P2255, DOI 10.1162/089976603322362356; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Sonderby CK, 2016, ADV NEUR IN, V29; Turner R. E., 2011, BAYESIAN TIME SERIES, P109; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P359, DOI 10.1098/rspb.1998.0303; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Zemel RS, 1998, NEURAL COMPUT, V10, P403, DOI 10.1162/089976698300017818	25	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304020
C	Wang, YN; Balakrishnan, S; Singh, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Yining; Balakrishnan, Sivaraman; Singh, Aarti			Optimization of Smooth Functions with Noisy Observations: Local Minimax Rates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CONVERGENCE	We consider the problem of global optimization of an unknown non-convex smooth function with noisy zeroth-order feedback. We propose a local minimax framework to study the fundamental difficulty of optimizing smooth functions with adaptive function evaluations. We show that for functions with fast growth around their global minima, carefully designed optimization algorithms can identify a near global minimizer with many fewer queries than worst-case global minimax theory predicts. For the special case of strongly convex and smooth functions, our implied convergence rates match the ones developed for zeroth-order convex optimization problems. On the other hand, we show that in the worst case no algorithm can converge faster than the minimax rate of estimating an unknown functions in l(infinity)-norm. Finally, we show that non-adaptive algorithms, although optimal in a global minimax sense, do not attain the optimal local minimax rate.	[Wang, Yining; Balakrishnan, Sivaraman; Singh, Aarti] Carnegie Mellon Univ, Dept Machine Learning & Stat, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Wang, YN (corresponding author), Carnegie Mellon Univ, Dept Machine Learning & Stat, Pittsburgh, PA 15213 USA.	yiningwa@cs.cmu.edu; siva@stat.cmu.edu; aarti@cs.cmu.edu		Wang, Yining/0000-0001-9410-0392	AFRL grant [FA8750-17-2-0212]	AFRL grant	This work is supported by AFRL grant FA8750-17-2-0212. We thank the anonymous reviewers for many helpful suggestions that improved the presentation of this paper.	Agarwal A., 2010, P ANN C LEARN THEOR; Agarwal A, 2013, SIAM J OPTIMIZ, V23, P213, DOI 10.1137/110850827; Agarwal Naman, 2017, P ANN ACM SIGACT S T; Balakrishnan S., 2013, P ADV NEUR INF PROC; Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003; Bubeck S., 2009, P INT C ALG LEARN TH; Bubeck S., 2017, P ANN ACM SIGACT S T; Bubeck S, 2011, J MACH LEARN RES, V12, P1655; Bull AD, 2011, J MACH LEARN RES, V12, P2879; Carmon Y., 2017, ARXIV170502766; Castro RM, 2008, IEEE T INFORM THEORY, V54, P2339, DOI 10.1109/TIT.2008.920189; Chaudhuri K, 2014, IEEE T INFORM THEORY, V60, P7900, DOI 10.1109/TIT.2014.2361055; CHEN H, 1988, ANN STAT, V16, P1330, DOI 10.1214/aos/1176350965; Dasgupta S., 2008, P ADV NEUR INF PROC; Duchi J. C., 2016, ARXIV161205612; Duchi J. C., 2016, NIPS; Fan J.Q., 1996, LOCAL POLYNOMIAL MOD; Flaxman A. D., 2005, P ACM SIAM S DISCR A; Ge R., 2015, P ANN C LEARN THEOR; Grill JB, 2015, ADV NEUR IN, V28; Hanneke S., 2007, P INT C MACH LEARN I; Hazan E., 2017, PREPRINT; Hazan E., 2015, P ADV NEUR INF PROC; Jamieson K. G., 2012, P ADV NEUR INF PROC; KAN AHGR, 1987, MATH PROGRAM, V39, P27, DOI 10.1007/BF02592070; KIEFER J, 1952, ANN MATH STAT, V23, P462, DOI 10.1214/aoms/1177729392; Kleinberg Robert D, 2005, ADV NEURAL INFORM PR; Korostelev Aleksandr Petrovich, 2012, MINIMAX THEORY IMAGE, V82; Lepski OV, 1997, ANN STAT, V25, P929; Malherbe C., 2016, P INT C MACH LEARN I; Malherbe C., 2017, P INT C MACH LEARN I; Minsker S., 2013, P C LEARN THEOR COLT; Minsker Stanislav, 2012, THESIS; Nakamura Nathan, 2017, APPL STOCHASTIC MODE; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; POLONIK W, 1995, ANN STAT, V23, P855, DOI 10.1214/aos/1176324626; Rasmussen C., 2006, GAUSSIAN PROCESSES M, V1; Reeja-Jayan B, 2012, SCI REP-UK, V2, DOI 10.1038/srep01003; Rigollet P, 2009, BERNOULLI, V15, P1154, DOI 10.3150/09-BEJ184; Rinnooy Kan A. H. G., 1987, Mathematical Programming, V39, P57, DOI 10.1007/BF02592071; Risteski A., 2016, P ADV NEUR INF PROC; Scarlett J., 2017, C LEARNING THEORY CO; Singh A, 2009, ANN STAT, V37, P2760, DOI 10.1214/08-AOS661; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Van der Vaart AW, 1998, ASYMPTOTIC STAT, V3; Zhang Y., 2017, P ANN C LEARN THEOR	48	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304036
C	Warlop, R; Lazaric, A; Mary, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Warlop, Romain; Lazaric, Alessandro; Mary, Jeremie			Fighting Boredom in Recommender Systems with Linear Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				REGRET BOUNDS	A common assumption in recommender systems (RS) is the existence of a best fixed recommendation strategy. Such strategy may be simple and work at the item level (e.g., in multi-armed bandit it is assumed one best fixed arm/item exists) or implement more sophisticated RS (e.g., the objective of A/B testing is to find the best fixed RS and execute it thereafter). We argue that this assumption is rarely verified in practice, as the recommendation process itself may impact the user's preferences. For instance, a user may get bored by a strategy, while she may gain interest again, if enough time passed since the last time that strategy was used. In this case, a better approach consists in alternating different solutions at the right frequency to fully exploit their potential. In this paper, we first cast the problem as a Markov decision process, where the rewards are a linear function of the recent history of actions, and we show that a policy considering the long-term influence of the recommendations may outperform both fixed-action and contextual greedy policies. We then introduce an extension of the UCRL algorithm (LINUCRL) to effectively balance exploration and exploitation in an unknown environment, and we derive a regret bound that is independent of the number of states. Finally, we empirically validate the model assumptions and the algorithm in a number of realistic scenarios.	[Warlop, Romain] Fifty Five, Paris, France; [Warlop, Romain] Inria Lille, SequeL Team, Lille, France; [Lazaric, Alessandro] Facebook AI Res, Paris, France; [Mary, Jeremie] Criteo AI Lab, Paris, France	Facebook Inc	Warlop, R (corresponding author), Fifty Five, Paris, France.; Warlop, R (corresponding author), Inria Lille, SequeL Team, Lille, France.	romain@fifty-five.com; lazaric@fb.com; j.mary@criteo.com						Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; [Anonymous], 2020, REINFORCEMENT LEARNI; Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663; Auer P., 2009, ADV NEURAL INFORM PR, V21, P89; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Dasdan A., 1999, Proceedings 1999 Design Automation Conference (Cat. No. 99CH36361), P37, DOI 10.1109/DAC.1999.781227; Filippi S., 2010, IEEE J SEL TOP QUANT, V5, P68; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Heidari H., 2016, P 25 INT JOINT C ART, P1562; Herlocker J.L., 1999, P 1999 C RES DEV INF; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Jamieson K. G., 2016, AISTATS; Kapoor K, 2015, WSDM'15: PROCEEDINGS OF THE EIGHTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P233, DOI 10.1145/2684822.2685306; KARP RM, 1978, DISCRETE MATH, V23, P309, DOI 10.1016/0012-365X(78)90011-0; Komiyama J., 2014, TIME DECAYING BANDIT, P460; Koren Y, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P447; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Li L., 2011, PROC 4 ACM INT C WEB, P297, DOI DOI 10.1145/1935826.1935878; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Ortner R, 2008, LECT NOTES ARTIF INT, V5254, P123, DOI 10.1007/978-3-540-87987-9_14; Ortner R, 2014, THEOR COMPUT SCI, V558, P62, DOI 10.1016/j.tcs.2014.09.026; Shani G, 2005, J MACH LEARN RES, V6, P1265; Soare M, 2014, ADV NEUR IN, V27; Swaminathan A, 2015, J MACH LEARN RES, V16, P1731; Tekin C, 2012, IEEE T INFORM THEORY, V58, P5588, DOI 10.1109/TIT.2012.2198613	25	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301072
C	Xu, JW; Ni, BB; Yang, XK		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Xu, Jingwei; Ni, Bingbing; Yang, Xiaokang			Video Prediction via Selective Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Most adversarial learning based video prediction methods suffer from image blur, since the commonly used adversarial and regression loss pair work rather in a competitive way than collaboration, yielding compromised blur effect. In the meantime, as often relying on a single-pass architecture, the predictor is inadequate to explicitly capture the forthcoming uncertainty. Our work involves two key insights: (1) Video prediction can be approached as a stochastic process: we sample a collection of proposals conforming to possible frame distribution at following time stamp, and one can select the final prediction from it. (2) De-coupling combined loss functions into dedicatedly designed sub-networks encourages them to work in a collaborative way. Combining above two insights we propose a two-stage framework called VPSS (Video Prediction via Selective Sampling). Specifically a Sampling module produces a collection of high quality proposals, facilitated by a multiple choice adversarial learning scheme, yielding diverse frame proposal set. Subsequently a Selection module selects high possibility candidates from proposals and combines them to produce final prediction. Extensive experiments on diverse challenging datasets demonstrate the effectiveness of proposed video prediction approach, i.e., yielding more diverse proposals and accurate prediction results.	[Ni, Bingbing] Shanghai Jiao Tong Univ, AI Inst, SJTU UCLA Joint Res Ctr Machine Percept & Inferen, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China; Shanghai Inst Adv Commun & Data Sci, Shanghai, Peoples R China	Shanghai Jiao Tong University	Ni, BB (corresponding author), Shanghai Jiao Tong Univ, AI Inst, SJTU UCLA Joint Res Ctr Machine Percept & Inferen, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.	xjwxjw@sjtu.edu.cn; nibingbing@sjtu.edu.cn; xkyang@sjtu.edu.cn	Yang, Xiaokang/C-6137-2009	Yang, Xiaokang/0000-0003-4029-3322	National Key Research and Development Program of China [2016YFB1001003]; NSFC [61527804, U1611461, 61502301, 61521062]; State Key Research and Development Program [18DZ2270700]; SJTU-UCLA Joint Center for Machine Perception and Inference; China's Thousand Youth Talents Plan; STCSM [18DZ2270700, 17511105401]; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); State Key Research and Development Program; SJTU-UCLA Joint Center for Machine Perception and Inference; China's Thousand Youth Talents Plan; STCSM(Science & Technology Commission of Shanghai Municipality (STCSM)); MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China	This work was supported by National Key Research and Development Program of China (2016YFB1001003), NSFC (61527804, U1611461, 61502301, 61521062). The work was partly supported by State Key Research and Development Program 18DZ2270700. This work was supported by SJTU-UCLA Joint Center for Machine Perception and Inference. The work was also partially supported by China's Thousand Youth Talents Plan, STCSM 17511105401, 18DZ2270700 and MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China.	Abadi Martin, 2016, arXiv; Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374; Babaeizadeh Mohammad, 2017, ARXIV171011252; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Bottou L., 2017, ARXIV170107875STATML; Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168; De Brabandere B, 2016, ADV NEUR IN, V29; Denton E., 2018, CORR; Dumoulin Vincent, 2016, ARXIV E PRINTS; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Glorot X., 2011, P 14 INT C ART INT S, P315; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guzman-Rivera A., 2012, ADV NEURAL INFORM PR, P1808; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jin Xiaojie, 2017, NEURIPS, P2; Kalchbrenner N, 2017, PR MACH LEARN RES, V70; Khan Faisal, 2011, ADV NEURAL INFORM PR, P1449; Kingma D.P, P 3 INT C LEARNING R; Larsen ABL, 2016, PR MACH LEARN RES, V48; Lee Alex X, 2018, ARXIV180401523; Li Chunyuan, 2017, NIPS; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; Marsland S, 2009, CH CRC MACH LEARN PA, P1; Mathieu Michael, 2016, ICLR; Reed S, 2015, ADV NEURAL INFORM PR, P1252; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Villegas R, 2017, PR MACH LEARN RES, V70; Villegas Ruben, 2017, CORR; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	36	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301067
C	Yan, X; Zhang, WZ; Ma, L; Liu, W; Wu, Q		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yan, Xing; Zhang, Weizhong; Ma, Lin; Liu, Wei; Wu, Qi			Parsimonious Quantile Regression of Financial Asset Tail Dynamics via Sequential Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				AUTOREGRESSIVE CONDITIONAL SKEWNESS; VOLATILITY; RETURNS	We propose a parsimonious quantile regression framework to learn the dynamic tail behaviors of financial asset returns. Our model captures well both the timevarying characteristic and the asymmetrical heavy-tail property of financial time series. It combines the merits of a popular sequential neural network model, i.e., LSTM, with a novel parametric quantile function that we construct to represent the conditional distribution of asset returns. Our model also captures individually the serial dependences of higher moments, rather than just the volatility. Across a wide range of asset classes, the out-of-sample forecasts of conditional quantiles or VaR of our model outperform the GARCH family. Further, the proposed approach does not suffer from the issue of quantile crossing, nor does it expose to the ill-posedness comparing to the parametric probability density function approach.	[Zhang, Weizhong; Ma, Lin; Liu, Wei] Tencent AI Lab, Bellevue, WA USA; [Wu, Qi] City Univ Hong Kong, Sch Data Sci, Hong Kong, Peoples R China; [Yan, Xing] Chinese Univ Hong Kong, Dept SEEM, Hong Kong, Peoples R China	City University of Hong Kong; Chinese University of Hong Kong	Wu, Q (corresponding author), City Univ Hong Kong, Sch Data Sci, Hong Kong, Peoples R China.	xyan@se.cuhk.edu.hk; zhangweizhongzju@gmail.com; forest.linma@gmail.com; wl2223@columbia.edu; qiwu55@cityu.edu.hk		Liu, Wei/0000-0002-3865-8145; Wu, Qi/0000-0002-4028-981X	Hong Kong Research Grants Council; Early Career Scheme [24200514];  [14211316];  [14206117]	Hong Kong Research Grants Council(Hong Kong Research Grants Council); Early Career Scheme; ; 	Qi WU acknowledges the financial support from the Hong Kong Research Grants Council, in particular the Early Career Scheme 24200514 and the General Research Funds 14211316 and 14206117.	Bali TG, 2008, J BANK FINANC, V32, P269, DOI 10.1016/j.jbankfin.2007.03.009; BOLLERSLEV T, 1986, J ECONOMETRICS, V31, P307, DOI 10.1016/0304-4076(86)90063-1; Carnero MA., 2004, J FINANCIAL ECONOMET, V2, P319, DOI DOI 10.1093/jjfinec/nbh012; Chen XP, 2018, COMPANION PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2018 (WWW 2018), P671, DOI 10.1145/3184558.3186584; Chernozhukov V, 2010, ECONOMETRICA, V78, P1093, DOI 10.3982/ECTA7880; Christoffersen PF, 1998, INT ECON REV, V39, P841, DOI 10.2307/2527341; Cont R, 2001, QUANT FINANC, V1, P223, DOI 10.1080/713665670; Dias A, 2013, J BANK FINANC, V37, P5248, DOI 10.1016/j.jbankfin.2013.04.015; Engle RF, 2004, J BUS ECON STAT, V22, P367, DOI 10.1198/073500104000000370; ENGLE RF, 1982, ECONOMETRICA, V50, P987, DOI 10.2307/1912773; Fan LJ, 2018, PROC CVPR IEEE, P6016, DOI 10.1109/CVPR.2018.00630; Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P55, DOI 10.1007/978-3-030-01264-9_4; Fleming J., 2003, J FINANC ECONOMET, V1, P365; Franses PH, 2008, J FINANC ECONOMET, V6, P291, DOI 10.1093/jjfinec/nbn008; Glasserman  Paul, 2018, MANAGEMENT SCI; GLOSTEN LR, 1993, J FINANC, V48, P1779, DOI 10.2307/2329067; HANSEN BE, 1994, INT ECON REV, V35, P705, DOI 10.2307/2527081; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; KOENKER R, 1978, ECONOMETRICA, V46, P33, DOI 10.2307/1913643; Kupiec, 1995, J DERIV, V3, P73, DOI [10.3905/jod.1995.407942, DOI 10.3905/jod.1995.407942]; Leon A, 2005, Q REV ECON FINANC, V45, P599, DOI 10.1016/j.qref.2004.12.020; Lipton Z, 2015, CRITICAL REV RECURRE, DOI DOI 10.1093/eurheartj/ehw128; NELSON DB, 1991, ECONOMETRICA, V59, P347, DOI 10.2307/2938260; Rockinger M, 2002, J ECONOMETRICS, V106, P119, DOI 10.1016/S0304-4076(01)00092-6; Sivakumar V, 2017, PR MACH LEARN RES, V70; Takeuchi I, 2006, J MACH LEARN RES, V7, P1231; Taylor S.J., 1994, MATH FINANC, V4, P183, DOI DOI 10.1111/J.1467-9965.1994.TB00057.X; Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795; Wang JW, 2018, PROC CVPR IEEE, P7190, DOI 10.1109/CVPR.2018.00751; Yang J., 2013, INT C MACH LEARN, P881; ZAKOIAN JM, 1994, J ECON DYN CONTROL, V18, P931, DOI 10.1016/0165-1889(94)90039-6	32	3	3	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301055
C	Zhang, HS; Chen, W; Liu, TY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Huishuai; Chen, Wei; Liu, Tie-Yan			On the Local Hessian in Back-propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NEURAL-NETWORKS	Back-propagation (BP) is the foundation for successfully training deep neural networks. However, BP sometimes has difficulties in propagating a learning signal deep enough effectively, e.g., the vanishing gradient phenomenon. Meanwhile, BP often works well when combining with "designing tricks" like orthogonal initialization, batch normalization and skip connection. There is no clear understanding on what is essential to the efficiency of BP. In this paper, we take one step towards clarifying this problem. We view BP as a solution of back-matching propagation which minimizes a sequence of back-matching losses each corresponding to one block of the network. We study the Hessian of the local back-matching loss (local Hessian) and connect it to the efficiency of BP. It turns out that those designing tricks facilitate BP by improving the spectrum of local Hessian. In addition, we can utilize the local Hessian to balance the training pace of each block and design new training algorithms. Based on a scalar approximation of local Hessian, we propose a scale-amended SGD algorithm. We apply it to train neural networks with batch normalization, and achieve favorable results over vanilla SGD. This corroborates the importance of local Hessian from another side.	[Zhang, Huishuai; Chen, Wei; Liu, Tie-Yan] Microsoft Res Asia, Beijing 100080, Peoples R China	Microsoft; Microsoft Research Asia	Zhang, HS (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.	huzhang@microsoft.com; wche@microsoft.com; tyliu@microsoft.com						Abadi M, 2015, P 12 USENIX S OPERAT; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bastien F., 2012, DEEP LEARN UNS FEAT; Ben-Tal A, 2001, LECT MODERN CONVEX O; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Carreira-Perpinan MA, 2014, JMLR WORKSH CONF PRO, V33, P10; Chen P, 2011, SIAM J NUMER ANAL, V49, P1417, DOI 10.1137/100799988; Cho M., 2017, NIPS, P5231; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Emin Orhan X.P.A., 2018, ICLR; Frerix T., 2018, INT C LEARN REPR ICL; Ganguli S., 2014, INT C LEARN REPR; Grosse R, 2016, PR MACH LEARN RES, V48; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Hinton, 2016, ARXIV PREPRINT ARXIV; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang L., 2017, ARXIV171002338; Jastrzebski S., 2018, SGD SMOOTHS SHARPEST; Jastrzebski S., 2018, INT C LEARN REPR ICL; Kolen J.F., 2001, GRADIENT FLOW RECURR, P237, DOI [10.1109/9780470544037.ch14, DOI 10.1109/9780470544037.CH14]; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lafond J., 2017, ARXIV170509319; LeCun, 1986, DISORDERED SYSTEMS B, DOI DOI 10.1007/978-3-642-82657-3_24; LECUN Y, 1991, PHYS REV LETT, V66, P2396, DOI 10.1103/PhysRevLett.66.2396; LeCun Y., 2016, ICLR, DOI DOI 10.1109/WCNC.2016.7564824; Lee DH, 2015, LECT NOTES ARTIF INT, V9284, P498, DOI 10.1007/978-3-319-23528-8_31; Li S., 2016, DEMYSTIFYING RESNET; Marenko V. A., 1967, MATH USSR SB, V1, P457, DOI [10.1070/SM1967v001n04ABEH001994, DOI 10.1070/SM1967V001N04ABEH001994]; Martens J., 2010, P 27 INT C MACH LEAR, P735; Martens J., 2016, THESIS; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Ollivier Y, 2015, INF INFERENCE, V4, P108, DOI 10.1093/imaiai/iav006; Pascanu R., 2014, ICLR; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Sagun Levent, 2017, ARXIV170604454; Seide F, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2135, DOI 10.1145/2939672.2945397; Singh B, 2015, 2015 IEEE 14TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P364, DOI 10.1109/ICMLA.2015.113; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Veit A, 2016, ADV NEUR IN, V29; Wiseman S., 2017, ARXIV170204770; Wright S. J, 2006, SEQUENTIAL QUADRATIC; Ye C., 2017, ARXIV PREPRINT ARXIV; You Y., 2017, ARXIV170803888V3; Zhang H., 2017, ARXIV171207296; Zoph B., 2016, ARXIV161101578	50	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001009
C	Zhang, MJ; Liu, XD; Wang, WH; Gao, JF; He, YX		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Minjia; Liu, Xiaodong; Wang, Wenhan; Gao, Jianfeng; He, Yuxiong			Navigating with Graph Representations for Fast and Scalable Decoding of Neural Language Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SMALL-WORLD	Neural language models (NLMs) have recently gained a renewed interest by achieving state-of-the-art performance across many natural language processing (NLP) tasks. However, NLMs are very computationally demanding largely due to the computational cost of the decoding process, which consists of a softmax layer over a large vocabulary. We observe that in the decoding of many NLP tasks, only the probabilities of the top-K hypotheses need to be calculated preciously and K is often much smaller than the vocabulary size. This paper proposes a novel softmax layer approximation algorithm, called Fast Graph Decoder (FGD), which quickly identifies, for a given context, a set of K words that are most likely to occur according to a NLM. We demonstrate that FGD reduces the decoding time by an order of magnitude while attaining close to the full softmax baseline accuracy on neural machine translation and language modeling tasks. We also prove the theoretical guarantee on the softmax approximation quality.	[Zhang, Minjia; Liu, Xiaodong; Wang, Wenhan; Gao, Jianfeng; He, Yuxiong] Microsoft, Redmond, WA 98052 USA	Microsoft	Zhang, MJ (corresponding author), Microsoft, Redmond, WA 98052 USA.	minjiaz@microsoft.com; xiaodl@microsoft.com; wenhanw@microsoft.com; jfgao@microsoft.com; yuxhe@microsoft.com						Bengio Y, 2001, ADV NEUR IN, V13, P932; Bengio Y., 2014, ARXIV14061078; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Boytsov L, 2013, LECT NOTES COMPUT SC, V8199, P280, DOI 10.1007/978-3-642-41062-8_28; Bradbury  James, 2016, ABS161101576 ARXIV; Cancho RFI, 2001, P ROY SOC B-BIOL SCI, V268, P2261, DOI 10.1098/rspb.2001.1800; Cettolo M., 2014, P 11 INT WORKSH SPOK, V57; Chen WL, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1975; Chiang M, 2016, IEEE INTERNET THINGS, V3, P854, DOI 10.1109/JIOT.2016.2584538; Chung J., 2014, ARXIV14123555; Devlin  Jacob, P 52 ANN M ASS COMP; Dorogovtsev SN, 2001, P ROY SOC B-BIOL SCI, V268, P2603, DOI 10.1098/rspb.2001.1824; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Filippova Katja, 2015, P 2015 C EMP METH NA, P360, DOI DOI 10.18653/V1/D15-1042; Frederic Morin, P 10 INT WORKSH ART; Gao JF, 2018, ACM/SIGIR PROCEEDINGS 2018, P1371, DOI 10.1145/3209978.3210183; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Gu  Jiatao, 2017, ARXIVABS171102281; Hannun A.Y., 2014, ARXIV14125567, P1; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jean S, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1; Jozefowicz R., 2016, ARXIV PREPRINT ARXIV; Klein G, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017): SYSTEM DEMONSTRATIONS, P67, DOI 10.18653/v1/P17-4012; Kleinberg J., 2000, Proceedings of the Thirty Second Annual ACM Symposium on Theory of Computing, P163, DOI 10.1145/335305.335325; Koehn P, 2007, 45 ANN M ASS COMP LI, P177, DOI DOI 10.3115/1557769.1557821; Lei  Tao, 2017, ARXIVABS170902755; Li Jiwei, 2016, P EMNLP; Luong M., 2015, P 2015 C EMP METH NA, P1412, DOI [10.18653/v1/D15-1166, DOI 10.18653/V1/D15-1166]; Malkov Y, 2014, INFORM SYST, V45, P61, DOI 10.1016/j.is.2013.10.006; Malkov Yury A., 2016, ABS160309320 CORR AR; Merity  Stephen, 2016, ABS160907843 CORR AR; Merity  Stephen, 2017, ARXIVABS170802182; Merity Stephen, 2018, ARXIV180308240; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; MILGRAM S, 1967, PSYCHOL TODAY, V1, P61; Mnih  Andriy, P 29 INT C MACH LEAR; Motter AE, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.065102; Munkres J.R., 2018, ELEMENTS ALGEBRAIC T; Newman MEJ, 2000, J STAT PHYS, V101, P819, DOI 10.1023/A:1026485807148; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Roy  Aurko, 2018, ARXIVABS180511063; Shim K, 2017, ADV NEUR IN, V30; Sordoni  Alessandro, 2015, NAACL HLT; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vinyals O, 2015, ICML DEEP LEARN WORK; Watts D. J., 1999, SMALL WORLDS DYNAMIC; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Yang  Zhilin, 2017, ABS171103953 CORR AR; Zweig G, 2017, INT CONF ACOUST SPEE, P4805, DOI 10.1109/ICASSP.2017.7953069	56	3	3	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000078
C	Zhe, SD; Du, YS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhe, Shandian; Du, Yishuai			Stochastic Nonparametric Event-Tensor Decomposition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Tensor decompositions are fundamental tools for multiway data analysis. Existing approaches, however, ignore the valuable temporal information along with data, or simply discretize them into time steps so that important temporal patterns are easily missed. Moreover, most methods are limited to multilinear decomposition forms, and hence are unable to capture intricate, nonlinear relationships in data. To address these issues, we formulate event-tensors, to preserve the complete temporal information for multiway data, and propose a novel Bayesian nonparametric decomposition model. Our model can (1) fully exploit the time stamps to capture the critical, causal/triggering effects between the interaction events, (2) flexibly estimate the complex relationships between the entities in tensor modes, and (3) uncover hidden structures from their temporal interactions. For scalable inference, we develop a doubly stochastic variational Expectation-Maximization algorithm to conduct an online decomposition. Evaluations on both synthetic and real-world datasets show that our model not only improves upon the predictive performance of existing methods, but also discovers interesting clusters underlying the data.	[Zhe, Shandian; Du, Yishuai] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA	Utah System of Higher Education; University of Utah	Zhe, SD (corresponding author), Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.	zhe@cs.utah.edu; yishuai.du@utah.edu						Acar E, 2011, CHEMOMETR INTELL LAB, V106, P41, DOI 10.1016/j.chemolab.2010.08.004; [Anonymous], 2012, ARXIV PREPRINT ARXIV; [Anonymous], 2014, ICML; Blundell C, 2012, ADV NEURAL INFORM PR, P2600; Choi JH, 2014, ADV NEUR IN, V27; Chu W., 2009, AISTATS; CINLAR E, 1968, J ROY STAT SOC B, V30, P576; De Reuck J, 2009, ACTA NEUROL BELG, V109, P271; Du N, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P219, DOI 10.1145/2783258.2783411; DuBois C., 2010, P 16 ACM SIGKDD INT, P803, DOI DOI 10.1145/1835804.1835906; DuBois C, 2013, J MATH PSYCHOL, V57, P297, DOI 10.1016/j.jmp.2013.04.001; Hansen S, 2015, OPTIM METHOD SOFTW, V30, P1002, DOI 10.1080/10556788.2015.1009977; Harshman R.A., 1970, MULTIMODAL FACTOR AN; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; He X., 2015, ICML, P871; Hoff PD, 2011, COMPUT STAT DATA AN, V55, P530, DOI 10.1016/j.csda.2010.05.020; Hu C., 2015, UAI; Hu CW, 2015, LECT NOTES ARTIF INT, V9285, P53, DOI 10.1007/978-3-319-23525-7_4; Kang U, 2012, P 18 ACM SIGKDD INT, P316, DOI 10.1145/2339530.2339583; Rai P., 2014, P 31 INT C MACH LEAR; Rai P., 2015, IJCAI; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Schein A, 2016, PR MACH LEARN RES, V48; Schein A, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1045, DOI 10.1145/2783258.2783414; Shashua A., 2005, P 22 INT C MACHINE L, P792, DOI [10.1145/1102351.1102451, DOI 10.1145/1102351.1102451]; Sutskever I., 2009, P 22 INT C NEURAL IN, P1821; Titsias M. K., 2009, ARTIF INTELL STAT, V3; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; Wang Y., 2017, ADV NEURAL INFORM PR, P1644; Xiong L., 2010, P SDM COL OH, P211; Xu H, 2016, INT C MACH LEARN, P1717; Xu H., 2017, NTERN C MACH LEARN; Xu Z., 2012, P 29 INT C MACH LEAR; Yang Y., 2013, J ROYAL STAT B UNPUB; Zhe S., 2016, 30 AAAI C ART INT; Zhe S., 2016, ADV NEURAL INFORM PR, P928; Zhe SD, 2015, JMLR WORKSH CONF PRO, V38, P1125; Zhou K, 2013, ICML	39	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001040
C	Zhou, MY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhou, Mingyuan			Parsimonious Bayesian deep networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MODELS	Combining Bayesian non-parametrics and a forward model selection strategy, we construct parsimonious Bayesian deep networks (PBDNs) that infer capacity-regularized network architectures from the data and require neither cross-validation nor fine-tuning when training the model. One of the two essential components of a PBDN is the development of a special infinite-wide single-hidden-layer neural network, whose number of active hidden units can be inferred from the data. The other one is the construction of a greedy layer-wise learning algorithm that uses a forward model selection criterion to determine when to stop adding another hidden layer. We develop both Gibbs sampling and stochastic gradient descent based maximum a posteriori inference for PBDNs, providing state-of-the-art classification accuracy and interpretable data subtypes near the decision boundaries, while maintaining low computational complexity for out-of-sample prediction.	[Zhou, Mingyuan] Univ Texas Austin, McCombs Sch Business, Dept IROM, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Zhou, MY (corresponding author), Univ Texas Austin, McCombs Sch Business, Dept IROM, Austin, TX 78712 USA.	mingyuan.zhou@mccombs.utexas.edu	Zhou, Mingyuan/AAE-8717-2021		U.S. National Science Foundation [IIS-1812699]; NVIDIA Corporation	U.S. National Science Foundation(National Science Foundation (NSF)); NVIDIA Corporation	M. Zhou acknowledges the support of Award IIS-1812699 from the U.S. National Science Foundation, the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research, and the computational support of Texas Advanced Computing Center.	Abadi M, 2015, P 12 USENIX S OPERAT; Aiolli F, 2005, J MACH LEARN RES, V6, P817; ARORA S., 2016, ARXIV161208795; Bishop, 1995, NEURAL NETWORKS PATT; Bucila C, 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464; Caron F, 2017, J R STAT SOC B, V79, P1295, DOI 10.1111/rssb.12233; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chang Y.W., 2010, J MACH LEARN RES, V11, P1471, DOI DOI 10.5555/1756006.1859899; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Diethe T., 2015, 13 BENCHMARK DATASET; Djuric N, 2013, J MACH LEARN RES, V14, P3813; Dunson DB, 2005, BIOSTATISTICS, V6, P11, DOI 10.1093/biostatistics/kxh025; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; Glorot X., 2011, P 14 INT C ART INT S, P315; Gong Yunchao, 2014, ARXIV14126115; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; Hannah LA, 2011, J MACH LEARN RES, V12, P1923; Hinton G., 2015, ARXIV150302531; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kantchelian A, 2014, ADV NEUR IN, V27; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Manwani N., 2011, ARXIV11071564; Manwani N, 2010, JMLR WORKSH CONF PRO, V13, P17; Nair V, 2010, P 27 INT C MACHINE L, P807; Polson N. G., 2011, ARXIV11094180V1; Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001; Rai P., 2015, ADV NEURAL INF PROCE, V28, P1805; Ratsch G, 2001, MACH LEARN, V42, P287, DOI 10.1023/A:1007618119488; Scholkopf B, 1999, ADVANCES IN KERNEL METHODS, P327; Shang WL, 2016, PR MACH LEARN RES, V48; Steinwart I, 2004, J MACH LEARN RES, V4, P1071, DOI 10.1162/1532443041827925; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Vapnik VN, 1998, STAT LEARNING THEORY, DOI DOI 10.1007/978-1-4419-1428-6_5864; Wang Z, 2011, P 17 ACM SIGKDD INT, P24; Zhang Quan, 2018, NEURIPS; Zhou M., 2015, NIPS; Zhou M., 2018, ARXIV180207721; Zhou M., 2016, ARXIV160806383; Zhou MY, 2015, JMLR WORKSH CONF PRO, V38, P1135; Zhou MY, 2016, J MACH LEARN RES, V17, P1; Zhou Mingyuan, 2012, Proc Int Conf Mach Learn, V2012, P1343; Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211; Zhou Mingyuan, 2012, AISTATS, P1462	49	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303021
C	Zhou, P; Yuan, XT; Feng, JS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhou, Pan; Yuan, Xiao-Tong; Feng, Jiashi			Efficient Stochastic Gradient Hard Thresholding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Stochastic gradient hard thresholding methods have recently been shown to work favorably in solving large-scale empirical risk minimization problems under sparsity or rank constraint. Despite the improved iteration complexity over full gradient methods, the gradient evaluation and hard thresholding complexity of the existing stochastic algorithms usually scales linearly with data size, which could still be expensive when data is huge and the hard thresholding step could be as expensive as singular value decomposition in rank-constrained problems. To address these deficiencies, we propose an efficient hybrid stochastic gradient hard thresholding (HSG-HT) method that can be provably shown to have sample-size-independent gradient evaluation and hard thresholding complexity bounds. Specifically, we prove that the stochastic gradient evaluation complexity of HSG-HT scales linearly with inverse of sub-optimality and its hard thresholding complexity scales logarithmically. By applying the heavy ball acceleration technique, we further propose an accelerated variant of HSG-HT which can be shown to have improved factor dependence on restricted condition number in the quadratic case. Numerical results confirm our theoretical affirmation and demonstrate the computational efficiency of the proposed methods.	[Zhou, Pan; Feng, Jiashi] Natl Univ Singapore, Learning & Vis Lab, Singapore, Singapore; [Yuan, Xiao-Tong] Nanjing Univ Informat Sci & Technol, B DAT Lab, Nanjing, Jiangsu, Peoples R China	National University of Singapore; Nanjing University of Information Science & Technology	Zhou, P (corresponding author), Natl Univ Singapore, Learning & Vis Lab, Singapore, Singapore.	pzhou@u.nus.edu; xtyuan@nuist.edu.cn; elefjia@nus.edu.sg	Feng, Jiashi/AGX-6209-2022		NUS [R-263-000-C08-133]; MOE [R-263-000-C21-112, R-263-000-D17-112]; Natural Science Foundation of China (NSFC) [61522308, 61876090]; Tencent AI Lab Rhino-Bird Joint Research Program [JR201801]; NUS IDS [R-263-000-C67-646]; ECRA [R-263-000-C87-133]	NUS(National University of Singapore); MOE(Ministry of Higher Education & Scientific Research (MHESR)); Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Tencent AI Lab Rhino-Bird Joint Research Program; NUS IDS; ECRA	Jiashi Feng was partially supported by NUS startup R-263-000-C08-133, MOE Tier-I R-263-000-C21-112, NUS IDS R-263-000-C67-646, ECRA R-263-000-C87-133 and MOE Tier-II R-263-000-D17-112. Xiao-Tong Yuan was supported in part by Natural Science Foundation of China (NSFC) under Grant 61522308 and Grant 61876090, and in part by Tencent AI Lab Rhino-Bird Joint Research Program No. JR201801.	Amit Y., 2007, ICML 07 P 24 INT C M, P17, DOI DOI 10.1145/1273496.1273499; Bahmani S, 2013, J MACH LEARN RES, V14, P807; Bertsekas DP, 1997, SIAM J OPTIMIZ, V7, P913, DOI 10.1137/S1052623495287022; Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002; Chen J., 2016, P C UNC ART INT; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Foucart S, 2011, SIAM J NUMER ANAL, V49, P2543, DOI 10.1137/100806278; Friedlander MP, 2012, SIAM J SCI COMPUT, V34, pA1380, DOI 10.1137/110830629; Garg R., 2009, P 26 ANN INT C MACHI, P337; Govan A., 2006, N CAROLINA STATE U S; Jain P., 2017, ARXIV170408227; Jalali A., 2011, ADV NEURAL INF PROCE, V24, P1935; Kar P., 2014, ADV NEURAL INFORM PR, P685; Khanna R, 2018, PR MACH LEARN RES, V84; Li X., 2016, P INT C MACH LEARN; Lin Q., 2014, ADV NEURAL INFORM PR, P3059; Nguyen N, 2017, IEEE T INFORM THEORY, V63, P6869, DOI 10.1109/TIT.2017.2749330; Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850; Nesterov Y., 2018, APPL OPTIMIZATION; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Reddi Sashank J., 2018, INT C LEARN REPR; Rohde A, 2011, ANN STAT, V39, P887, DOI 10.1214/10-AOS860; Shen J, 2018, J MACH LEARN RES, V18; Srebro N., 2005, P ADV NEURAL INFORM; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108; van de Geer SA, 2008, ANN STAT, V36, P614, DOI 10.1214/009053607000000929; Wang Y., 2017, AAAI C ART INT; Yuan X., 2016, P 30 ANN C NEUR INF, P3558; Yuan XT, 2018, J MACH LEARN RES, V18; Zhang, 2013, ADV NEURAL INFORM PR, P315; Zhang YC, 2015, PR MACH LEARN RES, V37, P353; Zhou P., 2018, P C NEUTR INF PROC S; Zhou P., 2017, P IEEE C COMP VIS PA, P1; Zhou P, 2018, IEEE T IMAGE PROCESS, V27, P1152, DOI 10.1109/TIP.2017.2762595	37	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302003
C	Abernethy, J; Wang, JK		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Abernethy, Jacob; Wang, Jun-Kun			On Frank-Wolfe and Equilibrium Computation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ALGORITHMS	We consider the Frank-Wolfe (FW) method for constrained convex optimization, and we show that this classical technique can be interpreted from a different perspective: FW emerges as the computation of an equilibrium (saddle point) of a special convex-concave zero sum game. This saddle-point trick relies on the existence of no-regret online learning to both generate a sequence of iterates but also to provide a proof of convergence through vanishing regret. We show that our stated equivalence has several nice properties, as it exhibits a modularity that gives rise to various old and new algorithms. We explore a few such resulting methods, and provide experimental results to demonstrate correctness and efficiency.	[Abernethy, Jacob; Wang, Jun-Kun] Georgia Inst Technol, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Abernethy, J (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	prof@gatech.edu; jimwang@gatech.edu	Jeong, Yongwook/N-7413-2016					Abernethy J, 2016, P 33 INT C MACH LEAR, V48, P2520; Abernethy J. D., 2014, COLT, P807; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Bach F., 2015, SIAM J OPTIMIZATION; Beck Amir, 2016, MATH PROGRAMMING; Boyd S, 2004, CONVEX OPTIMIZATION; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Christiano P, 2011, ACM S THEORY COMPUT, P273; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Freund Y., 1996, Proceedings of the Ninth Annual Conference on Computational Learning Theory, P325, DOI 10.1145/238061.238163; Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738; Garber D., 2015, ICML; Garber D., 2016, NIPS; Garber Dan, 2016, SIAM J OPTIMIZATION; Garber Dan, 2013, FOCS; Gidel G., 2016, AISTATS; Gorni Gianluca, 1991, J MATH ANAL APPL; Harchaoui Zaid, 2013, MATH PROG A; Hazan E., 2014, INTRO ONLINE CONVEX; Hazan E., 2016, ICML; Huang R., 2016, FOLLOWING LEADER FAS; Jaggi Martin., 2013, ICML; Kakade S. M., 2009, DUALITY STRONG CONVE; Kakutani S., 1941, GEN BROUWERS FIXED P; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Koren T, 2012, ICML; Lacoste-Julien S., 2015, NIPS; Lacoste-Julien S., 2013, INT C MACH LEARN PML, P53; Lan G, 2013, COMPLEXITY LARGE SCA; Lan Guanghui, 2014, SIAM J OPTIMIZATION; Osokin A., 2016, ICML; Sion M., 1958, PAC J MATH, V8, P171, DOI [10.2140/pjm.1958.8.171, DOI 10.2140/PJM.1958.8.171]; Torrance G W, 1989, Int J Technol Assess Health Care, V5, P559; Wang Y. X., 2016, ICML; Wolf P., 1970, INTEGER NONLINEAR PR; Yu Y., 2014, ARXIV14104828	39	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406063
C	Agrawal, S; Jia, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Agrawal, Shipra; Jia, Randy			Optimistic posterior sampling for reinforcement learning: worst-case regret bounds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We present an algorithm based on posterior sampling (aka Thompson sampling) that achieves near-optimal worst-case regret bounds when the underlying Markov Decision Process (MDP) is communicating with a finite, though unknown, diameter. Our main result is a high probability regret upper bound of (O) over tilde (D root SAT) for any communicating MDP with S states, A actions and diameter D, when T >= S-5 A. Here, regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy, in time horizon T. This result improves over the best previously known upper bound of (O) over tilde (DS root AT) achieved by any algorithm in this setting, and matches the dependence on S in the established lower bound of Omega(root DSAT) for this problem. Our techniques involve proving some novel results about the anti-concentration of Dirichlet distribution, which may be of independent interest.	[Agrawal, Shipra; Jia, Randy] Columbia Univ, New York, NY 10027 USA	Columbia University	Agrawal, S (corresponding author), Columbia Univ, New York, NY 10027 USA.	sa3305@columbia.edu; rqj2000@columbia.edu						Abbasi-Yadkori Yasin, 2014, ARXIV14063926; Agrawal S., 2013, ARTIF INTELL, P99; Agrawal S, 2013, P 30 INT C MACH LEAR; Agrawal Shipra, 2012, COLT; [Anonymous], [No title captured]; Asmuth J., 2009, P 25 C UNC ART INT, P19; BARTLETT PL, 2009, P 25 C UNC ART INT, P35; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Bubeck, 2013, ADV NEURAL INFORM PR, P638; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Burnetas AN, 1997, MATH OPER RES, V22, P222, DOI 10.1287/moor.22.1.222; Fonteneau R, 2013, NIPS 2013 WORKSH BAY; Grinstead C.M., 2012, INTRO PROBABILITY; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Kakade Sham M., 2003, THESIS; Kaufmann E., 2012, INT C ALG LEARN THEO; Kleinberg R, 2008, ACM S THEORY COMPUT, P681; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Osband I., 2016, ARXIV160700215; Osband I., 2014, GEN EXPLORATION VIA; Puterman M.L., 2014, MARKOV DECISION PROC; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Russo Daniel, 2015, J MACHINE LEARNING R; Seldin Y, 2012, IEEE T INFORM THEORY, V58, P7086, DOI 10.1109/TIT.2012.2211334; Shevtsova IG, 2010, DOKL MATH, V82, P862, DOI 10.1134/S1064562410060062; Stegun I., 1964, HDB MATH FUNCTIONS F, V55; Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009; Strehl Alexander L., 2005, P 22 INT C MACH LEAR, P856, DOI [DOI 10.1145/1102351.1102459, 10.1145/1102351.1102459]; TEWARI A, 2008, ADV NEURAL INFORM PR, P1505; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Van Roy, 2013, ADV NEURAL INFORM PR, P3003	33	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401022
C	Aitchison, L; Russell, L; Packer, A; Yan, JY; Castonguay, P; Mausser, M; Turaga, SC		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Aitchison, Laurence; Russell, Lloyd; Packer, Adam; Yan, Jinyao; Castonguay, Philippe; Mausser, Michael; Turaga, Srinivas C.			Model-based Bayesian inference of neural activity and connectivity from all-optical interrogation of a neural circuit	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Population activity measurement by calcium imaging can be combined with cellular resolution optogenetic activity perturbations to enable the mapping of neural connectivity in vivo. This requires accurate inference of perturbed and unperturbed neural activity from calcium imaging measurements, which are noisy and indirect, and can also be contaminated by photostimulation artifacts. We have developed a new fully Bayesian approach to jointly inferring spiking activity and neural connectivity from in vivo all-optical perturbation experiments. In contrast to standard approaches that perform spike inference and analysis in two separate maximum-likelihood phases, our joint model is able to propagate uncertainty in spike inference to the inference of connectivity and vice versa. We use the framework of variational autoencoders to model spiking activity using discrete latent variables, low-dimensional latent common input, and sparse spike-and-slab generalized linear coupling between neurons. Additionally, we model two properties of the optogenetic perturbation: off-target photostimulation and photostimulation transients. Using this model, we were able to fit models on 30 minutes of data in just 10 minutes. We performed an all-optical circuit mapping experiment in primary visual cortex of the awake mouse, and use our approach to predict neural connectivity between excitatory neurons in layer 2/3. Predicted connectivity is sparse and consistent with known correlations with stimulus tuning, spontaneous correlation and distance.	[Aitchison, Laurence] Univ Cambridge, Cambridge CB2 1PZ, England; [Russell, Lloyd; Packer, Adam; Mausser, Michael] UCL, London WC1E 6BT, England; [Yan, Jinyao; Castonguay, Philippe; Turaga, Srinivas C.] Janelia Res Campus, Ashburn, VA 20147 USA	University of Cambridge; University of London; University College London	Aitchison, L (corresponding author), Univ Cambridge, Cambridge CB2 1PZ, England.	laurence.aitchison@gmail.com; llerussell@gmail.com; adampacker@gmail.com; yanj11@janelia.hhmi.org; ph.castonguay@gmail.com; m.hausser@ucl.ac.uk; turagas@janelia.hhmi.org	Jeong, Yongwook/N-7413-2016; Hausser, Michael/AAW-6827-2020	Hausser, Michael/0000-0002-2673-8957; Packer, Adam/0000-0001-5884-794X; Russell, Lloyd/0000-0001-6332-756X				Abadi M, 2015, P 12 USENIX S OPERAT; Cossell L, 2015, NATURE, V518, P399, DOI 10.1038/nature14182; Djork-Arn, ICLR 2016; Jang E., 2016, ARXIV; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D.P, P 3 INT C LEARNING R; Ko H, 2011, NATURE, V473, P87, DOI 10.1038/nature09880; Lee WCA, 2016, NATURE, V532, P370, DOI 10.1038/nature17192; Maddison Chris J, 2016, ARXIV161100712; Mishchenko Y, 2011, ANN APPL STAT, V5, P1229, DOI 10.1214/09-AOAS303; Mnih A, 2016, PR MACH LEARN RES, V48; Okun M, 2015, NATURE, V521, P511, DOI 10.1038/nature14273; Packer AM, 2015, NAT METHODS, V12, P140, DOI 10.1038/nmeth.3217; Packer AM, 2012, NAT METHODS, V9, P1202, DOI [10.1038/NMETH.2249, 10.1038/nmeth.2249]; Raina R., 2009, P 26 ANN INT C MACH, P873; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Shababo B., 2013, ADV NEURAL INFORM PR, V26, P1304; Soudry D, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004464; Takemura S, 2013, NATURE, V500, P175, DOI 10.1038/nature12450; Vogelstein JT, 2010, J NEUROPHYSIOL, V104, P3691, DOI 10.1152/jn.01073.2009	20	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403054
C	Ambrogioni, L; Hinne, M; van Gerven, MAJ; Maris, E		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ambrogioni, Luca; Hinne, Max; van Gerven, Marcel A. J.; Maris, Eric			GP CaKe: Effective brain connectivity with causal kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				GRANGER CAUSALITY; RESPONSES; NETWORKS; FMRI	A fundamental goal in network neuroscience is to understand how activity in one brain region drives activity elsewhere, a process referred to as effective connectivity. Here we propose to model this causal interaction using integro-differential equations and causal kernels that allow for a rich analysis of effective connectivity. The approach combines the tractability and flexibility of autoregressive modeling with the biophysical interpretability of dynamic causal modeling. The causal kernels are learned nonparametrically using Gaussian process regression, yielding an efficient framework for causal inference. We construct a novel class of causal covariance functions that enforce the desired properties of the causal kernels, an approach which we call GP CaKe. By construction, the model and its hyperparameters have biophysical meaning and are therefore easily interpretable. We demonstrate the efficacy of GP CaKe on a number of simulations and give an example of a realistic application on magnetoencephalography (MEG) data.	[Ambrogioni, Luca; Hinne, Max; van Gerven, Marcel A. J.; Maris, Eric] Radboud Univ Nijmegen, Nijmegen, Netherlands	Radboud University Nijmegen	Ambrogioni, L (corresponding author), Radboud Univ Nijmegen, Nijmegen, Netherlands.	l.ambrogioni@donders.ru.nl; m.hinne@donders.ru.nl; m.vangerven@donders.ru.nl; e.maris@donders.ru.nl	Hinne, Max/ABD-1486-2021; Jeong, Yongwook/N-7413-2016	Hinne, Max/0000-0002-9279-6725; 				Ambrogioni L., 2016, ARXIV161110073; Ambrogioni L, 2016, ARXIV160502609; Bressler SL, 2011, NEUROIMAGE, V58, P323, DOI 10.1016/j.neuroimage.2010.02.059; Bressler SL, 2010, TRENDS COGN SCI, V14, P277, DOI 10.1016/j.tics.2010.04.004; Coombes S., 2014, NEURAL FIELDS; David O, 2006, NEUROIMAGE, V30, P1255, DOI 10.1016/j.neuroimage.2005.10.045; Dhamala M, 2008, NEUROIMAGE, V41, P354, DOI 10.1016/j.neuroimage.2008.02.020; Fornito A, 2015, EUR NEUROPSYCHOPHARM, V25, P733, DOI 10.1016/j.euroneuro.2014.02.011; Friston KJ, 2013, CURR OPIN NEUROBIOL, V23, P172, DOI 10.1016/j.conb.2012.11.010; Friston KJ, 2011, BRAIN CONNECT, V1, P13, DOI 10.1089/brain.2011.0008; Friston KJ, 2000, NEUROIMAGE, V12, P466, DOI 10.1006/nimg.2000.0630; Hinne M, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004534; Hinne M, 2014, NEUROIMAGE, V86, P294, DOI 10.1016/j.neuroimage.2013.09.075; Hjort N. L., 2010, BAYESIAN NONPARAMETR; Kaminski M, 2001, BIOL CYBERN, V85, P145, DOI 10.1007/s004220000235; Koch C., 2004, COMPUTATIONAL NEUROS; Pinotsis DA, 2012, NEUROIMAGE, V59, P1261, DOI [10.1016/j.neuroimage.2011.08.020, 10.1016/j.neuroimage.2010.12.039]; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Sameshima K, 1999, J NEUROSCI METH, V94, P93, DOI 10.1016/S0165-0270(99)00128-4; Schelter B, 2009, J NEUROSCI METH, V179, P121, DOI 10.1016/j.jneumeth.2009.01.006; Stephan KE, 2012, NEUROIMAGE, V62, P856, DOI 10.1016/j.neuroimage.2012.01.034; Tauber UC, 2014, CRITICAL DYNAMICS: A FIELD THEORY APPROACH TO EQUILIBRIUM AND NON-EQUILIBRIUM SCALING BEHAVIOR, P1	22	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400091
C	Amin, K; Jiang, N; Singh, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Amin, Kareem; Jiang, Nan; Singh, Satinder			Repeated Inverse Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We introduce a novel repeated Inverse Reinforcement Learning problem: the agent has to act on behalf of a human in a sequence of tasks and wishes to minimize the number of tasks that it surprises the human by acting suboptimally with respect to how the human would have acted. Each time the human is surprised, the agent is provided a demonstration of the desired behavior by the human. We formalize this problem, including how the sequence of tasks is chosen, in a few different ways and provide some foundational results.	[Amin, Kareem] Google Res, New York, NY 10011 USA; [Jiang, Nan; Singh, Satinder] Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48104 USA	Google Incorporated; University of Michigan System; University of Michigan	Amin, K (corresponding author), Google Res, New York, NY 10011 USA.	kamin@google.com; nanjiang@umich.edu; baveja@umich.edu	Jeong, Yongwook/N-7413-2016		NSF [IIS 1319365]; University of Michigan	NSF(National Science Foundation (NSF)); University of Michigan(University of Michigan System)	This work was supported in part by NSF grant IIS 1319365 (Singh & Jiang) and in part by a Rackham Predoctoral Fellowship from the University of Michigan (Jiang). Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the views of the sponsors.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Abbeel R., 2006, P NEURAL INFORM PROC, P1; Amodei D., 2016, CONCRETE PROBLEMS AI; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Bostrom N., 2003, SCI FICTION PHILOS T, P277; Chajewska U, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P363; Coates A., 2008, P 25 INT C MACH LEAR, P144, DOI DOI 10.1145/1390156.1390175; Dani V, 2008, P C LEARN THEOR COLT, P355; Grotschel M., 2012, GEOMETRIC ALGORITHMS; Hadfield-Menell D, 2016, ADV NEURAL INFORM PR, V29, P3909; ODonnell Ryan, 2011, 15 859 E LINEAR SEMI; Ramachandran D., 2007, URBANA, V51, P61801; Ratliff N. D., 2006, P 23 INT C MACH LEAR, P729, DOI DOI 10.1145/1143844.1143936; Regan K., 2010, AAAI; Regan K., 2009, P 25THCONFERENCE UNC, P444; Regan K, 2011, P IJCAI 2011, P2159; Rothkopf CA, 2011, LECT NOTES ARTIF INT, V6913, P34, DOI 10.1007/978-3-642-23808-6_3; Russell S, 2015, AI MAG, V36, P105, DOI 10.1609/aimag.v36i4.2577; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Singh S., 2016, ARXIV160106569; Syed U., 2007, ADV NEURAL INFORM PR; Von Neumann J., 2007, THEORY GAMES EC BEHA; Ziebart B. D., 2008, AAAI, V8, P1433	24	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401082
C	Arora, R; Marinov, TV; Mianjy, P; Srebro, N		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Arora, Raman; Marinov, Teodor V.; Mianjy, Poorya; Srebro, Nathan			Stochastic Approximation for Canonical Correlation Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose novel first-order stochastic approximation algorithms for canonical correlation analysis (CCA). Algorithms presented are instances of inexact matrix stochastic gradient (MSG) and inexact matrix exponentiated gradient (MEG), and achieve epsilon-suboptimality in the population objective in poly(1/epsilon) iterations. We also consider practical variants of the proposed algorithms and compare them with other methods for CCA both theoretically and empirically.	[Arora, Raman; Marinov, Teodor V.; Mianjy, Poorya] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21204 USA; [Srebro, Nathan] TTI Chicago, Chicago, IL 60637 USA	Johns Hopkins University	Arora, R (corresponding author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21204 USA.	arora@cs.jhu.edu; tmarino2@jhu.edu; mianjy@jhu.edu; nati@ttic.edu	Jeong, Yongwook/N-7413-2016		NSF BIGDATA grant [IIS-1546482]	NSF BIGDATA grant	This research was supported in part by NSF BIGDATA grant IIS-1546482.	Alexei V., 2003, NIPS, P1497; Allen-Zhu Z., 2017, INT C MACH LEARN ICM; [Anonymous], 2016, P 54 ANN M ASS COMP; Arora R., 2016, INT C MACH LEARN ICM, P1786; Arora R., 2013, ADV NEURAL INFORM PR; Arora R, 2013, INT CONF ACOUST SPEE, P7135, DOI 10.1109/ICASSP.2013.6639047; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Gao C., 2017, ARXIV170206533; Ge R, 2016, PR MACH LEARN RES, V48; GOLDEN S, 1965, PHYS REV, V137, P1127; Herbster M, 2001, J MACH LEARN RES, V1, P281, DOI 10.1162/153244301753683726; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Kale Satyen, 2007, EFFICIENT ALGORITHMS; Kidron E, 2005, PROC CVPR IEEE, P88; Lu Y., 2014, ADV NEURAL INF PROCE, P91; Ma Z, 2015, PR MACH LEARN RES, V37, P169; Schmidt M., 2011, ADV NEURAL INFORM PR, P1458; Shalev-Shwartz S., 2008, P 25 INT C MACH LEAR, V307, P928, DOI DOI 10.1145/1390156.1390273; Snoek C.G., 2006, P 14 ANN ACM INT C M, P421, DOI DOI 10.1145/1180639.1180727; Tropp JA, 2015, FOUND TRENDS MACH LE, V8, P2, DOI 10.1561/2200000048; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; Wang Weiran, 2016, NIPS; Warmuth M. K., 2006, NIPS 06; Warmuth M. K., 2008, J MACHINE LEARNING R, V9; Warmuth MK, 2006, LECT NOTES ARTIF INT, V4005, P514, DOI 10.1007/11776420_38	27	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404082
C	Boomsma, W; Frellsen, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Boomsma, Wouter; Frellsen, Jes			Spherical convolutions and their application in molecular modelling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Convolutional neural networks are increasingly used outside the domain of image analysis, in particular in various areas of the natural sciences concerned with spatial data. Such networks often work out-of-the box, and in some cases entire model architectures from image analysis can be carried over to other problem domains almost unaltered. Unfortunately, this convenience does not trivially extend to data in non-euclidean spaces, such as spherical data. In this paper, we introduce two strategies for conducting convolutions on the sphere, using either a spherical-polar grid or a grid based on the cubed-sphere representation. We investigate the challenges that arise in this setting, and extend our discussion to include scenarios of spherical volumes, with several strategies for parameterizing the radial dimension. As a proof of concept, we conclude with an assessment of the performance of spherical convolutions in the context of molecular modelling, by considering structural environments within proteins. We show that the models are capable of learning non-trivial functions in these molecular environments, and that our spherical convolutions generally outperform standard 3D convolutions in this setting. In particular, despite the lack of any domain specific feature-engineering, we demonstrate performance comparable to state-of-the-art methods in the field, which build on decades of domain-specific knowledge.	[Boomsma, Wouter] Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark; [Frellsen, Jes] IT Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark	University of Copenhagen; IT University Copenhagen	Boomsma, W (corresponding author), Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark.	wb@di.ku.dk; jefr@itu.dk	Boomsma, Wouter/K-2903-2014; Jeong, Yongwook/N-7413-2016	Boomsma, Wouter/0000-0002-8257-3827; 	Villum Foundation [VKR023445]	Villum Foundation(Villum Fonden)	This work was supported by the Villum Foundation (W.B., grant number VKR023445).	[Anonymous], 2015, 151002855 ARXIV; [Anonymous], 2017, BRIEF BIOINFORM, DOI DOI 10.1093/bib/bbw068; [Anonymous], 2015, CHANGINGCROPPINGPATT; Aurisano A, 2016, J INSTRUM, V11, DOI 10.1088/1748-0221/11/09/P09001; Behler J, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.146401; Boomsma W, 2008, P NATL ACAD SCI USA, V105, P8932, DOI 10.1073/pnas.0801715105; Boomsma W, 2014, P NATL ACAD SCI USA, V111, P13852, DOI 10.1073/pnas.1404948111; Cohen TS, 2016, PR MACH LEARN RES, V48; Cohen TS, 2018, P 6 INT C LEARNING R; Conchuir SO, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130433; Eastman P, 2013, J CHEM THEORY COMPUT, V9, P461, DOI 10.1021/ct300857j; Glorot X., 2011, P 14 INT C ART INT S, P315; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Hahnloser RHR, 2000, NATURE, V405, P947, DOI 10.1038/35016072; Hornak V, 2006, PROTEINS, V65, P712, DOI 10.1002/prot.21123; Irback A, 2006, J COMPUT CHEM, V27, P1548, DOI 10.1002/jcc.20452; Jasrasaria D., 2016, 160805747 ARXIV; KABSCH W, 1983, BIOPOLYMERS, V22, P2577, DOI 10.1002/bip.360221211; Kingma D.P., 2015, INT C LEARN REPR, P1; Mardia KV, 2009, DIRECTIONAL STAT; Mills K, 2017, PHYS REV A, V96, DOI 10.1103/PhysRevA.96.042113; Ronchi C, 1996, J COMPUT PHYS, V124, P93, DOI 10.1006/jcph.1996.0047; Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890; Smith JS, 2017, CHEM SCI, V8, P3192, DOI 10.1039/c6sc05720a; Torng W, 2017, BMC BIOINFORMATICS, V18, DOI 10.1186/s12859-017-1702-0; Wang GL, 2003, BIOINFORMATICS, V19, P1589, DOI 10.1093/bioinformatics/btg224; Wang S., 2016, SCI REPORTS, V6; Word JM, 1999, J MOL BIOL, V285, P1735, DOI 10.1006/jmbi.1998.2401	28	3	3	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403049
C	Bouchard, KE; Bujan, AF; Roosta-Khorasani, F; Ubaru, S; Prabhat; Snijders, AM; Mao, JH; Chang, EF; Mahoney, MW; Bhattacharyya, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Bouchard, Kristofer E.; Bujan, Alejandro F.; Roosta-Khorasani, Farbod; Ubaru, Shashanka; Prabhat; Snijders, Antoine M.; Mao, Jian-Hua; Chang, Edward F.; Mahoney, Michael W.; Bhattacharyya, Sharmodeep			Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SELECTION	The increasing size and complexity of scientific data could dramatically enhance discovery and prediction for basic scientific applications. Realizing this potential, however, requires novel statistical analysis methods that are both interpretable and predictive. We introduce Union of Intersections (UoI), a flexible, modular, and scalable framework for enhanced model selection and estimation. Methods based on UoI perform model selection and model estimation through intersection and union operations, respectively. We show that UoI-based methods achieve low-variance and nearly unbiased estimation of a small number of interpretable features, while maintaining high-quality prediction accuracy. We perform extensive numerical investigation to evaluate a UoI algorithm (UoI(Lasso)) on synthetic and real data. In doing so, we demonstrate the extraction of interpretable functional networks from human electrophysiology recordings as well as accurate prediction of phenotypes from genotype-phenotype data with reduced features. We also show (with the UoI(L1Logistic) and UoI(CUR) variants of the basic framework) improved prediction parsimony for classification and matrix factorization on several benchmark biomedical data sets. These results suggest that methods based on the UoI framework could improve interpretation and prediction in data-driven discovery across scientific fields.	[Bouchard, Kristofer E.] LBNL, Biol Syst & Engn Div, Berkeley, CA 94720 USA; [Bujan, Alejandro F.] Univ Calif Berkeley, Redwood Ctr, Berkeley, CA USA; [Roosta-Khorasani, Farbod; Mahoney, Michael W.] Univ Calif Berkeley, ICSI, Berkeley, CA USA; [Roosta-Khorasani, Farbod; Mahoney, Michael W.] Univ Calif Berkeley, Dept Stat, Berkeley, CA USA; [Ubaru, Shashanka] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA; [Prabhat] LBNL, NERSC, Berkeley, CA USA; [Snijders, Antoine M.; Mao, Jian-Hua] LBNL, Biol Syst & Engn Div, Berkeley, CA USA; [Chang, Edward F.] UC San Francisco, Dept Neurol Surg, San Francisco, CA USA; [Bhattacharyya, Sharmodeep] Oregon State Univ, Dept Stat, Corvallis, OR 97331 USA	United States Department of Energy (DOE); Lawrence Berkeley National Laboratory; University of California System; University of California Berkeley; University of California System; University of California Berkeley; University of California System; University of California Berkeley; University of Minnesota System; University of Minnesota Twin Cities; United States Department of Energy (DOE); Lawrence Berkeley National Laboratory; United States Department of Energy (DOE); Lawrence Berkeley National Laboratory; University of California System; University of California San Francisco; Oregon State University	Bouchard, KE (corresponding author), LBNL, Biol Syst & Engn Div, Berkeley, CA 94720 USA.	kebouchard@lbl.gov; afbujan@gmail.com; farbod@icsi.berkeley.edu; ubaru001@umn.edu; prabhat@lbl.gov; AMSnijders@lbl.gov; jhmao@lbl.gov; Chang@ucsf.edu; mmahoney@icsi.berkeley.edu; bhattash@science.oregonstate.edu	Mao, Jianhua/GOK-2713-2022; Jeong, Yongwook/N-7413-2016					Bach F.R., 2008, P 25 INT C MACH LEAR, P33, DOI [10.1145/1390156.1390161, DOI 10.1145/1390156.1390161]; Bickel PJ, 2006, TEST-SPAIN, V15, P271, DOI 10.1007/BF02607055; Bouchard K. E., 2017, TECHNICAL REPORT; Bouchard K. E., 2015, TECHNICAL REPORT; Bouchard KE, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00092; Bouchard KE, 2014, J NEUROSCI, V34, P12662, DOI 10.1523/JNEUROSCI.1219-14.2014; Bouchard KE, 2013, NATURE, V495, P327, DOI 10.1038/nature11911; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Ganguli S, 2012, ANNU REV NEUROSCI, V35, P485, DOI 10.1146/annurev-neuro-062111-150410; HASTIE T, 2003, ELEMENTS STAT LEARNI; Javanmard A, 2014, J MACH LEARN RES, V15, P2869; Mao JH, 2015, SCI REP-UK, V5, DOI 10.1038/srep16247; Marx V, 2013, NATURE, V496, P253, DOI [10.1038/496253a, 10.1038/498255a]; National Research Council, 2013, FRONT MASS DAT AN; Schapire Robert E., 2012, BOOSTING FDN ALGORIT; Sejnowski TJ, 2014, NAT NEUROSCI, V17, P1440, DOI 10.1038/nn.3839; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wainwright MJ, 2014, ANNU REV STAT APPL, V1, P233, DOI 10.1146/annurev-statistics-022513-115643	19	3	3	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401012
C	Deshmukh, AA; Dogan, U; Scott, C		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Deshmukh, Aniket Anand; Dogan, Urun; Scott, Clayton			Multi-Task Learning for Contextual Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Contextual bandits are a form of multi-armed bandit in which the agent has access to predictive side information (known as the context) for each arm at each time step, and have been used to model personalized news recommendation, ad placement, and other applications. In this work, we propose a multi-task learning framework for contextual bandit problems. Like multi-task learning in the batch setting, the goal is to leverage similarities in contexts for different arms so as to improve the agent's ability to predict rewards from contexts. We propose an upper confidence bound-based multi-task learning algorithm for contextual bandits, establish a corresponding regret bound, and interpret this bound to quantify the advantages of learning in the presence of high task (arm) similarity. We also describe an effective scheme for estimating task similarity from data, and demonstrate our algorithm's performance on several data sets.	[Deshmukh, Aniket Anand; Scott, Clayton] Univ Michigan, Dept EECS, Ann Arbor, MI 48105 USA; [Dogan, Urun] Microsoft Res, Cambridge CB1 2FB, England	University of Michigan System; University of Michigan; Microsoft	Deshmukh, AA (corresponding author), Univ Michigan, Dept EECS, Ann Arbor, MI 48105 USA.	aniketde@umich.edu; urun.dogan@skype.net; clayscot@umich.edu	Jeong, Yongwook/N-7413-2016					Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Blanchard Gilles, 2011, NIPS, V24, P3; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cesa-Bianchi Nicole, 2013, ADV NEURAL INFORM PR, P737; CHRISTMANN A, 2010, ADV NEURAL INFORM PR, P406; Chu W., CONTEXTUAL BANDITS L; Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109; Kale S., 2010, ADV NEURAL INFORM PR, V23, P1054; Kuleshov V., 2014, ARXIV14026028; Langford J., 2008, ADV NEURAL INFORM PR, P817; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Li S, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P539, DOI 10.1145/2911451.2911548; Ong C. S., 2011, ADV NEURAL INFORM PR, P2447; Robbins Herbert, 1985, H ROBBINS SELECTED P, P169; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Steinwart I., 2008, SUPPORT VECTOR MACHI; Valko M., 2013, P 20 9 C UNCERTAINTY, P654; Villar SS, 2015, STAT SCI, V30, P199, DOI 10.1214/14-STS504; White J., 2012, BANDIT ALGORITHMS WE	20	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404089
C	Erdogdu, MA; Deshpande, Y; Montanari, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Erdogdu, Murat A.; Deshpande, Yash; Montanari, Andrea			Inference in Graphical Models via Semidefinite Programming Hierarchies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				RELAXATIONS; PROPAGATION; ALGORITHM; RANK; CUT	Maximum A posteriori Probability (MAP) inference in graphical models amounts to solving a graph-structured combinatorial optimization problem. Popular inference algorithms such as belief propagation (BP) and generalized belief propagation (GBP) are intimately related to linear programming (LP) relaxation within the Sherali-Adams hierarchy. Despite the popularity of these algorithms, it is well understood that the Sum-of-Squares (SOS) hierarchy based on semidefinite programming (SDP) can provide superior guarantees. Unfortunately, SOS relaxations for a graph with n vertices require solving an SDP with n(Theta(d)) variables where d is the degree in the hierarchy. In practice, for d >= 4, this approach does not scale beyond a few tens of variables. In this paper, we propose binary SDP relaxations for MAP inference using the SOS hierarchy with two innovations focused on computational efficiency. Firstly, in analogy to BP and its variants, we only introduce decision variables corresponding to contiguous regions in the graphical model. Secondly, we solve the resulting SDP using a non-convex Burer-Monteiro style method, and develop a sequential rounding procedure. We demonstrate that the resulting algorithm can solve problems with tens of thousands of variables within minutes, and outperforms BP and GBP on practical problems such as image denoising and Ising spin glasses. Finally, for specific graph types, we establish a sufficient condition for the tightness of the proposed partial SOS relaxation.	[Erdogdu, Murat A.; Deshpande, Yash] Microsoft Res, Cambridge, MA 02142 USA; [Deshpande, Yash] MIT, Cambridge, MA 02139 USA; [Montanari, Andrea] Stanford Univ, Stanford, CA 94305 USA	Microsoft; Massachusetts Institute of Technology (MIT); Stanford University	Erdogdu, MA (corresponding author), Microsoft Res, Cambridge, MA 02142 USA.	erdogdu@cs.toronto.edu; yash@mit.edu; montanari@stanford.edu	Jeong, Yongwook/N-7413-2016					BARAHONA F, 1986, MATH PROGRAM, V36, P157, DOI 10.1007/BF02592023; BARAHONA F, 1982, J PHYS A-MATH GEN, V15, P3241, DOI 10.1088/0305-4470/15/10/028; Barak B., 2016, COURSE NOTES; BOUMAL N, 2016, ADV NEURAL INFORM PR, P2757, DOI DOI 10.5555/3157382.3157407; Cowell R.G., 2006, PROBABILISTIC NETWOR; EDWARDS SF, 1975, J PHYS F MET PHYS, V5, P965, DOI 10.1088/0305-4608/5/5/017; Erdogdu M. A., 2015, ADV NEURAL INFORM PR, V2, P3052; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Koller D., 2009, PROBABILISTIC GRAPHI; Lasserre J. B., 2001, Integer Programming and Combinatorial Optimization. 8th International IPCO Conference. Proceedings (Lecture Notes in Computer Science Vol.2081), P293; Mei S., 2017, C LEARN THEOR, P1476; Mezard M., 2009, INFORM PHYS COMPUTAT, pp 584, DOI [10.1093/acprof:oso/9780198570837.001.0001, DOI 10.1093/ACPROF:OSO/9780198570837.001, DOI 10.1093/ACPROF:OSO/9780198570837.001.0001]; MORE JJ, 1983, SIAM J SCI STAT COMP, V4, P553, DOI 10.1137/0904038; Parrilo PA, 2003, MATH PROGRAM, V96, P293, DOI 10.1007/s10107-003-0387-5; Pataki G, 1998, MATH OPER RES, V23, P339, DOI 10.1287/moor.23.2.339; PEARL J, 1986, ARTIF INTELL, V29, P241, DOI 10.1016/0004-3702(86)90072-X; Richardson T., 2008, MODERN CODING THEORY; SHERALI HD, 1990, SIAM J DISCRETE MATH, V3, P411, DOI 10.1137/0403036; SHOR NZ, 1987, CYBERNETICS+, V23, P731, DOI 10.1007/BF01070233; Sun J, 2002, LECT NOTES COMPUT SC, V2351, P510; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wainwright MJ, 2004, ADV NEUR IN, V16, P369; Weiss Y, 2001, IEEE T INFORM THEORY, V47, P736, DOI 10.1109/18.910585; Weller A, 2016, JMLR WORKSH CONF PRO, V51, P47; Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085; Yedidia JS, 2000, ADV NEURAL INFORM PR, V13, P689	27	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400040
C	Flajolet, A; Jaillet, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Flajolet, Arthur; Jaillet, Patrick			Real-Time Bidding with Side Information	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We consider the problem of repeated bidding in online advertising auctions when some side information (e.g. browser cookies) is available ahead of submitting a bid in the form of a d-dimensional vector. The goal for the advertiser is to maximize the total utility (e.g. the total number of clicks) derived from displaying ads given that a limited budget B is allocated for a given time horizon T. Optimizing the bids is modeled as a contextual Multi-Armed Bandit (MAB) problem with a knapsack constraint and a continuum of arms. We develop UCB-type algorithms that combine two streams of literature: the confidence-set approach to linear contextual MABs and the probabilistic bisection search method for stochastic root-finding. Under mild assumptions on the underlying unknown distribution, we establish distribution-independent regret bounds of order (O) over tilde (d . root T) when either B = infinity or when B scales linearly with T.	[Flajolet, Arthur] MIT, ORC, Cambridge, MA 02139 USA; [Jaillet, Patrick] MIT, EECS, LIDS, ORC, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Flajolet, A (corresponding author), MIT, ORC, Cambridge, MA 02139 USA.	flajolet@mit.edu; jaillet@mit.edu	Jeong, Yongwook/N-7413-2016		Office of Naval Research (ONR) [N00014-15-1-2083]	Office of Naval Research (ONR)(Office of Naval Research)	Research funded in part by the Office of Naval Research (ONR) grant N00014-15-1-2083.	Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Agrawal S., 2016, ADV NEURAL INFORM PR, V29, P3450; Agrawal S, 2016, P 29 ANN C LEARN THE, P4; Amin K, 2012, P 28 C UNC ART INT; Amin K., 2014, ADV NEURAL INFORM PR, P622; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Babaioff Moshe, 2012, P 13 ACM C EL COMM, P74; Badanidiyuru A, 2014, P C LEARN THEOR, V35, P1109; Badanidiyuru A, 2013, ANN IEEE SYMP FOUND, P207, DOI 10.1109/FOCS.2013.30; Balseiro SR, 2017, EC'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P609, DOI 10.1145/3033274.3084088; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bastani H, 2015, WORKING PAPER; Boucheron S., 2005, ESAIM-PROBAB STAT, V9, P323, DOI [DOI 10.1051/PS:2005018, 2182250]; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Cohen MC, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P817, DOI 10.1145/2940716.2940728; Dani V, 2008, P C LEARN THEOR COLT, P355; Ghosh A., 2009, WWW 09 P 18 INT C WO, P251; Lei Y, 2015, WORKING PAPER; Long TT, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P809; Lueker GS, 1998, J ALGORITHM, V29, P277, DOI 10.1006/jagm.1998.0954; Pasupathy R, 2011, ACM T MODEL COMPUT S, V21, DOI 10.1145/1921598.1921603; Wang ZZ, 2014, OPER RES, V62, P318, DOI 10.1287/opre.2013.1245; Weed J., 2016, PROC 29 ANN C LEARN, P1562	25	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405024
C	Gallagher, NM; Ulrich, K; Talbot, A; Dzirasa, K; Carin, L; Carlson, DE		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Gallagher, Neil M.; Ulrich, Kyle; Talbot, Austin; Dzirasa, Kafui; Carin, Lawrence; Carlson, David E.			Cross-Spectral Factor Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In neuropsychiatric disorders such as schizophrenia or depression, there is often a disruption in the way that regions of the brain synchronize with one another. To facilitate understanding of network-level synchronization between brain regions, we introduce a novel model of multisite low-frequency neural recordings, such as local field potentials (LFPs) and electroencephalograms (EEGs). The proposed model, named Cross-Spectral Factor Analysis (CSFA), breaks the observed signal into factors defined by unique spatio-spectral properties. These properties are granted to the factors via a Gaussian process formulation in a multiple kernel learning framework. In this way, the LFP signals can be mapped to a lower dimensional space in a way that retains information of relevance to neuroscientists. Critically, the factors are interpretable. The proposed approach empirically allows similar performance in classifying mouse genotype and behavioral context when compared to commonly used approaches that lack the interpretability of CSFA. We also introduce a semi-supervised approach, termed discriminative CSFA (dCSFA). CSFA and dCSFA provide useful tools for understanding neural dynamics, particularly by aiding in the design of causal follow-up experiments.	[Gallagher, Neil M.; Dzirasa, Kafui] Duke Univ, Dept Neurobiol, Durham, NC 27706 USA; [Ulrich, Kyle; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA; [Talbot, Austin] Duke Univ, Dept Stat Sci, Durham, NC 27706 USA; [Dzirasa, Kafui] Duke Univ, Dept Psychiat & Behav Sci, Durham, NC 27706 USA; [Carlson, David E.] Duke Univ, Dept Civil & Environm Engn, Durham, NC 27706 USA; [Carlson, David E.] Duke Univ, Dept Biostat & Bioinformat, Durham, NC 27706 USA	Duke University; Duke University; Duke University; Duke University; Duke University; Duke University	Gallagher, NM (corresponding author), Duke Univ, Dept Neurobiol, Durham, NC 27706 USA.	neil.gallagher@duke.edu; austin.talbot@duke.edu; kafui.dzirasa@duke.edu; lcarin@duke.edu; david.carlson@duke.edu	Dzirasa, Kafui/GQB-1424-2022; Jeong, Yongwook/N-7413-2016		DARPA HIST program; National Institutes of Health [R01MH099192-05S2]; W.M. Keck Foundation	DARPA HIST program; National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); W.M. Keck Foundation(W.M. Keck Foundation)	In working on this project L.C. received funding from the DARPA HIST program; K.D., L.C., and D.C. received funding from the National Institutes of Health by grant R01MH099192-05S2; K.D received funding from the W.M. Keck Foundation.	Abelaira H. M., 2013, REV BRASILEIRA PSIQU; Akil H., 2010, SCIENCE; Alvarez M., 2012, FDN TRENDS MACHINE L; Banerjee S, 2015, HIERARCHICAL MODELIN; Bastos AM, 2016, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00175; Beal M.J, 2003, THESIS; Bishop C. M., 2006, MACHINE LEARNING; Blei D. M., 2003, J MACHINE LEARNING R; Buzsaki G., 2012, NATURE REV NEUROSCIE; Carlson D., 2017, BIOL PSYCHIAT; Caruana R., 1997, MACHINE LEARNING; Chen B., 2013, IEEE T PATTERN ANAL; Cho Y., 2009, NIPS; Cunningham J.P., 2014, NATURE NEUROSCIENCE; Deisseroth K., 2011, NATURE METHODS; Eaton W. W., 2008, EPIDEMIOLOGIC REV; Gonen Mehmet, 2011, J MACHINE LEARNING R; Harris A. Z., 2015, ANN REV NEUROSCIENCE; Harris K. D., 2011, NATURE REV NEUROSCIE; Hultman R., 2016, NEURON; Iacoviello D., 2015, COMPUTER METHODS PRO; Kingma D.P, P 3 INT C LEARNING R; Kwak C., 2002, NURSING RES; Lisman J. E., 2013, NEURON; Mairal J., 2014, ADV NEURAL INFORM PR; Miesenbock G., 2004, CURRENT OPINION NEUR; Moran M. D., 2003, OIKOS; Nestler E. J., 2010, NATURE NEUROSCIENCE; Oppenheim A.V., 1999, DISCRETE TIME SIGNAL; Raina Rajat, 2004, ADV NEURAL INFORM PR, V1, P6; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Teh Y. W., 2005, AISTATS; Uhlhaas P. J., 2008, SCHIZOPHR B; Ulrich K. R., 2015, ADV NEURAL INFORM PR; van Enkhuizen J., 2013, BEHAV BRAIN RES; Wang HE, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00405; Welch P. D., 1967, IEEE T AUDIO ELECTRO, VAU-15; Wilson A.G., 2014, ADV NEURAL INFORM PR; Wilson AG, 2013, P 30 INT C MACH LEAR; Zhou M., 2009, ADV NEURAL INFORM PR	40	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406087
C	Geist, M; Piot, B; Pietquin, O		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Geist, Matthieu; Piot, Bilal; Pietquin, Olivier			Is the Bellman residual a bad proxy?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper aims at theoretically and empirically comparing two standard optimization criteria for Reinforcement Learning: i) maximization of the mean value and ii) minimization of the Bellman residual. For that purpose, we place ourselves in the framework of policy search algorithms, that are usually designed to maximize the mean value, and derive a method that minimizes the residual parallel to T(*)v(pi) - v(pi)parallel to(1,v) over policies. A theoretical analysis shows how good this proxy is to policy optimization, and notably that it is better than its value-based counterpart. We also propose experiments on randomly generated generic Markov decision processes, specifically designed for studying the influence of the involved concentrability coefficient. They show that the Bellman residual is generally a bad proxy to policy optimization and that directly maximizing the mean value is much better, despite the current lack of deep theoretical analysis. This might seem obvious, as directly addressing the problem of interest is usually better, but given the prevalence of (projected) Bellman residual minimization in value-based reinforcement learning, we believe that this question is worth to be considered.	[Geist, Matthieu] Univ Lorraine, F-57070 Metz, France; [Geist, Matthieu] CNRS, LIEC, UMR 7360, F-57070 Metz, France; [Piot, Bilal; Pietquin, Olivier] Univ Lille, Inria, Centrale Lille, CNRS,UMR 9189 CRIStAL, F-59000 Lille, France; [Piot, Bilal; Pietquin, Olivier] Google DeepMind, London, England	Universite de Lorraine; Centre National de la Recherche Scientifique (CNRS); CNRS - National Institute for Earth Sciences & Astronomy (INSU); Universite de Lorraine; Centre National de la Recherche Scientifique (CNRS); Inria; Universite de Lille - ISITE; Centrale Lille; Universite de Lille; Google Incorporated	Geist, M (corresponding author), Univ Lorraine, F-57070 Metz, France.; Geist, M (corresponding author), CNRS, LIEC, UMR 7360, F-57070 Metz, France.	matthieu.geist@univ-lorraine.fr; bilal.piot@univ-lille1.fr; olivier.pietquin@univ-lille1.fr	Jeong, Yongwook/N-7413-2016					Antos A, 2008, MACH LEARN, V71, P89, DOI 10.1007/s10994-007-5038-2; ARCHIBALD TW, 1995, J OPER RES SOC, V46, P354, DOI 10.2307/2584329; BAIRD LC, 1995, ICML, P00030; De Farias DP, 2003, OPER RES, V51, P850, DOI 10.1287/opre.51.6.850.24925; Desai VV, 2012, OPER RES, V60, P655, DOI 10.1287/opre.1120.1044; Ernst D, 2005, J MACH LEARN RES, V6, P503; Filar JA, 1991, STOCHASTIC GAMES REL, P59; Gordon Geoffrey, 1995, INT C MACH LEARN ICM; Kakade S., 2002, ICML; Lagoudakis M. G., 2003, J MACHINE LEARNING, P1107, DOI DOI 10.1162/JMLR.2003.4.6.1107; Lazaric Alessandro, 2010, INT C MACH LEARN ICM; Lillicrap Timothy P, 2016, 4 INT C LEARN REPR S, P1; Maei Hamid R, 2010, INT C MACH LEARN ICM; Munos R, 2007, SIAM J CONTROL OPTIM, V46, P541, DOI 10.1137/040614384; Perolat Julien, 2016, INT C MACH LEARN ICM; Piot B., 2014, ADV NEURAL INFORM PR; Scherrer B, 2014, PR MACH LEARN RES, V32, P1314; Scherrer Bruno, 2010, INT C MACH LEARN ICM; Scherrer Bruno, 2014, EUR C MACH LEARN PRI; Schulman J., 2015, INT C MACH LEARN ICM	23	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403027
C	Ghassami, A; Salehkaleybar, S; Kiyavash, N; Zhang, K		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ghassami, AmirEmad; Salehkaleybar, Saber; Kiyavash, Negar; Zhang, Kun			Learning Causal Structures Using Regression Invariance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NETWORK INFERENCE; MODELS	We study causal discovery in a multi-environment setting, in which the functional relations for producing the variables from their direct causes remain the same across environments, while the distribution of exogenous noises may vary. We introduce the idea of using the invariance of the functional relations of the variables to their causes across a set of environments for structure learning. We define a notion of completeness for a causal inference algorithm in this setting and prove the existence of such algorithm by proposing the baseline algorithm. Additionally, we present an alternate algorithm that has significantly improved computational and sample complexity compared to the baseline algorithm. Experiment results show that the proposed algorithm outperforms the other existing algorithms.	[Ghassami, AmirEmad; Kiyavash, Negar] Univ Illinois, Dept ECE, Urbana, IL 61801 USA; [Ghassami, AmirEmad; Salehkaleybar, Saber; Kiyavash, Negar] Univ Illinois, Coordinated Sci Lab, Urbana, IL 61801 USA; [Zhang, Kun] Carnegie Mellon Univ, Dept Philosophy, Pittsburgh, PA 15213 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign; Carnegie Mellon University	Ghassami, A (corresponding author), Univ Illinois, Dept ECE, Urbana, IL 61801 USA.; Ghassami, A (corresponding author), Univ Illinois, Coordinated Sci Lab, Urbana, IL 61801 USA.	ghassam2@illinois.edu; sabersk@illinois.edu; kiyavashl@illinois.edu; kunzl@cmu.edu	Jeong, Yongwook/N-7413-2016; Salehkaleybar, Saber/AAQ-3268-2021	Salehkaleybar, Saber/0000-0003-3934-9931	ARO [W911NF-15-1-0281]; ONR [W911NF-15-1-0479];  [NIH-1R01EB022858-01 FAIN-R01EB022858];  [NIH-1R01LM012087];  [NIH-5U54HG008540-02 FAINU54HG008540]	ARO; ONR(Office of Naval Research); ; ; 	This work was supported in part by ARO grant W911NF-15-1-0281 and ONR grant W911NF-15-1-0479. Also, KZ acknowledges the support from NIH-1R01EB022858-01 FAIN-R01EB022858, NIH-1R01LM012087, and NIH-5U54HG008540-02 FAINU54HG008540. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH.	Bollen K. A., 1989, WILEY SERIES PROBABI; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; Daniusis P., 2010, P 26 C UNC ART INT U; Eberhardt F, 2007, THESIS CARNEGIE MELL, P93; Eberhardt F., 2005, P 21 C UNC ART INT, P178; Etesami J., 2016, NEURAL COMPUTATION; Etesami J, 2014, P AMER CONTR CONF, P2563, DOI 10.1109/ACC.2014.6859362; Ghassami A., 2017, ARXIV170208567; Ghassami A., 2017, ARXIV170903625; Ghassami A, 2017, IEEE INT SYMP INFO, P1326, DOI 10.1109/ISIT.2017.8006744; Hauser A, 2014, INT J APPROX REASON, V55, P926, DOI 10.1016/j.ijar.2013.11.007; Hoyer P. O., 2009, ADV NEURAL INFORM PR, V21, P689; Janzing D, 2012, ARTIF INTELL, V182, P1, DOI 10.1016/j.artint.2012.01.002; Janzing D, 2010, IEEE T INFORM THEORY, V56, P5168, DOI 10.1109/TIT.2010.2060095; Kalisch M, 2012, J STAT SOFTW, V47, P1; Kim S, 2014, P IEEE, V102, P683, DOI 10.1109/JPROC.2014.2307888; Koller D., 2009, PROBABILISTIC GRAPHI; Lutkepohl H., 2005, NEW INTRO MULTIPLE T, DOI DOI 10.1007/078-3-540-27752-1; Marbach D, 2009, J COMPUT BIOL, V16, P229, DOI 10.1089/cmb.2008.09TT; Meek, 1997, GRAPHICAL MODELS SEL; Pearl J, 2009, CAUSALITY, DOI 10.1017/CBO9780511803161; Peters J, 2014, BIOMETRIKA, V101, P219, DOI 10.1093/biomet/ast043; Peters J, 2016, J R STAT SOC B, V78, P947, DOI 10.1111/rssb.12167; Peters J, 2014, J MACH LEARN RES, V15, P2009; Quinn CJ, 2015, IEEE T INFORM THEORY, V61, P6887, DOI 10.1109/TIT.2015.2478440; Quinn CJ, 2013, IEEE T SIGNAL PROCES, V61, P3173, DOI 10.1109/TSP.2013.2259161; ROUSE CE, 1995, J BUS ECON STAT, V13, P217, DOI 10.2307/1392376; Schaffter T, 2011, BIOINFORMATICS, V27, P2263, DOI 10.1093/bioinformatics/btr373; Scholkopf Bernhard, 2012, ICML; Sgouritsa E., 2015, AISTATS; Shanmugam K., 2015, ADV NEURAL INFORM PR, P3195; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Spirtes P., 2000, CAUSATION PREDICTION; Sun J, 2015, SIAM J APPL DYN SYST, V14, P73, DOI 10.1137/140956166; Tian J., 2001, P 17 C UNC ART INT, P512; Verma T., 1992, P 8 C UNC ART INT, P323; Zhang K., 2009, P 25 C UNC ART INT U; Zhang K., 2017, P INT JOINT C ART IN	38	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403008
C	Ghoshal, A; Honorio, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ghoshal, Asish; Honorio, Jean			Learning Identifiable Gaussian Bayesian Networks in Polynomial Time and Sample Complexity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				COVARIANCE ESTIMATION; MAXIMUM-LIKELIHOOD; MODEL; SELECTION	Learning the directed acyclic graph (DAG) structure of a Bayesian network from observational data is a notoriously difficult problem for which many non-identifiability and hardness results are known. In this paper we propose a provably polynomial time algorithm for learning sparse Gaussian Bayesian networks with equal noise variance - a class of Bayesian networks for which the DAG structure can be uniquely identified from observational data - under high-dimensional settings. We show that O(k(4) log p) number of samples suffices for our method to recover the true DAG structure with high probability, where p is the number of variables and k is the maximum Markov blanket size. We obtain our theoretical guarantees under a condition called restricted strong adjacency faithfulness (RSAF), which is strictly weaker than strong faithfulness - a condition that other methods based on conditional independence testing need for their success. The sample complexity of our method matches the information-theoretic limits in terms of the dependence on p. We validate our theoretical findings through synthetic experiments.	[Ghoshal, Asish; Honorio, Jean] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47906 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus	Ghoshal, A (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47906 USA.	aghoshal@purdue.edu; jhonorio@purdue.edu	Jeong, Yongwook/N-7413-2016					[Anonymous], 2013, NIPS; [Anonymous], 2015, ADV NEURAL INFORM PR; Aragam B, 2015, J MACH LEARN RES, V16, P2273; Banerjee O, 2008, J MACH LEARN RES, V9, P485; Bishop C.M, 2006, PATTERN RECOGN; Cai T, 2011, J AM STAT ASSOC, V106, P594, DOI 10.1198/jasa.2011.tm10155; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; Dasgupta S, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P134; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Ghoshal A, 2017, PR MACH LEARN RES, V54, P767; Hsieh C.-J., 2012, ADV NEURAL INFORM PR, V25, P2330; Hsu Daniel, 2011, P COLT CIT; Jaakkola T, 2010, P 13 INT C ART INT S, P358; Johnson C., 2012, P 15 INT C ART INT S, P574; Kalisch M, 2007, J MACH LEARN RES, V8, P613; Lu Y, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0001149; Mazumder R, 2012, J MACH LEARN RES, V13, P781; Meek C., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P411; Meek C., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P403; Peters J, 2014, BIOMETRIKA, V101, P219, DOI 10.1093/biomet/ast043; Peters J, 2014, J MACH LEARN RES, V15, P2009; Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631; Robinson RW., 1977, LECT NOTES MATH, P28, DOI DOI 10.1007/BFB0069178; Rolfs B., 2012, ADV NEURAL INFORM PR, P1574; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Shubbar E., 2013, BIOMEDCENTRAL CANC, V13; Spirtes P., 2000, CAUSATION PREDICTION; Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7; Uhler C, 2013, ANN STAT, V41, P436, DOI 10.1214/12-AOS1080; Van de Geer S, 2013, ANN STAT, V41, P536, DOI 10.1214/13-AOS1085; Vershynin Roman, 2010, ARXIV10113027CSMATH; Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018; Zhang J., 2002, P 19 C UNCERTAINTY, P632; Zhang J, 2008, MIND MACH, V18, P239, DOI 10.1007/s11023-008-9096-4	35	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406051
C	Gurbuzbalaban, M; Ozdaglar, A; Parrilo, PA; Vanli, ND		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Gurbuzbalaban, Mert; Ozdaglar, Asuman; Parrilo, Pablo A.; Vanli, N. Denizcan			When Cyclic Coordinate Descent Outperforms Randomized Coordinate Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CONVERGENCE; REGULARIZATION; CONVEXITY	The coordinate descent (CD) method is a classical optimization algorithm that has seen a revival of interest because of its competitive performance in machine learning applications. A number of recent papers provided convergence rate estimates for their deterministic (cyclic) and randomized variants that differ in the selection of update coordinates. These estimates suggest randomized coordinate descent (RCD) performs better than cyclic coordinate descent (CCD), although numerical experiments do not provide clear justification for this comparison. In this paper, we provide examples and more generally problem classes for which CCD (or CD with any deterministic order) is faster than RCD in terms of asymptotic worst-case convergence. Furthermore, we provide lower and upper bounds on the amount of improvement on the rate of CCD relative to RCD, which depends on the deterministic order used. We also provide a characterization of the best deterministic order (that leads to the maximum improvement in convergence rate) in terms of the combinatorial properties of the Hessian matrix of the objective function.	[Gurbuzbalaban, Mert] Rutgers State Univ, Piscataway, NJ 08854 USA; [Ozdaglar, Asuman; Parrilo, Pablo A.; Vanli, N. Denizcan] MIT, Cambridge, MA 02139 USA	Rutgers State University New Brunswick; Massachusetts Institute of Technology (MIT)	Gurbuzbalaban, M (corresponding author), Rutgers State Univ, Piscataway, NJ 08854 USA.	mg1366@rutgers.edu; asuman@mit.edu; parrilo@mit.edu; denizcan@mit.edu			NSF [DMS-1723085]; DARPA Foundations of Scalable Statistical Learning grants	NSF(National Science Foundation (NSF)); DARPA Foundations of Scalable Statistical Learning grants	This work is supported by NSF DMS-1723085 and DARPA Foundations of Scalable Statistical Learning grants.	[Anonymous], 2009, MATRIX ITERATIVE ANA; Beck A, 2013, SIAM J OPTIMIZ, V23, P2037, DOI 10.1137/120887679; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Bertsekas D., 2015, CONVEX OPTIMIZATION; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Bertsekas D. P., 1997, PARALLEL DISTRIBUTED; Chung F., 1997, AM MATH SOC, DOI 10.1090/cbms/092; Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; KINGMAN JF, 1961, Q J MATH, V12, P283, DOI 10.1093/qmath/12.1.283; Kirkland S.J, 2012, GROUP INVERSES M MAT; Lu ZS, 2015, MATH PROGRAM, V152, P615, DOI 10.1007/s10107-014-0800-2; LUO ZQ, 1992, J OPTIMIZ THEORY APP, V72, P7, DOI 10.1007/BF00939948; Malioutov DM, 2006, J MACH LEARN RES, V7, P2031; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; NUSSBAUM RD, 1986, LINEAR ALGEBRA APPL, V73, P59; Nutini J, 2015, PR MACH LEARN RES, V37, P1632; Ortega JM., 2000, ITERATIVE SOLUTION N, DOI [10.1137/1.9780898719468, DOI 10.1137/1.9780898719468]; PLEMMONS RJ, 1977, LINEAR ALGEBRA APPL, V18, P175, DOI 10.1016/0024-3795(77)90073-8; Rantzer A., 2014, ARXIV12030047; Richtarik P, 2016, MATH PROGRAM, V156, P433, DOI 10.1007/s10107-015-0901-6; Saha A, 2013, SIAM J OPTIMIZ, V23, P576, DOI 10.1137/110840054; Sun R., 2016, ARXIV160407130; Sun R, 2015, ADV NEURAL INFORM PR, P1306; Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3; YOUNG DM, 1971, ITERATIVE SOLUTION L; Zhi-Quan Luo, 1993, Annals of Operations Research, V46-47, P157	27	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407009
C	Habeck, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Habeck, Michael			Model evidence from nonequilibrium simulations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				FREE-ENERGY DIFFERENCES	The marginal likelihood, or model evidence, is a key quantity in Bayesian parameter estimation and model comparison. For many probabilistic models, computation of the marginal likelihood is challenging, because it involves a sum or integral over an enormous parameter space. Markov chain Monte Carlo (MCMC) is a powerful approach to compute marginal likelihoods. Various MCMC algorithms and evidence estimators have been proposed in the literature. Here we discuss the use of nonequilibrium techniques for estimating the marginal likelihood. Nonequilibrium estimators build on recent developments in statistical physics and are known as annealed importance sampling (AIS) and reverse AIS in probabilistic machine learning. We introduce estimators for the model evidence that combine forward and backward simulations and show for various challenging models that the evidence estimators outperform forward and reverse AIS.	[Habeck, Michael] Univ Gottingen, Stat Inverse Problems Biophys, Max Planck Inst Biophys Chem, D-37077 Gottingen, Germany; [Habeck, Michael] Univ Gottingen, Inst Math Stochast, D-37077 Gottingen, Germany	Max Planck Society; University of Gottingen; University of Gottingen	Habeck, M (corresponding author), Univ Gottingen, Stat Inverse Problems Biophys, Max Planck Inst Biophys Chem, D-37077 Gottingen, Germany.; Habeck, M (corresponding author), Univ Gottingen, Inst Math Stochast, D-37077 Gottingen, Germany.	mhabeck@gwdg.de	Jeong, Yongwook/N-7413-2016		Deutsche Forschungsgemeinschaft (DFG) [SFB 860]	Deutsche Forschungsgemeinschaft (DFG)(German Research Foundation (DFG))	This work has been funded by the Deutsche Forschungsgemeinschaft (DFG) SFB 860, subproject B09.	Beale PD, 1996, PHYS REV LETT, V76, P78, DOI 10.1103/PhysRevLett.76.78; BENNETT CH, 1976, J COMPUT PHYS, V22, P245, DOI 10.1016/0021-9991(76)90078-4; Burda Y, 2015, JMLR WORKSH CONF PRO, V38, P102; Crooks GE, 1999, PHYS REV E, V60, P2721, DOI 10.1103/PhysRevE.60.2721; Crooks GE, 1998, J STAT PHYS, V90, P1481, DOI 10.1023/A:1023208217925; Crooks GE, 1999, THESIS; Geyer C., 1991, COMP SCI STAT, P156, DOI DOI 10.1080/01621459.1995.10476590; Grosse R. B., 2015, ARXIV151102543; Grosse R. B., 2016, ADV NEURAL INFORM PR, V29, P2451; Habeck M., 2012, P 15 INT C ART INT S, V22, P486; Habeck M, 2012, PHYS REV LETT, V109, DOI 10.1103/PhysRevLett.109.100601; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hummer G, 2001, J CHEM PHYS, V114, P7330, DOI 10.1063/1.1363668; Jarzynski C, 1997, PHYS REV LETT, V78, P2690, DOI 10.1103/PhysRevLett.78.2690; Jaynes E.T., 2003, PROBABILITY THEORY L; KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572; Knuth KH, 2015, DIGIT SIGNAL PROCESS, V47, P50, DOI 10.1016/j.dsp.2015.06.012; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu JS., 2001, MONTE CARLO STRATEGI, DOI DOI 10.1007/978-0-387-76371-2; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Shirts MR, 2003, PHYS REV LETT, V91, DOI 10.1103/PhysRevLett.91.140601; Skilling J, 2006, BAYESIAN ANAL, V1, P833, DOI 10.1214/06-BA127; SWENDSEN RH, 1986, PHYS REV LETT, V57, P2607, DOI 10.1103/PhysRevLett.57.2607	25	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401076
C	He, D; Lu, HQ; Xia, YC; Qin, T; Wang, LW; Liu, TY		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		He, Di; Lu, Hanqing; Xia, Yingce; Qin, Tao; Wang, Liwei; Liu, Tie-Yan			Decoding with Value Networks for Neural Machine Translation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Neural Machine Translation (NMT) has become a popular technology in recent years, and beam search is its de facto decoding method due to the shrunk search space and reduced computational complexity. However, since it only searches for local optima at each time step through one-step forward looking, it usually cannot output the best target sentence. Inspired by the success and methodology of AlphaGo, in this paper we propose using a prediction network to improve beam search, which takes the source sentence x, the currently available decoding output y(1), ...,y(t-1) and a candidate word w at step t as inputs and predicts the long-term value (e.g., BLEU score) of the partial target sentence if it is completed by the NMT model. Following the practice in reinforcement learning, we call this prediction network value network. Specifically, we propose a recurrent structure for the value network, and train its parameters from bilingual data. During the test time, when choosing a word w for decoding, we consider both its conditional probability given by the NMT model and its long-term value predicted by the value network. Experiments show that such an approach can significantly improve the translation accuracy on several translation tasks.	[He, Di; Wang, Liwei] Peking Univ, Sch EECS, MOE, Key Lab Machine Percept, Beijing, Peoples R China; [Lu, Hanqing] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Xia, Yingce] Univ Sci & Technol China, Hefei, Anhui, Peoples R China; [Qin, Tao; Liu, Tie-Yan] Microsoft Res, Redmond, WA USA; [Wang, Liwei] Peking Univ, Beijing Inst Big Data Res, Ctr Data Sci, Beijing, Peoples R China	Peking University; Carnegie Mellon University; Chinese Academy of Sciences; University of Science & Technology of China, CAS; Microsoft; Peking University	He, D (corresponding author), Peking Univ, Sch EECS, MOE, Key Lab Machine Percept, Beijing, Peoples R China.	di_he@pku.edu.cn; hanqinglu@cmu.edu; xiayingc@mail.ustc.edu.cn; taoqin@microsoft.com; wanglw@cis.pku.edu.cn; tie-yan.liu@microsoft.com	Jeong, Yongwook/N-7413-2016		National Basic Research Program of China (973 Program) [2015CB352502]; NSFC [61573026]	National Basic Research Program of China (973 Program)(National Basic Research Program of China); NSFC(National Natural Science Foundation of China (NSFC))	This work was partially supported by National Basic Research Program of China (973 Program) (grant no. 2015CB352502), NSFC (61573026). We would like to thank the anonymous reviewers for their valuable comments on our paper.	Bahdanau Dzmitry, 2017, ICLR; Bahdanau Dzmitry, 2015, ICLR; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Cho K., 2014, P 2014 C EMP METH NA, P1724; He Di, 2016, NEURAL INFORM PROCES, P2; Jean S, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1; Liu T. Y., 2017, P AS C MACH LEARN; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Ranzato M., 2016, ICLR; Shen S, 2016, ACL; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; TESAURO G, 1994, NEURAL COMPUT, V6, P215, DOI 10.1162/neco.1994.6.2.215; Tu Z., 2016, CORR; Tu ZP, 2017, AAAI CONF ARTIF INTE, P3097; Wiseman S, 2016, EMNLP; Wu Y., 2016, GOOGLES NEURAL MACHI; Xia Y., 2017, ICML; Xia Y., 2017, 31 ANN C NEUR INF PR; Zeiler M.D, 2012, CORR ABS12125701; Zhou J., 2016, ARXIV160604199	22	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400017
C	Hein, M; Andriushchenko, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hein, Matthias; Andriushchenko, Maksym			Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Recent work has shown that state-of-the-art classifiers are quite brittle, in the sense that a small adversarial change of an originally with high confidence correctly classified input leads to a wrong classification again with high confidence. This raises concerns that such classifiers are vulnerable to attacks and calls into question their usage in safety-critical systems. We show in this paper for the first time formal guarantees on the robustness of a classifier by giving instance-specific lower bounds on the norm of the input manipulation required to change the classifier decision. Based on this analysis we propose the Cross-Lipschitz regularization functional. We show that using this form of regularization in kernel methods resp. neural networks improves the robustness of the classifier with no or small loss in prediction performance.	[Hein, Matthias; Andriushchenko, Maksym] Saarland Univ, Dept Math & Comp Sci, Saarbrucken Informat Campus, Saarbrucken, Germany	Saarland University	Hein, M (corresponding author), Saarland Univ, Dept Math & Comp Sci, Saarbrucken Informat Campus, Saarbrucken, Germany.		Jeong, Yongwook/N-7413-2016					Abadi M., TENSORFLOW LARGE SCA; [Anonymous], NIPS; [Anonymous], 2002, LEARNING KERNELS; Bastani Osbert, 2016, NIPS; Carlini N., 2017, ACM WORKSH ART INT S; Cisse M., 2017, ICML; Dalvi N., 2004, KDD; Drucker H., 1992, IJCNN; Goodfellow I. J., 2015, ICLR; Gu Shixiang, 2015, ICLR WORKSH; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Helmbold DP, 2015, J MACH LEARN RES, V16, P3403; Huang  R., 2016, ICLR; Kos J., 2017, ICLR WORKSH; Kurakin Alexey, 2017, ICLR WORKSH; Liu Yanpei, 2017, ICLR; Lowd Daniel, 2005, KDD; Moosavi-Dezfooli S.-M., 2017, CVPR; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Shaham U., 2016, NIPS; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Stallkamp J, 2012, NEURAL NETWORKS, V32, P323, DOI 10.1016/j.neunet.2012.02.016; SZEGEDY C, 2014, ICLR, P2503; Zagoruyko S., BMVC; Zheng S., 2016, CVPR	26	3	3	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402031
C	Ide, JS; Cappabianco, FA; Faria, FA; Li, CSR		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ide, Jaime S.; Cappabianco, Fabio A.; Faria, Fabio A.; Li, Chiang-shan R.			Detrended Partial Cross Correlation for Brain Connectivity Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				FMRI; DYNAMICS	Brain connectivity analysis is a critical component of ongoing human connectome projects to decipher the healthy and diseased brain. Recent work has highlighted the power-law (multi-time scale) properties of brain signals; however, there remains a lack of methods to specifically quantify short- vs. long- time range brain connections. In this paper, using detrended partial cross-correlation analysis (DPCCA), we propose a novel functional connectivity measure to delineate brain interactions at multiple time scales, while controlling for covariates. We use a rich simulated fMRI dataset to validate the proposed method, and apply it to a real fMRI dataset in a cocaine dependence prediction task. We show that, compared to extant methods, the DPCCA-based approach not only distinguishes short and long memory functional connectivity but also improves feature extraction and enhances classification accuracy. Together, this paper contributes broadly to new computational methodologies in understanding neural information processing.	[Ide, Jaime S.; Li, Chiang-shan R.] Yale Univ, New Haven, CT 06519 USA; [Cappabianco, Fabio A.; Faria, Fabio A.] Univ Fed Sao Paulo, BR-12231 Sao Jose Dos Campos, Brazil	Yale University; Universidade Federal de Sao Paulo (UNIFESP)	Ide, JS (corresponding author), Dept Psychiat, 34 Pk St S110, New Haven, CT 06519 USA.	jaime.ide@yale.edu; cappabianco@unifesp.br; ffaria@unifesp.br	Jeong, Yongwook/N-7413-2016		FAPESP [2016/21591-5]; CNPq [408919/2016-7]; NSF [BCS1309260]; NIH [AA021449, DA023248]	FAPESP(Fundacao de Amparo a Pesquisa do Estado de Sao Paulo (FAPESP)); CNPq(Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPQ)); NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	Supported by FAPESP (2016/21591-5), CNPq (408919/2016-7), NSF (BCS1309260) and NIH (AA021449, DA023248).	Bassett DS, 2009, CURR OPIN NEUROL, V22, P340, DOI 10.1097/WCO.0b013e32832d93dd; Chang C, 2010, NEUROIMAGE, V50, P81, DOI 10.1016/j.neuroimage.2009.12.011; Ciuciu P, 2014, NEUROIMAGE, V95, P248, DOI 10.1016/j.neuroimage.2014.03.047; Correa NM, 2009, INT CONF ACOUST SPEE, P385, DOI 10.1109/ICASSP.2009.4959601; Friman O, 2001, MAGNET RESON MED, V45, P323, DOI 10.1002/1522-2594(200102)45:2<323::AID-MRM1041>3.0.CO;2-#; Friston KJ, 2003, NEUROIMAGE, V19, P1273, DOI 10.1016/S1053-8119(03)00202-7; Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814; He BYJ, 2014, TRENDS COGN SCI, V18, P480, DOI 10.1016/j.tics.2014.04.003; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hu S, 2015, NEUROIMAGE, V119, P286, DOI 10.1016/j.neuroimage.2015.06.032; Ide JS, 2016, NEUROIMAGE-CLIN, V11, P349, DOI 10.1016/j.nicl.2016.03.004; Kantelhardt JW, 2002, PHYSICA A, V316, P87, DOI 10.1016/S0378-4371(02)01383-3; Kristoufek L, 2014, PHYSICA A, V402, P291, DOI 10.1016/j.physa.2014.01.058; LOGAN GD, 1984, J EXP PSYCHOL HUMAN, V10, P276, DOI 10.1037/0096-1523.10.2.276; Matthews PM, 2016, NEURON, V91, P511, DOI 10.1016/j.neuron.2016.07.031; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Podobnik B, 2008, PHYS REV LETT, V100, DOI 10.1103/PhysRevLett.100.084102; Qian XY, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.062816; Seth AK, 2015, J NEUROSCI, V35, P3293, DOI 10.1523/JNEUROSCI.4399-14.2015; Smith SM, 2011, NEUROIMAGE, V54, P875, DOI 10.1016/j.neuroimage.2010.08.063; Sporns O, 2013, NEUROIMAGE, V80, P53, DOI 10.1016/j.neuroimage.2013.03.023; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Van Essen DC, 2013, NEUROIMAGE, V80, P62, DOI 10.1016/j.neuroimage.2013.05.041; Xia MR, 2017, NEUROIMAGE, V160, P152, DOI 10.1016/j.neuroimage.2017.02.031; Yuan NM, 2016, SCI REP-UK, V6, DOI 10.1038/srep27707; Yuan NM, 2015, SCI REP-UK, V5, DOI 10.1038/srep08143; Zhou DL, 2009, NEUROIMAGE, V47, P1590, DOI 10.1016/j.neuroimage.2009.05.089	27	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400085
C	Jagatap, G; Hegde, C		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Jagatap, Gauri; Hegde, Chinmay			Fast, Sample-Efficient Algorithms for Structured Phase Retrieval	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				STABLE SIGNAL RECOVERY; SPARSE SIGNALS; CRYSTALLOGRAPHY	We consider the problem of recovering a signal x* is an element of R-n, from magnitude-only measurements, y(i) = vertical bar < a(i), x*> vertical bar for i = {1, 2,..., m}. Also known as the phase retrieval problem, it is a fundamental challenge in nano-, bio-and astronomical imaging systems, and speech processing. The problem is ill-posed, and therefore additional assumptions on the signal and/or the measurements are necessary. In this paper, we first study the case where the underlying signal x* is s-sparse. We develop a novel recovery algorithm that we call Compressive Phase Retrieval with Alternating Minimization, or CoPRAM. Our algorithm is simple and can be obtained via a natural combination of the classical alternating minimization approach for phase retrieval, with the CoSaMP algorithm for sparse recovery. Despite its simplicity, we prove that our algorithm achieves a sample complexity of O (s(2) log n) with Gaussian samples, which matches the best known existing results. It also demonstrates linear convergence in theory and practice and requires no extra tuning parameters other than the signal sparsity level s. We then consider the case where the underlying signal x * arises from structured sparsity models. We specifically examine the case of block-sparse signals with uniform block size of b and block sparsity k - s/b. For this problem, we design a recovery algorithm that we call Block CoPRAM that further reduces the sample complexity to O(ks log n). For sufficiently large block lengths of b = circle minus( s), this bound equates to O(s log n). To our knowledge, this constitutes the first end-toend linearly convergent family of algorithms for phase retrieval where the Gaussian sample complexity has a sub-quadratic dependence on the sparsity level of the signal.	[Jagatap, Gauri; Hegde, Chinmay] Iowa State Univ, Elect & Comp Engn, Ames, IA 50011 USA	Iowa State University	Jagatap, G (corresponding author), Iowa State Univ, Elect & Comp Engn, Ames, IA 50011 USA.	gauri@iastate.edu; chinmay@iastate.edu	Jagatap, Gauri/T-6844-2019; Jeong, Yongwook/N-7413-2016					[Anonymous], 2006, GENERIC CHAINING UPP; [Anonymous], 2015, ADV NEURAL INFORM PR; Bahmani S., 2015, P ADV NEUR INF PROC, V28, P523; Baraniuk RG, 2010, IEEE T INFORM THEORY, V56, P1982, DOI 10.1109/TIT.2010.2040894; Bentkus V, 2003, J THEOR PROBAB, V16, P161, DOI 10.1023/A:1022234622381; Cai S, 2014, IEEE INT SYMP INFO, P2007, DOI 10.1109/ISIT.2014.6875185; Cai TT, 2016, ANN STAT, V44, P2221, DOI 10.1214/16-AOS1443; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Candes EJ, 2015, APPL COMPUT HARMON A, V39, P277, DOI 10.1016/j.acha.2014.09.004; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Cevher V., 2009, P SAMPL THEOR APPL S; Cevher V., 2008, ADV NEURAL INF PROC; DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001; Dirksen S, 2015, ELECTRON J PROBAB, V20, DOI 10.1214/EJP.v20-3760; Do Ba K, 2010, PROC APPL MATH, V135, P1190; Duarte  M., 2009, P IEEE C INF SCI SYS; Eldar YC, 2010, IEEE T SIGNAL PROCES, V58, P3042, DOI 10.1109/TSP.2010.2044837; Fickus M, 2014, LINEAR ALGEBRA APPL, V449, P475, DOI 10.1016/j.laa.2014.02.011; FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758; GERCHBERG RW, 1972, OPTIK, V35, P237; Goldstein Tom, 2016, ARXIV161007531; Gross D, 2017, APPL COMPUT HARMON A, V42, P37, DOI 10.1016/j.acha.2015.05.004; Hegde C., 2014, P INT C AUT LANG PRO; Hegde C., 2015, B EATCS, V1, P197; Hegde C., 2014, P ACM S DISCR ALG SO; Hegde C., 2014, P IEEE INT S INF THE; Huang JZ, 2011, J MACH LEARN RES, V12, P3371; Iwen M, 2017, APPL COMPUT HARMON A, V42, P135, DOI 10.1016/j.acha.2015.06.007; Jaganathan Kishore, 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1473, DOI 10.1109/ISIT.2012.6283508; Jaganathan K, 2013, IEEE INT SYMP INFO, P1022, DOI 10.1109/ISIT.2013.6620381; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Laurent B, 2000, ANN STAT, V28, P1302; Li XD, 2013, SIAM J MATH ANAL, V45, P3019, DOI 10.1137/120893707; Maiden AM, 2009, ULTRAMICROSCOPY, V109, P1256, DOI 10.1016/j.ultramic.2009.05.012; Marchesini S, 2007, J OPT SOC AM A, V24, P3289, DOI 10.1364/JOSAA.24.003289; Miao JW, 2008, ANNU REV PHYS CHEM, V59, P387, DOI 10.1146/annurev.physchem.59.032607.093642; MILLANE RP, 1990, J OPT SOC AM A, V7, P394, DOI 10.1364/JOSAA.7.000394; Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002; Needell D, 2008, CONF REC ASILOMAR C, P1048, DOI 10.1109/ACSSC.2008.5074572; Netrapalli P., 2013, P ADV NEUR INF PROC, P2796; Nugent KA, 2003, PHYS REV LETT, V91, DOI 10.1103/PhysRevLett.91.203902; Ohlsson H., 2012, P NEUR INF PROC SYST; Pedarsani  R., 2017, IEEE T INFORM THEORY; Qiao H, 2015, 2015 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P522, DOI 10.1109/GlobalSIP.2015.7418250; Schmidt L, 2015, P INT C MACH LEARN I; Shechtman Y, 2015, IEEE SIGNAL PROC MAG, V32, P87, DOI 10.1109/MSP.2014.2352673; Shechtman Y, 2014, IEEE T SIGNAL PROCES, V62, P928, DOI 10.1109/TSP.2013.2297687; Soltanolkotabi M, 2017, ARXIV170206175; Sun J, 2016, IEEE INT SYMP INFO, P2379, DOI 10.1109/ISIT.2016.7541725; Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9; Wang G., 2016, P ADV NEUR INF PROC, P568; Wang Gang, 2016, ARXIV161107641; Wei K, 2015, INVERSE PROBL, V31, DOI 10.1088/0266-5611/31/12/125008; Yin D, 2016, ANN ALLERTON CONF, P758, DOI 10.1109/ALLERTON.2016.7852309; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Zhang H., 2016, ADV NEURAL INFORM PR, V29, P2630	60	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404096
C	Kaufmann, E; Koolen, WM		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kaufmann, Emilie; Koolen, Wouter M.			Monte-Carlo Tree Search by Best Arm Identification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MULTIARMED BANDIT	Recent advances in bandit tools and techniques for sequential learning are steadily enabling new applications and are promising the resolution of a range of challenging related problems. We study the game tree search problem, where the goal is to quickly identify the optimal move in a given game tree by sequentially sampling its stochastic payoffs. We develop new algorithms for trees of arbitrary depth, that operate by summarizing all deeper levels of the tree into confidence intervals at depth one, and applying a best arm identification procedure at the root. We prove new sample complexity guarantees with a refined dependence on the problem instance. We show experimentally that our algorithms outperform existing elimination-based algorithms and match previous special-purpose methods for depth-two trees.	[Kaufmann, Emilie] CNRS, Lille, France; [Kaufmann, Emilie] Univ Lille, UMR CRIStAL 9189, Inria SequeL, Lille, France; [Koolen, Wouter M.] Ctr Wiskunde & Informat, Sci Pk 123, NL-1098 XG Amsterdam, Netherlands	Centre National de la Recherche Scientifique (CNRS); Universite de Lille - ISITE; Universite de Lille	Kaufmann, E (corresponding author), CNRS, Lille, France.; Kaufmann, E (corresponding author), Univ Lille, UMR CRIStAL 9189, Inria SequeL, Lille, France.	emilie.kaufmann@univ-lille1.fr; wmkoolen@cwi.nl	Jeong, Yongwook/N-7413-2016		French Agence Nationale de la Recherche (ANR) [ANR-16-CE40-0002]; Netherlands Organization for Scientific Research (NWO) under Veni grant [639.021.439]	French Agence Nationale de la Recherche (ANR)(French National Research Agency (ANR)); Netherlands Organization for Scientific Research (NWO) under Veni grant(Netherlands Organization for Scientific Research (NWO))	Emilie Kaufmann acknowledges the support of the French Agence Nationale de la Recherche (ANR), under grant ANR-16-CE40-0002 (project BADASS). Wouter Koolen acknowledges support from the Netherlands Organization for Scientific Research (NWO) under Veni grant 639.021.439.	Audibert Jean-Yves, 2010, P 23 ANN C LEARN THE, p2010a; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Borsoniu L., 2014, ADPRL14; Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810; Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119; Cazenave T, 2015, IEEE T COMP INTEL AI, V7, P102, DOI 10.1109/TCIAIG.2014.2317737; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Gabillon V., 2012, ADV NEURAL INFORM PR, P3212; Garivier, 2016, P 29 C LEARN THEOR; Garivier A., 2016, C LEARN THEOR, P998; Huang Ruitong, 2017, 28 INT C ALG LEARN T; Jamieson K., 2014, C LEARN THEOR, P423; Kalyanakrishnan S., 2012, P ICML; Karnin Z., 2013, ICML; Kaufmann E., 2013, P 26 C LEARN THEOR; Kaufmann E, 2016, J MACH LEARN RES, V17; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Pepels T., 2014, COMP GAM WORKSH ECAI; Plaat A, 1996, ARTIF INTELL, V87, P255, DOI 10.1016/0004-3702(95)00126-3; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Teraoka K, 2014, IEICE T INF SYST, VE97D, P392, DOI 10.1587/transinf.E97.D.392; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285	23	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404094
C	Klindt, DA; Ecker, AS; Euler, T; Bethge, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Klindt, David A.; Ecker, Alexander S.; Euler, Thomas; Bethge, Matthias			Neural system identification for large populations separating "what" and "where"	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				RECEPTIVE-FIELDS; SPATIAL STRUCTURE; RESPONSES; NEURONS; MODELS	Neuroscientists classify neurons into different types that perform similar computations at different locations in the visual field. Traditional methods for neural system identification do not capitalize on this separation of "what" and "where". Learning deep convolutional feature spaces that are shared among many neurons provides an exciting path forward, but the architectural design needs to account for data limitations: While new experimental techniques enable recordings from thousands of neurons, experimental time is limited so that one can sample only a small fraction of each neuron's response space. Here, we show that a major bottleneck for fitting convolutional neural networks (CNNs) to neural data is the estimation of the individual receptive field locations - a problem that has been scratched only at the surface thus far. We propose a CNN architecture with a sparse readout layer factorizing the spatial (where) and feature (what) dimensions. Our network scales well to thousands of neurons and short recordings and can be trained end-to-end. We evaluate this architecture on ground-truth data to explore the challenges and limitations of CNN-based system identification. Moreover, we show that our network model outperforms current state-of-the art system identification models of mouse primary visual cortex.	[Klindt, David A.; Ecker, Alexander S.; Euler, Thomas; Bethge, Matthias] Univ Tubingen, Ctr Integrat Neurosci, Tubingen, Germany; [Klindt, David A.; Ecker, Alexander S.; Euler, Thomas; Bethge, Matthias] Univ Tubingen, Bernstein Ctr Computat Neurosci, Tubingen, Germany; [Klindt, David A.; Euler, Thomas] Univ Tubingen, Inst Ophthalm Res, Tubingen, Germany; [Ecker, Alexander S.; Bethge, Matthias] Univ Tubingen, Inst Theoret Phys, Tubingen, Germany; [Bethge, Matthias] Max Planck Inst Biol Cybernet, Tubingen, Germany; [Ecker, Alexander S.; Bethge, Matthias] Baylor Coll Med, Ctr Neurosci & Artificial Intelligence, Houston, TX 77030 USA	Eberhard Karls University of Tubingen; Eberhard Karls University of Tubingen; Eberhard Karls University of Tubingen; Eberhard Karls University Hospital; Eberhard Karls University of Tubingen; Max Planck Society; Baylor College of Medicine	Klindt, DA (corresponding author), Univ Tubingen, Ctr Integrat Neurosci, Tubingen, Germany.; Klindt, DA (corresponding author), Univ Tubingen, Bernstein Ctr Computat Neurosci, Tubingen, Germany.; Klindt, DA (corresponding author), Univ Tubingen, Inst Ophthalm Res, Tubingen, Germany.	klindt.david@gmail.com; alexander.ecker@uni-tuebingen.de; thomas.euler@cin.uni-tuebingen.de; matthias.bethge@bethgelab.org	Ecker, Alexander S/A-5184-2010	Ecker, Alexander S/0000-0003-2392-5105	German Research Foundation (DFG) through Collaborative Research Center [CRC 1233]; DFG [EC 479/1-1]; European Union [674901]; German Excellency Initiative through the Centre for Integrative Neuroscience Tubingen [EXC307]; Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) [D16PC00003]	German Research Foundation (DFG) through Collaborative Research Center(German Research Foundation (DFG)); DFG(German Research Foundation (DFG)); European Union(European Commission); German Excellency Initiative through the Centre for Integrative Neuroscience Tubingen; Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC)	This work was supported by the German Research Foundation (DFG) through Collaborative Research Center (CRC 1233) "Robust Vision" as well as DFG grant EC 479/1-1; the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 674901; the German Excellency Initiative through the Centre for Integrative Neuroscience Tubingen (EXC307). The research was also supported by Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.	ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2015, ARXIV150203167CS; Antolik J, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004927; Baden T, 2016, NATURE, V529, P345, DOI 10.1038/nature16468; Batty Eleanor, 2017, 5 INT C LEARN REPR; Benjamin AS, 2017, BIORXIV; Cadena Santiago A., 2017, BIORXIV, DOI [10.1101/201764, DOI 10.1101/201764]; Carandini M, 2005, J NEUROSCI, V25, P10577, DOI 10.1523/JNEUROSCI.3726-05.2005; Franke K, 2017, NATURE, V542, P439, DOI 10.1038/nature21394; Gollisch T, 2010, NEURON, V65, P150, DOI 10.1016/j.neuron.2009.12.009; Heitman Alexander, 2016, BIORXIV, P45336; HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837; JONES JP, 1987, J NEUROPHYSIOL, V58, P1187, DOI 10.1152/jn.1987.58.6.1187; Khaligh-Razavi Seyed-Mahdi, 2014, BIORXIV, P9936, DOI [10.1101/009936, DOI 10.1101/009936]; Kindel William F., 2017, ARXIV170606208CSQBIO; Kingma D.P, P 3 INT C LEARNING R; Lau B, 2002, P NATL ACAD SCI USA, V99, P8974, DOI 10.1073/pnas.122173799; LEHKY SR, 1992, J NEUROSCI, V12, P3568; McFarland JM, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003143; McIntosh Lane T., 2017, ARXIV170201825QBIOST; Prenger R, 2004, NEURAL NETWORKS, V17, P663, DOI 10.1016/j.neunet.2004.03.008; Real Esteban, 2017, CURRENT BIOL; Rowekamp RJ, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms15739; Rust NC, 2005, NEURON, V46, P945, DOI 10.1016/j.neuron.2005.05.021; Touryan J, 2005, NEURON, V45, P781, DOI 10.1016/j.neuron.2005.01.029; Vintch B, 2015, J NEUROSCI, V35, P14829, DOI 10.1523/JNEUROSCI.2815-13.2015; Weber Alison I., 2016, ARXIV160207389QBIO; Willmore B, 2008, NEURAL COMPUT, V20, P1537, DOI 10.1162/neco.2007.05-07-513; Wu MCK, 2006, ANNU REV NEUROSCI, V29, P477, DOI 10.1146/annurev.neuro.29.051605.113024; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111	31	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403056
C	Kontorovich, A; Sabato, S; Weiss, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kontorovich, Aryeh; Sabato, Sivan; Weiss, Roi			Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CLASSIFICATION; CONVERGENCE; BOUNDS; ERROR; RATES; CURSE	We examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based multiclass learning algorithm. This algorithm is derived from sample compression bounds and enjoys the statistical advantages of tight, fully empirical generalization bounds, as well as the algorithmic advantages of a faster runtime and memory savings. We prove that this algorithm is strongly Bayes-consistent in metric spaces with finite doubling dimension - the first consistency result for an efficient nearest-neighbor sample compression scheme. Rather surprisingly, we discover that this algorithm continues to be Bayes-consistent even in a certain infinite-dimensional setting, in which the basic measure-theoretic conditions on which classic consistency proofs hinge are violated. This is all the more surprising, since it is known that k-NN is not Bayes-consistent in this setting. We pose several challenging open problems for future research.	[Kontorovich, Aryeh; Sabato, Sivan] Ben Gurion Univ Negev, Dept Comp Sci, Beer Sheva, Israel; [Weiss, Roi] Weizmann Inst Sci, Dept Comp Sci & Appl Math, Rehovot, Israel	Ben Gurion University; Weizmann Institute of Science	Kontorovich, A (corresponding author), Ben Gurion Univ Negev, Dept Comp Sci, Beer Sheva, Israel.	karyeh@cs.bgu.ac.il; sabatos@bgu.ac.il; roiw@weizmann.ac.il	Kontorovich, Aryeh/X-9225-2019; Kontorovich, Aryeh/AAB-4744-2020; Sabato, Sivan/U-4730-2017; Jeong, Yongwook/N-7413-2016	Kontorovich, Aryeh/0000-0001-8038-8671; Sabato, Sivan/0000-0002-7975-0044; 	Israel Science Foundation [555/15, 755/15]; IBM; Paypal	Israel Science Foundation(Israel Science Foundation); IBM(International Business Machines (IBM)); Paypal	We thank Frederic Cerou for the numerous fruitful discussions and helpful feedback on an earlier draft. Aryeh Kontorovich was supported in part by the Israel Science Foundation (grant No. 755/15), Paypal and IBM. Sivan Sabato was supported in part by the Israel Science Foundation (grant No. 555/15).	Abraham C, 2006, ANN I STAT MATH, V58, P619, DOI 10.1007/s10463-006-0032-1; Berend D, 2013, ELECTRON COMMUN PROB, V18, P1, DOI 10.1214/ECP.v18-2359; Berend D, 2012, STAT PROBABIL LETT, V82, P1102, DOI 10.1016/j.spl.2012.02.014; Beygelzimer A., 2006, P 23 INT C MACH LEAR, P97, DOI DOI 10.1145/1143844.1143857; Biau G, 2005, IEEE T INFORM THEORY, V51, P2163, DOI 10.1109/TIT.2005.847705; Biau G, 2010, IEEE T INFORM THEORY, V56, P2034, DOI 10.1109/TIT.2010.2040857; Bogachev V.I., 2007, MEASURE THEORY, VI; Bogachev Vladimir I., 2007, MEASURE THEORY, VII; Boiman O., 2008, CVPR; Cerou F., 2006, ESAIM-PROBAB STAT, V10, P340, DOI [10.1051/ps:2006014, DOI 10.1051/PS:2006014]; Chaudhuri Kamalika, 2014, NIPS; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; DEVROYE L, 1981, IEEE T PATTERN ANAL, V3, P75, DOI 10.1109/TPAMI.1981.4767052; Devroye L., 1985, WILEY SERIES PROBABI; Devroye L, 2013, PROBABILISTIC THEORY, V31; Federer H., 1969, GRUNDLEHREN MATH WIS, V153; FIX E, 1989, INT STAT REV, V57, P238, DOI 10.2307/1403797; Floyd S, 1995, MACH LEARN, V21, P269; Gottlieb LA, 2016, THEOR COMPUT SCI, V620, P105, DOI 10.1016/j.tcs.2015.10.040; Gottlieb LA, 2014, IEEE T INFORM THEORY, V60, P5750, DOI 10.1109/TIT.2014.2339840; Gottlieb Lee-Ad, 2014, NEURAL INFORM PROCES; Gottlieb Lee-Ad, 2017, J MACHINE LEARNING R; Graepel T, 2005, MACH LEARN, V59, P55, DOI 10.1007/s10994-005-0462-7; Hall P, 2005, ANN STAT, V33, P284, DOI 10.1214/009053604000000959; Kallenberg O., 2002, FDN MODERN PROBABILI; KONTOROVICH A., 2014, ARTIFICIAL INTELLIGE; Kontorovich A., 2014, INT C MACH LEARN ICM; Kontorovich Aryeh, 2017, ABS170508184 CORR; Kontorovich Aryeh, 2016, ADV NEURAL INFORM PR, P856; Krauthgamer Robert, 2004, 15 ANN ACM SIAM S DI, P791; KULKARNI SR, 1995, IEEE T INFORM THEORY, V41, P1028, DOI 10.1109/18.391248; Littlestone N., 1986, RELATING DATA UNPUB; Munkres J.R., 1975, TOPOLOGY 1 COURSE; Pestov V, 2000, INFORM PROCESS LETT, V73, P47, DOI 10.1016/S0020-0190(99)00156-8; Pestov V, 2013, COMPUT MATH APPL, V65, P1427, DOI 10.1016/j.camwa.2012.09.011; Preiss D., 1981, COMMENT MATH UNIV CA, V22, P181; Preiss D., 1979, 7 WINT SCH ABSTR AN, P58; PSALTIS D, 1994, IEEE T INFORM THEORY, V40, P820, DOI 10.1109/18.335893; Rudin W, 1976, INT SERIES PURE APPL; Rudin W, 1987, REAL COMPLEX ANAL; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Snapp RR, 1998, ANN STAT, V26, P850; Tiser J, 2003, T AM MATH SOC, V355, P3277, DOI 10.1090/S0002-9947-03-03296-3; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; ZHAO LC, 1987, J MULTIVARIATE ANAL, V21, P168, DOI 10.1016/0047-259X(87)90105-9	48	3	3	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401059
C	Kuleshov, V; Ermon, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kuleshov, Volodymyr; Ermon, Stefano			Neural Variational Inference and Learning in Undirected Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ALGORITHM	Many problems in machine learning are naturally expressed in the language of undirected graphical models. Here, we propose black-box learning and inference algorithms for undirected models that optimize a variational approximation to the log-likelihood of the model. Central to our approach is an upper bound on the log-partition function parametrized by a function q that we express as a flexible neural network. Our bound makes it possible to track the partition function during learning, to speed-up sampling, and to train a broad class of hybrid directed/undirected models via a unified variational inference framework. We empirically demonstrate the effectiveness of our method on several popular generative modeling datasets.	[Kuleshov, Volodymyr; Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Kuleshov, V (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	kuleshov@cs.stanford.edu; ermon@cs.stanford.edu	Jeong, Yongwook/N-7413-2016		Intel Corporation; Toyota; NSF [1651565, 1649208, 1522054]; Future of Life Institute [2016-158687]	Intel Corporation(Intel Corporation); Toyota; NSF(National Science Foundation (NSF)); Future of Life Institute	This work is supported by the Intel Corporation, Toyota, NSF (grants 1651565, 1649208, 1522054) and by the Future of Life Institute (grant 2016-158687).	Alimoglu Fevzi, 1996, COMBINING MULTIPLE C; Bastien F., 2012, DEEP LEARN UNS FEAT; Bilmes JA, 2004, IMA VOL MATH APPL, V138, P191; Burda Yuri, 2015, ABS150900519 CORR; Desjardins Guillaume, 2011, ADV NEURAL INFORM PR, P2501; Dieng A. B., 2017, ADV NEURAL INFORM PR, P2732; Gershman S.J., 2012, P 29 INT C MACH LEAR, P235; Gopal S., 2013, INT C MACHINE LEARNI, P289; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Koller D., 2009, PROBABILISTIC GRAPHI; Larochelle H., 2011, INT C ART INT STAT; Maaloe L, 2016, PR MACH LEARN RES, V48; Minka, 2005, DIVERGENCE MEASURES; Mnih A, 2016, PR MACH LEARN RES, V48; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Ranganath R, 2016, PR MACH LEARN RES, V48; Ranganath R, 2014, JMLR WORKSH CONF PRO, V33, P814; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Rolfe J. T., 2016, ARXIV160902200; Ryu Ernest K., 2014, UNPUB; Salakhutdinov Ruslan, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPR.2009.5206577; Salimans T, 2015, PR MACH LEARN RES, V37, P1218; Scott J., 2012, SOCIAL NETWORK ANAL; Smolensky P., 1986, PARALLEL DISTRIBUTED, V1, P194; Srinivasan R, 2013, IMPORTANCE SAMPLING; Tieleman Tijmen, 2009, P 26 ANN INT C MACH, P1033, DOI DOI 10.1145/1553374.1553506; Tran Dustin, 2016, ARXIV161009787; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Zhang YY, 2001, IEEE T MED IMAGING, V20, P45, DOI 10.1109/42.906424	33	3	3	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406077
C	Lee, J; Carlson, D; Shokri, H; Yao, WC; Goetz, G; Hagen, E; Batty, E; Chichilnisky, EJ; Einevoll, G; Paninski, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lee, JinHyung; Carlson, David; Shokri, Hooshmand; Yao, Weichi; Goetz, Georges; Hagen, Espen; Batty, Eleanor; Chichilnisky, E. J.; Einevoll, Gaute; Paninski, Liam			YASS: Yet Another Spike Sorter	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Spike sorting is a critical first step in extracting neural signals from large-scale electrophysiological data. This manuscript describes an efficient, reliable pipeline for spike sorting on dense multi-electrode arrays (MEAs), where neural signals appear across many electrodes and spike sorting currently represents a major computational bottleneck. We present several new techniques that make dense MEA spike sorting more robust and scalable. Our pipeline is based on an efficient multi-stage "triage-then-cluster-then-pursuit" approach that initially extracts only clean, high-quality waveforms from the electrophysiological time series by temporarily skipping noisy or "collided" events (representing two neurons firing synchronously). This is accomplished by developing a neural network detection method followed by efficient outlier triaging. The clean waveforms are then used to infer the set of neural spike waveform templates through nonparametric Bayesian clustering. Our clustering approach adapts a "coreset" approach for data reduction and uses efficient inference methods in a Dirichlet process mixture model framework to dramatically improve the scalability and reliability of the entire pipeline. The "triaged" waveforms are then finally recovered with matching-pursuit deconvolution techniques. The proposed methods improve on the state-of-the-art in terms of accuracy and stability on both real and biophysically-realistic simulated MEA data. Furthermore, the proposed pipeline is efficient, learning templates and clustering faster than real-time for a similar or equal to 500-electrode dataset, largely on a single CPU core.	[Lee, JinHyung; Shokri, Hooshmand; Yao, Weichi; Batty, Eleanor; Paninski, Liam] Columbia Univ, New York, NY 10027 USA; [Carlson, David] Duke Univ, Durham, NC 27706 USA; [Goetz, Georges; Chichilnisky, E. J.] Stanford Univ, Stanford, CA 94305 USA; [Hagen, Espen] Univ Oslo, Oslo, Norway; [Einevoll, Gaute] Norwegian Univ Life Sci, As, Norway	Columbia University; Duke University; Stanford University; University of Oslo; Norwegian University of Life Sciences	Lee, J (corresponding author), Columbia Univ, New York, NY 10027 USA.		Jeong, Yongwook/N-7413-2016	Carlson, David/0000-0003-1005-6385; Chichilnisky, E.J./0000-0002-5613-0248	NSF [IIS-1546296, IIS-1430239]; DARPA [N66001-17-C-4002]	NSF(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work was partially supported by NSF grants IIS-1546296 and IIS-1430239, and DARPA Contract No. N66001-17-C-4002.	[Anonymous], 2012, IEEE SIGNAL PROCESSI; Arthur David, 2007, ACM SIAM S DISCR ALG; Bachem O., 2015, ICML; Bahmani B., 2012, P VLDB ENDOWMENT; Bankman I. N., 1993, IEEE T BIOMED ENG; Barnett A. H., 2016, J NEURO METHODS; Buzsaki G., 2004, NATURE NEUROSCIENCE; Campbell T., 2015, NIPS; Carlson D, 2013, NIPS; Carlson D. E., 2014, IEEE TBME; Chen B., 2011, NIPS; Dacey D. M., 2003, NEURON; Ekanadham C., 2014, J NEURO METHODS; Feldman D., 2011, NIPS; Fournier J, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0160494; Franke F., 2010, J COMP NEURO; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Hagen E., 2015, J NEURO METHODS; Har-Peled S., 2004, ACM THEORY COMPUTING; Hilgen G., 2017, CELL REPORTS; Hughes M. C., 2013, NIPS; Ishwaran H., 2001, JASA; Jun J.J., 2017, BIORXIV; Kadir Shabnam N, 2014, NEURAL COMPUTATION; Kim K. H., 2000, IEEE TBME; Kingma D.P., 2015, INT C LEARN REPR, P1; Knox E. M., 1998, VLDB; Knudson K. C., 2014, NIPS; Lewicki M., 1998, NETWORK COMPUTATION; Litke A., 2004, IEEE T NUCL SCI; Magland J. F., 2015, ARXIV150804841; Mukhopadhyay S., 1998, IEEE TBME; Muthmann JO, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00028; Neal R. M., 2000, J COMPUTATIONAL GRAP, V2; Ng A. Y., SPECTRAL CLUSTERING; Pachitariu M., 2016, NIPS; Pillow JW, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0062123; Quiroga R. Q., 2004, NEURAL COMPUTATION; Rey H. G., 2015, BRAIN RES B; Rodriguez A., 2014, SCIENCE; Schmidt E. M., 1984, J NEURO METHODS; Tarjan Robert, 1972, SIAM J COMPUTING; Thorbergsson P. T., 2010, IEEE EMBC; Ventura V., 2009, NEURAL COMPUTATION; Vogelstein R. J., 2004, IEEE EMBS, V1; Wang L., 2011, J COMP GRAPHICAL STA; Wiltschko A. B., 2008, J NEURO METHODS; Wood F., 2008, J NEURO METHODS; Wood F., 2004, IEEE TBME; Yang X., 1988, IEEE T BIOMED ENG; Yger P., 2016, BIORXIV; Zelnik-Manor L, 2004, NIPS, V17	52	3	3	4	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404008
C	Lou, Q; Dechter, R; Ihler, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lou, Qi; Dechter, Rina; Ihler, Alexander			Dynamic Importance Sampling for Anytime Bounds of the Partition Function	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Computing the partition function is a key inference task in many graphical models. In this paper, we propose a dynamic importance sampling scheme that provides anytime finite-sample bounds for the partition function. Our algorithm balances the advantages of the three major inference strategies, heuristic search, variational bounds, and Monte Carlo methods, blending sampling with search to refine a variationally defined proposal. Our algorithm combines and generalizes recent work on anytime search [16] and probabilistic bounds [15] of the partition function. By using an intelligently chosen weighted average over the samples, we construct an unbiased estimator of the partition function with strong finite-sample confidence intervals that inherit both the rapid early improvement rate of sampling and the long-term benefits of an improved proposal from search. This gives significantly improved anytime behavior, and more flexible trade-offs between memory, time, and solution quality. We demonstrate the effectiveness of our approach empirically on real-world problem instances taken from recent UAI competitions.	[Lou, Qi; Dechter, Rina; Ihler, Alexander] Univ Calif Irvine, Comp Sci, Irvine, CA 92697 USA	University of California System; University of California Irvine	Lou, Q (corresponding author), Univ Calif Irvine, Comp Sci, Irvine, CA 92697 USA.	qlou@ics.uci.edu; dechter@ics.uci.edu; ihler@ics.uci.edu	Jeong, Yongwook/N-7413-2016	Ihler, Alexander/0000-0002-4331-1015	NSF [IIS-1526842, IIS-1254071]; United States Air Force [FA8750-14-C-0011, FA9453-16-C-0508]	NSF(National Science Foundation (NSF)); United States Air Force(United States Department of Defense)	This work is sponsored in part by NSF grants IIS-1526842, IIS-1254071, and by the United States Air Force under Contract No. FA8750-14-C-0011 and FA9453-16-C-0508.	Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810; Chakraborty S., IJCAI16; Chakraborty S, 2014, AAAI CONF ARTIF INTE, P1722; Dagum P, 1997, ARTIF INTELL, V93, P1, DOI 10.1016/S0004-3702(97)00013-1; Darwiche A, 2009, MODELING AND REASONING WITH BAYESIAN NETWORKS, P1, DOI 10.1017/CBO9780511811357; Dechter R, 2003, J ACM, V50, P107, DOI 10.1145/636865.636866; Dechter R., 2013, SYNTH LECT ARTIF INT, V7, P1, DOI DOI 10.2200/S00529ED1V01Y201308AIM023; Elwert F., 2010, HEURISTICS PROBABILI, P327; Ermon S., 2013, P 30 INT C MACH LEAR; Ermon S, 2014, PR MACH LEARN RES, V32; Gogate V, 2011, INTELL ARTIF, V5, P171, DOI 10.3233/IA-2011-0026; HENRION M, 1991, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P142; Liu Q., 2014, THESIS; Liu Q., 2015, ADV NEURAL INFORM PR, P1432; Liu Q, 2011, P 28 INT C MACH LEAR; Lou Q., 2017, P 31 AAAI C ART INT; Maurer Andreas, 2009, ARXIV09073740; Oh M-S, 1992, J STAT COMPUT SIMUL, V41, P143, DOI [DOI 10.1080/00949659208810398, 10.1080/00949659208810398]; Valiant L. G., 1979, Theoretical Computer Science, V8, P189, DOI 10.1016/0304-3975(79)90044-6; Viricel C, 2016, LECT NOTES COMPUT SC, V9892, P733, DOI 10.1007/978-3-319-44953-1_46; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Yanover C., 2002, ADV NEURAL INFORM PR, P1457	22	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403026
C	Lueckmann, JM; Goncalves, PJ; Bassetto, G; Ocal, K; Nonnenmacher, M; Macke, JH		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lueckmann, Jan-Matthis; Goncalves, Pedro J.; Bassetto, Giacomo; Oecal, Kaan; Nonnenmacher, Marcel; Macke, Jakob H.			Flexible statistical inference for mechanistic models of neural dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SIMULATION	Mechanistic models of single-neuron dynamics have been extensively studied in computational neuroscience. However, identifying which models can quantitatively reproduce empirically measured data has been challenging. We propose to overcome this limitation by using likelihood-free inference approaches (also known as Approximate Bayesian Computation, ABC) to perform full Bayesian inference on single-neuron models. Our approach builds on recent advances in ABC by learning a neural network which maps features of the observed data to the posterior distribution over parameters. We learn a Bayesian mixture-density network approximating the posterior over multiple rounds of adaptively chosen simulations. Furthermore, we propose an efficient approach for handling missing features and parameter settings for which the simulator fails, as well as a strategy for automatically learning relevant features using recurrent neural networks. On synthetic data, our approach efficiently estimates posterior distributions and recovers ground-truth parameters. On in-vitro recordings of membrane voltages, we recover multivariate posteriors over biophysical parameters, which yield model-predicted voltage traces that accurately match empirical data. Our approach will enable neuroscientists to perform Bayesian inference on complex neuron models without having to design model-specific algorithms, closing the gap between mechanistic and statistical approaches to single-neuron modelling.	[Lueckmann, Jan-Matthis; Goncalves, Pedro J.; Bassetto, Giacomo; Oecal, Kaan; Nonnenmacher, Marcel; Macke, Jakob H.] Max Planck Gesell, Res Ctr Caesar, Bonn, Germany; [Oecal, Kaan] Univ Bonn, Math Inst, Bonn, Germany; [Macke, Jakob H.] Tech Univ Darmstadt, Ctr Cognit Sci, Darmstadt, Germany	Center of Advanced European Studies & Research (CAESAR); Max Planck Society; University of Bonn; Technical University of Darmstadt	Lueckmann, JM (corresponding author), Max Planck Gesell, Res Ctr Caesar, Bonn, Germany.	jan-matthis.lueckmann@caesar.de; pedro.goncalves@caesar.de; giacomo.bassetto@caesar.de; kaan.oecal@caesar.de; marcel.nonnenmacher@caesar.de; jakob.macke@caesar.de	Jeong, Yongwook/N-7413-2016		German Research Foundation (DFG) [SFB 1089, SFB 1233]; caesar foundation	German Research Foundation (DFG)(German Research Foundation (DFG)); caesar foundation	We thank Maneesh Sahani, David Greenberg and Balaji Lakshminarayanan for useful comments on the manuscript. This work was supported by SFB 1089 (University of Bonn) and SFB 1233 (University of Tubingen) of the German Research Foundation (DFG) to JHM and by the caesar foundation.	Beaumont M, 2002, GENETICS, V162; BEAUMONT MA, 2009, BIOMETRIKA; Blum MGB, 2013, STAT SCI, V28, P189, DOI 10.1214/12-STS406; Blum MGB, 2010, STAT COMPUT, V20, P63, DOI 10.1007/s11222-009-9116-0; Bonassi FV, 2015, BAYESIAN ANAL, V10, P171, DOI 10.1214/14-BA891; Carlson KD, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00010; Carnevale N.T., 2009, NEURON BOOK, V1st; Cho K, 2014, C EMP METH NAT LANG, DOI 10.3115/v1/D14-1179; Chung J., 2014, DEEP LEARN WORKSH C; Daly AC, 2015, ROY SOC OPEN SCI, V2, DOI 10.1098/rsos.150499; De Nicolao G, 1997, AUTOMATICA, V33; Diggle P J, 1984, J R STAT SOC B; DRUCKMANN S, 2007, FRONT NEUROSCI, V1; Fan Y, 2013, STAT-US, V2, P34, DOI 10.1002/sta4.15; Friedrich P, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00063; Gerhard F, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005390; Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615; Graves A, 2011, ADV NEUR IN; Gu S. S., 2015, ADV NEURAL INFORM PR, P2629; Hay E, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002107; Hinton G. E., 1993, P 6 ANN C COMP LEARN; Hodgkin A. L., 1952, J PHYSL, V117; Huys QJM, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000379; Jarvenpaa M., 2017, ARXIV170400520; Jiang B., 2015, ARXIV151002175; Kingma D P, 2015, VARIATIONAL DROPOUT, P2575; Linderman S W, 2017, BIORXIV; Linderman S. W., 2016, ADV NEURAL INFORM PR; Lintusaari J, 2016, SYST BIOL; Marjoram P, 2003, P NATL ACAD SCI USA, V100, P15324, DOI 10.1073/pnas.0306899100; Markram H, 2015, CELL, V163, P456, DOI 10.1016/j.cell.2015.09.029; Meeds E, 2014, UAI; Meeds E., 2015, ARXIV150301916; Meliza C D, 2014, BIOL CYBERN, V108; Meng L, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/6/065006; Ong V. M., 2016, ARXIV160803069; Papamakarios G, 2017, ADV NEUR IN; Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140; Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001; Pospischil M, 2008, BIOL CYBERN, V99, P427, DOI 10.1007/s00422-008-0263-8; Price L.F., 2017, J COMPUT GRAPH STAT; Prinz AA, 2003, J NEUROPHYSIOL, V90, P3998, DOI 10.1152/jn.00641.2003; Rossant C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI [10.3389/fnins.2011.00009, 10.3389/fninf.2011.00009]; Stringer C, 2016, ELIFE, V5, DOI 10.7554/eLife.19695; Turner B M, 2014, PSYCHONOMIC B REV, V21; Van Geit W, 2016, FRONT NEUROINFORM, V10, DOI 10.3389/fninf.2016.00017; vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724; Wilkinson R., 2014, AISTATS; Wood SN, 2010, NATURE, V466, P1102, DOI 10.1038/nature09319	51	3	3	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401032
C	Mallasto, A; Feragen, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Mallasto, Anton; Feragen, Aasa			Learning from uncertain curves: The 2-Wasserstein metric for Gaussian processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				WASSERSTEIN; TRACTOGRAPHY; BARYCENTERS; DISTANCE	We introduce a novel framework for statistical analysis of populations of non-degenerate Gaussian processes (GPs), which are natural representations of uncertain curves. This allows inherent variation or uncertainty in function-valued data to be properly incorporated in the population analysis. Using the 2-Wasserstein metric we geometrize the space of GPs with L-2 mean and covariance functions over compact index spaces. We prove uniqueness of the barycenter of a population of GPs, as well as convergence of the metric and the barycenter of their finite-dimensional counterparts. This justifies practical computations. Finally, we demonstrate our framework through experimental validation on GP datasets representing brain connectivity and climate development. A MATLAB library for relevant computations will be published at https://sites.google.com/view/antonmallasto/software.	[Mallasto, Anton; Feragen, Aasa] Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark	University of Copenhagen	Mallasto, A (corresponding author), Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark.	mallasto@di.ku.dk; aasa@di.ku.dk	Jeong, Yongwook/N-7413-2016; Feragen, Aasa/G-1465-2013	Feragen, Aasa/0000-0002-9945-981X	Centre for Stochastic Geometry and Advanced Bioimaging - Villum Foundation; McDonnell Center for Systems Neuroscience at Washington University	Centre for Stochastic Geometry and Advanced Bioimaging - Villum Foundation; McDonnell Center for Systems Neuroscience at Washington University	This research was supported by Centre for Stochastic Geometry and Advanced Bioimaging, funded by a grant from the Villum Foundation. Data were provided [in part] by the Human Connectome Project, WU-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University. The authors would also like to thank Mads Nielsen for valuable discussions and supervision. Finally, the authors would like to thank Victor Panaretos for valuable discussions and, in particular, for pointing out an error in an earlier version of the manuscript.	Agueh M, 2011, SIAM J MATH ANAL, V43, P904, DOI 10.1137/100805741; Aliprantis C., 1999, STUDIES EC THEORY, V4; Alvarez-Esteban PC, 2011, ANN I H POINCARE-PR, V47, P358, DOI 10.1214/09-AIHP354; Alvarez-Esteban PC, 2016, J MATH ANAL APPL, V441, P744, DOI 10.1016/j.jmaa.2016.04.045; Ambrosio L., 2008, LECT MATH ETH ZURICH, VSecond; Ambrosio L, 2013, LECT NOTES MATH, V2062, P1, DOI 10.1007/978-3-642-32160-3_1; ARVESON W., 2006, SHORT COURSE SPECTRA, V209; Berman J, 2009, MAGN RESON IMAGING C, V17, P205, DOI 10.1016/j.mric.2009.02.002; Bulygina O. N., 2012, DAILY TEMPERATURE PR; CuestaAlbertos JA, 1996, J THEOR PROBAB, V9, P263, DOI 10.1007/BF02214649; DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X; Faraki M, 2015, INT CONF ACOUST SPEE, P1364, DOI 10.1109/ICASSP.2015.7178193; GELBRICH M, 1990, MATH NACHR, V147, P185, DOI 10.1002/mana.19901470121; GIVENS CR, 1984, MICH MATH J, V31, P231; Glasser MF, 2013, NEUROIMAGE, V80, P105, DOI 10.1016/j.neuroimage.2013.04.127; Minh HQ, 2016, ADV COMPUT VIS PATT, P115, DOI 10.1007/978-3-319-45026-1_5; Harandi M, 2014, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2014.132; Hauberg S, 2015, LECT NOTES COMPUT SC, V9349, P597, DOI 10.1007/978-3-319-24553-9_73; Le M, 2015, LECT NOTES COMPUT SC, V9351, P38, DOI 10.1007/978-3-319-24574-4_5; Masarotto  V., 2018, ARXIV180101990; Masci J., 2015, P IEEE INT C COMP VI, P37; OLKIN I, 1982, LINEAR ALGEBRA APPL, V48, P257, DOI 10.1016/0024-3795(82)90112-4; Pigoli D, 2014, BIOMETRIKA, V101, P409, DOI 10.1093/biomet/asu008; Pujol S, 2015, J NEUROIMAGING, V25, P875, DOI 10.1111/jon.12283; Quang M. H., 2014, ADV NEURAL INFORM PR, P388; Rajput B. S., 1972, J MULTIVARIATE ANAL, V2, P382; Roberts S, 2013, PHILOS T R SOC A, V371, DOI 10.1098/rsta.2011.0550; Schober M, 2014, ADV NEURAL INFORM PR, V27, P739; Schober M, 2014, LECT NOTES COMPUT SC, V8675, P265, DOI 10.1007/978-3-319-10443-0_34; SEGUY V, 2015, ADV NEURAL INFORM PR, V28, P3312; Sotiropoulos SN, 2013, MAGN RESON MED, V70, P1682, DOI 10.1002/mrm.24623; Takatsu A, 2011, OSAKA J MATH, V48, P1005; Tatusko RL, 1990, COOPERATION CLIMATE; Van Essen DC, 2013, NEUROIMAGE, V80, P62, DOI 10.1016/j.neuroimage.2013.05.041; Villani C., 2003, TOPICS OPTIMAL TRANS, V58; Wassermann D, 2010, NEUROIMAGE, V51, P228, DOI 10.1016/j.neuroimage.2010.01.004; Yang X, 2015, LECT NOTES COMPUT SC, V9350, P289, DOI 10.1007/978-3-319-24571-3_35	38	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405072
C	Milstein, DJ; Pacheco, JL; Hochberg, LR; Simeral, JD; Jarosiewicz, B; Sudderth, EB		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Milstein, Daniel J.; Pacheco, Jason L.; Hochberg, Leigh R.; Simeral, John D.; Jarosiewicz, Beata; Sudderth, Erik B.			Multiscale Semi-Markov Dynamics for Intracortical Brain-Computer Interfaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				TETRAPLEGIA; MOVEMENTS	Intracortical brain-computer interfaces (iBCIs) have allowed people with tetraplegia to control a computer cursor by imagining the movement of their paralyzed arm or hand. State-of-the-art decoders deployed in human iBCIs are derived from a Kalman filter that assumes Markov dynamics on the angle of intended movement, and a unimodal dependence on intended angle for each channel of neural activity. Due to errors made in the decoding of noisy neural data, as a user attempts to move the cursor to a goal, the angle between cursor and goal positions may change rapidly. We propose a dynamic Bayesian network that includes the on-screen goal position as part of its latent state, and thus allows the person's intended angle of movement to be aggregated over a much longer history of neural activity. This multiscale model explicitly captures the relationship between instantaneous angles of motion and long-term goals, and incorporates semi-Markov dynamics for motion trajectories. We also introduce a multimodal likelihood model for recordings of neural populations which can be rapidly calibrated for clinical applications. In offline experiments with recorded neural data, we demonstrate significantly improved prediction of motion directions compared to the Kalman filter. We derive an efficient online inference algorithm, enabling a clinical trial participant with tetraplegia to control a computer cursor with neural activity in real time. The observed kinematics of cursor movement are objectively straighter and smoother than prior iBCI decoding models without loss of responsiveness.	[Milstein, Daniel J.; Sudderth, Erik B.] Brown Univ, Dept Comp Sci, Providence, RI 02912 USA; [Pacheco, Jason L.] MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Hochberg, Leigh R.; Simeral, John D.] Brown Univ, Sch Engn, Providence, RI 02912 USA; [Hochberg, Leigh R.; Simeral, John D.] Massachusetts Gen Hosp, Dept Neurol, Boston, MA 02114 USA; [Hochberg, Leigh R.; Simeral, John D.; Jarosiewicz, Beata] Dept Vet Affairs Med Ctr, Rehabil R&D Serv, Providence, RI USA; [Hochberg, Leigh R.; Simeral, John D.; Jarosiewicz, Beata] Brown Univ, Brown Inst Brain Sci, Providence, RI 02912 USA; [Hochberg, Leigh R.] Harvard Med Sch, Dept Neurol, Boston, MA USA; [Jarosiewicz, Beata] Brown Univ, Dept Neurosci, Providence, RI 02912 USA; [Jarosiewicz, Beata] Stanford Univ, Dept Neurosurg, Stanford, CA 94305 USA; [Sudderth, Erik B.] Univ Calif Irvine, Dept Comp Sci, Irvine, CA USA	Brown University; Massachusetts Institute of Technology (MIT); Brown University; Harvard University; Massachusetts General Hospital; Brown University; Harvard University; Harvard Medical School; Brown University; Stanford University; University of California System; University of California Irvine	Milstein, DJ (corresponding author), Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.	daniel_milstein@alumni.brown.edu; pachecoj@mit.edu; leigh_hochberg@brown.edu; john_simeral@brown.edu; beataj@stanford.edu; sudderth@uci.edu	Jeong, Yongwook/N-7413-2016; Hochberg, Leigh/R-6872-2019; Jarosiewicz, Beata/AAT-8548-2021; Simeral, John/CCI-2624-2022	Hochberg, Leigh/0000-0003-0261-2273; Sudderth, Erik/0000-0002-0595-9726	Office of Research and Development, Rehabilitation R&D Service, Department of Veterans Affairs [B4853C, B6453R, N9228C]; National Institute on Deafness and Other Communication Disorders of National Institutes of Health (NIDCD-NIH) [R01DC009899]; MGH-Deane Institute; Executive Committee on Research (ECOR) of Massachusetts General Hospital	Office of Research and Development, Rehabilitation R&D Service, Department of Veterans Affairs(US Department of Veterans Affairs); National Institute on Deafness and Other Communication Disorders of National Institutes of Health (NIDCD-NIH); MGH-Deane Institute; Executive Committee on Research (ECOR) of Massachusetts General Hospital	The authors thank Participants T9 and T10 and their families, Brian Franco, Tommy Hosman, Jessica Kelemen, Dave Rosler, Jad Saab, and Beth Travers for their contributions to this research. Support for this study was provided by the Office of Research and Development, Rehabilitation R&D Service, Department of Veterans Affairs (B4853C, B6453R, and N9228C), the National Institute on Deafness and Other Communication Disorders of National Institutes of Health (NIDCD-NIH: R01DC009899), MGH-Deane Institute, and The Executive Committee on Research (ECOR) of Massachusetts General Hospital. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health, or the Department of Veterans Affairs or the United States Government. CAUTION: Investigational Device. Limited by Federal Law to Investigational Use.	Ajiboye AB, 2017, LANCET, V389, P1821, DOI 10.1016/S0140-6736(17)30601-3; Amirikian B, 2000, NEUROSCI RES, V36, P73, DOI 10.1016/S0168-0102(99)00112-1; Bacher D, 2015, NEUROREHAB NEURAL RE, V29, P462, DOI 10.1177/1545968314554624; Boyen X., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P33; Collinger JL, 2013, LANCET, V381, P557, DOI 10.1016/S0140-6736(12)61816-9; Donoghue J. A., 2011, YOUMANS NEUROLOGICAL; Fraser GW, 2009, J NEURAL ENG, V6, DOI 10.1088/1741-2560/6/5/055004; GEORGOPOULOS AP, 1982, J NEUROSCI, V2, P1527; Gilja V, 2015, NAT MED, V21, P1142, DOI 10.1038/nm.3953; Hochberg LR, 2006, NATURE, V442, P164, DOI 10.1038/nature04970; Hochberg LR, 2012, NATURE, V485, P372, DOI 10.1038/nature11076; Jarosiewicz B, 2015, SCI TRANSL MED, V7, DOI 10.1126/scitranslmed.aac7328; Kim SP, 2008, J NEURAL ENG, V5, P455, DOI 10.1088/1741-2560/5/4/010; Koyama S, 2010, J COMPUT NEUROSCI, V29, P73, DOI 10.1007/s10827-009-0196-9; Levy P., 1954, P INT C MATH, VIII, P416; MacKenzie I. S., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P9, DOI 10.1145/365024.365028; Malik WQ, 2015, IEEE T BIO-MED ENG, V62, P570, DOI 10.1109/TBME.2014.2360393; Murphy K., 2001, P 17 C UNC ART INT, P378; Murphy KP., 2002, DYNAMIC BAYESIAN NET; Pandarinath C, 2017, ELIFE, V6, DOI 10.7554/eLife.18554; Simeral JD, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/2/025027; SMITH WL, 1955, PROC R SOC LON SER-A, V232, P6, DOI 10.1098/rspa.1955.0198; Willett FR, 2017, J NEURAL ENG, V14, DOI 10.1088/1741-2560/14/1/016001; Yu SZ, 2010, ARTIF INTELL, V174, P215, DOI 10.1016/j.artint.2009.11.011	24	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400083
C	Musco, C; Woodruff, DP		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Musco, Cameron; Woodruff, David P.			Is Input Sparsity Time Possible for Kernel Low-Rank Approximation?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NYSTROM METHOD; MATRIX	Low-rank approximation is a common tool used to accelerate kernel methods: the n x n kernel matrix K is approximated via a rank-k matrix (K) over tilde which can be stored in much less space and processed more quickly. In this work we study the limits of computationally efficient low-rank kernel approximation. We show that for a broad class of kernels, including the popular Gaussian and polynomial kernels, computing a relative error k-rank approximation to K is at least as difficult as multiplying the input data matrix A is an element of R-nxd by an arbitrary matrix C is an element of R-dxk. Barring a breakthrough in fast matrix multiplication, when k is not too large, this requires Omega(nnz(A)k) time where nnz(A) is the number of non-zeros in A. This lower bound matches, in many parameter regimes, recent work on subquadratic time algorithms for low-rank approximation of general kernels [MM16, MW17], demonstrating that these algorithms are unlikely to be significantly improved, in particular to O(nnz(A)) input sparsity runtimes. At the same time there is hope: we show for the first time that O(nnz(A)) time approximation is possible for general radial basis function kernels (e.g., the Gaussian kernel) for the closely related problem of low-rank approximation of the kernelized dataset.	[Musco, Cameron] MIT, Cambridge, MA 02139 USA; [Woodruff, David P.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Massachusetts Institute of Technology (MIT); Carnegie Mellon University	Musco, C (corresponding author), MIT, Cambridge, MA 02139 USA.	cnmusco@mit.edu; dwoodruf@cs.cmu.edu	Jeong, Yongwook/N-7413-2016					Achlioptas Dimitris, 2001, ADV NEURAL INFORM PR; Alaoui A., 2015, P 28 INT C NEURAL IN, P775; Avron H, 2014, ADV NEUR IN, V27; Avron H, 2017, PR MACH LEARN RES, V70; BACH FR, 2002, J MACHINE LEARNING R, V3, P1; Backurs Arturs, 2017, ADV NEURAL INFORM PR; Belabbas MA, 2009, P NATL ACAD SCI USA, V106, P369, DOI 10.1073/pnas.0810600105; Clarkson KL, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2061; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Cohen MB, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1758; Cohen MB, 2015, ACM S THEORY COMPUT, P163, DOI 10.1145/2746539.2746569; Cotter A., 2011, ARXIV11094603; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619; Friedland S, 2007, SIAM J MATRIX ANAL A, V29, P656, DOI 10.1137/06065551; Gittens A., 2013, INT C MACHINE LEARNI, P567; Hedenfalk I, 2001, NEW ENGL J MED, V344, P539, DOI 10.1056/NEJM200102223440801; Javed A, 2011, ANN HUM GENET, V75, P707, DOI 10.1111/j.1469-1809.2011.00673.x; Kapralov M, 2016, PR MACH LEARN RES, V48; Le Gall F, 2012, ANN IEEE SYMP FOUND, P514, DOI 10.1109/FOCS.2012.80; Le Gall Francois, 2017, ARXIV170805622; Le Quoc V., 2013, INT C MACH LEARN, P244; Legall Francois, 2014, P 39 INT S SYMBOLIC, P296, DOI [DOI 10.1145/2608628.2608664, 10.1145/2608628.2608664]; Musco Cameron, 2017, P 58 ANN IEEE S FDN; Musco Cameron, 2016, ADV NEURAL INFORM PR; Nelson J, 2013, ANN IEEE SYMP FOUND, P117, DOI 10.1109/FOCS.2013.21; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Smola A. J., 2000, P 17 INT C MACH LEAR, P911; Wang SS, 2013, J MACH LEARN RES, V14, P2729; Williams CKI, 2001, ADV NEUR IN, V13, P682; Zhang K., 2008, P 25 INT C MACHINE L, P1232	31	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404049
C	Noh, YK; Sugiyama, M; Kim, KE; Park, FC; Lee, DD		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Noh, Yung-Kyun; Sugiyama, Masashi; Kim, Kee-Eung; Park, Frank C.; Lee, Daniel D.			Generative Local Metric Learning for Kernel Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DENSITY-FUNCTION	DThis paper shows how metric learning can be used with Nadaraya-Watson (NW) kernel regression. Compared with standard approaches, such as bandwidth selection, we show how metric learning can significantly reduce the mean square error (MSE) in kernel regression, particularly for high-dimensional data. We propose a method for efficiently learning a good metric function based upon analyzing the performance of the NW estimator for Gaussian-distributed data. A key feature of our approach is that the NW estimator with a learned metric uses information from both the global and local structure of the training data. Theoretical and empirical results confirm that the learned metric can considerably reduce the bias and MSE for kernel regression even when the data are not confined to Gaussian.	[Noh, Yung-Kyun; Park, Frank C.] Seoul Natl Univ, Seoul, South Korea; [Sugiyama, Masashi] Univ Tokyo, RIKEN, Tokyo, Japan; [Kim, Kee-Eung] Korea Adv Inst Sci & Technol, Daejeon, South Korea; [Lee, Daniel D.] Univ Penn, Philadelphia, PA 19104 USA	Seoul National University (SNU); RIKEN; University of Tokyo; Korea Advanced Institute of Science & Technology (KAIST); University of Pennsylvania	Noh, YK (corresponding author), Seoul Natl Univ, Seoul, South Korea.	nohyung@snu.ac.kr; sugi@k.u-tokyo.ac.jp; kekim@cs.kaist.ac.kr; fcp@snu.ac.kr; ddlee@seas.upenn.edu	Sugiyama, Masashi/AEO-1176-2022	Sugiyama, Masashi/0000-0001-6658-6743	BK21Plus in Korea; KAKENHI in Japan [17H01760]; IITP/MSIT in Korea [2017-0-01778]; BK21Plus in Korea [MITIP-10048320]; NSF in US; ONR in US; ARL in US; AFOSR in US; DOT in US; DARPA in US;  [NRF/MSIT-2017R1E1A1A03070945]	BK21Plus in Korea; KAKENHI in Japan(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); IITP/MSIT in Korea; BK21Plus in Korea; NSF in US; ONR in US; ARL in US; AFOSR in US; DOT in US; DARPA in US; 	YKN acknowledges support from NRF/MSIT-2017R1E1A1A03070945, BK21Plus in Korea, MS from KAKENHI 17H01760 in Japan, KEK from IITP/MSIT 2017-0-01778 in Korea, FCP from BK21Plus, MITIP-10048320 in Korea, and DDL from the NSF, ONR, ARL, AFOSR, DOT, DARPA in US.	Alcala-Fdez J, 2011, J MULT-VALUED LOG S, V17, P255; Atkeson CG, 1997, ARTIF INTELL REV, V11, P11, DOI 10.1023/A:1006559212014; Bellet A, 2013, ABS13066709 CORR; CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568; Choi E, 2000, ANN STAT, V28, P1339; FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330; Goldberger Jacob, 2005, ADV NEURAL INFORM PR, V17, P8, DOI DOI 10.1109/TCSVT.2013.2242640; HALL P, 1991, BIOMETRIKA, V78, P263; Hastie TJ, 2001, ELEMENTS STAT LEARNI; Haykin S., 2010, NEURAL NETWORKS LEAR; Huang RQ, 2013, J INTELL FUZZY SYST, V24, P775, DOI 10.3233/IFS-2012-0597; Keller PW, 2006, P 23 INT C MACH LEAR, P449; Kpotufe S., 2016, J MACHINE LEARNING R, V17, P1; Lazaro-Gredilla M, 2010, IEEE T NEURAL NETWOR, V21, P1345, DOI 10.1109/TNN.2010.2049859; Nadaraya E.A., 1964, THEOR PROBAB APPL+, V9, P141, DOI [DOI 10.1137/1109020, 10.1137/1109020]; Nguyen B, 2016, NEUROCOMPUTING, V214, P805, DOI 10.1016/j.neucom.2016.07.005; Noh YK, 2018, IEEE T PATTERN ANAL, V40, P106, DOI 10.1109/TPAMI.2017.2666151; Nosofsky RM, 1997, PSYCHOL REV, V104, P266, DOI 10.1037/0033-295X.104.2.266; Park B. U., 1992, Computational Statistics, V7, P251; PARK BU, 1990, J AM STAT ASSOC, V85, P66, DOI 10.2307/2289526; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; RUPPERT D, 1994, ANN STAT, V22, P1346, DOI 10.1214/aos/1176325632; SCHUCANY WR, 1977, J AM STAT ASSOC, V72, P420, DOI 10.2307/2286810; Shi L, 2010, PSYCHON B REV, V17, P443, DOI 10.3758/PBR.17.4.443; Watson G.S., 1964, SANKHYA SER A, V26, P359, DOI DOI 10.2307/25049340; Weinberger Kilian Q, 2006, ADV NEURAL INFORM PR, P1473, DOI DOI 10.1007/978-3-319-13168-9_; Weinberger KQ, 2007, P 11 INT C ARTIFICIA, P612	28	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402049
C	Nonnenmacher, M; Turaga, SC; Macke, JH		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Nonnenmacher, Marcel; Turaga, Srinivas C.; Macke, Jakob H.			Extracting low-dimensional dynamics from multiple large-scale neural population recordings by learning to predict correlations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					A powerful approach for understanding neural population dynamics is to extract low-dimensional trajectories from population recordings using dimensionality reduction methods. Current approaches for dimensionality reduction on neural data are limited to single population recordings, and can not identify dynamics embedded across multiple measurements. We propose an approach for extracting low-dimensional dynamics from multiple, sequential recordings. Our algorithm scales to data comprising millions of observed dimensions, making it possible to access dynamics distributed across large populations or multiple brain areas. Building on subspace-identification approaches for dynamical systems, we perform parameter estimation by minimizing a moment-matching objective using a scalable stochastic gradient descent algorithm: The model is optimized to predict temporal covariations across neurons and across time. We show how this approach naturally handles missing data and multiple partial recordings, and can identify dynamics and predict correlations even in the presence of severe subsampling and small overlap between recordings. We demonstrate the effectiveness of the approach both on simulated data and a whole-brain larval zebrafish imaging dataset.	[Nonnenmacher, Marcel; Macke, Jakob H.] Max Planck Gesell, Res Ctr Caesar, Bonn, Germany; [Turaga, Srinivas C.] HHMI Janelia Res Campus, Ashburn, VA USA; [Macke, Jakob H.] Tech Univ Darmstadt, Ctr Cognit Sci, Darmstadt, Germany	Center of Advanced European Studies & Research (CAESAR); Max Planck Society; Technical University of Darmstadt	Nonnenmacher, M (corresponding author), Max Planck Gesell, Res Ctr Caesar, Bonn, Germany.	marcel.nonnenmacher@caesar.de; turagas@janelia.hhmi.org; jakob.macke@caesar.de	Jeong, Yongwook/N-7413-2016		caesar foundation	caesar foundation	We thank M. Ahrens for the larval zebrafish data. Our work was supported by the caesar foundation.	Ahrens MB, 2013, NAT METHODS, V10, P413, DOI [10.1038/NMETH.2434, 10.1038/nmeth.2434]; Aoki M., 2013, STATE SPACE MODELING; Balzano L., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P704, DOI 10.1109/ALLERTON.2010.5706976; Bishop W. E., 2014, ADV NEURAL INF PROCE, P2762; Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005; Briggman KL, 2005, SCIENCE, V307, P896, DOI 10.1126/science.1103736; Buesing L., 2014, ADV NEURAL INFORM PR, P3500; Buesing Lars, 2012, ADV NEURAL INFORM PR, P1682; Buonomano DV, 2009, NAT REV NEUROSCI, V10, P113, DOI 10.1038/nrn2558; Churchland MM, 2012, NATURE, V487, P51, DOI 10.1038/nature11129; Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Dhawale AK, 2017, ELIFE, V6, DOI 10.7554/eLife.27702; Gao PR, 2015, CURR OPIN NEUROBIOL, V32, P148, DOI 10.1016/j.conb.2015.04.003; Gao Y., 2015, ADV NEURAL INFORM PR, V28, P2044; Ghahramani Z., 1996, CRGTR962 U TOTR DEP; He J., 2011, IT 2011; Huys QJM, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000379; Katayama T., 2006, SUBSPACE METHODS SYS; Kingma D.P, P 3 INT C LEARNING R; Li N, 2016, NATURE, V532, P459, DOI 10.1038/nature17643; Liu Z, 2013, SYST CONTROL LETT, V62, P605, DOI 10.1016/j.sysconle.2013.04.005; Macke Jakob H, 2012, ADV NEURAL INFORM PR, V24, P1350; Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742; Markovsky I., 2016, SYSTEMS CONTROL LETT; Markovsky I., 2016, IEEE T AUTOMATIC CON; Mazor O, 2005, NEURON, V48, P661, DOI 10.1016/j.neuron.2005.09.032; Paninski L., 2013, ADV NEURAL INF PROCE, V26, P2391; Pnevmatikakis EA, 2014, J COMPUT GRAPH STAT, V23, P316, DOI 10.1080/10618600.2012.760461; Shenoy KV, 2013, ANNU REV NEUROSCI, V36, P337, DOI 10.1146/annurev-neuro-062111-150509; Sofroniew NJ, 2016, ELIFE, V5, DOI 10.7554/eLife.14472; Soudry D, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004464; Turaga S., 2013, ADV NEURAL INF PROCE, P539; Van Overschee P., 2012, THEORY IMPLEMENTATIO; Yu Byron M, 2009, ADV NEURAL INFORM PR, P1881, DOI DOI 10.1152/JN.90941	35	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405076
C	Perrault-Joncas, DC; Meila, M; McQueen, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Perrault-Joncas, Dominique C.; Meila, Marina; McQueen, James			Improved Graph Laplacian via Geometric Consistency	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CONVERGENCE; REDUCTION	In all manifold learning algorithms and tasks setting the kernel bandwidth epsilon used construct the graph Laplacian is critical. We address this problem by choosing a quality criterion for the Laplacian, that measures its ability to preserve the geometry of the data. For this, we exploit the connection between manifold geometry, represented by the Riemannian metric, and the Laplace-Beltrami operator. Experiments show that this principled approach is effective and robust.	[Perrault-Joncas, Dominique C.] Google Inc, Mountain View, CA 94043 USA; [Meila, Marina] Univ Washington, Dept Stat, Seattle, WA 98195 USA; [McQueen, James] Amazon, Seattle, WA USA	Google Incorporated; University of Washington; University of Washington Seattle; Amazon.com	Perrault-Joncas, DC (corresponding author), Google Inc, Mountain View, CA 94043 USA.	dominiquep@google.com; mmp2@uw.edu; jmcq@amazon.com						[Anonymous], 2012, METRIC LEARNIN UNPUB; Aswani A, 2011, ANN STAT, V39, P48, DOI 10.1214/10-AOS823; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Belkin M., 2007, NIPS PROCESSING, V19, P129; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Berry T., 2016, CONSISTENT MANIFOLD; Carter KM, 2007, 2007 IEEE/SP 14TH WORKSHOP ON STATISTICAL SIGNAL PROCESSING, VOLS 1 AND 2, P601, DOI 10.1109/SSP.2007.4301329; Chapelle O., 2006, IEEE T NEURAL NETW, V20, P542; Chazal F., 2016, ADV NEURAL INFORM PR, P3963; Chen GL, 2011, APPL NUMER HARMON AN, P199, DOI 10.1007/978-0-8176-8095-4_10; Chen LS, 2009, J AM STAT ASSOC, V104, P209, DOI 10.1198/jasa.2009.0111; Coifman R. R., 2006, APPL COMPUTATIONAL H, V21, p[6, 1]; Dasgupta S, 2008, ACM S THEORY COMPUT, P537; Gine E., 2006, HIGH DIMENSIONAL PRO, V51, P238; Goldberg Y, 2008, J MACH LEARN RES, V9, P1909; Hein M, 2007, J MACH LEARN RES, V8, P1325; Lee JA, 2007, INFORM SCI STAT, P1; Lee John M, 2018, INTRO RIEMANNIAN MAN; Levina E., 2005, ADV NIPS, V17; Rosenberg S., 1997, LONDON MATH SOC STUD, DOI DOI 10.1017/CBO9780511623783; Saul LK, 2004, J MACH LEARN RES, V4, P119, DOI 10.1162/153244304322972667; Sindhwani V., 2007, P INT JOINT C ART IN; Singer A, 2006, APPL COMPUT HARMON A, V21, P128, DOI 10.1016/j.acha.2006.03.004; Smola A., 2003, P ANN C COMP LEARN T; Ting D., 2010, P ICML; von Luxburg U, 2008, ANN STAT, V36, P555, DOI 10.1214/009053607000000640; Wang Xu, 2015, SPECTRAL CONVERGENCE; Yue HH, 2004, IEEE DECIS CONTR P, P4262, DOI 10.1109/CDC.2004.1429421; Zhou X., 2011, AISTAT; Zhu X., 2003, TECHNICAL REPORT	30	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404051
C	Ramdas, A; Yang, F; Wainwright, MJ; Jordan, MI		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ramdas, Aaditya; Yang, Fanny; Wainwright, Martin J.; Jordan, Michael, I			Online control of the false discovery rate with decaying memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In the online multiple testing problem, p-values corresponding to different null hypotheses are observed one by one, and the decision of whether or not to reject the current hypothesis must be made immediately, after which the next p-value is observed. Alpha-investing algorithms to control the false discovery rate (FDR), formulated by Foster and Stine, have been generalized and applied to many settings, including quality-preserving databases in science and multiple A/B or multi-armed bandit tests for internet commerce. This paper improves the class of generalized alpha-investing algorithms (GAI) in four ways: (a) we show how to uniformly improve the power of the entire class of monotone GAI procedures by awarding more alpha-wealth for each rejection, giving a win-win resolution to a recent dilemma raised by Javanmard and Montanari, (b) we demonstrate how to incorporate prior weights to indicate domain knowledge of which hypotheses are likely to be non-null, (c) we allow for differing penalties for false discoveries to indicate that some hypotheses may be more important than others, (d) we define a new quantity called the decaying memory false discovery rate (mem-FDR) that may be more meaningful for truly temporal applications, and which alleviates problems that we describe and refer to as "piggybacking" and "alpha-death." Our GAI++ algorithms incorporate all four generalizations simultaneously, and reduce to more powerful variants of earlier algorithms when the weights and decay are all set to unity. Finally, we also describe a simple method to derive new online FDR rules based on an estimated false discovery proportion.	[Ramdas, Aaditya; Yang, Fanny; Wainwright, Martin J.; Jordan, Michael, I] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Ramdas, A (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	aramdas@berkeley.edu; fanny-yang@berkeley.edu; wainwrig@berkeley.edu; jordan@berkeley.edu	Jordan, Michael I/C-5253-2013; Jeong, Yongwook/N-7413-2016	Jordan, Michael/0000-0001-8935-817X	Army Research Office [W911NF-17-1-0304]; National Science Foundation grant [NSF-DMS-1612948]	Army Research Office; National Science Foundation grant(National Science Foundation (NSF))	We thank A. Javanmard, R. F. Barber, K. Johnson, E. Katsevich, W. Fithian and L. Lei for related discussions, and A. Javanmard for sharing code to reproduce experiments in Javanmard and Montanari [9]. This material is based upon work supported in part by the Army Research Office under grant number W911NF-17-1-0304, and National Science Foundation grant NSF-DMS-1612948.	Aharoni E, 2014, J R STAT SOC B, V76, P771, DOI 10.1111/rssb.12048; Barber, 2016, ARXIV160607926; Benjamini Y, 1997, SCAND J STAT, V24, P407, DOI 10.1111/1467-9469.00072; BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x; Blanchard G, 2008, ELECTRON J STAT, V2, P963, DOI 10.1214/08-EJS180; Foster DP, 2008, J R STAT SOC B, V70, P429, DOI 10.1111/j.1467-9868.2007.00643.x; Genovese CR, 2006, BIOMETRIKA, V93, P509, DOI 10.1093/biomet/93.3.509; Heesen Philipp, 2014, ARXIV14106296; Javanmard A., 2017, ANN STAT; Javanmard A., 2015, ARXIV150206197; Ramdas A., 2017, ARXIV170306222; Tukey JW., 1953, PROBLEM MULTIPLE COM; Yang Fan, 2017, ADV NEURAL INFORM PR	13	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405071
C	Song, CB; Cui, SB; Jiang, Y; Xia, ST		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Song, Chaobing; Cui, Shaobo; Jiang, Yong; Xia, Shu-Tao			Accelerated Stochastic Greedy Coordinate Descent by Soft Thresholding Projection onto Simplex	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				OPTIMIZATION	In this paper we study the well-known greedy coordinate descent (GCD) algorithm to solve l(1)-regularized problems and improve GCD by the two popular strategies: Nesterov's acceleration and stochastic optimization. Firstly, based on an l(1)-norm square approximation, we propose a new rule for greedy selection which is non-trivial to solve but convex; then an efficient algorithm called "SOft ThreshOlding PrOjection (SOTOPO)" is proposed to exactly solve an l(1)-regularized l(1)-norm square approximation problem, which is induced by the new rule. Based on the new rule and the SOTOPO algorithm, the Nesterov's acceleration and stochastic optimization strategies are then successfully applied to the GCD algorithm. The resulted algorithm called accelerated stochastic greedy coordinate descent (ASGCD) has the optimal convergence rate O(root 1/epsilon); meanwhile, it reduces the iteration complexity of greedy selection up to a factor of sample size. Both theoretically and empirically, we show that ASGCD has better performance for high-dimensional and dense problems with sparse solutions.	[Song, Chaobing; Cui, Shaobo; Jiang, Yong; Xia, Shu-Tao] Tsinghua Univ, Beijing, Peoples R China	Tsinghua University	Song, CB (corresponding author), Tsinghua Univ, Beijing, Peoples R China.	songcb16@mails.tsinghua.edu.cn; cuishaobo16@mails.tsinghua.edu.cn; jiangy@sz.tsinghua.edu.cn; xiast@sz.tsinghua.edu.cn	Jeong, Yongwook/N-7413-2016		National Natural Science Foundation of China [61771273, 61371078]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is supported by the National Natural Science Foundation of China under grant Nos. 61771273, 61371078.	Allen- Zhu Zeyuan, 2014, ARXIV E PRINTS; Allen- Zhu Zeyuan, 2016, ARXIV E PRINTS; Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Boyd S, 2004, CONVEX OPTIMIZATION; CHANG CC, 2000, LIBSVM INTRO BENCHMA; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Dhillon I. S., 2011, ADV NEURAL INFORM PR, V24, P2160; Duchi J., 2008, PROC 25 INT C MACH L, P272; Duchi J. C., 2010, COLT, P14; Fercoq O, 2015, SIAM J OPTIMIZ, V25, P1997, DOI 10.1137/130949993; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Lin Q., 2014, ADV NEURAL INFORM PR, P3059; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nutini J, 2015, PR MACH LEARN RES, V37, P1632; Shalev-Shwartz S, 2006, J MACH LEARN RES, V7, P1567; Shalev-Shwartz S, 2014, PR MACH LEARN RES, V32; Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865; Tseng P, 2009, MATH PROGRAM, V117, P387, DOI 10.1007/s10107-007-0170-0; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zhang Yuchen, 2015, P 32 INT C MACH LEAR, V951, P2015	23	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404088
C	Song, L; Vempala, S; Wilmes, J; Xie, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Song, Le; Vempala, Santosh; Wilmes, John; Xie, Bo			On the Complexity of Learning Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The stunning empirical successes of neural networks currently lack rigorous theoretical explanation. What form would such an explanation take, in the face of existing complexity-theoretic lower bounds? A first step might be to show that data generated by neural networks with a single hidden layer, smooth activation functions and benign input distributions can be learned efficiently. We demonstrate here a comprehensive lower bound ruling out this possibility: for a wide class of activation functions (including all currently used), and inputs drawn from any logconcave distribution, there is a family of one-hidden-layer functions whose output is a sum gate, that are hard to learn in a precise sense: any statistical query algorithm (which includes all known variants of stochastic gradient descent with any loss function) needs an exponential number of queries even using tolerance inversely proportional to the input dimensionality. Moreover, this hard family of functions is realizable with a small (sublinear in dimension) number of activation units in the single hidden layer. The lower bound is also robust to small perturbations of the true weights. Systematic experiments illustrate a phase transition in the training error as predicted by the analysis.	[Song, Le; Vempala, Santosh; Wilmes, John; Xie, Bo] Georgia Inst Technol, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Song, L (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	lsong@cc.gatech.edu; vempala@gatech.edu; wilmesj@gatech.edu; bo.xie@gatech.edu	Jeong, Yongwook/N-7413-2016		NSF [CCF-1563838, CCF-1717349]	NSF(National Science Foundation (NSF))	The authors are grateful to Vitaly Feldman for discussions about statistical query lower bounds, and for suggestions that simplified the presentation of our results, and also to Adam Kalai for an inspiring discussion. This research was supported in part by NSF grants CCF-1563838 and CCF-1717349.	Andoni A, 2014, PR MACH LEARN RES, V32, P1908; BALAZS S, 2009, ALT, V5809, P186; BLUM AL, 1992, NEURAL NETWORKS, V5, P117, DOI 10.1016/S0893-6080(05)80010-3; Brutzkus A., 2017, CORR; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Eldan R., 2016, P 29 C LEARNING THEO, V49, P907; Feldman V, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P655; Goel Surbhi, 2016, CORR; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Kearns M., 1993, Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, P392, DOI 10.1145/167088.167200; Kearns M, 1998, J ACM, V45, P983, DOI 10.1145/293347.293351; Klivans Adam R., 2016, ENCY ALGORITHMS, P475; Majid Janzamin, 2015, CORR; Shalev-Shwartz S., 2016, C LEARNING THEORY, P815; Shalev-Shwartz S., 2017, CORR; Shamir Ohad, 2016, CORR; Song Le, 2017, ARXIV170704615; Telgarsky M., 2016, COLT	20	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405058
C	Syrgkanis, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Syrgkanis, Vasilis			A Sample Complexity Measure with Applications to Learning Optimal Auctions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We introduce a new sample complexity measure, which we refer to as split-sample growth rate. For any hypothesis H and for any sample S of size m, the split-sample growth rate (T) over cap (H) (m) counts how many different hypotheses can empirical risk minimization output on any sub-sample of S of size m/2. We show that the expected generalization error is upper bounded by O (root log((T) over bar (H)(2m))/m). Our result is enabled by a strengthening of the Rademacher complexity analysis of the expected generalization error. We show that this sample complexity measure, greatly simplifies the analysis of the sample complexity of optimal auction design, for many auction classes studied in the literature. Their sample complexity can be derived solely by noticing that in these auction classes, ERM on any sample or sub-sample will pick parameters that are equal to one of the points in the sample.	[Syrgkanis, Vasilis] Microsoft Res, Redmond, WA 98052 USA	Microsoft	Syrgkanis, V (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	vasy@microsoft.com	Jeong, Yongwook/N-7413-2016					Balcan Maria-Florina F, 2016, P 29 C NEURAL INFORM, P2083; Balcan MF, 2005, ANN IEEE SYMP FOUND, P605, DOI 10.1109/SFCS.2005.50; Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282; Cole R, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P243, DOI 10.1145/2591796.2591867; Devanur NR, 2016, ACM S THEORY COMPUT, P426, DOI 10.1145/2897518.2897553; Gonczarowski Yannai A., 2016, CORR; Hartline JD, 2009, 10TH ACM CONFERENCE ON ELECTRONIC COMMERCE - EC 2009, P225; Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914; Morgenstern J, 2015, P 28 INT C NEUR INF, V1, P136; Morgenstern Jamie, 2016, COLT 2016; MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58; Pollard D., 2011, SPRINGER SERIES STAT; Roughgarden T, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P1, DOI 10.1145/2940716.2940723; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019	14	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405042
C	Varatharajah, Y; Chong, MJ; Saboo, K; Berry, B; Brinkmann, B; Worrell, G; Iyer, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Varatharajah, Yogatheesan; Chong, Min Jin; Saboo, Krishnakant; Berry, Brent; Brinkmann, Benjamin; Worrell, Gregory; Iyer, Ravishankar			EEG-GRAPH: A Factor-Graph-Based Model for Capturing Spatial, Temporal, and Observational Relationships in Electroencephalograms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper presents a probabilistic-graphical model that can be used to infer characteristics of instantaneous brain activity by jointly analyzing spatial and temporal dependencies observed in electroencephalograms (EEG). Specifically, we describe a factor-graph-based model with customized factor-functions defined based on domain knowledge, to infer pathologic brain activity with the goal of identifying seizure-generating brain regions in epilepsy patients. We utilize an inference technique based on the graph-cut algorithm to exactly solve graph inference in polynomial time. We validate the model by using clinically collected intracranial EEG data from 29 epilepsy patients to show that the model correctly identifies seizure-generating brain regions. Our results indicate that our model outperforms two conventional approaches used for seizure-onset localization (5-7% better AUC: 0.72, 0.67, 0.65) and that the proposed inference technique provides 3-10% gain in AUC (0.72, 0.62, 0.69) compared to sampling-based alternatives.	[Varatharajah, Yogatheesan; Chong, Min Jin; Saboo, Krishnakant; Iyer, Ravishankar] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA; [Berry, Brent; Brinkmann, Benjamin; Worrell, Gregory] Mayo Clin, Dept Neurol, Rochester, MN 55904 USA	University of Illinois System; University of Illinois Urbana-Champaign; Mayo Clinic	Varatharajah, Y (corresponding author), Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.	varatha2@illinois.edu; mchong6@illinois.edu; ksaboo2@illinois.edu; Berry.Brent@mayo.edu; Brinkmann.Benjamin@mayo.edu; Worrell.Gregory@mayo.edu; rkiyer@illinois.edu	Jeong, Yongwook/N-7413-2016	Brinkmann, Benjamin/0000-0002-2392-8608	National Science Foundation [CNS-1337732, CNS-1624790]; National Institute of Health [NINDS-U01-NS073557, NINDS-R01-NS92882, NHLBI-HL105355, NINDS-UH2-NS095495-01]; Mayo Clinic and Illinois Alliance Fellowships for Technology-based Healthcare Research; IBM faculty award	National Science Foundation(National Science Foundation (NSF)); National Institute of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Mayo Clinic and Illinois Alliance Fellowships for Technology-based Healthcare Research; IBM faculty award(International Business Machines (IBM))	This work was partly supported by National Science Foundation grants CNS-1337732 and CNS-1624790, National Institute of Health grants NINDS-U01-NS073557, NINDS-R01-NS92882, NHLBI-HL105355, and NINDS-UH2-NS095495-01, Mayo Clinic and Illinois Alliance Fellowships for Technology-based Healthcare Research and an IBM faculty award. We thank Subho Banerjee, Phuong Cao, Jenny Applequist, and the reviewers for their valuable feedback.	Alvarado-Rojas C, 2014, SCI REP-UK, V4, DOI 10.1038/srep04545; Andersen L. R., 1991, J APPL STAT, V18, P139; Andrzejak RG, 2009, CLIN NEUROPHYSIOL, V120, P1465, DOI 10.1016/j.clinph.2009.05.019; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Cao P., 2015, P 2015 S BOOTC SCI S, P1; CHIB S, 1995, AM STAT, V49, P327, DOI 10.2307/2684568; DAGUM P, 1992, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P41; Delong A, 2009, IEEE I CONF COMP VIS, P285, DOI 10.1109/ICCV.2009.5459263; Esquenazi Y, 2014, EPILEPSY RES, V108, P547, DOI 10.1016/j.eplepsyres.2014.01.009; Frey B. J., 1997, PROC ANN ALLERTON C, P666; Gilks W. R., 1995, MARKOV CHAIN MONTE C, DOI 10.1201/b14835; Katznelson R.D, 1981, ELECTRIC FIELDS BRAI, P176; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Lee RW, 2014, J CLIN NEUROPHYSIOL, V31, P199, DOI 10.1097/WNP.0000000000000047; Liu Jie, 2012, JMLR Workshop Conf Proc, V22, P712; Liu S, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/2/026026; Martinez-Vargas JD, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00156; Rubinov M, 2010, NEUROIMAGE, V52, P1059, DOI 10.1016/j.neuroimage.2009.10.003; Stead M, 2010, BRAIN, V133, P2789, DOI 10.1093/brain/awq190; Varatharajah Y, 2017, I IEEE EMBS C NEUR E, P533, DOI 10.1109/NER.2017.8008407; Varatharajah Y, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065716500465; Warren CP, 2010, J NEUROPHYSIOL, V104, P3530, DOI 10.1152/jn.00368.2010; Weiss Y, 2001, IEEE T INFORM THEORY, V47, P736, DOI 10.1109/18.910585; Wetjen NM, 2009, J NEUROSURG, V110, P1147, DOI 10.3171/2008.8.JNS17643; Wiegerinck W, 2000, P 16 C UNC ART INT S, V16, P626; Worrell GA, 2008, BRAIN, V131, P928, DOI 10.1093/brain/awn006; Yedidia JS, 2000, ADV NEURAL INFORM PR, V13, P689; Zhang Y, 2010, PROC INT CONF DATA, P1157, DOI 10.1109/ICDE.2010.5447819	28	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405044
C	Wang, C; Lu, YM		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wang, Chuang; Lu, Yue M.			The Scaling Limit of High-Dimensional Online Independent Component Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DYNAMICS	We analyze the dynamics of an online algorithm for independent component analysis in the high-dimensional scaling limit As the ambient dimension tends to infinity, and with proper time scaling, we show that the time-varying joint empirical measure of the target feature vector and the estimates provided by the algorithm will converge weakly to a deterministic measured-valued process that can be characterized as the unique solution of a nonlinear PDE. Numerical solutions of this PDE, which involves two spatial variables and one time variable, can be efficiently obtained. These solutions provide detailed information about the performance of the ICA algorithm, as many practical performance metrics are functionals of the joint empirical measures. Numerical simulations show that our asymptotic analysis is accurate even for moderate dimensions. In addition to providing a tool for understanding the performance of the algorithm, our PDE analysis also provides useful insight. In particular, in the high-dimensional limit, the original coupled dynamics associated with the algorithm will be asymptotically "decoupled", with each coordinate independently solving a 1-D effective minimization problem via stochastic gradient descent. Exploiting this insight to design new algorithms for achieving optimal trade-offs between computational and statistical efficiency may prove an interesting line of future research.	[Wang, Chuang; Lu, Yue M.] Harvard Univ, John A Paulson Sch Engn & Appl Sci, 33 Oxford St, Cambridge, MA 02138 USA	Harvard University	Wang, C (corresponding author), Harvard Univ, John A Paulson Sch Engn & Appl Sci, 33 Oxford St, Cambridge, MA 02138 USA.	chuangwang@seas.harvard.edu; yuelu@seas.harvard.edu	Jeong, Yongwook/N-7413-2016		US Army Research Office [W911NF-16-1-0265]; US National Science Foundation [CCF-1319140, CCF-1718698]	US Army Research Office; US National Science Foundation(National Science Foundation (NSF))	This work is supported by US Army Research Office under contract W911NF-16-1-0265 and by the US National Science Foundation under grants CCF-1319140 and CCF-1718698.	Barbier J., 2016, ADV NEURAL INFORM PR, V29, P424; Basalyga G, 2004, J MACH LEARN RES, V4, P1393; Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817; BIEHL M, 1994, EUROPHYS LETT, V25, P391, DOI 10.1209/0295-5075/25/5/014; Biehl Michael, 1998, J PHYS A, V31, P97; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Chuang Wang, 2016, 2016 IEEE Information Theory Workshop (ITW), P186, DOI 10.1109/ITW.2016.7606821; CUGLIANDOLO LF, 1994, J PHYS A-MATH GEN, V27, P5749, DOI 10.1088/0305-4470/27/17/011; Donoho D, 2016, PROBAB THEORY REL, V166, P935, DOI 10.1007/s00440-015-0675-z; Ge R., 2015, JORNAL MACHINE LEARN, P1; Haykin Simon, LEAST MEAN SQUARE AD, V31; Hyvarinen A, 1997, ADV NEUR IN, V9, P480; Jin C, 2017, PR MACH LEARN RES, V70; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Li C J, 2016, ADV NEURAL INF PROCE, P4961; Li X., 2016, ARXIV160604933; McKean H.P., 1967, STOCHASTIC DIFFERENT, P41; MCKEAN HP, 1966, P NATL ACAD SCI USA, V56, P1907, DOI 10.1073/pnas.56.6.1907; MELEARD S, 1987, STOCH PROC APPL, V26, P317, DOI 10.1016/0304-4149(87)90184-0; Mitliagkas Ioannis, 2013, ADV NEURAL INF PROCE; Netrapalli P., 2013, P ADV NEUR INF PROC, P2796; OJA E, 1985, J MATH ANAL APPL, V106, P69, DOI 10.1016/0022-247X(85)90131-3; Rattray M, 2002, ADV NEUR IN, V14, P495; Rattray M, 2002, NEURAL COMPUT, V14, P421, DOI 10.1162/08997660252741185; Roberts GO, 1997, ANN APPL PROBAB, V7, P110, DOI 10.1214/aoap/1034625254; Saad D, 1997, PHYS REV LETT, V79, P2578, DOI 10.1103/PhysRevLett.79.2578; SAAD D, 1995, PHYS REV LETT, V74, P4337, DOI 10.1103/PhysRevLett.74.4337; Shamir O., 2013, INT C MACH LEARN, P71; Sznitman Alain-Sol, 1991, PROBAB SAINT FLOUR 1, P165	30	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406068
C	Welleck, S; Mao, JL; Cho, K; Zhang, Z		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Welleck, Sean; Mao, Jialin; Cho, Kyunghyun; Zhang, Zheng			Saliency-based Sequential Image Attention with Multiset Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				VISUAL-ATTENTION	Humans process visual scenes selectively and sequentially using attention. Central to models of human visual attention is the saliency map. We propose a hierarchical visual architecture that operates on a saliency map and uses a novel attention mechanism to sequentially focus on salient regions and take additional glimpses within those regions. The architecture is motivated by human visual attention, and is used for multi-label image classification on a novel multiset task, demonstrating that it achieves high precision and recall while localizing objects with its attention. Unlike conventional multi-label image classification models, the model supports multiset prediction due to a reinforcement-learning based training process that allows for arbitrary label permutation and multiple instances per label.	[Welleck, Sean; Mao, Jialin; Cho, Kyunghyun; Zhang, Zheng] NYU, New York, NY 10003 USA	New York University	Welleck, S (corresponding author), NYU, New York, NY 10003 USA.	wellecks@nyu.edu; jialin.mao@nyu.edu; kyunghyun.cho@nyu.edu; zz@nyu.edu	Jeong, Yongwook/N-7413-2016		NYU Global Seed Funding <Model-Free Object Tracking with Recurrent Neural Networks>; STCSM [17JC1404100/1]; Huawei HIPP Open 2017	NYU Global Seed Funding <Model-Free Object Tracking with Recurrent Neural Networks>; STCSM(Science & Technology Commission of Shanghai Municipality (STCSM)); Huawei HIPP Open 2017(Huawei Technologies)	This work was partly supported by the NYU Global Seed Funding <Model-Free Object Tracking with Recurrent Neural Networks>, STCSM 17JC1404100/1, and Huawei HIPP Open 2017.	Awh E, 2000, J EXP PSYCHOL HUMAN, V26, P834, DOI 10.1037/0096-1523.26.2.834; Ba J., 2014, ARXIV; Ba Jimmy, 2015, ADV NEURAL INFORM PR, P2; Bellver Miriam, 2016, DEEP REINF LEARN WOR; Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286; Carrasco M, 2011, VISION RES, V51, P1484, DOI 10.1016/j.visres.2011.04.012; Carrasco M, 2002, J VISION, V2, DOI 10.1167/2.6.4; Cavanagh P, 2005, TRENDS COGN SCI, V9, P349, DOI 10.1016/j.tics.2005.05.009; Cheung Brian, 2016, ARXIV161109430; Cornia M., 2016, CORR; Cornia Marcella, 2017, ARXIV170608474; Fecteau JH, 2006, TRENDS COGN SCI, V10, P382, DOI 10.1016/j.tics.2006.06.011; Fischer J, 2009, CURR BIOL, V19, P1356, DOI 10.1016/j.cub.2009.06.059; Graves A, 2013, ARXIV13080850; Hendricks Lisa Anne, 2016, EUR C ECCV; Hinton, 1993, ADV NEURAL INFORM PR, V5, P271; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Karayev S., 2012, ADV NEURAL INFORM PR, V25, P890; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Klein RM, 2000, TRENDS COGN SCI, V4, P138, DOI 10.1016/S1364-6613(00)01452-2; KOCH C, 1985, HUM NEUROBIOL, V4, P219; Kowler E, 2011, VISION RES, V51, P1457, DOI 10.1016/j.visres.2010.12.014; Kulkarni T. D., 2016, PROC 30 INT C NEURAL, P3675; Lamme VAF, 2000, TRENDS NEUROSCI, V23, P571, DOI 10.1016/S0166-2236(00)01657-X; Larochelle H., 2010, ADV NEURAL INFORM PR, P1243; Li Yi, 2017, P IEEE C COMP VIS PA; Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43; Liu X., 2016, ARXIV160506217; Mathe S, 2013, ADV NEURAL INFORM PR, P1923; Mathe S, 2016, PROC CVPR IEEE, P2894, DOI 10.1109/CVPR.2016.316; McMains SA, 2004, NEURON, V42, P677, DOI 10.1016/S0896-6273(04)00263-6; Mnih V, 2014, ADV NEUR IN, V27; Ng AY, READING DIGITS NATUR; Schull J, 2015, ASSETS'15: PROCEEDINGS OF THE 17TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS & ACCESSIBILITY, P1, DOI 10.1145/2700648.2809870; Semeniuta S, 2016, P IEEE C ANT MEAS AP, P1, DOI DOI 10.1109/CAMA.2016.7815752; Seo Paul Hongsuck, 2016, ARXIV160602393; Tavakoli Hamed R., 2017, ARXIV170407402; Veale R, 2017, PHILOS T R SOC B, V372, DOI 10.1098/rstb.2016.0113; Vezhnevets AS, 2017, PR MACH LEARN RES, V70; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yu F., 2016, P ICLR 2016; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319	47	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405025
C	Yan, BW; Yin, MZ; Sarkar, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yan, Bowei; Yin, Mingzhang; Sarkar, Purnamrita			Convergence of Gradient EM on Multi-component Mixture of Gaussians	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MAXIMUM-LIKELIHOOD; ALGORITHM	In this paper, we study convergence properties of the gradient variant of Expectation-Maximization algorithm [11] for Gaussian Mixture Models for arbitrary number of clusters and mixing coefficients. We derive the convergence rate depending on the mixing coefficients, minimum and maximum pairwise distances between the true centers, dimensionality and number of components; and obtain a near-optimal local contraction radius. While there have been some recent notable works that derive local convergence rates for EM in the two symmetric mixture of Gaussians, in the more general case, the derivations need structurally different and non-trivial arguments. We use recent tools from learning theory and empirical processes to achieve our theoretical results.	[Yan, Bowei; Yin, Mingzhang; Sarkar, Purnamrita] Univ Texas Austin, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Yan, BW (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	boweiy@utexas.edu; mzyin@utexas.edu; purna.sarkar@austin.utexas.edu	Yin, Mingzhang/R-5702-2018; Jeong, Yongwook/N-7413-2016		NSF [DMS 1713082]	NSF(National Science Foundation (NSF))	PS was partially supported by NSF grant DMS 1713082.	Awasthi Pranjal, 2012, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques. Proceedings 15th International Workshop, APPROX 2012 and 16th International Workshop, RANDOM 2012, P37, DOI 10.1007/978-3-642-32512-0_4; Balakrishnan S, 2017, ANN STAT, V45, P77, DOI 10.1214/16-AOS1435; Combes Richard, 2015, ARXIV151105240; Conniffe D., 1987, J ROY STAT SOC D-STA, V36, P317; Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639; Dasgupta S., 2000, P 16 C UNC ART INT, P152; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Friggstad Z, 2016, ANN IEEE SYMP FOUND, P365, DOI 10.1109/FOCS.2016.47; Jin Chi, 2016, ADV NEURAL INFORM PR, P4116; Kumar A, 2010, ANN IEEE SYMP FOUND, P299, DOI 10.1109/FOCS.2010.35; LANGE K, 1995, J ROY STAT SOC B MET, V57, P425; Ledoux M., 2013, PROBABILITY BANACH S, P86; Lu Y., 2016, ARXIV161202099; Matousek J, 2000, DISCRETE COMPUT GEOM, V24, P61, DOI 10.1007/s004540010019; Maurer A, 2016, LECT NOTES ARTIF INT, V9925, P3, DOI 10.1007/978-3-319-46379-7_1; McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148, DOI DOI 10.1017/CBO9781107359949.008; Mixon Dustin G, 2016, ARXIV160206612; Mohri M., 2018, FDN MACHINE LEARNING; Naim I., 2012, ARXIV12066427; REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034; Vempala S, 2004, J COMPUT SYST SCI, V68, P841, DOI 10.1016/j.jcss.2003.11.008; WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060; Xu Ji, 2016, ADV NEURAL INFORM PR, P2676; Xu L, 1996, NEURAL COMPUT, V8, P129, DOI 10.1162/neco.1996.8.1.129; Yan B., 2016, ADV NEURAL INFORM PR, P3090	25	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407005
C	Yurochkin, M; Guha, A; Nguyen, X		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yurochkin, Mikhail; Guha, Aritra; Nguyen, XuanLong			Conic Scan-and-Cover algorithms for nonparametric topic modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose new algorithms for topic modeling when the number of topics is unknown. Our approach relies on an analysis of the concentration of mass and angular geometry of the topic simplex, a convex polytope constructed by taking the convex hull of vertices representing the latent topics. Our algorithms are shown in practice to have accuracy comparable to a Gibbs sampler in terms of topic estimation, which requires the number of topics be given. Moreover, they are one of the fastest among several state of the art parametric techniques.(1) Statistical consistency of our estimator is established under some conditions.	[Yurochkin, Mikhail; Guha, Aritra; Nguyen, XuanLong] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Yurochkin, M (corresponding author), Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.	moonfolk@umich.edu; aritra@umich.edu; xuanlong@umich.edu	Jeong, Yongwook/N-7413-2016		Margaret and Herman Sokol Faculty Award;  [NSF CAREER DMS-1351362];  [NSF CNS-1409303]	Margaret and Herman Sokol Faculty Award; ; 	This research is supported in part by grants NSF CAREER DMS-1351362, NSF CNS-1409303, a research gift from Adobe Research and a Margaret and Herman Sokol Faculty Award.	Anandkumar  A., 2012, NIPS; Arora S., 2012, ARXIV12124777; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Hsu W., 2016, ADV NEURAL INFORM PR, P4529; Nguyen XL, 2015, BERNOULLI, V21, P618, DOI 10.3150/13-BEJ582; Pritchard JK, 2000, GENETICS, V155, P945; Tang J, 2014, PR MACH LEARN RES, V32; Teh Y. W., 2006, J AM STAT ASS, V101; Xu W., 2003, P 26 ANN INT ACM SIG, P267, DOI DOI 10.1145/860435.860485; Yurochkin M., 2016, ADV NEURAL INFORM PR, P2505	13	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403091
C	Zung, J; Tartavull, I; Lee, K; Seung, HS		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Zung, Jonathan; Tartavull, Ignacio; Lee, Kisuk; Seung, H. Sebastian			An Error Detection and Correction Framework for Connectomics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We define and study error detection and correction tasks that are useful for 3D reconstruction of neurons from electron microscopic imagery, and for image segmentation more generally. Both tasks take as input the raw image and a binary mask representing a candidate object. For the error detection task, the desired output is a map of split and merge errors in the object. For the error correction task, the desired output is the true object. We call this object mask pruning, because the candidate object mask is assumed to be a superset of the true object. We train multiscale 3D convolutional networks to perform both tasks. We find that the error-detecting net can achieve high accuracy. The accuracy of the error-correcting net is enhanced if its input object mask is "advice" (union of erroneous objects) from the error-detecting net.	[Zung, Jonathan; Tartavull, Ignacio; Lee, Kisuk; Seung, H. Sebastian] Princeton Univ, Princeton, NJ 08544 USA; [Lee, Kisuk] MIT, Cambridge, MA 02139 USA	Princeton University; Massachusetts Institute of Technology (MIT)	Zung, J (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	jzung@princeton.edu; tartavull@princeton.edu; kisukiee@mit.edu; sseung@princeton.edu	Lee, Kisuk/AAY-7118-2021; Jeong, Yongwook/N-7413-2016		Mathers Foundation; Samsung Scholarship; Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC) [D16PC0005]; Amazon	Mathers Foundation; Samsung Scholarship(Samsung); Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC); Amazon	We acknowledge NVIDIA Corporation for providing us with early access to Titan X Pascal GPU used in this research, and Amazon for assistance through an AWS Research Grant. This research was supported by the Mathers Foundation, the Samsung Scholarship and the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC) contract number D16PC0005. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.	Beier T, 2017, NAT METHODS, V14, P101, DOI 10.1038/nmeth.4151; Berger Daniel, VAST LITE; De Brabandere Bert, 2017, CORR; Fathi A., 2017, CORR; Fourure D., 2017, CORR; Funke J., 2017, ARXIV170902974; Harley A. W., 2015, CORR; He K., 2015, CORR; Huang Gao, 2017, CORR; Jain V., 2011, NIPS; Januszewski M., 2017, BIORXIV, P200675; Januszewski M., 2016, FLOOD FILLING NETWOR; Juan Nunez-Iglesias, 2014, GRAPH BASED ACTIVE L; Kasthuri N, 2015, CELL, V162, P648, DOI 10.1016/j.cell.2015.06.054; Lee Kisuk, 2017, CORR; Luc P., 2016, CORR; Meila M, 2007, J MULTIVARIATE ANAL, V98, P873, DOI 10.1016/j.jmva.2006.11.013; Meirovitch Y, 2016, MULTIPASS APPROACH L; Mirza M., 2014, ARXIV PREPRINT ARXIV; Nunez-Iglesias J, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0071715; Pinheiro P. O. O., 2015, ADV NEURAL INFORM PR, V28, P1990; Ren M., 2016, CORR; Rolnick D., 2017, CORR; Romera-Paredes B., 2015, CORR; Ronneberger O., 2015, P MEDICAL IMAGE COMP, P234; Saxena Shreyas, 2016, CORR; Schmidt H, 2017, NATURE, V549, P469, DOI 10.1038/nature24005; Turaga S C, 2010, CONVOLUTIONAL NETWOR; Wang S, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURE, P1; WHITE JG, 1986, PHILOS T R SOC B, V314, P1, DOI 10.1098/rstb.1986.0056; Zeng T, 2017, BIOINFORMATICS, V33, P2555, DOI 10.1093/bioinformatics/btx188	31	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406085
C	Agrawal, P; Nair, A; Abbeel, P; Malik, J; Levine, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Agrawal, Pulkit; Nair, Ashvin; Abbeel, Pieter; Malik, Jitendra; Levine, Sergey			Learning to Poke by Poking: Experiential Learning of Intuitive Physics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MANIPULATION; MODEL	We investigate an experiential learning paradigm for acquiring an internal model of intuitive physics. Our model is evaluated on a real-world robotic manipulation task that requires displacing objects to target locations by poking. The robot gathered over 400 hours of experience by executing more than 100K pokes on different objects. We propose a novel approach based on deep neural networks for modeling the dynamics of robot's interactions directly from images, by jointly estimating forward and inverse models of dynamics. The inverse model objective provides supervision to construct informative visual features, which the forward model can then predict and in turn regularize the feature space for the inverse model. The interplay between these two objectives creates useful, accurate models that can then be used for multi-step decision making. This formulation has the additional benefit that it is possible to learn forward models in an abstract feature space and thus alleviate the need of predicting pixels. Our experiments show that this joint modeling approach outperforms alternative methods.	[Agrawal, Pulkit; Nair, Ashvin; Abbeel, Pieter; Malik, Jitendra; Levine, Sergey] Univ Calif Berkeley, Berkeley Artificial Intelligence Res Lab BAIR, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Agrawal, P (corresponding author), Univ Calif Berkeley, Berkeley Artificial Intelligence Res Lab BAIR, Berkeley, CA 94720 USA.	pulkitag@berkeley.edu; anair17@berkeley.edu; pabbeel@berkeley.edu; malik@berkeley.edu; svlevine@berkeley.edu	Agrawal, Pulkit/O-8574-2019	Agrawal, Pulkit/0000-0001-8463-9917	ONR MURI [N00014-14-1-0671]; ONR YIP; ARL	ONR MURI(MURIOffice of Naval Research); ONR YIP(Office of Naval Research); ARL(United States Department of DefenseUS Army Research Laboratory (ARL))	We thank Alyosha Efros for inspiration and fruitful discussions throughout this work. The title of this paper is partly influenced by the term "pokebot" that Alyosha has been using for several years. We thank Ruzena Bajcsy for access to Baxter robot and Shubham Tulsiani for helpful comments. This work was supported in part by ONR MURI N00014-14-1-0671, ONR YIP and by ARL through the MAST program. We are grateful to NVIDIA corporation for donating K40 GPUs and providing access to the NVIDIA PSG cluster.	Dogar MR, 2012, AUTON ROBOT, V33, P217, DOI 10.1007/s10514-012-9306-z; Finn C., 2016, ICRA; Fragkiadaki K., 2016, ICLR; Gopnik A., 1999, SCI CRIB MINDS BRAIN; Hamrick J., 2011, C COG SC, P1545; JORDAN MI, 1992, COGNITIVE SCI, V16, P307, DOI 10.1207/s15516709cog1603_1; Kietzmann Tim C, 2009, ICMLA; Kolev S, 2015, IEEE-RAS INT C HUMAN, P1036, DOI 10.1109/HUMANOIDS.2015.7363481; Kopicki M., 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P5722, DOI 10.1109/ICRA.2011.5980295; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lau Manfred, 2011, 2011 IEEE International Conference on Robotics and Automation, P3733; LaValle S. M., 2006, PLANNING ALGORITHMS; Lerer A., 2016, ARXIV160301312; Levine S., 2016, JMLR; Levine S., 2016, LEARNING HAND EYE CO; Lillicrap Timothy P, 2016, ICLR; Mayne DQ, 2014, AUTOMATICA, V50, P2967, DOI 10.1016/j.automatica.2014.10.128; MCCLOSKEY M, 1983, SCI AM, V248, P122, DOI 10.1038/scientificamerican0483-122; Mericli T, 2015, AUTON ROBOT, V38, P317, DOI 10.1007/s10514-014-9414-z; Michotte A. E, 1963, PERCEPTION CAUSALITY; Mnih V, 2015, NATURE, V518, P529; Mottaghi R., 2016, CVPR; Oh J., 2015, NIPS; Pinto L., 2016, ICRA; Pinto L, 2016, LECT NOTES COMPUT SC, V9906, P3, DOI 10.1007/978-3-319-46475-6_1; Sascha L., 2012, 2012 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2012.6252823; Smith L, 2005, ARTIF LIFE, V11, P13, DOI 10.1162/1064546053278973; Wahlstrom Niklas, 2015, CORR; Watter Manuel, 2015, ADV NEURAL INFORM PR, P2746; WOLPERT DM, 1995, SCIENCE, V269, P1880, DOI 10.1126/science.7569931; Wu J., 2015, ADV NEURAL INFORM PR	32	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700077
C	Agrawal, S; Devanur, NR		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Agrawal, Shipra; Devanur, Nikhil R.			Linear Contextual Bandits with Knapsacks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider the linear contextual bandit problem with resource consumption, in addition to reward generation. In each round, the outcome of pulling an arm is a reward as well as a vector of resource consumptions. The expected values of these outcomes depend linearly on the context of that arm. The budget/capacity constraints require that the total consumption doesn't exceed the budget for each resource. The objective is once again to maximize the total reward. This problem turns out to be a common generalization of classic linear contextual bandits (linContextual) [8, 11, 1], bandits with knapsacks (BwK) [3, 9], and the online stochastic packing problem (OSPP) [4, 14]. We present algorithms with near-optimal regret bounds for this problem. Our bounds compare favorably to results on the unstructured version of the problem [5, 10] where the relation between the contexts and the outcomes could be arbitrary, but the algorithm only competes against a fixed set of policies accessible through an optimization oracle. We combine techniques from the work on linContextual, BwK and OSPP in a nontrivial manner while also tackling new difficulties that are not present in any of these special cases.	[Agrawal, Shipra] Columbia Univ, New York, NY 10027 USA; [Devanur, Nikhil R.] Microsoft Res, Redmond, WA USA	Columbia University; Microsoft	Agrawal, S (corresponding author), Columbia Univ, New York, NY 10027 USA.	sa3305@columbia.edu; nikdev@microsoft.com						Abbasi-Yadkori Y., 2012, NIPS; Agarwal A., 2014, ICML 2014; Agrawal S., 2014, P 15 ACM C EC COMP E; Agrawal S, 2015, P 26 ANN ACM SIAM S, P1405; Agrawal S, 2014, OPER RES, V62, P876, DOI 10.1287/opre.2014.1289; Agrawal Shipra, 2016, COLT; [Anonymous], [No title captured]; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Auer P., 2003, J MACH LEARN RES, V3; Badanidiyuru A, 2014, P C LEARN THEOR, V35, P1109; Badanidiyuru A, 2013, ANN IEEE SYMP FOUND, P207, DOI 10.1109/FOCS.2013.30; Chu W., 2011, AISTATS; Dani V., 2008, COLT; DEVANUR N. R., 2011, EC; Devanur N. R., 2009, EC; Wu H., 2015, P 28 INT C NEUR INF	17	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702039
C	Bautista, MA; Sanakoyeu, A; Sutter, E; Ommer, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bautista, Miguel A.; Sanakoyeu, Artsiom; Sutter, Ekaterina; Ommer, Bjoern			CliqueCNN: Deep Unsupervised Exemplar Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				SIMILARITY	Exemplar learning is a powerful paradigm for discovering visual similarities in an unsupervised manner. In this context, however, the recent breakthrough in deep learning could not yet unfold its full potential. With only a single positive sample, a great imbalance between one positive and many negatives, and unreliable relationships between most samples, training of Convolutional Neural networks is impaired. Given weak estimates of local distance we propose a single optimization problem to extract batches of samples with mutually consistent relations. Conflicting relations are distributed over different batches and similar samples are grouped into compact cliques. Learning exemplar similarities is framed as a sequence of clique categorization tasks. The CNN then consolidates transitivity relations within and between cliques and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification.	[Bautista, Miguel A.; Sanakoyeu, Artsiom; Sutter, Ekaterina; Ommer, Bjoern] Heidelberg Univ, Heidelberg Collaboratory Image Proc, IWR, Heidelberg, Germany	Ruprecht Karls University Heidelberg	Bautista, MA (corresponding author), Heidelberg Univ, Heidelberg Collaboratory Image Proc, IWR, Heidelberg, Germany.	miguel.bautista@iwr.uni-heidelberg.de; artsiom.sanakoyeu@iwr.uni-heidelberg.de; ekaterina.sutter@iwr.uni-heidelberg.de; bjoern.ommer@iwr.uni-heidelberg.de			Ministry for Science, Baden-Wurttemberg; Heidelberg Academy of Sciences, Heidelberg, Germany	Ministry for Science, Baden-Wurttemberg; Heidelberg Academy of Sciences, Heidelberg, Germany	This research has been funded in part by the Ministry for Science, Baden-Wurttemberg and the Heidelberg Academy of Sciences, Heidelberg, Germany. We are grateful to the NVIDIA corporation for donating a Titan X GPU.	[Anonymous], 2014, 2014 IEEE C COMP VIS, P580, DOI [10.1109/CVPR.2014.81, DOI 10.1109/CVPR.2014.81]; Burkard R.E., 1998, HDB COMBINATORIAL OP; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597; Dosovitskiy A., 2014, ADV NEURAL INFORM PR, P766; Eigenstetter A., CVPR 14; El-Naqa I, 2004, IEEE T MED IMAGING, V23, P1233, DOI 10.1109/TMI.2004.834601; Felzenszwalb P, 2008, PROC CVPR IEEE, P1984; Ferrari V, 2009, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2009.5206495; HARIHARAN B, 2012, ECCV, V7575, P459; He XueFeng, 2014, China Rural Survey, P4; Johnson MK, 2011, PROC CVPR IEEE; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229; Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29; Pirsiavash H., 2014, CVPR; Ramakrishna V., 2014, ECCV 14; Rubio JC, 2015, PATTERN RECOGN, V48, P3871, DOI 10.1016/j.patcog.2015.06.013; Shapovalova N, 2015, IEEE IMAGE PROC, P93, DOI 10.1109/ICIP.2015.7350766; Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180; Wang X., 2015, ICCV; Xia H, 2014, IEEE T PATTERN ANAL, V36, P536, DOI 10.1109/TPAMI.2013.149; Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958; Zagoruyko S., CVPR 14	25	3	3	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703023
C	De, A; Valera, I; Ganguly, N; Bhattacharya, S; Gomez-Rodriguez, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		De, Abir; Valera, Isabel; Ganguly, Niloy; Bhattacharya, Sourangshu; Gomez-Rodriguez, Manuel			Learning and Forecasting Opinion Dynamics in Social Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MODEL	Social media and social networking sites have become a global pinboard for exposition and discussion of news, topics, and ideas, where social media users often update their opinions about a particular topic by learning from the opinions shared by their friends. In this context, can we learn a data-driven model of opinion dynamics that is able to accurately forecast users' opinions? In this paper, we introduce SLANT, a probabilistic modeling framework of opinion dynamics, which represents users' opinions over time by means of marked jump diffusion stochastic differential equations, and allows for efficient model simulation and parameter estimation from historical fine grained event data. We then leverage our framework to derive a set of efficient predictive formulas for opinion forecasting and identify conditions under which opinions converge to a steady state. Experiments on data gathered from Twitter show that our model provides a good fit to the data and our formulas achieve more accurate forecasting than alternatives.	[De, Abir; Ganguly, Niloy; Bhattacharya, Sourangshu] IIT Kharagpur, Kharagpur, W Bengal, India; [Valera, Isabel; Gomez-Rodriguez, Manuel] MPI Software Syst, Saarbrucken, Germany	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Kharagpur	De, A (corresponding author), IIT Kharagpur, Kharagpur, W Bengal, India.	abir.de@cse.iitkgp.ernet.in; ivalera@mpi-sws.org; niloy@cse.iitkgp.ernet.in; sourangshu@cse.iitkgp.ernet.in; manuelgr@mpi-sws.org	Rodriguez, Manuel Gomez/AAB-5005-2021		Google India; Humboldt post-doctoral fellowship	Google India(Google Incorporated); Humboldt post-doctoral fellowship	Abir De is partially supported by Google India under the Google India PhD Fellowship Award, and Isabel Valera is supported by a Humboldt post-doctoral fellowship.	Aalen OO, 2008, STAT BIOL HEALTH, P1; Al-Mohy AH, 2011, SIAM J SCI COMPUT, V33, P488, DOI 10.1137/100788860; Axelrod R, 1997, J CONFLICT RESOLUT, V41, P203, DOI 10.1177/0022002797041002001; Birgin EG, 2000, SIAM J OPTIMIZ, V10, P1196, DOI 10.1137/S1052623497330963; Blundell C, 2012, ADV NEURAL INFORM PR, P2600; CLIFFORD P, 1973, BIOMETRIKA, V60, P581, DOI 10.1093/biomet/60.3.581; Das A., 2014, WSDM; De A., 2014, CIKM; DeGroot M. H., 1974, J AM STAT ASSOC, V69; Farajtabar M., 2015, NIPS; Farajtabar M., 2014, NIPS; GomezRodriguez M., 2011, ICML; Hannak A, 2012, ICWSM; Hanson FB, 2007, ADV DES CONTROL, P1; Hegselmann R, 2002, JASSS-J ARTIF SOC S, V5; HINRICHSEN D, 1989, J DIFFER EQUATIONS, V82, P219, DOI 10.1016/0022-0396(89)90132-0; Holme P, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.056108; Karppi T., 2015, TC S; Kim J., 2013, ICWSM; Leskovec J., 2010, JMLR; Pang B., 2008, F T INFORM RETRIEVAL, V2; Raven B. H., 1993, J SOCIAL ISSUES, V49; SAAD Y, 1986, SIAM J SCI STAT COMP, V7, P856, DOI 10.1137/0907058; Valera I., 2015, P 2015 IEEE INT C DA; Wang Yichen, 2016, ARXIV160309021; Yildiz ME, 2010, 2010 INFORMATION THEORY AND APPLICATIONS WORKSHOP (ITA), P419	26	3	3	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701050
C	Elhamifar, E		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Elhamifar, E.			High-Rank Matrix Completion and Clustering under Self-Expressive Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose efficient algorithms for simultaneous clustering and completion of incomplete high-dimensional data that lie in a union of low-dimensional subspaces. We cast the problem as finding a completion of the data matrix so that each point can be reconstructed as a linear or affine combination of a few data points. Since the problem is NP-hard, we propose a lifting framework and reformulate the problem as a group-sparse recovery of each incomplete data point in a dictionary built using incomplete data, subject to rank-one constraints. To solve the problem efficiently, we propose a rank pursuit algorithm and a convex relaxation. The solution of our algorithms recover missing entries and provides a similarity matrix for clustering. Our algorithms can deal with both low-rank and high-rank matrices, does not suffer from initialization, does not need to know dimensions of subspaces and can work with a small number of data points. By extensive experiments on synthetic data and real problems of video motion segmentation and completion of motion capture data, we show that when the data matrix is low-rank, our algorithm performs on par with or better than low-rank matrix completion methods, while for high-rank data matrices, our method significantly outperforms existing algorithms.	[Elhamifar, E.] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA	Northeastern University	Elhamifar, E (corresponding author), Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.	eelhami@ccs.neu.edu						Balzano L., 2012, IEEE STAT SIGN PROC; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Bhojanapalli S., 2013, INT C MACH LEARN ICM; Bishop C. M., 1999, NEURAL COMPUTATION, V11; Boyd S., 2010, FDN TRENDS MACHINE L, V3; Candes E., 2009, P IEEE; Candes E. J., 2014, ROBUST SUBSPAC UNPUB ROBUST SUBSPAC UNPUB; Candes E. J., 2008, FDN COMPUTATIONAL MA, V9; Chen G., 2009, INT J COMPUTER VISIO, V81; Chen Y., 2011, INT C MACH LEARN ICM; Chiang K. Y., 2015, NEURAL INFORM PROCES; Costeira J., 1998, INT J COMPUTER VISIO, V29; Elhamifar E., 2013, IEEE T PATTERN ANAL; Elhamifar E., 2016, IEEE T PATTERN ANAL; Elhamifar E., 2009, IEEE C COMP VIS PATT; Elhamifar E, 2011, PROC CVPR IEEE; Gabay D., 1976, COMP MATH APPL, V2; Ganti R., 2015, NEURAL INFORM PROCES; Ghahramani Z., 1996, CRGTR961 U TOR DEP C; Gruber A., 2004, IEEE C COMP VIS PATT; Hastie T., 1998, STAT SCI; Jenatton R., 2011, J MACHINE LEARNING R, V12; Kanatani K., 2001, IEEE INT C COMP VIS, V2; Keshavan R. H., 2010, IEEE T INFORM THEORY; Knott M., 1999, LATENT VARIABLE MODE; Ng A. Y., 2001, NEURAL INFORM PROCES; Nowak, 2012, INT C ART INT STAT; Park D., 2015, INT C MACH LEARN ICM; Sapiro G, 2009, INT C MACH LEARN ICM; Soltanolkotabi M., 2014, ANN STAT; Tipping M., 1999, J ROYAL STAT SOC, V61; TOMASI C, 1992, INT J COMPUTER VISIO, V9; Tropp J. A., 2009, ACM SIAM S DISCR ALG; Vidal R., 2008, INT J COMPUTER VISIO, V79; Yan Jingyu, 2006, EUR C COMP VIS; Yang C., 2015, INT C MACH LEARN ICM; Zhang A., 2012, UNCERTAINTY ARTIFICI; Zhao B., 2009, ANN STAT, V37	38	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702104
C	Figurnov, M; Ibraimova, A; Vetrov, D; Kohli, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Figurnov, Michael; Ibraimova, Aijan; Vetrov, Dmitry; Kohli, Pushmeet			PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose a novel approach to reduce the computational cost of evaluation of convolutional neural networks, a factor that has hindered their deployment in low-power devices such as mobile phones. Inspired by the loop perforation technique from source code optimization, we speed up the bottleneck convolutional layers by skipping their evaluation in some of the spatial positions. We propose and analyze several strategies of choosing these positions. We demonstrate that perforation can accelerate modern convolutional networks such as AlexNet and VGG-16 by a factor of 2x - 4x. Additionally, we show that perforation is complementary to the recently proposed acceleration method of Zhang et al. [28].	[Figurnov, Michael; Vetrov, Dmitry] Natl Res Univ, Higher Sch Econ, Moscow, Russia; [Figurnov, Michael] Lomonosov Moscow State Univ, Moscow, Russia; [Vetrov, Dmitry] Yandex, Moscow, Russia; [Ibraimova, Aijan] Skolkovo Inst Sci & Technol, Moscow, Russia; [Kohli, Pushmeet] Microsoft Res, Redmond, WA USA	HSE University (National Research University Higher School of Economics); Lomonosov Moscow State University; Skolkovo Institute of Science & Technology; Microsoft	Figurnov, M (corresponding author), Natl Res Univ, Higher Sch Econ, Moscow, Russia.; Figurnov, M (corresponding author), Lomonosov Moscow State Univ, Moscow, Russia.	michael@figurnov.ru; aijan.ibraimova@gmail.com; vetrovd@yandex.ru; pkohli@microsoft.com			RFBR [15-31-20596 (mol-a-ved)]; Microsoft: Moscow State University Joint Research Center [RPD 1053945]	RFBR(Russian Foundation for Basic Research (RFBR)); Microsoft: Moscow State University Joint Research Center	We would like to thank Alexander Kirillov and Dmitry Kropotov for helpful discussions, and Yandex for providing computational resources for this project. This work was supported by RFBR project No. 15-31-20596 (mol-a-ved) and by Microsoft: Moscow State University Joint Research Center (RPD 1053945).	[Anonymous], 2015, P 28 INT C NEUR INF; [Anonymous], 2014, CUDNN EFFICIENT PRIM; Ba J., 2015, NIPS; Chen T., 2015, MATRIX SHADOW LIB; Collins M. D., 2014, MEMORY BOUNDED DEEP; Courbariaux M., 2015, ICLR; Denton E., 2014, NIPS; Graham B, 2014, FRACTIONAL MAX POOLI; Graham B., 2014, SPATIALLY SPARSE CON; Gupta S., 2015, ICML; Jaderberg M., 2014, BMVC; Jia Y., 2014, ACM ICM; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Krizhevsky A., 2014, CUDA CONVNET2; Lebedev Vadim, 2015, ICLR; Lebedev Vadim, 2016, P IEEE C COMP VIS PA; Lin M., 2014, ICLR; Misailovic S., 2011, STAT ANAL; Misailovic  S., 2010, ICSE; Mnih V., 2014, NIPS; Novikov A., 2015, NIPS; Ovtcharov K, 2015, MICROSOFT RES WHITEP; Samadi Mehrzad, 2014, ASPLOS; Sidiroglou-Douskos S., 2011, ACM SIGSOFT; Simonyan Karen, 2015, INT C LEARN REPR; Vedaldi A., 2014, MATCONVNET CONVOLUTI; Yang Z., 2015, ICCV; Zhang X., 2015, ACCELERATING VERY DE	28	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703099
C	Garivier, A; Kaufmann, E; Lattimore, T		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Garivier, Aurelien; Kaufmann, Emilie; Lattimore, Tor			On Explore-Then-Commit Strategies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MULTIARMED BANDIT; ALLOCATION; BOUNDS	We study the problem of minimising regret in two-armed bandit problems with Gaussian rewards. Our objective is to use this simple setting to illustrate that strategies based on an exploration phase (up to a stopping time) followed by exploitation are necessarily suboptimal. The results hold regardless of whether or not the difference in means between the two arms is known. Besides the main message, we also refine existing deviation inequalities, which allow us to design fully sequential strategies with finite-time regret guarantees that are (a) asymptotically optimal as the horizon grows and (b) order-optimal in the minimax sense. Furthermore we provide empirical evidence that the theory also holds in practice and discuss extensions to non-gaussian and multiple-armed case.	[Garivier, Aurelien] Inst Math Toulouse, F-31062 Toulouse 9, France; [Garivier, Aurelien] Univ Toulouse, UMR5219, F-31062 Toulouse 9, France; [Garivier, Aurelien] CNRS, UPS IMT, F-31062 Toulouse 9, France; [Kaufmann, Emilie] Univ Lille, CNRS, Cent Lille,Inria SequeL,UMR 9189, CRIStAL Ctr Rech Informat Signal & Automat Lille, F-59000 Lille, France; [Lattimore, Tor] Univ Alberta, 116 St & 85 Ave, Edmonton, AB T6G 2R3, Canada	Centre National de la Recherche Scientifique (CNRS); Centre National de la Recherche Scientifique (CNRS); Universite de Lille - ISITE; Centrale Lille; Universite de Lille; University of Alberta	Garivier, A (corresponding author), Inst Math Toulouse, F-31062 Toulouse 9, France.; Garivier, A (corresponding author), Univ Toulouse, UMR5219, F-31062 Toulouse 9, France.; Garivier, A (corresponding author), CNRS, UPS IMT, F-31062 Toulouse 9, France.	aurelien.garivier@math.univ-toulouse.fr; emilie.kaufmann@univ-lille1.fr; tor.lattimore@gmail.com			CIMI (Centre International de Mathematiques et d'Informatique) Excellence program; French Agence Nationale de la Recherche (ANR) [ANR-13-BS01-0005, ANR-13-CORD-0020]	CIMI (Centre International de Mathematiques et d'Informatique) Excellence program; French Agence Nationale de la Recherche (ANR)(French National Research Agency (ANR))	This work was partially supported by the CIMI (Centre International de Mathematiques et d'Informatique) Excellence program while Emilie Kaufmann visited Toulouse in November 2015. The authors acknowledge the support of the French Agence Nationale de la Recherche (ANR), under grants ANR-13-BS01-0005 (project SPADRO) and ANR-13-CORD-0020 (project ALICIA).	Audibert J.-Y., 2009, P 22 ANN C LEARN THE, P217; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Auer P, 2010, PERIOD MATH HUNG, V61, P55, DOI 10.1007/s10998-010-3055-6; Bubeck, 2013, ADV NEURAL INFORM PR, P638; Bubeck S., 2013, P 26 ANN C LEARN THE, P122; Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Garivier, 2016, P 29 C LEARN THEOR; Garivier A., 2016, ARXIV160207182; Hoofar A., 2008, J INEQUALITIES PURE, V9, P5; KATEHAKIS MN, 1995, P NATL ACAD SCI USA, V92, P8584, DOI 10.1073/pnas.92.19.8584; Kaufmann Emilie, 2014, P 27 C LEARN THEOR; LAI TL, 1987, ANN STAT, V15, P1091, DOI 10.1214/aos/1176350495; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lai Tze Leung, 1983, RECENT ADV STAT, P51; Lattimore T., 2015, TECHNICAL REPORT; Morters P., 2010, BROWNIAN MOTION, V30; Perchet V., 2013, ANN STAT; Perchet Vianney, 2015, P 28 C LEARN THEOR; Siegmund D., 1985, SEQUENTIAL ANAL; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; WALD A, 1945, ANN MATH STAT, V16, P117, DOI 10.1214/aoms/1177731118	22	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701036
C	Jain, S; White, M; Radivojac, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Jain, Shantanu; White, Martha; Radivojac, Predrag			Estimating the class prior and posterior from noisy positives and unlabeled data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We develop a classification algorithm for estimating posterior distributions from positive-unlabeled data, that is robust to noise in the positive labels and effective for high-dimensional data. In recent years, several algorithms have been proposed to learn from positive-unlabeled data; however, many of these contributions remain theoretical, performing poorly on real high-dimensional data that is typically contaminated with noise. We build on this previous work to develop two practical classification algorithms that explicitly model the noise in the positive labels and utilize univariate transforms built on discriminative classifiers. We prove that these univariate transforms preserve the class prior, enabling estimation in the univariate space and avoiding kernel density estimation for high-dimensional data. The theoretical development and parametric and nonparametric algorithms proposed here constitute an important step towards wide-spread use of robust classification algorithms for positive-unlabeled data.	[Jain, Shantanu; White, Martha; Radivojac, Predrag] Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA	Indiana University System; Indiana University Bloomington	Jain, S (corresponding author), Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA.	shajain@indiana.edu; martha@indiana.edu; predrag@indiana.edu	White, Martha/AAF-7066-2020	White, Martha/0000-0002-5356-2950	NSF [DBI-1458477]; NIH [R01MH105524, R01GM103725]; Indiana University Precision Health Initiative	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Indiana University Precision Health Initiative	We thank Prof. Michael W. Trosset for helpful comments. Grant support: NSF DBI-1458477, NIH R01MH105524, NIH R01GM103725, and the Indiana University Precision Health Initiative.	Bashir S, 2005, J MULTIVARIATE ANAL, V93, P102, DOI 10.1016/j.jmva.2003.12.003; Blanchard G, 2010, J MACH LEARN RES, V11, P2973; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Bouveyron C, 2009, PATTERN RECOGN, V42, P2649, DOI 10.1016/j.patcog.2009.03.027; Cortes C, 2008, LECT NOTES ARTIF INT, V5254, P38, DOI 10.1007/978-3-540-87987-9_8; Denis F, 2005, THEOR COMPUT SCI, V348, P70, DOI 10.1016/j.tcs.2005.09.007; Du Plessis MC, 2014, IEICE T INF SYST, VE97D, P1358, DOI 10.1587/transinf.E97.D.1358; Elkan Charles, 2008, P 14 ACM SIGKDD INT, P213, DOI DOI 10.1145/1401890.1401920; Hawkins DM, 1997, J AM STAT ASSOC, V92, P136, DOI 10.2307/2291457; Jain S., 2016, ARXIV160101944; Katz-Samuels J., 2016, ARXIV160206235; Lawrence N. D., 2001, 18 INT C MACHINE LEA, V1, P306; Lichman M., 2013, UCI MACHINE LEARNING; Liu H., 2007, P 11 INT C ART INT S, V2, P283, DOI DOI 10.1214/009053607000000811.508; Long PM, 2010, MACH LEARN, V78, P287, DOI 10.1007/s10994-009-5165-z; Manwani N, 2013, IEEE T CYBERNETICS, V43, P1146, DOI 10.1109/TSMCB.2012.2223460; Menon AK, 2015, PR MACH LEARN RES, V37, P125; Niculescu-Mizil A., 2005, P 21 C UNC ART INT, P413, DOI DOI 10.5555/3020336.3020388; Platt J. C., 1999, PROBABILISTIC OUTPUT, P61; Ramaswamy H. G, 2016, ARXIV160302501; Reid MD, 2010, J MACH LEARN RES, V11, P2387; Saerens M, 2002, NEURAL COMPUT, V14, P21, DOI 10.1162/089976602753284446; Sanderson T, 2014, JMLR WORKSH CONF PRO, V33, P850; Scott C., 2013, P 26 ANN C LEARN THE, P489; Scott D. W., 2008, MULTIVARIATE DENSITY, P195; Steen H, 2004, NAT REV MOL CELL BIO, V5, P699, DOI 10.1038/nrm1468; Ward G, 2009, BIOMETRICS, V65, P554, DOI 10.1111/j.1541-0420.2008.01116.x	27	3	3	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701025
C	Kanagawa, M; Sriperumbudur, BK; Fukumizu, K		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kanagawa, Motonobu; Sriperumbudur, Bharath K.; Fukumizu, Kenji			Convergence guarantees for kernel-based quadrature rules in misspecified settings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ERROR	Kernel-based quadrature rules are becoming important in machine learning and statistics, as they achieve super-root n convergence rates in numerical integration, and thus provide alternatives to Monte Carlo integration in challenging settings where integrands are expensive to evaluate or where integrands are high dimensional. These rules are based on the assumption that the integrand has a certain degree of smoothness, which is expressed as that the integrand belongs to a certain reproducing kernel Hilbert space (RKHS). However, this assumption can be violated in practice (e.g., when the integrand is a black box function), and no general theory has been established for the convergence of kernel quadratures in such misspecified settings. Our contribution is in proving that kernel quadratures can be consistent even when the integrand does not belong to the assumed RKHS, i.e., when the integrand is less smooth than assumed. Specifically, we derive convergence rates that depend on the (unknown) lesser smoothness of the integrand, where the degree of smoothness is expressed via powers of RKHSs or via Sobolev spaces.	[Kanagawa, Motonobu; Fukumizu, Kenji] Inst Stat Math, Tokyo 1908562, Japan; [Sriperumbudur, Bharath K.] Penn State Univ, Dept Stat, University Pk, PA 16802 USA	Research Organization of Information & Systems (ROIS); Institute of Statistical Mathematics (ISM) - Japan; Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park	Kanagawa, M (corresponding author), Inst Stat Math, Tokyo 1908562, Japan.	kanagawa@ism.ac.jp; bks18@psu.edu; fukumizu@ism.ac.jp			MEXT [25120012]	MEXT(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT))	We wish to thank the anonymous reviewers for valuable comments. We also thank Chris Oates for fruitful discussions. This work has been supported in part by MEXT Grant-in-Aid for Scientific Research on Innovative Areas (25120012).	Adams R., 2003, SOBOLEV SPACES, V2nd; Bach F., 2012, P 29 INT C INT C MAC, P1355; Bach F., 2015, HAL01118276V2; Berlinet A., 2004, REPRODUCING KERNEL H, DOI [10.1007/978-1-4419-9096-9, DOI 10.1007/978-1-4419-9096-9]; Briol F-X., 2016, ARXIV151200933V4STAT; Briol FX, 2015, ADV NEUR IN, V28; Chen YH, 2010, ADV INTEL SOFT COMPU, V66, P109, DOI 10.1145/1866919.1866935; Dick J., 2013, ACTA NUMER, V22, P133; Dick J, 2007, SIAM J NUMER ANAL, V45, P2141, DOI 10.1137/060658916; Dick J, 2011, ANN STAT, V39, P1372, DOI 10.1214/11-AOS880; Dick J, 2008, SIAM J NUMER ANAL, V46, P1519, DOI 10.1137/060666639; Gerber M, 2015, J R STAT SOC B, V77, P509, DOI 10.1111/rssb.12104; Hickernell FJ, 1998, MATH COMPUT, V67, P299, DOI 10.1090/S0025-5718-98-00894-1; Lacoste-Julien S., 2015, P AISTATS 2015; Novak E., 1988, DETERMINISTIC STOCHA; Novak E, 2008, EMS TRACTS MATH, V6, P3; Oates CJ, 2016, JMLR WORKSH CONF PRO, V51, P56; Oates CJ, 2017, J R STAT SOC B, V79, P695, DOI 10.1111/rssb.12185; OHAGAN A, 1991, J STAT PLAN INFER, V29, P245, DOI 10.1016/0378-3758(91)90002-V; Rasmussen E, 2006, GAUSSIAN PROCESSES M; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Steinwart I., 2012, CONSTRUCTIVE APPROXI, V35; Yang J., 2014, P ICML 2014	23	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701031
C	Krotov, D; Hopfield, JJ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Krotov, Dmitry; Hopfield, John J.			Dense Associative Memory for Pattern Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				NEURAL-NETWORKS; CAPACITY	A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions-the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set.	[Krotov, Dmitry] Inst Adv Study, Simons Ctr Syst Biol, Olden Lane, Princeton, NJ 08540 USA; [Hopfield, John J.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA	Institute for Advanced Study - USA; Princeton University	Krotov, D (corresponding author), Inst Adv Study, Simons Ctr Syst Biol, Olden Lane, Princeton, NJ 08540 USA.	krotov@ias.edu; hopfield@princeton.edu						ABBOTT LF, 1987, PHYS REV A, V36, P5091, DOI 10.1103/PhysRevA.36.5091; AMIT DJ, 1985, PHYS REV LETT, V55, P1530, DOI 10.1103/PhysRevLett.55.1530; BALDI P, 1987, PHYS REV LETT, V58, P913, DOI 10.1103/PhysRevLett.58.913; Chen H. H., 1986, AIP Conference Proceedings, P86, DOI 10.1063/1.36224; GARDNER E, 1987, J PHYS A-MATH GEN, V20, P3453, DOI 10.1088/0305-4470/20/11/046; Glorot X, 2011, P 14 INT C ART INT S, V15, P315, DOI DOI 10.1177/1753193410395357; Goodfellow I. J., 2014, ARXIV14126572; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; HORN D, 1988, J PHYS-PARIS, V49, P389, DOI 10.1051/jphys:01988004903038900; Kamyshanska H., 2013, ICML, V28, P720; Kamyshanska H, 2015, IEEE T PATTERN ANAL, V37, P1261, DOI 10.1109/TPAMI.2014.2362140; KANTER I, 1987, PHYS REV A, V35, P380, DOI 10.1103/PhysRevA.35.380; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Yann, 2006, PREDICTING STRUCTURE, V1; MCELIECE RJ, 1987, IEEE T INFORM THEORY, V33, P461, DOI 10.1109/TIT.1987.1057328; Minsky M., 1969, PERCEPTRON INTRO COM, V19, P2; Nair V., 2010, ICML, P807; Psaltis D., 1986, AIP Conference Proceedings, P370, DOI 10.1063/1.36241; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546; Simard PY, 2003, PROC INT CONF DOC, P958; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Wan L., 2013, P INT C MACHINE LEAR, P1058	24	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700085
C	Lagree, P; Vernade, C; Cappe, O		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lagree, Paul; Vernade, Claire; Cappe, Olivier			Multiple-Play Bandits in the Position-Based Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Sequentially learning to place items in multi-position displays or lists is a task that can be cast into the multiple-play semi-bandit setting. However, a major concern in this context is when the system cannot decide whether the user feedback for each item is actually exploitable. Indeed, much of the content may have been simply ignored by the user. The present work proposes to exploit available information regarding the display position bias under the so-called Position-based click model (PBM). We first discuss how this model differs from the Cascade model and its variants considered in several recent works on multiple-play bandits. We then provide a novel regret lower bound for this model as well as computationally efficient algorithms that display good empirical and theoretical performance.	[Lagree, Paul] Univ Paris Saclay, Univ Paris Sud, LRI, St Aubin, France; [Vernade, Claire; Cappe, Olivier] Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, St Aubin, France	UDICE-French Research Universities; Universite Paris Saclay; Centre National de la Recherche Scientifique (CNRS); IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Saclay	Lagree, P (corresponding author), Univ Paris Saclay, Univ Paris Sud, LRI, St Aubin, France.	paul.lagree@u-psud.fr; vernade@enst.fr			French research project ALICIA [ANR-13-CORD-0020]; Machine Learning for Big Data Chair at Telecom ParisTech	French research project ALICIA; Machine Learning for Big Data Chair at Telecom ParisTech	This work was partially supported by the French research project ALICIA (grant ANR-13-CORD-0020) and by the Machine Learning for Big Data Chair at Telecom ParisTech.	ANANTHARAM V, 1987, IEEE T AUTOMAT CONTR, V32, P968, DOI 10.1109/TAC.1987.1104491; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Chen W., 2013, JMLR WORKSHOP C P, V28, P151; Chuklin A., 2015, SYNTHESIS LECT INF C, V7, P1; Combes R., 2015, ADV NEURAL INFORM PR; Combes Richard, 2015, P 2015 ACM SIGMETRIC; Craswell N., 2008, P INT C WEB SEARCH D; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Garivier A., 2011, P C LEARN THEOR; Graves TL, 1997, SIAM J CONTROL OPTIM, V35, P715, DOI 10.1137/S0363012994275440; Kaufmann Emilie, 2015, J MACHINE LEARNING R; Komiyama J., 2015, P 32 INT C MACH LEAR; Kveton B., 2015, P 32 INT C MACH LEAR; Kveton Branislav, 2015, P 18 INT C ART INT S; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Magureanu S., 2014, P C LEARN THEOR; Radlinski Filip., 2008, P 17 ACM C INF KNOWL; Richardson Matthew, 2007, 16 INT C WORLD WID W; Sumeet K., 2016, P 33 INT C MACH LEAR	19	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704070
C	Malkomes, G; Schaff, C; Garnett, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Malkomes, Gustavo; Schaff, Chip; Garnett, Roman			Bayesian optimization for automated model selection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Despite the success of kernel-based nonparametric methods, kernel selection still requires considerable expertise, and is often described as a "black art." We present a sophisticated method for automatically searching for an appropriate kernel from an infinite space of potential choices. Previous efforts in this direction have focused on traversing a kernel grammar, only examining the data via computation of marginal likelihood. Our proposed search method is based on Bayesian optimization in model space, where we reason about model evidence as a function to be maximized. We explicitly reason about the data distribution and how it induces similarity between potential model choices in terms of the explanations they can offer for observed data. In this light, we construct a novel kernel between models to explain a given dataset. Our method is capable of finding a model that explains a given dataset well without any human assistance, often with fewer computations of model evidence than previous approaches, a claim we demonstrate empirically.	[Malkomes, Gustavo; Schaff, Chip; Garnett, Roman] Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA	Washington University (WUSTL)	Malkomes, G (corresponding author), Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.	luizgustavo@wustl.edu; cbschaff@wustl.edu; garnett@wustl.edu			National Science Foundation (NSF) [IIA-1355406]; Brazilian Federal Agency for Support and Evaluation of Graduate Education (CAPES)	National Science Foundation (NSF)(National Science Foundation (NSF)); Brazilian Federal Agency for Support and Evaluation of Graduate Education (CAPES)(Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES))	This material is based upon work supported by the National Science Foundation (NSF) under award number IIA-1355406. Additionally, GM acknowledges support from the Brazilian Federal Agency for Support and Evaluation of Graduate Education (CAPES).	Bach F. R., 2008, C NEUR INF PROC SYST; Bergstra J., 2011, NEURAL INFORM PROCES, V24; Brochu E, 2010, ARXIV PREPRINT ARXIV; Damianou A. C., 2013, INT C ART INT STAT A; Duvenaud David, 2013, INT C MACH LEARN ICM; Gardner Jacob R., 2015, C NEUR INF PROC SYST; Gonen M, 2011, J MACH LEARN RES, V12, P2211; Grosse Roger B, 2012, C UNC ART INT UAI; Hinton G. E., 2008, C NEUR INF PROC SYST; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Kuha J, 2004, SOCIOL METHOD RES, V33, P188, DOI 10.1177/0049124103262065; Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865; MacKay D. J. C., 1998, Neural Networks and Machine Learning. Proceedings, P133; Raftery AE, 1996, BIOMETRIKA, V83, P251, DOI 10.1093/biomet/83.2.251; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Snoek J, 2012, C NEUR INF PROC SYST; Wilson A., 2014, C NEUR INF PROC SYST; Wilson A. G., 2013, INT C MACH LEARN ICM; Wilson A. G., 2012, INT C MACH LEARN ICM	21	3	3	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703102
C	Ndiaye, E; Fercoq, O; Gramfort, A; Salmon, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ndiaye, Eugene; Fercoq, Olivier; Gramfort, Alexandre; Salmon, Joseph			GAP Safe Screening Rules for Sparse-Group Lasso	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ALGORITHMS; REGRESSION; SELECTION	For statistical learning in high dimension, sparse regularizations have proven useful to boost both computational and statistical efficiency. In some contexts, it is natural to handle more refined structures than pure sparsity, such as for instance group sparsity. Sparse-Group Lasso has recently been introduced in the context of linear regression to enforce sparsity both at the feature and at the group level. We propose the first (provably) safe screening rules for Sparse-Group Lasso, i.e., rules that allow to discard early in the solver features/groups that are inactive at optimal solution. Thanks to efficient dual gap computations relying on the geometric properties of epsilon-norm, safe screening rules for Sparse-Group Lasso lead to significant gains in term of computing time for our coordinate descent implementation.	[Ndiaye, Eugene; Fercoq, Olivier; Gramfort, Alexandre; Salmon, Joseph] Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, F-75013 Paris, France	Centre National de la Recherche Scientifique (CNRS); IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Cite; Universite Paris Saclay	Ndiaye, E (corresponding author), Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, F-75013 Paris, France.	eugene.ndiaye@telecom-paristech.fr; olivier.fercoq@telecom-paristech.fr; alexandre.gramfort@telecom-paristech.fr; joseph.salmon@telecom-paristech.fr			ANR THALAMEEG [ANR-14-NEUC-0002-01]; NIH [R01 MH106174]; ERC Starting Grant SLAB [ERC-YStG-676943]; Chair Machine Learning for Big Data at Telecom ParisTech	ANR THALAMEEG(French National Research Agency (ANR)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); ERC Starting Grant SLAB; Chair Machine Learning for Big Data at Telecom ParisTech	this work was supported by the ANR THALAMEEG ANR-14-NEUC-0002-01, the NIH R01 MH106174, by ERC Starting Grant SLAB ERC-YStG-676943 and by the Chair Machine Learning for Big Data at Telecom ParisTech.	Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Bonnefoy A., 2014, EUSIPCO; Bonnefoy A, 2015, IEEE T SIGNAL PROCES, V63, P5121, DOI 10.1109/TSP.2015.2447503; Borwein J.M., 2006, CONVEX ANAL NONLINEA; Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Burdakov O., 1988, 33 INT WISS K FORTR, P15; Burdakov O., 2001, TECH REP; Chatterjee S., 2012, P SIAM INT C DAT MIN, P47, DOI DOI 10.1137/1.9781611972825.5; El Ghaoui L, 2012, PAC J OPTIM, V8, P667; Fercoq O, 2015, PR MACH LEARN RES, V37, P333; Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131; Jenatton R, 2011, J MACH LEARN RES, V12, P2297; Johnson TB, 2015, PR MACH LEARN RES, V37, P1171; Kalnay E, 1996, B AM METEOROL SOC, V77, P437, DOI 10.1175/1520-0477(1996)077<0437:TNYRP>2.0.CO;2; Ndiaye E, 2015, ADV NEUR IN, V28; Simon N, 2013, J COMPUT GRAPH STAT, V22, P231, DOI 10.1080/10618600.2012.681250; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x; Wang J., 2014, ARXIV14104210; Xiang Z., 2011, ADV NEURAL INFORM PR, V24, P900; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x	22	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703041
C	Nokland, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Nokland, Arild			Direct Feedback Alignment Provides Learning in Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task.				arild.nokland@gmail.com						[Anonymous], 2014, CORR; Bengio Y., 2015, CORR; Gilbert CD, 2013, NAT REV NEUROSCI, V14, P350, DOI 10.1038/nrn3476; Hinton GE., 1983, P IEEE C COMP VIS PA; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Lee DH, 2015, LECT NOTES ARTIF INT, V9284, P498, DOI 10.1007/978-3-319-23528-8_31; Liao Q., 2015, CORR; Lillicrap Timothy P., 2014, CORR; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Salakhutdinov R., 2009, P 12 INT C ART INT S, P448; Saxe Andrew M., 2013, CORR; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sussillo David, 2014, CORR; Tieleman Tijmen, 2012, LECT 65 RMSPROP DIVI, V4; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Xie XH, 2003, NEURAL COMPUT, V15, P441, DOI 10.1162/089976603762552988	16	3	2	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703077
C	Sadhanala, V; Wang, YX; Tibshirani, RJ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Sadhanala, Veeranjaneyulu; Wang, Yu-Xiang; Tibshirani, Ryan J.			Total Variation Classes Beyond 1d: Minimax Rates, and the Limitations of Linear Smoothers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				TOTAL VARIATION MINIMIZATION; ALGORITHM; RECOVERY; PATH	We consider the problem of estimating a function defined over n locations on a d-dimensional grid (having all side lengths equal to n(1/d)). When the function is constrained to have discrete total variation bounded by C-n, we derive the minimax optimal (squared) l(2) estimation error rate, parametrized by n, C-n. Total variation denoising, also known as the fused lasso, is seen to be rate optimal. Several simpler estimators exist, such as Laplacian smoothing and Laplacian eigenmaps. A natural question is: can these simpler estimators perform just as well? We prove that these estimators, and more broadly all estimators given by linear transformations of the input data, are suboptimal over the class of functions with bounded variation. This extends fundamental findings of Donoho and Johnstone [12] on 1-dimensional total variation spaces to higher dimensions. The implication is that the computationally simpler methods cannot be used for such sophisticated denoising tasks, without sacrificing statistical accuracy. We also derive minimax rates for discrete Sobolev spaces over d-dimensional grids, which are, in some sense, smaller than the total variation function spaces. Indeed, these are small enough spaces that linear estimators can be optimal-and a few well-known ones are, such as Laplacian smoothing and Laplacian eigenmaps, as we show. Lastly, we investigate the adaptivity of the total variation denoiser to these smaller Sobolev function spaces.	[Sadhanala, Veeranjaneyulu; Wang, Yu-Xiang] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Tibshirani, Ryan J.] Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	Sadhanala, V (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	vsadhana@cs.cmu.edu; yuxiangw@cs.cmu.edu; ryantibs@stat.cmu.edu			NSF [DMS-1309174, DMS-1554123, BCS-0941518]; Singapore NRF under its International Research Centre @ Singapore Funding Initiative; Baidu Scholarship	NSF(National Science Foundation (NSF)); Singapore NRF under its International Research Centre @ Singapore Funding Initiative; Baidu Scholarship	We thank Jan-Christian Hutter and Philippe Rigollet, whose paper [16] inspired us to think carefully about problem scalings (i.e., radii of TV and Sobolev classes) in the first place. YW was supported by NSF Award BCS-0941518 to CMU Statistics, a grant by Singapore NRF under its International Research Centre @ Singapore Funding Initiative, and a Baidu Scholarship. RT was supported by NSF Grants DMS-1309174 and DMS-1554123.	ACAR R, 1994, INVERSE PROBL, V10, P1217, DOI 10.1088/0266-5611/10/6/003; Barbero A lvaro, 2014, MODULAR PROXIMAL OPT; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; BELKIN M, 2002, ADV NEURAL INFORM PR, V15; Belkin Mikhail, 2005, C LEARN THEOR COLT 0, V18; Birge L., 2001, J EUR MATH SOC, V3, P203, DOI [10.1007/s100970100031, DOI 10.1007/S100970100031]; Chambolle A, 1997, NUMER MATH, V76, P167, DOI 10.1007/s002110050258; Chambolle A, 2009, INT J COMPUT VISION, V84, P288, DOI 10.1007/s11263-009-0238-9; CONTE SD, 1980, INT SERIES PURE APPL; Dobson DC, 1996, SIAM J APPL MATH, V56, P1181, DOI 10.1137/S003613999427560X; DONOHO DL, 1990, ANN STAT, V18, P1416, DOI 10.1214/aos/1176347758; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Donoho DL, 1998, ANN STAT, V26, P879; Gyorfi L., 2002, DISTRIBUTION FREE TH; Hoefling H, 2010, J COMPUT GRAPH STAT, V19, P984, DOI 10.1198/jcgs.2010.09208; Hutter Jan-Christian, 2016, C LEARN THEOR COLT 1; KUNSCH HR, 1994, ANN I STAT MATH, V46, P1, DOI 10.1007/BF00773588; Mammen E, 1997, ANN STAT, V25, P387; Needell D, 2013, SIAM J IMAGING SCI, V6, P1035, DOI 10.1137/120868281; Ng MK, 1999, SIAM J SCI COMPUT, V21, P851, DOI 10.1137/S1064827598341384; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Sharpnack J., 2012, AISTATS JMLR WCP; Sharpnack James, 2010, ADV NEURAL INFORM PR, V13; Smola Alexander, 2003, P ANN C LEARN THEOR, V16; Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; Tibshirani RJ, 2014, ANN STAT, V42, P285, DOI 10.1214/13-AOS1189; Tibshirani RJ, 2011, ANN STAT, V39, P1335, DOI 10.1214/11-AOS878; Wang YL, 2008, SIAM J IMAGING SCI, V1, P248, DOI 10.1137/080724265; Wang Yu-Xiang, 2016, J MACHINE LEARNING R; Zhu Xiaojin, 2003, INT C MACH LEARN ICM, V20	30	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704094
C	Sukhbaatar, S; Szlam, A; Fergus, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Sukhbaatar, Sainbayar; Szlam, Arthur; Fergus, Rob			Learning Multiagent Communication with Backpropagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNet, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.	[Sukhbaatar, Sainbayar] NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA; [Szlam, Arthur; Fergus, Rob] Facebook AI Res, New York, NY USA	New York University; Facebook Inc	Sukhbaatar, S (corresponding author), NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA.	sainbar@cs.nyu.edu; aszlam@fb.com; robfergus@fb.com			CIFAR	CIFAR(Canadian Institute for Advanced Research (CIFAR))	The authors wish to thank Daniel Lee and Y-Lan Boureau for their advice and guidance. Rob Fergus is grateful for the support of CIFAR.	[Anonymous], 2015, NIPS; [Anonymous], 2020, REINFORCEMENT LEARNI; Busoniu L, 2008, IEEE T SYST MAN CY C, V38, P156, DOI 10.1109/TSMCC.2007.913919; Cao YC, 2013, IEEE T IND INFORM, V9, P427, DOI 10.1109/TII.2012.2219061; Crites RH, 1998, MACH LEARN, V33, P235, DOI 10.1023/A:1007518724497; Foerster J. N., 2016, ABS160202672 ARXIV; Fox D, 2000, AUTON ROBOT, V8, P325, DOI 10.1023/A:1008937911390; Giles CL, 2002, LECT NOTES ARTIF INT, V2564, P377; Guestrin C., 2001, NIPS; Guo Xiaoxiao, 2014, NIPS; Kaiser Lukasz, 2016, ICLR; Kasai Tatsuya, 2008, 2008 IEEE Conference on Soft Computing in Industrial Applications. SMCia/08, P1, DOI 10.1109/SMCIA.2008.5045926; Kingma D.P., 2015, INT C LEARN REPR, P1; Lauer Martin, 2000, ICML; Levine S, 2016, J MACH LEARN RES, V17; Li Y., 2015, ICLR; Littman M. L., 2001, Cognitive Systems Research, V2, P55, DOI 10.1016/S1389-0417(01)00015-8; Maddison CJ, 2015, ICLR; Maravall D, 2013, ROBOT AUTON SYST, V61, P661, DOI 10.1016/j.robot.2012.09.016; Mataric MJ, 1997, AUTON ROBOT, V4, P73, DOI 10.1023/A:1008819414322; Melo F. S., 2011, MULTIAGENT SYSTEMS, P189; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Olfati-Saber R, 2007, P IEEE, V95, P215, DOI 10.1109/JPROC.2006.887293; Pearl J., 1982, AAAI; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Stone P., 1998, INT J HUMAN COMPUTER; Sukhbaatar S., 2015, CORR; Tampuu A., 2015, ARXIV151108779; Tan M., 1993, ICML; Tieleman T., 2012, COURSERA NEURAL NETW; Varshavskaya P, 2009, DISTRIBUTED AUTONOMOUS ROBOTIC SYSTEMS 8, P367, DOI 10.1007/978-3-642-00644-9_33; Wang X., 2002, ADV NEURAL INFORM PR, P1603; Weston Jason, 2016, ICLR; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xiong C., 2016, P INT C MACHINE LEAR; Zhang Y, 2013, 2013 INTERNATIONAL CONFERENCE ON MECHANICAL AND AUTOMATION ENGINEERING (MAEE 2013), P110, DOI 10.1109/MAEE.2013.37	37	3	3	4	16	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703034
C	Winner, K; Sheldon, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Winner, Kevin; Sheldon, Daniel			Probabilistic Inference with Generating Functions for Poisson Latent Variable Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ESTIMATING ABUNDANCE; COUNTS	Graphical models with latent count variables arise in a number of fields. Standard exact inference techniques such as variable elimination and belief propagation do not apply to these models because the latent variables have countably infinite support. As a result, approximations such as truncation or MCMC are employed. We present the first exact inference algorithms for a class of models with latent count variables by developing a novel representation of countably infinite factors as probability generating functions, and then performing variable elimination with generating functions. Our approach is exact, runs in pseudo-polynomial time, and is much faster than existing approximate techniques. It leads to better parameter estimates for problems in population ecology by avoiding error introduced by approximate likelihood computations.	[Winner, Kevin; Sheldon, Daniel] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA; [Sheldon, Daniel] Mt Holyoke Coll, Dept Comp Sci, S Hadley, MA 01075 USA	University of Massachusetts System; University of Massachusetts Amherst; Mount Holyoke College	Winner, K (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.	kwinner@cs.umass.edu; sheldon@cs.umass.edu			National Science Foundation [1617533]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported by the National Science Foundation under Grant No. 1617533.	Al-Osh M., 1987, J TIME SER ANAL, V8, P261, DOI DOI 10.1111/J.1467-9892.1987.TB00438.X; Bickson D., 2010, ADV NEURAL INFORM PR; Casella G, 2002, STAT INFERENCE, V2nd; Chandler RB, 2011, ECOLOGY, V92, P1429, DOI 10.1890/10-2433.1; Couturier T, 2013, J WILDLIFE MANAGE, V77, P454, DOI 10.1002/jwmg.499; Dail D, 2011, BIOMETRICS, V67, P577, DOI 10.1111/j.1541-0420.2010.01465.x; Dennis EB, 2015, BIOMETRICS, V71, P237, DOI 10.1111/biom.12246; EICK SG, 1993, OPER RES, V41, P731, DOI 10.1287/opre.41.4.731; Feller W., 1968, INTRO PROBABILITY TH, V3; Fiske IJ, 2011, J STAT SOFTW, V43, P1; Gross K, 2007, POPUL ECOL, V49, P191, DOI 10.1007/s10144-007-0034-8; Jensen F. V., 1990, COMPUTATIONAL STAT Q; Jha A., 2010, P 24 ANN C NEUR INF, P973; Kery M, 2009, J APPL ECOL, V46, P1163, DOI 10.1111/j.1365-2664.2009.01724.x; LAURITZEN SL, 1988, J ROY STAT SOC B MET, V50, P157; Mao YY, 2005, IEEE T INFORM THEORY, V51, P1635, DOI 10.1109/TIT.2005.846404; McKenzie E, 2003, HANDB STAT, V21, P573, DOI 10.1016/S0169-7161(03)21018-X; PEARL J, 1986, ARTIF INTELL, V29, P241, DOI 10.1016/0004-3702(86)90072-X; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Resnick S.I., 2013, ADVENTURES STOCHASTI; Royle JA, 2004, BIOMETRICS, V60, P108, DOI 10.1111/j.0006-341X.2004.00142.x; Shenoy P. P., 1990, UNCERTAINTY ARTIFICI; Weiss CH, 2008, ASTA-ADV STAT ANAL, V92, P319, DOI 10.1007/s10182-008-0072-3; Winner K, 2015, PR MACH LEARN RES, V37, P2512; Xue Y., 2016, 2016 INT C HARDWARES, P1; Zhang NL, 1994, P 10 CAN C ART INT; Zipkin EF, 2014, ECOLOGY, V95, P22, DOI 10.1890/13-1131.1; ZONNEVELD C, 1991, ECOL ENTOMOL, V16, P115, DOI 10.1111/j.1365-2311.1991.tb00198.x	28	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705001
C	Andoni, A; Indyk, P; Laarhoven, T; Razenshteyn, I; Schmidt, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Andoni, Alexandr; Indyk, Piotr; Laarhoven, Thijs; Razenshteyn, Ilya; Schmidt, Ludwig			Practical and Optimal LSH for Angular Distance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [1, 2]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [3] in practice. We also introduce a multiprobe version of this algorithm and conduct an experimental evaluation on real and synthetic data sets. We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions.	[Andoni, Alexandr] Columbia Univ, New York, NY 10027 USA; [Indyk, Piotr; Razenshteyn, Ilya; Schmidt, Ludwig] MIT, Cambridge, MA 02139 USA; [Laarhoven, Thijs] TU Eindhoven, Eindhoven, Netherlands	Columbia University; Massachusetts Institute of Technology (MIT); Eindhoven University of Technology	Andoni, A (corresponding author), Columbia Univ, New York, NY 10027 USA.				NSF; Simons Foundation	NSF(National Science Foundation (NSF)); Simons Foundation	We thank Michael Kapralov for many valuable discussions during various stages of this work. We also thank Stefanie Jegelka and Rasmus Pagh for helpful conversations. This work was supported in part by the NSF and the Simons Foundation. Work done in part while the first author was at the Simons Institute for the Theory of Computing.	Ailon N, 2014, DISCRETE COMPUT GEOM, V52, P780, DOI 10.1007/s00454-014-9632-3; Ailon N, 2009, SIAM J COMPUT, V39, P302, DOI 10.1137/060673096; Andoni A, 2015, TIGHT LOWER BOUNDS D; Andoni A., 2015, STOC; Andoni A., 2014, SODA; Charikar M. S., 2002, STOC; Dasgupta Anirban, 2011, KDD; DIACONIS P, 1987, ANN I H POINCARE-PR, V23, P397; Dubiner M, 2010, IEEE T INFORM THEORY, V57, P4166, DOI 10.1109/TIT.2010.2050814; Eshghi Kave, 2008, KDD; Feige U, 2002, RANDOM STRUCT ALGOR, V20, P403, DOI 10.1002/rsa.10036; Har-Peled S., 2012, THEORY COMPUT, V8, P321, DOI DOI 10.4086/TOC.2012.V008A014; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Lichman M., 2013, UCI MACHINE LEARNING; Lv Qin, 2007, VLDB; Motwani R, 2007, SIAM J DISCRETE MATH, V21, P930, DOI 10.1137/050646858; O'Donnell Ryan, 2014, ACM Transactions on Computation Theory, V6, DOI 10.1145/2578221; Samet H, 2006, MORGAN KAUFMANN SERI; Schmidt L., 2014, ICASSP; Shakhnarovich G., 2005, NEAREST NEIGHBOR MET; Shrivastava Anshumali, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P474, DOI 10.1007/978-3-642-33460-3_36; Shrivastava A., 2014, ICML; Slaney M, 2012, P IEEE, V100, P2604, DOI 10.1109/JPROC.2012.2193849; Sundaram N., 2013, VLDB; Terasawa K, 2007, LECT NOTES COMPUT SC, V4619, P27; Weinberger K., 2009, ICML	26	3	3	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102042
C	Bardenet, R; Titsias, MK		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Bardenet, Remi; Titsias, Michalis K.			Inference for determinantal point processes without spectral knowledge	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Determinantal point processes (DPPs) are point process models that naturally encode diversity between the points of a given realization, through a positive definite kernel K. DPPs possess desirable properties, such as exact sampling or analyticity of the moments, but learning the parameters of kernel K through likelihood-based inference is not straightforward. First, the kernel that appears in the likelihood is not K, but another kernel L related to K through an often intractable spectral decomposition. This issue is typically bypassed in machine learning by directly parametrizing the kernel L, at the price of some interpretability of the model parameters. We follow this approach here. Second, the likelihood has an intractable normalizing constant, which takes the form of a large determinant in the case of a DPP over a finite set of objects, and the form of a Fredholm determinant in the case of a DPP over a continuous domain. Our main contribution is to derive bounds on the likelihood of a DPP, both for finite and continuous domains. Unlike previous work, our bounds are cheap to evaluate since they do not rely on approximating the spectrum of a large matrix or an operator. Through usual arguments, these bounds thus yield cheap variational inference and moderately expensive exact Markov chain Monte Carlo inference methods for DPPs.	[Bardenet, Remi] Univ Lille, CNRS, Lille, France; [Bardenet, Remi] Univ Lille, CRIStAL, UMR 9189, Lille, France; [Titsias, Michalis K.] Athens Univ Econ & Business, Dept Informat, Athens, Greece	Centre National de la Recherche Scientifique (CNRS); Universite de Lille - ISITE; Universite de Lille; Universite de Lille - ISITE; Centrale Lille; Universite de Lille; Athens University of Economics & Business	Bardenet, R (corresponding author), Univ Lille, CNRS, Lille, France.	remi.bardenet@gmail.com; mtitsias@aueb.gr		Bardenet, Remi/0000-0002-1094-9493	EPSRC [EP/I017909/1]; ANR [ANR-13-BS-03-0006-01]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); ANR(French National Research Agency (ANR))	We would like to thank Adrien Hardy for useful discussions and Emily Fox for kindly providing access to the diabetic neuropathy dataset. RB was funded by a research fellowship through the 2020 Science programme, funded by EPSRC grant number EP/I017909/1, and by ANR project ANR-13-BS-03-0006-01.	Affandi R. H., 2013, P C ART INT STAT AIS; Affandi R. H., 2014, P INT C MACH LEARN I; Bornemann F, 2010, MATH COMPUT, V79, P871, DOI 10.1090/S0025-5718-09-02280-7; Cristianini N., 2004, KERNEL METHODS PATTE; Daley D., 2003, INTRO THEORY POINT P, V1; Devroye L., 1986, NONUNIFORM RANDOM VA, P61; Fasshauer G. E., 2012, SIAM J SCI COMPUTING, V34; Gillenwater J., 2014, ADV NEURAL INFORM PR; Gohberg I., 1990, CLASSES LINEAR OPERA, V1; Haario H, 2001, BERNOULLI, V7, P223, DOI 10.2307/3318737; Hansen N., 2006, NEW EVOLUTIONARY COM; Hough J. B., 2006, PROBABILITY SURVEYS; Kulesza A., 2012, FDN TRENDS MACHINE L; Lavancier F., 2014, J ROYAL STAT SOC; MACCHI O, 1975, ADV APPL PROBAB, V7, P83, DOI 10.2307/1425855; Mariet Z., 2015, ADV NEURAL INFORM SY; Michalis K., 2009, AISTATS, V5; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Robert C, 2004, MONTE CARLO STAT MET, DOI DOI 10.1007/978-1-4757-4145-2; Seiler E., 1975, P NATL ACAD SCI; Simon B., 2005, TRACE IDEALTHEIR A, V2; Waller LA, 2011, STAT MED, V30, P2827, DOI 10.1002/sim.4315; Zou James Y., 2012, ADV NEURAL INFORM PR	23	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101070
C	Bekker, J; Davis, J; Choi, A; Darwiche, A; Van den Broeck, G		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Bekker, Jessa; Davis, Jesse; Choi, Arthur; Darwiche, Adnan; Van den Broeck, Guy			Tractable Learning for Complex Probability Queries	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				INFERENCE; MODELS	Tractable learning aims to learn probabilistic models where inference is guaranteed to be efficient. However, the particular class of queries that is tractable depends on the model and underlying representation. Usually this class is MPE or conditional probabilities Pr(x vertical bar y) for joint assignments x, y. We propose a tractable learner that guarantees efficient inference for a broader class of queries. It simultaneously learns a Markov network and its tractable circuit representation, in order to guarantee and measure tractability. Our approach differs from earlier work by using Sentential Decision Diagrams (SDD) as the tractable language instead of Arithmetic Circuits (AC). SDDs have desirable properties, which more general representations such as ACs lack, that enable basic primitives for Boolean circuit compilation. This allows us to support a broader class of complex probability queries, including counting, threshold, and parity, in polytime.	[Bekker, Jessa; Davis, Jesse] Katholieke Univ Leuven, Leuven, Belgium; [Choi, Arthur; Darwiche, Adnan; Van den Broeck, Guy] Univ Calif Los Angeles, Los Angeles, CA USA	KU Leuven; University of California System; University of California Los Angeles	Bekker, J (corresponding author), Katholieke Univ Leuven, Leuven, Belgium.	jessa.bekker@cs.kuleuven.be; jesse.davis@cs.kuleuven.be; aychoi@cs.ucla.edu; darwiche@cs.ucla.edu; guyvdb@cs.ucla.edu	Bekker, Jessa/AAE-8144-2021	Bekker, Jessa/0000-0003-1928-7374; Davis, Jesse/0000-0002-3748-9263	IWT [SB/141744]; Research Fund KU Leuven [OT/11/051, C22/15/015]; EU FP7 Marie Curie CIG [294068]; FWO-Vlaanderen [G.0356.12]; NSF [IIS-1514253]; ONR [N00014-12-1-0423]; IWT (SBO-HYMOP)	IWT(Institute for the Promotion of Innovation by Science and Technology in Flanders (IWT)); Research Fund KU Leuven(KU Leuven); EU FP7 Marie Curie CIG; FWO-Vlaanderen(FWO); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); IWT (SBO-HYMOP)	We thank Songbai Yan for prior collaborations on related projects. JB is supported by IWT (SB/141744). JD is partially supported by the Research Fund KU Leuven (OT/11/051, C22/15/015), EU FP7 Marie Curie CIG (#294068), IWT (SBO-HYMOP) and FWO-Vlaanderen (G.0356.12). AC and AD are partially supported by NSF (#IIS-1514253) and ONR (#N00014-12-1-0423).	Bach F. R., 2001, ADV NEURAL INFORM PR, P569; Buchman D., 2015, P AAAI; Chavira M, 2008, ARTIF INTELL, V172, P772, DOI 10.1016/j.artint.2007.11.002; Chechetka A., 2007, ADV NEURAL INFORM PR, P273; Chen SM, 2014, J ARTIF INTELL RES, V49, P601, DOI 10.1613/jair.4218; Choi A., 2015, P IJCAI; Darwiche A, 2003, J ACM, V50, P280, DOI 10.1145/765568.765570; Darwiche A., 2011, P 22 INT JOINT C ART, P819, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-143; DellaPietra S, 1997, IEEE T PATTERN ANAL, V19, P380, DOI 10.1109/34.588021; Domingos P., 2014, ICML WORKSH LEARN TR; Gens R., 2013, 30 INT C MACHINE LEA, P873; Kisa D., 2014, KR; Krause C., 2005, P IJCAI; Kulesza A., 2012, FDN TRENDS MACHINE L; Lowd D., 2013, P 16 INT C ART INT S, P406; Lowd Daniel, 2014, J MACHINE LEARNING R, V15; Maua DD, 2013, ARTIF INTELL, V205, P30, DOI 10.1016/j.artint.2013.10.002; Narasimhan M., 2004, P UAI; Niepert M., 2014, P ICML; Niepert M., 2014, P AAAI; PARBERRY I, 1989, NEURAL NETWORKS, V2, P59, DOI 10.1016/0893-6080(89)90015-4; Park J. D., 2002, P UAI; Rahman Tahrima, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P630, DOI 10.1007/978-3-662-44851-9_40; Rooshenas A, 2014, PR MACH LEARN RES, V32; van der Gaag L., 2004, P 20 C UNCERTAINTY A, P569; Van Haaren J., 2015, MACHINE LEARNING; Zhang NL, 2004, J MACH LEARN RES, V5, P697	27	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100044
C	Campbell, T; Straub, J; Fisher, JW; How, JP		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Campbell, Trevor; Straub, Julian; Fisher, John W., III; How, Jonathan P.			Streaming, Distributed Variational Inference for Bayesian Nonparametrics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					This paper presents a methodology for creating streaming, distributed inference algorithms for Bayesian nonparametric (BNP) models. In the proposed framework, processing nodes receive a sequence of data minibatches, compute a variational posterior for each, and make asynchronous streaming updates to a central model. In contrast to previous algorithms, the proposed framework is truly streaming, distributed, asynchronous, learning-rate-free, and truncation-free. The key challenge in developing the framework, arising from the fact that BNP models do not impose an inherent ordering on their components, is finding the correspondence between minibatch and central BNP posterior components before performing each update. To address this, the paper develops a combinatorial optimization problem over component correspondences, and provides an efficient solution technique. The paper concludes with an application of the methodology to the DP mixture model, with experimental results demonstrating its practical scalability and performance.	[Campbell, Trevor; How, Jonathan P.] MIT, LIDS, Cambridge, MA 02139 USA; [Straub, Julian; Fisher, John W., III] MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Campbell, T (corresponding author), MIT, LIDS, Cambridge, MA 02139 USA.	tdjc@mit.edu; jstraub@csailmit.edu; fisher@csailmit.edu; jhow@mit.edu			Office of Naval Research under ONR MURI [N000141110688]	Office of Naval Research under ONR MURI(MURI)	This work was supported by the Office of Naval Research under ONR MURI grant N000141110688.	Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104; Broderick Tamara, 2013, ADV NEURAL INFORM PR, V26; Bryant Michael, 2009, ADV NEURAL INFORM PR, V23; Carvalho CM, 2010, BAYESIAN ANAL, V5, P709, DOI 10.1214/10-BA525; Chang Jason, 2013, ADV NEURAL INFORM PR, V26; Doshi-Velez Finale, 2009, P INT C MACH LEARN; Dubey Avinava, 2014, P 30 C UNC ART INT; EDMONDS J, 1972, J ACM, V19, P248, DOI 10.1145/321694.321699; Griffiths T. L., 2005, ADV NEURAL INFORM PR, V22; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Hughes Michael, 2013, ADV NEURAL INFORM PR, V26; Jasra A, 2005, STAT SCI, V20, P50, DOI 10.1214/088342305000000016; LeCun Yann, MNIST DATABASE HANDW; Lin D., 2013, ADV NEURAL INFORM PR, V26; Miller JW, 2013, ADV NEURAL INFORM PR, V26; Neiswanger W., 2014, P 30 C UNC ART INT; Nobile A., 1994, THESIS; PITMAN J, 1995, PROBAB THEORY REL, V102, P145, DOI 10.1007/BF01213386; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Teh Yee Whye, 2010, ENCY MACHINE LEARNIN; Trevor Campbell, 2014, P 30 C UNC ART INT; Wang Chong, 2011, P 11 INT C ART INT S; Wang Chong, 2012, ADV NEURAL INFORM PR, V25; Xiao Jianxiong, SUN 397 IMAGE DATABA; Zhang XL, 2014, J COMPUT GRAPH STAT, V23, P1143, DOI 10.1080/10618600.2013.870906	26	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102025
C	Cao, W; Li, J; Tao, YF; Li, ZZ		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Cao, Wei; Li, Jian; Tao, Yufei; Li, Zhize			On Top-k Selection in Multi-Armed Bandits and Hidden Bipartite Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					This paper discusses how to efficiently choose from n unknown distributions the k ones whose means are the greatest by a certain metric, up to a small relative error. We study the topic under two standard settings-multi-armed bandits and hidden bipartite graphs-which differ in the nature of the input distributions. In the former setting, each distribution can be sampled (in the i.i.d. manner) an arbitrary number of times, whereas in the latter, each distribution is defined on a population of a finite size m (and hence, is fully revealed after m samples). For both settings, we prove lower bounds on the total number of samples needed, and propose optimal algorithms whose sample complexities match those lower bounds.	[Cao, Wei; Li, Jian; Li, Zhize] Tsinghua Univ, Beijing, Peoples R China; [Tao, Yufei] Chinese Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China	Tsinghua University; Chinese University of Hong Kong	Cao, W (corresponding author), Tsinghua Univ, Beijing, Peoples R China.	cao-w13@mails.tsinghua.edu.cn; lijian83@mail.tsinghua.edu.cn; taoyf@cse.cuhk.edu.hk; zz-li14@mails.tsinghua.edu.cn		Tao, Yufei/0000-0003-3883-5452	National Basic Research Program of China [2015CB358700, 2011CBA00300, 2011CBA00301]; National NSFC [61202009, 61033001, 61361136003]; HKRGC [GRF 4168/13, GRF 142072/14]	National Basic Research Program of China(National Basic Research Program of China); National NSFC(National Natural Science Foundation of China (NSFC)); HKRGC(Hong Kong Research Grants Council)	Jian Li, Wei Cao, Zhize Li were supported in part by the National Basic Research Program of China grants 2015CB358700, 2011CBA00300, 2011CBA00301, and the National NSFC grants 61202009, 61033001, 61361136003. Yufei Tao was supported in part by projects GRF 4168/13 and GRF 142072/14 from HKRGC.	Amsterdamer Y, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P589, DOI 10.1145/2588555.2610514; Audibert Jean-Yves, 2010, P 23 ANN C LEARN THE, p2010a; Bar- Yossef Z., 2002, THESIS; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Bubeck S, 2011, THEOR COMPUT SCI, V412, P1832, DOI 10.1016/j.tcs.2010.12.059; Chen S., 2014, P ADV NEUR INF PROC, P379; Dubhashi DP, 2009, CONCENTRATION OF MEASURE FOR THE ANALYSIS OF RANDOMIZED ALGORITHMS, P1, DOI 10.1017/CBO9780511581274; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Kalyanakrishnan S., 2010, P 27 INT C MACH LEAR, P511; Kalyanakrishnan S, 2012, P 29 INT C MACH LEAR, P655; Lazaric A., 2012, ADV NEURAL INFORM PR, V25, P3212; Mannor S, 2004, J MACH LEARN RES, V5, P623; Parameswaran A, 2014, PROC VLDB ENDOW, V7, P685, DOI 10.14778/2732939.2732942; Sheng C., 2012, TODS, V37, P12; Wang JG, 2013, IEEE T KNOWL DATA EN, V25, P2245, DOI 10.1109/TKDE.2012.178; Zhou Y, 2014, PR MACH LEARN RES, V32, P217; Zhu ML, 2005, IEEE T KNOWL DATA EN, V17, P567, DOI 10.1109/TKDE.2005.65	17	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103065
C	Combes, R; Talebi, MS; Proutiere, A; Lelarge, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Combes, Richard; Talebi, M. Sadegh; Proutiere, Alexandre; Lelarge, Marc			Combinatorial Bandits Revisited	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				OPTIMIZATION	This paper investigates stochastic and adversarial combinatorial multi-armed bandit problems. In the stochastic setting under semi-bandit feedback, we derive a problem-specific regret lower bound, and discuss its scaling with the dimension of the decision space. We propose ESCB, an algorithm that efficiently exploits the structure of the problem and provide a finite-time analysis of its regret. ESCB has better performance guarantees than existing algorithms, and significantly outperforms these algorithms in practice. In the adversarial setting under bandit feedback, we propose COMBEXP, an algorithm with the same regret scaling as state-of-the-art algorithms, but with lower computational complexity for some combinatorial problems.	[Combes, Richard] Cent Supelec, L2S, Gif Sur Yvette, France; [Talebi, M. Sadegh; Proutiere, Alexandre] KTH, Dept Automat Control, Stockholm, Sweden; [Lelarge, Marc] INRIA, Paris, France; [Lelarge, Marc] ENS, Paris, France	UDICE-French Research Universities; Universite Paris Saclay; Royal Institute of Technology; Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Combes, R (corresponding author), Cent Supelec, L2S, Gif Sur Yvette, France.	richard.combes@supelec.fr; mstms@kth.se; alepro@kth.se; marc.lelarge@ens.fr			ERC FSA grant; SSF ICT-Psi project	ERC FSA grant; SSF ICT-Psi project	A. Proutiere's research is supported by the ERC FSA grant, and the SSF ICT-Psi project.	Ailon N, 2014, LECT NOTES ARTIF INT, V8776, P215, DOI 10.1007/978-3-319-11662-4_16; ANANTHARAM V, 1987, IEEE T AUTOMAT CONTR, V32, P968, DOI 10.1109/TAC.1987.1104491; [Anonymous], 2004, INFORM THEORY STAT T; Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Boyd S, 2004, CONVEX OPTIMIZATION; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Bubeck Sebastien, 2012, P COLT; Cesa-Bianchi N., 2006, PREDICTION LEARNING, V1; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Chen Wei, 2013, P ICML; Gai Y, 2012, IEEE ACM T NETWORK, V20, P1466, DOI 10.1109/TNET.2011.2181864; Gai Yi, 2010, P IEEE DYSPAN; Garivier Aurelien, 2011, P COLD; Graves TL, 1997, SIAM J CONTROL OPTIM, V35, P715, DOI 10.1137/S0363012994275440; Gyorgy A., 2007, J MACHINE LEARNING R, V8; Helmbold DP, 2009, J MACH LEARN RES, V10, P1705; Kale S., 2010, ADV NEURAL INFORM PR, V23, P1054; Kveton Branislav, 2014, P UAI; Kveton Branislav, 2015, P AISTATS; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Magureanu Stefan, 2014, P 27 C LEARNING THEO, P975; Neu Gergely, 2015, P COLT; Robbins Herbert, 1985, H ROBBINS SELECTED P, P169; Sherali H. D., 1987, American Journal of Mathematical and Management Sciences, V7, P253; Wen Zheng, 2015, P ICML	26	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101090
C	De Sa, C; Zhang, C; Olukotun, K; Re, C		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		De Sa, Christopher; Zhang, Ce; Olukotun, Kunle; Re, Christopher			Rapidly Mixing Gibbs Sampling for a Class of Factor Graphs Using Hierarchy Width	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Gibbs sampling on factor graphs is a widely used inference technique, which often produces good empirical results. Theoretical guarantees for its performance are weak: even for tree structured graphs, the mixing time of Gibbs may be exponential in the number of variables. To help understand the behavior of Gibbs sampling, we introduce a new (hyper)graph property, called hierarchy width. We show that under suitable conditions on the weights, bounded hierarchy width ensures polynomial mixing time. Our study of hierarchy width is in part motivated by a class of factor graph templates, hierarchical templates, which have bounded hierarchy width-regardless of the data used to instantiate them. We demonstrate a rich application from natural language processing in which Gibbs sampling provably mixes rapidly and achieves accuracy that exceeds human volunteers.	[De Sa, Christopher] Stanford Univ, Dept Elect Engn, Stanford, CA 94309 USA; Stanford Univ, Dept Comp Sci, Stanford, CA 94309 USA	Stanford University; Stanford University	De Sa, C (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94309 USA.	cdesa@stanford.edu; czhang@cs.wisc.edu; kunle@stanford.edu; chrismre@stanford.edu			DARPA [FA8750-13-2-0039, FA8750-12-2-0335]; NSF [IIS-1353606, IIS-1247701, CCF-1111943, CCF-1337375]; NIH [U54EB020405]; NVIDIA; Moore Foundation; American Family Insurance; Google; Toshiba; DOE [108845]; ONR [N000141210041, N000141310129]; Oracle; Huawei; SAP Labs; Sloan Research Fellowship	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NVIDIA; Moore Foundation(Gordon and Betty Moore Foundation); American Family Insurance; Google(Google Incorporated); Toshiba; DOE(United States Department of Energy (DOE)); ONR(Office of Naval Research); Oracle; Huawei(Huawei Technologies); SAP Labs; Sloan Research Fellowship(Alfred P. Sloan Foundation)	The authors acknowledge the support of: DARPA FA8750-12-2-0335; NSF IIS-1247701; NSF CCF-1111943; DOE 108845; NSF CCF-1337375; DARPA FA8750-13-2-0039; NSF IIS-1353606; ONR N000141210041 and N000141310129; NIH U54EB020405; Oracle; NVIDIA; Huawei; SAP Labs; Sloan Research Fellowship; Moore Foundation; American Family Insurance; Google; and Toshiba.	Chandrasekaran Venkat, 2012, ARXIV12063240; Diaconis P, 2008, STAT SCI, V23, P151, DOI 10.1214/07-STS252; Diaconis P, 2010, SANKHYA SER A, V72, P136, DOI 10.1007/s13171-010-0004-7; Domingos P. M, 2012, AAAI; Gonzalez J., 2011, P 14 INT C ART INT S, V15, P324; Gottlob G, 2014, TRACTABILITY, P3; Ihler AT, 2005, J MACH LEARN RES, V6, P905; Koch Christoph, 2011, SYNTHESIS LECT DATA, V2, P1, DOI [10.2200/s00362ed1v01y201105dtm016, DOI 10.2200/S00362ED1V01Y201105DTM016, 10.2200/S00362ED1V01Y201105DTM016]; Koller D., 2009, PROBABILISTIC GRAPHI; KWISTHOUT JHP, 2010, ECAI, V215, P237, DOI DOI 10.3233/978-1-60750-606-5-237; Levin D. A., 2009, MARKOV CHAINS MIXING; Liu Xianghang, 2014, ADV NEURAL INFORM PR, P1377; Lunn D, 2009, STAT MED, V28, P3049, DOI 10.1002/sim.3680; Marx D, 2013, J ACM, V60, DOI 10.1145/2535926; McCallum Andrew, 2009, ADV NEURAL INFORM PR, P1249; Newman D., 2007, ADV NEURAL INFORM PR, P1081; Ng KS, 2008, ANN MATH ARTIF INTEL, V54, P159, DOI 10.1007/s10472-009-9136-7; Peters SE, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0113523; Poole D., 2003, P INT JOINT C ART IN, P985; ROBERTSON N, 1986, J ALGORITHM, V7, P309, DOI 10.1016/0196-6774(86)90023-4; Shin J., 2015, PVLDB; Singla P., 2008, P 23 AAAI C ART INT, V8, P1094; Smola Alexander, 2010, PVLDB; Surdeanu Mihai, OVERVIEW ENGLISH SLO; Theis Lucas, 2012, NIPS, P1124; Venugopal D., 2012, NIPS, V3, P1655; Venugopal Deepak, 2015, AAAI C ART INT; Zhang C., 2014, PVLDB	28	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101017
C	Koolen, WM; Malek, A; Bartlett, PL; Abbasi-Yadkori, Y		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Koolen, Wouter M.; Malek, Alan; Bartlett, Peter L.; Abbasi-Yadkori, Yasin			Minimax Time Series Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				TRACKING	We consider an adversarial formulation of the problem of predicting a time series with square loss. The aim is to predict an arbitrary sequence of vectors almost as well as the best smooth comparator sequence in retrospect. Our approach allows natural measures of smoothness such as the squared norm of increments. More generally, we consider a linear time series model and penalize the comparator sequence through the energy of the implied driving noise terms. We derive the minimax strategy for all problems of this type and show that it can be implemented efficiently. The optimal predictions are linear in the previous observations. We obtain an explicit expression for the regret in terms of the parameters defining the problem. For typical, simple definitions of smoothness, the computation of the optimal predictions involves only sparse matrices. In the case of norm-constrained data, where the smoothness is defined in terms of the squared norm of the comparator's increments, we show that the regret grows as T/root lambda(T), where T is the length of the game and lambda(T) is an increasing limit on comparator smoothness.	[Koolen, Wouter M.] Ctr Wiskunde & Informat, Amsterdam, Netherlands; [Malek, Alan; Bartlett, Peter L.] Univ Calif Berkeley, Berkeley, CA USA; [Bartlett, Peter L.; Abbasi-Yadkori, Yasin] QUT, Brisbane, Qld, Australia	University of California System; University of California Berkeley; Queensland University of Technology (QUT)	Koolen, WM (corresponding author), Ctr Wiskunde & Informat, Amsterdam, Netherlands.	wmkoolen@cwi.nl; malek@berkeley.edu; bartlett@cs.berkeley.edu; yasin.abbasiyadkori@qut.edu.au			NSF [CCF-1115788]; Australian Research Council through an Australian Laureate Fellowship [FL110100281]; Australian Research Council through ARC Centre of Excellence for Mathematical and Statistical Frontiers	NSF(National Science Foundation (NSF)); Australian Research Council through an Australian Laureate Fellowship(Australian Research Council); Australian Research Council through ARC Centre of Excellence for Mathematical and Statistical Frontiers(Australian Research Council)	We gratefully acknowledge the support of the NSF through grant CCF-1115788, and of the Australian Research Council through an Australian Laureate Fellowship (FL110100281) and through the ARC Centre of Excellence for Mathematical and Statistical Frontiers. Thanks also to the Simons Institute for the Theory of Computing Spring 2015 Information Theory Program.	Abernethy J., 2008, PROC 19 ANN C LEARNI, P415; [Anonymous], 2012, ADV NEURAL INFORM PR; BARTLETT P. L., 2015, C LEARN THEORY, P226; Blum A, 2000, MACH LEARN, V39, P35, DOI 10.1023/A:1007621832648; Bousquet O., 2003, Journal of Machine Learning Research, V3, P363, DOI 10.1162/153244303321897654; Chaudhuri Kamalika, 2010, P 26 C UNC ART INT U, P101; Herbster M, 2001, J MACH LEARN RES, V1, P281, DOI 10.1162/153244301753683726; Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876; Hu GY, 1996, J PHYS A-MATH GEN, V29, P1511, DOI 10.1088/0305-4470/29/7/020; Koolen WM, 2014, ADV NEURAL INFORM PR, P3230; Monteleoni Claire, 2003, THESIS; Moroshko Edward, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P245, DOI 10.1007/978-3-642-34106-9_21; Moroshko Edward, 2013, J MACH LEARN RES, P451; TAKIMOTO EIJI, 2000, 13 COLT, P100	14	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100098
C	Li, H; Lin, ZC		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Li, Huan; Lin, Zhouchen			Accelerated Proximal Gradient Methods for Nonconvex Programming	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				VARIABLE SELECTION; ALGORITHMS; MINIMIZATION	Nonconvex and nonsmooth problems have recently received considerable attention in signal/image processing, statistics and machine learning. However, solving the nonconvex and nonsmooth optimization problems remains a big challenge. Accelerated proximal gradient (APG) is an excellent method for convex programming. However, it is still unknown whether the usual APG can ensure the convergence to a critical point in nonconvex programming. In this paper, we extend APG for general nonconvex and nonsmooth programs by introducing a monitor that satisfies the sufficient descent property. Accordingly, we propose a monotone APG and a nonmonotone APG. The latter waives the requirement on monotonic reduction of the objective function and needs less computation in each iteration. To the best of our knowledge, we are the first to provide APG-type algorithms for general nonconvex and nonsmooth problems ensuring that every accumulation point is a critical point, and the convergence rates remain O(1/k(2)) when the problems are convex, in which k is the number of iterations. Numerical results testify to the advantage of our algorithms in speed.	[Lin, Zhouchen] Peking Univ, Sch EECS, Key Lab Machine Percept MOE, Beijing, Peoples R China; Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai, Peoples R China	Peking University; Shanghai Jiao Tong University	Lin, ZC (corresponding author), Peking Univ, Sch EECS, Key Lab Machine Percept MOE, Beijing, Peoples R China.	lihuanss@pku.edu.cn; zlin@pku.edu.cn			National Basic Research Program of China (973 Program) [2015CB352502]; National Natural Science Foundation (NSF) of China [61272341, 61231002]; Microsoft Research Asia Collaborative Research Program	National Basic Research Program of China (973 Program)(National Basic Research Program of China); National Natural Science Foundation (NSF) of China(National Natural Science Foundation of China (NSFC)); Microsoft Research Asia Collaborative Research Program(Microsoft)	Zhouchen Lin is supported by National Basic Research Program of China (973 Program) (grant no. 2015CB352502), National Natural Science Foundation (NSF) of China (grant nos. 61272341 and 61231002), and Microsoft Research Asia Collaborative Research Program. He is the corresponding author.	Attouch H, 2013, MATH PROGRAM, V137, P91, DOI 10.1007/s10107-011-0484-9; Beck A, 2009, IEEE T IMAGE PROCESS, V18, P2419, DOI 10.1109/TIP.2009.2028250; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bolte J, 2014, MATH PROGRAM, V146, P459, DOI 10.1007/s10107-013-0701-9; Bot R. L., 2014, INERTIAL FORWARD BAC, V2, P7; Candes EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Foucart S, 2009, APPL COMPUT HARMON A, V26, P395, DOI 10.1016/j.acha.2008.09.001; Frankel P, 2015, J OPTIMIZ THEORY APP, V165, P874, DOI 10.1007/s10957-014-0642-3; GEMAN D, 1995, IEEE T IMAGE PROCESS, V4, P932, DOI 10.1109/83.392335; Genkin A, 2007, TECHNOMETRICS, V49, P291, DOI 10.1198/004017007000000245; Ghadimi S., 2013, ARXIV13103787, p[1, 2]; Gong Pinghua, 2013, JMLR Workshop Conf Proc, V28, P37; Mohan K, 2012, J MACH LEARN RES, V13, P3441; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y. E., 2007, TECHNICAL REPORT, V1; Ochs P., 2014, SIAM J IMAGING SCI, P2; Ochs P, 2014, SIAM J IMAGING SCI, V7, P1388, DOI 10.1137/130942954; Shevade SK, 2003, BIOINFORMATICS, V19, P2246, DOI 10.1093/bioinformatics/btg308; Tseng P., 2008, TECHNICAL REPORT, V1; Yun F., 2014, LOW RANK SPARSE MODE, V1; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; Zhang HC, 2004, SIAM J OPTIMIZ, V14, P1043, DOI 10.1137/S1052623403428208; Zhang T, 2010, J MACH LEARN RES, V11, P1081; Zhong W., 2014, AAAI, P2	26	3	3	1	13	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100096
C	Lin, H; Mairal, J; Harchaoui, Z		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Lin, Hongzhou; Mairal, Julien; Harchaoui, Zaid			A Universal Catalyst for First-Order Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				ALGORITHM; INEXACT	We introduce a generic scheme for accelerating first-order optimization methods in the sense of Nesterov, which builds upon a new analysis of the accelerated proximal point algorithm. Our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. This strategy applies to a large class of algorithms, including gradient descent, block coordinate descent, SAG, SAGA, SDCA, SVRG, Finito/MISO, and their proximal variants. For all of these methods, we provide acceleration and explicit support for non-strongly convex objectives. In addition to theoretical speed-up, we also show that acceleration is useful in practice, especially for ill-conditioned problems where we measure significant improvements.	[Lin, Hongzhou; Mairal, Julien; Harchaoui, Zaid] INRIA, Paris, France; [Harchaoui, Zaid] NYU, New York, NY 10003 USA	Inria; New York University	Lin, H (corresponding author), INRIA, Paris, France.	hongzhou.lin@inria.fr; julien.mairal@inria.fr; zaid.harchaoui@nyu.edu	Mairal, Julien/AAL-5611-2021		ANR [MACARON ANR-14-CE23-0003-01]; MSR-Inria joint centre; CNRS-Mastodons program (Titan); NYU Moore-Sloan Data Science Environment	ANR(French National Research Agency (ANR)); MSR-Inria joint centre; CNRS-Mastodons program (Titan); NYU Moore-Sloan Data Science Environment	This work was supported by ANR (MACARON ANR-14-CE23-0003-01), MSR-Inria joint centre, CNRS-Mastodons program (Titan), and NYU Moore-Sloan Data Science Environment.	Agarwal A., 2015, P INT C MACH LEARN I; [Anonymous], 2012, ARXIV12112717; [Anonymous], ARXIV150702000; Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015; Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bertsekas D., 2015, CONVEX OPTIMIZATION; Defazio A., 2014, P INT C MACH LEARN I; Defazio Aaron, 2014, ADV NEURAL INFORM PR; Frostig R., 2015, P INT C MACH LEARN I; Guler O, 1992, SIAM J OPTIMIZ, V2, P649, DOI 10.1137/0802032; He BS, 2012, J OPTIMIZ THEORY APP, V154, P536, DOI 10.1007/s10957-011-9948-6; Hiriart-Urruty JB., 1996, OPTIMISATION; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z; Salzo S, 2012, J CONVEX ANAL, V19, P1167; Schmidt Mark, 2013, ARXIV13092388; Shalev- Shwartz S., 2015, MATH PROGRAMMING; Sra S, 2012, OPTIMIZATION FOR MACHINE LEARNING, P1; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zhang Y., 2015, P INT C MACH LEARN I	28	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102076
C	Qu, Z; Richtarik, P; Zhang, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Qu, Zheng; Richtarik, Peter; Zhang, Tong			Quartz: Randomized Dual Coordinate Ascent with Arbitrary Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA			empirical risk minimization; dual coordinate ascent; arbitrary sampling; data-driven speedup	OPTIMIZATION	We study the problem of minimizing the average of a large number of smooth convex functions penalized with a strongly convex regularizer. We propose and analyze a novel primal-dual method (Quartz) which at every iteration samples and updates a random subset of the dual variables, chosen according to an arbitrary distribution. In contrast to typical analysis, we directly bound the decrease of the primal-dual error (in expectation), without the need to first analyze the dual error. Depending on the choice of the sampling, we obtain efficient serial and mini-batch variants of the method. In the serial case, our bounds match the best known bounds for SDCA (both with uniform and importance sampling). With standard mini-batching, our bounds predict initial data-independent speedup as well as additional data-driven speedup which depends on spectral and sparsity properties of the data.	[Qu, Zheng] Univ Hong Kong, Dept Math, Hong Kong, Hong Kong, Peoples R China; [Richtarik, Peter] Univ Edinburgh, Sch Math, Edinburgh EH9 3FD, Midlothian, Scotland; [Zhang, Tong] Rutgers State Univ, Dept Stat, Piscataway, NJ 08854 USA	University of Hong Kong; University of Edinburgh; Rutgers State University New Brunswick	Qu, Z (corresponding author), Univ Hong Kong, Dept Math, Hong Kong, Hong Kong, Peoples R China.	zhengqu@maths.hku.hk; peter.richtarik@ed.ac.uk; tzhang@stat.rutgers.edu	Zhang, Tong/HGC-1090-2022; Richtarik, Peter/O-5797-2018					[Anonymous], 2012, ARXIV12112717; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Fercoq O., 2013, ARXIV13095885; Fercoq O., 2013, SIAM J OPTIMIZATION; Hsieh C.-J., 2008, P 25 INT C MACH LEAR, P408, DOI [10.1145/1390156.1390208, DOI 10.1145/1390156.1390208]; Jaggi M., 2014, ADV NEURAL INF PROCE, V4, P3068; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Konecny J., 2014, ARXIV14104744; Konecny J., 2013, ARXIV PREPRINT ARXIV; Lin Qihang, 2014, NIPS, P3059; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Qu Z., 2014, ARXIV14128060; QU Z, 2014, ARXIV14128063; Richtarik P., 2015, MATH PROGRAM; Richtarik P., 2015, OPTIMIZATION LETT; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Schmidt Mark, 2013, ARXIV13092388; Shalev-Shwartz S., 2013, ADV NEURAL INFORM PR, P378; Taka c. M, 2013, P 30 INT C MACH LEAR, V28, P1022; Yang T., 2013, ADV NEURAL INFORM PR, P629; Zhang T., 2004, P 21 INT C MACH LEAR, P116, DOI 10.1145/1015330.1015332; Zhang YC, 2015, PR MACH LEARN RES, V37, P353; Zhao Peilin, 2015, ICML	27	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102074
C	Ren, JSJ; Xu, L; Yan, Q; Sun, WX		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Ren, Jimmy S. J.; Xu, Li; Yan, Qiong; Sun, Wenxiu			Shepard Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Deep learning has recently been introduced to the field of low-level computer vision and image processing. Promising results have been obtained in a number of tasks including super-resolution, inpainting, deconvolution, filtering, etc. However, previously adopted neural network approaches such as convolutional neural networks and sparse auto-encoders are inherently with translation invariant operators. We found this property prevents the deep learning approaches from outperforming the state-of-the-art if the task itself requires translation variant interpolation (TVI). In this paper, we draw on Shepard interpolation and design Shepard Convolutional Neural Networks (ShCNN) which efficiently realizes endto- end trainable TVI operators in the network. We show that by adding only a few feature maps in the new Shepard layers, the network is able to achieve stronger results than a much deeper architecture. Superior performance on both image inpainting and super-resolution is obtained where our system outperforms previous ones while keeping the running time competitive.	[Ren, Jimmy S. J.; Xu, Li; Yan, Qiong; Sun, Wenxiu] SenseTime Grp Ltd, Hong Kong, Hong Kong, Peoples R China		Ren, JSJ (corresponding author), SenseTime Grp Ltd, Hong Kong, Hong Kong, Peoples R China.	rensijie@sensetime.com; xuli@sensetime.com; yanqiong@sensetime.com; sunwenxiu@sensetime.com						Bevilacqua M., 2012, BMVC; Burger H.C., 2012, CVPR; Chang H., 2004, CVPR; Dong Chao, 2014, ECCV; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Eigen D., 2013, ICCV; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y., 1998, P IEEE; Shepard D., 1968, 23 ACM NAT C; Sun Y, 2015, ARXIV150200873; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; Timofte R., 2014, P AS C COMP VIS SING; Timofte R., 2013, ICCV; Xie Junyuan, 2012, ADV NEURAL INFORM PR, V25; Xu L., 2014, NIPS; Xu L., 2015, ICML; Zeyde Roman, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47	17	3	3	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101034
C	Sarkar, P; Chakrabarti, D; Bickel, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Sarkar, Purnamrita; Chakrabarti, Deepayan; Bickel, Peter			The Consistency of Common Neighbors for Link Prediction in Stochastic Blockmodels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Link prediction and clustering are key problems for network-structured data. While spectral clustering has strong theoretical guarantees under the popular stochastic blockmodel formulation of networks, it can be expensive for large graphs. On the other hand, the heuristic of predicting links to nodes that share the most common neighbors with the query node is much fast, and works very well in practice. We show theoretically that the common neighbors heuristic can extract clusters with high probability when the graph is dense enough, and can do so even in sparser graphs with the addition of a "cleaning" step. Empirical results on simulated and real-world data support our conclusions.	[Sarkar, Purnamrita] Univ Texas Austin, Dept Stat, Austin, TX 78712 USA; [Chakrabarti, Deepayan; Bickel, Peter] Univ Texas Austin, McCombs Sch Business, IROM, Austin, TX 78712 USA	University of Texas System; University of Texas Austin; University of Texas System; University of Texas Austin	Sarkar, P (corresponding author), Univ Texas Austin, Dept Stat, Austin, TX 78712 USA.	purnamritas@austin.utexas.edu; deepay@utexas.edu; bickel@stat.berkeley.edu		Chakrabarti, Deepayan/0000-0002-3863-4928				Adamic LA, 2003, SOC NETWORKS, V25, P211, DOI 10.1016/S0378-8733(03)00009-1; [Anonymous], [No title captured]; Backstrom Lars, 2011, P 4 ACM INT C WEB SE, P635; Bickel PJ, 2009, P NATL ACAD SCI USA, V106, P21068, DOI 10.1073/pnas.0907096106; Chaudhuri Kamalika, 2012, J MACHINE LEARNING R, P35; Handcock MS, 2007, J ROY STAT SOC A STA, V170, P301, DOI 10.1111/j.1467-985X.2007.00471.x; Hoff PD, 2002, J AM STAT ASSOC, V97, P460; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Katz L., 1953, PSYCHOMETRIKA, V18, P39, DOI [10.1007/BF02289026, DOI 10.1007/BF02289026, DOI 10.1016/j.clinph.2016.12.016]; Lu L, 2011, PHYSICA A, V390; McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929; Olhede SC, 2014, P NATL ACAD SCI USA, V111, P14722, DOI 10.1073/pnas.1400374111; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; Sarkar P., 2007, P UAI; Sarkar P., 2014, ANN STAT; Sarkar Purnamrita, 2010, C LEARN THEOR	16	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101069
C	Shibagaki, A; Suzuki, Y; Karasuyama, M; Takeuchi, I		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Shibagaki, Atsushi; Suzuki, Yoshiki; Karasuyama, Masayuki; Takeuchi, Ichiro			Regularization Path of Cross-Validation Error Lower Bounds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Careful tuning of a regularization parameter is indispensable in many machine learning tasks because it has a significant impact on generalization performances. Nevertheless, current practice of regularization parameter tuning is more of an art than a science, e.g., it is hard to tell how many grid-points would be needed in cross-validation (CV) for obtaining a solution with sufficiently small CV error. In this paper we propose a novel framework for computing a lower bound of the CV errors as a function of the regularization parameter, which we call regularization path of CV error lower bounds. The proposed framework can be used for providing a theoretical approximation guarantee on a set of solutions in the sense that how far the CV error of the current best solution could be away from best possible CV error in the entire range of the regularization parameters. Our numerical experiments demonstrate that a theoretically guaranteed choice of a regularization parameter in the above sense is possible with reasonable computational costs.	[Shibagaki, Atsushi; Suzuki, Yoshiki; Karasuyama, Masayuki; Takeuchi, Ichiro] Nagoya Inst Technol, Nagoya, Aichi 4668555, Japan	Nagoya Institute of Technology	Shibagaki, A (corresponding author), Nagoya Inst Technol, Nagoya, Aichi 4668555, Japan.	shibagaki.a.mllab.nit@gmail.com; suzuki.mllab.nit@gmail.com; karasuyama@nitech.ac.jp; takeuchi.ichiro@nitech.ac.jp						Bergstra J, 2012, J MACH LEARN RES, V13, P281; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chapelle O, 2002, MACH LEARN, V46, P131, DOI 10.1023/A:1012450327387; Chapelle O, 2007, NEURAL COMPUT, V19, P1155, DOI 10.1162/neco.2007.19.5.1155; Chung K., 2003, NEURAL COMPUTATION; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; El Ghaoui L, 2012, PAC J OPTIM, V8, P667; Giesen J., 2012, ADV NEURAL INFORM PR; Giesen J., 2014, INT C MACH LEARN; Giesen J, 2012, ACM T ALGORITHMS, V9, DOI 10.1145/2390176.2390186; Hastie T, 2004, J MACH LEARN RES, V5, P1391; Joachims T., 2000, INT C MACH LEARN; Lee MMS, 2004, IEEE T NEURAL NETWOR, V15, P750, DOI 10.1109/TNN.2004.824266; Lin CJ, 2008, J MACH LEARN RES, V9, P627; Liu J, 2014, PR MACH LEARN RES, V32, P289; Mairal J., 2012, INT C MACH LEARN; Rosset S, 2007, ANN STAT, V35, P1012, DOI 10.1214/009053606000001370; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Takeuchi, 2013, P 30 INT C MACH LEAR, P1382; Vapnik V, 2000, NEURAL COMPUT, V12, P2013, DOI 10.1162/089976600300015042; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Wang J, 2014, ADV NEUR IN, V27; Xiang Z., 2011, ADV NEURAL INFORM PR, V24, P900	25	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101076
C	Sindhwani, V; Sainath, TN; Kumar, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Sindhwani, Vikas; Sainath, Tara N.; Kumar, Sanjiv			Structured Transforms for Small-Footprint Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices. We propose a unified framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank. Our structured transforms admit fast function and gradient evaluation, and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured. Experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training, and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques. In keyword spotting applications in mobile speech recognition, our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models, while providing more than 3.5-fold compression.	[Sindhwani, Vikas; Sainath, Tara N.; Kumar, Sanjiv] Google, New York, NY 10011 USA	Google Incorporated	Sindhwani, V (corresponding author), Google, New York, NY 10011 USA.	sindhwani@google.com; tsainath@google.com; sanjivk@google.com						[Anonymous], 2015, SUPPLEMENTARY MAT ST; Chen G., 2014, ICASSP; Chen W., 2015, ICML; Cheng Y., 2015, ARXIV150203436; Ciresan DC, 2011, ARXIV11020183; Collins M. D., 2013, ICASSP; Courbariaux M., 2015, ICLR; Dean J., 2012, ADV NEURAL INFORM PR, V25; Denil M., 2013, NIPS; Gray R. M., 2005, FDN TRENDS COMMUNICA; Hinton G., 2014, NIPS WORKSH; KAILATH T, 1995, SIAM REV, V37, P297, DOI 10.1137/1037082; KAILATH T, 1979, J MATH ANAL APPL, V68, P395, DOI 10.1016/0022-247X(79)90124-0; Kailath T., 1994, SIAM J MATRIX ANAL A, V15; Larochelle Hugo, 2007, ICML; Le Q., 2013, ICML; Liberty E., 2009, INFORM PROCESSING LE; Pan V.Y., 2001, STRUCTURED MATRICES, DOI [10.1007/978-1-4612-0129-8, DOI 10.1007/978-1-4612-0129-8]; Pan VY, 2003, SIAM J MATRIX ANAL A, V24, P660, DOI 10.1137/S089547980238627X; Rahimi A., 2007, NIPS; Rakhuba MV, 2015, SIAM J SCI COMPUT, V37, pA565, DOI 10.1137/140958529; Sainath T. N., 2013, ICASSP; Sainath T. N., 2015, P 16 ANN C INT SPEEC; Vanhoucke V., 2011, NIPS WORKSH DEEP LEA; Yang Z., 2015, ARXIV14127149	25	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102018
C	Soma, T; Yoshida, Y		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Soma, Tasuku; Yoshida, Yuichi			A Generalization of Submodular Cover via the Diminishing Return Property on the Integer Lattice	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We consider a generalization of the submodular cover problem based on the concept of diminishing return property on the integer lattice. We are motivated by real scenarios in machine learning that cannot be captured by (traditional) submodular set functions. We show that the generalized submodular cover problem can be applied to various problems and devise a bicriteria approximation algorithm. Our algorithm is guaranteed to output a log-factor approximate solution that satisfies the constraints with the desired accuracy. The running time of our algorithm is roughly O (n log(nr) log r), where n is the size of the ground set and r is the maximum value of a coordinate. The dependency on r is exponentially better than the naive reduction algorithms. Several experiments on real and artificial datasets demonstrate that the solution quality of our algorithm is comparable to naive algorithms, while the running time is several orders of magnitude faster.	[Soma, Tasuku] Univ Tokyo, Tokyo, Japan; [Yoshida, Yuichi] Natl Inst Informat, Tokyo, Japan; [Yoshida, Yuichi] Preferred Infrastruct Inc, Tokyo, Japan	University of Tokyo; Research Organization of Information & Systems (ROIS); National Institute of Informatics (NII) - Japan	Soma, T (corresponding author), Univ Tokyo, Tokyo, Japan.	tasukusoma@mist.i.u-tokyo.ac.jp; yyoshida@nii.ac.jp	Soma, Tasuku/AAI-5374-2020	Soma, Tasuku/0000-0001-9519-2487	JSPS [26730009]; MEXT [24106003]; JST, ERATO, Kawarabayashi Large Graph Project	JSPS(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science); MEXT(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)); JST, ERATO, Kawarabayashi Large Graph Project	The first author is supported by JSPS Grant-in-Aid for JSPS Fellows. The second author is supported by JSPS Grant-in-Aid for Young Scientists (B) (No. 26730009), MEXT Grant-in-Aid for Scientific Research on Innovative Areas (24106003), and JST, ERATO, Kawarabayashi Large Graph Project. The authors thank Satoru Iwata and Yuji Nakatsukasa for reading a draft of this paper.	Alon N., 2012, P 21 INT C WORLD WID, P381; Badanidiyuru A., 2014, P 25 ANN ACM SIAM S, P1497; Chen YX, 2014, PR MACH LEARN RES, V32; Iyer Rishabh K., 2013, P 8 C WORKSHOP NEURA, P2436; Kapralov M., 2012, P SODA, P1216; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Krause A, 2008, J WATER RES PL-ASCE, V134, P516, DOI 10.1061/(ASCE)0733-9496(2008)134:6(516); Krause A, 2008, J MACH LEARN RES, V9, P235; Krause A, 2014, TRACTABILITY, P71; Leskovec J, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P420; Lin H., 2010, HUMAN LANGUAGE TECHN, P912; Lin Hui, 2011, P 49 ANN M ASS COMP, P510; Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234, DOI 10.1007/BFb0006528; Ostfeld A, 2008, J WATER RES PLAN MAN, V134, P556, DOI 10.1061/(ASCE)0733-9496(2008)134:6(556); Raz R., 1997, P 20 9 ANN ACM S THE, V97, P475, DOI 10.1145/258533.258641; Soma T., 2014, P ICML; Song Hyun Oh, 2014, P ICML; Sviridenko M., 2015, PROC 26 ACM SIAM S D, P1134; Wan P.-J., 2009, COMPUTATIONAL OPTIMI, V45, P463; WOLSEY LA, 1982, COMBINATORICA, V2, P385, DOI 10.1007/BF02579435	20	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102075
C	Strathmann, H; Sejdinovic, D; Livingstone, S; Szabo, Z; Gretton, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Strathmann, Heiko; Sejdinovic, Dino; Livingstone, Samuel; Szabo, Zoltan; Gretton, Arthur			Gradient-free Hamiltonian Monte Carlo with Efficient Kernel Exponential Families	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				CONVERGENCE; HASTINGS	We propose Kernel Hamiltonian Monte Carlo (KMC), a gradient-free adaptive MCMC algorithm based on Hamiltonian Monte Carlo (HMC). On target densities where classical HMC is not an option due to intractable gradients, KMC adaptively learns the target's gradient structure by fitting an exponential family model in a Reproducing Kernel Hilbert Space. Computational costs are reduced by two novel efficient approximations to this gradient. While being asymptotically exact, KMC mimics HMC in terms of sampling efficiency, and offers substantial mixing improvements over state-of-the-art gradient free samplers. We support our claims with experimental studies on both toy and real-world applications, including Approximate Bayesian Computation and exact-approximate MCMC.	[Strathmann, Heiko; Szabo, Zoltan; Gretton, Arthur] UCL, Gatsby Unit, London, England; [Sejdinovic, Dino] Univ Oxford, Dept Stat, Oxford, England; [Livingstone, Samuel] Univ Bristol, Sch Math, Bristol, Avon, England	University of London; University College London; University of Oxford; University of Bristol	Strathmann, H (corresponding author), UCL, Gatsby Unit, London, England.		Livingstone, Samuel/AAK-3873-2020	Livingstone, Samuel/0000-0002-7277-086X; Gretton, Arthur/0000-0003-3169-7624				Andrieu C, 2008, STAT COMPUT, V18, P343, DOI 10.1007/s11222-008-9110-y; Andrieu C, 2009, ANN STAT, V37, P697, DOI 10.1214/07-AOS574; Beaumont MA, 2003, GENETICS, V164, P1139; Berlinet A., 2004, REPRODUCING KERNEL H, DOI [10.1007/978-1-4419-9096-9, DOI 10.1007/978-1-4419-9096-9]; Betancourt M., 2015, ARXIV150201510; Betancourt M.J., 2015, OPTIMIZING INTEGRATO; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Filippone M., 2014, IEEE T PATTERN ANAL; Gretton A, 2012, J MACH LEARN RES, V13, P723; Haario H, 1999, COMPUTATION STAT, V14, P375, DOI 10.1007/s001800050022; Hyvarinen A, 2005, J MACH LEARN RES, V6, P695; Hyvarinen A, 2007, COMPUT STAT DATA AN, V51, P2499, DOI 10.1016/j.csda.2006.09.003; Le Q., 2013, ICML; Lichman M, 2013, UCI MACHINE LEARNING; Marjoram P, 2003, P NATL ACAD SCI USA, V100, P15324, DOI 10.1073/pnas.0306899100; Meeds E., 2015, UAI; Mengersen KL, 1996, ANN STAT, V24, P101; Neal R. M., 2011, HDB MARKOV CHAIN MON, V2; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Rasmussen CE, 2003, BAYESIAN STATISTICS 7, P651; Roberts GO, 2007, J APPL PROBAB, V44, P458, DOI 10.1239/jap/1183667414; Roberts GO, 1996, BIOMETRIKA, V83, P95, DOI 10.1093/biomet/83.1.95; Sejdinovic D., 2014, ICML; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Sisson S. A., 2010, HDB MARKOV CHAIN MON; Sriperumbudur B., 2014, ARXIV13123516; Sriperumbudur B. K., 2015, NIPS; Wood SN, 2010, NATURE, V466, P1102, DOI 10.1038/nature09319; Zhang C., 2015, ARXIV150605555	30	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102039
C	Theis, L; Bethge, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Theis, Lucas; Bethge, Matthias			Generative Image Modeling Using Spatial LSTMs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multidimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.	[Theis, Lucas; Bethge, Matthias] Univ Tubingen, D-72076 Tubingen, Germany	Eberhard Karls University of Tubingen	Theis, L (corresponding author), Univ Tubingen, D-72076 Tubingen, Germany.	lucas@bethgelab.org; matthias@bethgelab.org			German Research Foundation (DFG) [1527, BE 3848/2-1]	German Research Foundation (DFG)(German Research Foundation (DFG))	The authors would like to thank Aaron van den Oord for insightful discussions and Wieland Brendel, Christian Behrens, and Matthias Kummerer for helpful input on this paper. This study was financially supported by the German Research Foundation (DFG; priority program 1527, BE 3848/2-1).	Ba J., 2017, P 3 INT C LEARN REPR; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Brodatz P., 1966, TEXTURES PHOTOGRAPHI; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; Denton E, 2015, ADV NEUR IN, V28; Domke Justin, 2008, CVPR; Donahue J., 2014, ICML 31; Gerhard HE, 2015, BIOL INSPIRED COMPUT; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A., 2009, ADV NEURAL INFORM PR, V22; Gregor K., 2015, P 32 INT C MACH LEAR; Gregor K., 2014, P 31 INT C MACH LEAR; Heess N., 2009, BMCV; Hinton G. E., 2006, NEURAL COMP; Hochreiter S., 1997, NEURAL COMPUTATION, V9; Hosseini R., 2010, VIS RES; Hyvarinen A, 2000, NEURAL COMPUT, V12, P1705, DOI 10.1162/089976600300015312; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Kingma DP, 2014, ADV NEUR IN, V27; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Larochelle H., 2011, P 14 INT C ART INT S; Lee Ann B, 2001, INT J COMPUTER VISIO, P2; Lee H., 2009, ICML 26; Li Y., 2015, ICML 32; Martin D. R., 2001, ICCV; Matheron Georges, 1968, TECHNICAL REPORT, V2, P3; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Ngiam J., 2011, ICML 28; OSINDERO S, 2008, ADV NEURAL INFORM PR, V20; Ranzato M. A., 2015, ARXIV14126604V2; Ranzato M, 2011, PROC CVPR IEEE; Robinson A. J., 1987, TECHNICAL REPORT; Roth S., 2009, INT J COMPUTER VISIO, V82; Simonyan Karen, 2015, VERY DEEP CONVOLUTIO; Sohl-Dickstein J., 2015, ICML 32; Srivastava N., 2014, JMLR; Srivastava N., 2015, P 32 INT C MACH LEAR; Sundermeyer M., 2010, INTERSPEECH; Sutskever I, 2014, ADV NEUR IN, V27; Theis L., 2012, ADV NEURAL INFORM PR, V25; Theis L., 2011, JMLR; Theis L, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0039857; Tierney L., 1994, ANN STAT; Uria B., 2014, ICML 31; Uria B., 2013, ADV NEURAL INFORM PR, V26; Van den Oord A, 2014, ADV NEUR IN, V27; van den Oord A, 2014, J MACH LEARN RES, V15, P2061; van Hateren JH, 1998, P ROYAL SOC B, V265; Zoran D., 2012, NIPS; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	50	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100005
C	Verma, N; Branson, K		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Verma, Nakul; Branson, Kristin			Sample Complexity of Learning Mahalanobis Distance Metrics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Metric learning seeks a transformation of the feature space that enhances prediction quality for a given task. In this work we provide PAC-style sample complexity rates for supervised metric learning. We give matching lower-and upper-bounds showing that sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution. In addition, by leveraging the structure of the data distribution, we provide rates fine-tuned to a specific notion of the intrinsic complexity of a given dataset, allowing us to relax the dependence on representation dimension. We show both theoretically and empirically that augmenting the metric learning optimization criterion with a simple norm-based regularization is important and can help adapt to a dataset's intrinsic complexity yielding better generalization, thus partly explaining the empirical success of similar regularizations reported in previous works.	[Verma, Nakul; Branson, Kristin] HHMI, Janelia Res Campus, Chevy Chase, MD 20815 USA	Howard Hughes Medical Institute	Verma, N (corresponding author), HHMI, Janelia Res Campus, Chevy Chase, MD 20815 USA.	verman@janelia.hhmi.org; bransonk@janelia.hhmi.org						Anthony M., 1999, NEURAL NETWORK LEARN, V9; Balcan M-F., 2008, C COMP LEARN THEOR C; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bellet A., 2014, CORR; Bellet A., 2012, INT C MACH LEARN ICM; Bellet A., 2012, CORR; Bian W., 2011, P 22 INT JOINT C ART, V2, P1186; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Cao Q., 2013, CORR; Cortes C., 2010, INT C MACH LEARN ICM; Guo ZC, 2014, NEURAL COMPUT, V26, P497, DOI 10.1162/NECO_a_00556; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Jin R., 2009, ADV NEURAL INFORM PR, V22; Law M. T., 2014, COMPUTER VISION PATT; Lim D. K. H., 2013, INT C MACH LEARN ICM; McFee B., 2010, INT C MACH LEARN ICM; Russell S. J, 2002, ADV NEURAL INFORM PR, P12, DOI DOI 10.5555/2968618.2968683; Schultz M., 2004, NEURAL INFORM PROCES; Shaw B., 2011, NEURAL INFORM PROCES; Vershynin R., 2010, COMPRESSED SENSING T; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Ying Y., 2009, C COMP LEARN THEOR C	23	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101071
C	Wang, J; Ye, JP		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Wang, Jie; Ye, Jieping			Multi-Layer Feature Reduction for Tree Structured Group Lasso via Hierarchical Projection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				SELECTION; RULES	Tree structured group Lasso (TGL) is a powerful technique in uncovering the tree structured sparsity over the features, where each node encodes a group of features. It has been applied successfully in many real-world applications. However, with extremely large feature dimensions, solving TGL remains a significant challenge due to its highly complicated regularizer. In this paper, we propose a novel Multi-Layer Feature reduction method (MLFre) to quickly identify the inactive nodes (the groups of features with zero coefficients in the solution) hierarchically in a top-down fashion, which are guaranteed to be irrelevant to the response. Thus, we can remove the detected nodes from the optimization without sacrificing accuracy. The major challenge in developing such testing rules is due to the overlaps between the parents and their children nodes. By a novel hierarchical projection algorithm, MLFre is able to test the nodes independently from any of their ancestor nodes. Moreover, we can integrate MLFre-that has a low computational cost-with any existing solvers. Experiments on both synthetic and real data sets demonstrate that the speedup gained by MLFre can be orders of magnitude.	[Wang, Jie; Ye, Jieping] Univ Michigan, Computat Med & Bioinformat, Ann Arbor, MI 48109 USA; [Ye, Jieping] Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan; University of Michigan System; University of Michigan	Wang, J (corresponding author), Univ Michigan, Computat Med & Bioinformat, Ann Arbor, MI 48109 USA.	jwangumi@umich.edu; jpye@umich.edu			NIH [R01 LM010730, U54 EB020403]; NSF [IIS-0953662, III-1539991, III-1539722]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF))	This work is supported in part by research grants from NIH (R01 LM010730, U54 EB020403) and NSF (IIS-0953662, III-1539991, III-1539722).	[Anonymous], 2006, NONLINEAR PROGRAMMIN; Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015; Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; Borwein J.M., 2006, CONVEX ANAL NONLINEA; Boyd S, 2004, CONVEX OPTIMIZATION; Castro J, 2015, IEEE MTT S INT MICR; Chen X, 2012, ANN APPL STAT, V6, P719, DOI 10.1214/11-AOAS514; Deng W., 2011, TECHNICAL REPORT; El Ghaoui L, 2012, PAC J OPTIM, V8, P667; Hiriart-Urruty J.-B., 1988, NONSMOOTH OPTIMIZATI; Jenatton R, 2012, SIAM J IMAGING SCI, V5, P835, DOI 10.1137/110832380; Jenatton R, 2011, J MACH LEARN RES, V12, P2297; Jia K., 2012, EUR C COMP VIS; Kim S., 2012, ANN APPL STAT; Kim S, 2010, INT C MACH LEARN; Liu J., 2009, SLEP SPARSE LEARNING; Liu J., 2010, 23TH ANN C ADV NEURA, P1459; Liu J, 2014, PR MACH LEARN RES, V32, P289; Liu M., 2012, MED IMAGE COMPUTING; Ruszczynski A. P., 2006, NONLINEAR OPTIMIZATI; Takeuchi, 2013, P 30 INT C MACH LEAR, P1382; Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x; Wang J, 2014, ADV NEUR IN, V27; Wang J, 2015, J MACH LEARN RES, V16, P1063; Wang J, 2014, PR MACH LEARN RES, V32, P523; Xiang Z., 2011, NIPS; Yogatama D., 2015, INT C MACH LEARN; Yogatama D, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P786; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Zhao P., 2009, ANN STAT; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	31	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101097
C	Wu, JJ; Yildirim, I; Lim, JJ; Freeman, WT; Tenenbaum, JB		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Wu, Jiajun; Yildirim, Ilker; Lim, Joseph J.; Freeman, William T.; Tenenbaum, Joshua B.			Galileo: Perceiving Physical Object Properties by Integrating a Physics Engine with Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Humans demonstrate remarkable abilities to predict physical events in dynamic scenes, and to infer the physical properties of objects from static images. We propose a generative model for solving these problems of physical scene understanding from real-world videos and images. At the core of our generative model is a 3D physics engine, operating on an object-based representation of physical properties, including mass, position, 3D shape, and friction. We can infer these latent properties using relatively brief runs of MCMC, which drive simulations in the physics engine to fit key features of visual observations. We further explore directly mapping visual inputs to physical properties, inverting a part of the generative process using deep learning. We name our model Galileo, and evaluate it on a video dataset with simple yet physically rich scenarios. Results show that Galileo is able to infer the physical properties of objects and predict the outcome of a variety of physical events, with an accuracy comparable to human subjects. Our study points towards an account of human vision with generative physical knowledge at its core, and various recognition models as helpers leading to efficient inference.	[Wu, Jiajun; Lim, Joseph J.; Freeman, William T.] MIT, EECS, Cambridge, MA 02139 USA; [Yildirim, Ilker] Rockefeller Univ, BCS MIT, New York, NY 10021 USA; [Tenenbaum, Joshua B.] MIT, BCS, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Rockefeller University; Massachusetts Institute of Technology (MIT)	Wu, JJ (corresponding author), MIT, EECS, Cambridge, MA 02139 USA.	jiajunwu@mit.edu; ilkery@mit.edu; lim@csail.mit.edu; billf@mit.edu; jbt@mit.edu	Wu, JiaJun/GQH-7885-2022		NSF [1212849]; Center for Brains, Minds, and Machines (NSF STC award) [CCF-1231216]	NSF(National Science Foundation (NSF)); Center for Brains, Minds, and Machines (NSF STC award)	This work was supported by NSF Robust Intelligence 1212849 Reconstructive Recognition and the Center for Brains, Minds, and Machines (funded by NSF STC award CCF-1231216).	Baillargeon R, 2004, CURR DIR PSYCHOL SCI, V13, P89, DOI 10.1111/j.0963-7214.2004.00281.x; Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110; Carey S., 2009, ORIGIN CONCEPTS; Coumans E, 2010, OPEN SOURCE SOFTWARE; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; Jia Zhaoyin, 2014, IEEE TPAMI; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Sanborn AN, 2013, PSYCHOL REV, V120, P411, DOI 10.1037/a0031912; Schulman J, 2013, IEEE INT CONF ROBOT, P1130, DOI 10.1109/ICRA.2013.6630714; Tomasi C., 1991, INT J COMPUTER VISIO; Ullman Tomer, 2014, COGSCI; Yildirim I., 2015, 37 ANN C COGN SCI SO; Zheng B., 2014, ICRA	13	3	3	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101040
C	Yang, E; Lozano, AC		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Yang, Eunho; Lozano, Aurelie C.			Robust Gaussian Graphical Modeling with the Trimmed Graphical Lasso	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				REGRESSION; SELECTION	Gaussian Graphical Models (GGMs) are popular tools for studying network structures. However, many modern applications such as gene network discovery and social interactions analysis often involve high-dimensional noisy data with outliers or heavier tails than the Gaussian distribution. In this paper, we propose the Trimmed Graphical Lasso for robust estimation of sparse GGMs. Our method guards against outliers by an implicit trimming mechanism akin to the popular Least Trimmed Squares method used for linear regression. We provide a rigorous statistical analysis of our estimator in the high-dimensional setting. In contrast, existing approaches for robust sparse GGMs estimation lack statistical guarantees. Our theoretical results are complemented by experiments on simulated and real gene expression data which further demonstrate the value of our approach.	[Yang, Eunho; Lozano, Aurelie C.] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Yang, E (corresponding author), IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.	eunhyang@us.ibm.com; aclozano@us.ibm.com	Yang, Eunho/K-8395-2016					Alfons A, 2013, ANN APPL STAT, V7, P226, DOI 10.1214/12-AOAS575; Banerjee O, 2008, J MACH LEARN RES, V9, P485; Boyd S, 2004, CONVEX OPTIMIZATION; Brem RB, 2005, P NATL ACAD SCI USA, V102, P1572, DOI 10.1073/pnas.0408709102; CROSS GR, 1983, IEEE T PATTERN ANAL, V5, P25, DOI 10.1109/TPAMI.1983.4767341; Daye ZJ, 2012, BIOMETRICS, V68, P316, DOI 10.1111/j.1541-0420.2011.01652.x; Finegold M, 2011, ANN APPL STAT, V5, P1057, DOI 10.1214/10-AOAS410; Friedman J., 2007, BIOSTATISTICS; Hassner M., 1978, Proceedings of the 4th International Joint Conference on Pattern Recognition, P538; Hsieh C. J., 2011, NEUR INFO PROC SYS N, V24; Ising E, 1925, Z PHYS, V31, P253, DOI 10.1007/BF02980577; Kanehisa M, 2014, NUCLEIC ACIDS RES, V42, pD199, DOI 10.1093/nar/gkt1076; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Loh P-L, 2013, ARXIV13052436V2; Manning CD, 1999, FDN STAT NATURAL LAN; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Nesterov Y., 2007, 76 CORE UCL; Nguyen NH, 2013, IEEE T INFORM THEORY, V59, P2036, DOI 10.1109/TIT.2012.2232347; Oh JH, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/1471-2105-15-S7-S5; Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631; Ripley B, 1981, U PENN LAW REV, P252, DOI DOI 10.2307/3313062; Sun H, 2012, BIOMETRICS, V68, P1197, DOI 10.1111/j.1541-0420.2012.01785.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; WOODS JW, 1978, IEEE T AUTOMAT CONTR, V23, P846, DOI 10.1109/TAC.1978.1101866; Yang E., 2012, NEUR INFO PROC SYS N, V25; Yang E., 2015, ARXIV151008512; Yang E, 2014, ADV NEUR IN, V27; Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018	29	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100071
C	Zhang, HS; Zhou, Y; Liang, YB		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Zhang, Huishuai; Zhou, Yi; Liang, Yingbin			Analysis of Robust PCA via Local Incoherence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We investigate the robust PCA problem of decomposing an observed matrix into the sum of a low-rank and a sparse error matrices via convex programming Principal Component Pursuit (PCP). In contrast to previous studies that assume the support of the error matrix is generated by uniform Bernoulli sampling, we allow non-uniform sampling, i.e., entries of the low-rank matrix are corrupted by errors with unequal probabilities. We characterize conditions on error corruption of each individual entry based on the local incoherence of the low-rank matrix, under which correct matrix decomposition by PCP is guaranteed. Such a refined analysis of robust PCA captures how robust each entry of the low rank matrix combats error corruption. In order to deal with non-uniform error corruption, our technical proof introduces a new weighted norm and develops/exploits the concentration properties that such a norm satisfies.	[Zhang, Huishuai; Zhou, Yi; Liang, Yingbin] Syracuse Univ, Dept EECS, Syracuse, NY 13244 USA	Syracuse University	Zhang, HS (corresponding author), Syracuse Univ, Dept EECS, Syracuse, NY 13244 USA.	hzhan23@syr.edu; yzhou35@syr.edu; yliang06@syr.edu						Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793; Chen Y., 2012, ADV NEURAL INFORM PR, P2204; Chen YD, 2015, IEEE T INFORM THEORY, V61, P2909, DOI 10.1109/TIT.2015.2415195; Chen YD, 2014, J MACH LEARN RES, V15, P2213; Chen YD, 2014, IEEE T INFORM THEORY, V60, P6440, DOI 10.1109/TIT.2014.2346205; Chen Yudong, 2013, ARXIV13062979; Gross D, 2011, IEEE T INFORM THEORY, V57, P1548, DOI 10.1109/TIT.2011.2104999; Hsu D, 2011, IEEE T INFORM THEORY, V57, P7221, DOI 10.1109/TIT.2011.2158250; Lin Z., 2010, ARXIV PREPRINT ARXIV; Oymak S., 2011, ARXIV11045186; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Srebro N., 2010, PROC INT C NEURAL IN, P2056; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z	20	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103056
C	Zoghi, M; Karnin, Z; Whiteson, S; de Rijke, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Zoghi, Masrour; Karnin, Zohar; Whiteson, Shimon; de Rijke, Maarten			Copeland Dueling Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					A version of the dueling bandit problem is addressed in which a Condorcet winner may not exist. Two algorithms are proposed that instead seek to minimize regret with respect to the Copeland winner, which, unlike the Condorcet winner, is guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed for small numbers of arms, while the second, Scalable Copeland Bandits (SCB), works better for large-scale problems. We provide theoretical results bounding the regret accumulated by CCB and SCB, both substantially improving existing results. Such existing results either offer bounds of the form O(K log T) but require restrictive assumptions, or offer bounds of the form O(K-2 log T) without requiring such assumptions. Our results offer the best of both worlds: O(K log T) bounds without restrictive assumptions.	[Zoghi, Masrour; de Rijke, Maarten] Univ Amsterdam, Inst Informat, Amsterdam, Netherlands; [Karnin, Zohar] Yahoo Labs, New York, NY USA; [Whiteson, Shimon] Univ Oxford, Dept Comp Sci, Oxford, England	University of Amsterdam; University of Oxford	Zoghi, M (corresponding author), Univ Amsterdam, Inst Informat, Amsterdam, Netherlands.	m.zoghi@uva.nl; zkarnin@yahoo-inc.com; shimon.whiteson@cs.ox.ac.uk; derijke@uva.nl			Amsterdam Data Science; Dutch national program COMMIT; Elsevier; European Community [312827]; ESF Research Network Program ELIAS; Royal Dutch Academy of Sciences (KNAW) under the Elite Network Shifts project; Microsoft Research Ph.D. program; Netherlands eScience Center [027.012.105]; Netherlands Institute for Sound and Vision; Netherlands Organisation for Scientific Research (NWO) [727.011.005, 612.001.116, HOR-11-10, 640.006.013, 612.066.930, CI-14-25, SH-322-15]; Yahoo! Faculty Research and Engagement Program; Yandex	Amsterdam Data Science; Dutch national program COMMIT; Elsevier; European Community(European Commission); ESF Research Network Program ELIAS; Royal Dutch Academy of Sciences (KNAW) under the Elite Network Shifts project; Microsoft Research Ph.D. program; Netherlands eScience Center; Netherlands Institute for Sound and Vision; Netherlands Organisation for Scientific Research (NWO)(Netherlands Organization for Scientific Research (NWO)); Yahoo! Faculty Research and Engagement Program; Yandex	We would like to thank Nir Ailon and Ulle Endriss for helpful discussions. This research was supported by Amsterdam Data Science, the Dutch national program COMMIT, Elsevier, the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement nr 312827 (VOX-Pol), the ESF Research Network Program ELIAS, the Royal Dutch Academy of Sciences (KNAW) under the Elite Network Shifts project, the Microsoft Research Ph.D. program, the Netherlands eScience Center under project number 027.012.105, the Netherlands Institute for Sound and Vision, the Netherlands Organisation for Scientific Research (NWO) under project nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013, 612.066.930, CI-14-25, SH-322-15, the Yahoo! Faculty Research and Engagement Program, and Yandex. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.	Ailon N., 2014, ICML; Altman A., 2008, JAIR; Bartok Gabor, 2012, ICML; Bubeck S., 2011, JMLR, V12; Bull A. D., 2011, JMLR, V12; Busa-Fekete R., 2013, ICML; Busa-Fekete Robert, 2014, AAAI; Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119; de Freitas N., 2012, ICML; Dudik M., 2015, COLT; Hofmann K, 2013, INFORM RETRIEVAL, V16, P63, DOI 10.1007/s10791-012-9197-9; Joachims T., 2002, KDD; Kleinberg R., 2008, STOC; Kohavi R., 2013, KDD; Li L., 2015, WSDM; Munos R., 2011, NIPS; Negahban Sahand, 2012, NIPS; Piccolboni A., 2001, COLT; Schulze M, 2011, SOC CHOICE WELFARE, V36, P267, DOI 10.1007/s00355-010-0475-4; Schuth Anne, 2014, CIKM; Schutze H., 2008, INTRO INFORM RETRIEV, V39; Srinivas N., 2010, ICML; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Urvoy T., 2013, ICML; Valko M., 2013, ICML; Yue Y., 2011, ICML; Yue Y., 2012, J COMPUTER SYSTEM SC, V78; Yue Y., 2009, ICML; Zoghi M., 2014, ICML; Zoghi Masrour, 2015, WSDM; Zoghi Masrour, 2014, WSDM; 2010, PREFERENCE LEARNING, P00001	32	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103061
C	Bareinboim, E; Pearl, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Bareinboim, Elias; Pearl, Judea			Transportability from Multiple Environments with Limited Experiments: Completeness Results	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					This paper addresses the problem of mz-transportability, that is, transferring causal knowledge collected in several heterogeneous domains to a target domain in which only passive observations and limited experimental data can be collected. The paper first establishes a necessary and sufficient condition for deciding the feasibility of mz-transportability, i.e., whether causal effects in the target domain are estimable from the information available. It further proves that a previously established algorithm for computing transport formula is in fact complete, that is, failure of the algorithm implies non-existence of a transport formula. Finally, the paper shows that the do-calculus is complete for the mz-transportability class.	[Bareinboim, Elias; Pearl, Judea] Univ Calif Los Angeles, Comp Sci, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Bareinboim, E (corresponding author), Univ Calif Los Angeles, Comp Sci, Los Angeles, CA 90095 USA.	eb@cs.ucla.edu; judea@cs.ucla.edu						Bareinboim E., 2013, P 16 INT C ART INT S, V31, P135; Bareinboim E., 2013, ADV NEURAL INFORM PR, V26, P136; Bareinboim E, 2013, P 27 AAAI C ART INT, P95; Bareinboim E., 2012, P 26 AAAI C ARTIFICI, V26, P698; Bareinboim E, 2013, J CAUSAL INFERENCE, V1, P107, DOI 10.1515/jci-2012-0004; Campbell Donald T., 1963, EXPT QUASIEXPERIMENT; Hedges L.V., 2014, STAT METHODS META AN; Lee Sanghack, 2013, AAAI, P583; Lee Sanghack, 2013, P 29 C UNC ART INT U, P361; Manski C. F., 2007, IDENTIFICATION PREDI; PEARL J., 2011, P 25 AAAI C ART INT, P247; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Scholkopf Bernhard, 2012, ICML; Shadish WR, 2002, EXPT QUASIEXPERIMENT; Spirtes P., 2000, CAUSATION PREDICTION; Storkey A, 2009, NEURAL INF PROCESS S, P3; Tian J, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P567; Tian J., 2002, THESIS; Zhang K., 2013, P 30 INT C MACH LEAR, V28	22	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102097
C	Bateni, M; Bhaskara, A; Lattanzi, S; Mirrokni, V		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Bateni, MohammadHossein; Bhaskara, Aditya; Lattanzi, Silvio; Mirrokni, Vahab			Distributed Balanced Clustering via Mapping Coresets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Large-scale clustering of data points in metric spaces is an important problem in mining big data sets. For many applications, we face explicit or implicit size constraints for each cluster which leads to the problem of clustering under capacity constraints or the "balanced clustering" problem. Although the balanced clustering problem has been widely studied, developing a theoretically sound distributed algorithm remains an open problem. In this paper we develop a new framework based on "mapping coresets" to tackle this issue. Our technique results in first distributed approximation algorithms for balanced clustering problems for a wide range of clustering objective functions such as k-center, k-median, and k-means.	[Bateni, MohammadHossein; Bhaskara, Aditya; Lattanzi, Silvio; Mirrokni, Vahab] Google NYC, New York, NY 10011 USA		Bateni, M (corresponding author), Google NYC, New York, NY 10011 USA.	bateni@google.com; bhaskaraaditya@google.com; silviol@google.com; mirrokni@google.com						Agarwal P. K., 2012, P 31 ACM SIGMOD SIGA, P23, DOI DOI 10.1145/2213556.2213562; Agarwal PK, 2004, J ACM, V51, P606, DOI 10.1145/1008731.1008736; AN H.-C., 2013, ABS13042983 CORR; Bahmani B, 2012, PROC VLDB ENDOW, V5, P622, DOI 10.14778/2180912.2180915; Balcan M.-F., 2013, NIPS; BARILAN J, 1993, J ALGORITHM, V15, P385, DOI 10.1006/jagm.1993.1047; Charikar M., 2003, P 35 ANN ACM S THEOR, P30, DOI DOI 10.1145/780542.780548; Charikar M., 1997, P 29 ANN ACM S THEOR, P626; Chuzhoy J, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P952; Cygan M, 2012, ANN IEEE SYMP FOUND, P273, DOI 10.1109/FOCS.2012.63; Ene A., 2011, SIGKDD, DOI DOI 10.1145/2020408.2020515; Guha S., 2001, STOC; Gupta A., 2008, CORR; INDYK P., 2014, COMPOSABLE COR UNPUB; Karloff H, 2010, PROC APPL MATH, V135, P938; Khuller S, 2000, SIAM J DISCRETE MATH, V13, P403, DOI 10.1137/S0895480197329776; KORUPOLU MR, 1998, P 9 ANN ACM SIAM S D, P1; Lattanzi S, 2011, SPAA 11: PROCEEDINGS OF THE TWENTY-THIRD ANNUAL SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P85; LIN JH, 1992, INFORM PROCESS LETT, V44, P245, DOI [10.1016/0020-0190(92)90208-D, 10.1109/ICASSP.1992.226074]; Rahimian F, 2013, INT CONF SELF SELF, P51, DOI 10.1109/SASO.2013.13; Ugander J., 2013, PROC 6 ACM INT C WEB, P507	21	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100090
C	Calandriello, D; Lazaric, A; Restelli, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Calandriello, Daniele; Lazaric, Alessandro; Restelli, Marcello			Sparse Multi-Task Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In multi-task reinforcement learning (MTRL), the objective is to simultaneously learn multiple tasks and exploit their similarity to improve the performance w.r.t. single-task learning. In this paper we investigate the case when all the tasks can be accurately represented in a linear approximation space using the same small subset of the original (large) set of features. This is equivalent to assuming that the weight vectors of the task value functions are jointly sparse, i.e., the set of their non-zero components is small and it is shared across tasks. Building on existing results in multi-task regression, we develop two multi-task extensions of the fitted Q-iteration algorithm. While the first algorithm assumes that the tasks are jointly sparse in the given representation, the second one learns a transformation of the features in the attempt of finding a more sparse representation. For both algorithms we provide a sample complexity analysis and numerical simulations.	[Calandriello, Daniele; Lazaric, Alessandro] INRIA Lille Nord Europe, Team SequeL, Lille, France; [Restelli, Marcello] Politecn Milan, DEIB, Milan, Italy	Polytechnic University of Milan	Calandriello, D (corresponding author), INRIA Lille Nord Europe, Team SequeL, Lille, France.	daniele.calandriello@inria.fr; alessandro.lazaric@inria.fr; fmarcello.restelli@polimi.it			French Ministry of Higher Education and Research; European Community [270327]; French National Research Agency (ANR) [ANR-14-CE24-0010-01]	French Ministry of Higher Education and Research; European Community(European Commission); French National Research Agency (ANR)(French National Research Agency (ANR))	This work was supported by the French Ministry of Higher Education and Research, the European Community's Seventh Framework Programme under grant agreement 270327 (project CompLACS), and the French National Research Agency (ANR) under project ExTra-Learn n.ANR-14-CE24-0010-01.	[Anonymous], 2020, REINFORCEMENT LEARNI; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Calandriello Daniele, 2014, SPARSE MULTI TASK RE; Castelletti A, 2011, IEEE ADPRL; Ernst  D., 2005, J MACHINE LEARNING R, V6; Friedman J., 2009, ELEMENTS STAT LEARNI, DOI 10.1007/978-0-387-84858-7; Ghavamzadeh Mohammad, 2011, ICML; Grunewalder S., 2012, ICML; Hachiya H., 2010, ECML PKDD; Hoffman Matthew W., 2012, Recent Advances in Reinforcement Learning. 9th European Workshop (EWRL 2011). Revised Selected Papers, P102, DOI 10.1007/978-3-642-29946-9_13; Jacob L., 2009, P 26 INT C MACH LEAR, P433, DOI DOI 10.1145/1553374.1553431; Kolter J. Z., 2009, ICML; Lazaric A., 2010, ICML; Lazaric A., 2011, REINFORCEMENT LEARNI; Lazaric Alessandro, 2011, NIPS; Li H, 2009, J MACH LEARN RES, V10, P1131; Lounici K, 2011, ANN STAT, V39, P2164, DOI 10.1214/11-AOS896; Munos R, 2008, J MACH LEARN RES, V9, P815; Painter-Wakefield C., 2012, ICML; Scherrer Bruno, 2012, ICML; Snel Matthijs, 2011, EWRL; Tanaka F, 2003, 2003 IEEE INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN ROBOTICS AND AUTOMATION, VOLS I-III, PROCEEDINGS, P1108; Taylor ME, 2009, J MACH LEARN RES, V10, P1633; Wilson A., 2007, PROC INT C MACH LEAR, P1015, DOI DOI 10.1145/1273496; Zhang Yi, 2010, ADV NEURAL INF PROCE, P2550	29	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100026
C	Chen, CY; Zhu, J; Zhang, XH		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Chen, Changyou; Zhu, Jun; Zhang, Xinhua			Robust Bayesian Max-Margin Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We present max-margin Bayesian clustering (BMC), a general and robust framework that incorporates the max-margin criterion into Bayesian clustering models, as well as two concrete models of BMC to demonstrate its flexibility and effectiveness in dealing with different clustering tasks. The Dirichlet process max-margin Gaussian mixture is a nonparametric Bayesian clustering model that relaxes the underlying Gaussian assumption of Dirichlet process Gaussian mixtures by incorporating max-margin posterior constraints, and is able to infer the number of clusters from data. We further extend the ideas to present max-margin clustering topic model, which can learn the latent topic representation of each document while at the same time cluster documents in the max-margin fashion. Extensive experiments are performed on a number of real datasets, and the results indicate superior clustering performance of our methods compared to related baselines.	[Chen, Changyou] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA; [Zhu, Jun] Tsinghua Univ, Dept Comp Sci & Tech, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China; [Zhu, Jun] Tsinghua Univ, Dept Comp Sci & Tech, Tsinghua Natl TNList Lab, Beijing 100084, Peoples R China; [Zhang, Xinhua] Australian Natl Univ, Canberra, ACT, Australia; [Zhang, Xinhua] Natl ICT Australia NICTA, Canberra, ACT, Australia	Duke University; Tsinghua University; Tsinghua University; Australian National University; NICTA	Chen, CY (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.	cchangyou@gmail.com; dcszj@tsinghua.edu.cn; xinhua.zhang@anu.edu.au			Australia China Science and Research Fund from the Department of Industry, Innovation, Climate Change, Science, Research and Tertiary Education of the Australian Government [ACSRF-06283]; National Key Project for Basic Research of China [2013CB329403]; NSF of China [61322308, 61332007]; Australian Government; Australian Research Council through the ICT Centre of Excellence program	Australia China Science and Research Fund from the Department of Industry, Innovation, Climate Change, Science, Research and Tertiary Education of the Australian Government; National Key Project for Basic Research of China(National Basic Research Program of China); NSF of China(National Natural Science Foundation of China (NSFC)); Australian Government(Australian GovernmentCGIAR); Australian Research Council through the ICT Centre of Excellence program(Australian Research Council)	This work was supported by an Australia China Science and Research Fund grant (ACSRF-06283) from the Department of Industry, Innovation, Climate Change, Science, Research and Tertiary Education of the Australian Government, the National Key Project for Basic Research of China (No. 2013CB329403), and NSF of China (Nos. 61322308, 61332007). NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program.	[Anonymous], 2008, THESIS U CAMBRIDGE; [Anonymous], 2007, CONJUGATE BAYESIAN A; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Ben-Hur A, 2002, J MACH LEARN RES, V2, P125, DOI 10.1162/15324430260185565; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Cheng H., 2013, UAI; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Davy M, 2010, IEEE T PATTERN ANAL, V32, P1781, DOI 10.1109/TPAMI.2010.21; Favaro S., 2013, STAT SCI; Fox C., 1987, INTRO CALCULUS VARIA; Franti P, 2006, PATTERN RECOGN, V39, P761, DOI 10.1016/j.patcog.2005.09.012; Ganchev K, 2010, J MACH LEARN RES, V11, P2001; Gomes R., 2010, NIPS; Heller K., 2005, ICML; Hu Y., 2013, NIPS; Jain AK, 2005, LECT NOTES COMPUT SC, V3776, P1, DOI 10.1145/1083091.1083101; JOrgensen B., 1982, STAT PROPERTIES GEN, V1st; Li Y.-F., 2009, AISTATS; Lichman M, 2013, UCI MACHINE LEARNING; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; Rasmussen C. E., 2000, NIPS; Shah A., 2013, UAI; Shi J., 2000, TPAMI, V22, P705; Sindhwani V., 2006, NIPS WORKSH MACH LEA; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Teh Yee Whye, 2008, NIPS; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vinh NX, 2010, J MACH LEARN RES, V11, P2837; Wang L., 2012, AAAI; Wang P., 2011, SDM; Xu L., 2005, NIPS, p[2, 3]; Xu M., 2013, ICML; Zhao B, 2008, ICML; Zhou G. T., 2013, NIPS; Zhu J., 2013, PROC INT C MACH LEAR; Zhu J., 2012, ICML; Zhu J, 2012, J MACH LEARN RES, V13, P2237; Zhu Jun, 2014, JMLR	38	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100084
C	Gu, SH; Zhang, L; Zuo, WM; Feng, XC		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Gu, Shuhang; Zhang, Lei; Zuo, Wangmeng; Feng, Xiangchu			Projective dictionary pair learning for pattern classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				FACE RECOGNITION; K-SVD; SPARSE	Discriminative dictionary learning (DL) has been widely studied in various pattern classification problems. Most of the existing DL methods aim to learn a synthesis dictionary to represent the input signal while enforcing the representation coefficients and/or representation residual to be discriminative. However, the l(0) or l(1)-norm sparsity constraint on the representation coefficients adopted in most DL methods makes the training and testing phases time consuming. We propose a new discriminative DL framework, namely projective dictionary pair learning (DPL), which learns a synthesis dictionary and an analysis dictionary jointly to achieve the goal of signal representation and discrimination. Compared with conventional DL methods, the proposed DPL method can not only greatly reduce the time complexity in the training and testing phases, but also lead to very competitive accuracies in a variety of visual classification tasks.	[Gu, Shuhang; Zhang, Lei] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China; [Zuo, Wangmeng] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin, Heilongjiang, Peoples R China; [Feng, Xiangchu] Xidian Univ, Dept Appl Math, Xian, Shaanxi, Peoples R China	Hong Kong Polytechnic University; Harbin Institute of Technology; Xidian University	Gu, SH (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.	cssgu@comp.polyu.edu.hk; cslzhang@comp.polyu.edu.hk; cswmzuo@gmail.com; xcfeng@mail.xidian.edu.cn	Zuo, Wangmeng/B-3701-2008					Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199; Coates A., 2011, ICML; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Feng ZZ, 2013, PATTERN RECOGN, V46, P2134, DOI 10.1016/j.patcog.2013.01.016; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Hale ET, 2008, SIAM J OPTIMIZ, V19, P1107, DOI 10.1137/070698920; Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88; Lazebnik S., 2006, P 2006 IEEE COMP VIS, P2169; LeCun Y., 2010, ICML; Lee H., 2007, NIPS; Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012; Mairal J., 2008, NIPS; Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156; Martinez A., 1998, AR FACE DATABASE CVC; Ramirez I., 2010, CVPR; Ranzato M., 2006, NIPS; Reddy KK, 2013, MACH VISION APPL, V24, P971, DOI 10.1007/s00138-012-0450-4; Rubinstein R, 2010, P IEEE, V98, P1045, DOI 10.1109/JPROC.2010.2040551; Sadanand S., 2012, P 2012 IEEE C COMP V; Soltanolkotabi M., 2013, ARXIV13012603; Sprechmann P., 2013, NIPS; Wang Z., 2013, ICCV; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Yang M., 2011, ICCV; Yunjin C., 2012, NIPS WORKSH; Zhang L., 2011, ICCV	28	3	3	1	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103051
C	Hajek, B; Oh, S; Xu, JM		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Hajek, Bruce; Oh, Sewoong; Xu, Jiaming			Minimax-optimal Inference from Partial Rankings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					This paper studies the problem of rank aggregation under the Plackett-Luce model. The goal is to infer a global ranking and related scores of the items, based on partial rankings provided by multiple users over multiple subsets of items. A question of particular interest is how to optimally assign items to users for ranking and how many item assignments are needed to achieve a target estimation error. Without any assumptions on how the items are assigned to users, we derive an oracle lower bound and the Cramer-Rao lower bound of the estimation error. We prove an upper bound on the estimation error achieved by the maximum likelihood estimator, and show that both the upper bound and the Cramer-Rao lower bound inversely depend on the spectral gap of the Laplacian of an appropriately defined comparison graph. Since random comparison graphs are known to have large spectral gaps, this suggests the use of random assignments when we have the control. Precisely, the matching oracle lower bound and the upper bound on the estimation error imply that the maximum likelihood estimator together with a random assignment is minimax-optimal up to a logarithmic factor. We further analyze a popular rank-breaking scheme that decompose partial rankings into pairwise comparisons. We show that even if one applies the mismatched maximum likelihood estimator that assumes independence (on pairwise comparisons that are now dependent due to rank-breaking), minimax optimal performance is still achieved up to a logarithmic factor.	[Hajek, Bruce; Oh, Sewoong; Xu, Jiaming] UIUC, Champaign, IL 61820 USA	University of Illinois System; University of Illinois Urbana-Champaign	Hajek, B (corresponding author), UIUC, Champaign, IL 61820 USA.	b-hajek@illinois.edu; swoh@illinois.edu; jxu18@illinois.edu						Azari Soufiani H, 2013, ARXIV13096864; Ben-Akiva M.E., 1985, DISCRETE CHOICE ANAL, V9; Braverman M., 2009, ARXIV09101191; Duchi J. C., 2010, P ICML C HAIF ISR JU; Gill R., 1995, BERNOULLI, V1, P59; Guadagni P.M., 1983, MARKET SCI, V2, P203, DOI DOI 10.1287/MKSC.2.3.203; Guiver J., 2009, P 26 ANN INT C MACHI, P377; Hossein A. S., 2012, P 25 ANN C NEUR INF; Hunter DR, 2004, ANN STAT, V32, P384; Jagabathula S., 2008, NIPS, V2008; Lozano J. A., 2012, PROBABILISTIC MODELI; MCFADDEN D, 1980, J BUS, V53, pS13, DOI 10.1086/296093; Negahban S., 2012, ARXIV12091688; Qin T., 2010, ADV NEURAL INFORM PR; Rajkumar A., 2014, P INT C MACH LEARN; SHAM PC, 1995, ANN HUM GENET, V59, P323, DOI 10.1111/j.1469-1809.1995.tb00751.x; Simons G, 1999, ANN STAT, V27, P1041; Soufiani H. Azari, 2014, P INT C MACH LEARN; Soufiani Hossein Azari, 2013, P 26 INT C NEUR INF, P2706	19	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101032
C	He, H; Daume, H; Eisner, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		He, He; Daume, Hal, III; Eisner, Jason			Learning to Search in Branch-and-Bound Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Branch-and-bound is a widely used method in combinatorial optimization, including mixed integer programming, structured prediction and MAP inference. While most work has been focused on developing problem-specific techniques, little is known about how to systematically design the node searching strategy on a branch-and-bound tree. We address the key challenge of learning an adaptive node searching order for any class of problem solvable by branch-and-bound. Our strategies are learned by imitation learning. We apply our algorithm to linear programming based branch-and-bound for solving mixed integer programs (MIP). We compare our method with one of the fastest open-source solvers, SCIP; and a very efficient commercial solver, Gurobi. We demonstrate that our approach achieves better solutions faster on four MIP libraries.	[He, He; Daume, Hal, III] Univ Maryland, Dept Comp Sci, College Pk, MD 20740 USA; [Eisner, Jason] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA	University System of Maryland; University of Maryland College Park; Johns Hopkins University	He, H (corresponding author), Univ Maryland, Dept Comp Sci, College Pk, MD 20740 USA.	hhe@cs.umd.edu; hall@cs.umd.edu; jason@cs.jhu.edu			National Science Foundation [0964681]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported by the National Science Foundation under Grant No. 0964681.	Abbeel P., 2004, ICML; Alvarez Alejandro Marcos, 2014, ECML; Atamturk A, 2003, MATH PROGRAM, V98, P145, DOI 10.1007/s10107-003-0400-z; Boyan Justin A., 1998, NAT C ART INT; Daume III H., 2005, ICML; Fern Alan, 2007, SPEEDUP LEARNING; Gomes Carla P., 2008, CONNECTIONS NETWORKS; Gu Zonghao, LATEST ADV MIXED INT; Hutter Frank, 2010, AUTOMATED CONFIGURAT; Kappes J. H., 2013, CVPR; LAND AH, 1960, AUTOMATIC METHOD SOL, V28, P497; Leyton-Brown Kevin, 2000, P ACM C EL COMM; Lowrie Matthew, 1988, P 12 ANN INT COMP SO; Mittelmann Hans, 2014, MIXED INTEGER LINEAR; Nannicini Giacomo, 2011, PROBING ALGORITHM MI; Otten Lars, 2012, UAI; Pupko T, 2002, BIOINFORMATICS, V18, P1116, DOI 10.1093/bioinformatics/18.8.1116; Qian Xian, 2013, TACL; Riedel Sebastian, 2012, EMNLP; Ross Stephane, 2011, P AISTATS; Rothberg Ed, PARALLELISM LINEAR M; Sarkar S, 1998, IEEE T SYST MAN CY A, V28, P535, DOI 10.1109/3468.686716; Schwing A., 2012, ECCV; Syed Umar, 2010, NIPS; Xu YH, 2009, J MACH LEARN RES, V10, P1571; [No title captured]	27	3	3	2	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102056
C	Hennequin, G; Aitchison, L; Lengyel, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Hennequin, Guillaume; Aitchison, Laurence; Lengyel, Mate			Fast Sampling-Based Inference in Balanced Neuronal Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Multiple lines of evidence support the notion that the brain performs probabilistic inference in multiple cognitive domains, including perception and decision making. There is also evidence that probabilistic inference may be implemented in the brain through the (quasi-)stochastic activity of neural circuits, producing samples from the appropriate posterior distributions, effectively implementing a Markov chain Monte Carlo algorithm. However, time becomes a fundamental bottleneck in such sampling-based probabilistic representations: the quality of inferences depends on how fast the neural circuit generates new, uncorrelated samples from its stationary distribution (the posterior). We explore this bottleneck in a simple, linear-Gaussian latent variable model, in which posterior sampling can be achieved by stochastic neural networks with linear dynamics. The well-known Langevin sampling (LS) recipe, so far the only sampling algorithm for continuous variables of which a neural implementation has been suggested, naturally fits into this dynamical framework. However, we first show analytically and through simulations that the symmetry of the synaptic weight matrix implied by LS yields critically slow mixing when the posterior is high-dimensional. Next, using methods from control theory, we construct and inspect networks that are optimally fast, and hence orders of magnitude faster than LS, while being far more biologically plausible. In these networks, strong - but transient - selective amplification of external noise generates the spatially correlated activity fluctuations prescribed by the posterior. Intriguingly, although a detailed balance of excitation and inhibition is dynamically maintained, detailed balance of Markov chain steps in the resulting sampler is violated, consistent with recent findings on how statistical irreversibility can overcome the speed limitation of random walks in other domains.	[Hennequin, Guillaume; Lengyel, Mate] Univ Cambridge, Dept Engn, Computat & Biol Learning Lab, Cambridge, England; [Aitchison, Laurence] UCL, Gatsby Computat Neurosci Unit, London, England	University of Cambridge; University of London; University College London	Hennequin, G (corresponding author), Univ Cambridge, Dept Engn, Computat & Biol Learning Lab, Cambridge, England.	gjeh2@cam.ac.uk; laurence@gatsby.ucl.ac.uk; m.lengyel@eng.cam.ac.uk	Lengyel, Mate/A-6665-2013	Lengyel, Mate/0000-0001-7266-0049	Wellcome Trust; Swiss National Science Foundation; Gatsby Charitable Foundation	Wellcome Trust(Wellcome TrustEuropean Commission); Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission); Gatsby Charitable Foundation	This work was supported by the Wellcome Trust (GH, ML), the Swiss National Science Foundation (GH) and the Gatsby Charitable Foundation (LA). Our code will be made freely available from GH's personal webpage.	Berkes P, 2011, SCIENCE, V331, P83, DOI 10.1126/science.1195870; Bierkens Joris, 2014, ARXIV14018087MATH; Buesing L., 2011, PLOS COMPUTATIONAL B, V7, P1; Fiser J, 2010, TRENDS COGN SCI, V14, P119, DOI 10.1016/j.tics.2010.01.003; Gardiner C W, 2009, HDB STOCHASTIC METHO, V4th; Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x; Grabska-Barwinska A., 2013, ADV NEURAL INFORM PR, V26, P1968; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; Hennequin G, 2014, NEURON, V82, P1394, DOI 10.1016/j.neuron.2014.04.045; Hennequin G, 2012, PHYS REV E, V86, DOI 10.1103/PhysRevE.86.011909; Hinton G. E., 1986, PARALLEL DISTRIBUTED, V1, DOI DOI 10.1234/12345678; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Ichiki A, 2013, PHYS REV E, V88, DOI 10.1103/PhysRevE.88.020101; Knill DC, 2004, TRENDS NEUROSCI, V27, P712, DOI 10.1016/j.tins.2004.10.007; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; Martin J, 2012, SIAM J SCI COMPUT, V34, pA1460, DOI 10.1137/110845598; Mezard M., 2009, INFORM PHYS COMPUTAT, pp 584, DOI [10.1093/acprof:oso/9780198570837.001.0001, DOI 10.1093/ACPROF:OSO/9780198570837.001, DOI 10.1093/ACPROF:OSO/9780198570837.001.0001]; Moreno-Bote R, 2011, P NATL ACAD SCI USA, V108, P12491, DOI 10.1073/pnas.1101430108; Murphy BK, 2009, NEURON, V61, P635, DOI 10.1016/j.neuron.2009.02.005; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Rudelson M, 2009, COMMUN PUR APPL MATH, V62, P1707, DOI 10.1002/cpa.20294; Sun Y., 2010, ADV NEURAL INFORM PR, V23, P2235; Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0; Trefethen L. N., 2005, SPECTRA PSEUDOSPECTR; Turitsyn KS, 2011, PHYSICA D, V240, P410, DOI 10.1016/j.physd.2010.10.003; Vanbiervliet J, 2009, SIAM J OPTIMIZ, V20, P156, DOI 10.1137/070704034; Welling M, 2011, P INT C MACH LEARN	27	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100044
C	Jaggi, M; Smith, V; Takac, M; Terhorst, J; Krishnan, S; Hofmann, T; Jordan, MI		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Jaggi, Martin; Smith, Virginia; Takac, Martin; Terhorst, Jonathan; Krishnan, Sanjay; Hofmann, Thomas; Jordan, Michael, I			Communication-Efficient Distributed Dual Coordinate Ascent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Communication remains the most significant bottleneck in the performance of distributed optimization algorithms for large-scale machine learning. In this paper, we propose a communication-efficient framework, COCOA, that uses local computation in a primal-dual setting to dramatically reduce the amount of necessary communication. We provide a strong convergence rate analysis for this class of algorithms, as well as experiments on real-world distributed datasets with implementations in Spark. In our experiments, we find that as compared to state-of-the-art mini-batch versions of SGD and SDCA algorithms, COCOA converges to the same .001-accurate solution quality on average 25x as quickly.	[Jaggi, Martin; Hofmann, Thomas] Swiss Fed Inst Technol, Zurich, Switzerland; [Smith, Virginia; Terhorst, Jonathan; Krishnan, Sanjay; Jordan, Michael, I] Univ Calif Berkeley, Berkeley, CA USA; [Takac, Martin] Lehigh Univ, Bethlehem, PA 18015 USA	Swiss Federal Institutes of Technology Domain; ETH Zurich; University of California System; University of California Berkeley; Lehigh University	Jaggi, M (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.		Jordan, Michael I/C-5253-2013; Takac, Martin/AAA-8564-2022	Takac, Martin/0000-0001-7455-2025	Simons Institute for the Theory of Computing	Simons Institute for the Theory of Computing	We thank Shivaram Venkataraman, Ameet Talwalkar, and Peter Richtarik for fruitful discussions. MJ acknowledges support by the Simons Institute for the Theory of Computing.	Agarwal A, 2011, ADV NEURAL INFORM PR, P873; Balcan Maria-Florina, 2012, COLT, V23; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Bradley J., 2011, ICML; Dekel O, 2012, J MACH LEARN RES, V13, P165; Duchi J. C., 2013, NIPS; Fercoq O, 2014, IEEE INT WORKS MACH; Hsieh Cho-Jui, 2008, ICML; Liu J., 2014, ICML; Marecek Jakub, 2014, ARXIV14082467; Necoara I, 2013, J PROCESS CONTR, V23, P243, DOI 10.1016/j.jprocont.2012.12.012; Niu F., 2011, NIPS; Richtarik P., 2012, ARXIV12120873; Richtarik P., 2013, ARXIV13102059; Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Shamir O, 2014, PR MACH LEARN RES, V32, P1000; Takac M., 2013, ICML; Takac Martin, 2014, PRIMAL DUAL PA UNPUB; TSITSIKLIS JN, 1986, IEEE T AUTOMAT CONTR, V31, P803, DOI 10.1109/TAC.1986.1104412; Yang T., 2013, ARXIV13121031; Yang T., 2013, NIPS; Yu H.-F., 2010, P 16 ACM SIGKDD INT, P833; Yu H.-F., 2012, ACM T KNOWL DISCOV D, V5, P1; Zaharia M., 2012, NSDI; Zhang YC, 2013, J MACH LEARN RES, V14, P3321; Zinkevich Martin A, 2010, NIPS, V23	30	3	3	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103050
C	Kocak, T; Neu, G; Valko, M; Munos, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Kocak, Tomas; Neu, Gergely; Valko, Michal; Munos, Remi			Efficient learning by implicit exploration in bandit problems with side observations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We consider online learning problems under a a partial observability model capturing situations where the information conveyed to the learner is between full information and bandit feedback. In the simplest variant, we assume that in addition to its own loss, the learner also gets to observe losses of some other actions. The revealed losses depend on the learner's action and a directed observation system chosen by the environment. For this setting, we propose the first algorithm that enjoys near-optimal regret guarantees without having to know the observation system before selecting its actions. Along similar lines, we also define a new partial information setting that models online combinatorial optimization problems where the feedback received by the learner is between semi-bandit and full feedback. As the predictions of our first algorithm cannot be always computed efficiently in this setting, we propose another algorithm with similar properties and with the benefit of always being computationally efficient, at the price of a slightly more complicated tuning mechanism. Both algorithms rely on a novel exploration strategy called implicit exploration, which is shown to be more efficient both computationally and information-theoretically than previously studied exploration strategies for the problem.	[Kocak, Tomas; Neu, Gergely; Valko, Michal; Munos, Remi] INRIA Lille Nord Europe, SequeL Team, Lille, France		Kocak, T (corresponding author), INRIA Lille Nord Europe, SequeL Team, Lille, France.	tomas.kocak@inria.fr; gergely.neu@inria.fr; michal.valko@inria.fr; remi.munos@inria.fr			French Ministry of Higher Education and Research; European Community [270327]; FUI project Hermes	French Ministry of Higher Education and Research; European Community(European Commission); FUI project Hermes	The research presented in this paper was supported by French Ministry of Higher Education and Research, by European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement no270327 (CompLACS), and by FUI project Hermes.	Alon N., 2013, ADV NEURAL INF PROCE, V26, P1612; Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, J COMPUT SYST SCI, V64, P48, DOI 10.1006/jcss.2001.1795; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; CesaBianchi N, 1997, J ACM, V44, P427, DOI 10.1145/258128.258179; Chen W., 2013, ICML 2013, P151; Gyorfi L, 2007, IEEE T INFORM THEORY, V53, P1866, DOI 10.1109/TIT.2007.894660; Hutter M, 2004, LECT NOTES ARTIF INT, V3244, P279; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Koolen W. M., 2010, P 23 ANN C LEARN THE, P93; Mannor Shie, 2011, P NIPS; Neu G, 2013, LECT NOTES ARTIF INT, V8139, P234; Vovk V. G., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory, P371	17	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102023
C	Lieder, F; Plunkett, D; Hamrick, JB; Russell, SJ; Hay, NJ; Griffiths, TL		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Lieder, Falk; Plunkett, Dillon; Hamrick, Jessica B.; Russell, Stuart J.; Hay, Nicholas J.; Griffiths, Thomas L.			Algorithm selection by rational metareasoning as a model of human strategy selection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Selecting the right algorithm is an important problem in computer science, because the algorithm often has to exploit the structure of the input to be efficient. The human mind faces the same challenge. Therefore, solutions to the algorithm selection problem can inspire models of human strategy selection and vice versa. Here, we view the algorithm selection problem as a special case of metareasoning and derive a solution that outperforms existing methods in sorting algorithm selection. We apply our theory to model how people choose between cognitive strategies and test its prediction in a behavioral experiment. We find that people quickly learn to adaptively choose between cognitive strategies. People's choices in our experiment are consistent with our model but inconsistent with previous theories of human strategy selection. Rational metareasoning appears to be a promising framework for reverse-engineering how people choose among cognitive strategies and translating the results into better solutions to the algorithm selection problem.	[Lieder, Falk] Univ Calif Berkeley, Helen Wills Neurosci Inst, Berkeley, CA 94720 USA; [Plunkett, Dillon; Hamrick, Jessica B.; Griffiths, Thomas L.] Univ Calif Berkeley, Dept Psychol, Berkeley, CA USA; [Russell, Stuart J.; Hay, Nicholas J.] Univ Calif Berkeley, EECS Dept, Berkeley, CA USA	University of California System; University of California Berkeley; University of California System; University of California Berkeley; University of California System; University of California Berkeley	Lieder, F (corresponding author), Univ Calif Berkeley, Helen Wills Neurosci Inst, Berkeley, CA 94720 USA.	falk.lieder@berkeley.edu; dillonplunkett@berkeley.edu; jhamrick@berkeley.edu; russell@cs.berkeley.edu			ONR MURI [N00014-13-1-0341]	ONR MURI(MURIOffice of Naval Research)	This work was supported by ONR MURI N00014-13-1-0341.	[Anonymous], [No title captured]; Erev I, 2005, PSYCHOL REV, V112, P912, DOI 10.1037/0033-295X.112.4.912; Gigerenzer G., 2002, BOUNDED RATIONALITY; Gonzalez C, 2011, PSYCHOL REV, V118, P523, DOI 10.1037/a0024558; Harada D., 1998, NIPS 98 WORKSH ABSTR; Jaakkola T., 1997, 6 INT WORKSH ART INT; KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572; Kotthoff L., 2014, AI MAGAZINE; LAGOUDAKIS MG, 2001, P 2001 AAAI FALL S S; Marewski JN, 2014, WIRES COGN SCI, V5, P39, DOI 10.1002/wcs.1265; PAYNE JW, 1988, J EXP PSYCHOL LEARN, V14, P534, DOI 10.1037/0278-7393.14.3.534; Penny WD, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0059655; Rice J. R., 1976, Advances in computers, vol.15, P65, DOI 10.1016/S0065-2458(08)60520-3; Rieskamp J, 2006, J EXP PSYCHOL GEN, V135, P207, DOI 10.1037/0096-3445.135.2.207; RUSSELL S, 1991, ARTIF INTELL, V49, P361, DOI 10.1016/0004-3702(91)90015-C; Shrager J, 1998, PSYCHOL SCI, V9, P405, DOI 10.1111/1467-9280.00076; Siegler RS, 1999, TRENDS COGN SCI, V3, P430, DOI 10.1016/S1364-6613(99)01372-8; Smith-Miles KA, 2008, ACM COMPUT SURV, V41, DOI 10.1145/1456650.1456656	18	3	3	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103003
C	Nitanda, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Nitanda, Atsushi			Stochastic Proximal Gradient Descent with Acceleration Techniques	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Proximal gradient descent (PGD) and stochastic proximal gradient descent (SPGD) are popular methods for solving regularized risk minimization problems in machine learning and statistics. In this paper, we propose and analyze an accelerated variant of these methods in the mini-batch setting. This method incorporates two acceleration techniques: one is Nesterov's acceleration method, and the other is a variance reduction for the stochastic gradient. Accelerated proximal gradient descent (APG) and proximal stochastic variance reduction gradient (Prox-SVRG) are in a trade-off relationship. We show that our method, with the appropriate mini-batch size, achieves lower overall complexity than both APG and Prox-SVRG.	[Nitanda, Atsushi] NTT DATA Math Syst Inc, Shinjuku Ku, 1F Shinanomachi Rengakan,35 Shinanomachi, Tokyo 1600016, Japan		Nitanda, A (corresponding author), NTT DATA Math Syst Inc, Shinjuku Ku, 1F Shinanomachi Rengakan,35 Shinanomachi, Tokyo 1600016, Japan.	nitanda@msi.co.jp						Agarwal A, 2011, ADV NEURAL INFORM PR, P873; [Anonymous], 2012, ARXIV12112717; Dekel O, 2012, J MACH LEARN RES, V13, P165; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y., 2007, CORE DISCUSSION PAPE; Roux R. N., 2012, ADV NEURAL INFORM PR, P2663; Shalev-Shwartz S., 2013, ADV NEURAL INFORM PR, P378; Shalev-Shwartz S, 2014, PR MACH LEARN RES, V32; Xiao L., 2014, ARXIV14034699	11	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103061
C	Paige, B; Wood, F; Doucet, A; Teh, YW		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Paige, Brooks; Wood, Frank; Doucet, Arnaud; Teh, Yee Whye			Asynchronous Anytime Sequential Monte Carlo	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We introduce a new sequential Monte Carlo algorithm we call the particle cascade. The particle cascade is an asynchronous, anytime alternative to traditional sequential Monte Carlo algorithms that is amenable to parallel and distributed implementations. It uses no barrier synchronizations which leads to improved particle throughput and memory efficiency. It is an anytime algorithm in the sense that it can be run forever to emit an unbounded number of particles while keeping within a fixed memory budget. We prove that the particle cascade provides an unbiased marginal likelihood estimator which can be straightforwardly plugged into existing pseudo-marginal methods.	[Paige, Brooks; Wood, Frank] Univ Oxford, Dept Engn Sci, Oxford, England; [Doucet, Arnaud; Teh, Yee Whye] Univ Oxford, Dept Stat, Oxford, England	University of Oxford; University of Oxford	Paige, B (corresponding author), Univ Oxford, Dept Engn Sci, Oxford, England.	brooks@robots.ox.ac.uk; fwood@robots.ox.ac.uk; doucet@stats.ox.ac.uk; y.w.teh@stats.ox.ac.uk			EPSRC [EP/K009850/1, EP/K000276/1, EP/K009362/1]; ERC under the EU's FP7 Programme [617411]; DARPA PPAML through the U.S. AFRL [FA8750-14-2-0004]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); ERC under the EU's FP7 Programme; DARPA PPAML through the U.S. AFRL	Yee Whye Teh's research leading to these results has received funding from EPSRC (grant EP/K009362/1) and the ERC under the EU's FP7 Programme (grant agreement no. 617411). Arnaud Doucet's research is partially funded by EPSRC (grants EP/K009850/1 and EP/K000276/1). Frank Wood is supported under DARPA PPAML through the U.S. AFRL under Cooperative Agreement number FA8750-14-2-0004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation heron. The views and conclusions contained herein are those of the authors and should be not interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA, the U.S. Air Force Research Laboratory or the U.S. Government.	Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; Brockwell A, 2010, ANN STAT, V38, P3387, DOI 10.1214/09-AOS747; Carpenter J, 1999, IEE P-RADAR SON NAV, V146, P2, DOI 10.1049/ip-rsn:19990255; Chopin N, 2013, J R STAT SOC B, V75, P397, DOI 10.1111/j.1467-9868.2012.01046.x; CRISAN D, 1999, MARKOV PROCESS RELAT, V5, P293; Del Moral P., 2004, PROB APPL S; Douc R, 2005, ISPA 2005: Proceedings of the 4th International Symposium on Image and Signal Processing and Analysis, P64, DOI 10.1109/ISPA.2005.195385; Jun S., 2014, P 31 INT C MACH LEAR P 31 INT C MACH LEAR; Murray L. M, 2014, ARXIV13014019; Naesseth CA, 2014, ADV NEUR IN, V27; Paige Brooks, 2014, P 31 INT C MACH LEAR; Whiteley Nick, 2013, ARXIV13092918; Wood F, 2014, P 17 INT C ART INT S	13	3	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102011
C	Rangapuram, SS; Mudrakarta, PK; Hein, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Rangapuram, Syama Sundar; Mudrakarta, Pramod Kaushik; Hein, Matthias			Tight Continuous Relaxation of the Balanced k-Cut Problem	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				EIGENVECTORS; QUALITY	Spectral Clustering as a relaxation of the normalized/ratio cut has become one of the standard graph-based clustering methods. Existing methods for the computation of multiple clusters, corresponding to a balanced k-cut of the graph, are either based on greedy techniques or heuristics which have weak connection to the original motivation of minimizing the normalized cut. In this paper we propose a new tight continuous relaxation for any balanced k-cut problem and show that a related recently proposed relaxation is in most cases loose leading to poor performance in practice. For the optimization of our tight continuous relaxation we propose a new algorithm for the difficult sum-of-ratios minimization problem which achieves monotonic descent. Extensive comparisons show that our method outperforms all existing approaches for ratio cut and other balanced k-cut criteria.	[Rangapuram, Syama Sundar; Mudrakarta, Pramod Kaushik; Hein, Matthias] Saarland Univ, Dept Math & Comp Sci, Saarbrucken, Germany	Saarland University	Rangapuram, SS (corresponding author), Saarland Univ, Dept Math & Comp Sci, Saarbrucken, Germany.							[Anonymous], 2012, NIPS; Arora R., 2011, P 28 INT C MACH LEAR, P761; Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039; Bresson X., 2012, ADV NEURAL INFORM PR; Bresson X., 2013, ADV NEURAL INFORM PR, P1421; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI [10.1109/TPAMI.2007.1115, 10.1109/TP'AMI.2007.1115]; Ding C, 2008, IEEE DATA MINING, P183, DOI 10.1109/ICDM.2008.130; DONATH WE, 1973, IBM J RES DEV, V17, P420, DOI 10.1147/rd.175.0420; Guattery S, 1998, SIAM J MATRIX ANAL A, V19, P701, DOI 10.1137/S0895479896312262; Hagen L., 1991, 1991 IEEE International Conference on Computer-Aided Design. Digest of Technical Papers (91CH3026-2), P10, DOI 10.1109/ICCAD.1991.185177; Hein M., 2011, ADV NEURAL INFORM PR, V24, P2366; Hein M, 2010, ADV NEURAL INFORM PR, V1, P847; Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997; Ng AY, 2002, ADV NEUR IN, V14, P849; Pock T, 2011, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2011.6126441; POTHEN A, 1990, SIAM J MATRIX ANAL A, V11, P430, DOI 10.1137/0611030; Rangapuram S. S., 2012, PROC INT C ARTIF INT; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Soper AJ, 2004, J GLOBAL OPTIM, V29, P225, DOI 10.1023/B:JOGO.0000042115.44455.f3; Szlam A., 2010, P 27 INT C MACH LEAR, P1039, DOI DOI 10.0RG/PAPERS/233.PDF; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Yang ZR, 2010, IEEE T NEURAL NETWOR, V21, P734, DOI 10.1109/TNN.2010.2041361	24	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103034
C	Schober, M; Duvenaud, D; Hennig, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Schober, Michael; Duvenaud, David; Hennig, Philipp			Probabilistic ODE Solvers with Runge-Kutta Means	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Runge-Kutta methods are the classic family of solvers for ordinary differential equations (ODEs), and the basis for the state of the art. Like most numerical methods, they return point estimates. We construct a family of probabilistic numerical methods that instead return a Gauss-Markov process defining a probability distribution over the ODE solution. In contrast to prior work, we construct this family such that posterior means match the outputs of the Runge-Kutta family exactly, thus inheriting their proven good properties. Remaining degrees of freedom not identified by the match to Runge-Kutta are chosen such that the posterior probability measure fits the observed structure of the ODE. Our results shed light on the structure of Runge-Kutta solvers from a new direction, provide a richer, probabilistic output, have low computational cost, and raise new research questions.	[Schober, Michael; Hennig, Philipp] MPI Intelligent Syst, Tubingen, Germany; [Duvenaud, David] Univ Cambridge, Dept Engn, Cambridge, England	Max Planck Society; University of Cambridge	Schober, M (corresponding author), MPI Intelligent Syst, Tubingen, Germany.	mschober@tue.mpg.de; dkd23@cam.ac.uk; phennig@tue.mpg.de						[Anonymous], [No title captured]; Butcher JC., 1963, J AUST MATH SOC, V3, P185, DOI [10.1017/S1446788700027932, DOI 10.1017/S1446788700027932]; Calderhead B., 2008, ADV NEURAL INFORM PR; Ceschino F., 1963, PROBLEMES DIFFERENTI; Chkrebtii O., 2013, 13062365 ARXIV; Dondelinger F, 2013, JMLR WORKSHOP C P, V31, P216; Dormand J.R., 1980, J COMPUT APPL MATH, V6, P19, DOI [10.1016/0771-050X(80)90013-3, DOI 10.1016/0771-050X(80)90013-3]; Ghahramani Z., 1996, CRGTR962 U TOTR DEP; Graepel T., 2003, INT C MACH LEARN ICM; Hairer E., 2012, PRINCETON COMPANION; Hairer E, 2000, MATH COMPUT SIMUL, Vsecond, DOI [DOI 10.1016/0378-4754(87)90083-8, DOI 10.1007/978-3-662-12607-3]; Hennig P., 2014, P 17 INT C ART INT S, V33; Kutta W., 1901, MATH PHY, V46, P435; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Runge C., 1895, MATH ANN, V46, P167, DOI DOI 10.1007/BF01446807; Sarkka S., 2013, BAYESIAN FILTERING S; SHANKS EB, 1966, MATH COMPUT, V20, P21, DOI 10.2307/2004265; Shumway R. H., 1982, Journal of Time Series Analysis, V3, P253, DOI 10.1111/j.1467-9892.1982.tb00349.x; Skilling J., 1991, MAXIMUM ENTROPY BAYE; Wang Yali, 2014, INT C MACH LEARN ICM; Wiener N., 1950, B AM MATH SOC, V56, P378	21	3	3	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102012
C	Soare, M; Lazaric, A; Munos, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Soare, Marta; Lazaric, Alessandro; Munos, Remi			Best-Arm Identification in Linear Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We study the best-arm identification problem in linear bandit, where the rewards of the arms depend linearly on an unknown parameter theta* and the objective is to return the arm with the largest reward. We characterize the complexity of the problem and introduce sample allocation strategies that pull arms to identify the best arm with a fixed confidence, while minimizing the sample budget. In particular, we show the importance of exploiting the global linear structure to improve the estimate of the reward of near-optimal arms. We analyze the proposed strategies and compare their empirical performance. Finally, as a by-product of our analysis, we point out the connection to the G-optimality criterion used in optimal experimental design.	[Soare, Marta; Lazaric, Alessandro; Munos, Remi] INRIA Lille Nord Europe, SequeL Team, Lille, France; [Munos, Remi] Microsoft Res New England, London, England		Soare, M (corresponding author), INRIA Lille Nord Europe, SequeL Team, Lille, France.	marta.soare@inria.fr; alessandro.lazaric@inria.fr; remi.munos@inria.fr			French Ministry of Higher Education and Research; Nord-Pas de Calais Regional Council; FEDER through the "Contrat de Projets Etat Region 2007-2013"; European Community [270327]	French Ministry of Higher Education and Research; Nord-Pas de Calais Regional Council(Region Hauts-de-France); FEDER through the "Contrat de Projets Etat Region 2007-2013"; European Community(European Commission)	This work was supported by the French Ministry of Higher Education and Research, Nord-Pas de Calais Regional Council and FEDER through the "Contrat de Projets Etat Region 2007-2013", and European Community's Seventh Framework Programme under grant agreement no 270327 (project CompLACS).	Audibert J.-Y., 2010, P 23 C LEARN THEOR C; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Bouhtou M., 2010, ELECT NOTES DISCRETE, V36, P67, DOI 10.1016/j.endm.2010.05.086; Bubeck S., 2013, INT C MACHINE LEARNI, P258; Bubeck S., 2009, P 20 INT C ALG LEARN; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Gabillon V., 2011, NEURIPS, P2222; Gabillon V, 2012, P 26 ANN C NEUR INF; Hoffman MW, 2014, JMLR WORKSH CONF PRO, V33, P365; Jamieson K., 2014, P 27 C LEARN THEOR C; Kaufmann E., 2013, C LEARNING THEORY, P228; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Pukelsheim F., 2006, CLASSICS APPL MATH; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; Sagnol G, 2013, DISCRETE APPL MATH, V161, P258, DOI 10.1016/j.dam.2012.07.016; Soare Marta, TECHNICAL REPORT; Yasin Abbasi-Yadkori, 2011, P 25 ANN C NEUR INF; Yu K, 2006, P 23 INT C MACH LEAR, ppp1081	19	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102021
C	Stachenfeld, KL; Botvinick, MM; Gershman, SJ		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Stachenfeld, Kimberly L.; Botvinick, Matthew M.; Gershman, Samuel J.			Design Principles of the Hippocampal Cognitive Map	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				PLACE FIELDS; ENVIRONMENT; NAVIGATION; GOAL; REPRESENTATIONS; LOCATION; MODEL	Hippocampal place fields have been shown to reflect behaviorally relevant aspects of space. For instance, place fields tend to be skewed along commonly traveled directions, they cluster around rewarded locations, and they are constrained by the geometric structure of the environment. We hypothesize a set of design principles for the hippocampal cognitive map that explain how place fields represent space in a way that facilitates navigation and reinforcement learning. In particular, we suggest that place fields encode not just information about the current location, but also predictions about future locations under the current transition distribution. Under this model, a variety of place field phenomena arise naturally from the structure of rewards, barriers, and directional biases as reflected in the transition policy. Furthermore, we demonstrate that this representation of space can support efficient reinforcement learning. We also propose that grid cells compute the eigendecomposition of place fields in part because is useful for segmenting an enclosure along natural boundaries. When applied recursively, this segmentation can be used to discover a hierarchical decomposition of space. Thus, grid cells might be involved in computing subgoals for hierarchical reinforcement learning.	[Stachenfeld, Kimberly L.; Botvinick, Matthew M.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA; [Stachenfeld, Kimberly L.; Botvinick, Matthew M.] Princeton Univ, Dept Psychol, Princeton, NJ 08544 USA; [Gershman, Samuel J.] MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA	Princeton University; Princeton University; Massachusetts Institute of Technology (MIT)	Stachenfeld, KL (corresponding author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.	kls4@princeton.edu; matthewb@princeton.edu; sjgershm@mit.edu						Alvernhe A, 2011, EUR J NEUROSCI, V33, P1696, DOI 10.1111/j.1460-9568.2011.07653.x; [Anonymous], 2005, IDENTIFYING USEFUL S, DOI [DOI 10.1145/1102351.1102454, 10.1145/1102351.1102454]; [Anonymous], 2011, AAAI; Blair HT, 2007, J NEUROSCI, V27, P3211, DOI 10.1523/JNEUROSCI.4724-06.2007; Botvinick MM, 2009, COGNITION, V113, P262, DOI 10.1016/j.cognition.2008.08.011; Brandon MP, 2014, NEURON, V82, P789, DOI 10.1016/j.neuron.2014.04.013; Daw ND, 2005, NAT NEUROSCI, V8, P1704, DOI 10.1038/nn1560; DAYAN P, 1993, NEURAL COMPUT, V5, P613, DOI 10.1162/neco.1993.5.4.613; Derdikman D, 2009, NAT NEUROSCI, V12, P1325, DOI 10.1038/nn.2396; Diuk C, 2013, J NEUROSCI, V33, P5797, DOI 10.1523/JNEUROSCI.5445-12.2013; Fanselow MS, 2010, TRENDS COGN SCI, V14, P7, DOI 10.1016/j.tics.2009.10.008; Fenton A., 2001, EUROPEAN J NEUROSCIE, V12, P3450; Foster DJ, 2000, HIPPOCAMPUS, V10, P1, DOI 10.1002/(SICI)1098-1063(2000)10:1<1::AID-HIPO1>3.0.CO;2-1; Franzius M., 2007, PLOS COMPUTATIONAL B, V3, P3287; Gallistel CR, 1990, ORG LEARNING; Gershman SJ, 2012, NEURAL COMPUT, V24, P1553, DOI 10.1162/NECO_a_00282; Gustafson NJ, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002235; Hafting T, 2005, NATURE, V436, P801, DOI 10.1038/nature03721; HIRTLE SC, 1985, MEM COGNITION, V13, P208, DOI 10.3758/BF03197683; Hollup SA, 2001, J NEUROSCI, V21, P1635, DOI 10.1523/JNEUROSCI.21-05-01635.2001; Howard MW, 2005, PSYCHOL REV, V112, P75, DOI 10.1037/0033-295X.112.1.75; Howard MW, 2002, J MATH PSYCHOL, V46, P269, DOI 10.1006/jmps.2001.1388; Kobayashi T, 2003, NEUROSCIENCE, V117, P1025, DOI 10.1016/S0306-4522(02)00700-5; Krupic J, 2012, SCIENCE, V337, P853, DOI 10.1126/science.1222403; Lenck-Santini PP, 2001, EUR J NEUROSCI, V13, P1055, DOI 10.1046/j.0953-816x.2001.01481.x; Lenck-Santini PP, 2002, J NEUROSCI, V22, P9035; Mahadevan S, 2009, FOUND TRENDS MACH LE, V1, P403, DOI 10.1561/2200000003; McNaughton BL, 2006, NAT REV NEUROSCI, V7, P663, DOI 10.1038/nrn1932; Mehta MR, 2000, NEURON, V25, P707, DOI 10.1016/S0896-6273(00)81072-7; Mehta MR, 1997, P NATL ACAD SCI USA, V94, P8918, DOI 10.1073/pnas.94.16.8918; Menache I, 2002, LECT NOTES ARTIF INT, V2430, P295; Morgan LK, 2011, J NEUROSCI, V31, P1238, DOI 10.1523/JNEUROSCI.4667-10.2011; Muller RU, 1996, J GEN PHYSIOL, V107, P663, DOI 10.1085/jgp.107.6.663; MULLER RU, 1987, J NEUROSCI, V7, P1951; O'Keefe J, 1978, HIPPOCAMPUS COGNITIV; Reid AK, 1998, PSYCHOL REV, V105, P585, DOI 10.1037/0033-295X.105.3.585; Ribas-Fernandes JJF, 2011, NEURON, V71, P370, DOI 10.1016/j.neuron.2011.05.042; Schapiro A. C., 2013, NAT NEUROSCI, V16; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Skaggs WE, 1998, J NEUROSCI, V18, P8455; SPEAKMAN A, 1990, EUR J NEUROSCI, V2, P544, DOI 10.1111/j.1460-9568.1990.tb00445.x; Sprekeler H, 2011, NEURAL COMPUT, V23, P3287, DOI 10.1162/NECO_a_00214; STEVENS A, 1978, COGNITIVE PSYCHOL, V10, P422, DOI 10.1016/0010-0285(78)90006-3; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; TOLMAN EC, 1948, PSYCHOL REV, V55, P189, DOI 10.1037/h0061626; Wills TJ, 2010, SCIENCE, V328, P1573, DOI 10.1126/science.1188224; Wiltgen BJ, 2006, J NEUROSCI, V26, P5484, DOI 10.1523/JNEUROSCI.2685-05.2006	48	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101011
C	Wilson, AG; Gilboa, E; Nehorai, A; Cunningham, JP		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wilson, Andrew Gordon; Gilboa, Elad; Nehorai, Arye; Cunningham, John P.			Fast Kernel Learning for Multidimensional Pattern Extrapolation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The ability to automatically discover patterns and perform extrapolation is an essential quality of intelligent systems. Kernel methods, such as Gaussian processes, have great potential for pattern extrapolation, since the kernel flexibly and interpretably controls the generalisation properties of these methods. However, automatically extrapolating large scale multidimensional patterns is in general difficult, and developing Gaussian process models for this purpose involves several challenges. A vast majority of kernels, and kernel learning methods, currently only succeed in smoothing and interpolation. This difficulty is compounded by the fact that Gaussian processes are typically only tractable for small datasets, and scaling an expressive kernel learning approach poses different challenges than scaling a standard Gaussian process model. One faces additional computational constraints, and the need to retain significant model structure for expressing the rich information available in a large dataset. In this paper, we propose a Gaussian process approach for large scale multidimensional pattern extrapolation. We recover sophisticated out of class kernels, perform texture extrapolation, inpainting, and video extrapolation, and long range forecasting of land surface temperatures, all on large multidimensional datasets, including a problem with 383,400 training points. The proposed method significantly outperforms alternative scalable and flexible Gaussian process methods, in speed and accuracy. Moreover, we show that a distinct combination of expressive kernels, a fully non-parametric representation, and scalable inference which exploits existing model structure, are critical for large scale multidimensional pattern extrapolation.	[Wilson, Andrew Gordon] CMU, Pittsburgh, PA 15213 USA; [Gilboa, Elad; Nehorai, Arye] WUSTL, St Louis, MO USA	Carnegie Mellon University; Washington University (WUSTL)	Wilson, AG (corresponding author), CMU, Pittsburgh, PA 15213 USA.				ONR [N000141410684]; NIH [R01GM093156]; Simons Foundation [325171, 325233]; Grossman Center at Columbia	ONR(Office of Naval Research); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Simons Foundation; Grossman Center at Columbia	AGW thanks ONR grant N000141410684 and NIH grant R01GM093156. JPC thanks Simons Foundation grants SCGB #325171, #325233, and the Grossman Center at Columbia.	Adams R., 2013, INT C MACH LEARN; Atkinson K. E., 2008, INTRO NUMERICAL ANAL; Baker C., 1977, NUMERICAL TREATMENT; Bochner Salomon, 1959, LECT FOURIER INTEGRA, V42; Duvenaud D, 2013, INT C MACH LEARN; Gilboa E., 2013, IEEE T PATTERN ANAL; Gonen M, 2011, J MACH LEARN RES, V12, P2211; Guillemot C, 2014, IEEE SIGNAL PROC MAG, V31, P127, DOI 10.1109/MSP.2013.2273004; Hays J, 2008, COMMUN ACM, V51, P87, DOI 10.1145/1400181.1400202; Hensman J., 2013, UNCERTAINTY ARTIFICI; Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865; Le Quoc V., 2013, INT C MACH LEARN, P244; Luo Y., 2013, INT C ART INT STAT; MacKay DJC, 1994, ASHRAE T, V100, P1053; Naish-Guzman A., 2007, ADV NEURAL INFORM PR, V20, P1057; OHAGAN A, 1978, J R STAT SOC B, V40, P1; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rahimi A., 2007, P NIPS; Rasmussen C.E, 1996, EVALUATION GAUSSIAN; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Saatci Y, 2011, THESIS; SEEGER M, 2003, WORKSH AI STAT, V9; Snelson Edward, 2006, ADV NEURAL INFORM PR, V3; Williams CKI, 2001, ADV NEUR IN, V13, P682; Williams CKI, 2003, ADV NEURAL INFORM PR, V15, P383; Wilson A.G., 2014, COVARIANCE KERNELS F; Wilson A. G., 2012, TECHNICAL REPORT	27	3	3	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101043
C	Wu, Y; Lobato, JMH; Ghahramani, Z		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wu, Yue; Lobato, Jose Miguel Hernandez; Ghahramani, Zoubin			Gaussian Process Volatility Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ASSET RETURNS; INFERENCE	The prediction of time-changing variances is an important task in the modeling of financial data. Standard econometric models are often limited as they assume rigid functional relationships for the evolution of the variance. Moreover, functional parameters are usually learned by maximum likelihood, which can lead to overfitting. To address these problems we introduce GP-Vol, a novel non-parametric model for time-changing variances based on Gaussian Processes. This new model can capture highly flexible functional relationships for the variances. Furthermore, we introduce a new online algorithm for fast inference in GP-Vol. This method is much faster than current offline inference procedures and it avoids overfitting problems by following a fully Bayesian approach. Experiments with financial data show that GP-Vol performs significantly better than current standard alternatives.	[Wu, Yue; Lobato, Jose Miguel Hernandez; Ghahramani, Zoubin] Univ Cambridge, Cambridge, England	University of Cambridge	Wu, Y (corresponding author), Univ Cambridge, Cambridge, England.	wu5@post.harvard.edu; jmh233@cam.ac.uk; zoubin@eng.cam.ac.uk	Wu, Yue/AAJ-7041-2021	Wu, Yue/0000-0002-2917-5862				Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; BAUM LE, 1966, ANN MATH STAT, V37, P1554, DOI 10.1214/aoms/1177699147; Bekaert G, 2000, REV FINANC STUD, V13, P1, DOI 10.1093/rfs/13.1.1; BOLLERSLEV T, 1986, J ECONOMETRICS, V31, P307, DOI 10.1016/0304-4076(86)90063-1; Brandt MW, 2006, J BUS ECON STAT, V24, P470, DOI 10.1198/073500106000000206; CAMPBELL JY, 1992, J FINANC ECON, V31, P281, DOI 10.1016/0304-405X(92)90037-X; Cont R, 2001, QUANT FINANC, V1, P223, DOI 10.1080/713665670; Deisenroth M. P., 2009, P 26 ANN INT C MACH, P225; Deisenroth Marc P., 2012, ADV NEURAL INFORM PR, P2618; Dem J., 2006, J MACH LEARN RES, V7, P1; Doucet A., 2001, SEQUENTIAL MONTE CAR; Frigola R., 2013, ADV NEURAL INFORM PR, V26, P3156; GLOSTEN LR, 1993, J FINANC, V48, P1779, DOI 10.2307/2329067; HARVEY A, 1994, REV ECON STUD, V61, P247, DOI 10.2307/2297980; HENTSCHEL L, 1995, J FINANC ECON, V39, P71, DOI 10.1016/0304-405X(94)00821-H; Kim S, 1998, REV ECON STUD, V65, P361, DOI 10.1111/1467-937X.00050; Ko J, 2009, AUTON ROBOT, V27, P75, DOI 10.1007/s10514-009-9119-x; Lazaro-Gredilla M., 2011, P INT C MACH LEARN M, P841; Lindsten F., 2012, ADV NEURAL INFORM PR, V25, P2600; Liu J., 1999, COMBINED PARAMETER S; NELSON DB, 1991, ECONOMETRICA, V59, P347, DOI 10.2307/2938260; Pitt MK, 1999, J AM STAT ASSOC, V94, P590, DOI 10.2307/2670179; Poon SH, 2005, FINANC ANAL J, V61, P45, DOI 10.2469/faj.v61.n1.2683; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Turner R., 2010, W CP, P868; Wilson AG., 2010, ADV NEURAL INFORM PR, P2460; Wu Y., 2013, P 30 INT C MACH LEAR, P558	28	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101047
C	Yerebakan, HZ; Rajwa, B; Dundar, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Yerebakan, Halid Z.; Rajwa, Bartek; Dundar, Murat			The Infinite Mixture of Infinite Gaussian Mixtures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Dirichlet process mixture of Gaussians (DPMG) has been used in the literature for clustering and density estimation problems. However, many real-world data exhibit cluster distributions that cannot be captured by a single Gaussian. Modeling such data sets by DPMG creates several extraneous clusters even when clusters are relatively well-defined. Herein, we present the infinite mixture of infinite Gaussian mixtures (I-2 GMM) for more flexible modeling of data sets with skewed and multi-modal cluster distributions. Instead of using a single Gaussian for each cluster as in the standard DPMG model, the generative model of I-2 GMM uses a single DPMG for each cluster. The individual DPMGs are linked together through centering of their base distributions at the atoms of a higher level DP prior. Inference is performed by a collapsed Gibbs sampler that also enables partial parallelization. Experimental results on several artificial and real-world data sets suggest the proposed I-2 GMM model can predict clusters more accurately than existing variational Bayes and Gibbs sampler versions of DPMG.	[Yerebakan, Halid Z.; Dundar, Murat] IUPUI, Dept Comp & Informat Sci, Indianapolis, IN 46202 USA; [Rajwa, Bartek] Purdue Univ, Bindley Biosci Ctr, W Lafayette, IN 47907 USA	Indiana University System; Indiana University-Purdue University Indianapolis; Purdue University System; Purdue University; Purdue University West Lafayette Campus	Yerebakan, HZ (corresponding author), IUPUI, Dept Comp & Informat Sci, Indianapolis, IN 46202 USA.	hzyereba@cs.iupui.edu; rajwa@cyto.purdue.edu; dundar@cs.iupui.edu	Rajwa, Bartek/B-3169-2009; Dundar, M. Murat/C-3846-2011; Dundar, Murat/U-1837-2019	Rajwa, Bartek/0000-0001-7540-8236; 	National Science Foundation (NSF) [IIS-1252648]; National Institute of Biomedical Imaging and Bioengineering (NIBIB) [5R21EB015707]; PhRMA Foundation	National Science Foundation (NSF)(National Science Foundation (NSF)); National Institute of Biomedical Imaging and Bioengineering (NIBIB)(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Biomedical Imaging & Bioengineering (NIBIB)); PhRMA Foundation	This research was sponsored by the National Science Foundation (NSF) under Grant Number IIS-1252648 (CAREER), by the National Institute of Biomedical Imaging and Bioengineering (NIBIB) under Grant Number 5R21EB015707, and by the PhRMA Foundation (2012 Research Starter Grant in Informatics). The content is solely the responsibility of the authors and does not represent the official views of NSF, NIBIB or PhRMA.	Aghaeepour N, 2013, NAT METHODS, V10, P228, DOI [10.1038/NMETH.2365, 10.1038/nmeth.2365]; Aldous D.J., 1985, LECT NOTES MATH, V1117, P1, DOI DOI 10.1007/BFB0099421; Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104; Cron AJ, 2011, AM STAT, V65, P16, DOI 10.1198/tast.2011.10170; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; FlowCAP, FLOW CYT CRIT ASS PO; Ishwaran H, 2001, J AM STAT ASSOC, V96, P161, DOI 10.1198/016214501750332758; Kim S, 2007, ADV NEURAL INFORM PR, V19, P697; Kurihara K., 2002, ADV NEURAL INFORM PR, V19; Lichman M, 2013, UCI MACHINE LEARNING; Pyne S, 2009, P NATL ACAD SCI USA, V106, P8519, DOI 10.1073/pnas.0903028106; Rodriguez A, 2008, J AM STAT ASSOC, V103, P1131, DOI 10.1198/016214508000000553; Sudderth EB, 2008, INT J COMPUT VISION, V77, P291, DOI 10.1007/s11263-007-0069-5; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302	15	3	3	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100078
C	Bererton, C; Gordon, G; Thrun, S; Khosla, P		Thrun, S; Saul, K; Scholkopf, B		Bererton, C; Gordon, G; Thrun, S; Khosla, P			Auction mechanism design for multi-robot coordination	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					The design of cooperative multi-robot systems is a highly active research area in robotics. Two lines of research in particular have generated interest: the solution of large, weakly coupled MDPs, and the design and implementation of market architectures. We propose a new algorithm which joins together these two lines of research. For a class of coupled MDPs, our algorithm automatically designs a market architecture which causes a decentralized multi-robot system to converge to a consistent policy. We can show that this policy is the same as the one which would be produced by a particular centralized planning algorithm. We demonstrate the new algorithm on three simulation examples: multi-robot towing, multi-robot path planning with a limited fuel resource, and coordinating behaviors in a game of paint ball.	Carnegie Mellon Univ, Pittsburgh, PA 15217 USA	Carnegie Mellon University	Bererton, C (corresponding author), Carnegie Mellon Univ, 5000 Forbes Ave, Pittsburgh, PA 15217 USA.	curt@cs.cmu.edu; ggordon@cs.cmu.edu; thrun@cs.cmu.edu; pkk@cs.cmu.edu						BENNEWITZ M, 2001, IEEE INT C ROB AUT I; Burgard W ., 2000, P IEEE INT C ROB AUT; CAO YU, 1997, AUTON ROBOT, V1, P1; Carlos Guestrin and Geoffrey Gordon, 2002, UNCERTAINTY ARTIFICI, V18; Chvatal V., 1983, LINEAR PROGRAMMING; Dantzig G., 1963, LINEAR PROGRAMMING T; DIAS M, 2001, MARKET APPROACH MULT; GERKEY BP, SOLD MARKET METHODS; Goldberg D., 2000, IRIS00387 USC; Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X; KITANO H, 1998, P ROB 97 1 ROB WORLD; PARKER LE, 1996, J ADV ROBOTICS, V10; Rardin R. L., 1998, OPTIMIZATION OPERATI, V166; ROSENCRANTZ M, 2003, ACM AGENTS; Roumeliotis SI, 2000, DISTRIBUTED AUTONOMOUS ROBOTIC SYSTEMS, P179; SALIDO J, 1997, P INT C SENS FUS DEC, P90; Zlot R, 2002, MULTIROBOT EXPLORATI	17	3	3	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						879	886						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500110
C	de Sa, VR		Thrun, S; Saul, K; Scholkopf, B		de Sa, VR			Sensory modality segregation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Why are sensory modalities segregated the way they are? In this paper we show that sensory modalities are well designed for self-supervised cross-modal learning. Using the Minimizing-Disagreement algorithm on an unsupervised speech categorization task with visual (moving lips) and auditory (sound signal) inputs, we show that very informative auditory dimensions actually harm performance when moved to the visual side of the network. It is better to throw them away than to consider them part of the "visual input". We explain this finding in terms of the statistical structure in sensory inputs.	Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA	University of California System; University of California San Diego	de Sa, VR (corresponding author), Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA.							Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; De Sa V.R., 1997, PSYCHOL LEARN MOTIV, V36, P309; de Sa VR, 1998, NEURAL COMPUT, V10, P1097, DOI 10.1162/089976698300017368; DODWELL PC, 1990, PSYCHOL REV; DURGIN FH, P ANN M PSYCH SOC; KOHONEN T, 1990, IJCNN INT JOINT C NE, V1; MCCOLLOUGH C, 1965, SCIENCE, V149, P1115, DOI 10.1126/science.149.3688.1115; Muslea I., 2002, INT C MACH LEARN, VVolume 2, P435; POLANA R, 1994, THESIS U ROCHESTER	10	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						913	920						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500114
C	Fukumizu, K; Bach, FR; Jordan, MI		Thrun, S; Saul, K; Scholkopf, B		Fukumizu, K; Bach, FR; Jordan, MI			Kernel dimensionality reduction for supervised learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				REGRESSION	We propose a novel method of dimensionality reduction for supervised learning. Given a regression or classification problem in which we wish to predict a variable Y from an explanatory vector X, we treat the problem of dimensionality reduction as that of finding a low-dimensional "effective subspace" of X which retains the statistical relationship between X and Y. We show that this problem can be formulated in terms of conditional independence. To turn this formulation into an optimization problem, we characterize the notion of conditional independence using covariance operators on reproducing kernel Hilbert spaces; this allows us to derive a contrast function for estimation of the effective subspace. Unlike many conventional methods, the proposed method requires neither assumptions on the marginal distribution of X, nor a parametric model of the conditional distribution of Y.	Inst Stat Math, Tokyo 1068569, Japan	Research Organization of Information & Systems (ROIS); Institute of Statistical Mathematics (ISM) - Japan	Fukumizu, K (corresponding author), Inst Stat Math, Tokyo 1068569, Japan.		Jordan, Michael I/C-5253-2013	Fukumizu, Kenji/0000-0002-3488-2625				ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; BACH FR, 2002, J MACHINE LEARNING R, V3, P1; BREIMAN L, 1985, J AM STAT ASSOC, V80, P580, DOI 10.2307/2288473; FRIEDMAN JH, 1981, J AM STAT ASSOC, V76, P817, DOI 10.2307/2287576; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; LI KC, 1992, J AM STAT ASSOC, V87, P1025, DOI 10.1080/01621459.1992.10476258; LI KC, 1991, J AM STAT ASSOC, V86, P316, DOI 10.2307/2290563; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Scholkopf B., 1999, ADV KERNEL METHODS S; Wold H., 1985, ENCY STAT SCI, DOI DOI 10.1002/0471667196	12	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						81	88						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500011
C	Kording, KP; Wolpert, DM		Thrun, S; Saul, K; Scholkopf, B		Kording, KP; Wolpert, DM			Probabilistic inference in human sensorimotor processing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					When we learn a new motor skill, we have to contend with both the variability inherent in our sensors and the task. The sensory uncertainty can be reduced by using information about the distribution of previously experienced tasks. Here we impose a distribution on a novel sensorimotor task and manipulate the variability of the sensory feedback. We show that subjects internally represent both the distribution of the task as well as their sensory uncertainty. Moreover, they combine these two sources of information in a way that is qualitatively predicted by optimal Bayesian processing. We further analyze if the subjects can represent multimodal distributions such as mixtures of Gaussians. The results show that the CNS employs probabilistic models during sensorimotor learning even when the priors are multimodal.	UCL London, Inst Neurol, London WC1N 3BG, England	University of London; University College London	Kording, KP (corresponding author), UCL London, Inst Neurol, London WC1N 3BG, England.		Kording, Konrad P/A-1233-2007	Kording, Konrad P/0000-0001-8408-4499; Wolpert, Daniel/0000-0003-2011-2790				Basso MA, 1998, J NEUROSCI, V18, P7519; BERNRDO JM, 1994, BAYESIAN THEORY; BERROU C, 1993, P ICC 93 GEN SWITZ; CARPENTER RH, NATURE, V377, P59; COX RP, 1946, AM J PHYS, V17, P1; Fiorillo CD, 2003, SCIENCE, V299, P1898, DOI 10.1126/science.1077349; Goodbody SJ, 1998, J NEUROPHYSIOL, V79, P1825, DOI 10.1152/jn.1998.79.4.1825; Platt ML, 1999, NATURE, V400, P233, DOI 10.1038/22268; SIMONCELLI EP, 1996, P 3 INT C IM PROC; Stickgold R, 2000, NAT NEUROSCI, V3, P1237, DOI 10.1038/81756; Weiss Y, 2002, NAT NEUROSCI, V5, P598, DOI 10.1038/nn858	11	3	3	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1327	1334						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500165
C	Miyawaki, Y; Okada, M		Thrun, S; Saul, K; Scholkopf, B		Miyawaki, Y; Okada, M			Mechanism of neural interference by transcranial magnetic stimulation: network or single neuron?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				HUMAN VISUAL-CORTEX; ORIENTATION SELECTIVITY; NEOCORTICAL NEURONS; COIL STIMULATION; MODEL; INHIBITION; PERCEPTION	This paper proposes neural mechanisms of transcranial magnetic stimulation (TMS). TMS can stimulate the brain non-invasively through a brief magnetic pulse delivered by a coil placed on the scalp, interfering with specific cortical functions with a high temporal resolution. Due to these advantages, TMS has been a popular experimental tool in various neuroscience fields. However, the neural mechanisms underlying TMS-induced interference are still unknown; a theoretical basis for TMS has not been developed. This paper provides computational evidence that inhibitory interactions in a neural population, not an isolated single neuron, play a critical role in yielding the neural interference induced by TMS.	RIKEN, Brain Sci Inst, Wako, Saitama 3510198, Japan	RIKEN	Miyawaki, Y (corresponding author), RIKEN, Brain Sci Inst, Wako, Saitama 3510198, Japan.			Miyawaki, Yoichi/0000-0001-9581-926X				Amassian V. E., 2002, HDB TRANSCRANIAL MAG, P323; AMASSIAN VE, 1989, ELECTROEN CLIN NEURO, V74, P458, DOI 10.1016/0168-5597(89)90036-1; AMASSIAN VE, 1993, BRAIN RES, V605, P317, DOI 10.1016/0006-8993(93)91758-K; BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844; INGHILLERI M, 1993, J PHYSIOL-LONDON, V466, P521; Kamitani Y, 1999, NAT NEUROSCI, V2, P767, DOI 10.1038/11245; Kamitani Y, 2001, NEUROCOMPUTING, V38, P697, DOI 10.1016/S0925-2312(01)00447-7; Kammer T, 1998, NEUROPSYCHOLOGIA, V36, P1161, DOI 10.1016/S0028-3932(98)00003-7; Mainen ZF, 1996, NATURE, V382, P363, DOI 10.1038/382363a0; NAGARAJAN SS, 1993, IEEE T BIO-MED ENG, V40, P1175, DOI 10.1109/10.245636; PRIORI A, 1995, ELECTROMYOGR MOTOR C, V97, P69, DOI 10.1016/0924-980X(94)00224-U; Ray PG, 1998, J CLIN NEUROPHYSIOL, V15, P351, DOI 10.1097/00004691-199807000-00007; ROTH BJ, 1990, IEEE T BIO-MED ENG, V37, P588, DOI 10.1109/10.55662; SOMERS DC, 1995, J NEUROSCI, V15, P5448; Sompolinsky H, 1997, CURR OPIN NEUROBIOL, V7, P514, DOI 10.1016/S0959-4388(97)80031-1; Walsh V, 2000, NAT REV NEUROSCI, V1, P73, DOI 10.1038/35036239; Ziemann U, 1996, J PHYSIOL-LONDON, V496, P873, DOI 10.1113/jphysiol.1996.sp021734	17	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1295	1302						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500161
C	Opper, M; Winther, O		Thrun, S; Saul, K; Scholkopf, B		Opper, M; Winther, O			Variational linear response	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					A general linear response method for deriving improved estimates of correlations in the variational Bayes framework is presented. Three applications are given and it is discussed how to use linear response as a general principle for improving mean field approximations.	Aston Univ, Sch Engn & Appl Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England	Aston University	Opper, M (corresponding author), Aston Univ, Sch Engn & Appl Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England.							ATTIAS H, 2000, ADV NEURAL INFORMATI, V12; Bishop Christopher M., 2003, ARTIFICIAL INTELLIGE; BISHOP CM, 2002, ADV NEURAL INFORMATI, V15; Ghahramani Z, 2001, ADV NEUR IN, V13, P507; Hojen-Sorensen PADFR, 2002, NEURAL COMPUT, V14, P889, DOI 10.1162/089976602317319009; Kappen HJ, 1998, NEURAL COMPUT, V10, P1137, DOI 10.1162/089976698300017386; MacKay D. J. C., 2003, INFORM THEORY INFERE, P468; Miskin J. W., 2000, ADV INDEPENDENT COMP; Opper M, 2001, ADV MEAN FIELD METHO; Welling M., 2003, ARTIFICIAL INTELLIGE; WELLING M, 2003, PROPAGATION RULES LI	13	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1157	1164						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500144
C	Platt, JC		Thrun, S; Saul, K; Scholkopf, B		Platt, JC			Fast embedding of sparse music similarity graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					This paper applies fast sparse multidimensional scaling (MDS) to a large graph of music similarity, with 267K vertices that represent artists, albums, and tracks; and 3.22M edges that represent similarity between those entities. Once vertices are assigned locations in a Euclidean space, the locations can be used to browse music and to generate playlists. MDS on very large sparse graphs can be effectively performed by a family of algorithms called Rectangular Dijsktra (RD) MDS algorithms. These RD algorithms operate on a dense rectangular slice of the distance matrix, created by calling Dijsktra a constant number of times. Two RD algorithms are compared: Landmark MDS, which uses the Nystrom approximation to perform MDS; and a new algorithm called Fast Sparse Embedding, which uses FastMap. These algorithms compare favorably to Laplacian Eigenmaps, both in terms of speed and embedding quality.	Microsoft Res, Redmond, WA 98052 USA	Microsoft	Platt, JC (corresponding author), Microsoft Res, 1 Microsoft Way, Redmond, WA 98052 USA.	jplatt@microsoft.com	Platt, John/GOH-2678-2022	Platt, John/0000-0002-5652-5303				Baker C., 1977, NUMERICAL TREATMENT; BENGIO Y, 2004, P NIPS, V16; Bradley AP, 1997, PATTERN RECOGN, V30, P1145, DOI 10.1016/S0031-3203(96)00142-2; COX TF, 2001, MONOGRAPHS STAT APPL, V88; Dijkstra EW, 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]; ELLIS DPW, 2002, P INT C MUS INF RETR; Faloutsos C., 1995, P 1995 ACM SIGMOD IN, P163; FLOYD RW, 1962, COMMUN ACM, V5, P345, DOI 10.1145/367766.368168; FOWLKES C, 2001, P CVPR, V1; JOHNSON DB, 1977, J ACM, V24, P1, DOI 10.1145/321992.321993; Platt JC, 2002, ADV NEUR IN, V14, P1425; Silva V.D., 2003, NIPS, P721; TAKANE Y, 1977, PSYCHOMETRIKA, V42, P7, DOI 10.1007/BF02293745; Tenenbaum JB, 1998, ADV NEUR IN, V10, P682	14	3	3	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						571	578						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500072
C	Tenore, F; Etienne-Cummings, R; Lewis, MA		Thrun, S; Saul, K; Scholkopf, B		Tenore, F; Etienne-Cummings, R; Lewis, MA			Entrainment of silicon central pattern generators for legged locomotory control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				ADAPTATION	A second generation CPG chip capable of generating the necessary timing to control the leg of a walking machine was constructed. This chip builds on the basic results achieved on a previous chip. Various improvements - most importantly the larger number of more sophisticated silicon neurons - were implemented to enhance its capabilities and to move toward a significantly more versatile device. The new neurons are all interconnected and synaptic weights on each neuron can be easily programmed. This allows on-chip creation of neural networks used to control a robotic leg. The enhanced versatility thus obtained allows us to get closer to a self-contained locomotion controller for walking robots.	Johns Hopkins Univ, Dept Elect & Comp Engn, Baltimore, MD 21218 USA	Johns Hopkins University	Tenore, F (corresponding author), Johns Hopkins Univ, Dept Elect & Comp Engn, Baltimore, MD 21218 USA.		Tenore, Francesco/H-2146-2013; Etienne-Cummings, Ralph/A-3227-2010	Tenore, Francesco/0000-0002-1197-9643; 				ARSHAVSKY YI, 1983, TRENDS NEUROSCI, V6, P417, DOI 10.1016/0166-2236(83)90191-1; COHEN AH, 1999, AUTONOMOUS ROBOTS SP, V7, P225; FORSSBERG H, 1975, BRAIN RES, V85, P103, DOI 10.1016/0006-8993(75)91013-6; HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764; Lewis MA, 2002, AUTON ROBOT, V12, P301, DOI 10.1023/A:1015221832567; Lewis MA, 2003, BIOL CYBERN, V88, P137, DOI 10.1007/s00422-002-0365-7; LEWIS MA, 2000, P INT C ROB AUT SAN; MORRIS C, 1981, BIOPHYS J, V35, P193, DOI 10.1016/S0006-3495(81)84782-0; RASCHE C, 1998, NEUROMORPHIC SYSTEMS; Simoni MF, 1999, IEEE T CIRCUITS-II, V46, P967, DOI 10.1109/82.775396	10	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1043	1050						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500130
C	Yuille, A; Fang, F; Schrater, P; Kersten, D		Thrun, S; Saul, K; Scholkopf, B		Yuille, A; Fang, F; Schrater, P; Kersten, D			Human and ideal observers for detecting image curves	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				NATURAL IMAGES; GESTALT LAWS; INTEGRATION; STATISTICS	This paper compares the ability of human observers to detect target image curves with that of an ideal observer. The target curves are sampled from a generative model which specifies (probabilistically) the geometry and local intensity properties of the curve. The ideal observer performs Bayesian inference on the generative model using MAP estimation. Varying the probability model for the curve geometry enables us investigate whether human performance is best for target curves that obey specific shape statistics, in particular those observed on natural shapes. Experiments are performed with data on both rectangular and hexagonal lattices. Our results show that human observers' performance approaches that of the ideal observer and are, in general, closest to the ideal for conditions where the target curve tends to be straight or similar to natural statistics on curves. This suggests a bias of human observers towards straight curves and natural statistics.	Univ Calif Los Angeles, Dept Stat & Psychol, Los Angeles, CA 90024 USA	University of California System; University of California Los Angeles	Yuille, A (corresponding author), Univ Calif Los Angeles, Dept Stat & Psychol, Los Angeles, CA 90024 USA.	yuille@stat.ucla.edu; fang0057@tc.umn.edu; schrater@umn.edu; kersten@umn.edu		Yuille, Alan L./0000-0001-5207-9249				BRADY MJ, 1999, THESIS U MINNESOTA; FIELD DJ, 1993, VISION RES, V33, P173, DOI 10.1016/0042-6989(93)90156-Q; Field DJ, 2000, SPATIAL VISION, V13, P51, DOI 10.1163/156856800741018; Geisler WS, 2001, VISION RES, V41, P711, DOI 10.1016/S0042-6989(00)00277-7; Geman D, 1996, IEEE T PATTERN ANAL, V18, P1, DOI 10.1109/34.476006; Hess R, 1999, TRENDS COGN SCI, V3, P480, DOI 10.1016/S1364-6613(99)01410-2; KOVACS I, 1993, P NATL ACAD SCI USA, V90, P7495, DOI 10.1073/pnas.90.16.7495; LEE AB, 2000, INT J COMPUTER V OCT; REN X, 2002, P ECCV; RUDERMAN DL, 1994, PHYS REV LETT, V73, P814, DOI 10.1103/PhysRevLett.73.814; Sigman M, 2001, P NATL ACAD SCI USA, V98, P1935, DOI 10.1073/pnas.031571498; Wainwright MJ, 2000, ADV NEUR IN, V12, P855; Yuille AL, 2001, INT J COMPUT VISION, V41, P9, DOI 10.1023/A:1011156931605; YUILLE AL, 2000, IEEE PAMI, V22; Zhu SC, 1999, IEEE T PATTERN ANAL, V21, P1170, DOI 10.1109/34.809110	16	3	3	1	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1459	1466						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500181
C	Zheng, AX; Jordan, MI; Liblit, B; Aiken, A		Thrun, S; Saul, K; Scholkopf, B		Zheng, AX; Jordan, MI; Liblit, B; Aiken, A			Statistical debugging of sampled programs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We present a novel strategy for automatically debugging programs given sampled data from thousands of actual user runs. Our goal is to pinpoint those features that are most correlated with crashes. This is accomplished by maximizing an appropriately defined utility function. It has analogies with intuitive debugging heuristics, and, as we demonstrate, is able to deal with various types of bugs that occur in real programs.	Univ Calif Berkeley, EE Div, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Zheng, AX (corresponding author), Univ Calif Berkeley, EE Div, Berkeley, CA 94720 USA.		Jordan, Michael I/C-5253-2013	Liblit, Ben/0000-0002-2245-2839				Blum AL, 1997, ARTIF INTELL, V97, P245, DOI 10.1016/S0004-3702(97)00063-5; Elisseeff A., 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616; Hastie T., 2009, ELEMENTS STAT LEARNI, V2nd, DOI DOI 10.1007/978-0-387-21606-5; Hiriart-Urruty J-B, 1993, CONVEX ANAL MINIMIZA, VII; JAPKOWICZ N, 2002, INTELLIGENT DATA ANA, V6; Lehmann E RJ, 2005, TESTING STAT HYPOTHE, V3; LIBLIT B, 2003, ACM SIGPLAN PLDI 200	7	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						603	610						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500076
C	Bofill, A; Murray, AF; Thompson, DP		Dietterich, TG; Becker, S; Ghahramani, Z		Bofill, A; Murray, AF; Thompson, DP			Circuits for VLSI implementation of temporally-asymmetric Hebbian learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Experimental data has shown that synaptic strength modification in some types of biological neurons depends upon precise spike timing differences between presynaptic and postsynaptic spikes. Several temporally-asymmetric Hebbian learning rules motivated by this data have been proposed. We argue that such learning rules are suitable to analog VLSI implementation. We describe an easily tunable circuit to modify the weight of a silicon spiking neuron according to those learning rules. Test results from the fabrication of the circuit using a 0.6mum CMOS process are given.	Univ Edinburgh, Dept Elect Engn, Edinburgh EH9 3JL, Midlothian, Scotland	University of Edinburgh	Bofill, A (corresponding author), Univ Edinburgh, Dept Elect Engn, Edinburgh EH9 3JL, Midlothian, Scotland.							Abbott LF, 1999, ADV NEUR IN, V11, P69; GERSTNER W, 1999, PULSED NEURAL NETWOR; Hafliger P, 1997, ADV NEUR IN, V9, P692; Indiveri G, 2000, NEURAL COMPUT, V12, P2857, DOI 10.1162/089976600300014755; Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213; MURRAY AF, 1987, ELECTRON LETT, V23, P642, DOI 10.1049/el:19870459; MURRAY AF, 1994, NEURAL COMPUTING ANA; Rao RPN, 2000, ADV NEUR IN, V12, P164; Shams M, 1998, IEEE T VLSI SYST, V6, P563, DOI 10.1109/92.736128; Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829; van Schaik A., 1996, Proceedings of the Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems. MicroNeuro'96, P52, DOI 10.1109/MNNFS.1996.493772; Zhang LI, 1998, NATURE, V395, P37, DOI 10.1038/25665	12	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1091	1098						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100136
C	Chang, YH; Kaelbling, LP		Dietterich, TG; Becker, S; Ghahramani, Z		Chang, YH; Kaelbling, LP			Playing is believing: The role of beliefs in multi-agent learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We propose a new classification for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. Using this classification, we review the optimality of existing algorithms, including the case of interleague play. We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the long-run against fair opponents.	MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Chang, YH (corresponding author), MIT, Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.							BOWLING M, UNPUB MULTIAGENT LEA; Claus C., 1998, P 15 NAT C ART INT; Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738; FUDENBERG D, 1995, J ECON DYN CONTROL, V19, P1065, DOI 10.1016/0165-1889(94)00819-4; HALL K, 2001, DIMACS WORKSH COMP I; Hu J., 1998, P 15 INT C MACH LEAR; Littman M. L., 1994, P 11 INT C MACH LEAR; Littman M. L., 2001, 17 INT JOINT C ART I; LITTMAN ML, 2001, P 18 INT C MACH LEAR; NACHBAR JH, 1996, EC THEORY; NAGAYUKI Y, 2000, P INT C MULT SYST IC; Singh S, 2000, P 16 C UNC ART INT	12	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1483	1490						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100184
C	Courville, AC; Touretzky, DS		Dietterich, TG; Becker, S; Ghahramani, Z		Courville, AC; Touretzky, DS			Modeling temporal structure in classical conditioning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				INFORMATION	The Temporal Coding Hypothesis of Miller and colleagues [7] suggests that animals integrate related temporal patterns of stimuli into single memory representations. We formalize this concept using quasi-Bayes estimation to update the parameters of a constrained hidden Markov model. This approach allows us to account for some surprising temporal effects in the second order conditioning experiments of Miller et al. [1, 2, 3], which other models are unable to explain.	Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Courville, AC (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.							[Anonymous], 1927, CONDITIONAL REFLEXES; BARNET RC, 1991, LEARN MOTIV, V22, P253, DOI 10.1016/0023-9690(91)90008-V; Cole RP, 1999, LEARN MOTIV, V30, P129, DOI 10.1006/lmot.1998.1027; COLE RP, 1995, ANIM LEARN BEHAV, V23, P144, DOI 10.3758/BF03199929; DAYAN P, 1993, NEURAL COMPUT, V5, P613, DOI 10.1162/neco.1993.5.4.613; Huo Q, 1997, IEEE T SPEECH AUDI P, V5, P161, DOI 10.1109/89.554778; KRISHNAMURTHY V, 1993, IEEE T SIGNAL PROCES, V41, P2557, DOI 10.1109/78.229888; MATZEL LD, 1988, LEARN MOTIV, V19, P317, DOI 10.1016/0023-9690(88)90044-6; Miller R.R., 1993, CURR DIR PSYCHOL SCI, V2, P106, DOI [10.1111/1467-8721.ep10772577, DOI 10.1111/1467-8721.EP10772577]; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Rescorla R. A., 1972, CLASSICAL CONDITIONI; SMITH AFM, 1978, J R STAT SOC B, V40, P106; Suri RE, 2001, NEURAL COMPUT, V13, P841, DOI 10.1162/089976601300014376; Sutton R.S., 1990, LEARNING COMPUTATION, P497; Sutton R. S., 1985, 7 ANN C COGN SCI SOC, P54	15	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						3	10						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100001
C	Genov, R; Cauwenberghs, G		Dietterich, TG; Becker, S; Ghahramani, Z		Genov, R; Cauwenberghs, G			Stochastic mixed-signal VLSI architecture for high-dimensional kernel machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					A mixed-signal paradigm is presented for high-resolution parallel inner-product computation in very high dimensions, suitable for efficient implementation of kernels in image processing. At the core of the externally digital architecture is a high-density, low-power analog array performing binary-binary partial matrix-vector multiplication. Full digital resolution is maintained even with low-resolution analog-to-digital conversion, owing to random statistics in the analog summation of binary products. A random modulation scheme produces near-Bernoulli statistics even for highly correlated inputs. The approach is validated with real image data, and with experimental results from a CID/DRAM analog array prototype in 0.5 mum CMOS.	Johns Hopkins Univ, Dept Elect & Comp Engn, Baltimore, MD 21218 USA	Johns Hopkins University	Genov, R (corresponding author), Johns Hopkins Univ, Dept Elect & Comp Engn, Baltimore, MD 21218 USA.							Cauwenberghs G., 1995, Advances in Neural Information Processing Systems 7, P779; Cauwenberghs G., 1999, LEARNING SILICON ADA; CHIANG AM, 1990, IEEE J SOLID-ST CIRC, V25, P1510, DOI 10.1109/4.62187; GENOV R, 2001, IEEE T CIRCUITS SYST, V48; Gersho A., 1992, VECTOR QUANTIZATION; HAN G, 1996, P IEEE INT S CIRC SY, V3, P495; HOWES MJ, 1979, CHARGE COUPLED DEVIC; Indiveri G, 1996, IEEE MICRO, V16, P40, DOI 10.1109/40.540079; KUB FJ, 1990, IEEE J SOLID-ST CIRC, V25, P207, DOI 10.1109/4.50305; MURRAY AF, 1993, ADV NEURAL INFORMATI, V5, P491; NEUGEBAUER C, 1991, P IEEE INT JOINT C N, V1, P447; Papageorgiou C., 1998, P INT C COMP VIS; PEDRONI V, 1992, P IEEE INT JOINT C N, V3, P620; Vapnik V.N., 1999, NATURE STAT LEARNING; Wawrzynek J, 1996, ADV NEUR IN, V8, P619	15	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1099	1105						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100137
C	Ikeda, S; Tanaka, T; Amari, S		Dietterich, TG; Becker, S; Ghahramani, Z		Ikeda, S; Tanaka, T; Amari, S			Information geometrical framework for analyzing belief propagation decoder	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				TURBO	The mystery of belief propagation (BP) decoder, especially of the turbo decoding, is studied from information geometrical viewpoint. The loopy belief network (BN) of turbo codes makes it difficult to obtain the true "belief" by BP, and the characteristics of the algorithm and its equilibrium are not clearly understood. Our study gives an intuitive understanding of the mechanism, and a new framework for the analysis. Based on the framework, we reveal basic properties of the turbo decoding.	JST, Kyushu Inst Technol, Fukuoka 8080196, Japan	Japan Science & Technology Agency (JST); Kyushu Institute of Technology	Ikeda, S (corresponding author), JST, Kyushu Inst Technol, Fukuoka 8080196, Japan.		Tanaka, Toshiyuki/C-2749-2011; Ikeda, Shiro/E-1736-2016	Ikeda, Shiro/0000-0002-2462-1448				Amari S., 2000, METHODS INFORM GEOME; Berrou C, 1996, IEEE T COMMUN, V44, P1261, DOI 10.1109/26.539767; IKEDA S, 2001, UNPUB IEEE T INFORMA; KABASHIMA Y, 2001, ADV MEAN FIELD METHO, P65; McEliece RJ, 1998, IEEE J SEL AREA COMM, V16, P140, DOI 10.1109/49.661103; Richardson T, 2000, IEEE T INFORM THEORY, V46, P9, DOI 10.1109/18.817505; TANAKA T, 2002, ADV NEURAL INFORMATI, V14; TANAKA T, 2001, ADV MEAN FIELD METHO, P259; YEDIDIA JS, 2001, TR200116 MITS EL RES	9	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						407	414						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100051
C	Kositsky, M; Barto, AG		Dietterich, TG; Becker, S; Ghahramani, Z		Kositsky, M; Barto, AG			The emergence of multiple movement units in the presence of noise and feedback delay	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				DIRECTED MOVEMENTS; MODEL; INFORMATION	Tangential hand velocity profiles of rapid human arm movements often appear as sequences of several bell-shaped acceleration-deceleration phases called submovements or movement units. This suggests how the nervous system might efficiently control a motor plant in the presence of noise and feedback delay. Another critical observation is that stochasticity in a motor control problem makes the optimal control policy essentially different from the optimal control policy for the deterministic case. We use a simplified dynamic model of an arm and address rapid aimed arm movements. We use reinforcement learning as a tool to approximate the optimal policy in the presence of noise and feedback delay. Using a simplified model we show that multiple submovements emerge as an optimal policy in the presence of noise and feedback delay. The optimal policy in this situation is to drive the arm's end point close to the target by one fast submovement and then apply a few slow submovements to accurately drive the arm's end point into the target region. In our simulations, the controller sometimes generates corrective submovements before the initial fast submovement is completed, much like the predictive corrections observed in a number of psychophysical experiments.	Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Kositsky, M (corresponding author), Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.							ALBUS JS, 1975, J DYNAMIC SYSTEMS ME, V97, P220, DOI DOI 10.1115/1.3426922; Barto AG, 1999, NEURAL COMPUT, V11, P565, DOI 10.1162/089976699300016575; BEAUBATON D, 1986, HUM MOVEMENT SCI, V5, P19, DOI 10.1016/0167-9457(86)90003-5; Beiser, 1995, MODELS INFORM PROCES, P215; Berthier NE, 1996, DEV PSYCHOL, V32, P811; CARLTON LG, 1981, J EXP PSYCHOL HUMAN, V7, P1019, DOI 10.1037/0096-1523.7.5.1019; GHEZ C, 1979, INTEGRATION NERVOUS, P305; GIELEN CCAM, 1987, BIOL CYBERN, V57, P217, DOI 10.1007/BF00338815; Harris CM, 1998, NATURE, V394, P780, DOI 10.1038/29528; Houk JC, 1995, MODELS INFORM PROCES, P249; KOSITSKY M, 1998, THESIS WEIZMANN I SC; MEYER DE, 1988, PSYCHOL REV, V95, P340, DOI 10.1037/0033-295X.95.3.340; MIALL RC, 1993, J MOTOR BEHAV, V25, P53, DOI 10.1080/00222895.1993.9941639; MILLER LE, 1984, THESIS NW U EVANSTON; Novak KE, 2000, EXP BRAIN RES, V132, P419, DOI 10.1007/s002210000366; PARTRIDGE LD, 1973, ENG PRINCIPLES PHYSL, P47; Plamondon R, 1997, BEHAV BRAIN SCI, V20, P279, DOI 10.1017/S0140525X97001441; REDON C, 1991, J MOTOR BEHAV, V23, P101, DOI 10.1080/00222895.1991.9942027; Robinson DA, 1975, BASIC MECH OCULAR MO, P337; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; VONHOFSTEN C, 1991, J MOTOR BEHAV, V23, P280, DOI 10.1080/00222895.1991.9942039; WU CH, 1990, MULTIPLE MUSCLE SYST, P214; ZELEVINSKY L, 1998, THESIS U MASSACHUSET	23	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						43	50						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100006
C	Lagoudakis, MG; Parr, R		Dietterich, TG; Becker, S; Ghahramani, Z		Lagoudakis, MG; Parr, R			Model-free least squares policy iteration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We propose a new approach to reinforcement learning which combines least squares function approximation with policy iteration. Our method is model-free and completely off policy. We are motivated by the least squares temporal difference learning algorithm (LSTD), which is known for its efficient use of sample experiences compared to pure temporal difference algorithms. LSTD is ideal for prediction problems, however it heretofore has not had a straightforward application to control problems. Moreover, approximations learned by LSTD are strongly influenced by the visitation distribution over states. Our new algorithm, Least Squares Policy Iteration (LSPI) addresses these issues. The result is an off-policy method which can use (or reuse) data collected from any source. We have tested LSPI on several problems, including a bicycle simulator in which it learns to guide the bicycle to a goal efficiently by merely observing a relatively small number of completely random trials.	Duke Univ, Dept Comp Sci, Durham, NC 27708 USA	Duke University	Lagoudakis, MG (corresponding author), Duke Univ, Dept Comp Sci, Durham, NC 27708 USA.	mgl@cs.duke.edu; parr@cs.duke.edu						Bartlett P. L, 2000, P 17 INT C MACH LEAR, P41; Boyan JA, 1999, MACHINE LEARNING, PROCEEDINGS, P49; KOLLER D, 2000, P 16 C UNC ART INT U; KONDA V, 1999, ADV NEURAL INFORMATI, V12; LAGOUDAKIS MG, 2001, CS200105 DUK U DEP C; NG A, 2000, ADV NEURAL INFORMATI, V12; NG A, 2000, P 16 C UNC ART INT U; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; ORMONEIT D, 2001, IN PRESS MACHINE LEA; Randlov J., 1998, 15 INT C MACH LEARN; SUTTON R, 2000, ADV NEURAL INFORMATI, V12	13	3	3	0	3	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1547	1554						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100192
C	Malzahn, D; Opper, M		Dietterich, TG; Becker, S; Ghahramani, Z		Malzahn, D; Opper, M			A variational approach to learning curves	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				GAUSSIAN-PROCESSES; MODEL	We combine the replica approach from statistical physics with a variational approach to analyze learning curves analytically. We apply the method to Gaussian process regression. As a main result we derive approximative relations between empirical error measures, the generalization error and the posterior variance.	Aston Univ, Sch Engn & Appl Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England	Aston University	Malzahn, D (corresponding author), Aston Univ, Sch Engn & Appl Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England.	malzahnd@aston.ac.uk; opperm@aston.ac.uk						Feynman R. P., 1965, QUANTUM MECH PATH IN; GAREL T, 1988, EUROPHYS LETT, V6, P307, DOI 10.1209/0295-5075/6/4/005; Malzahn D, 2001, LECT NOTES COMPUT SC, V2130, P271; Malzahn D, 2001, ADV NEUR IN, V13, P273; Mezard M., 1987, SPIN GLASS THEORY IN; SOLLICH P, 2002, NEURAL INFORMATION P, V14	9	3	3	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						463	469						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100058
C	Meinicke, P; Ritter, H		Dietterich, TG; Becker, S; Ghahramani, Z		Meinicke, P; Ritter, H			Quantizing density estimators	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				MAXIMUM-LIKELIHOOD; PRINCIPAL CURVES; MODELS	We suggest a nonparametric framework for unsupervised learning of projection models in terms of density estimation on quantized sample spaces. The objective is not to optimally reconstruct the data but instead the quantizer is chosen to optimally reconstruct the density of the data. For the resulting quantizing density estimator (QDE) we present a general method for parameter estimation and model selection. We show how projection sets which correspond to traditional unsupervised methods like vector quantization or PCA appear in the new framework. For a principal component quantizer we present results on synthetic and real-world data, which show that the QDE can improve the generalization of the kernel density estimator although its estimate is based on significantly lower-dimensional projection indices of the data.	Univ Bielefeld, Neuroinformat Grp, Bielefeld, Germany	University of Bielefeld	Meinicke, P (corresponding author), Univ Bielefeld, Neuroinformat Grp, Bielefeld, Germany.							Buhmann J. M., 1998, Neural Networks and Machine Learning. Proceedings, P57; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; GEMAN S, 1982, ANN STAT, V10, P401, DOI 10.1214/aos/1176345782; HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936; Jones MC, 1996, J AM STAT ASSOC, V91, P401, DOI 10.2307/2291420; Kegl B, 2000, IEEE T PATTERN ANAL, V22, P281, DOI 10.1109/34.841759; Meinicke P, 2001, NEURAL COMPUT, V13, P453, DOI 10.1162/089976601300014600; MEINICKE P, 2000, THESIS U BIELEFELD; ROSE K, 1990, PHYS REV LETT, V65, P945, DOI 10.1103/PhysRevLett.65.945; Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674; Scott D. W., 1992, MULTIVARIATE DENSITY, DOI 10.1002/9780470316849; Silverman B.W., 1986, DENSITY ESTIMATION S, V26; Smola AJ, 1999, LECT NOTES ARTIF INT, V1572, P214; Vapnik VN, 2000, ADV NEUR IN, V12, P659	14	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						825	832						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100103
C	O'Reilly, RC; Soto, R		Dietterich, TG; Becker, S; Ghahramani, Z		O'Reilly, RC; Soto, R			A model of the phonological loop: Generalization and binding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				PREFRONTAL CORTEX; SERIAL ORDER; MEMORY; PRINCIPLES	We present a neural network model that shows how the prefrontal cortex, interacting with the basal ganglia, can maintain a sequence of phonological information in activation-based working memory (i.e., the phonological loop). The primary function of this phonological loop may be to transiently encode arbitrary bindings of information necessary for tasks - the combinatorial expressive power of language enables very flexible binding of essentially arbitrary pieces of information. Our model takes advantage of the closed-class nature of phonemes, which allows different neural representations of all possible phonemes at each sequential position to be encoded. To make this work, we suggest that the basal ganglia provide a region-specific update signal that allocates phonemes to the appropriate sequential coding slot. To demonstrate that flexible, arbitrary binding of novel sequences can be supported by this mechanism, we show that the model can generalize to novel sequences after moderate amounts of training.	Univ Colorado, Dept Psychol, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	O'Reilly, RC (corresponding author), Univ Colorado, Dept Psychol, 345 UCB, Boulder, CO 80309 USA.							Baddeley A, 1998, PSYCHOL REV, V105, P158, DOI 10.1037/0033-295X.105.1.158; Baddeley Alan, 1986, WORKING MEMORY; Burgess N, 1999, PSYCHOL REV, V106, P551, DOI 10.1037/0033-295X.106.3.551; Cohen J. D., 1996, PROSPECTIVE MEMORY T, P267; Cohen JD, 1996, PHILOS T R SOC B, V351, P1515, DOI 10.1098/rstb.1996.0138; Frank MJ, 2001, COGN AFFECT BEHAV NE, V1, P137, DOI 10.3758/CABN.1.2.137; Henson RNA, 1996, Q J EXP PSYCHOL-A, V49, P80, DOI 10.1080/027249896392810; Ivry RB, 1996, CURR OPIN NEUROBIOL, V6, P851, DOI 10.1016/S0959-4388(96)80037-7; LEE CL, 1977, J VERB LEARN VERB BE, V16, P395, DOI 10.1016/S0022-5371(77)80036-4; LEVITT JB, 1993, J COMP NEUROL, V338, P360, DOI 10.1002/cne.903380304; LEWANDOWSKY S, 1989, PSYCHOL REV, V96, P25, DOI 10.1037/0033-295X.96.1.25; Mel BW, 2000, NEURAL COMPUT, V12, P731, DOI 10.1162/089976600300015574; Miller EK, 2001, ANNU REV NEUROSCI, V24, P167, DOI 10.1146/annurev.neuro.24.1.167; MIYAKE A, UNPUB ROLE PHONOLOGI; O'reilly R.C., 2000, COMPUTATIONAL EXPLOR; O'Reilly RC, 1998, TRENDS COGN SCI, V2, P455, DOI 10.1016/S1364-6613(98)01241-8; O'Reilly RC, 2001, PSYCHOL REV, V108, P311, DOI 10.1037//0033-295X.108.2.311; OReilly R., 1999, MODELS WORKING MEMOR, P375, DOI [10.1017/cbo9781139174909.014, DOI 10.1017/CBO9781139174909.014]; OREILLY RC, 2002, ADV NEURAL INFORMATI; OREILLY RC, IN PRESS UNITY CONSC; PAULESU E, 1993, NATURE, V362, P342, DOI 10.1038/362342a0; WICKELGR.WA, 1969, PSYCHOL REV, V76, P1, DOI 10.1037/h0026823	22	3	3	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						83	90						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100011
C	Pelillo, M		Dietterich, TG; Becker, S; Ghahramani, Z		Pelillo, M			Matching free trees with replicator equations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Motivated by our recent work on rooted tree matching, in this paper we provide a solution to the problem of matching two free (i.e., unrooted) trees by constructing an association graph whose maximal cliques are in one-to-one correspondence with maximal common subtrees. We then solve the problem using simple replicator dynamics from evolutionary game theory. Experiments on hundreds of uniformly random trees are presented. The results are impressive: despite the inherent inability of these simple dynamics to escape from local optima, they always returned a globally optimal solution.	Univ Ca Foscari Venezia, Dipartimento Informat, I-30172 Venice, Italy	Universita Ca Foscari Venezia	Pelillo, M (corresponding author), Univ Ca Foscari Venezia, Dipartimento Informat, Via Torino 155, I-30172 Venice, Italy.	pelillo@dsi.unive.it						BLUM H, 1978, PATTERN RECOGN, V10, P167, DOI 10.1016/0031-3203(78)90025-0; Bomze IM, 1997, J GLOBAL OPTIM, V10, P143, DOI 10.1023/A:1008230200610; Bomze IM, 2000, IEEE T NEURAL NETWOR, V11, P1228, DOI 10.1109/72.883403; BOMZE IM, 1999, HDB COMBINATORIAL S, VA, P1; Gold S, 1996, IEEE T PATTERN ANAL, V18, P377, DOI 10.1109/34.491619; MOTZKIN TS, 1965, CANADIAN J MATH, V17, P533, DOI 10.4153/CJM-1965-053-6; Pelillo M, 1999, IEEE T PATTERN ANAL, V21, P1105, DOI 10.1109/34.809105; Pelillo M., 2001, Visual Form 2001. 4th International Workshop on Visual Form IWVF4. Proceedings (Lecture Notes in Computer Science Vol.2059), P583; PELILLO M, 1995, J ARTIF NEURONETW, V0002, P00411; WILF HS, 1981, J ALGORITHM, V2, P204, DOI 10.1016/0196-6774(81)90021-3; Zhu SC, 1996, INT J COMPUT VISION, V20, P187; [No title captured]; [No title captured]	13	3	3	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						865	872						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100108
C	Rattray, M; Basalyga, G		Dietterich, TG; Becker, S; Ghahramani, Z		Rattray, M; Basalyga, G			Scaling laws and local minima in Hebbian ICA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				SEPARATION	We study the dynamics of a Hebbian ICA algorithm extracting a single non-Gaussian component from a high-dimensional Gaussian background. For both on-line and batch learning we find that a surprisingly large number of examples are required to avoid trapping in a sub-optimal state close to the initial conditions. To extract a skewed signal at least O(N-2) examples are required for N-dimensional data and O(N-3) examples are required to extract a symmetrical signal with non-zero kurtosis.	Univ Manchester, Dept Comp Sci, Manchester M13 9PL, Lancs, England	University of Manchester	Rattray, M (corresponding author), Univ Manchester, Dept Comp Sci, Oxford Rd, Manchester M13 9PL, Lancs, England.		Rattray, Magnus/AAE-3297-2021; Rattray, Magnus/B-4393-2009	Rattray, Magnus/0000-0001-8196-5565; Rattray, Magnus/0000-0001-8196-5565				Amari S, 1996, ADV NEUR IN, V8, P757; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; BIEHL M, 1995, J PHYS A-MATH GEN, V28, P643, DOI 10.1088/0305-4470/28/3/018; BIEHL M, 1994, EUROPHYS LETT, V25, P391, DOI 10.1209/0295-5075/25/5/014; Cardoso JF, 1996, IEEE T SIGNAL PROCES, V44, P3017, DOI 10.1109/78.553476; Gardiner C W, 2009, HDB STOCHASTIC METHO, V4th; Hyvarinen A., 1999, Neural Computing Surveys, V2; Hyvarinen A, 1998, SIGNAL PROCESS, V64, P301, DOI 10.1016/S0165-1684(97)00197-7; RATTRAY M, 2002, IN PRESS NEURAL COMP, V14; SAAD D, 1995, PHYS REV LETT, V74, P4337, DOI 10.1103/PhysRevLett.74.4337; SAAD D, 1998, LINE LEARNING NEURAL; WONG KYM, 2000, NEURAL INFORMATION P, V12	12	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						495	501						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100062
C	Wong, KYM; Li, FL		Dietterich, TG; Becker, S; Ghahramani, Z		Wong, KYM; Li, FL			Fast parameter estimation using Green's functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				NEURAL NETWORKS	We propose a method for the fast estimation of hyperparameters in large networks, based on the linear response relation in the cavity method, and an empirical measurement of the Green's function. Simulation results show that it is efficient and precise, when compared with cross-validation and other techniques which require matrix inversion.	Hong Kong Univ Sci & Technol, Dept Phys, Hong Kong, Hong Kong, Peoples R China	Hong Kong University of Science & Technology	Wong, KYM (corresponding author), Hong Kong Univ Sci & Technol, Dept Phys, Clear Water Bay, Hong Kong, Hong Kong, Peoples R China.			Wong, Kwok Yee Michael/0000-0002-3078-4577				Amari S, 1997, IEEE T NEURAL NETWOR, V8, P985, DOI 10.1109/72.623200; Bishop, 1995, NEURAL NETWORKS PATT; Chapelle O, 2000, ADV NEUR IN, V12, P230; Fetter A. L., 2012, QUANTUM THEORY MANY; KEERTHI SS, 2001, CD0102; LARSEN J, 1996, ADV COMPUTATIONAL MA, V5, P269; Opper M, 1996, PHYS REV LETT, V76, P1964, DOI 10.1103/PhysRevLett.76.1964; OPPER M, 1999, ADV LARGE MARGIN CLA, P43; Orr GB, 1998, NEURAL NETWORKS TRIC; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; SAAD D, 1995, PHYS REV LETT, V74, P4337, DOI 10.1103/PhysRevLett.74.4337; Wong KYM, 2000, PHYS REV E, V62, P4036, DOI 10.1103/PhysRevE.62.4036	13	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						535	542						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100067
C	Yao, K; Nakamura, S		Dietterich, TG; Becker, S; Ghahramani, Z		Yao, K; Nakamura, S			Sequential noise compensation by sequential Monte Carlo method	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We present a sequential Monte Carlo method applied to additive noise compensation for robust speech recognition in time-varying noise. The method generates a set of samples according to the prior distribution given by clean speech models and noise prior evolved from previous estimation. An explicit model representing noise effects on speech features is used, so that an extended Kalman filter is constructed for each sample, generating the updated continuous state estimate as the estimation of the noise parameter, and prediction likelihood for weighting each sample. Minimum mean square error (MMSE) inference of the time-varying noise parameter is carried out over these samples by fusion the estimation of samples according to their weights. A residual resampling selection step and a Metropolis-Hastings smoothing step are used to improve calculation efficiency. Experiments were conducted on speech recognition in simulated non-stationary noises, where noise power changed artificially, and highly non-stationary Machinegun noise. In all the experiments carried out, we observed that the method can have significant recognition performance improvement, over that achieved by noise compensation with stationary noise assumption.	ATR Spoken Language Translat Res Labs, Kyoto 6190288, Japan		Yao, K (corresponding author), ATR Spoken Language Translat Res Labs, 2-2-2 Hikaridai Seika Cho, Kyoto 6190288, Japan.	kaisheng.yao@slt.atr.co.jp; satoshi.nakamura@slt.atr.co.jp						FREY B, 2001, EUROSPEECH, P901; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; KIM NS, 1998, IEEE SIGNAL PROCESSI, V5; Liu JS, 1998, J AM STAT ASSOC, V93, P1032, DOI 10.2307/2669847; VARGA AP, 1990, INT CONF ACOUST SPEE, P845, DOI 10.1109/ICASSP.1990.115970; YAO K, 2000, ICSLP, V1, P770; YAO K, 2001, UNPUB EUROSPEECH, P1139	7	3	3	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1213	1220						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100151
C	Aharonov-Barki, R; Meilijson, I; Ruppin, E		Leen, TK; Dietterich, TG; Tresp, V		Aharonov-Barki, R; Meilijson, I; Ruppin, E			Who does what? A novel algorithm to determine function localization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We introduce a novel algorithm, termed PPA (Performance Prediction Algorithm), that quantitatively measures the contributions of elements of a neural system to the tasks it performs. The algorithm identifies the neurons or areas which participate in a cognitive or behavioral task, given data about performance decrease in a small set of lesions. It also allows the accurate prediction of performances due to multi-element lesions. The effectiveness of the new algorithm is demonstrated in two models of recurrent neural networks with complex interactions among the elements. The algorithm is scalable and applicable to the analysis of large neural networks. Given the recent advances in reversible inactivation techniques, it has the potential to significantly contribute to the understanding of the organization of biological nervous systems, and to shed light on the long-lasting debate about local versus distributed computation in the brain.	Hebrew Univ Jerusalem, Interdisciplinary Ctr Neural Computat, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Aharonov-Barki, R (corresponding author), Hebrew Univ Jerusalem, Interdisciplinary Ctr Neural Computat, IL-91904 Jerusalem, Israel.		Ruppin, Eytan/R-9698-2017	Ruppin, Eytan/0000-0002-7862-3940				AHARONOVBARKI R, IN PRESS NEURAL COMP; Barlow R. E., 1972, STAT INFERENCE ORDER; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; KNOTT GD, 2000, PROGR COMPUTER SCI A; Lashley KS., 1929, BRAIN MECH INTELLIGE; Lomber SG, 1999, J NEUROSCI METH, V86, P109, DOI 10.1016/S0165-0270(98)00160-5; McClelland J.L., 1986, PARALLEL DISTRIBUTED; THORPE S, 1995, HDB BRAIN THEORY NEU; WU J, 1994, SCIENCE, V263, P820, DOI 10.1126/science.8303300	10	3	3	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						3	9						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800001
C	Downs, OB		Leen, TK; Dietterich, TG; Tresp, V		Downs, OB			High-temperature expansions for learning models of nonnegative data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Recent work has exploited boundedness of data in the unsupervised learning of new types of generative model. For nonnegative data it was recently shown that the maximum-entropy generative model is a Nonnegative Boltzmann Distribution not a Gaussian distribution, when the model is constrained to match the first and second order statistics of the data. Learning for practical sized problems is made difficult by the need to compute expectations under the model distribution. The computational cost of Markov chain Monte Carlo methods and low fidelity of naive mean field techniques has led to increasing interest in advanced mean field theories and variational methods. Here I present a second-order mean-field approximation for the Nonnegative Boltzmann Machine model, obtained using a "high-temperature" expansion. The theory is tested on learning a bimodal 2-dimensional model, a high-dimensional translationally invariant distribution, and a generative model for handwritten digits.	Princeton Univ, Dept Math, Princeton, NJ 08544 USA	Princeton University	Downs, OB (corresponding author), Princeton Univ, Dept Math, Princeton, NJ 08544 USA.							BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844; Downs OB, 2000, ADV NEUR IN, V12, P428; GEORGES A, 1991, J PHYS A-MATH GEN, V24, P2173, DOI 10.1088/0305-4470/24/9/024; Kappen HJ, 1998, NEURAL COMPUT, V10, P1137, DOI 10.1162/089976698300017386; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Neal R.M., 1997, 9722 U TOR DEP STAT; Socci ND, 1998, ADV NEUR IN, V10, P350; YEDIDIA JS, 2000, TR200026 MITS EL RES	8	3	3	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						465	471						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800066
C	Herbrich, R; Graepel, T		Leen, TK; Dietterich, TG; Tresp, V		Herbrich, R; Graepel, T			Large scale Bayes point machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					The concept of averaging over classifiers is fundamental to the Bayesian analysis of learning. Based on this viewpoint, it has recently been demonstrated for linear classifiers that the centre of mass of version space (the set of all classifiers consistent with the training set) - also known as the Bayes point - exhibits excellent generalisation abilities. However, the billiard algorithm as presented in [4] is restricted to small sample size because it requires O (m(2)) of memory and O (N . m(2)) computational steps where m is the number of training patterns and N is the number of random draws from the posterior distribution. In this paper we present a method based on the simple perceptron learning algorithm which allows to overcome this algorithmic drawback. The method is algorithmically simple and is easily extended to the multi-class case. We present experimental results on the MNIST data set of handwritten digits which show that Bayes point machines (BPMs) are competitive with the current world champion, the support vector machine. In addition, the computational complexity of BPMs can be tuned by varying the number of samples from the posterior. Finally, rejecting test points on the basis of their (approximative) posterior probability leads to a rapid decrease in generalisation error, e.g. 0.1% generalisation error for a given rejection rate of 10%.	Tech Univ Berlin, Dept Comp Sci, Stat Res Grp, Berlin, Germany	Technical University of Berlin	Herbrich, R (corresponding author), Tech Univ Berlin, Dept Comp Sci, Stat Res Grp, Berlin, Germany.							CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; GRAEPEL T, 2001, ADV NEURAL INFORMATI, V13; HERBRICH R, 2001, ADV NEURAL INFORMATI, V13; HERBRICH R, 2000, P ESANN 2000, P49; McAllester D. A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P230, DOI 10.1145/279943.279989; NEAL RM, 1997, TR9722 U TOR DEP STA; NOVIKOFF A, 1962, S MATH THEOR AUT POL, P24; OPPER M, 2000, NEURAL COMPUTATION, V12; Platt JC, 2000, ADV NEUR IN, P61; Rujan P, 2000, ADV NEUR IN, P329; SMOLA AJ, 1998, THESIS TU BERLIN; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; WATKIN TLH, 1993, EUROPHYS LETT, V21, P871, DOI 10.1209/0295-5075/21/8/013; Williams C. K. I., 1997, NCRG97012 AST U	15	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						528	534						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800075
C	Kappen, HJ; Wiegerinck, W		Leen, TK; Dietterich, TG; Tresp, V		Kappen, HJ; Wiegerinck, W			Second order approximations for probability models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					In this paper, we derive a second order mean field theory for directed graphical probability models. By using an information theoretic argument it is shown how this can be done in the absense of a partition function. This method is a direct generalisation of the well-known TAP approximation for Boltzmann Machines. In a numerical example, it is shown that the method greatly improves the first order mean field approximation. For a restricted class of graphical models, so-called single overlap graphs, the second order method has comparable complexity to the first order method. For sigmoid belief networks, the method is shown to be particularly fast and effective.	Univ Nijmegen, Dept Biophys, Nijmegen, Netherlands	Radboud University Nijmegen	Kappen, HJ (corresponding author), Univ Nijmegen, Dept Biophys, Nijmegen, Netherlands.	bert@mbfys.kun.nl; wimw@mbfys.kun.nl	Kappen, H.J./L-4425-2015					Barber D, 1999, ADV NEUR IN, V11, P183; KAPPEN H, 2001, ADV MEAN FIELD THEOR; KAPPEN H, 1999, PHYSICAL REV E, V61, P5658; Kappen HJ, 1998, NEURAL COMPUT, V10, P1137, DOI 10.1162/089976698300017386; LAURITZEN SL, 1988, J ROY STAT SOC B MET, V50, P157; PLEFKA T, 1982, J PHYS A-MATH GEN, V15, P1971, DOI 10.1088/0305-4470/15/6/035; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251; THOULESS DJ, 1977, PHILOS MAG, V35, P593, DOI 10.1080/14786437708235992; Wiegerinck W, 2000, NEW GENERAT COMPUT, V18, P167, DOI 10.1007/BF03037595	9	3	3	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						238	244						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800034
C	Liu, SC; Minch, BA		Leen, TK; Dietterich, TG; Tresp, V		Liu, SC; Minch, BA			Homeostasis in a silicon integrate and fire neuron	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				DEPENDENT CONDUCTANCES; ADAPTATION; MODEL	In this work, we explore homeostasis in a silicon integrate-and-fire neuron. The neuron adapts its firing, rate over long time periods on the order of seconds or minutes so that it returns to its spontaneous firing rate after a lasting perturbation. Homeostasis is implemented via two schemes. One scheme looks at the presynaptic activity and adapts the synaptic weight depending on the presynaptic spiking rate. The second scheme adapts the synaptic "threshold" depending on the neuron's activity. The threshold is lowered if the neuron's activity decreases over a long time and is increased for prolonged increase in postsynaptic activity. Both these mechanisms for adaptation use floating-gate technology. The results shown here are measured from a chip fabricated in a 2-mum CMOS process.	Univ Zurich, Inst Neuroinformat, CH-8057 Zurich, Switzerland	University of Zurich	Liu, SC (corresponding author), Univ Zurich, Inst Neuroinformat, Winterthurstr 190, CH-8057 Zurich, Switzerland.							Desai NS, 1999, NAT NEUROSCI, V2, P515, DOI 10.1038/9165; DIORIO C, 1999, P INT S FUT INT INT, P515; LENZLINGER M, 1969, J APPL PHYS, V40, P278, DOI 10.1063/1.1657043; Liu Z, 1998, J NEUROSCI, V18, P2309; Mead, 1989, ANALOG VLSI NEURAL S; OHZAWA I, 1985, J NEUROPHYSIOL, V54, P651, DOI 10.1152/jn.1985.54.3.651; Shin JH, 1999, IEEE T NEURAL NETWOR, V10, P1232, DOI 10.1109/72.788662; Simoni MF, 1999, IEEE T CIRCUITS-II, V46, P967, DOI 10.1109/82.775396; Stemmler M, 1999, NAT NEUROSCI, V2, P521, DOI 10.1038/9173	9	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						727	733						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800103
C	Myung, IJ; Pitt, MA; Zhang, SB; Balasubramanian, V		Leen, TK; Dietterich, TG; Tresp, V		Myung, IJ; Pitt, MA; Zhang, SB; Balasubramanian, V			The use of MDL to select among computational models of cognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				CATEGORIZATION; INFORMATION; RECOGNITION; SIMILARITY	How should we decide among competing explanations of a cognitive process given limited observations? The problem of model selection is at the heart of progress in cognitive science. In this paper, Minimum Description Length (MDL) is introduced as a method for selecting among computational models of cognition. We also show that differential geometry provides an intuitive understanding of what drives model selection in MDL. Finally, adequacy of MDL is demonstrated in two areas of cognitive modeling.	Ohio State Univ, Dept Psychol, Columbus, OH 43210 USA	Ohio State University	Myung, IJ (corresponding author), Ohio State Univ, Dept Psychol, Columbus, OH 43210 USA.							Akaike H., 1973, 2 INT S INFORM THEOR, P267, DOI DOI 10.1007/978-1-4612-1694-0_15; Amari S.-i., 1985, DIFFERENTIAL GEOMETR, V28; ANDERSON NH, 1981, FDN INFORMATION INTE; Balasubramanian V, 1997, NEURAL COMPUT, V9, P349, DOI 10.1162/neco.1997.9.2.349; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Myung IJ, 2000, P NATL ACAD SCI USA, V97, P11170, DOI 10.1073/pnas.170283897; NOSOFSKY RM, 1986, J EXP PSYCHOL GEN, V115, P39, DOI 10.1037/0096-3445.115.1.39; ODEN GC, 1978, PSYCHOL REV, V85, P172, DOI 10.1037/0033-295X.85.3.172; PITT MA, 2000, UNPUB METHOD SELECTI; REED SK, 1972, COGNITIVE PSYCHOL, V3, P382, DOI 10.1016/0010-0285(72)90014-X; Rissanen JJ, 1996, IEEE T INFORM THEORY, V42, P40, DOI 10.1109/18.481776; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; SHIN HJ, 1992, J EXP PSYCHOL GEN, V121, P278, DOI 10.1037/0096-3445.121.3.278	13	3	3	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						38	44						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800006
C	Natschlager, T; Maass, W		Leen, TK; Dietterich, TG; Tresp, V		Natschlager, T; Maass, W			Finding the key to a synapse	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				NEURONS; INTERNEURONS	Experimental data have shown that synapses are heterogeneous: different synapses respond with different sequences of amplitudes of postsynaptic responses to the same spike train. Neither the role of synaptic dynamics itself nor the role of the heterogeneity of synaptic dynamics for computations in neural circuits is well understood. We present in this article methods that make it feasible to compute for a given synapse with known synaptic parameters the spike train that is optimally fitted to the synapse, for example in the sense that it produces the largest sum of postsynaptic responses. To our surprise we find that most of these optimally fitted spike trains match common firing patterns of specific types of neurons that are discussed in the literature.	Graz Tech Univ, Inst Theoret Comp Sci, A-8010 Graz, Austria	Graz University of Technology	Natschlager, T (corresponding author), Graz Tech Univ, Inst Theoret Comp Sci, A-8010 Graz, Austria.							Bertsekas D. P., 1995, DYNAMIC PROGRAMMING, V1; Brumberg JC, 2000, J NEUROSCI, V20, P4829; Gupta A, 2000, SCIENCE, V287, P273, DOI 10.1126/science.287.5451.273; Markram H, 1998, P NATL ACAD SCI USA, V95, P5323, DOI 10.1073/pnas.95.9.5323; NATSCHLAGER T, 2000, UNPUB COMPUTING OPTI; Powell M.J.D., 1983, MATH PROGRAMMING STA, P288; Steriade M, 1998, J NEUROPHYSIOL, V79, P483, DOI 10.1152/jn.1998.79.1.483; WANG Z, 1993, J NEUROSCI, V13, P2199	8	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						138	144						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800020
C	Ormoneit, D; Glynn, P		Leen, TK; Dietterich, TG; Tresp, V		Ormoneit, D; Glynn, P			Kernel-based reinforcement learning in average-cost problems: An application to optimal portfolio choice	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Many approaches to reinforcement learning combine neural networks or other parametric function approximators with a form of temporal-difference learning to estimate the value function of a Markov Decision Process. A significant disadvantage of those procedures is that the resulting learning algorithms are frequently unstable. In this work, we present a new, kernel-based approach to reinforcement learning which overcomes this difficulty and provably converges to a unique solution. By contrast to existing algorithms, our method can also be shown to be consistent in the sense that its costs converge to the optimal costs asymptotically. Our focus is on learning in an average-cost framework and on a practical application to the optimal portfolio choice problem.	Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Ormoneit, D (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.							Bertsekas D. P., 1995, DYNAMIC PROGRAMMING, V1; Bertsekas D. P., 1995, DYNAMIC PROGRAMMING, V2; BOYAN JA, 1995, NIPS, V7; Gordon G., 1999, THESIS CARNEGIE MELL; ORMONEIT D, 2001, IN PRESS MACHINE LEA; ORMONEIT D, UNPUB KERNEL BASED R; Rust J, 1997, ECONOMETRICA, V65, P487, DOI 10.2307/2171751; SUTTON R, 2000, NIPS, V12; Tsitsiklis JN, 1996, MACH LEARN, V22, P59, DOI 10.1007/BF00114724; Tsitsiklis JN, 1999, AUTOMATICA, V35, P1799, DOI 10.1016/S0005-1098(99)00099-0; TSITSIKLIS JN, 2000, NIPS, V12	11	3	3	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						1068	1074						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800150
C	Sallans, B; Hinton, GE		Leen, TK; Dietterich, TG; Tresp, V		Sallans, B; Hinton, GE			Using free energies to represent Q-values in a multiagent reinforcement learning task	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					The problem of reinforcement learning in large factored Markov decision processes is explored. The Q-value of a state-action pair is approximated by the free energy of a product of experts network. Network parameters are learned on-line using a modified SARSA algorithm which minimizes the inconsistency of the Q-values of consecutive state-action pairs. Actions are chosen based on the current value estimates by fixing the current state and sampling actions from the network using Gibbs sampling. The algorithm is tested on a co-operative multi-agent task. The product of experts model is found to perform comparably to table-based Q-learning for small instances of the task, and continues to perform well when the problem becomes too large for a table-based representation.	Univ Toronto, Dept Comp Sci, Toronto, ON M5S 2Z9, Canada	University of Toronto	Sallans, B (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON M5S 2Z9, Canada.	sallans@cs.toronto.edu; hinton@gatsby.ucl.ac.uk						BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; HINTON G, 2000, 2000004 GCNU TR UCL; Jaakkola T., 1995, Advances in Neural Information Processing Systems 7, P345; Jaakkola T., 1997, THESIS MIT CAMBRIDGE; Littman M., 1995, P INT C MACH LEARN; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; RUMMERY GA, 1994, 166 CUED F INFENG TR; Sabes PN, 1996, ADV NEUR IN, V8, P1080; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Sutton RS, 1996, ADV NEUR IN, V8, P1038; SUTTON RS, 1990, P INT C MACH LEARN; TOURETZKY DS, 1996, ADV NEURAL INFORMATI, V8	14	3	3	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						1075	1081						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800151
C	Scarpetta, S; Li, ZP; Hertz, J		Leen, TK; Dietterich, TG; Tresp, V		Scarpetta, S; Li, ZP; Hertz, J			Spike-timing-dependent learning for oscillatory networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				HIPPOCAMPAL-NEURONS; OLFACTORY-BULB; PLASTICITY	We apply to oscillatory networks a class of learning rules in which synaptic weights change proportional to pre- and post-synaptic activity, with a kernel A(tau) measuring the effect for a postsynaptic spike a timer after the presynaptic one. The resulting synaptic matrices have an outer-product form in which the oscillating patterns are represented as complex vectors. In a simple model, the even part of A(tau) enhances the resonant response to learned stimulus by reducing the effective damping, while the odd part determines the frequency of oscillation. We relate our model to the olfactory cortex and hippocampus and their presumed roles in forming associative memories and input representations.	Univ Salerno, Dipartimento Fis, I-84081 Baronissi, Italy	University of Salerno	Scarpetta, S (corresponding author), Univ Salerno, Dipartimento Fis, I-84081 Baronissi, Italy.		scarpetta, silvia/D-5217-2014; Scarpetta, Silvia/M-8591-2015	Scarpetta, Silvia/0000-0003-4189-0065				Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998; Debanne D, 1998, J PHYSIOL-LONDON, V507, P237, DOI 10.1111/j.1469-7793.1998.237bu.x; HASSELMO ME, 1993, NEURAL COMPUT, V5, P32, DOI 10.1162/neco.1993.5.1.32; LI Z, 1989, BIOL CYBERN, V61, P379, DOI 10.1007/BF00200803; Li ZP, 2000, NETWORK-COMP NEURAL, V11, P83, DOI 10.1088/0954-898X/11/1/305; Magee JC, 1997, SCIENCE, V275, P209, DOI 10.1126/science.275.5297.209; Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213	7	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						152	158						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800022
C	Adorjan, P; Schwabe, L; Piepenbrock, C; Obermayer, K		Solla, SA; Leen, TK; Muller, KR		Adorjan, P; Schwabe, L; Piepenbrock, C; Obermayer, K			Recurrent cortical competition: Strengthen or weaken?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				ORIENTATION SELECTIVITY; VISUAL-CORTEX; MODEL; NEURONS; CODE	We investigate the short term dynamics of the recurrent competition and neural activity in the primary visual cortex in terms of information processing and in the context of orientation selectivity. We propose that after stimulus onset, the strength of the recurrent excitation decreases due to fast synaptic depression. As a consequence, the network shifts from an initially highly nonlinear to a more linear operating regime. Sharp orientation tuning is established in the first highly competitive phase. In the second and less competitive phase, precise signaling of multiple orientations and long range modulation, e.g., by intra- and inter-areal connections becomes possible (surround effects). Thus the network first extracts the salient features from the stimulus, and then starts to process the details. We show that this signal processing strategy is optimal if the neurons have limited bandwidth and their objective is to transmit the maximum amount of information in any time interval beginning with the stimulus onset.	Epigen GmbH, D-10435 Berlin, Germany		Adorjan, P (corresponding author), Epigen GmbH, Kastanienallee 24, D-10435 Berlin, Germany.							Abbott LF, 1997, SCIENCE, V275, P220, DOI 10.1126/science.275.5297.221; Adorjan P, 1999, VISUAL NEUROSCI, V16, P303, DOI 10.1017/S0952523899162114; Adorjan P, 1999, REV NEUROSCIENCE, V10, P181; Artun OB, 1998, P NATL ACAD SCI USA, V95, P11999, DOI 10.1073/pnas.95.20.11999; Baddeley R, 1996, NATURE, V381, P560, DOI 10.1038/381560a0; BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844; Carandini M, 1997, VISION RES, V37, P3061, DOI 10.1016/S0042-6989(97)00100-4; FIELD DJ, 1994, NEURAL COMPUT, V6, P559, DOI 10.1162/neco.1994.6.4.559; Lee DK, 1999, NAT NEUROSCI, V2, P375, DOI 10.1038/7286; SOMERS DC, 1995, J NEUROSCI, V15, P5448; Sompolinsky H, 1997, CURR OPIN NEUROBIOL, V7, P514, DOI 10.1016/S0959-4388(97)80031-1; Troyer TW, 1998, J NEUROSCI, V18, P5908; Tsodyks MV, 1997, P NATL ACAD SCI USA, V94, P719, DOI 10.1073/pnas.94.2.719	14	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						89	95						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700013
C	Barber, D; Sollich, P		Solla, SA; Leen, TK; Muller, KR		Barber, D; Sollich, P			Gaussian fields for approximate inference in layered Sigmoid Belief Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Layered Sigmoid Belief Networks are directed graphical models in which the local conditional probabilities are parameterised by weighted sums of parental states. Learning and inference in such networks are generally intractable, and approximations need to be considered. Progress in learning these networks has been made by using variational procedures. We demonstrate, however, that variational procedures can be inappropriate for the equally important issue of inference - that is, calculating marginals of the network. We introduce an alternative procedure, based on assuming that the weighted input to a node is approximately Gaussian distributed. Our approach goes beyond previous Gaussian field assumptions in that we take into account correlations between parents of nodes. This procedure is specialized for calculating marginals and is significantly faster and simpler than the variational procedure.	Aston Univ, NCRG, Birmingham B4 7ET, W Midlands, England	Aston University	Barber, D (corresponding author), Aston Univ, NCRG, Birmingham B4 7ET, W Midlands, England.	barberd@aston.ac.uk; peter.sollich@kcl.ac.uk	Sollich, Peter/H-2174-2011; Sollich, Peter/ABC-2993-2020	Sollich, Peter/0000-0003-0169-7893; 				BARBER D, 1999, ADV NEURAL INFORMATI, V11; Castillo E., 1997, EXPERT SYSTEMS PROBA; Jordan MI, 1998, NATO ADV SCI I D-BEH, V89, P105; KEARNS M, 1999, ADV NEURAL INFORMATI, V11; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251; SAUL LK, 1998, LEARNING GRAPHICAL M	7	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						393	399						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700056
C	Briegel, T; Tresp, V		Solla, SA; Leen, TK; Muller, KR		Briegel, T; Tresp, V			Robust neural network regression for offline and online learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We replace the commonly used Gaussian noise model in nonlinear regression by a more flexible noise model based on the Student-t-distribution. The degrees of freedom of the t-distribution can be chosen such that as special cases either the Gaussian distribution or the Cauchy distribution are realized. The latter is commonly used in robust regression. Since the t-distribution can be interpreted as being an infinite mixture of Gaussians, parameters and hyperparameters such as the degrees of freedom of the t-distribution can be learned from the data based on an EM-learning algorithm. We show that modeling using the t-distribution leads to improved predictors on real world data sets. In particular, if outliers are present, the t-distribution is superior to the Gaussian noise model. In effect, by adapting the degrees of freedom, the system can "learn" to distinguish between outliers and non-outliers. Especially for online learning tasks, one is interested in avoiding inappropriate weight changes due to measurement outliers to maintain stable online learning capability. We show experimentally that using the t-distribution as a noise model leads to stable online learning algorithms and outperforms state-of-the art online learning methods like the extended Kalman filter algorithm.	McKinsey & Co Inc, New York, NY 10022 USA		Briegel, T (corresponding author), McKinsey & Co Inc, New York, NY 10022 USA.	thomas.briegel@mchp.siemens.de; volker.tresp@mchp.siemens.de						BRIEGEL T, 1999, SEM STAT LUDW MAX U; DEFREITAS N, 1998, NIPS 98 WORKSH BRECK; Fahrmeir L, 1999, METRIKA, V49, P173, DOI 10.1007/s001840050007; FAHRMEIR L, 1991, METRIKA, V38, P37, DOI DOI 10.1007/BF02613597; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; LANGE KL, 1989, J AM STAT ASSOC, V84, P881, DOI 10.2307/2290063; MEINHOLD R, 1989, JASA, V84, P470; Rousseeuw P.J., 1987, ROBUST REGRESSION OU; WEST M, 1981, J ROY STAT SOC B MET, V43, P157	9	3	3	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						407	413						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700058
C	Frey, BJ		Solla, SA; Leen, TK; Muller, KR		Frey, BJ			Local probability propagation for factor analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				SLEEP	Ever since Pearl's probability propagation algorithm in graphs with cycles was shown to produce excellent results for error-correcting decoding a few years ago, we have been curious about whether local probability propagation could be used successfully far machine learning. One of the simplest adaptive models is the factor analyzer, which is a two-layer network that models bottom layer sensory inputs as a linear combination of top layer factors plus independent Gaussian sensor noise. We show that local probability propagation in the factor analyzer network usually takes just a few iterations to perform accurate inference, even in networks with 320 sensors and 80 factors. We derive an expression for the algorithm's fixed point and show that this fixed point matches the exact solution in a variety of networks, even when the fixed point is unstable. We also show that this method can be used successfully to perform inference for approximate EM and we give results on an online face recognition task.	Univ Waterloo, Waterloo, ON N2L 3G1, Canada	University of Waterloo	Frey, BJ (corresponding author), Univ Waterloo, Waterloo, ON N2L 3G1, Canada.							Berrou C, 1996, IEEE T COMMUN, V44, P1261, DOI 10.1109/26.539767; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; FREY BJ, 1997, ADV NEURAL INFORMATI, V10; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; MACKAY DJC, 1999, INFORMATION THEORY I; Neal RM, 1997, NEURAL COMPUT, V9, P1781, DOI 10.1162/neco.1997.9.8.1781; SMYTH P, 1997, WORKSH INF LEARN GRA; WEISS Y, 1998, IN PRESS NEURAL COMP; [No title captured]	9	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						442	448						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700063
C	Kabashima, Y; Murayama, T; Saad, D; Vicente, R		Solla, SA; Leen, TK; Muller, KR		Kabashima, Y; Murayama, T; Saad, D; Vicente, R			Regular and irregular Gallager-type error-correcting codes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				SPIN-GLASSES; MODEL	The performance of regular and irregular Gallager-type error-correcting code is investigated via methods of statistical physics. The transmitted codeword comprises products of the original message bits selected by two randomly-constructed sparse matrices; the number of non-zero row/column elements in these matrices constitutes a family of codes. We show that Shannon's channel capacity may be saturated in equilibrium for many of the regular codes while slightly lower performance is obtained for others which may be of higher practical relevance. Decoding aspects are considered by employing the TAP approach which is identical to the commonly used belief-propagation-based decoding. We show that irregular codes may saturate Shannon's capacity but with improved dynamical properties.	Tokyo Inst Technol, Dept Compt Intl & Syst Sci, Yokohama, Kanagawa 2268502, Japan	Tokyo Institute of Technology	Kabashima, Y (corresponding author), Tokyo Inst Technol, Dept Compt Intl & Syst Sci, Yokohama, Kanagawa 2268502, Japan.		Vicente, Renato/B-2347-2008; Vicente, Renato/A-4956-2010; Vicente, Renato/X-8205-2019; Murayama, Tatsuto/E-7575-2012	Vicente, Renato/0000-0003-0671-9895; Vicente, Renato/0000-0003-0671-9895; Murayama, Tatsuto/0000-0002-5134-9793				DEDOMINICIS C, 1987, J PHYS A-MATH GEN, V20, pL1267, DOI 10.1088/0305-4470/20/18/009; GALLAGER RG, 1962, IRE T INFORM THEOR, V8, P21, DOI 10.1109/tit.1962.1057683; Kabashima Y, 1999, EUROPHYS LETT, V45, P97, DOI 10.1209/epl/i1999-00137-2; Kabashima Y, 1998, EUROPHYS LETT, V44, P668, DOI 10.1209/epl/i1998-00524-7; KABASHIMA Y, 1999, IN PRESS PHYS REV LE; LUBY M, 1998, IEEE P ISIT98; Mackay DJC, 1999, IEEE T COMMUN, V47, P1449, DOI 10.1109/26.795809; MacKay DJC, 1999, IEEE T INFORM THEORY, V45, P399, DOI 10.1109/18.748992; NISHIMORI H, 1981, PROG THEOR PHYS, V66, P1169, DOI 10.1143/PTP.66.1169; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; SOURLAS N, 1994, EUROPHYS LETT, V25, P159, DOI 10.1209/0295-5075/25/3/001; THOULESS DJ, 1977, PHILOS MAG, V35, P593, DOI 10.1080/14786437708235992; WONG KYM, 1987, J PHYS A-MATH GEN, V20, pL793, DOI 10.1088/0305-4470/20/12/008	15	3	3	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						272	278						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700039
C	Mesterharm, C		Solla, SA; Leen, TK; Muller, KR		Mesterharm, C			A multi-class linear learning algorithm related to Winnow	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					In this paper, we present Committee, a new multi-class learning algorithm related to the Winnow family of algorithms. Committee is an algorithm for combining the predictions of a set of sub-experts in the online mistake-bounded model of learning. A sub-expert is a special type of attribute that predicts with a distribution over a finite number of classes. Committee learns a linear function of sub-experts and uses this function to make class predictions. We provide bounds for Committee that show it performs well when the target can be represented by a few relevant sub-experts. We also show how Committee can be used to solve more traditional problems composed of attributes. This leads to a natural ex tension that learns on multi-class problems that contain both traditional attributes and sub-experts.	Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA	Rutgers State University New Brunswick	Mesterharm, C (corresponding author), Rutgers State Univ, Dept Comp Sci, 110 Frelinghuysen Rd, Piscataway, NJ 08854 USA.							Blum A., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P64; DAGAN I, 1997, EMNLP 97 2 C EMP MET, P55; Duda R.O., 1973, J ROYAL STAT SOC SER; GOLDING AR, 1996, ML 96; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Littlestone N., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P353; LITTLESTONE N, 1991, COLT 91, P147; LITTLESTONE N, UNPUB SIMULATION STU; MESTERHARM C, 1999, MULTICLASS LINEAR LE; MITCHELL T, 1994, COMMUN ACM, V37, P81, DOI 10.1145/176789.176798; [No title captured]	11	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						519	525						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700074
C	Ormoneit, D; Hastie, T		Solla, SA; Leen, TK; Muller, KR		Ormoneit, D; Hastie, T			Optimal kernel shapes for local linear regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Local linear regression performs very well in many low-dimensional forecasting problems. In high-dimensional spaces, its performance typically decays due to the well-known "curse-of-dimensionality". A possible way to approach this problem is by varying the "shape" of the weighting kernel. In this work we suggest a new, data-driven method to estimating the optimal kernel shape. Experiments using an artificially generated data set and data from the UC Irvine repository show the benefits of kernel shaping.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Stanford University	Ormoneit, D (corresponding author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.			Hastie, Trevor/0000-0002-0164-3142				Atkeson CG, 1997, ARTIF INTELL REV, V11, P11, DOI 10.1023/A:1006559212014; BIRATTARI M, 1999, ADV NEURAL INFORMATI, V11; BLAKE C, UCI REPSOITORY MACHI; CLEVELAND WS, 1979, J AM STAT ASSOC, V74, P829, DOI 10.2307/2286407; Fan J.Q., 1996, LOCAL POLYNOMIAL MOD; ORMONEIT D, 1999, 199911 STANF U DEP S	6	3	3	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						540	546						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700077
C	Roweis, S		Solla, SA; Leen, TK; Muller, KR		Roweis, S			Constrained hidden Markov models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					By thinking of each state in a hidden Markov model as corresponding to some spatial region of a fictitious topology space it is possible to naturally define neighbouring states as those which are connected in that space. The transition matrix can then be constrained to allow transitions only between neighbours; this means that all valid state sequences correspond to connected paths in the topology space. I show how such constrained HMMs can learn to discover underlying structure in complex sequences of high dimensional data, and apply them to the problem of recovering mouth movements from acoustics in continuous speech.	Univ Coll London, Gatsby Unit, London, England	University of London; University College London	Roweis, S (corresponding author), Univ Coll London, Gatsby Unit, Mortimer St, London, England.							BLACKBURN S, ICSLP 1996, V2, P969; CHENNOUKH S, EUROSPEECH 1997, V1, P429; NIS D, 1999, NIPS 11, P744; RAMSAY G, 1994, J ACOUST SOC AM, V95, P2873; ROWEIS S, EUROSPEECH 1997, V3, P1227; Schroeter J, 1994, IEEE T SPEECH AUDI P, V2, P133, DOI 10.1109/89.260356; SMYTH P, 1997, NIPS 9, P648; Westbury JR, 1994, XRAY MICROBEAM SPEEC	8	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						782	788						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700111
C	Song, XB; Sill, J; Abu-Mostafa, Y; Kasdan, H		Solla, SA; Leen, TK; Muller, KR		Song, XB; Sill, J; Abu-Mostafa, Y; Kasdan, H			Image recognition in context: Application to microscopic urinalysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We propose a new and efficient technique for incorporating contextual information into object classification. Most of the current techniques face the problem of exponential computation cost. In this paper, we propose a new general framework that incorporates partial context at a linear cost. This technique is applied to microscopic urinalysis image recognition, resulting in a significant improvement of recognition rate over the context free approach. This gain would have been impossible using conventional context incorporation techniques.	Oregon Grad Inst Sci & Technol, Dept Elect & Comp Engn, Beaverton, OR 97006 USA		Song, XB (corresponding author), Oregon Grad Inst Sci & Technol, Dept Elect & Comp Engn, Beaverton, OR 97006 USA.							*BOEHR MANNH CORP, 1991, UR TOD; KITTLER J, 1985, IMAGE VISION COMPUT, V3, P206, DOI 10.1016/0262-8856(85)90009-5; KITTLER J, 1987, PATTERN RECOGN, P99; SONG XB, 1999, THESIS CALTECH; SONG XB, 1997, ADV NEURAL INFORMATI, V7, P950; SWAIN PH, 1981, PATTERN RECOGN, V13, P429, DOI 10.1016/0031-3203(81)90005-4; TOUSSAINT GT, 1978, PATTERN RECOGN, V10, P189, DOI 10.1016/0031-3203(78)90027-4	7	3	4	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						963	969						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700136
C	Zhang, LQ; Amari, S; Cichocki, A		Solla, SA; Leen, TK; Muller, KR		Zhang, LQ; Amari, S; Cichocki, A			Semiparametric approach to multichannel blind deconvolution of nonminimum phase systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				SOURCE SEPARATION; INFORMATION	In this paper we discuss the semiparametric statistical model for blind deconvolution. First we introduce a Lie Group to the manifold of non-causal FIR filters. Then blind deconvolution problem is formulated in the framework of a semiparametric model, and a family of estimating functions is derived for blind deconvolution. A natural gradient learning algorithm is developed for training noncausal filters. Stability of the natural gradient algorithm is also analyzed in this framework.	Inst Phys & Chem Res, BSI, Brain Style Informat Syst Res Grp, Wako, Saitama 3510198, Japan	RIKEN	Zhang, LQ (corresponding author), Inst Phys & Chem Res, BSI, Brain Style Informat Syst Res Grp, Wako, Saitama 3510198, Japan.	zha@open.brain.riken.go.jp; amari@brain.riken.go.jp; cia@brain.riken.go.jp	Cichocki, Andrzej/AAI-4209-2020; Cichocki, Andrzej/A-1545-2015	Cichocki, Andrzej/0000-0002-8364-7226				Amari S, 1997, IEEE T SIGNAL PROCES, V45, P2692, DOI 10.1109/78.650095; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; AMARI S, 1988, ANN STAT, V16, P1044, DOI 10.1214/aos/1176350947; Amari S, 1999, IEEE T SIGNAL PROCES, V47, P936, DOI 10.1109/78.752592; AMARI S, IN PRESS NEURAL COMP; Amari S., 1985, LECT NOTES STAT, V28; AMARI S, 1999, P 1 INT WORKSH IND C, P37; AMARI S, 1998, MONOGRAPH SERIES, V32, P65; AMARI S, 1997, P IEEE WORKSH SIGN P, P101; Amari SI, 1998, P IEEE, V86, P2026, DOI 10.1109/5.720251; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Cardoso JF, 1998, P IEEE, V86, P2009, DOI 10.1109/5.720250; Cardoso JF, 1996, IEEE T SIGNAL PROCES, V44, P3017, DOI 10.1109/78.553476; Cichocki A, 1996, IEEE T CIRCUITS-I, V43, P894, DOI 10.1109/81.542280; LANG T, 1991, IEEE T CIRCUITS SYST, V38, P499, DOI 10.1109/31.76486; Wellner, 1993, EFFICIENT ADAPTIVE E; Yang HH, 1997, NEURAL COMPUT, V9, P1457, DOI 10.1162/neco.1997.9.7.1457; ZHANG L, 1999, P INT IEEE WORKSH NE, P303; ZHANG L, 1999, P 5 INT C NEUR INF P, P210	19	3	3	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						363	369						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700052
C	Zhang, T		Solla, SA; Leen, TK; Muller, KR		Zhang, T			Some theoretical results concerning the convergence of compositions of regularized linear functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				APPROXIMATION; NETWORK	Recently, sample complexity bounds have been derived for problems involving linear functions such as neural networks and support vector machines. In this paper, we extend some theoretical results in this area by deriving dimensional independent covering number bounds for regularized linear functions under certain regularization conditions. We show that such bounds lead to a class of new methods for training linear classifiers with similar theoretical advantages of the support vector machine. Furthermore, we also present a theoretical analysis for these new methods fi-om the asymptotic statistical point of view. This technique provides better description for large sample behaviors of these algorithms.	IBM Corp, Thomas J Watson Res Ctr, Dept Math Sci, Yorktown Heights, NY 10598 USA	International Business Machines (IBM)	Zhang, T (corresponding author), IBM Corp, Thomas J Watson Res Ctr, Dept Math Sci, Yorktown Heights, NY 10598 USA.							Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502; Dudley R. M., 1984, LECT NOTES MATH, V1097; Golub Gene H., 2013, MATRIX COMPUTATION, V3; HAUSSLER D, 1989, P 30 ANN S FDN COMP, P40; JONES LK, 1992, ANN STAT, V20, P608, DOI 10.1214/aos/1176348546; Kolmogorov A. N., 1956, DOKL AKAD NAUK SSSR, V108, P585; Kolmogorov A. N., 1961, AM MATH SOC TRANSL, V17, P277; Pollard David, 1984, CONVERGENCE STOCHAST; Sauer N., 1972, J COMB THEORY A, V13, P145, DOI [10.1016/0097-3165(72)90019-2, DOI 10.1016/0097-3165(72)90019-2]; Schapire RE, 1998, ANN STAT, V26, P1651; Sun BT, 1996, T NONFERR METAL SOC, V6, P211; Vapnik V., 1982, ESTIMATION DEPENDENC; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; ZHANG T, 1999, RC21572 IBM	18	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						370	376						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700053
C	Al-Ansari, MA; Williams, RJ		Kearns, MS; Solla, SA; Cohn, DA		Al-Ansari, MA; Williams, RJ			Robust, efficient, globally-optimized reinforcement learning with the parti-game algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Parti-game (Moore 1994a; Moore 1994b; Moore and Atkeson 1995) is a reinforcement learning (RL) algorithm that has a lot of promise in overcoming the curse of dimensionality that can plague RL algorithms when applied to high-dimensional problems. In this paper we introduce modifications to the algorithm that further improve its performance and robustness. In addition, while parti-game solutions can be improved locally by standard local path-improvement techniques, we introduce an add-on algorithm in the same spirit as parti-game that instead tries to improve solutions in a non-local manner.	Northeastern Univ, Coll Comp Sci, Boston, MA 02115 USA	Northeastern University	Al-Ansari, MA (corresponding author), Northeastern Univ, Coll Comp Sci, 161 CN, Boston, MA 02115 USA.							ALANSARI MA, 1998, NUCCS9813; MOORE A, 1994, P 8 YAL WORKSH AD LE; MOORE AW, 1994, P NEUR INF PROC SYST, P6; MOORE AW, 1995, MACH LEARN, P21; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2	5	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						961	967						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700135
C	Chechik, G; Meilijson, I; Ruppin, E		Kearns, MS; Solla, SA; Cohn, DA		Chechik, G; Meilijson, I; Ruppin, E			Neuronal regulation implements efficient synaptic pruning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				NETWORKS	Human and animal studies show that mammalian brain undergoes massive synaptic pruning during childhood, removing about half of the synapses until puberty. We have previously shown that maintaining network memory performance while synapses are deleted, requires that synapses are properly modified and pruned, removing the weaker synapses. We now show that neuronal regulation, a mechanism recently observed to maintain the average neuronal input field, results in weight-dependent synaptic modification. Under the correct range of the degradation dimension and synaptic upper bound, neuronal regulation removes the weaker synapses and judiciously modifies the remaining synapses. It implements near optimal synaptic modification, and maintains the memory performance of a network undergoing massive synaptic pruning. Thus, this paper shows that in addition to the known effects of Hebbian changes, neuronal regulation may play an important role in the self-organization of brain networks during development.	Tel Aviv Univ, Sch Math Sci, IL-69978 Tel Aviv, Israel	Tel Aviv University	Chechik, G (corresponding author), Tel Aviv Univ, Sch Math Sci, IL-69978 Tel Aviv, Israel.		Ruppin, Eytan/R-9698-2017	Ruppin, Eytan/0000-0002-7862-3940				[Anonymous], 1993, BRAIN ACTIVATION; CHECHIK G, 1998, IN PRESS NEURAL COMP; Colman H, 1997, SCIENCE, V275, P356, DOI 10.1126/science.275.5298.356; Davis GW, 1998, NATURE, V392, P82, DOI 10.1038/32176; Horn D, 1998, NEURAL COMPUT, V10, P1, DOI 10.1162/089976698300017863; INNOCENTI GM, 1995, TRENDS NEUROSCI, V18, P397, DOI 10.1016/0166-2236(95)93936-R; Meilijson I, 1996, BIOL CYBERN, V74, P479, DOI 10.1007/BF00209419; TSODYKS MV, 1988, EUROPHYS LETT, V6, P101, DOI 10.1209/0295-5075/6/2/002; Turrigiano GG, 1998, NATURE, V391, P892, DOI 10.1038/36103; WOLFF JR, 1995, BEHAV BRAIN RES, V66, P13, DOI 10.1016/0166-4328(94)00118-Y	10	3	3	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						97	103						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700014
C	Huet, B; Cross, ADJ; Hancock, ER		Kearns, MS; Solla, SA; Cohn, DA		Huet, B; Cross, ADJ; Hancock, ER			Graph matching for shape retrieval	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				DISCRETE RELAXATION	This paper describes a Bayesian graph matching algorithm for data-mining from large structural data-bases. The matching algorithm uses edge-consistency and node attribute similarity to determine the a posteriori probability of a query graph for each of the candidate matches in the data-base. The node feature-vectors are constructed by computing normalised histograms of pairwise geometric attributes, attribute similarity is assessed by computing the Bhattacharyya distance between the histograms. Recognition is realised by selecting the candidate from the data-base which has the largest a posteriori probability. We illustrate the recognition technique on a data-base containing 2500 line patterns extracted from real-world imagery. Here the recognition technique is shown to significantly outperform a number of algorithm alternatives.	Univ York, Dept Comp Sci, York YO1 5DD, N Yorkshire, England	University of York - UK	Hancock, ER (corresponding author), Univ York, Dept Comp Sci, York YO1 5DD, N Yorkshire, England.		Hancock, Edwin/N-7548-2019; Hancock, Edwin R/C-6071-2008	Hancock, Edwin/0000-0003-4496-2028; Hancock, Edwin R/0000-0003-4496-2028; Huet, Benoit/0000-0002-0608-6939				BARROW HG, 1971, MACH INTELL, V5, P377; Finch AM, 1997, ADV NEUR IN, V9, P438; Gold S, 1996, IEEE T PATTERN ANAL, V18, P377, DOI 10.1109/34.491619; HUET B, 1998, IEEE ICCV 98 JAN, P563; PENTLAND A, 1994, SPIE STORAGE RETRIEV, V2, P34; SENGUPTA K, 1995, IEEE T PATTERN ANAL, V17, P321, DOI 10.1109/34.385984; SHAPIRA R, 1985, IEEE T PATTERN ANAL, V7, P1, DOI 10.1109/TPAMI.1985.4767614; SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487; Wilson RC, 1997, IEEE T PATTERN ANAL, V19, P634, DOI 10.1109/34.601251; [No title captured]	10	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						896	902						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700126
C	Kearns, M; Saul, L		Kearns, MS; Solla, SA; Cohn, DA		Kearns, M; Saul, L			Inference in multilayer networks via large deviation bounds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				BELIEF NETWORKS	We study probabilistic inference in large, layered Bayesian net works represented as directed acyclic graphs. We show that the intractability of exact inference in such networks does not preclude their effective use. We give algorithms for approximate probabilistic inference that exploit averaging phenomena occurring at nodes with large numbers of parents. We show that these algorithms compute rigorous lower and upper hounds on marginal probabilities of interest, prove that these bounds become exact in the limit of large networks, and provide rates of convergence.	AT&T Labs Res, Shannon Lab, Florham Park, NJ 07932 USA	AT&T	Kearns, M (corresponding author), AT&T Labs Res, Shannon Lab, 180 Pk Ave,A-235, Florham Park, NJ 07932 USA.							COOPER GF, 1990, ARTIF INTELL, V42, P393, DOI 10.1016/0004-3702(90)90060-D; HERTZ J, 1991, INTRODUCTION THEORY; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; JORDAN MI, 1997, LEARNING GRAPHICAL M; KEARNS M, 1998, P 14 ANN C UNC ART I; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4	7	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						260	266						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700037
C	Magdon-Ismail, M; Atiya, A		Kearns, MS; Solla, SA; Cohn, DA		Magdon-Ismail, M; Atiya, A			Neural networks for density estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					We introduce two new techniques for density estimation. Our approach poses the problem as a supervised learning task which can be performed using Neural Networks. We introduce a stochastic method for learning the cumulative distribution and an analogous deterministic technique. We demonstrate convergence of our methods both theoretically and experimentally, and provide comparisons with the Parzen estimate. Our theoretical results demonstrate better convergence properties than the Parzen estimate.	CALTECH, Dept Elect Engn, Caltech Learning Syst Grp, Pasadena, CA 91125 USA	California Institute of Technology	Magdon-Ismail, M (corresponding author), CALTECH, Dept Elect Engn, Caltech Learning Syst Grp, 136-93, Pasadena, CA 91125 USA.			Atiya, Amir/0000-0001-8766-5600				FUKUNAGA K, 1973, IEEE T INFORM THEORY, V19, P320, DOI 10.1109/TIT.1973.1055003; HORNIK K, 1990, NEURAL NETWORKS, V3, P551, DOI 10.1016/0893-6080(90)90005-6; MAGDONISMAIL M, 1998, UNPUB CONSISTENT DEN; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; Sill J, 1997, ADV NEUR IN, V9, P634; SILVERMAN BW, 1993, DENSITY ESTIMATION S; Tikhonov A., 1977, SOLUTIONS ILL POSED	7	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						522	528						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700074
C	Mozer, MC		Kearns, MS; Solla, SA; Cohn, DA		Mozer, MC			A principle for unsupervised hierarchical decomposition of visual scenes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				BINDING; CORTEX; CAT	Structure in a visual scene can be described at many levels of granularity. At a coarse level, the scene is composed of objects, at a finer level, each object is made up of parts, and the parts of subparts. In this work, I propose a simple principle by which such hierarchical structure can be extracted from visual scenes: Regularity in the relations among different parts of an object is weaker than in the internal structure of a part. This principle can be applied recursively to define part-whole relationships among elements in a scene. The principle does not make use of object models, categories, or other sorts of higher-level knowledge; rather, part-whole relationships can be established based on the statistics of a set of sample visual scenes. I illustrate with a model that performs unsupervised decomposition of simple scenes. The model can account for the results from a human learning experiment on the ontogeny of part-whole relationships.	Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Mozer, MC (corresponding author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.							BECKER S, 1995, ADV NEURAL INFORMATI, V7, P933; ECKHORN R, 1988, BIOL CYBERN, V60, P121, DOI 10.1007/BF00202899; GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0; MOZER MC, 1992, NEURAL COMPUT, V4, P650, DOI 10.1162/neco.1992.4.5.650; Plaut D.C, 1986, CMUCS86126; POLLACK JB, 1988, P 10 ANN C COGN SCI, P33; SCHYNS PG, 1993, PROCEEDINGS OF THE FIFTEENTH ANNUAL CONFERENCE OF THE COGNITIVE SCIENCE SOCIETY, P917; SCHYNS PG, 1992, P 14 ANN C COGN SCI, P197; SMOLENSKY P, 1990, ARTIF INTELL, V46, P159, DOI 10.1016/0004-3702(90)90007-M; TENENBAUM J, 1994, PROCEEDINGS OF THE SIXTEENTH ANNUAL CONFERENCE OF THE COGNITIVE SCIENCE SOCIETY, P864; VONDERMALSBERG C, 1981, 812 M PLANCK I BIOPH; ZEMEL RS, 1995, NEURAL NETWORKS, V8, P503, DOI 10.1016/0893-6080(94)00094-3	12	3	3	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						52	58						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700008
C	Piepenbrock, C; Obermayer, K		Kearns, MS; Solla, SA; Cohn, DA		Piepenbrock, C; Obermayer, K			The role of lateral cortical competition in ocular dominance development	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				ORIENTATION; MAPS	Lateral competition within a layer of neurons sharpens and localizes the response to an input stimulus. Here, we investigate a model for the activity dependent development of ocular dominance maps which allows to vary the degree of lateral competition. For weak competition, it resembles a correlation-based learning model and for strong competition, it becomes a self-organizing map. Thus, in the regime of weak competition the receptive fields are shaped by the second order statistics of the input patterns, whereas in the regime of strong competition, the higher moments and "features" of the individual patterns become important. When correlated localized stimuli from two eyes drive the cortical development we find (i) that a topographic map and binocular, localized receptive fields emerge when the degree of competition exceeds a critical value and (ii) that receptive fields exhibit eye dominance beyond a second critical value. For anti-correlated activity between the eyes, the second order statistics drive the system to develop ocular dominance even for weak competition, but no topography emerges. Topography is established only beyond a critical degree of competition.	Tech Univ Berlin, Dept Comp Sci, D-10587 Berlin, Germany	Technical University of Berlin	Piepenbrock, C (corresponding author), Tech Univ Berlin, Dept Comp Sci, FR 2-1,Franklinstr 28-29, D-10587 Berlin, Germany.							AMARI SI, 1977, BIOL CYBERN, V27, P77, DOI 10.1007/BF00337259; MILLER KD, 1989, SCIENCE, V245, P605, DOI 10.1126/science.2762813; OBERMAYER K, 1990, P NATL ACAD SCI USA, V87, P8345, DOI 10.1073/pnas.87.21.8345; Piepenbrock C, 1997, NEURAL COMPUT, V9, P959, DOI 10.1162/neco.1997.9.5.959; Riesenhuber M, 1996, BIOL CYBERN, V75, P397, DOI 10.1007/s004220050305; SOMERS DC, 1995, J NEUROSCI, V15, P5448; Yuille AL, 1996, NEURAL NETWORKS, V9, P309, DOI 10.1016/0893-6080(95)00085-2	7	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						139	145						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700020
C	Rinberg, D; Davidowitz, H; Tishby, N		Kearns, MS; Solla, SA; Cohn, DA		Rinberg, D; Davidowitz, H; Tishby, N			Multi-electrode spike sorting by clustering transfer functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				STIMULUS DIRECTION; GIANT INTERNEURONS; COCKROACH	A new paradigm is proposed for sorting spikes in multi-electrode data using ratios of transfer functions between cells slid electrodes. It is assumed that for every cell and electrode there is a stable linear relation. These are dictated by the properties of the tissue. the electrodes and their relative geometries, The main advantage of the method is that it is insensitive to variations in the shape and amplitude of a spike. Spike sorting is carried out ill two separate steps. First. templates describing the statistics of each spike type are generated by clustering transfer function ratios then spikes are detected in the data using the spike statistics. These techniques were applied to data generated in the escape response system of the cockroach.	NEC Res Inst, Princeton, NJ 08540 USA	NEC Corporation	Rinberg, D (corresponding author), NEC Res Inst, 4 Independence Way, Princeton, NJ 08540 USA.							ABELES M, 1988, J NEUROPHYSIOL, V60, P909, DOI 10.1152/jn.1988.60.3.909; Abeles M., 1991, CORTICONICS; Barinaga M, 1998, SCIENCE, V280, P376, DOI 10.1126/science.280.5362.376; CAMHI JM, 1989, J COMP PHYSIOL A, V165, P83, DOI 10.1007/BF00613802; Fee MS, 1996, J NEUROSCI METH, V69, P175, DOI 10.1016/S0165-0270(96)00050-7; KOLTON L, 1995, J COMP PHYSIOL A, V176, P691, DOI 10.1007/BF01021589; Lewicki MS, 1998, NETWORK-COMP NEURAL, V9, pR53, DOI 10.1088/0954-898X/9/4/001; MCNAUGHTON BL, 1983, J NEUROSCI METH, V8, P391, DOI 10.1016/0165-0270(83)90097-3; Papoulis A., 2002, PROBABILITY RANDOM V; REECE M, 1989, SOC NEUR ABSTR, V15, P1250; WESTIN J, 1977, J COMP PHYSIOL, V121, P307, DOI 10.1007/BF00613011	11	3	3	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						146	152						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700021
C	Williams, JK; Singh, S		Kearns, MS; Solla, SA; Cohn, DA		Williams, JK; Singh, S			Experimental results on learning stochastic memoryless policies for Partially Observable Markov Decision Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Partially Observable Markov Decision Processes (POMDPs) constitute an important class of reinforcement learning problems which present unique theoretical and computational difficulties. In the absence of the Markov property, popular reinforcement learning algorithms such as Q-learning may no longer be effective, and memory-based methods which remove partial observability via state-estimation. are notoriously expensive. An alternative approach is to seek a stochastic memoryless policy which for each observation of the environment prescribes a probability distribution over available actions that maximizes the average reward per timestep. A reinforcement learning algorithm which learns a locally optimal stochastic memoryless policy has been proposed by Jaakkola, Singh and Jordan, but not empirically verified. We present a variation of this algorithm, discuss its implementation, and demonstrate its viability using four test problems.	Univ Colorado, Dept Math, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Williams, JK (corresponding author), Univ Colorado, Dept Math, Boulder, CO 80309 USA.							CHRISMAN L, 1992, P 10 NAT C ART INT; JAAKKOLA T, 1995, ADV NEURAL INFORMATI, V7; LITTMAN M, 1994, P 3 INT C SIM AD BEH; LITTMAN ML, 1995, P 12 INT C MACH LEAR; LOCH J, 1998, MACHINE LEARNING; Lovejoy W., 1991, ANN OPERATIONS RES, V28; Morris P., 1994, INTRO GAME THEORY; PARR R, 1995, P INT JOINT C ART IN; SINGH S, 1994, MACHINE LEARNING; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2	10	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						1073	1079						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700151
C	Dailey, MN; Cottrell, GW		Jordan, MI; Kearns, MJ; Solla, SA		Dailey, MN; Cottrell, GW			Task and spatial frequency effects on face specialization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					There is strong evidence that face processing is localized in the brain. The double dissociation between prosopagnosia, a face recognition deficit occurring after brain damage, and visual object agnosia, difficulty recognizing other kinds of complex objects, indicates that face and non-face object recognition may be served by partially independent mechanisms in the brain. Is neural specialization innate or learned? We suggest that this specialization could be the result of a competitive learning mechanism that, during development, devotes neural resources to the tasks they are best at performing. further, we suggest that the specialization arises as an interaction between task requirements and developmental constraints. In this paper, we present a feed-forward computational model of visual processing, in which two modules compete to classify input stimuli. When one module receives low spatial frequency information and the other receives high spatial frequency information, and the task is to identify the faces while simply classifying the objects, the low frequency network shows a strong specialization for faces. No other combination of tasks and inputs shows this strong specialization. We take these results as support for the idea that an innately-specified face processing module is unnecessary.	Univ Calif San Diego, Dept Comp Sci & Engn, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Dailey, MN (corresponding author), Univ Calif San Diego, Dept Comp Sci & Engn, La Jolla, CA 92093 USA.			Cottrell, Garrison/0000-0001-7538-1715					0	3	3	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						17	23						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700003
C	De Bonet, JS; Viola, P		Jordan, MI; Kearns, MJ; Solla, SA		De Bonet, JS; Viola, P			Structure driven image database retrieval	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					A new algorithm is presented which approximates the perceived visual similarity between images. The images are initially transformed into a feature space which captures visual structure, texture and color using a tree of filters. Similarity is the inverse of the distance in this perceptual feature space. Using this algorithm we have constructed an image database system which can perform example based retrieval on large image databases. Using carefully constructed target sets, which limit variation to only a single visual characteristic, retrieval rates are quantitatively compared to those of standard methods.	MIT, Artificial Intelligence Lab, Learning & Vis Grp, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	De Bonet, JS (corresponding author), MIT, Artificial Intelligence Lab, Learning & Vis Grp, 545 Technol Sq, Cambridge, MA 02139 USA.								0	3	3	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						866	872						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700122
C	de Freitas, JFG; Niranjan, M; Gee, AH		Jordan, MI; Kearns, MJ; Solla, SA		de Freitas, JFG; Niranjan, M; Gee, AH			Regularisation in sequential learning algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					In this paper, we discuss regularisation in online/sequential learning algorithms. In environments where data arrives sequentially, techniques such as cross-validation to achieve regularisation or model selection are not possible. Further, bootstrapping to determine a confidence level is not practical. To surmount these problems, a minimum variance estimation approach that makes use of the extended Kalman algorithm for training multi-layer perceptrons is employed. The novel contribution of this paper is to show the theoretical links between extended Kalman filtering, Sutton's variable learning rate algorithms and Mackay's Bayesian estimation framework. In doing so, we propose algorithms to overcome the need for heuristic choices of the initial conditions and noise covariance matrices in the Kalman approach.	Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England	University of Cambridge	de Freitas, JFG (corresponding author), Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.			Niranjan, Mahesan/0000-0001-7021-140X					0	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						458	464						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700065
C	deCharms, RC; Merzenich, MM		Jordan, MI; Kearns, MJ; Solla, SA		deCharms, RC; Merzenich, MM			Characterizing neurons in the primary auditory cortex of the awake primate using reverse correlation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					While the understanding of the functional role of different classes of neurons in the awake primary visual cortex has been extensively studied since the time of Hubel and Wiesel (Hubel and Wiesel, 1962), our understanding of the feature selectivity and functional role of neurons in the primary auditory cortex is much farther from complete. Moving bars have long been recognized as an optimal stimulus for many visual cortical neurons, and this finding has recently been confirmed and extended in detail using reverse correlation methods (Jones and Palmer, 1987; Reid and Alonso, 1995; Reid et al., 1991; Ringach et al., 1997). In this study, we recorded from neurons in the primary auditory cortex of the awake primate, and used a novel reverse correlation technique to compute receptive fields (or preferred stimuli), encompassing both multiple frequency components and ongoing time. These spectrotemporal receptive fields make clear that neurons in the primary auditory cortex, as in the primary visual cortex, typically show considerable structure in their feature processing properties, often including multiple excitatory and inhibitory regions in their receptive fields. These neurons can be sensitive to stimulus edges in frequency composition or in time, and sensitive to stimulus transitions such as changes in frequency. These neurons also show strong responses and selectivity to continuous frequency modulated stimuli analogous to visual drifting gratings.	Univ Calif San Francisco, WM Keck Ctr Integrat Neurosci, San Francisco, CA 94143 USA	University of California System; University of California San Francisco	deCharms, RC (corresponding author), Univ Calif San Francisco, WM Keck Ctr Integrat Neurosci, San Francisco, CA 94143 USA.								0	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						124	130						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700018
C	Etienne-Cummings, R; Cai, DH		Jordan, MI; Kearns, MJ; Solla, SA		Etienne-Cummings, R; Cai, DH			A general purpose image processing chip: Orientation detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					A 80 x 78 pixel general purpose vision chip for spatial focal plane processing is presented. The size and configuration of the processing receptive field are programmable. The chip's architecture allows the photoreceptor cells to be small and densely packed by performing all computation on the read-out, away from the array. In addition to the raw intensity image, the chip outputs four processed images in parallel. Also presented is an application of the chip to line segment orientation detection, as found in the retinal receptive fields of toads.	So Illinois Univ, Dept Elect Engn, Carbondale, IL 62901 USA	Southern Illinois University System; Southern Illinois University	Etienne-Cummings, R (corresponding author), So Illinois Univ, Dept Elect Engn, Carbondale, IL 62901 USA.		Etienne-Cummings, Ralph/A-3227-2010						0	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						873	879						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700123
C	Pareigis, S		Jordan, MI; Kearns, MJ; Solla, SA		Pareigis, S			Adaptive choice of grid and time in reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We propose local error estimates together with algorithms for adaptive a-posteriori grid and time refinement in reinforcement learning. We consider a deterministic system with continuous state and time with infinite horizon discounted cost functional. For grid refinement we follow the procedure of numerical methods for the Bellman-equation. For time refinement we propose a new criterion, based on consistency estimates of discrete solutions of the Bellman-equation. We demonstrate, that an optimal ratio of time to space discretization is crucial for optimal learning rates and accuracy of the approximate optimal value function.	Univ Kiel, Lehrstuhl Prakt Math, Kiel, Germany	University of Kiel	Pareigis, S (corresponding author), Univ Kiel, Lehrstuhl Prakt Math, Gutenbergstr 76-78, Kiel, Germany.								0	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						1036	1042						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700146
C	Abramowicz, H; Horn, D; Naftaly, U; SaharPikielny, C		Mozer, MC; Jordan, MI; Petsche, T		Abramowicz, H; Horn, D; Naftaly, U; SaharPikielny, C			An orientation selective neural network for pattern identification in particle detectors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We present an algorithm for identifying linear patterns on a two-dimensional lattice based on the concept of an orientation selective cell, a concept borrowed from neurobiology of vision. Constructing a multi-layered neural network with fixed architecture which implements orientation selectivity, we define output elements corresponding to different orientations, which allow us to make a selection decision. The algorithm takes into account the granularity of the lattice as well as the presence of noise and inefficiencies. The method is applied to a sample of data collected with the ZEUS detector at HERA in order to identify cosmic muons that leave a linear pattern of signals in the segmented calorimeter. A two dimensional representation of the relevant part of the detector is used. The algorithm performs very well. Given its architecture, this system becomes a good candidate for fast pattern recognition in parallel processing devices.			Abramowicz, H (corresponding author), TEL AVIV UNIV,SCH PHYS & ASTRON,IL-69978 TEL AVIV,ISRAEL.			Horn, David/0000-0003-2708-186X					0	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						925	931						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00130
C	Burgess, AN		Mozer, MC; Jordan, MI; Petsche, T		Burgess, AN			Estimating equivalent kernels for neural networks: A data perturbation approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We describe the notion of ''equivalent kernels'' and suggest that this provides a framework for comparing different classes of regression models, including neural networks and both parametric and non-parametric statistical techniques. Unfortunately, standard techniques break down when faced with models, such as neural networks, in which there is more than one ''layer'' of adjustable parameters. We propose an algorithm which overcomes this limitation, estimating the equivalent kernels for neural network models using a data perturbation approach. Experimental results indicate that the networks do not use the maximum possible number of degrees of freedom, that these can be controlled using regularisation techniques and that the equivalent kernels learnt by the network vary both in ''size'' and in ''shape'' in different regions of the input space.			Burgess, AN (corresponding author), LONDON BUSINESS SCH,DEPT DECIS SCI,LONDON NW1 4SA,ENGLAND.								0	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						382	388						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00054
C	Cohn, DA; Singh, S		Mozer, MC; Jordan, MI; Petsche, T		Cohn, DA; Singh, S			Predicting lifetimes in dynamically allocated memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Predictions of lifetimes of dynamically allocated objects can be used to improve time and space efficiency of dynamic memory management in computer programs. Barrett and Zorn [1993] used a simple lifetime predictor and demonstrated this improvement on a variety of computer programs. In this paper, we use decision trees to do lifetime prediction on the same programs and show significantly better prediction. Our method also has the advantage that during training we can use a large number of features and let the decision tree automatically choose the relevant subset.			Cohn, DA (corresponding author), HARLEQUIN INC,ADAPT SYST GRP,MENLO PK,CA 94025, USA.								0	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						939	945						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00132
C	Jaakkola, TS; Jordon, MI		Mozer, MC; Jordan, MI; Petsche, T		Jaakkola, TS; Jordon, MI			Recursive algorithms for approximating probabilities in graphical models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We develop a recursive node-elimination formalism for efficiently approximating large probabilistic networks. No constraints are set on the network topologies. Yet the formalism can be straightforwardly integrated with exact methods whenever they are/become applicable. The approximations we use are controlled: they maintain consistently upper and lower bounds on the desired quantities at all times. We show that Boltzmann machines, sigmoid belief networks, or any combination (i.e., chain graphs) can be handled within the same framework. The accuracy of the methods is verified experimentally.			Jaakkola, TS (corresponding author), MIT,DEPT BRAIN & COGNIT SCI,E25-618,CAMBRIDGE,MA 02139, USA.		Jordan, Michael I/C-5253-2013						0	3	3	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						487	493						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00069
C	Krzyzak, A; Linder, T		Mozer, MC; Jordan, MI; Petsche, T		Krzyzak, A; Linder, T			Radial basis function networks and complexity regularization in function learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				In this paper we apply the method of complexity regularization to derive estimation bounds for nonlinear function estimation using a single hidden layer radial basis function network. Our approach differs from the previous complexity regularization neural network function learning schemes in that we operate with random covering numbers and l(1) metric entropy, making it possible to consider much broader families of activation functions, namely functions of bounded variation. Some constraints previously imposed on the network parameters are also eliminated this way. The network is trained by means of complexity regularization involving empirical risk minimization. Bounds on the expected risk in terms of the sample size are obtained for a large class of loss functions. Rates of convergence to the optimal loss are also derived.			Krzyzak, A (corresponding author), CONCORDIA UNIV,DEPT COMP SCI,MONTREAL,PQ H3G 1M8,CANADA.								0	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						197	203						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00028
C	Ragarajan, A; Yuille, A; Gold, S; Mjolsness, E		Mozer, MC; Jordan, MI; Petsche, T		Ragarajan, A; Yuille, A; Gold, S; Mjolsness, E			A convergence proof for the softassign quadratic assignment algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The softassign quadratic assignment algorithm has recently emerged as an effective strategy for a variety of optimization problems in pattern recognition and combinatorial optimization. While the effectiveness of the algorithm was demonstrated in thousands of simulations, there was no known proof of convergence. Here, we provide a proof of convergence for the most general form of the algorithm.			Ragarajan, A (corresponding author), YALE UNIV,SCH MED,DEPT DIAGNOST RADIOL,333 CEDAR ST,NEW HAVEN,CT 06520, USA.		Rangarajan, Anand/A-8652-2009	Rangarajan, Anand/0000-0001-8695-8436; Yuille, Alan L./0000-0001-5207-9249					0	3	3	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						620	626						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00088
C	Sollich, P; Barber, D		Mozer, MC; Jordan, MI; Petsche, T		Sollich, P; Barber, D			Online learning from finite training sets: An analytical case study	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We analyse online learning from finite training sets at non-infinitesimal learning rates eta. By an extension of statistical mechanics methods, we obtain exact results for the time-dependent generalization error of a linear network with a large number of weights N. We find, for example, that for small training sets of size p approximate to N, larger learning rates can be used without compromising asymptotic generalization performance or convergence speed. Encouragingly, for optimal settings of eta (and, less importantly, weight decay lambda) at given final learning time, the generalization performance of online learning is essentially as good as that of offline learning.			Sollich, P (corresponding author), UNIV EDINBURGH,DEPT PHYS,MAYFIELD RD,EDINBURGH EH9 3JZ,MIDLOTHIAN,SCOTLAND.		Sollich, Peter/ABC-2993-2020; Sollich, Peter/H-2174-2011	Sollich, Peter/0000-0003-0169-7893					0	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						274	280						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00039
C	Sona, D; Sperduti, A; Starita, A		Mozer, MC; Jordan, MI; Petsche, T		Sona, D; Sperduti, A; Starita, A			A constructive learning algorithm for discriminant tangent models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				To reduce the computational complexity of classification systems using tangent distance, Hastie et al. (WSS) developed an algorithm to devise rich models for representing large subsets of the data which computes automatically the ''best'' associated tangent subspace. Schwenk & Milgram proposed a discriminant modular classification system (Diabolo) based on several autoassociative multilayer perceptrons which use tangent distance as error reconstruction measure. We propose a gradient based constructive learning algorithm for building a tangent subspace model with discriminant capabilities which combines several of the the advantages of both HSS and Diabolo: devised tangent models hold discriminant capabilities, space requirements are improved with respect to HSS since our algorithm is discriminant and thus it needs fewer prototype models, dimension of the tangent subspace is determined automatically by the constructive algorithm, and our algorithm is able to learn new transformations.			Sona, D (corresponding author), UNIV PISA,DIPARTIMENTO INFORMAT,CORSO ITALIA 40,I-56125 PISA,ITALY.		Sona, Diego/A-9667-2014	Sona, Diego/0000-0003-2883-5336					0	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						786	792						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00111
C	Todorov, E; Siapas, A; Somers, D		Mozer, MC; Jordan, MI; Petsche, T		Todorov, E; Siapas, A; Somers, D			A model of recurrent interactions in primary visual cortex	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				A general feature of the cerebral cortex is its massive interconnectivity - it has been estimated anatomically [19] that cortical neurons receive upwards of 5,000 synapses, the majority of which originate from other nearby cortical neurons. Numerous experiments in primary visual cortex (V1) have revealed strongly nonlinear interactions between stimulus elements which activate classical and non-classical receptive field regions. Recurrent cortical connections likely contribute substantially to these effects. However, most theories of visual processing have either assumed a feedforward processing scheme [7], or have used recurrent interactions to account for isolated effects only [1, 16, 18]. Since nonlinear systems cannot in general be taken apart and analyzed in pieces, it is not clear what one learns by building a recurrent model that only accounts for one, or very few phenomena. Here we develop a relatively simple model of recurrent interactions in V1, that reflects major anatomical and physiological features of intracortical connectivity, and simultaneously accounts for a wide range of phenomena observed physiologically. All phenomena we address are strongly nonlinear, and cannot be explained by linear feedforward models.			Todorov, E (corresponding author), MIT,DEPT BRAIN & COGNIT SCI,E25-526,CAMBRIDGE,MA 02139, USA.		Somers, David C./G-5802-2010	Somers, David C./0000-0002-4169-5895					0	3	3	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						118	124						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00017
C	Cherkauer, KJ; Shavlik, JW		Touretzky, DS; Mozer, MC; Hasselmo, ME		Cherkauer, KJ; Shavlik, JW			Rapid quality estimation of neural network input representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV WISCONSIN,DEPT COMP SCI,MADISON,WI 53706	University of Wisconsin System; University of Wisconsin Madison									0	3	4	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						45	51						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00007
C	Omohundro, SM		Touretzky, DS; Mozer, MC; Hasselmo, ME		Omohundro, SM			Family discovery	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						NEC RES INST,PRINCETON,NJ 08540	NEC Corporation									0	3	3	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						402	408						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00057
C	Singer, Y		Touretzky, DS; Mozer, MC; Hasselmo, ME		Singer, Y			Adaptive mixture of probabilistic transducers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						AT&T BELL LABS,NAPERVILLE,IL 60566	AT&T									0	3	3	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						381	387						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00054
C	Yasui, S; Furukawa, T; Yamada, M; Saito, T		Touretzky, DS; Mozer, MC; Hasselmo, ME		Yasui, S; Furukawa, T; Yamada, M; Saito, T			Plasticity of center-surround opponent receptive fields in real and artificial neural systems of vision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						KYUSHU INST TECHNOL,IIZUKA,FUKUOKA 820,JAPAN	Kyushu Institute of Technology									0	3	3	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						159	165						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00023
C	BAUER, HU; PAWELZIK, K; GEISEL, T		MOODY, JE; HANSON, SJ; LIPPMANN, RP		BAUER, HU; PAWELZIK, K; GEISEL, T			A TOPOGRAPHIC PRODUCT FOR THE OPTIMIZATION OF SELF-ORGANIZING FEATURE MAPS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	3	3	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1141	1147						7	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00141
C	LANGE, TE		MOODY, JE; HANSON, SJ; LIPPMANN, RP		LANGE, TE			DYNAMICALLY-ADAPTIVE WINNER-TAKE-ALL NETWORKS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	3	3	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						341	348						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00042
C	ROSCHEISEN, M; HOFMANN, R; TRESP, V		MOODY, JE; HANSON, SJ; LIPPMANN, RP		ROSCHEISEN, M; HOFMANN, R; TRESP, V			NEURAL CONTROL FOR ROLLING-MILLS - INCORPORATING DOMAIN THEORIES TO OVERCOME DATA DEFICIENCY	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	3	3	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						659	666						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00081
C	SHAWETAYLOR, J		MOODY, JE; HANSON, SJ; LIPPMANN, RP		SHAWETAYLOR, J			THRESHOLD NETWORK LEARNING IN THE PRESENCE OF EQUIVALENCES	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO											Shawe-Taylor, John/0000-0002-2030-0073					0	3	3	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						879	886						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00108
C	SUMIDA, RA; DYER, MG		MOODY, JE; HANSON, SJ; LIPPMANN, RP		SUMIDA, RA; DYER, MG			PROPAGATION FILTERS IN PDS NETWORKS FOR SEQUENCING AND AMBIGUITY RESOLUTION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	3	3	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						233	240						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00029
C	THIRIA, S; BADRAN, F; MEJIA, C; CREPON, M		MOODY, JE; HANSON, SJ; LIPPMANN, RP		THIRIA, S; BADRAN, F; MEJIA, C; CREPON, M			MULTIMODULAR ARCHITECTURE FOR REMOTE-SENSING OPERATIONS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	3	3	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						675	682						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00083
C	Garcelon, E; Roziere, B; Meunier, L; Tarbouriech, J; Teytaud, O; Lazaric, A; Pirotta, M		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Garcelon, Evrard; Roziere, Baptiste; Meunier, Laurent; Tarbouriech, Jean; Teytaud, Olivier; Lazaric, Alessandro; Pirotta, Matteo			Adversarial Attacks on Linear Contextual Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Contextual bandit algorithms are applied in a wide range of domains, from advertising to recommender systems, from clinical trials to education. In many of these domains, malicious agents may have incentives to force a bandit algorithm into a desired behavior. For instance, an unscrupulous ad publisher may try to increase their own revenue at the expense of the advertisers; a seller may want to increase the exposure of their products, or thwart a competitor's advertising campaign. In this paper, we study several attack scenarios and show that a malicious agent can force a linear contextual bandit algorithm to pull any desired arm T - o(T) times over a horizon of T steps, while applying adversarial modifications to either rewards or contexts with a cumulative cost that only grow logarithmically as O(log T). We also investigate the case when a malicious agent is interested in affecting the behavior of the bandit algorithm in a single context (e.g., a specific user). We first provide sufficient conditions for the feasibility of the attack and an efficient algorithm to perform an attack. We empirically validate the proposed approaches in synthetic and real-world datasets.	[Garcelon, Evrard; Roziere, Baptiste; Meunier, Laurent; Tarbouriech, Jean; Teytaud, Olivier; Lazaric, Alessandro; Pirotta, Matteo] Facebook AI Res, Paris, France	Facebook Inc	Garcelon, E (corresponding author), Facebook AI Res, Paris, France.	evrard@fb.com; broz@fb.com; laurentmeunier@fb.com; jtarbouriech@fb.com; oteytaud@fb.com; lazaric@fb.com; pirotta@fb.com		Roziere, Baptiste/0000-0002-9014-4379				Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Abeille M, 2017, ELECTRON J STAT, V11, P5165, DOI 10.1214/17-EJS1341SI; Agrawal A., 2018, J CONTROL DECIS, V5, P42, DOI [10.1080/23307706.2017.1397554, DOI 10.1080/23307706.2017.1397554]; Agrawal S, 2020, INDIAN J CRIT CARE M, V24, P1272, DOI 10.5005/jp-journals-10071-23682; Anish Nicholas Carlini Athalye, 2018, INT C MACH LEARN; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Biggio B., 2012, 29 INT C MACH LEARN, P1807; Bubeck S., 2012, P MACHINE LEARNING R, V23, P1; Christakopoulou K, 2019, RECSYS 2019: 13TH ACM CONFERENCE ON RECOMMENDER SYSTEMS, P322, DOI 10.1145/3298689.3347031; Davidson J., 2010, P 4 ACM C RECOMMENDE, P293, DOI DOI 10.1145/1864708.1864770; Dong WS, 2014, IEEE IMAGE PROC, P4018, DOI 10.1109/ICIP.2014.7025816; Goldberg K, 2001, INFORM RETRIEVAL, V4, P133, DOI 10.1023/A:1011419012209; Gomez-Uribe CA, 2016, ACM TRANS MANAG INF, V6, DOI 10.1145/2843948; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Guan Ziwei, 2020, ARXIV200207214; Gupta A., 2019, ARXIV190208647; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; HE J, 2010, P 19 INT C WORLD WID, P1061, DOI DOI 10.1145/1772690.1772758; Hussenot L., 2019, ARXIV190512282; Immorlica N, 2019, ANN IEEE SYMP FOUND, P202, DOI 10.1109/FOCS.2019.00022; Jagielski M, 2018, P IEEE S SECUR PRIV, P19, DOI 10.1109/SP.2018.00057; Jun KS, 2018, ADV NEUR IN, V31; Kapoor S, 2019, MACH LEARN, V108, P687, DOI 10.1007/s10994-018-5758-5; Lazic N, 2018, ADV NEUR IN, V31; Li B., 2016, ARXIV PREPRINT ARXIV; Li Yingkai, 2019, ARXIV190902109; Liu CF, 2017, DES AUT TEST EUROPE, P91, DOI 10.23919/DATE.2017.7926964; Liu Fang, 2019, INT C MACH LEARN, P4042; Lykouris T, 2018, ACM S THEORY COMPUT, P114, DOI 10.1145/3188745.3188918; Lykouris Thodoris, 2019, ABS191108689 ARXIV; Ma YZ, 2019, ADV NEUR IN, V32; Mehta B., 2008, P 31 ANN INT ACM SIG, P75; Park H, 2017, IEEE INT CONF BIG DA, P756; Sun Jianwen, 2020, P AAAI C ART INT; Szepesvdri C., 2018, BANDIT ALGORITHMS; Tuy H., 1995, HDB GLOBAL OPTIMIZAT, V25, P149, DOI DOI 10.1007/978-1-4615-2025-24; VARAH JM, 1975, LINEAR ALGEBRA APPL, V11, P3, DOI 10.1016/0024-3795(75)90112-3; Virtanen P, 2020, NAT METHODS, V17, P261, DOI 10.1038/s41592-019-0686-2	38	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000053
C	Li, MH; Zhang, MJ; Wang, C; Li, MQ		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Li, Menghao; Zhang, Minjia; Wang, Chi; Li, Mingqin			AdaTune: Adaptive Tensor Program Compilation Made Efficient	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Deep learning models are computationally intense, and implementations often have to be highly optimized by experts or hardware vendors to be usable in practice. The DL compiler, together with Learning-to-Compile has proven to be a powerful technique for optimizing tensor programs. However, a limitation of this approach is that it still suffers from unbearably long overall optimization time. In this paper, we present a new method, called AdaTune, that significantly reduces the optimization time of tensor programs for high-performance deep learning inference. In particular, we propose an adaptive evaluation method that statistically early terminates a costly hardware measurement without losing much accuracy. We further devise a surrogate model with uncertainty quantification that allows the optimization to adapt to hardware and model heterogeneity better. Finally, we introduce a contextual optimizer that provides adaptive control of the exploration and exploitation to improve the transformation space searching effectiveness. We evaluate and compare the levels of optimization obtained by AutoTVM, a stateof-the-art Learning-to-Compile technique on top of TVM, and AdaTune. The experiment results show that AdaTune obtains up to 115% higher GFLOPS than the baseline under the same optimization time budget. Furthermore, AdaTune provides 1.3-3.9x speedup in optimization time over the baseline to reach the same optimization quality for a range of models across different hardware architectures.	[Li, Menghao; Zhang, Minjia; Wang, Chi; Li, Mingqin] Microsoft Corp, Redmond, WA 98052 USA	Microsoft	Li, MH (corresponding author), Microsoft Corp, Redmond, WA 98052 USA.	t-meli@microsoft.com; minjiaz@microsoft.com; wang.chi@microsoft.com; mingqli@microsoft.com						Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Adams A, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322967; Ahn B. H., 2020, P 8 INT C LEARN REPR; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ansel J, 2014, INT CONFER PARA, P303, DOI 10.1145/2628071.2628092; Ashouri AH, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3197978; Brochu E., 2010, TUTORIAL BAYESIAN OP; CHEN T, 2018, ADV NEURAL INFORM PR, P3932; Chen TQ, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P579; Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3; Chetlur S., 2014, ARXIV; DeVito Zachary, 2018, ARXIV180204730; Devlin J., 2018, P 2019 C N AM CHAPT, DOI [DOI 10.18653/V1/N19-1423, 10.18653/v1/N19-1423]; Falkner S, 2018, PR MACH LEARN RES, V80; Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758; Goyal Priya, 2017, ARXIV170602677; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hegeman J., 2018, GLOW GRAPH LOWERING; Holmes C, 2019, PROCEEDINGS OF THE FOURTEENTH EUROSYS CONFERENCE 2019 (EUROSYS '19), DOI 10.1145/3302424.3303949; Howard A.G., 2017, MOBILENETS EFFICIENT; Iandola F.N., 2016, ARXIV; Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246; Lattner C, 2020, ARXIV200211054; Li Lisha, 2017, 5 INT C LEARN REPR I; Liang S, 2018, NEUROCOMPUTING, V275, P1072, DOI 10.1016/j.neucom.2017.09.046; Lin Ji, 2019, ARXIV191000932; Liu C, 2019, ARXIV190407404; Moreau T., 2018, ARXIV180704188; Paszke A, 2019, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.1912.01703; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Radford A., 2019, OPENAI BLOG, V1, P9, DOI DOI 10.18653/V1/P19-1195; Raffel Colin, 2019, ABS191010683 CORR; Ragan-Kelley Jonathan, 2013, P 34 ACM SIGPLAN C P, P519; Shoeybi Mohammad, 2019, ARXIV190908053; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang Z, 2018, P IEEE, V106, P1879, DOI 10.1109/JPROC.2018.2817118; Wu CJ, 2019, INT S HIGH PERF COMP, P331, DOI 10.1109/HPCA.2019.00048; Yamazaki Masafumi, 2019, ARXIV190312650; You Yang, 2017, ARXIV170803888, V6, P12; You Yang, 2019, ABS190400962 CORR; Zhang MJ, 2019, PROCEEDINGS OF THE 2019 USENIX CONFERENCE ON OPERATIONAL MACHINE LEARNING, P5	43	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													13	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000047
C	Squires, C; Magliacane, S; Greenewald, K; Katz, D; Kocaoglu, M		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Squires, Chandler; Magliacane, Sara; Greenewald, Kristjan; Katz, Dmitriy; Kocaoglu, Murat			Active Structure Learning of Causal DAGs via Directed Clique Trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				NETWORKS	A growing body of work has begun to study intervention design for efficient structure learning of causal directed acyclic graphs (DAGs). A typical setting is a causally sufficient setting, i.e. a system with no latent confounders, selection bias, or feedback, when the essential graph of the observational equivalence class (EC) is given as an input and interventions are assumed to be noiseless. Most existing works focus on worst-case or average-case lower bounds for the number of interventions required to orient a DAG. These worst-case lower bounds only establish that the largest clique in the essential graph could make it difficult to learn the true DAG. In this work, we develop a universal lower bound for single-node interventions that establishes that the largest clique is always a fundamental impediment to structure learning. Specifically, we present a decomposition of a DAG into independently orientable components through directed clique trees and use it to prove that the number of single-node interventions necessary to orient any DAG in an EC is at least the sum of half the size of the largest cliques in each chain component of the essential graph. Moreover, we present a two-phase intervention design algorithm that, under certain conditions on the chordal skeleton, matches the optimal number of interventions up to a multiplicative logarithmic factor in the number of maximal cliques. We show via synthetic experiments that our algorithm can scale to much larger graphs than most of the related work and achieves better worst-case performance than other scalable approaches.(1)	[Squires, Chandler] MIT, LIDS, MIT IBM Watson AI Lab, Cambridge, MA 02139 USA; [Magliacane, Sara; Greenewald, Kristjan; Katz, Dmitriy; Kocaoglu, Murat] IBM Res, MIT IBM Watson Lab, Ossining, NY USA	Massachusetts Institute of Technology (MIT); International Business Machines (IBM)	Squires, C (corresponding author), MIT, LIDS, MIT IBM Watson AI Lab, Cambridge, MA 02139 USA.	csquires@mit.edu; sara.magliacane@gmail.com; kristjan.h.greenewald@ibm.com; dkatzrog@us.ibm.com; murat@ibm.com			NSF Graduate Research Fellowship; MIT Presidential Fellowship; MIT-IBM Watson AI Lab; IBM Research	NSF Graduate Research Fellowship(National Science Foundation (NSF)); MIT Presidential Fellowship; MIT-IBM Watson AI Lab(International Business Machines (IBM)); IBM Research(International Business Machines (IBM))	Chandler Squires was supported by an NSF Graduate Research Fellowship and an MIT Presidential Fellowship and part of the work was performed during an internship at IBM Research. The work was supported by the MIT-IBM Watson AI Lab,	Andersen M. S., 2015, FDN TRENDS OPTIMIZAT, V1, P241, DOI DOI 10.1561/2400000006; BORODIN A, 1992, J ACM, V39, P745, DOI 10.1145/146585.146588; Daniely A., 2019, ARXIV190403602; Eberhardt F, 2006, STUD FUZZ SOFT COMP, V194, P97; Eberhardt F., 2007, THESIS, P93; Friedman N, 2000, J COMPUT BIOL, V7, P601, DOI 10.1089/106652700750050961; Galinier P, 1995, LECT NOTES COMPUT SC, V1017, P358; Ghassami A., 2017, ARXIV170208567; Ghassami A., 2018, P MACH LEARN RES, P1724; Ghassami A, 2019, AAAI CONF ARTIF INTE, P3664; Gillispie S.B., 2013, ARXIV13012272 ARXIV13012272; Greenewald K., 2019, ADV NEURAL INFORM PR; Hauser A, 2014, INT J APPROX REASON, V55, P926, DOI 10.1016/j.ijar.2013.11.007; He YB, 2008, J MACH LEARN RES, V9, P2523; Hyttinen A, 2013, J MACH LEARN RES, V14, P3041; Kocaoglu M, 2017, PR MACH LEARN RES, V70; Koller D., 2009, PROBABILISTIC GRAPHI; Kumar PS, 2002, DISCRETE APPL MATH, V117, P109, DOI 10.1016/S0166-218X(00)00336-X; Lindgren E., 2018, ADV NEURAL INFORM PR, P5279; Maathuis M., 2018, HDB GRAPHICAL MODELS; Meek C., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P403; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Peters J, 2017, ADAPT COMPUT MACH LE; Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809; Shanmugam K., 2015, ADV NEURAL INFORM PR, P3195; Spirtes P., 2000, CAUSATION PREDICTION	26	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000028
C	Adlam, B; Cortes, C; Mohri, M; Zhang, NS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Adlam, Ben; Cortes, Corinna; Mohri, Mehryar; Zhang, Ningshan			Learning GANs and Ensembles Using Discrepancy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Generative adversarial networks (GANs) generate data based on minimizing a divergence between two distributions. The choice of that divergence is therefore critical. We argue that the divergence must take into account the hypothesis set and the loss function used in a subsequent learning task, where the data generated by a GAN serves for training. Taking that structural information into account is also important to derive generalization guarantees. Thus, we propose to use the discrepancy measure, which was originally introduced for the closely related problem of domain adaptation and which precisely takes into account the hypothesis set and the loss function. We show that discrepancy admits favorable properties for training GANs and prove explicit generalization guarantees. We present efficient algorithms using discrepancy for two tasks: training a GAN directly, namely DGAN, and mixing previously trained generative models, namely EDGAN. Our experiments on toy examples and several benchmark datasets show that DGAN is competitive with other GANs and that EDGAN outperforms existing GAN ensembles, such as AdaGAN.	[Adlam, Ben; Cortes, Corinna; Mohri, Mehryar] Google Res, New York, NY 10011 USA; [Mohri, Mehryar] CIMS, New York, NY 10012 USA; [Zhang, Ningshan] NYU, New York, NY 10012 USA	Google Incorporated; New York University	Adlam, B (corresponding author), Google Res, New York, NY 10011 USA.	adlam@google.com; corinna@google.com; mohri@google.com; nzhang@stern.nyu.edu			NSF [CCF-1535987, IIS-1618662]; Google Research Award	NSF(National Science Foundation (NSF)); Google Research Award(Google Incorporated)	This work was partly supported by NSF CCF-1535987, NSF IIS-1618662, and a Google Research Award. We thank Judy Hoffman for helpful pointers to the literature.	Arjovsky M, 2017, PR MACH LEARN RES, V70; Ben-David Shai, 2007, NEURIPS, P7; Brock A., 2019, INT C LEARNING REPRE; Chhabra A, 2017, 2017 IEEE 3RD INTERNATIONAL CONFERENCE ON COLLABORATION AND INTERNET COMPUTING (CIC), P243, DOI 10.1109/CIC.2017.00040; Cortes C, 2017, PR MACH LEARN RES, V70; Cortes C, 2014, THEOR COMPUT SCI, V519, P103, DOI 10.1016/j.tcs.2013.09.027; Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202; Deshpande I, 2018, PROC CVPR IEEE, P3483, DOI 10.1109/CVPR.2018.00367; DONSKER MD, 1975, COMMUN PUR APPL MATH, V28, P1, DOI 10.1002/cpa.3160280102; Dziugaite GK, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P258; Feizi S., 2017, ARXIV PREPRINT ARXIV; Foo C.S., 2018, PROC WORKSHOP INT C; Frid-Adar M, 2018, NEUROCOMPUTING, V321, P321, DOI 10.1016/j.neucom.2018.09.013; Ghosh A, 2018, PROC CVPR IEEE, P8513, DOI 10.1109/CVPR.2018.00888; Golub G. H., 2012, MATRIX COMPUTATIONS; Goodfellow I., 2017, ARXIV170100160; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A, 2012, J MACH LEARN RES, V13, P723; Gulrajani I., 2011, ADV NEURAL INFORM PR, P5769; Hoang Quan, 2018, ICLR; Jin Yanghua, 2017, ARXIV170805509; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kuznetsov V., 2015, ADV NEURAL INFORM PR, P541; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Liu C., 2018, P EUR C COMP VIS ECC, P19, DOI DOI 10.1007/978-3-030-01246-5_2; Liu Ruishan, 2018, ARXIV181202271; Mansour Yishay, 2009, COLT 2009 22 C LEARN; Mao Xudong, 2017, ICCV, DOI [10.1109/ICCV.2017.304, DOI 10.1109/ICCV.2017.304]; Mohri Mehryar, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P124, DOI 10.1007/978-3-642-34106-9_13; Mroueh Y, 2017, PR MACH LEARN RES, V70; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Salimans T, 2016, ADV NEUR IN, V29; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tolstikhin I. O., 2017, ADV NEURAL INFORM PR, P5424, DOI DOI 10.5555/3295222.3295294; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Zoph B., 2017, P1	42	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305075
C	Aitchison, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Aitchison, Laurence			Tensor Monte Carlo: Particle Methods for the GPU era	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Multi-sample, importance-weighted variational autoencoders (IWAE) give tighter bounds and more accurate uncertainty estimates than variational autoencoders (VAEs) trained with a standard single-sample objective. However, IWAEs scale poorly: as the latent dimensionality grows, they require exponentially many samples to retain the benefits of importance weighting. While sequential Monte-Carlo (SMC) can address this problem, it is prohibitively slow because the resampling step imposes sequential structure which cannot be parallelised, and moreover, resampling is non-differentiable which is problematic when learning approximate posteriors. To address these issues, we developed tensor Monte-Carlo (TMC) which gives exponentially many importance samples by separately drawing K samples for each of the n latent variables, then averaging over all K-n possible combinations. While the sum over exponentially many terms might seem to be intractable, in many cases it can be computed efficiently as a series of tensor inner-products. We show that TMC is superior to IWAE on a generative model with multiple stochastic layers trained on the MNIST handwritten digit database, and we show that TMC can be combined with standard variance reduction techniques.	[Aitchison, Laurence] Univ Bristol, Bristol, Avon, England; [Aitchison, Laurence] Janelia Res Campus, Ashburn, VA USA	University of Bristol	Aitchison, L (corresponding author), Univ Bristol, Bristol, Avon, England.	laurence.aitchison@gmail.com			HHMI	HHMI(Howard Hughes Medical Institute)	I would like to thank HHMI for funding and computational infrastructure.	Ba J., 2017, P 3 INT C LEARN REPR; Bengtsson T, 2008, PROBABILITY STAT ESS, V2, P316, DOI [DOI 10.1214/193940307000000518, 10.1214/193940307000000518]; Bingham Eli, 2018, J MACHINE LEARNING R; Bishop C.M, 2006, PATTERN RECOGN; Bornschein Jorg, 2014, ARXIV14062751; Burda Yuri, 2015, ICLR; Chatterjee S, 2015, ARXIV151101437; Cremer Chris, 2017, ARXIV170402916; Doucet A., 2009, HDB NONLINEAR FILTER, V12, P3; Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170; Fearnhead P, 2004, STAT COMPUT, V14, P11, DOI 10.1023/B:STCO.0000009418.04621.cd; Frey B. J., 2003, UAI 03 P 19 C UNC AR, P257; Kingma DP, 2 INT C LEARN REPR I, P1; Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572; LAURITZEN SL, 1988, J ROY STAT SOC B MET, V50, P157; Le T. A., 2017, ARXIV170510306; Maddison C. J., 2017, ADV NEURAL INFORM PR, P6576; Minka Tom, 2005, TECHNICAL REPORT; Naesseth C. A., 2017, ARXIV170511140; Neal RM, 2004, ADV NEUR IN, V16, P401; PEARL J, 1986, ARTIF INTELL, V29, P241, DOI 10.1016/0004-3702(86)90072-X; Pearl J., 1982, AAAI 82 P 2 AAAI C A, P133; Rainforth T, 2018, PR MACH LEARN RES, V80; Rezende D.J., 2014, PROC INT CONFER ENCE; Roeder G, 2017, ADV NEUR IN, V30; Snyder C, 2008, MON WEATHER REV, V136, P4629, DOI 10.1175/2008MWR2529.1; Sonderby CK, 2016, ADV NEUR IN, V29; Teh Yee Whye, 2018, REVISITING REWEIGHTE; Tucker George, 2018, ARXIV181004152; Turner R., 2011, BAYESIAN TIME SERIES; Wood F, 2014, JMLR WORKSH CONF PRO, V33, P1024	33	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307019
C	Akrout, M; Wilson, C; Humphreys, PC; Lillicrap, T; Tweed, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Akrout, Mohamed; Wilson, Collin; Humphreys, Peter C.; Lillicrap, Timothy; Tweed, Douglas			Deep Learning without Weight Transport	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Current algorithms for deep learning probably cannot run in the brain because they rely on weight transport, where forward-path neurons transmit their synaptic weights to a feedback path, in a way that is likely impossible biologically. An algorithm called feedback alignment achieves deep learning without weight transport by using random feedback weights, but it performs poorly on hard visual-recognition tasks. Here we describe two mechanisms - a neural circuit called a weight mirror and a modification of an algorithm proposed by Kolen and Pollack in 1994 - both of which let the feedback path learn appropriate synaptic weights quickly and accurately even in large networks, without weight transport or complex wiring. Tested on the ImageNet visual-recognition task, these mechanisms learn almost as well as backprop (the standard algorithm of deep learning, which uses weight transport) and they outperform feedback alignment and another, more-recent transport-free algorithm, the sign-symmetry method.	[Akrout, Mohamed] Univ Toronto, Triage, Toronto, ON, Canada; [Wilson, Collin] Univ Toronto, Toronto, ON, Canada; [Humphreys, Peter C.] DeepMind, London, England; [Lillicrap, Timothy] UCL, DeepMind, London, England; [Tweed, Douglas] York Univ, Univ Toronto, Toronto, ON, Canada	University of Toronto; University of Toronto; University of London; University College London; University of Toronto; York University - Canada	Akrout, M (corresponding author), Univ Toronto, Triage, Toronto, ON, Canada.							[Anonymous], [No title captured]; Bartunov S., 2018, ADV NEURAL INFORM PR, P9368; Buchlovsky Peter, 2019, ARXIV190200465; Chen YH, 2016, CONF PROC INT SYMP C, P367, DOI 10.1109/ISCA.2016.40; Chowdhury Dhrubajyoti, 2018, F1000RESEARCH, V7; Crafton B., 2019, ARXIV190302083; CRICK F, 1989, NATURE, V337, P129, DOI 10.1038/337129a0; Eliasmith C., 2004, NEURAL ENG COMPUTATI; GROSSBERG S, 1987, COGNITIVE SCI, V11, P23, DOI 10.1111/j.1551-6708.1987.tb00862.x; Guergiuev J., 2016, ARXIV161000161; Guerguiev J, 2017, ELIFE, V6, DOI 10.7554/eLife.22901; Guerguiev Jordan, 2019, ARXIV191001689; Gushchin A, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0145029; INTRATOR N, 1992, NEURAL NETWORKS, V5, P3, DOI 10.1016/S0893-6080(05)80003-6; Ioffe S, 2015, ARXIV 1502 03167, V32, P448; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; KOLEN JF, 1994, 1994 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOL 1-7, P1375, DOI 10.1109/ICNN.1994.374486; Krotov D, 2019, P NATL ACAD SCI USA, V116, P7723, DOI 10.1073/pnas.1820458116; Kunin D., 2019, ARXIV190108168; Kwon H., 2018, ARXIV180502566; Langen M, 2015, CELL, V162, P120, DOI 10.1016/j.cell.2015.05.055; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276; Naud R, 2018, P NATL ACAD SCI USA, V115, pE6329, DOI 10.1073/pnas.1720995115; Palay S.L., 1974, CEREBELLAR CORTEX; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Turrigiano GG, 2008, CELL, V135, P422, DOI 10.1016/j.cell.2008.10.008; Urbanczik R, 2014, NEURON, V81, P521, DOI 10.1016/j.neuron.2013.11.030; Xiao W., 2018, ARXIV181103567; Zee DS., 1999, NEUROLOGY EYE MOVEME	32	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301002
C	Aliakbarpour, M; Diakonikolas, I; Kane, D; Rubinfeld, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Aliakbarpour, Maryam; Diakonikolas, Ilias; Kane, Daniel; Rubinfeld, Ronitt			Private Testing of Distributions via Sample Permutations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				VARIABLES	Statistical tests are at the heart of many scientific tasks. To validate their hypotheses, researchers in medical and social sciences use individuals' data. The sensitivity of participants' data requires the design of statistical tests that ensure the privacy of the individuals in the most efficient way. In this paper, we use the framework of property testing to design algorithms to test the properties of the distribution that the data is drawn from with respect to differential privacy. In particular, we investigate testing two fundamental properties of distributions: (1) testing the equivalence of two distributions when we have unequal numbers of samples from the two distributions. (2) Testing independence of two random variables. In both cases, we show that our testers achieve near optimal sample complexity (up to logarithmic factors). Moreover, our dependence on the privacy parameter is an additive term, which indicates that differential privacy can be obtained in most regimes of parameters for free.	[Aliakbarpour, Maryam] MIT, CSAIL, Cambridge, MA 02139 USA; [Diakonikolas, Ilias] Univ Wisconsin, Madison, WI 53706 USA; [Kane, Daniel] Univ Calif San Diego, La Jolla, CA 92093 USA; [Rubinfeld, Ronitt] MIT, CSAIL, TAU, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); University of Wisconsin System; University of Wisconsin Madison; University of California System; University of California San Diego; Massachusetts Institute of Technology (MIT)	Aliakbarpour, M (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	maryama@mit.edu; ilias.diakonikolas@gmail.com; dakane@ucsd.edu; ronitt@csail.mit.edu			MIT-IBM Watson AI Lab [W1771646]; NSF [CCF-1650733, CCF-1733808, IIS-1741137, CCF-1740751]; NSF Award [CCF-1652862, CCF-1553288]; NSF AiTF award [CCF-1733796]; Sloan Research Fellowship	MIT-IBM Watson AI Lab; NSF(National Science Foundation (NSF)); NSF Award(National Science Foundation (NSF)); NSF AiTF award; Sloan Research Fellowship(Alfred P. Sloan Foundation)	MA is supported by funds from the MIT-IBM Watson AI Lab (Agreement No. W1771646), the NSF grants IIS-1741137, and CCF-1733808. ID is supported by NSF Award CCF-1652862 (CAREER), NSF AiTF award CCF-1733796 and a Sloan Research Fellowship. DK is supported by NSF Award CCF-1553288 (CAREER) and a Sloan Research Fellowship. RR is supported by funds from the MIT-IBM Watson AI Lab (Agreement No. W1771646) the NSF grants CCF-1650733, CCF-1733808, IIS-1741137 and CCF-1740751.	Acharya J., 2018, ADV NEURAL INFORM PR, P6879; Acharya J., 2019, 22 INT C ART INT STA, P2067; Acharya J, 2014, IEEE INT SYMP INFO, P3200, DOI 10.1109/ISIT.2014.6875425; Acharya  Jayadev, 2015, ADV NEURAL INFORM PR, P3591; Aliakbarpour Maryam, 2018, P 35 INT C MACH LEAR, P169; [Anonymous], 2011, ICS; Batu T, 2001, ANN IEEE SYMP FOUND, P442, DOI 10.1109/SFCS.2001.959920; Batu T, 2000, ANN IEEE SYMP FOUND, P259, DOI 10.1109/SFCS.2000.892113; Batu T., 2004, P 36 ANN ACM S THEOR, P381; Batu T, 2017, ANN IEEE SYMP FOUND, P880, DOI 10.1109/FOCS.2017.86; Batu T, 2013, J ACM, V60, DOI 10.1145/2432622.2432626; Cai B, 2017, PR MACH LEARN RES, V70; Canonne C. L., 2017, NEURIPS 2018; Canonne C. L., 2017, STOC 18; Canonne CL, 2016, LEIBNIZ INT PR INFOR, V47, DOI 10.4230/LIPIcs.STACS.2016.25; Canonne Clement L., 2015, ELECT C COMPUTATIONA, V22, P63; Canonne Clement L., 2017, P 30 C LEARN THEOR C, P370; Chan S., 2014, P 25 ANN ACM SIAM S, P1193, DOI [10.1137/1.9781611973402.88, DOI 10.1137/1.9781611973402.88]; Daskalakis C, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1833; Daskalakis Constantinos, 2017, P 30 C LEARNING THEO, P697; Diakonikolas I., 2019, C LEARN THEOR COLT 2, P1107; Diakonikolas I., 2015, ADV NEURAL INFORM PR, P2566; Diakonikolas I., 2018, ADV NEURAL INFORM PR, P6204; Diakonikolas I., 2017, ICALP 2018; Diakonikolas I., 2016, ELECT C COMPUTATIONA, V23, P178; Diakonikolas I., 2017, 44 INT C AUT LANG PR, DOI 10.4230/LIPIcs.ICALP.2017.8; Diakonikolas I, 2016, ANN IEEE SYMP FOUND, P685, DOI 10.1109/FOCS.2016.78; Diakonikolas I, 2015, ANN IEEE SYMP FOUND, P1183, DOI 10.1109/FOCS.2015.76; Diakonikolas Ilias, 2015, P TWENTYSIXTH ANN AC, P1841, DOI [10.1137/1.9781611973730.123, DOI 10.1137/1.9781611973730.123]; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork C, 2009, LECT NOTES COMPUT SC, V5444, P496; Gaboardi M, 2016, PR MACH LEARN RES, V48; Gaboardi Marco, 2018, P MACHINE LEARNING R, V80, P1612; Goldreich O., 2017, INTRO PROPERTY TESTI; Kakizaki K., 2017, P 34 INT C MACH LEAR, P1761; Kifer D, 2017, PR MACH LEARN RES, V54, P991; Neyman J, 1933, PHILOS T R SOC LOND, V231, P289, DOI 10.1098/rsta.1933.0009; Paninski L, 2008, IEEE T INFORM THEORY, V54, P4750, DOI 10.1109/TIT.2008.928987; Pearson K, 1900, PHILOS MAG, V50, P157, DOI 10.1080/14786440009463897; Rubinfeld R., 2012, XRDS CROSSROADS FAL, V19, P24, DOI DOI 10.1145/2331042.2331052; Sheffet Or, 2018, P INT C MACH LEARN, V80, P4612; Valiant G., 2014, FOCS; Wang Y., 2015, ABS151103376 CORR	45	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902050
C	Allen, J; Ding, BL; Kulkarni, J; Nori, H; Ohrimenko, O; Yekhanin, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Allen, Joshua; Ding, Bolin; Kulkarni, Janardhan; Nori, Harsha; Ohrimenko, Olga; Yekhanin, Sergey			An Algorithmic Framework For Differentially Private Data Analysis on Trusted Processors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Differential privacy has emerged as the main definition for private data analysis and machine learning. The global model of differential privacy, which assumes that users trust the data collector, provides strong privacy guarantees and introduces small errors in the output. In contrast, applications of differential privacy in commercial systems by Apple, Google, and Microsoft, use the local model. Here, users do not trust the data collector, and hence randomize their data before sending it to the data collector. Unfortunately, local model is too strong for several important applications and hence is limited in its applicability. In this work, we propose a framework based on trusted processors and a new definition of differential privacy called Oblivious Differential Privacy, which combines the best of both local and global models. The algorithms we design in this framework show interesting interplay of ideas from the streaming algorithms, oblivious algorithms, and differential privacy.	[Allen, Joshua; Ding, Bolin; Kulkarni, Janardhan; Nori, Harsha; Ohrimenko, Olga; Yekhanin, Sergey] Microsoft, Redmond, WA 98052 USA; [Ding, Bolin] Alibaba Grp, Hangzhou, Peoples R China	Microsoft; Alibaba Group	Allen, J (corresponding author), Microsoft, Redmond, WA 98052 USA.							AJTAI M., 1983, ACM S THEOR COMP STO; Allen Joshua, 2018, CORR; Anati I., 2013, WORKSH HARDW ARCH SU; Asharov Gilad, 2018, 2018892 CRYPT EPRINT; Balle B, 2019, LECT NOTES COMPUT SC, V11693, P638, DOI 10.1007/978-3-030-26951-7_22; Bassily R., 2017, ADV NEURAL INFORM PR, P2285; Bassily R, 2015, ACM S THEORY COMPUT, P127, DOI 10.1145/2746539.2746632; Batcher K.E, 1968, AFIPS 68, DOI DOI 10.1145/1468075.1468121; Bernstein D. J., 2005, TECHNICAL REPORT; Bittau A., 2017, ACM S OP SYST PRINC; Brasser F., 2017, USENIX WORKSH OFF TE; Carlini N., 2019, USENIX SEC S; Chan T., 2019, PROC 30 ANN ACM SIAM, P2448; Cheu Albert, 2019, ADV CRYPTOLOGY AAS E; Cormode G, 2005, J ALGORITHMS, V55, P58, DOI 10.1016/j.jalgor.2003.12.001; Costan Victor, 2016, USENIX SEC S; Differential Privacy Team Apple, 2017, LEARN PRIV SCAL; Ding Bolin, 2017, C NEUR INF PROC SYST; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2009, ACM S THEORY COMPUT, P381; Dwork Cynthia, 2014, FDN TRENDS THEOR COM, V9; Erlingsson U, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1054, DOI 10.1145/2660267.2660348; Erlingsson Ulfar, 2019, ACM SIAM S DISCR ALG; FLAJOLET P, 1985, J COMPUT SYST SCI, V31, P182, DOI 10.1016/0022-0000(85)90041-8; Goldreich O, 1996, J ACM, V43, P431, DOI 10.1145/233551.233553; Goodrich Michael T., 2011, P 23 ANN ACM S PAR A; Goodrich Michael T., 2011, INT C AUT LANG PROGR; Goodrich Michael T., 2012, ACM SIAM S DISCR ALG; Gotzfried J., 2017, EUR WORKSH SYST SEC; Hoekstra M., 2013, WORKSH HARDW ARCH SU; Johnson N, 2018, PROC VLDB ENDOW, V11, P526, DOI 10.1145/3177732.3177733; Kane DM, 2010, PODS 2010: PROCEEDINGS OF THE TWENTY-NINTH ACM SIGMOD-SIGACT-SIGART SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P41, DOI 10.1145/1807085.1807094; Kocher PC, 1996, P CRYPTO; Liu Fangfei, 2015, IEEE S SEC PRIV S P; Mazloom Sahar, 2018, ACM C COMP COMM SEC; Mironov Ilya, 2012, ACM C COMP COMM SEC; Moghimi Ahmad, 2017, CRYPTOGRAPHIC HARDWA; Muthukrishnan S., 2003, FDN TRENDS THEORETIC, V1; Ohrimenko O., 2016, USENIX SEC S; OHRIMENKO O., 2014, INT C AUT LANG PROGR, V8573; OSVIK D. A., 2006, RSA C CRYPT TRACK CT; PAGE D., 2002, 2002169 CRYPT EPRINT; Patel S., 2018, INT C AUT LANG PROGR; Percival C., 2005, P BSDCAN; Pinkas Benny, 2010, ADV CRYPTOLOGY CRYPT; Sasy Sajin, 2019, C NEUR INF PROC SYST; SCHUSTER F., 2015, IEEE S SEC PRIV S P; SCHWARZ M., 2017, C DET INTR MALW VULN; Shokri Reza, 2017, IEEE S SEC PRIV S P; STEFANOV E, 2013, ACM C COMP COMM SEC; Stefanov Emil, 2012, S NETW DISTR SYST SE; Wagh S., 2018, P PRIV ENH TECHN; Williams P., 2008, ACM C COMP COMM SEC; XU Y., 2015, IEEE S SEC PRIV S P; Zheng Wenting, 2017, USENIX S NETW SYST D	55	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905032
C	Anil, R; Gupta, V; Koren, T; Singer, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Anil, Rohan; Gupta, Vineet; Koren, Tomer; Singer, Yoram			Memory-Efficient Adaptive Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Adaptive gradient-based optimizers such as Adagrad and Adam are crucial for achieving state-of-the-art performance in machine translation and language modeling. However, these methods maintain second-order statistics for each parameter, thus introducing significant memory overheads that restrict the size of the model being used as well as the number of examples in a mini-batch. We describe an effective and flexible adaptive optimization method with greatly reduced memory overhead. Our method retains the benefits of per-parameter adaptivity while allowing significantly larger models and batch sizes. We give convergence guarantees for our method, and demonstrate its effectiveness in training very large translation and language models with up to 2-fold speedups compared to the state-of-the-art.	[Anil, Rohan; Gupta, Vineet; Koren, Tomer] Google Brain, Mountain View, CA 94043 USA; [Koren, Tomer] Tel Aviv Univ, Tel Aviv, Israel; [Singer, Yoram] Princeton Univ, Princeton, NJ 08544 USA	Google Incorporated; Tel Aviv University; Princeton University	Anil, R (corresponding author), Google Brain, Mountain View, CA 94043 USA.	rohananil@google.com; vineet@google.com; tkoren@google.com; y.s@cs.princeton.edu						Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Agarwal N., 2018, CORR; Anil R., 2019, SM3 TENSORFLOW OPTIM; Anil Rohan, 2019, ARXIV190111150; Auer P, 2002, J COMPUT SYST SCI, V64, P48, DOI 10.1006/jcss.2001.1795; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Charikar M, 2002, LECT NOTES COMPUT SC, V2380, P693; Chen MX, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P76; Coleman C., 2018, ARXIV180601427; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246; Kingma D.P, P 3 INT C LEARNING R; Radford Alec, 2019, LANGUAGE MODELS ARE; Real E., 2018, ARXIV180201548; Reddi S., 2018, P INT C LEARN REPR; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schuster M, 2012, INT CONF ACOUST SPEE, P5149, DOI 10.1109/ICASSP.2012.6289079; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Shazeer N, 2018, PR MACH LEARN RES, V80; Shen J., LINGVO; Spring R., 2019, P MACHINE LEARNING R, P5946; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Vaswani Ashish, 2018, CORR; Wu Y., 2016, ADV NEURAL INFORM PR, V1609, DOI 10.48550/arXiv.1609.08144; Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11	29	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901038
C	Arnold, SMR; Manzagol, PA; Babanezhad, R; Mitliagkas, I; Le Roux, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Arnold, Sebastien M. R.; Manzagol, Pierre-Antoine; Babanezhad, Reza; Mitliagkas, Ioannis; Le Roux, Nicolas			Reducing the variance in online optimization by transporting past gradients	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Most stochastic optimization methods use gradients once before discarding them. While variance reduction methods have shown that reusing past gradients can be beneficial when there is a finite number of datapoints, they do not easily extend to the online setting. One issue is the staleness due to using past gradients. We propose to correct this staleness using the idea of implicit gradient transport (IGT) which transforms gradients computed at previous iterates into gradients evaluated at the current iterate without using the Hessian explicitly. In addition to reducing the variance and bias of our updates over time, IGT can be used as a drop-in replacement for the gradient estimate in a number of well-understood methods such as heavy ball or Adam. We show experimentally that it achieves state-of-the-art results on a wide range of architectures and benchmarks. Additionally, the IGT gradient estimator yields the optimal asymptotic convergence rate for online stochastic optimization in the restricted setting where the Hessians of all component functions are equal.	[Arnold, Sebastien M. R.] Univ Southern Calif, Los Angeles, CA 90007 USA; [Manzagol, Pierre-Antoine] Google Brain, Montreal, PQ, Canada; [Babanezhad, Reza] Univ British Columbia, Vancouver, BC, Canada; [Mitliagkas, Ioannis] Univ Montreal, Mila, Montreal, PQ, Canada; [Le Roux, Nicolas] Google Brain, Mila, Montreal, PQ, Canada	University of Southern California; University of British Columbia; Universite de Montreal	Arnold, SMR (corresponding author), Univ Southern Calif, Los Angeles, CA 90007 USA.	seb.arnold@usc.edu; manzagop@google.com; rezababa@cs.ubc.ca; ioannis@iro.umontreal.ca; nlr@google.com						Andrychowicz M., 2018, CORR; Babanezhad Reza, 2015, ADV NEURAL INFORM PR; Bach F., 2013, ADV NEURAL INFORM PR, V26, P773; Bubeck S., 2015, FDN TRENDS MACHINE L; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Dieuleveut A, 2017, J MACH LEARN RES, V18; Dieuleveut Aymeric, 2017, ARXIV170706386; Finn C, 2017, PR MACH LEARN RES, V70; Flammarion N., 2015, C LEARN THEOR, P658; Gower R, 2018, INT C ARTIFICIAL INT, P707; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Hofmann Thomas, 2015, ADV NEURAL INFORM PR, P2305; HOGAN E, 2013, ADV NEURAL INFORM PR, P315; Jain P., 2018, C LEARN THEOR, P545; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kwakernaak Huibert, LINEAR OPTIMAL CONTR, V1; Lacoste-Julien Simon, 2012, ARXIV12122002; Le Roux Nicolas, 2019, ARXIV190205083; LeCun Y, 2010, ATT LAB; Loizou N., 2017, ARXIV171209677; Maas A., 2011, P 49 ANN M ASS COMPU, P142; Mairal J., 2013, P 30 INT C INT C MAC, P783; Mark Schmidt, 2014, CONVERGENCE RATE STO; O'Donoghue B, 2015, FOUND COMPUT MATH, V15, P715, DOI 10.1007/s10208-013-9150-3; Pennington J., 2014, P 2014 C EMPIRICAL M, P1532; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Ravi S, 2017, 5 INT C LEARN REPR I; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roux N.L., 2012, ADV NEURAL INFORM PR, V25, P2663; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; SCHMIDT M., 2011, ADV NEURAL INFORM PR, V24; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Shallue CJ, 2019, J MACH LEARN RES, V20; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Vaswani S, 2019, PR MACH LEARN RES, V89; WAI HT, 2017, ANN ALLERTON CONF, P526; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; WILSON A. C., 2017, ADV NEURAL INFORM PR, V30, P4148; Zhang Jian, 2019, SYSML, P11; Zhang L., 2013, P 26 INT C NEUR INF, P980	48	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305039
C	Arora, R; Marinov, TV; Mohri, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Arora, Raman; Marinov, Teodor V.; Mohri, Mehryar			Bandits with Feedback Graphs and Switching Costs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DOMINATION	We study the adversarial multi-armed bandit problem where the learner is supplied with partial observations modeled by afeedback graphand where shifting to a new action incurs a fixed switching cost. We give two new algorithms for this problem in the informed setting. Our best algorithm achieves a pseudo-regret of (O) over bar(gamma(G)T-1/3(2/3)), where gamma(G) is the domination number of the feedback graph. This significantly improves upon the previous best result for the same problem, which was based on the independence number of G. We also present matching lower bounds for our result that we describe in detail. Finally, we give a new algorithm with improved policy regret bounds when partial counterfactual feedback is available.	[Arora, Raman; Marinov, Teodor V.] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21204 USA; [Mohri, Mehryar] Google Res, New York, NY 10012 USA; [Mohri, Mehryar] Courant Inst Math Sci, New York, NY 10012 USA	Johns Hopkins University; Google Incorporated	Arora, R (corresponding author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21204 USA.	arora@cs.jhu.edu; tmarino2@jhu.edu; mohri@google.com			NSF BIGDATA [IIS-1546482]; NSF [IIS-1618662, IIS-1838139, CCF-1535987]; Google Research Award	NSF BIGDATA; NSF(National Science Foundation (NSF)); Google Research Award(Google Incorporated)	This research was partly supported by NSF BIGDATA grants IIS-1546482 and NSF IIS-1838139, and by NSF CCF-1535987, NSF IIS-1618662, and a Google Research Award.	Agarwal A., 2016, ARXIV161206246; Alon N., 2013, ADV NEURAL INFORM PR, P1610; Alon N, 2017, SIAM J COMPUT, V46, P1785, DOI 10.1137/140989455; Alon Noga, 2015, JMLR WORKSHOP C P, V40; Arora R., 2012, P 29 INT C MACH LEAR, P1747; Audibert J.-Y., 2009, P 22 ANN C LEARN THE, P217; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; BOLLOBAS B, 1979, J GRAPH THEOR, V3, P241, DOI 10.1002/jgt.3190030306; Buccapatnam Swapna, 2014, ACM SIGMETRICS Performance Evaluation Review, V42, P289, DOI 10.1145/2591971.2591989; Caron S., 2012, UAI; Cesa-Bianchi Ofer Dekel, ADV NEURAL INFORM PR, P1160; CesaBianchi Nicolb, J ACM JACM, V44, P427; Chvatal V., 1979, Mathematics of Operations Research, V4, P233, DOI 10.1287/moor.4.3.233; Cohen A, 2016, PR MACH LEARN RES, V48; Cortes Corinna, 2018, 35 ICML; Dekel O, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P459, DOI 10.1145/2591796.2591868; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Geulen S., 2010, P 23 ANN C LEARN THE, P132; Goddard W, 2013, DISCRETE MATH, V313, P839, DOI 10.1016/j.disc.2012.11.031; Gyorgy A, 2014, IEEE T INFORM THEORY, V60, P2823, DOI 10.1109/TIT.2014.2307062; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Kocak Tomas, 2014, ADV NEURAL INFORM PR, P613; Liu Fang, 2018, 32 AAAI C ART INT; Mannor S., 2011, NIPS, P684; Rangi A., 2019, 22 INT C ART INT STA, P2435; Tossou Aristide, 2017, 31 AAAI C ART INT; Wu Y., 2015, ADV NEURAL INFORM PR, P1360; YUN DG, 2018, P ACM MEAS ANAL COMP, V2	29	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902007
C	Bai, Y; Xie, TY; Jiang, N; Wang, YX		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bai, Yu; Xie, Tengyang; Jiang, Nan; Wang, Yu-Xiang			Provably Efficient Q-Learning with Low Switching Cost	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We take initial steps in studying PAC-MDP algorithms with limited adaptivity, that is, algorithms that change its exploration policy as infrequently as possible during regret minimization. This is motivated by the difficulty of running fully adaptive algorithms in real-world applications (such as medical domains), and we propose to quantify adaptivity using the notion of local switching cost. Our main contribution, Q-Learning with UCB2 exploration, is a model-free algorithm for H-step episodic MDP that achieves sublinear regret whose local switching cost in K episodes is O(H(3)SA log K), and we provide a lower bound of Omega(HSA) on the local switching cost for any no-regret algorithm. Our algorithm can be naturally adapted to the concurrent setting [13], which yields nontrivial results that improve upon prior work in certain aspects.	[Bai, Yu] Stanford Univ, Stanford, CA 94305 USA; [Xie, Tengyang; Jiang, Nan] UIUC, Urbana, IL USA; [Wang, Yu-Xiang] UC Santa Barbara, Santa Barbara, CA USA	Stanford University; University of Illinois System; University of Illinois Urbana-Champaign; University of California System; University of California Santa Barbara	Bai, Y (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	yub@stanford.edu; tx10@illinois.edu; nanjiang@illinois.edu; yuxiangw@cs.ucsb.edu		Wang, Yu-Xiang/0000-0002-6403-212X	UCSB CS department; NSF-OAC [1934641]	UCSB CS department; NSF-OAC	The authors would like to thank Emma Brunskill, Ramtin Keramati, Andrea Zanette, and the staff of CS234 at Stanford for the valuable feedback at an earlier version of this work, and Chao Tao for the very insightful feedback and discussions on the concurrent Q-learning algorithm. YW was supported by a start-up grant from UCSB CS department, NSF-OAC 1934641 and a gift from AWS ML Research Award.	AGRAWAL S, 2017, ADV NEURAL INFORM PR, P1184; Almirall D, 2014, TRANSL BEHAV MED, V4, P260, DOI 10.1007/s13142-014-0265-0; Almirall D, 2012, STAT MED, V31, P1887, DOI 10.1002/sim.4512; Ashouri AH, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3197978; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Azar MG, 2017, PR MACH LEARN RES, V70; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Cesa-Bianchi N., 2013, P 26 INT C NEUR INF, P1160; Chow Y., 2018, ADV NEURAL INFORM PR, P8092; Dann Christopher, 2018, ARXIV181103056; Duchi J. C., 2018, P 31 C LEARNING THEO, P3065; Gao Z., 2019, ARXIV190401763; Guo Z., 2015, 29 AAAI C ART INT; Hanna J., 2017, P 34 INT C MACH LEAR, P1394; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Jin C., 2018, ADV NEURAL INFORM PR; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; Lei H, 2012, ANNU REV CLIN PSYCHO, V8, P21, DOI 10.1146/annurev-clinpsy-032511-143152; Mirhoseini A, 2017, PR MACH LEARN RES, V70; Nguyen P., 2019, P 2019 SIAM INT C DA, P549; Perchet V, 2016, ANN STAT, V44, P660, DOI 10.1214/15-AOS1381; Raccuglia P, 2016, NATURE, V533, P73, DOI 10.1038/nature17439; Theocharous G., 2015, 24 INT JOINT C ART I; Van Roy, 2013, ADV NEURAL INFORM PR, P3003; Yang Z., 2018, ARXIV180803196; Yu M., 2019, ADV NEURAL INFORM PR	26	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308007
C	Balsubramani, A; Dasgupta, S; Freund, Y; Moran, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Balsubramani, Akshay; Dasgupta, Sanjoy; Freund, Yoav; Moran, Shay			An adaptive nearest neighbor rule for classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CONVERGENCE; BALLS; RATES	We introduce a variant of the k-nearest neighbor classifier in which k is chosen adaptively for each query, rather than being supplied as a parameter. The choice of k depends on properties of each neighborhood, and therefore may significantly vary between different points. For example, the algorithm will use larger k for predicting the labels of points in noisy regions. We provide theory and experiments that demonstrate that the algorithm performs comparably to, and sometimes better than, k-NN with an optimal choice of k. In particular, we bound the convergence rate of our classifier in terms of a local quantity we call the "advantage", giving results that are both more general and more accurate than the smoothness-based bounds of earlier nearest neighbor work. Our analysis uses a variant of the uniform convergence theorem of Vapnik-Chervonenkis that is for empirical estimates of conditional probabilities and may be of independent interest.				abalsubr@stanford.edu; dasgupta@eng.ucsd.edu; yfreund@eng.ucsd.edu; shaym@princeton.edu		freund, yoav/0000-0002-3850-6184				[Anonymous], 1996, MNIST DATASET; [Anonymous], 2010, ADV NEURAL INFORM PR; [Anonymous], 2011, NOTMNIST DATASET; [Anonymous], 2018, MOUSE CELL ATLAS DAT; Audibert JY, 2007, ANN STAT, V35, P608, DOI 10.1214/009053606000001217; Boucheron S., 2005, ESAIM-PROBAB STAT, V9, P323, DOI [DOI 10.1051/PS:2005018, 2182250]; Cerou F., 2006, ESAIM-PROBAB STAT, V10, P340, DOI [10.1051/ps:2006014, DOI 10.1051/PS:2006014]; Chaudhuri Kamalika, 2014, ADV NEURAL INFORM PR, P3437; Chen G., 2018, FDN TRENDS MACHINE L; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; DEVROYE L, 1994, ANN STAT, V22, P1371, DOI 10.1214/aos/1176325633; Dong Wei, 2011, P 20 INT C WORLD WID, P577, DOI DOI 10.1145/1963405.1963487; Fix E., 1951, 2149004 USAF SCH AV; GYORFI L, 1981, IEEE T INFORM THEORY, V27, P362, DOI 10.1109/TIT.1981.1056344; Kpotufe S., 2011, NEURAL INFORM PROCES; KULKARNI SR, 1995, IEEE T INFORM THEORY, V41, P1028, DOI 10.1109/18.391248; Mammen E, 1999, ANN STAT, V27, P1808; Raab M, 1998, LECT NOTES COMPUT SC, V1518, P159; Schaum N, 2018, NATURE, V562, P367, DOI 10.1038/s41586-018-0590-4; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025	24	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307058
C	Basri, R; Jacobs, D; Kasten, Y; Kritchman, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Basri, Ronen; Jacobs, David; Kasten, Yoni; Kritchman, Shira			The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the relationship between the frequency of a function and the speed at which a neural network learns it. We build on recent results that show that the dynamics of overparameterized neural networks trained with gradient descent can be well approximated by a linear system. When normalized training data is uniformly distributed on a hypersphere, the eigenfunctions of this linear system are spherical harmonic functions. We derive the corresponding eigenvalues for each frequency after introducing a bias term in the model. This bias term had been omitted from the linear network model without significantly affecting previous theoretical results. However, we show theoretically and experimentally that a shallow neural network without bias cannot represent or learn simple, low frequency functions with odd frequencies. Our results lead to specific predictions of the time it will take a network to learn functions of varying frequency. These predictions match the empirical behavior of both shallow and deep networks.	[Basri, Ronen; Kasten, Yoni; Kritchman, Shira] Weizmann Inst Sci, Dept Comp Sci, Rehovot, Israel; [Jacobs, David] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA	Weizmann Institute of Science; University System of Maryland; University of Maryland College Park	Basri, R (corresponding author), Weizmann Inst Sci, Dept Comp Sci, Rehovot, Israel.				National Science Foundation [DMS1439786, IIS-1526234]	National Science Foundation(National Science Foundation (NSF))	The authors thank Adam Klivans, Boaz Nadler, and Uri Shaham for helpful discussions. This material is based upon work supported by the National Science Foundation under Grant No. DMS1439786 while the authors were in residence at the Institute for Computational and Experimental Research in Mathematics in Providence, RI, during the Computer Vision program. This research is supported by the National Science Foundation under grant no. IIS-1526234.	Allen-Zhu Zeyuan, 2018, ARXIV181012065; Arora Sanjeev, 2019, ARXIV190108584; Bach F., 2017, J MACH LEARN RES, V18, p1 629 681; Bietti Alberto, 2019, ARXIV190512173; Brutzkus A., 2018, 6 INT C LEARN REPR I; Chizat Lenaic, 2019, ADV NEURAL INFORM PR; Du Simon S, 2019, INT C LEARN REPR ICL; Eldan R., 2016, P 29 C LEARNING THEO, V49, P907; Farnia F., 2018, SPECTRAL APPROACH GE; Finn C, 2017, PR MACH LEARN RES, V70; Gallier Jean, 2009, PREPRINT; Ghorbani B., 2019, ARXIV190412191; Goodfellow I., 2016, DEEP LEARNING; Gunasekar Suriya, 2018, ADV NEURAL INFORM PR, P9461; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Ji Z., 2018, ARXIV180307300; Ji Z., 2018, PROC INT C LEARN REP; Li M, PMLR; Li Y., 2018, ADV NEURAL INFORM PR; Montufar G. F., 2014, ADV NEURAL INFORM PR, P2924; Rahaman N, 2018, ARXIV180608734; SHALEV-SHWARTZ S., 2017, ARXIV170600687; Soudry D, 2018, J MACH LEARN RES, V19; Telgarsky M.., 2016, C LEARNING THEORY, P1517; Xie B, 2017, PR MACH LEARN RES, V54, P1216; Xu Zhi-Qin John, 2019, CORRABS190106523; Xu Zhiqin John, 2018, CORRABS180804295; Zou D, 2018, ARXIV181108888	28	2	2	3	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304073
C	Blaes, S; Pogancic, MV; Zhu, JJ; Martius, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Blaes, Sebastian; Pogancic, Marin Vlastelica; Zhu, Jia-Jie; Martius, Georg			Control What You Can Intrinsically Motivated Task-Planning Agent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present a novel intrinsically motivated agent that learns how to control the environment in a sample efficient manner, that is with as few environment interactions as possible, by optimizing learning progress. It learns what can be controlled, how to allocate time and attention as well as the relations between objects using surprise-based motivation. The effectiveness of our method is demonstrated in a synthetic and robotic manipulation environment yielding considerably improved performance and smaller sample complexity compared to an intrinsically motivated, non-hierarchical and state-of-the-art hierarchical baseline. In a nutshell, our work combines several task-level planning agent structures (backtracking search on task-graph, probabilistic road-maps, allocation of search efforts) with intrinsic motivation to achieve learning from scratch.	[Blaes, Sebastian; Pogancic, Marin Vlastelica; Zhu, Jia-Jie; Martius, Georg] Max Planck Inst Intelligent Syst, Autonomous Learning Grp, Tubingen, Germany	Max Planck Society	Blaes, S (corresponding author), Max Planck Inst Intelligent Syst, Autonomous Learning Grp, Tubingen, Germany.	sebastian.blaes@tue.mpg.de; marin.vlastelica@tue.mpg.de; jzhu@tue.mpg.de; georg.martius@tue.mpg.de		Martius, Georg/0000-0002-8963-7627	European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant [798321]; International Max Planck Research School for Intelligent Systems (IMPRS-IS); German Federal Ministry of Education and Research (BMBF) through the Tibingen Al Center [FKZ: 01IS 18039B]	European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant; International Max Planck Research School for Intelligent Systems (IMPRS-IS); German Federal Ministry of Education and Research (BMBF) through the Tibingen Al Center(Federal Ministry of Education & Research (BMBF))	Jia-Jie Zhu is supported by funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 798321. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Sebastian Blaes and Marin Vlastelica Pogancic. We acknowledge the support from the German Federal Ministry of Education and Research (BMBF) through the Tibingen Al Center (FKZ: 01IS 18039B).	Baranes A, 2013, ROBOT AUTON SYST, V61, P49, DOI 10.1016/j.robot.2012.05.008; Baumann D, 2018, IEEE DECIS CONTR P, P943, DOI 10.1109/CDC.2018.8619335; Brockman G., 2016, OPENAI GYM; Burgess Christopher P, 2019, ARXIV190111390; Chentanez, 2005, ADV NEURAL INFORM PR, P1281, DOI DOI 10.21236/ADA440280; Colas C, 2019, PR MACH LEARN RES, V97; Florensa C, 2018, PR MACH LEARN RES, V80; Gopnik Alison, 2016, GARDENERAND CARPENTE; Gumbsch C, 2017, P 39 ANN M COGN SCI, P452; Haarnoja T, 2017, PR MACH LEARN RES, V70; Haarnoja T, 2018, PR MACH LEARN RES, V80; Heemels WPMH, 2012, IEEE DECIS CONTR P, P3270, DOI 10.1109/CDC.2012.6425820; Jaderberg Max, 2017, 5 INT C LEARN REPR I; Kaplan F, 2004, LECT NOTES ARTIF INT, V3139, P259; Kavraki LE, 1996, IEEE T ROBOTIC AUTOM, V12, P566, DOI 10.1109/70.508439; Klyubin AS, 2005, IEEE C EVOL COMPUTAT, P128; Kulkarni T. D., 2016, PROC 30 INT C NEURAL, P3675; Lavalle Steven M., 1998, TECHNICAL REPORT; Little DY, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00037; Nachum O., 2018, NEURIPS; Oudeyer, 2017, ARXIV170802190; Oudeyer PY, 2007, IEEE T EVOLUT COMPUT, V11, P265, DOI 10.1109/TEVC.2006.890271; Pathak D., 2017, INT C MACH LEARN ICM, V2017; Posa M, 2014, INT J ROBOT RES, V33, P69, DOI 10.1177/0278364913506757; Riedmiller M, 2018, PR MACH LEARN RES, V80; Santoro A., 2017, ADV NEURAL INFORM PR, P4967; Schmidhuber J., 1991, P INT C SIM AD BEH A, P222; Schmidhuber J, 2006, CONNECT SCI, V18, P173, DOI 10.1080/09540090600768658; Siciliano B., 2016, SPRINGER HDB ROBOTIC, DOI DOI 10.1007/978-3-540-30301-5; Sigaud Olivier, 2018, INT C LEARN ICLR 18; Sukhbaatar Sainbayar, 2018, INT C LEARN ICLR 201; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Wiese, 2017, PHILOS PREDICTIVE PR, DOI 10.15502/9783958573093; Yi S., 2011, P 4 C ART GEN INT AG; Zahedi K, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00801; Zambaldi V., 2018, ARXIV180601830	38	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904022
C	Blumenfeld, Y; Gilboa, D; Soudry, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Blumenfeld, Yaniv; Gilboa, Dar; Soudry, Daniel			A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Reducing the precision of weights and activation functions in neural network training, with minimal impact on performance, is essential for the deployment of these models in resource-constrained environments. We apply mean field techniques to networks with quantized activations in order to evaluate the degree to which quantization degrades signal propagation at initialization. We derive initialization schemes which maximize signal propagation in such networks, and suggest why this is helpful for generalization. Building on these results, we obtain a closed form implicit equation for L-max, the maximal trainable depth (and hence model capacity), given N, the number of quantization levels in the activation function. Solving this equation numerically, we obtain asymptotically: L-max proportional to N-1.82.	[Blumenfeld, Yaniv; Soudry, Daniel] Technion, Haifa, Israel; [Gilboa, Dar] Columbia Univ, New York, NY 10027 USA	Columbia University	Blumenfeld, Y (corresponding author), Technion, Haifa, Israel.	yanivblm6@gmail.com; dargilboa@gmail.com; daniel.soudry@gmail.com			Israel Science foundation [31/1031]; Taub Foundation; NSF NeuroNex Award [DBI-1707398]; Gatsby Charitable Foundation	Israel Science foundation(Israel Science Foundation); Taub Foundation; NSF NeuroNex Award; Gatsby Charitable Foundation	The work of DS was supported by the Israel Science foundation (grant No. 31/1031), the Taub Foundation and used a Titan Xp donated by the NVIDIA Corporation. The work of DG was supported by the NSF NeuroNex Award DBI-1707398 and the Gatsby Charitable Foundation. The work of DG and DS was done in part while the authors were visiting the Simons Institute for the Theory of Computing.	Anderson A. D., 2018, 2018 AIAA IEEE EL AI, P1; Arora S, 2019, ADV NEUR IN, V32; Banner R, 2018, ARXIV180511046; Courbariaux M., 2016, ADV NEURAL INFORM PR; Ganguli, 2017, ADV NEURAL INFORM PR, P4785; Gilboa D., 2019, ARXIV190108987; Gilmer Justin, 2016, INT C LEARN REPR; Glorot X., 2010, PROC MACH LEARN RES, P249; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; Hinton G., 2012, NEURAL NETWORKS MACH, V264, P1; Howard A.G., 2017, MOBILENETS EFFICIENT; Hubara I, 2018, J MACH LEARN RES, V18; Lee J., 2019, ARXIV190206720; Lee Jaehoon, 2017, ARXIV171100165; Li H, 2017, ADV NEUR IN, V30; Lyu HL, 2015, CHIN CONT DECIS CONF, P2885; Maass W, 1998, NEURAL COMPUT, V10, P1071, DOI 10.1162/089976698300017359; Mellempudi N, 2018, ARXIV180200930; Mishra A., 2018, ARXIV170901134; Miyashita Daisuke, 2016, ARXIV160301025; Poole B, 2016, ADV NEUR IN, V29; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Schoenholz S. S., 2019, ARXIV190208129; SIEGELMANN HT, 1991, APPL MATH LETT, V4, P77, DOI 10.1016/0893-9659(91)90080-F; Wang N., 2018, PROC C WORKSHOP NEUR, P1; Wu A., 2019, ICLR; Xiao LC, 2018, PR MACH LEARN RES, V80; Yin P, 2019, ICLR, P1; Zhang Chiyuan, 2016, ARXIV161103530; Zhang Han, 2019, ICLR; Zhao Y, 2017, ADVANCES IN ENERGY AND ENVIRONMENT RESEARCH, P345; Zhou YR, 2018, AAAI CONF ARTIF INTE, P4596	36	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307009
C	Burkholz, R; Dubatovka, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Burkholz, Rebekka; Dubatovka, Alina			Initialization of ReLUs for Dynamical Isometry	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep learning relies on good initialization schemes and hyperparameter choices prior to training a neural network. Random weight initializations induce random network ensembles, which give rise to the trainability, training speed, and sometimes also generalization ability of an instance. In addition, such ensembles provide theoretical insights into the space of candidate models of which one is selected during training. The results obtained so far rely on mean field approximations that assume infinite layer width and that study average squared signals. We derive the joint signal output distribution exactly, without mean field assumptions, for fully-connected networks with Gaussian weights and biases, and analyze deviations from the mean field results. For rectified linear units, we further discuss limitations of the standard initialization scheme, such as its lack of dynamical isometry, and propose a simple alternative that overcomes these by initial parameter sharing.	[Burkholz, Rebekka] Harvard TH Chan Sch Publ Hlth, Dept Biostat, 655 Huntington Ave, Boston, MA 02115 USA; [Dubatovka, Alina] Swiss Fed Inst Technol, Dept Comp Sci, Univ Str 6, CH-8092 Zurich, Switzerland	Harvard University; Harvard T.H. Chan School of Public Health; Swiss Federal Institutes of Technology Domain; ETH Zurich	Burkholz, R (corresponding author), Harvard TH Chan Sch Publ Hlth, Dept Biostat, 655 Huntington Ave, Boston, MA 02115 USA.	rburkholz@hsph.harvard.edu; alina.dubatovka@inf.ethz.ch		Dubatovka, Alina/0000-0002-2385-977X	Swiss Heart Failure Network [PHRT122/2018DRI14]; US National Cancer Institute [1R35CA220523]	Swiss Heart Failure Network; US National Cancer Institute(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Cancer Institute (NCI))	We would like to thank Joachim M. Buhmann and Alkis Gotovos for their valuable feedback on the manuscript and the reviewers for their insightful comments. This work was partially funded by the Swiss Heart Failure Network, PHRT122/2018DRI14 (J. M. Buhmann, PI). RB was supported by a grant from the US National Cancer Institute (1R35CA220523).	[Anonymous], 5 INT C LEARN REPR I; Cho Youngmin, 2009, P ADV NEUR INF PROC, V22, P342; Du Simon S., 2019, 7 INT C LEARNING REP, P1; Ganguli, 2017, ADV NEURAL INFORM PR, P4785; Ganguli S., 2014, INT C LEARN REPR; Glorot X., 2010, PROC MACH LEARN RES, P249; Hanin B., 2018, ADV NEURAL INFORM PR, P582; Hanin B, 2018, ADV NEUR IN, V31; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y., 2016, ICLR, DOI DOI 10.1109/WCNC.2016.7564824; Lee Jooyoung, 2018, INT C LEARN REPR; Neuschel T, 2014, RANDOM MATRICES-THEO, V3, DOI 10.1142/S2010326314500038; Perez-Cruz F, 2018, INT C ART INT STAT, V84, P1924; Poole B, 2016, ADV NEUR IN, V29; Pretorius Arnu, 2018, ADV NEURAL INFORM PR, V31, P5717; Raghu M, 2017, PR MACH LEARN RES, V70; Shang WL, 2016, PR MACH LEARN RES, V48; Wan L., 2013, P INT C MACHINE LEAR, P1058; Xiao LC, 2018, PR MACH LEARN RES, V80; Yang G., 2019, INT C LEARN REPR; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519	26	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302039
C	Carrara, N; Leurent, E; Laroche, R; Urvoy, T; Maillard, OA; Pietquin, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Carrara, Nicolas; Leurent, Edouard; Laroche, Romain; Urvoy, Tanguy; Maillard, Odalric-Ambrym; Pietquin, Olivier			Budgeted Reinforcement Learning in Continuous State Space	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A Budgeted Markov Decision Process (BMDP) is an extension of a Markov Decision Process to critical applications requiring safety constraints. It relies on a notion of risk implemented in the shape of a cost signal constrained to lie below an - adjustable - threshold. So far, BMDPs could only be solved in the case of finite state spaces with known dynamics. This work extends the state-of-the-art to continuous spaces environments and unknown dynamics. We show that the solution to a BMDP is a fixed point of a novel Budgeted Bellman Optimality operator. This observation allows us to introduce natural extensions of Deep Reinforcement Learning algorithms to address large-scale BMDPs. We validate our approach on two simulated applications: spoken dialogue and autonomous driving(3).	[Carrara, Nicolas; Leurent, Edouard; Maillard, Odalric-Ambrym; Pietquin, Olivier] INRIA Lille, Nord Europe, SequeL Team, Lille, France; [Leurent, Edouard] Renault Grp, Boulogne, France; [Laroche, Romain] Microsoft Res, Montreal, PQ, Canada; [Urvoy, Tanguy] Orange Labs, Lannion, France; [Pietquin, Olivier] Google Res, Brain Team, Mountain View, CA USA; [Carrara, Nicolas; Leurent, Edouard; Pietquin, Olivier] Univ Lille, CNRS, Cent Lille, CRIStAL,INRIA,UMR 9189, Lille, France	Renault SA; Orange SA; Google Incorporated; Centre National de la Recherche Scientifique (CNRS); Inria; Universite de Lille - ISITE; Centrale Lille; Universite de Lille	Carrara, N (corresponding author), INRIA Lille, Nord Europe, SequeL Team, Lille, France.; Carrara, N (corresponding author), Univ Lille, CNRS, Cent Lille, CRIStAL,INRIA,UMR 9189, Lille, France.	nicolas.carrara@inria.fr; edouard.leurent@inria.fr; romain.laroche@microsoft.com; tanguy.urvoy@orange.com; odalric.maillard@inria.fr; pietquin@google.com			CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020; French Ministry of Higher Education and Research; INRIA; French Agence Nationale de la Recherche (ANR)	CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020; French Ministry of Higher Education and Research; INRIA; French Agence Nationale de la Recherche (ANR)(French National Research Agency (ANR))	This work has been supported by CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020, the French Ministry of Higher Education and Research, INRIA, and the French Agence Nationale de la Recherche (ANR). We thank Guillaume Gautier, Fabrice Clerot, and Xuedong Shang for the helpful discussions and valuable insights.	Abe Naoki, 2010, SPECIAL INTEREST GRO; Achiam J, 2017, PR MACH LEARN RES, V70; Altman E., 1999, STOCH MODEL SER; Beutler F. J., 1985, J MATH ANAL APPL; Boutilier Craig, 2016, UNCERTAINTY ARTIFICI; Carrara N, 2018, SAFE TRANSFER LEARNI; Chandramohan Senthilkumar, 2010, C INT SPEECH COMM AS; Chow Y., 2015, P 28 INT C NEUR INF, P1522; Chow Yinlam, 2018, J MACHINE LEARNING R; Dann C, 2019, PR MACH LEARN RES, V97; Ernst D, 2005, J MACH LEARN RES, V6, P503; Garcia J, 2015, J MACH LEARN RES, V16, P1437; Geibel Peter, 2005, J ARTIFICIAL INTELLI; Iyengar GN, 2005, MATH OPER RES, V30, P257, DOI 10.1287/moor.1040.0129; Khouzaimi Hatim, 2015, SPECIAL INTEREST GRO; Laroche Romain, 2019, P INT C MACH LEARN I; Le HM, 2019, PR MACH LEARN RES, V97; Leurent E., 2018, ENV AUTONOMOUS DRIVI; Li Lihong, 2009, C INT SPEECH COMM AS; Liu Chunming, 2014, IEEE T SYSTEMS MAN C; Luenberger D.G., 2013, INVESTMENT SCI; Mausser H., 2003, P IEEE C COMP INT FI; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nilim A, 2005, OPER RES, V53, P780, DOI 10.1287/opre.1050.0216; Petrik M., 2016, ADV NEURAL INFORM PR, V29, P2298; Pietquin O., 2011, ACM T SPEECH LANGUAG, V7, P7, DOI DOI 10.1145/1966407.1966412; Poupart Pascal, 2015, P ASS ADV ART INT C; Riedmiller Martin, 2005, LECT NOTES COMPUTER; Roijers Diederik M., 2013, J ARTIFICIAL INTELLI; Tamar Aviv, 2012, P INT C MACH LEARN I; Thomas Philip, 2015, P INT C MACH LEARN I; Undurti Aditya, 2011, TECH REPORT; Wiesemann Wolfram, 2013, MATH OPERATIONS RES	33	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900084
C	Chakrabarti, S; Huang, YM; Li, TY; Feizi, S; Wu, XD		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chakrabarti, Shouvanik; Huang, Yiming; Li, Tongyang; Feizi, Soheil; Wu, Xiaodi			Quantum Wasserstein GANs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The study of quantum generative models is well motivated, not only because of its importance in quantum machine learning and quantum chemistry but also because of the perspective of its implementation on near-term quantum machines. Inspired by previous studies on the adversarial training of classical and quantum generative models, we propose the first design of quantum Wasserstein Generative Adversarial Networks (WGANs), which has been shown to improve the robustness and the scalability of the adversarial training of quantum generative models even on noisy quantum hardware. Specifically, we propose a definition of the Wasserstein semimetric between quantum data, which inherits a few key theoretical merits of its classical counterpart. We also demonstrate how to turn the quantum Wasserstein semimetric into a concrete design of quantum WGANs that can be efficiently implemented on quantum machines. Our numerical study, via classical simulation of quantum systems, shows the more robust and scalable numerical performance of our quantum WGANs over other quantum GAN proposals. As a surprising application, our quantum WGAN has been used to generate a 3-qubit quantum circuit of similar to 50 gates that well approximates a 3-qubit 1-d Hamiltonian simulation circuit that requires over 10k gates using standard techniques.	[Chakrabarti, Shouvanik; Huang, Yiming; Li, Tongyang; Wu, Xiaodi] Univ Maryland, Joint Ctr Quantum Informat & Comp Sci, College Pk, MD 20742 USA; [Chakrabarti, Shouvanik; Li, Tongyang; Feizi, Soheil; Wu, Xiaodi] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA; [Huang, Yiming] Univ Elect Sci & Technol China, Sch Informat & Software Engn, Hefei, Peoples R China	University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park; University of Electronic Science & Technology of China	Chakrabarti, S (corresponding author), Univ Maryland, Joint Ctr Quantum Informat & Comp Sci, College Pk, MD 20742 USA.; Chakrabarti, S (corresponding author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.	shouv@cs.umd.edu; yiminghwang@gmail.com; tongyang@cs.umd.edu; sfeizi@cs.umd.edu; xwu@cs.umd.edu			U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Quantum Algorithms Teams program; Capital One; NSF [CDSE-1854532, CCF-1755800, CCF-1816695]; IBM Ph.D. Fellowship; NSF QISE-NET Triplet Award [DMR-1747426]	U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Quantum Algorithms Teams program(United States Department of Energy (DOE)); Capital One; NSF(National Science Foundation (NSF)); IBM Ph.D. Fellowship(International Business Machines (IBM)); NSF QISE-NET Triplet Award	We thank anonymous reviewers for many constructive comments and Yuan Su for helpful discussions about the reference [11]. SC, TL, and XW received support from the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Quantum Algorithms Teams program. SF received support from Capital One and NSF CDS&E-1854532. TL also received support from an IBM Ph.D. Fellowship and an NSF QISE-NET Triplet Award (DMR-1747426). XW also received support from NSF CCF-1755800 and CCF-1816695.	Arjovsky M., 2017, ARXIV170107875; Balaji Yogesh, 2019, ARXIV190200415; Benamou JD, 2000, NUMER MATH, V84, P375, DOI 10.1007/s002119900117; Biamonte J, 2017, NATURE, V549, P195, DOI 10.1038/nature23474; Caglioti Emanuele, 2019, ARXIV190801829; Carlen EA, 2017, J FUNCT ANAL, V273, P1810, DOI 10.1016/j.jfa.2017.05.003; Carlen EA, 2014, COMMUN MATH PHYS, V331, P887, DOI 10.1007/s00220-014-2124-8; Chen Y., 2018, EMERGING APPL CONTRO, P139; Chen YX, 2018, IEEE T AUTOMAT CONTR, V63, P2612, DOI 10.1109/TAC.2017.2767707; Childs AM, 2018, P NATL ACAD SCI USA, V115, P9456, DOI 10.1073/pnas.1801723115; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Dallaire-Demers PL, 2018, PHYS REV A, V98, DOI 10.1103/PhysRevA.98.012324; Farhi E, 2014, QUANTUM APPROXIMATE; GOLDEN S, 1965, PHYS REV, V137, P1127; Golse F, 2016, COMMUN MATH PHYS, V343, P165, DOI 10.1007/s00220-015-2485-7; Goodfellow I.J., 2014, PROC NEURAL INFORM P, DOI DOI 10.1002/RCS.128; Gulrajani I, 2017, ADV NEURAL INFORM PR, V2017, P5767; Guo X., 2017, ARXIV PREPRINT ARXIV; Harrow A. W., 2013, ARXIV13086595; KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694; Mescheder L, 2018, PR MACH LEARN RES, V80; Moll N, 2018, QUANTUM SCI TECHNOL, V3, DOI 10.1088/2058-9565/aab822; Nielsen M. A, 2005, ARXIVQUANTPH0505030; Nielsen M.A., 2000, QUANTUM COMPUTATION; Ning LP, 2015, IEEE T AUTOMAT CONTR, V60, P373, DOI 10.1109/TAC.2014.2350171; Peruzzo A, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5213; Petzka H., 2017, ARXIV170908894; Peyre G., 2016, ARXIV161208731; Preskill J, 2018, QUANTUM-AUSTRIA, V2, DOI 10.22331/q-2018-08-06-79; Romero J., 2019, ARXIV190100848; Sanjabi Maziar, 2018, ADV NEURAL INFORM PR, V31, P7091; Schuld M, 2019, PHYS REV A, V99, DOI 10.1103/PhysRevA.99.032331; Seguy Vivien, 2017, ARXIV PREPRINT ARXIV; Situ H., 2018, ARXIV180701235; THOMPSON CJ, 1965, J MATH PHYS, V6, P1812, DOI 10.1063/1.1704727; Trotter H. F., 1959, P AM MATH SOC, V10, P545, DOI [10.1090/S0002-9939-1959-0108732-6, DOI 10.1090/S0002-9939-1959-0108732-6]; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Yu Nengkun, 2018, QUANTUM EARTH MOVERS; Zeng J., 2018, ARXIV180803425; Zhu Daiwei, 2019, COMMUNICATION   0219; Zoufal Christa, 2019, ARXIV190400043	49	2	2	2	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306075
C	Chaudhuri, K; Imola, J; Machanavajjhala, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chaudhuri, Kamalika; Imola, Jacob; Machanavajjhala, Ashwin			Capacity Bounded Differential Privacy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Differential privacy has emerged as the gold standard for measuring the risk posed by an algorithm's output to the privacy of a single individual in a dataset. It is defined as the worst-case distance between the output distributions of an algorithm that is run on inputs that differ by a single person. In this work, we present a novel relaxation of differential privacy, capacity bounded differential privacy, where the adversary that distinguishes the output distributions is assumed to be capacity-bounded - i.e. bounded not in computational power, but in terms of the function class from which their attack algorithm is drawn. We model adversaries of this form using restricted f-divergences between probability distributions, and study properties of the definition and algorithms that satisfy them. Our results demonstrate that these definitions possess a number of interesting properties enjoyed by differential privacy and some of its existing relaxations; additionally, common mechanisms such as the Laplace and Gaussian mechanisms enjoy better privacy guarantees for the same added noise under these definitions.	[Chaudhuri, Kamalika; Imola, Jacob] Univ Calif San Diego, La Jolla, CA 92093 USA; [Machanavajjhala, Ashwin] Duke Univ, Durham, NC 27706 USA	University of California System; University of California San Diego; Duke University	Chaudhuri, K; Imola, J (corresponding author), Univ Calif San Diego, La Jolla, CA 92093 USA.	kamalika@cs.ucsd.edu; jimola@eng.ucsd.edu; ashwin@cs.duke.edu			ONR [N00014-161-261]; UC Lab Fees under LFR [18-548554]; NSF [1253942, 1804829]; National Science Foundation [1253327, 1408982]; DARPA; SPAWAR [N66001-15-C-4067]	ONR(Office of Naval Research); UC Lab Fees under LFR; NSF(National Science Foundation (NSF)); National Science Foundation(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); SPAWAR	We thank Shuang Liu and Arnab Kar for early discussions. KC and JI thank ONR under N00014-161-261, UC Lab Fees under LFR 18-548554 and NSF under 1253942 and 1804829 for support. AM was supported by the National Science Foundation under grants 1253327, 1408982; and by DARPA and SPAWAR under contract N66001-15-C-4067.	Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Arora Sanjeev, 2017, INT C MACH LEARN; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Csiszar I., 1967, STUD SCI MATH HUNG, V2, P299; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork Cynthia, 2014, ABS14112664 CORR; Dwork Cynthia, 2014, FDN TRENDS THEOR COM; Farnia Farzan, 2018, ABS181011740 CORR; Feldman Vitaly, 2017, C LEARN THEOR JUN, V65, P728; Groce A, 2011, LECT NOTES COMPUT SC, V6597, P417, DOI 10.1007/978-3-642-19571-6_25; He Xi, 2013, ABS13123913 CORR; Head ML, 2015, PLOS BIOL, V13, DOI 10.1371/journal.pbio.1002106; Kifer D., 2012, P 31 ACM SIGMOD SIGA, P77, DOI DOI 10.1145/2213556.2213571; KIFER D, 2012, J MACH LEARN RES, V23, P1; Kifer D, 2014, ACM T DATABASE SYST, V39, DOI 10.1145/2514689; Kifer D, 2010, PODS 2010: PROCEEDINGS OF THE TWENTY-NINTH ACM SIGMOD-SIGACT-SIGART SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P147, DOI 10.1145/1807085.1807106; Li C, 2010, PODS 2010: PROCEEDINGS OF THE TWENTY-NINTH ACM SIGMOD-SIGACT-SIGART SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P123, DOI 10.1145/1807085.1807104; Li Chao, 2012, ABS12023399 CORR; Liu S., 2018, ARXIV180904542; Liu S, 2017, I C COMM SOFTW NET, P454; McKenna Ryan, 2018, ABS180803537 CORR; Mironov I, 2009, LECT NOTES COMPUT SC, V5677, P126, DOI 10.1007/978-3-642-03356-8_8; Mironov Ilya, 2017, CSF SYNPOSIUM; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Russo D., 2015, CONTROLLING BIAS VIA; Sarwate AD, 2013, IEEE SIGNAL PROC MAG, V30, P86, DOI 10.1109/MSP.2013.2259911; Smith Adam D., 2017, ABS170600820 CORR; Sriperumbudur Bharath K., 2009, ABS09012698 CORR; Wang YX, 2016, LECT NOTES COMPUT SC, V9867, P121, DOI 10.1007/978-3-319-45381-1_10; Wang Yu-Xiang, 2016, AVERAGE KL PRIVACY I	32	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303046
C	Chen, BD; Xu, YC; Shrivastava, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Beidi; Xu, Yingchen; Shrivastava, Anshumali			Fast and Accurate Stochastic Gradient Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Stochastic Gradient Descent or SGD is the most popular optimization algorithm for large-scale problems. SGD estimates the gradient by uniform sampling with sample size one. There have been several other works that suggest faster epoch-wise convergence by using weighted non-uniform sampling for better gradient estimates. Unfortunately, the per-iteration cost of maintaining this adaptive distribution for gradient estimation is more than calculating the full gradient itself, which we call the chicken-and-the-egg loop. As a result, the false impression of faster convergence in iterations, in reality, leads to slower convergence in time. In this paper, we break this barrier by providing the first demonstration of a scheme, Locality sensitive hashing (LSH) sampled Stochastic Gradient Descent (LGD), which leads to superior gradient estimation while keeping the sampling cost per iteration similar to that of the uniform sampling. Such an algorithm is possible due to the sampling view of LSH, which came to light recently. As a consequence of superior and fast estimation, we reduce the running time of all existing gradient descent algorithms, that relies on gradient estimates including Adam, Ada-grad, etc. We demonstrate the effectiveness of our proposal with experiments on linear models as well as the non-linear BERT, which is a recent popular deep learning based language representation model.	[Chen, Beidi; Xu, Yingchen; Shrivastava, Anshumali] Rice Univ, Houston, TX 77005 USA	Rice University	Chen, BD (corresponding author), Rice Univ, Houston, TX 77005 USA.	beidi.chen@rice.edu; yx26@rice.edu; anshumali@rice.edu			Amazon Research Award; ONR BRC grant for Randomized Numerical Linear Algebra;  [NSF-1652131];  [NSF-BIGDATA 1838177];  [AFOSRYIPFA9550-18-1-0152]	Amazon Research Award; ONR BRC grant for Randomized Numerical Linear Algebra; ; ; 	We thank the reviewers for their valuable comments. We also thank Ben Benjamin Coleman for the helpful discussions. The work was supported by NSF-1652131, NSF-BIGDATA 1838177, AFOSRYIPFA9550-18-1-0152, Amazon Research Award, and ONR BRC grant for Randomized Numerical Linear Algebra.	Abadi Martfn, 2015, TENSORFLOW LARGE SCA, P9; Arnold BC, 2008, ECON STUD INEQUAL SO, V5, P119, DOI 10.1007/978-0-387-72796-7_7; Bengio, 2015, ARXIV151106481; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Charikar M.S., 2002, P 34 ANN ACM S THEOR, V34, P380, DOI DOI 10.1145/509907.509965; Charikar M, 2017, ANN IEEE SYMP FOUND, P1032, DOI 10.1109/FOCS.2017.99; Chen B., 2019, ARXIV190303129; Chen B, 2018, ANN APPL STAT, V12, P1039, DOI 10.1214/18-AOAS1163; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Dhillon I. S., 2011, ADV NEURAL INFORM PR, P2160; Dolan William B, 2005, P INT WORKSH PAR; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Gopal S, 2016, PR MACH LEARN RES, V48; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Lichman M, 2013, UCI MACHINE LEARNING; Namkoong H, 2017, PR MACH LEARN RES, V70; Needell D, 2016, MATH PROGRAM, V155, P549, DOI 10.1007/s10107-015-0864-7; Paszke A., 2017, AUTOMATIC DIFFERENTI; Perekrestenko Dmytro, 2017, ARXIV170302518; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Shamir O., 2013, INT C MACH LEARN, P71; Spring R., 2016, ARXIV160208194; Spring Ryan, 2017, ARXIV170305160; Torres-Sospedra J, 2014, INT C INDOOR POSIT, P261, DOI 10.1109/IPIN.2014.7275492; Wang Alex, 2018, ARXIV180407461, DOI DOI 10.18653/V1/W18-5446; Xu W., 2011, ARXIV11072490; Yang J, 2016, ACSR ADV COMPUT, V67, P558; Zhao P., 2014, ARXIV14053080	31	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904005
C	Chen, DX; Jacob, L; Mairal, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Dexiong; Jacob, Laurent; Mairal, Julien			Recurrent Kernel Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PROTEIN HOMOLOGY DETECTION; CLASSIFICATION	Substring kernels are classical tools for representing biological sequences or text. However, when large amounts of annotated data are available, models that allow end-to-end training such as neural networks are often preferred. Links between recurrent neural networks (RNNs) and substring kernels have recently been drawn, by formally showing that RNNs with specific activation functions were points in a reproducing kernel Hilbert space (RKHS). In this paper, we revisit this link by generalizing convolutional kernel networks-originally related to a relaxation of the mismatch kernel-to model gaps in sequences. It results in a new type of recurrent neural network which can be trained end-to-end with backpropagation, or without supervision by using kernel approximation techniques. We experimentally show that our approach is well suited to biological sequences, where it outperforms existing methods for protein classification tasks.	[Chen, Dexiong; Mairal, Julien] Univ Grenoble Alpes, CNRS, INRIA, Grenoble INP,LJK, F-38000 Grenoble, France; [Jacob, Laurent] Univ Lyon 1, Univ Lyon, CNRS, Lab Biometrie & Biol Evolut,UMR 5558, F-69000 Lyon, France	Centre National de la Recherche Scientifique (CNRS); Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); Inria; Centre National de la Recherche Scientifique (CNRS); CNRS - Institute of Ecology & Environment (INEE); UDICE-French Research Universities; Universite Claude Bernard Lyon 1; VetAgro Sup	Chen, DX (corresponding author), Univ Grenoble Alpes, CNRS, INRIA, Grenoble INP,LJK, F-38000 Grenoble, France.	dexiong.chen@inria.fr; laurent.jacob@univ-lyon1.fr; julien.mairal@inria.fr	Mairal, Julien/AAL-5611-2021		ANR [ANR-17-CE23-0011-01]; ERC [714381]; ANR 31A MIAI@GrenobleAlpes	ANR(French National Research Agency (ANR)); ERC(European Research Council (ERC)European Commission); ANR 31A MIAI@GrenobleAlpes(French National Research Agency (ANR))	We thank the anonymous reviewers for their insightful comments and suggestions. This work has been supported by the grants from ANR (FAST-BIG project ANR-17-CE23-0011-01), by the ERC grant number 714381 (SOLARIS), and ANR 31A MIAI@GrenobleAlpes.	1000 Genomes Project Consortium, 2015, Nature, V526, P68, DOI 10.1038/nature15393; Alipanahi B, 2015, NAT BIOTECHNOL, V33, P831, DOI 10.1038/nbt.3300; Angermueller C, 2016, MOL SYST BIOL, V12, DOI 10.15252/msb.20156651; Arita M, 2002, BIOINFORMATICS, V18, pS27, DOI 10.1093/bioinformatics/18.suppl_2.S27; Chen DX, 2019, BIOINFORMATICS, V35, P3294, DOI 10.1093/bioinformatics/btz094; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Cho Youngmin, 2009, P ADV NEUR INF PROC, V22, P342; Flagel L, 2019, MOL BIOL EVOL, V36, P220, DOI 10.1093/molbev/msy224; Giles M.B., 2008, ADV AUTOMATIC DIFFER, V64, P35, DOI [10.1007/978-3-540-68942-34, DOI 10.1007/978-3-540-68942-34]; Handstad T, 2007, BMC BIOINFORMATICS, V8, DOI 10.1186/1471-2105-8-23; Haussler D, 1999, CONVOLUTION KERNELS, P95; HENIKOFF S, 1992, P NATL ACAD SCI USA, V89, P10915, DOI 10.1073/pnas.89.22.10915; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter S, 2007, BIOINFORMATICS, V23, P1728, DOI 10.1093/bioinformatics/btm247; Hou J, 2018, BIOINFORMATICS, V34, P1295, DOI 10.1093/bioinformatics/btx780; Jaakkola T. S., 1999, C INT SYST MOL BIOL; Kalchbrenner N., 2014, ASS COMPUTATIONAL LI; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lei T, 2017, PR MACH LEARN RES, V70; Leslie C., 2001, BIOC 2002 P PAC S, V575, P564; Leslie CS, 2004, BIOINFORMATICS, V20, P467, DOI 10.1093/bioinformatics/btg431; Liao L, 2003, J COMPUT BIOL, V10, P857, DOI 10.1089/106652703322756113; Lodhi H, 2002, J MACH LEARN RES, V2, P419, DOI 10.1162/153244302760200687; Mairal J, 2016, ADV NEUR IN, V29; Marschall T, 2018, BRIEF BIOINFORM, V19, P118, DOI 10.1093/bib/bbw089; Merity Stephen, 2018, INT C LEARN REPR ICL; Morrow A., 2017, ARXIV170600125; Murray N., 2014, P IEEE C COMP VIS PA; Peters J, 2017, ADAPT COMPUT MACH LE; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rangwala H, 2005, BIOINFORMATICS, V21, P4239, DOI 10.1093/bioinformatics/bti687; Saigo H, 2004, BIOINFORMATICS, V20, P1682, DOI 10.1093/bioinformatics/bth141; Schu┬lkopf B., 2004, KERNEL METHODS COMPU; Topol Eric J, 2019, NATURE MED, V25; VERT JP, 2004, KERNEL METHODS COMPU, P131; Watkins C., 1999, ADV NEURAL INFORM PR; Williams CKI, 2001, ADV NEUR IN, V13, P682; Zhang K., 2008, INT C MACH LEARN ICM; Zhang YC, 2017, PR MACH LEARN RES, V70; Zhou J, 2015, NAT METHODS, V12, P931, DOI [10.1038/nmeth.3547, 10.1038/NMETH.3547]	41	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905014
C	Chen, FH; Ji, RR; Ji, JY; Sun, XS; Zhang, BC; Ge, XR; Wu, YJ; Huang, FY; Wang, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Fuhai; Ji, Rongrong; Ji, Jiayi; Sun, Xiaoshuai; Zhang, Baochang; Ge, Xuri; Wu, Yongjian; Huang, Feiyue; Wang, Yan			Variational Structured Semantic Inference for Diverse Image Captioning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Despite the exciting progress in image captioning, generating diverse captions for a given image remains as an open problem. Existing methods typically apply generative models such as Variational Auto-Encoder to diversify the captions, which however neglect two key factors of diverse expression, i.e., the lexical diversity and the syntactic diversity. To model these two inherent diversities in image captioning, we propose a Variational Structured Semantic Inferring model (termed VSSI-cap) executed in a novel structured encoder-inferer-decoder schema. VSSI-cap mainly innovates in a novel structure, i.e., Variational Multi-modal Inferring tree (termed VarMI-tree). In particular, conditioned on the visual-textual features from the encoder, the VarMI-tree models the lexical and syntactic diversities by inferring their latent variables (with variations) in an approximate posterior inference guided by a visual semantic prior. Then, a reconstruction loss and the posterior-prior KL-divergence are jointly estimated to optimize the VSSI-cap model. Finally, diverse captions are generated upon the visual features and the latent variables from this structured encoder-inferer-decoder model. Experiments on the benchmark dataset show that the proposed VSSI-cap achieves significant improvements over the state-of-the-arts.	[Chen, Fuhai; Ji, Rongrong; Ji, Jiayi; Sun, Xiaoshuai; Ge, Xuri] Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Xiamen, Fujian, Peoples R China; [Ji, Rongrong] Peng Cheng Lab, Shenzhen, Guangdong, Peoples R China; [Zhang, Baochang] Beihang Univ, Beijing, Peoples R China; [Wu, Yongjian; Huang, Feiyue] Tencent Youtu Lab, Beijing, Peoples R China; [Wang, Yan] Pinterest, San Francisco, CA USA	Xiamen University; Peng Cheng Laboratory; Beihang University; Tencent	Ji, RR (corresponding author), Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Xiamen, Fujian, Peoples R China.; Ji, RR (corresponding author), Peng Cheng Lab, Shenzhen, Guangdong, Peoples R China.	cfh3c.xmu@gmail.com; rrji@xmu.edu.cn; jjyxmu@gmail.com; xssun@xmu.edu.cn; bczhang@buaa.edu.cn; xurigexmu@gmail.com; littlekenwu@tencent.com; garyhuang@tencent.com; yanw@pinterest.com			National Key RD Program [2017YFC0113000, 2016YFB1001503]; Nature Science Foundation of China [U1705262, 61772443, 61572410, 61702136]; Post Doctoral Innovative Talent Support Program [BX201600094]; China Post-Doctoral Science Foundation [2017M612134]; Scientific Research Project of National Language Committee of China [YB135-49]; Nature Science Foundation of Fujian Province, China [2017J01125, 2018J01106]	National Key RD Program; Nature Science Foundation of China(National Natural Science Foundation of China (NSFC)); Post Doctoral Innovative Talent Support Program; China Post-Doctoral Science Foundation(China Postdoctoral Science Foundation); Scientific Research Project of National Language Committee of China; Nature Science Foundation of Fujian Province, China(Natural Science Foundation of Fujian Province)	This work is supported by the National Key R&D Program (No.2017YFC0113000, and No.2016YFB1001503), Nature Science Foundation of China (No.U1705262, No.61772443, No.61572410, and No.61702136), Post Doctoral Innovative Talent Support Program under Grant BX201600094, China Post-Doctoral Science Foundation under Grant 2017M612134, Scientific Research Project of National Language Committee of China (Grant No. YB135-49), and Nature Science Foundation of Fujian Province, China (No. 2017J01125 and No. 2018J01106).	Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bird S., 2006, P ACL WORKSH EFF TOO, P69; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Chen FH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P46, DOI 10.1145/3123266.3123275; Chen FH, 2018, PROC CVPR IEEE, P1345, DOI 10.1109/CVPR.2018.00146; Chen X, 2015, CORR, V1504, P325; Dai B, 2018, ADV NEUR IN, V31; Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323; Devlin J., 2015, ARXIV150504467; Gan C, 2017, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2017.108; Hadley PA, 2018, AM J SPEECH-LANG PAT, V27, P553, DOI 10.1044/2017_AJSLP-17-0098; Hershey JR, 2007, INT CONF ACOUST SPEE, P317, DOI 10.1109/icassp.2007.366913; Jain U, 2017, PROC CVPR IEEE, P5415, DOI 10.1109/CVPR.2017.575; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lester Paul Martin, 2006, SYNTACTIC THEORY VIS, V3, P1; Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345; Mathews A, 2018, PROC CVPR IEEE, P8591, DOI 10.1109/CVPR.2018.00896; Meecham M., 2005, CONT LINGUISTICS, P537; Park C. C., P IEEE C COMP VIS PA, P895; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Shetty R, 2017, IEEE I CONF COMP VIS, P4155, DOI 10.1109/ICCV.2017.445; Socher R., 2011, P 28 INT C INT C MAC, P129; Sohn Kihyuk, 2015, ADV NEURAL INFORM PR, P3483, DOI DOI 10.5555/2969442.2969628; Vedantam R, 2017, PROC CVPR IEEE, P1070, DOI 10.1109/CVPR.2017.120; Vijayakumar Ashwin K, 2016, ARXIV161002424; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wang LW, 2017, ADV NEUR IN, V30; Wang Z., 2016, P 25 INT JOINT C ART, P2957; Wu X, 2018, PROC CVPR IEEE, P6829, DOI 10.1109/CVPR.2018.00714; Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Zhang K, 2018, J ENERGY CHEM, V27, P1, DOI 10.1016/j.jechem.2017.11.027	34	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301087
C	Chen, VS; Wu, S; Weng, ZZ; Ratner, A; Re, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Vincent S.; Wu, Sen; Weng, Zhenzhen; Ratner, Alexander; Re, Christopher			Slice-based Learning: A Programming Model for Residual Learning in Critical Data Slices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In real-world machine learning applications, data subsets correspond to especially critical outcomes: vulnerable cyclist detections are safety-critical in an autonomous driving task, and "question" sentences might be important to a dialogue agent's language understanding for product purposes. While machine learning models can achieve high quality performance on coarse-grained metrics like F1-score and overall accuracy, they may underperform on critical subsets-we define these as slices, the key abstraction in our approach. To address slice-level performance, practitioners often train separate "expert" models on slice subsets or use multi-task hard parameter sharing. We propose Slice-based Learning, a new programming model in which the slicing function (SF), a programming interface, specifies critical data subsets for which the model should commit additional capacity. Any model can leverage SFs to learn slice expert representations, which are combined with an attention mechanism to make slice-aware predictions. We show that our approach maintains a parameter-efficient representation while improving over baselines by up to 19.0 F1 on slices and 4.6 F1 overall on datasets spanning language understanding (e.g. SuperGLUE), computer vision, and production-scale industrial systems.	[Chen, Vincent S.; Wu, Sen; Weng, Zhenzhen; Ratner, Alexander; Re, Christopher] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Chen, VS (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	vincentsc@cs.stanford.edu; senwu@stanford.edu; zzweng@stanford.edu; ajratner@stanford.edu; chrismre@cs.stanford.edu			DARPA [FA87501720095, FA86501827865, FA86501827882]; NIH [U54EB020405]; NSF [CCF1763315, CCF1563078]; ONR [N000141712266]; Moore Foundation; Xilinx; Intel; Microsoft; Toshiba; Accenture; Okawa Foundation; American Family Insurance; NXP; LETI-CEA; NEC; TSMC; ARM; Hitachi; BASF; Ericsson; Qualcomm; Analog Devices; Google Cloud; Swiss Re	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); Moore Foundation(Gordon and Betty Moore Foundation); Xilinx; Intel(Intel Corporation); Microsoft(Microsoft); Toshiba; Accenture; Okawa Foundation; American Family Insurance; NXP; LETI-CEA; NEC; TSMC; ARM; Hitachi; BASF(BASF); Ericsson(Ericsson); Qualcomm; Analog Devices; Google Cloud(Google Incorporated); Swiss Re	We would like to thank Braden Hancock, Feng Niu, and Charles Srisuwananukorn for many helpful discussions, tests, and collaborations throughout the development of slicing. We gratefully acknowledge the support of DARPA under Nos. FA87501720095 (D3M), FA86501827865 (SDH), FA86501827882 (ASED), NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity) and CCF1563078 (Volume to Velocity), ONR under No. N000141712266 (Unifying Weak Supervision), the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, and American Family Insurance, Google Cloud, Swiss Re, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, SAP, VMWare, and Infosys. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.	[Anonymous], 2016, ARXIV160506391; Bach SH, 2019, INT CONF MANAGE DATA, P362, DOI 10.1145/3299869.3314036; Bach Stephen H, 2018, ARXIV181200417; Bar-Haim R., 2006, P 2 PASCAL CHALL WOR, V6, P6; Bentivogli Luisa, 2009, TAC; Berend D., 2014, P 27 INT C NEUR INF, P3446; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Cheng H., 2015, P 2015 C EMP METH NA, P737; Corney D., 2016, NEWSIR ECIR, P42; Dagan Ido, 2005, MACH LEARN CHALL WOR, P177, DOI DOI 10.1007/11736790; Dalvi N, 2013, P 22 INT C WORLD WID, P285, DOI DOI 10.1145/2488388.2488414; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Giampiccolo Danilo, 2007, P ACL PASCAL WORKSH, P1; Honnibal M., 2015, P 2015 C EMP METH NA, P1373, DOI [DOI 10.18653/V1/D15-1162, 10.18653/v1/D15-1162.8]; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Karpathy Andrej, 2019, BUILDING SOFTWARE 2; Kim Najoung, 2019, ARXIV190411544; Mann GS, 2010, J MACH LEARN RES, V11, P955; Masalov A, 2018, IEEE INT VEH SYM, P2143; MATTHEWS BW, 1975, BIOCHIM BIOPHYS ACTA, V405, P442, DOI 10.1016/0005-2795(75)90109-9; Mintz M., 2009, P ACL, P1003, DOI DOI 10.3115/1690219.1690287; Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433; Mitchell T. M., 2015, AAAI; Oakden-Rayner Luke, 2019, ARXIV190912475; Pilehvar Mohammad Taher, 2018, ARXIV180809121; R~e C., 2019, OVERTON DATA SYSTEM; Ratner A.J., 2018, VLDB; Ratner A, 2016, ADV NEUR IN, V29; Ratner Alexander, 2019, ROLE MASSIVELY MULTI; Rei M., 2017, ARXIV170407156; Roemmele M., 2011, 2011 AAAI SPRING S S; Ruder S., 2017, PREPRINT; Sagi O, 2018, WIRES DATA MIN KNOWL, V8, DOI 10.1002/widm.1249; Sigaud O., 2015, ARXIV151203201; Wang A., 2018, P 2018 EMNLP WORKSH; Wang A., 2019, ARXIV190500537; Warstadt Alex, 2018, ARXIV180512471; Wei CH, 2015, P 5 BIOCREATIVE CHAL, P154	42	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901007
C	Corona, R; Alaniz, S; Akata, Z		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Corona, Rodolfo; Alaniz, Stephan; Akata, Zeynep			Modeling Conceptual Understanding in Image Reference Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					An agent who interacts with a wide population of other agents needs to be aware that there may be variations in their understanding of the world. Furthermore, the machinery which they use to perceive may be inherently different, as is the case between humans and machines. In this work, we present both an image reference game between a speaker and a population of listeners where reasoning about the concepts other agents can comprehend is necessary and a model formulation with this capability. We focus on reasoning about the conceptual understanding of others, as well as adapting to novel gameplay partners and dealing with differences in perceptual machinery. Our experiments on three benchmark image/attribute datasets suggest that our learner indeed encodes information directly pertaining to the understanding of other agents, and that leveraging this information is crucial for maximizing gameplay performance.	[Corona, Rodolfo] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Alaniz, Stephan] Max Planck Inst Informat, Saarbrucken, Germany; [Akata, Zeynep] Univ Tubingen, Tubingen, Germany; [Corona, Rodolfo; Alaniz, Stephan; Akata, Zeynep] Univ Amsterdam, Amsterdam, Netherlands	University of California System; University of California Berkeley; Max Planck Society; Eberhard Karls University of Tubingen; University of Amsterdam	Corona, R (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	rcorona@berkeley.edu; salaniz@mpi-inf.mpg.de; zeynep.akata@uni-tuebingen.de			ERC under the Horizon 2020 program [853489]; DFG-EXC [390727645, 2064/1]; DARPA XAI program; Fulbright U.S. Student Program	ERC under the Horizon 2020 program; DFG-EXC; DARPA XAI program; Fulbright U.S. Student Program	This work has received funding from the ERC under the Horizon 2020 program (grant agreement No. 853489), DFG-EXC-Nummer 2064/1-Projektnummer 390727645 and DARPA XAI program. R. Corona was supported in part by the DYFulbright U.S. Student Program.	Akata Z., 2013, P IEEE C COMP VIS PA; [Anonymous], 2018, PAMI; Baker CL, 2017, NAT HUM BEHAV, V1, DOI 10.1038/s41562-017-0064; Bengio Samy, 1992, OPTIMIZATION SYNAPTI; Butterfield J., 2009, INT J SOC ROBOT, V1; Chandrasekaran A., 2017, ARXIV PREPRINT ARXIV; Choi Edward, 2018, ARXIV180402341; Cogswell M., 2019, CORR; Das Abhishek, 2017, ICCV; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Evtimova K, 2018, INT C LEARN REPR; Finn C, 2017, PR MACH LEARN RES, V70; Foerster JN, 2016, ADV NEUR IN, V29; Freitas A, 2014, ACM SIGKDD EXPLORATI, V15, P1, DOI DOI 10.1145/2594473.2594475; Gilpin LH, 2018, PR INT CONF DATA SC, P80, DOI 10.1109/DSAA.2018.00018; Havrylov S., 2017, ADV NEURAL INFORM PR; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jorge E., 2016, ARXIV161103218; Kottur Satwik, 2017, P C EMP METH NAT LAN; Lakkaraju Himabindu, 2016, KDD; Lazaridou A., 2017, P 5 INT C LEARN REPR; Lazaridou Angeliki, 2018, 6 INT C LEARN REPR I; Letham B, 2015, ANN APPL STAT, V9, P1350, DOI 10.1214/15-AOAS848; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Meili M., 2003, LEARNING THEORY KERN; Mordatch I, 2018, AAAI CONF ARTIF INTE, P1495; Nakahashi R., 2016, 13 AAAI C ART INT; Park Dong Huk, 2018, P IEEE C COMP VIS PA; PATTERSON G, 2014, INT J COMPUTER VISIO, V108; Rabinowitz NC, 2018, PR MACH LEARN RES, V80; Rudin C, 2019, NAT MACH INTELL, V1, P206, DOI 10.1038/s42256-019-0048-x; Shu T, 2019, INT C LEARN REPR; Shu T., 2018, CORR; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Tenenbaum, 2011, P 32 ANN C COGN SCI, V33, P2469, DOI DOI 10.1093/BRAIN/AWW287; Wah Catherine, 2011, CALTECH UCSD BIRDS 2; Warnier M., 2012, IEEE ROM 21 IEEE INT, P948, DOI DOI 10.1109/ROMAN.2012.6343872; Wu JL, 2019, BLACKBOXNLP WORKSHOP ON ANALYZING AND INTERPRETING NEURAL NETWORKS FOR NLP AT ACL 2019, P103	41	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904077
C	Degenne, R; Koolen, WM; Menard, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Degenne, Remy; Koolen, Wouter M.; Menard, Pierre			Non-Asymptotic Pure Exploration by Solving Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Pure exploration (aka active testing) is the fundamental task of sequentially gathering information to answer a query about a stochastic environment. Good algorithms make few mistakes and take few samples. Lower bounds (for multi-armed bandit models with arms in an exponential family) reveal that the sample complexity is determined by the solution to an optimisation problem. The existing state of the art algorithms achieve asymptotic optimality by solving a plug-in estimate of that optimisation problem at each step. We interpret the optimisation problem as an unknown game, and propose sampling rules based on iterative strategies to estimate and converge to its saddle point. We apply no-regret learners to obtain the first finite confidence guarantees that are adapted to the exponential family and which apply to any pure exploration query and bandit structure. Moreover, our algorithms only use a best response oracle instead of fully solving the optimisation problem.	[Degenne, Remy; Koolen, Wouter M.] Ctr Wiskunde & Informat, Sci Pk 123, NL-1098 XG Amsterdam, Netherlands; [Menard, Pierre] Inria Lille, 40 Ave Halley, F-59650 Villeneuve Dascq, France		Degenne, R (corresponding author), Ctr Wiskunde & Informat, Sci Pk 123, NL-1098 XG Amsterdam, Netherlands.	remy.degenne@cwi.nl; wmkoolen@cwi.nl; menardprr@gmail.com			INRIA Associate Team 6PAC	INRIA Associate Team 6PAC	We are grateful to Zakaria Mhammedi and Emilie Kaufmann for multiple generous discussions. Travel funding was provided by INRIA Associate Team <SUP>6</SUP>PAC. The experiments were carried out on the Dutch national e-infrastructure with the support of SURF Cooperative.	Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Berthet Quentin, 2017, ADV NEURAL INFORM PR, P2222; Bubeck S, 2011, THEOR COMPUT SCI, V412, P1832, DOI 10.1016/j.tcs.2010.12.059; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chen L., 2017, P 30 COLT, V65, P535; Chen S., 2014, ADV NEURAL INFORM PR; Combes R, 2014, PR MACH LEARN RES, V32; de Rooij S, 2014, J MACH LEARN RES, V15, P1281; Degenne R., 2019, ADV NEURAL INFORM PR; Even-Dar E., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P255; Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738; Garivier A., 2017, ARXIV171104454; Garivier A., 2016, C LEARN THEOR, P998; Garivier A, 2013, 2013 IEEE INFORMATION THEORY WORKSHOP (ITW); Kalyanakrishnan S., 2012, P ICML; Kalyanakrishnan Shivaram, 2010, P 27 INT C MACHINE L; Katariya S, 2017, PR MACH LEARN RES, V54, P392; Kaufmann E, 2018, PREPRINT; Kaufmann E, 2016, J MACH LEARN RES, V17; Kaufmann Emilie, 2018, P 32 C NEUR INF PROC, V31, P6333; Lattimore Tor, 2019, BANDIT ALGORITHMS; Locatelli A, 2016, PR MACH LEARN RES, V48; Magureanu S., 2014, P C LEARN THEOR COLT, P975; Menard P., 2019, ARXIV190508165; Rakhlin S., 2013, ADV NEURAL INFORM PR, P3066; Simchowitz M., 2017, C LEARN THEOR, V65, P1794; Teraoka K, 2014, IEICE T INF SYST, VE97D, P392, DOI 10.1587/transinf.E97.D.392; Wang J.-K., 2018, 31 ADV NEUR INF PROC, P3828; Zhou YC, 2017, PR MACH LEARN RES, V70	29	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906018
C	Dennis, DK; Acar, DAE; Mandikal, V; Sadasivan, VS; Simhadri, HV; Saligrama, V; Jain, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dennis, Don Kurian; Acar, Durmus Alp Emre; Mandikal, Vikram; Sadasivan, Vinu Sankar; Simhadri, Harsha Vardhan; Saligrama, Venkatesh; Jain, Prateek			Shallow RNNs: A Method for Accurate Time-series Classification on Tiny Devices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recurrent Neural Networks (RNNs) capture long dependencies and context, and hence are the key component of typical sequential data based tasks. However, the sequential nature of RNNs dictates a large inference cost for long sequences even if the hardware supports parallelization. To induce long-term dependencies, and yet admit parallelization, we introduce novel shallow RNNs. In this architecture, the first layer splits the input sequence and runs several independent RNNs. The second layer consumes the output of the first layer using a second RNN thus capturing long dependencies. We provide theoretical justification for our architecture under weak assumptions that we verify on real-world benchmarks. Furthermore, we show that for time-series classification, our technique leads to substantially improved inference time over standard RNNs without compromising accuracy. For example, we can deploy audio-keyword classification on tiny Cortex M4 devices (100MHz processor, 256KB RAM, no DSP available) which was not possible using standard RNN models. Similarly, using ShaRNN in the popular Listen-Attend-Spell (LAS) architecture for phoneme classification [4], we can reduce the lag in phoneme classification by 10-12x while maintaining state-of-the-art accuracy.	[Dennis, Don Kurian] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Acar, Durmus Alp Emre; Saligrama, Venkatesh] Boston Univ, Boston, MA 02215 USA; [Mandikal, Vikram] Univ Texas Austin, Austin, TX 78712 USA; [Sadasivan, Vinu Sankar] IIT Gandhinagar, Ahmadabad, Gujarat, India; [Dennis, Don Kurian; Mandikal, Vikram; Sadasivan, Vinu Sankar; Simhadri, Harsha Vardhan; Jain, Prateek] Microsoft Res India, Bengaluru, India	Carnegie Mellon University; Boston University; University of Texas System; University of Texas Austin; Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Gandhinagar	Dennis, DK (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.							Bengio Yoshua, 2013, 2013 IEEE INT C AC S; Bradbury J., 2016, ARXIV161101576; Campos V., 2018, ICLR, P1; Chan W., 2016, 2016 IEEE INT C AC S; Chang SY, 2017, ADV NEUR IN, V30; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Collins J., 2016, ARXIV PREPRINT ARXIV; Dauphin YN, 2017, PR MACH LEARN RES, V70; Dennis D. K., EDGEML MACHINE LEARN; Dennis Don Kurian, 2018, NEURIPS; Escur Janna, 2018, THESIS; Garofolo J.S., 1993, DARPA TIMIT ACOUSTIC; Gehring J., 2017, P ICML; Graves A., 2013, P 2013 IEEE INT C AC, P6645, DOI [10.1109/ICASSP.2013.6638947, DOI 10.1109/ICASSP.2013.6638947]; Hermans M., 2013, P ADV NEUR INF PROC, V26, P190; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jaeger Herbert, 2007, NEURAL NETWORKS OFFI, V20; Kingma D.P, P 3 INT C LEARNING R; Kumar Rajath, 2018, P INT 2018; Kusupati A, 2018, ADV NEUR IN, V31; Latr6 Benoit, 2011, WIRELESS NETWORKS; Lee S, 2014, INT CONF UBIQ FUTUR, P140, DOI 10.1109/ICUFN.2014.6876768; Miller J, 2018, ARXIV180510369; Oord A.V.D., 2016, SSW; Patil Shishir G., 2019, TECHNICAL REPORT; Sainath TN, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Sutskever I., 2015, NEURAL GPUS LEARN AL; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vogel Adam P., 2015, Frontiers in Bioengineering and Biotechnology, V3, P98, DOI 10.3389/fbioe.2015.00098; Yang Allen Y, 2008, 2008 IEEE COMP SOC C; Zhang J, 2018, PR MACH LEARN RES, V80	32	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904054
C	Detlefsen, NS; Hauberg, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Detlefsen, Nicki S.; Hauberg, Soren			Explicit Disentanglement of Appearance and Perspective in Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INDEPENDENT COMPONENT ANALYSIS	Disentangled representation learning finds compact, independent and easy-to-interpret factors of the data. Learning such has been shown to require an inductive bias, which we explicitly encode in a generative model of images. Specifically, we propose a model with two latent spaces: one that represents spatial transformations of the input data, and another that represents the transformed data. We find that the latter naturally captures the intrinsic appearance of the data. To realize the generative model, we propose a Variationally Inferred Transformational Autoencoder (VITAE) that incorporates a spatial transformer into a variational autoencoder. We show how to perform inference in the model efficiently by carefully designing the encoders and restricting the transformation class to be diffeomorphic. Empirically, our model separates the visual style from digit type on MNIST, separates shape and pose in images of human bodies and facial features from facial shape on CelebA.	[Detlefsen, Nicki S.; Hauberg, Soren] Tech Univ Denmark, Sect Cognit Syst, Lyngby, Denmark	Technical University of Denmark	Detlefsen, NS (corresponding author), Tech Univ Denmark, Sect Cognit Syst, Lyngby, Denmark.	nsde@dtu.dk; sohau@dtu.dk	Hauberg, Soren/L-2104-2016	Hauberg, Soren/0000-0001-7223-877X	European Research Council (ERC) under the European Union [757360]; VILLUM FONDEN [15334]; NVIDIA Corporation	European Research Council (ERC) under the European Union(European Research Council (ERC)); VILLUM FONDEN(Villum Fonden); NVIDIA Corporation	This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement no 757360). NSD and SH were supported in part by a research grant (15334) from VILLUM FONDEN. We gratefully acknowledge the support of NVIDIA Corporation with the donation of GPU hardware used for this research.	Annunziata R., 2018, ABS180704050 CORR; [Anonymous], 2015, ABS150602025 CORR; Baird HS., 1992, STRUCTURED DOCUMENT, P546, DOI [10.1007/978-3-642-77281-8_26, DOI 10.1007/978-3-642-77281-8_26]; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Bengio Y., 2012, CORR; Burda Yuri, 2015, ABS150900519 CORR; Chen R. T. Q., 2018, ISOLATING SOURCES DI; Chen X., 2016, ARXIV160603657, P2172; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; Detlefsen N. S., 2018, IEEE C COMP VIS PATT; DUISTERMAAT JJ, 2000, UNIVERSITEX, P1; Eastwood C., 2019, FRAMEWORK QUANTITATI; Eslami S. M. A., 2016, ATTEND INFER REPEAT, DOI [10.1038/nature14236, DOI 10.1038/NATURE14236]; Freifeld O., 2015, ICCV; Goodfellow I., 2015, ADV NEURAL INFORM PR, P2672; Hauberg S., 2016, P 19 INT C ART INT S, V41; Higgins I., 2017, ICLR; Higgins I., 2018, ARXIV PREPRINT ARXIV; Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5; Hyvarinen A., 2018, ARXIV180508651; Kendall David G, 1989, STAT SCI, P6, DOI DOI 10.1214/SS/1177012582; Kingma D.P., 2013, INT C LEARN REPR; Kingma D. P., 2016, ADV NEURAL INF PROCE; Kulkarni T. D., 2015, ABS150303167 CORR; Kumar Abhishek, 2017, VARIATIONAL INFERENC; Lake B. M., 2016, ABS160400289 CORR; Learned-Miller EG, 2006, IEEE T PATTERN ANAL, V28, P236, DOI 10.1109/TPAMI.2006.34; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Lewicki MS, 2002, NAT NEUROSCI, V5, P356, DOI 10.1038/nn831; Lin C., 2016, ABS161203897 CORR; Lin C.-H., 2018, ST GAN SPATIAL TRANS; Liu Ziwei, 2015, P INT C COMP VIS ICC; Locatello F., 2019, P 36 INT C MACHINE L; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Lorenz D., 2019, P COMP VIS PATT REC; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Rezende DJ, 2014, STOCHASTIC BACKPROPA; Shakunaga T., 2001, COMP VIS PATT REC 20, V1, pI; Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349; Thompson DW, 1917, GROWTH FORM; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P2315, DOI 10.1098/rspb.1998.0577; Xing X., 2019, P COMP VIS PATT REC	46	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301006
C	Dezfouli, A; Ashtiani, H; Ghattas, O; Nock, R; Dayan, P; Ong, CS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dezfouli, Amir; Ashtiani, Hassan; Ghattas, Omar; Nock, Richard; Dayan, Peter; Ong, Cheng Soon			Disentangled behavioral representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INDIVIDUAL-DIFFERENCES; MODELS	Individual characteristics in human decision-making are often quantified by fitting a parametric cognitive model to subjects' behavior and then studying differences between them in the associated parameter space. However, these models often fit behavior more poorly than recurrent neural networks (RNNs), which are more flexible and make fewer assumptions about the underlying decision-making processes. Unfortunately, the parameter and latent activity spaces of RNNs are generally high-dimensional and uninterpretable, making it hard to use them to study individual differences. Here, we show how to benefit from the flexibility of RNNs while representing individual differences in a low-dimensional and interpretable space. To achieve this, we propose a novel end-to-end learning framework in which an encoder is trained to map the behavior of subjects into a low-dimensional latent space. These low-dimensional representations are used to generate the parameters of individual RNNs corresponding to the decision-making process of each subject. We introduce terms into the loss function that ensure that the latent dimensions are informative and disentangled, i.e., encouraged to have distinct effects on behavior. This allows them to align with separate facets of individual differences. We illustrate the performance of our framework on synthetic data as well as a dataset including the behavior of patients with psychiatric disorders.	[Dezfouli, Amir; Ghattas, Omar; Nock, Richard; Ong, Cheng Soon] CSIRO, Data61, Canberra, ACT, Australia; [Ashtiani, Hassan] McMaster Univ, Hamilton, ON, Canada; [Ghattas, Omar] Univ Chicago, Chicago, IL 60637 USA; [Nock, Richard; Ong, Cheng Soon] Australian Natl Univ, Canberra, ACT, Australia; [Nock, Richard] Univ Sydney, Sydney, NSW, Australia; [Dayan, Peter] Max Planck Inst, Munich, Germany	Commonwealth Scientific & Industrial Research Organisation (CSIRO); McMaster University; University of Chicago; Australian National University; University of Sydney; Max Planck Society	Dezfouli, A (corresponding author), CSIRO, Data61, Canberra, ACT, Australia.; Dayan, P (corresponding author), Max Planck Inst, Munich, Germany.	amir.dezfouli@data61.csiro.au; dayan@tue.mpg.de						Abadi M., TENSORFLOW LARGE SCA; Bowman S. R., 2016, ARXIV, P10; Busemeyer JR, 2002, PSYCHOL ASSESSMENT, V14, P253, DOI 10.1037//1040-3590.14.3.253; CARROLL JB, 1979, ANNU REV PSYCHOL, V30, P603, DOI 10.1146/annurev.ps.30.020179.003131; Cho K., 2014, P 2014 C EMP METH NA, P1724; Daw N.D., 2011, DECISION MAKING AFFE; den Ouden HEM, 2013, NEURON, V80, P1090, DOI 10.1016/j.neuron.2013.08.030; Dezfouli A, 2018, ADV NEURAL INFORM PR; Dezfouli Amir, 2019, PLoS Comput Biol, V15, pe1006903, DOI 10.1371/journal.pcbi.1006903; Fox EB, 2011, ANN APPL STAT, V5, P1020, DOI 10.1214/10-AOAS395; Frank MJ, 2009, NAT NEUROSCI, V12, P1062, DOI 10.1038/nn.2342; Ha David, 2016, ARXIV160909106; Karaletsos T, 2018, ARXIV181000555; Kingma D.P, P 3 INT C LEARNING R; Lau B, 2005, J EXP ANAL BEHAV, V84, P555, DOI 10.1901/jeab.2005.110-04; Navarro DJ, 2006, J MATH PSYCHOL, V50, P101, DOI 10.1016/j.jmp.2005.11.006; Pandarinath C, 2018, NAT METHODS, V15, P805, DOI 10.1038/s41592-018-0109-9; Rumelhart D. E., 1985, TECHNICAL REPORT; SIEGELMANN HT, 1995, J COMPUT SYST SCI, V50, P132, DOI 10.1006/jcss.1995.1013; Song HK, 2017, ELIFE, V6, DOI 10.7554/eLife.26646; Tolstikhin Ilya, 2017, ARXIV171101558; Wang J.X., 2016, ARXIV161105763; Watkins CJCH., 1989, THESIS; Wood SN, 2011, J R STAT SOC B, V73, P3, DOI 10.1111/j.1467-9868.2010.00749.x; Yang GR, 2019, NAT NEUROSCI, V22, P297, DOI 10.1038/s41593-018-0310-2; Yechiam E, 2005, PSYCHOL SCI, V16, P973, DOI 10.1111/j.1467-9280.2005.01646.x	27	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302027
C	Doersch, C; Zisserman, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Doersch, Carl; Zisserman, Andrew			Sim2real transfer learning for 3D human pose estimation: motion to the rescue	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PERCEPTION; SHAPE	Synthetic visual data can provide practically infinite diversity and rich labels, while avoiding ethical issues with privacy and bias. However, for many tasks, current models trained on synthetic data generalize poorly to real data. The task of 3D human pose estimation is a particularly interesting example of this sim2real problem, because learning-based approaches perform reasonably well given real training data, yet labeled 3D poses are extremely difficult to obtain in the wild, limiting scalability. In this paper, we show that standard neural-network approaches, which perform poorly when trained on synthetic RGB images, can perform well when the data is pre-processed to extract cues about the person's motion, notably as optical flow and the motion of 2D keypoints. Therefore, our results suggest that motion can be a simple way to bridge a sim2real gap when video is available. We evaluate on the 3D Poses in the Wild dataset, the most challenging modern benchmark for 3D pose estimation, where we show full 3D mesh recovery that is on par with state-of-the-art methods trained on real 3D sequences, despite training only on synthetic humans from the SURREAL dataset.	[Doersch, Carl; Zisserman, Andrew] Deepmind, London, England; [Zisserman, Andrew] Univ Oxford, Dept Engn Sci, VGG, Oxford, England	University of Oxford	Doersch, C (corresponding author), Deepmind, London, England.							Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Akhter I., 2015, CVPR; Alldieck T, 2017, LECT NOTES COMPUT SC, V10496, P347, DOI 10.1007/978-3-319-66709-6_28; Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207; [Anonymous], 2018, CVPR; Arnab Anurag, 2019, CVPR; Balaei A. T., 2007, CVPR, P1; Bogo F, 2016, ECCV; Bousmalis K., 2017, P IEEE C COMP VIS PA, P3722; Bousmalis K, 2018, IEEE INT CONF ROBOT, P4243; Chen WZ, 2016, INT CONF 3D VISION, P479, DOI 10.1109/3DV.2016.58; Csurka Gabriela, 2017, ARXIV170205374; CUTTING JE, 1977, B PSYCHONOMIC SOC, V9, P353, DOI 10.3758/BF03337021; Dabral Rishabh, 2018, ECCV; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Du Y, 2016, LECT NOTES COMPUT SC, V9908, P20, DOI 10.1007/978-3-319-46493-0_2; GAIDON A, 2016, PROC CVPR IEEE, P4340, DOI DOI 10.1109/CVPR.2016.470; Ganin Y., 2016, JMLR, V17, P2096; Ghezelghieh MF, 2016, INT CONF 3D VISION, P685, DOI 10.1109/3DV.2016.75; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Guan P, 2009, IEEE I CONF COMP VIS, P1381, DOI 10.1109/iccv.2009.5459300; Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254; Hasler N, 2010, PROC CVPR IEEE, P1823, DOI 10.1109/CVPR.2010.5539853; Hinterstoisser S., 2019, ARXIV190209967; Hinterstoisser S., 2018, P EUR C COMP VIS ECC; Hossain K, 2018, ROUTL EXPL ENV STUD, P3; Huang YH, 2017, INT CONF 3D VISION, P421, DOI 10.1109/3DV.2017.00055; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Jakab T., 2018, ADV NEURAL INFORM PR, P4016; James S., 2018, ABS181207252 CORR; JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378; Jordan MI, 2015, SCIENCE, V349, P255, DOI 10.1126/science.aaa8415; Kanazawa A., 2019, CVPR; Kanazawa Angjoo, 2018, CVPR; Kay M, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3819, DOI 10.1145/2702123.2702520; Kim J, 2016, INT CONF UBIQ ROBOT, P903, DOI 10.1109/URAI.2016.7734006; KOZLOWSKI LT, 1977, PERCEPT PSYCHOPHYS, V21, P575, DOI 10.3758/BF03198740; Lassner C., 2017, P IEEE C COMP VIS PA, V2, P3; Li Z., 2019, ARXIV190411111; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Linder Kai O, 2018, ARXIV180809316, P2; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Mehta A, 2017, OBES SCI PRACT, V3, P3, DOI 10.1002/osp4.84; Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170; Mueller F, 2018, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2018.00013; Mueller F, 2017, IEEE INT CONF COMP V, P1284, DOI 10.1109/ICCVW.2017.82; Muller Matthias, 2018, ARXIV PREPRINT ARXIV; Okada R, 2008, LECT NOTES COMPUT SC, V5303, P434, DOI 10.1007/978-3-540-88688-4_32; Omran Mohamed, 2018, 3DV; Papandreou G., 2017, P IEEE C COMP VIS PA, P4903; Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059; Pavlakos G, 2017, PROC CVPR IEEE, P1263, DOI 10.1109/CVPR.2017.139; Peng X. B., 2018, ARXIV181003599; Pishchulin L, 2012, PROC CVPR IEEE, P3178, DOI 10.1109/CVPR.2012.6248052; Pishchulin L, 2011, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2011.5995574; Qiu W, 2016, THESIS; Rahmani H, 2016, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2016.167; Rahmani H, 2015, PROC CVPR IEEE, P2458, DOI 10.1109/CVPR.2015.7298860; Ramakrishna V, 2012, LECT NOTES COMPUT SC, V7575, P573, DOI 10.1007/978-3-642-33765-9_41; Ranjan A., 2017, P IEEE C COMP VIS PA, P4161; Ranjan Anurag, 2018, BRIT MACH VIS C; Rogez G., 2017, P CVPR; Rogez G, 2016, ADV NEUR IN, V29; Romero J, 2015, LECT NOTES COMPUT SC, V9358, P412, DOI 10.1007/978-3-319-24947-6_34; Sadeghi F., 2016, ROB SCI SYST C; Shotton J., 2011, IEEE COMP SOC C COMP, V2, P3; Shotton J., 2011, CVPR, V2; Shrivastava A., 2017, P IEEE C COMP VIS PA, P2107; Sigal L., 2008, NEURIPS, P1337; Sminchisescu Cristian, 2006, P INT C COMP VIS PAT, P1743; Sridhar S, 2015, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2015.7298941; Sun X., 2017, ICCV; Sun Xiao, 2018, ECCV; Taylor CJ, 2000, COMPUT VIS IMAGE UND, V80, P349, DOI 10.1006/cviu.2000.0878; Taylor J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925965; Tobin J, 2017, IEEE INT C INT ROBOT, P23; Tokmakov P, 2019, INT J COMPUT VISION, V127, P282, DOI 10.1007/s11263-018-1122-2; Tokmakov P, 2017, IEEE I CONF COMP VIS, P4491, DOI 10.1109/ICCV.2017.480; Tung H.-Y., 2017, ADV NEURAL INFORM PR, P5236; Valmadre J., 2010, ECCV; Varol G., 2017, CVPR; von Marcard Timo, 2018, ECCV; WANG C, 2014, CVPR, P2360; Wichmann F. A., 2018, INT C LEARN REPR; Wu JJ, 2016, LECT NOTES COMPUT SC, V9910, P365, DOI 10.1007/978-3-319-46466-4_22; Zanfir Andrei, 2018, CVPR; Zhang Xiuming, 2018, ARXIV180905491; Zhao Jieyu, 2017, P 2017 C EMP METH NA, P2941; Zhao R., 2017, PAMI; Zhou B., 2019, ARXIV190512887; Zhou TH, 2016, PROC CVPR IEEE, P117, DOI 10.1109/CVPR.2016.20; Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537; Zhou Xingyi, 2017, ICCV; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zimmermann C, 2017, IEEE I CONF COMP VIS, P4913, DOI 10.1109/ICCV.2017.525	98	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904057
C	Freeman, CD; Metz, L; Ha, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Freeman, C. Daniel; Metz, Luke; Ha, David			Learning to Predict Without Looking Ahead: World Models Without Forward Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Much of model-based reinforcement learning involves learning a model of an agent's world, and training an agent to leverage this model to perform a task more efficiently. While these models are demonstrably useful for agents, every naturally occurring model of the world of which we are aware-e.g., a brain-arose as the byproduct of competing evolutionary pressures for survival, not minimization of a supervised forward-predictive loss via gradient descent. That useful models can arise out of the messy and slow optimization process of evolution suggests that forward-predictive modeling can arise as a side-effect of optimization under the right circumstances. Crucially, this optimization process need not explicitly be a forward-predictive loss. In this work, we introduce a modification to traditional reinforcement learning which we call observational dropout, whereby we limit the agents ability to observe the real environment at each timestep. In doing so, we can coerce an agent into learning a world model to fill in the observation gaps during reinforcement learning. We show that the emerged world model, while not explicitly trained to predict the future, can help the agent learn key skills required to perform well in its environment.	[Freeman, C. Daniel; Metz, Luke; Ha, David] Google Brain, Mountain View, CA 94043 USA	Google Incorporated	Freeman, CD (corresponding author), Google Brain, Mountain View, CA 94043 USA.	cdfreeman@google.com; lmetz@google.com; hadavid@google.com						Allen J., 1983, P 8 INT JOINT C ARTI, VVolume 2, P741; Amos B, 2018, ADV NEUR IN, V31; [Anonymous], [No title captured]; [Anonymous], 2016, ARXIV PREPRINT ARXIV; Asadi K, 2018, PR MACH LEARN RES, V80; Banino A, 2018, NATURE, V557, P429, DOI 10.1038/s41586-018-0102-6; BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bongard J, 2006, SCIENCE, V314, P1118, DOI 10.1126/science.1133687; Buesing L., 2018, ARXIV181106272; Cueva C. J, 2018, ARXIV180307770; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Denton E, 2018, PR MACH LEARN RES, V80; Doll BB, 2012, CURR OPIN NEUROBIOL, V22, P1075, DOI 10.1016/j.conb.2012.08.003; Ebert Frederik, 2018, ARXIV181200568; Gal Y., 2016, P DAT EFF MACH LEARN, P1; Gigliotta O, 2011, THEOR BIOSCI, V130, P259, DOI 10.1007/s12064-011-0128-x; Goldber D. E., 1988, Machine Learning, V3, P95, DOI 10.1023/A:1022602019183; Ha D., 2018, ARXIV181003779; Ha D, 2018, ADV NEUR IN, V31; Ha David, 2017, EVOLVING STABLE STRA; Hafner D., 2018, ARXIV181104551; Hansen N, 2003, EVOL COMPUT, V11, P1, DOI 10.1162/106365603321828970; Hassabis Demis, 2018, ARXIV180203006; Higgins I., 2018, ARXIV PREPRINT ARXIV; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Holland J. H, 1992, ADAPTATION NATURAL A; Kaiser L., 2019, ARXIV190300374; Kalchbrenner N, 2017, PR MACH LEARN RES, V70; Kingma D. P., 2013, AUTO ENCODING VARIAT; Klimov Oleg, 2016, CARRACING V0; Kumar M., 2019, ARXIV190301434; Lehman J., 2018, ARXIV180303453; Lenz I, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; LeQuoc V, 2011, ARXIV11126209; Marques H, 2006, IEEE SYMP ART LIFE, P370; Mathieu Michael, 2015, ARXIV151105440; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nagabandi A, 2018, IEEE INT C INT ROBOT, P4606, DOI 10.1109/IROS.2018.8594193; Nagabandi A, 2018, IEEE INT CONF ROBOT, P7579; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Patraucean Viorica, 2015, ARXIV151106309; Pillonetto G, 2014, AUTOMATICA, V50, P657, DOI 10.1016/j.automatica.2014.01.001; Rechenberg I., 1973, EVOLUTIONSSTRATEGIE; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Risi S, 2019, PROCEEDINGS OF THE 2019 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'19), P456, DOI 10.1145/3321707.3321817; Salimans T., 2017, ARXIV170303864; Schmidhuber J., 2015, ARXIV151109249; Schmidhuber J., 1990, TECHNICAL REPORT; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Sermanet P, 2018, IEEE INT CONF ROBOT, P1134; Silver D, 2017, PR MACH LEARN RES, V70; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Such F. P., 2017, ARXIV171206567; Sutton Richard S, 1998, INTRO REINFORCEMENT, V135; Talvitie E, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P780; Tedrake R., 2009, UNDERACTUATED ROBOTI, V3; Thrun S., 1991, ADV NEURAL INFORM PR, P450; van den Oord Aaron, 2018, ARXIV180703748; Watter Manuel, 2015, ADV NEURAL INFORM PR, P2746; Werbos Paul J, 1987, P IEEE INT C SYST MA; Wierstra D, 2008, IEEE C EVOL COMPUTAT, P3381, DOI 10.1109/CEC.2008.4631255; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Zuo Xingdong, 2018, PYTORCH IMPLEMENTATI	69	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305038
C	Freivalds, K; Ozolins, E; Sostaks, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Freivalds, Karlis; Ozolins, Emils; Sostaks, Agris			Neural Shuffle-Exchange Networks - Sequence Processing in O(n log n) Time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A key requirement in sequence to sequence processing is the modeling of long range dependencies. To this end, a vast majority of the state-of-the-art models use attention mechanism which is of O(n(2)) complexity that leads to slow execution for long sequences. We introduce a new Shuffle-Exchange neural network model for sequence to sequence tasks which have O(log n) depth and O(n log n) total complexity. We show that this model is powerful enough to infer efficient algorithms for common algorithmic benchmarks including sorting, addition and multiplication. We evaluate our architecture on the challenging LAMBADA question answering dataset and compare it with the state-of-the-art models which use attention. Our model achieves competitive accuracy and scales to sequences with more than a hundred thousand of elements. We are confident that the proposed model has the potential for building more efficient architectures for processing large interrelated data in language modeling, music generation and other application domains.	[Freivalds, Karlis; Ozolins, Emils; Sostaks, Agris] Univ Latvia, Inst Math & Comp Sci, Raina Bulvaris 29, LV-1459 Riga, Latvia	University of Latvia	Freivalds, K (corresponding author), Univ Latvia, Inst Math & Comp Sci, Raina Bulvaris 29, LV-1459 Riga, Latvia.	Karlis.Freivalds@lumii.lv; Emils.Ozolins@lumii.lv; Agris.Sostaks@lumii.lv		Freivalds, Karlis/0000-0003-2684-559X	Latvian Council of Science [lzp-2018/1-0327]	Latvian Council of Science(Latvian Ministry of Education and Science)	We would like to thank Renars Liepin, s for the valuable discussion regarding the subject matter of the paper, the IMCS UL Scientific Cloud for the computing power and Leo Truksans for the technical support. We sincerely thank all the reviewers for their comments and suggestions. This research is funded by the Latvian Council of Science, project No. lzp-2018/1-0327.	AJTAI M, 1983, COMBINATORICA, V3, P1, DOI 10.1007/BF02579338; Al-Rfou Rami, 2018, ARXIV180804444; Andrychowicz M., 2016, LEARNING EFFICIENT A; Bengio Y., 2014, ARXIV14061078; BRENT RP, 1982, IEEE T COMPUT, V31, P260, DOI 10.1109/TC.1982.1675982; Child R., 2019, ARXIV190410509; Chu Z., 2017, P 15 C EUR CHAPT ASS, V2, P52; Clark Christopher, 2017, ARXIV171010723; Dai Zihang, 2019, ARXIV190102860, P2; Dally W. J., 2004, PRINCIPLES PRACTICES; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Freivalds K., 2018, ICML WORKSH NEUR MAC; Gehring J., 2017, P ICML; Graves A., 2014, ARXIV14105401; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Grefenstette E, 2015, ADV NEUR IN, V28; Guo Qipeng, 2019, P 2019 C N AM CHAPT, P1315, DOI DOI 10.18653/V1/N19-1133; Harvey D., 2019, INTEGER MULTIPLICATI; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang Cheng-Zhi Anna, 2019, ICLR; Joulin A, 2015, ADV NEUR IN, V28; Kaiser L., 2015, COMPUTER SCI; Kaiser L, 2016, ADV NEUR IN, V29; Kalchbrenner N., 2015, COMPUTER SCI; Kant N., 2018, ARXIV180202353; Kingma D.P, P 3 INT C LEARNING R; Knuth D. E., 1998, SORTING SEARCHING, V3; Kurach K, 2016, ERCIM NEWS, P25; Mikolov T, 2018, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2018), P52; Nowak A., 2018, INT C LEARN REPR; Oord A.V.D., 2016, SSW; Paperno D, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1525; Reed Scott E., 2016, 4 INT C LEARN REPR I; Seiferas J, 2009, ALGORITHMICA, V53, P374, DOI 10.1007/s00453-007-9025-6; Vaswani A., 2018, MT RES TRACK; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vinyals O., 2015, ADV NEURAL INFORM PR, P2692; Yu F., 2016, P ICLR 2016; Zaremba W, 2016, PR MACH LEARN RES, V48; Zaremba Wojciech, 2015, ARXIV150500521	41	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306061
C	Gan, JR; Guo, QY; Tran-Thanh, L; An, B; Wooldridge, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gan, Jiarui; Guo, Qingyu; Long Tran-Thanh; An, Bo; Wooldridge, Michael			Manipulating a Learning Defender and Ways to Counteract	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In Stackelberg security games when information about the attacker's payoffs is uncertain, algorithms have been proposed to learn the optimal defender commitment by interacting with the attacker and observing their best responses. In this paper, we show that, however, these algorithms can be easily manipulated if the attacker responds untruthfully. As a key finding, attacker manipulation normally leads to the defender learning a maximin strategy, which effectively renders the learning attempt meaningless as to compute a maximin strategy requires no additional information about the other player at all. We then apply a game-theoretic framework at a higher level to counteract such manipulation, in which the defender commits to a policy that specifies her strategy commitment according to the learned information. We provide a polynomial-time algorithm to compute the optimal such policy, and in addition, a heuristic approach that applies even when the attacker's payoff space is infinite or completely unknown. Empirical evaluation shows that our approaches can improve the defender's utility significantly as compared to the situation when attacker manipulation is ignored.	[Gan, Jiarui; Wooldridge, Michael] Univ Oxford, Oxford, England; [Guo, Qingyu; An, Bo] Nanyang Technol Univ, Singapore, Singapore; [Long Tran-Thanh] Univ Southampton, Southampton, Hants, England	University of Oxford; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; University of Southampton	Gan, JR (corresponding author), Univ Oxford, Oxford, England.	jiarui.gan@cs.ox.ac.uk; qguo005@e.ntu.edu.sg; l.tran-thanh@soton.ac.uk; boan@ntu.edu.sg; mjw@cs.ox.ac.uk			EPSRC International Doctoral Scholars Grant [EP/N509711/1]	EPSRC International Doctoral Scholars Grant(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	Jiarui Gan was supported by the EPSRC International Doctoral Scholars Grant EP/N509711/1.	[Anonymous], 2012, P 29 INT COFERENCE I; Balcan M.-F., 2015, P 16 ACM C EC COMP, P61, DOI DOI 10.1145/2764468.2764478; Biggio B., 2011, PROC ACML, P97; Blum A., 2014, ADV NEURAL INFORM PR, P1826; Chakraborty A, 2018, ABS181000069 CORR; Conitzer V., 2006, P 7 ACM C ELECT COMM, P82, DOI DOI 10.1145/1134707.1134717; Gan JR, 2019, ACM EC '19: PROCEEDINGS OF THE 2019 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P639, DOI 10.1145/3328526.3329629; Haghtalab Nika, 2016, IJCAI, V16, P308; Jain Manish, 2008, ACM SIGECOM EXCHANGE, V7, P10; Kiekintveld C., 2013, P 12 INT C AUT AG MU, P231; Letchford J, 2009, LECT NOTES COMPUT SC, V5814, P250, DOI 10.1007/978-3-642-04645-2_23; Liu Q, 2018, IEEE ACCESS, V6, P12103, DOI 10.1109/ACCESS.2018.2805680; MCKELVEY RD, 1995, GAME ECON BEHAV, V10, P6, DOI 10.1006/game.1995.1023; Nguyen TH, 2014, AAAI CONF ARTIF INTE, P756; Nudelman E., 2004, P 3 INT JOINT C AUT, V2, P880; Paruchuri P., 2008, P 7 INT JOINT C AUTO, V2, P895; Peng Binghui, 2019, P 33 AAAI C ART INT; Pita J., 2009, P 8 AAMAS, V1, P369; Roth A, 2016, ACM S THEORY COMPUT, P949, DOI 10.1145/2897518.2897579; Tambe M., 2011, SECURITY GAME THEORY; von Stengel Bernhard, 2004, LSECDAM200401; Xu HF, 2016, AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P104	22	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308031
C	Ge, DD; Wang, HY; Xiong, ZK; Ye, YY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ge, Dongdong; Wang, Haoyue; Xiong, Zikai; Ye, Yinyu			Interior-point Methods Strike Back: Solving the Wasserstein Barycenter Problem	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				WARM-START STRATEGIES; OPTIMAL TRANSPORT; IMPLEMENTATION; ALGORITHMS	Computing the Wasserstein barycenter of a set of probability measures under the optimal transport metric can quickly become prohibitive for traditional second-order algorithms, such as interior-point methods, as the support size of the measures increases. In this paper, we overcome the difficulty by developing a new adapted interior-point method that fully exploits the problem's special matrix structure to reduce the iteration complexity and speed up the Newton procedure. Different from regularization approaches, our method achieves a well-balanced tradeoff between accuracy and speed. A numerical comparison on various distributions with existing algorithms exhibits the computational advantages of our approach. Moreover, we demonstrate the practicality of our algorithm on image benchmark problems including MNIST and Fashion-MNIST.	[Ge, Dongdong] Shanghai Univ Finance & Econ, Res Inst Interdisciplinary Sci, Shanghai, Peoples R China; [Wang, Haoyue; Xiong, Zikai] Fudan Univ, Sch Math Sci, Shanghai, Peoples R China; [Ye, Yinyu] Stanford Univ, Dept Management Sci & Engn, Stanford, CA 94305 USA	Shanghai University of Finance & Economics; Fudan University; Stanford University	Wang, HY; Xiong, ZK (corresponding author), Fudan Univ, Sch Math Sci, Shanghai, Peoples R China.	ge.dongdong@mail.shufe.edu.cn; haoyuewang14@fudan.edu.cn; zkxiong16@fudan.edu.cn; yyye@stanford.edu						Agueh M, 2011, SIAM J MATH ANAL, V43, P904, DOI 10.1137/100805741; Altschuler J., 2017, ADV NEURAL INFORM PR, P1961; Alvarez-Esteban PC, 2016, J MATH ANAL APPL, V441, P744, DOI 10.1016/j.jmaa.2016.04.045; Amari S, 2019, NEURAL COMPUT, V31, P827, DOI 10.1162/neco_a_01178; Anderes E, 2016, MATH METHOD OPER RES, V84, P389, DOI 10.1007/s00186-016-0549-x; Arjovsky M, 2017, PR MACH LEARN RES, V70; Benamou JD, 2015, SIAM J SCI COMPUT, V37, pA1111, DOI 10.1137/141000439; Bertsimas D., 1997, INTRO LINEAR OPTIMIZ; Blanchet J., ARXIV181007717; Carlier G, 2015, ESAIM-MATH MODEL NUM, V49, P1621, DOI 10.1051/m2an/2015033; Claici S., ARXIV180205757; Courty N, 2017, ADV NEUR IN, V30; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi M, 2014, PR MACH LEARN RES, V32, P685; Dessein A, 2017, ARXIV171104366; Dvurechensky P., 2018, ADV NEURAL INFORM PR, V31, P10783; Dvurechensky P, 2018, PR MACH LEARN RES, V80; Frogner Charlie, 2015, ADV NEURAL INF PROCE, V2, P2053; Gall F. L., 2017, ARXIV170805622; Gao R., 2018, P 32 INT C NEUR INF, P7913; Gulrajani I, 2017, P NIPS 2017; Ho N., 2018, ARXIV181011911; Huang G., 2016, PROC NEURAL INF PROC, P4869; Inc. Gurobi Optimization, 2018, GUROBI OPTIMIZER REF; John E, 2008, COMPUT OPTIM APPL, V41, P151, DOI 10.1007/s10589-007-9096-y; Kusner MJ, 2015, PR MACH LEARN RES, V37, P957; Lacombe T, 2018, ADV NEURAL INFORM PR, V31, P9792; Lee Jaeho, 2018, ADV NEURAL INFORM PR, P2692; Lin T., ARXIV190106482; Mallasto A., 2017, ADV NEURAL INFORM PR, P5660; Mehrotra S, 1992, SIAM J OPTIMIZ, V2, P575, DOI 10.1137/0802028; MIZUNO S, 1993, MATH OPER RES, V18, P964, DOI 10.1287/moor.18.4.964; Muzellec Boris, 2018, ADV NEURAL INFORM PR, P10237; Nguyen X, 2016, BERNOULLI, V22, P1535, DOI 10.3150/15-BEJ703; Ho N, 2017, PR MACH LEARN RES, V70; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Peyre G., ARXIV180300567; Peyre G, 2016, PR MACH LEARN RES, V48; Rabin J, 2012, LECT NOTES COMPUT SC, V6667, P435, DOI 10.1007/978-3-642-24785-9_37; Shafieezadeh Abadeh S, 2018, ADV NEURAL INFORM PR, V31, P8483; Skajaa A, 2013, MATH PROGRAM COMPUT, V5, P1, DOI 10.1007/s12532-012-0046-z; Solomon J, 2014, PR MACH LEARN RES, V32; Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963; Srivastava S, 2018, J MACH LEARN RES, V19; Staib M, 2017, ADV NEUR IN, V30; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wang H, 2014, ADV NEURAL INFORM PR, P2816; Wright S., 1997, PRIMAL DUAL INTERIOR, P21; Nguyen X, 2013, ANN STAT, V41, P370, DOI 10.1214/12-AOS1065; Yang L, 2018, ARXIV180904249; Ye JB, 2017, IEEE T SIGNAL PROCES, V65, P2317, DOI 10.1109/TSP.2017.2659647; Ye Y., 2014, MATH PROGRAM, V59, P151; YE YY, 1992, MATH PROGRAM, V56, P285, DOI 10.1007/BF01580903; Yildirim EA, 2002, SIAM J OPTIMIZ, V12, P782, DOI 10.1137/S1052623400369235	56	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306085
C	Goel, S; Karmalkar, S; Klivans, AR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Goel, Surbhi; Karmalkar, Sushrut; Klivans, Adam R.			Time/Accuracy Tradeoffs for Learning a ReLU with respect to Gaussian Marginals	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider the problem of computing the best-fitting ReLU with respect to square-loss on a training set when the examples have been drawn according to a spherical Gaussian distribution (the labels can be arbitrary). Let opt < 1 be the population loss of the best-fitting ReLU. We prove: Finding a ReLU with square-loss opt+. is as hard as the problem of learning sparse parities with noise, widely thought to be computationally intractable. This is the first hardness result for learning a ReLU with respect to Gaussian marginals, and our results imply -unconditionally- that gradient descent cannot converge to the global minimum in polynomial time. There exists an efficient approximation algorithm for finding the best-fitting ReLU that achieves error O(opt(2/3)). The algorithm uses a novel reduction to noisy halfspace learning with respect to 0/1 loss. Prior work due to Soltanolkotabi [Sol17] showed that gradient descent can find the best-fitting ReLU with respect to Gaussian marginals, if the training set is exactly labeled by a ReLU.	[Goel, Surbhi; Karmalkar, Sushrut; Klivans, Adam R.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Goel, S (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	surbhi@cs.utexas.edu; sushrutk@cs.utexas.edu; klivans@cs.utexas.edu			NSF [CNS-1414023, CCF-1717896]	NSF(National Science Foundation (NSF))	Surbhi Goel and Adam R. Klivans were supported by NSF Award CCF-1717896. Sushrut Karmalkar was supported by NSF Award CNS-1414023.	Abbe Emmanuel, 2018, ABS181206369 CORR; Awasthi P, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P449, DOI 10.1145/2591796.2591839; Boob Digvijay, 2018, ABS180910787 CORR; Bresler Guy, 2014, NIPS, P2852; Brutzkus A, 2017, PR MACH LEARN RES, V70; Diakonikolas I, 2018, ACM S THEORY COMPUT, P1061, DOI 10.1145/3188745.3188754; Ge Rong, 2019, INT C LEARN REPR; Ge Rong, 2018, INT C LEARN REPR; Goel S., 2017, C LEARNING THEORY, P1004; Goel S., 2018, ICML, V80, P1778; Kakade S., 2011, ADV NEURAL INFORM PR; KALAI A. T., 2009, COLT; Kalai AT, 2008, SIAM J COMPUT, V37, P1777, DOI 10.1137/060649057; Klivans A., 2014, APPROXIMATION RANDOM; Li J, 2018, CHIN AUTOM CONGR, P1856, DOI 10.1109/CAC.2018.8623382; Manurangsi Pasin, 2018, ABS181004207 CORR; Shalev-Shwartz S, 2017, PR MACH LEARN RES, V70; Soltanolkotabi Mahdi, 2017, ADV NEURAL INFORM PR, P2007; Valiant G, 2015, J ACM, V62, DOI 10.1145/2728167; Vempala S., 2019, COLT, P3115; Zhang X, 2019, 3RD INTERNATIONAL CONFERENCE ON INNOVATION IN ARTIFICIAL INTELLIGENCE (ICIAI 2019), P248, DOI 10.1145/3319921.3319962; Zhong K, 2017, PR MACH LEARN RES, V70; [No title captured]	24	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900020
C	Han, J; Lombardo, S; Schroers, C; Mandt, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Han, Jun; Lombardo, Salvator; Schroers, Christopher; Mandt, Stephan			Deep Generative Video Compression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The usage of deep generative models for image compression has led to impressive performance gains over classical codecs while neural video compression is still in its infancy. Here, we propose an end-to-end, deep generative modeling approach to compress temporal sequences with a focus on video. Our approach builds upon variational autoencoder (VAE) models for sequential data and combines them with recent work on neural image compression. The approach jointly learns to transform the original sequence into a lower-dimensional representation as well as to discretize and entropy code this representation according to predictions of the sequential VAE. Rate-distortion evaluations on small videos from public data sets with varying complexity and diversity show that our model yields competitive results when trained on generic video content. Extreme compression performance is achieved when training the model on specialized content.	[Han, Jun] Dartmouth Coll, Hanover, NH 03755 USA; [Lombardo, Salvator] Disney Res LA, Los Angeles, CA USA; [Schroers, Christopher] DisneyRes Studios, Zurich, Switzerland; [Mandt, Stephan] Univ Calif Irvine, Irvine, CA USA	Dartmouth College; University of California System; University of California Irvine	Han, J (corresponding author), Dartmouth Coll, Hanover, NH 03755 USA.	junhan@cs.dartmouth.edu; salvator.d.lombardo@disney.com; christopher.schroers@disney.com; mandt@uci.edu		Mandt, Stephan/0000-0001-7836-7839				Agustsson E, 2017, ADV NEUR IN, V30; Alemi AA, 2018, PR MACH LEARN RES, V80; [Anonymous], 2016, P WORKSH ADV APPR BA; Babaeizadeh Mohammad, 2018, INT C LEARN REPR, V2, P7; Ball Johannes, 2018, INT C LEARN REPR ICL; Balle Johannes, 2016, ARXIV161101704; Bayer Justin, 2014, WORKSH ADV APPR BAYE; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Chen T, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP); Chen Z, 2019, IEEE T CIRCUITS SYST; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; Denton E, 2018, PR MACH LEARN RES, V80; Djelouah Aziz, 2019, INT C COMP VIS; Ebert Frederik, 2017, ARXIV171005268; Gershman SJ, 2014, P 36 ANN C COGN SCI; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Habibian Amirhossein, 2019, ARXIV190805717; He JW, 2018, LECT NOTES COMPUT SC, V11209, P466, DOI 10.1007/978-3-030-01228-1_28; Higgins I., 2017, P INT C LEARN REPR T; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnston N, 2018, PROC CVPR IEEE, P4385, DOI 10.1109/CVPR.2018.00461; Kay W., 2017, ARXIV PREPRINT ARXIV; Kingma D. P, 2014, ARXIV13126114; Krishnan Rahul G, 2015, ADV APPR BAYES INF B; Langdon Glen G, 1984, IBM J RES DEV; Lee Alex X, 2018, ARXIV180401523; Li Y., 2018, ARXIV; Lu Guo, 2019, C COMP VIS PATT REC; MacKay DJ, 2003, INFORM THEORY INFERE; Mandt Stephan, 2016, ARTIFICIAL INTELLIGE; Marino J, 2018, PR MACH LEARN RES, V80; Minnen David, 2018, ARXIV180902736; Mukherjee D., 2015, SMPTE MOT IMAGING J, DOI [10.5594/j18499, DOI 10.5594/J18499]; MUSMANN HG, 1985, P IEEE, V73, P523, DOI 10.1109/PROC.1985.13183; Reed S, 2015, ADV NEURAL INFORM PR, P1252; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; RISSANEN J, 1979, IBM J RES DEV, V23, P149, DOI 10.1147/rd.232.0149; Santurkar S, 2018, PICT COD SYMP, P258; Shannon CE, 2001, BELL SYSTEM TECHNICA; Sullivan Gary J, 2012, IEEE T; Theis Lucas, 2017, INT C LEARN REPR; Toderici G., 2016, 4 INT C LEARN REPR I, P2; Toderici George, 2017, FULL RESOLUTION IMAG; Visual Network Index Cisco, 2017, CISC VIS NETW IND GL; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wiegand Thomas, 2003, IEEE T; Wu Chao-Yuan, 2018, EUR C COMP VIS; Xu Qiangeng, 2020, WINT C APPL COMP VIS, P2020; Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865	53	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900083
C	Hara, S; Nitanda, A; Maehara, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hara, Satoshi; Nitanda, Atsushi; Maehara, Takanori			Data Cleansing for Models Trained with SGD	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Data cleansing is a typical approach used to improve the accuracy of machine learning models, which, however, requires extensive domain knowledge to identify the influential instances that affect the models. In this paper, we propose an algorithm that can identify influential instances without using any domain knowledge. The proposed algorithm automatically cleans the data, which does not require any of the users' knowledge. Hence, even non-experts can improve the models. The existing methods require the loss function to be convex and an optimal model to be obtained, which is not always the case in modern machine learning. To overcome these limitations, we propose a novel approach specifically designed for the models trained with stochastic gradient descent (SGD). The proposed method infers the influential instances by retracing the steps of the SGD while incorporating intermediate models computed in each step. Through experiments, we demonstrate that the proposed method can accurately infer the influential instances. Moreover, we used MNIST and CIFAR10 to show that the models can be effectively improved by removing the influential instances suggested by the proposed method.	[Hara, Satoshi] Osaka Univ, Suita, Osaka, Japan; [Nitanda, Atsushi] Univ Tokyo, Tokyo, Japan; [Maehara, Takanori] RIKEN, AIP, Tokyo, Japan	Osaka University; University of Tokyo; RIKEN	Hara, S (corresponding author), Osaka Univ, Suita, Osaka, Japan.	satohara@ar.sanken.osaka-u.ac.jp; nitanda@mist.i.u-tokyo.ac.jp; takanori.maehara@riken.jp			JSPS KAKENHI [JP18K18106, JP19K20337]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	Satoshi Hara is supported by JSPS KAKENHI Grant Number JP18K18106. Atsushi Nitanda is supported by JSPS KAKENHI Grant Number JP19K20337.	Aggarwal Charu C., 2016, OUTLIER ANAL; Aslam JA, 1996, INFORM PROCESS LETT, V57, P189, DOI 10.1016/0020-0190(96)00006-3; BECKMAN RJ, 1974, J AM STAT ASSOC, V69, P199, DOI 10.2307/2285524; Breunig MM, 2000, SIGMOD REC, V29, P93, DOI 10.1145/335191.335388; Brodley CE, 1999, J ARTIF INTELL RES, V11, P131, DOI 10.1613/jair.606; COOK RD, 1977, TECHNOMETRICS, V19, P15, DOI 10.2307/1268249; COOK RD, 1980, TECHNOMETRICS, V22, P495; Dua D., 2017, UCI MACHINE LEARNING; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Ghorbani Amirata, 2019, P 36 INT C MACH LEAR; Khanna R., 2019, P 22 INT C ART INT S, P3382; Koh PW, 2017, PR MACH LEARN RES, V70; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu FT, 2008, IEEE DATA MINING, P413, DOI 10.1109/ICDM.2008.17; Maclaurin D, 2015, PR MACH LEARN RES, V37, P2113; Natarajan Nagarajan, 2013, ADV NEURAL INFORM PR; Ng A, 2017, MACHINE LEARNING YEA; PREGIBON D, 1981, ANN STAT, V9, P705, DOI 10.1214/aos/1176345513; Ren M., 2018, P 35 INT C MACH LEAR; Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; Zhang XZ, 2018, AAAI CONF ARTIF INTE, P4482; Zhou C, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P665, DOI 10.1145/3097983.3098052	24	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304024
C	Hasani, H; Baghshah, MS; Aghajan, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hasani, Hosein; Baghshah, Mahdieh Soleymani; Aghajan, Hamid			Surround Modulation: A Bio-inspired Connectivity Structure for Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CLASSICAL RECEPTIVE-FIELD; FUNCTIONAL ARCHITECTURE; HIERARCHICAL-MODELS; OBJECT RECOGNITION; VISUAL-CORTEX; SPARSE; RESPONSES; SUPPRESSION; STRIATE; SEGREGATION	Numerous neurophysiological studies have revealed that a large number of the primary visual cortex neurons operate in a regime called surround modulation. Surround modulation has a substantial effect on various perceptual tasks, and it also plays a crucial role in the efficient neural coding of the visual cortex. Inspired by the notion of surround modulation, we designed new excitatory-inhibitory connections between a unit and its surrounding units in the convolutional neural network (CNN) to achieve a more biologically plausible network. Our experiments show that this simple mechanism can considerably improve both the performance and training speed of traditional CNNs in visual tasks. We further explore additional outcomes of the proposed structure. We first evaluate the model under several visual challenges, such as the presence of clutter or change in lighting conditions and show its superior generalization capability in handling these challenging situations. We then study possible changes in the statistics of neural activities such as sparsity and decorrelation and provide further insight into the underlying efficiencies of surround modulation. Experimental results show that importing surround modulation into the convolutional layers ensues various effects analogous to those derived by surround modulation in the visual cortex.	[Hasani, Hosein; Aghajan, Hamid] Sharif Univ Technol, Dept Elect Engn, Tehran, Iran; [Baghshah, Mahdieh Soleymani] Sharif Univ Technol, Dept Comp Engn, Tehran, Iran	Sharif University of Technology; Sharif University of Technology	Hasani, H (corresponding author), Sharif Univ Technol, Dept Elect Engn, Tehran, Iran.	hasani.hosein@ee.sharif.edu; soleymani@sharif.edu; aghajan@ee.sharif.edu						Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; ALLMAN J, 1985, ANNU REV NEUROSCI, V8, P407, DOI 10.1146/annurev.ne.08.030185.002203; Angelucci A, 2006, PROG BRAIN RES, V154, P93, DOI 10.1016/S0079-6123(06)54005-1; Barlow H, 2001, NETWORK-COMP NEURAL, V12, P241, DOI 10.1088/0954-898X/12/3/301; BORN RT, 1992, NATURE, V357, P497, DOI 10.1038/357497a0; Buracas GT, 1996, VISION RES, V36, P869, DOI 10.1016/0042-6989(95)00192-1; Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963; Cho K., 2014, P 2014 C EMP METH NA, P1724; DEANGELIS GC, 1994, J NEUROPHYSIOL, V71, P347, DOI 10.1152/jn.1994.71.1.347; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1; FIELD DJ, 1993, VISION RES, V33, P173, DOI 10.1016/0042-6989(93)90156-Q; FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Grossberg S, 1997, TRENDS NEUROSCI, V20, P106, DOI 10.1016/S0166-2236(96)01002-8; Guclu U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015; Haider B, 2010, NEURON, V65, P107, DOI 10.1016/j.neuron.2009.12.005; HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640; HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455; HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837; HUBEL DH, 1965, J NEUROPHYSIOL, V28, P229, DOI 10.1152/jn.1965.28.2.229; Hung CP, 2005, SCIENCE, V310, P863, DOI 10.1126/science.1117593; Hurley N, 2009, IEEE T INFORM THEORY, V55, P4723, DOI 10.1109/TIT.2009.2027527; Ioffe S, 2015, ARXIV 1502 03167, V32, P448; Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500; Jones HE, 2001, J NEUROPHYSIOL, V86, P2011, DOI 10.1152/jn.2001.86.4.2011; Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915; Kingma D.P, P 3 INT C LEARNING R; KNIERIM JJ, 1992, J NEUROPHYSIOL, V67, P961, DOI 10.1152/jn.1992.67.4.961; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; LAMME VAF, 1995, J NEUROSCI, V15, P1605; LeCun Y, 2004, PROC CVPR IEEE, P97; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 1995, HDB BRAIN THEORY NEU, V3361, P1995, DOI DOI 10.1007/S13398-014-0173-7.2; Li X, 2018, PATTERN RECOGN, V79, P183, DOI 10.1016/j.patcog.2018.01.015; Liao Q., 2016, CORR; Linsley D., 2018, P 32 INT C NEURAL IN, P152; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; Nayebi A., 2018, ADV NEURAL INFORM PR, P5290; Nurminen L, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04500-5; Olshausen BA, 2004, CURR OPIN NEUROBIOL, V14, P481, DOI 10.1016/j.conb.2004.07.007; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; PETERHANS E, 1991, TRENDS NEUROSCI, V14, P112, DOI 10.1016/0166-2236(91)90072-3; Rajaei Karim, 2019, BIORXIV; Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580; Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819; Sakai K, 2006, J COGNITIVE NEUROSCI, V18, P562, DOI 10.1162/jocn.2006.18.4.562; Sceniak MP, 1999, NAT NEUROSCI, V2, P733, DOI 10.1038/11197; Sceniak MP, 2001, J NEUROPHYSIOL, V85, P1873, DOI 10.1152/jn.2001.85.5.1873; Shen ZM, 2007, J PHYSIOL-LONDON, V583, P581, DOI 10.1113/jphysiol.2007.130294; Spoerer CJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01551; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sundberg KA, 2009, NEURON, V61, P952, DOI 10.1016/j.neuron.2009.02.023; Vinje WE, 2002, J NEUROSCI, V22, P2904, DOI 10.1523/JNEUROSCI.22-07-02904.2002; Vinje WE, 2000, SCIENCE, V287, P1273, DOI 10.1126/science.287.5456.1273; Walker GA, 1999, J NEUROSCI, V19, P10536; Willmore BDB, 2011, J NEUROPHYSIOL, V105, P2907, DOI 10.1152/jn.00594.2010; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111; Yao HS, 2007, NAT NEUROSCI, V10, P772, DOI 10.1038/nn1895; Zamir AR, 2017, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2017.196	63	2	2	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907055
C	Huang, BW; Zhang, K; Xie, PT; Gong, MM; Xing, E; Glymour, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Huang, Biwei; Zhang, Kun; Xie, Pengtao; Gong, Mingming; Xing, Eric; Glymour, Clark			Specific and Shared Causal Relation Modeling and Mechanism-Based Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NETWORKS	State-of-the-art approaches to causal discovery usually assume a fixed underlying causal model. However, it is often the case that causal models vary across domains or subjects, due to possibly omitted factors that affect the quantitative causal effects. As a typical example, causal connectivity in the brain network has been reported to vary across individuals, with significant differences across groups of people, such as autistics and typical controls. In this paper, we develop a unified framework for causal discovery and mechanism-based group identification. In particular, we propose a specific and shared causal model (SSCM), which takes into account the variabilities of causal relations across individuals/groups and leverages their commonalities to achieve statistically reliable estimation. The learned SSCM gives the specific causal knowledge for each individual as well as the general trend over the population. In addition, the estimated model directly provides the group information of each individual. Experimental results on synthetic and real-world data demonstrate the efficacy of the proposed method.	[Huang, Biwei; Zhang, Kun; Glymour, Clark] Carnegie Mellon Univ, Dept Philosophy, Pittsburgh, PA 15213 USA; [Xie, Pengtao; Xing, Eric] Petuum Inc, Pittsburgh, PA USA; [Gong, Mingming] Univ Melbourne, Sch Math & Stat, Melbourne, Vic, Australia; [Xing, Eric] Carnegie Mellon Univ, Dept Machine Learning, Pittsburgh, PA 15213 USA	Carnegie Mellon University; University of Melbourne; Carnegie Mellon University	Huang, BW (corresponding author), Carnegie Mellon Univ, Dept Philosophy, Pittsburgh, PA 15213 USA.	biweih@andrew.cmu.edu		Gong, Mingming/0000-0001-7147-5589	National Institutes of Health [NIH-1R01EB022858-01, FAIN-R01EB022858, NIH-1R01LM012087, NIH-5U54HG008540-02, FAIN-U54HG008540]; United States Air Force [FA8650-17-C-7715]; National Science Foundation EAGER Grant [IIS-1829681]; Living Analytics Research Center; Singapore Management University	National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); United States Air Force(United States Department of Defense); National Science Foundation EAGER Grant; Living Analytics Research Center; Singapore Management University(Singapore Management University)	We thank Petar Stojanov for helping to revise the paper. We would like to acknowledge the support by National Institutes of Health under Contract No. NIH-1R01EB022858-01, FAIN-R01EB022858, NIH-1R01LM012087, NIH-5U54HG008540-02, and FAIN-U54HG008540, by the United States Air Force under Contract No. FA8650-17-C-7715, and by National Science Foundation EAGER Grant No. IIS-1829681. The National Institutes of Health, the U.S. Air Force, and the National Science Foundation are not responsible for the views reported in this article. KZ also benefited from funding from Living Analytics Research Center and Singapore Management University.	Bird CM, 2008, NAT REV NEUROSCI, V9, P182, DOI 10.1038/nrn2335; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; Delyon B, 1999, ANN STAT, V27, P94; Ebisch SJH, 2011, HUM BRAIN MAPP, V32, P1013, DOI 10.1002/hbm.21085; Erdos P., 1959, PUBL MATH-DEBRECEN, V6, P290, DOI DOI 10.2307/1999405; Eriksson J, 2004, IEEE SIGNAL PROC LET, V11, P601, DOI 10.1109/LSP.2004.830118; Figueiredo MAT, 2002, IEEE T PATTERN ANAL, V24, P381, DOI 10.1109/34.990138; Gates KM, 2010, NEUROIMAGE, V50, P1118, DOI 10.1016/j.neuroimage.2009.12.117; Gazzaniga Michael S, 2014, COGNITIVE NEUROSCIEN, V2006; Ghassami AmirEmad, 2018, Adv Neural Inf Process Syst, V31, P6266; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; Hoyer P.O., 2009, ADV NEURAL INFORM PR, P689; Huang B., 2019, ABS190301672 CORR; Huang B., 2019, INT C MACH LEARN ICM; Huang BW, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1551, DOI 10.1145/3219819.3220104; Huang B, 2017, IEEE DATA MINING, P913, DOI 10.1109/ICDM.2017.114; Huang BW, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3561; Hyvarinen A, 2010, J MACH LEARN RES, V11, P1709; Jabbari E, 2018, P MACH LEARN RES; Kim J, 2007, HUM BRAIN MAPP, V28, P85, DOI 10.1002/hbm.20259; Lacerda G., 2008, P 24 C UNC ART UAI H; Martino L, 2015, IEEE T SIGNAL PROCES, V63, P3123, DOI 10.1109/TSP.2015.2420537; Peters Jonas, 2016, J ROYAL STAT SOC B, P2; RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239; Remondes M, 2004, NATURE, V431, P699, DOI 10.1038/nature02965; Riegert C, 2004, BEHAV BRAIN RES, V152, P23, DOI 10.1016/j.bbr.2003.09.011; Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Song D., 2015, RECENT ADV MODULAR O, P385; Spirtes P., 2000, CAUSATION PREDICTION; Uddin LQ, 2010, FRONT SYST NEUROSCI, V4, DOI 10.3389/fnsys.2010.00021; Vandermeulen R. A., 2015, ARXIV150206644; Wang Yi, 2018, NIPS, P2; Zhang K., 2009, P TWENTYFIFTH C UNCE, P647; Zhang K., 2017, IJCAI; Zhang K, 2006, P 13 INT C NEUR INF; Zhang K, 2016, PROCEEDINGS OF THE ASME 35TH INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING , 2016, VOL 3	38	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905021
C	Huang, ZF; Huang, ZY; Wang, YL; Yi, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Huang, Zengfeng; Huang, Ziyue; Wang, Yilei; Yi, Ke			Optimal Sparsity-Sensitive Bounds for Distributed Mean Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider the problem of estimating the mean of a set of vectors, which are stored in a distributed system. This is a fundamental task with applications in distributed SGD and many other distributed problems, where communication is a main bottleneck for scaling up computations. We propose a new sparsity-aware algorithm, which improves previous results both theoretically and empirically. The communication cost of our algorithm is characterized by Hoyer's measure of sparseness. Moreover, we prove that the communication cost of our algorithm is information-theoretic optimal up to a constant factor in all sparseness regime. We have also conducted experimental studies, which demonstrate the advantages of our method and confirm our theoretical findings.	[Huang, Zengfeng] Fudan Univ, Sch Data Sci, Shanghai, Peoples R China; [Huang, Ziyue; Wang, Yilei; Yi, Ke] HKUST, Dept CSE, Hong Kong, Peoples R China	Fudan University; Hong Kong University of Science & Technology	Huang, ZF (corresponding author), Fudan Univ, Sch Data Sci, Shanghai, Peoples R China.	huangzf@fudan.edu.cn; zhuangbq@cse.ust.hk; ywanggq@cse.ust.hk; yike@cse.ust.hk			National Natural Science Foundation of China [61802069]; Shanghai Sailing Program [18YF1401200]; Shanghai Science and Technology Commission [17JC1420200]; HKRGC [16200415, 16202317, 16201318]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shanghai Sailing Program; Shanghai Science and Technology Commission(Shanghai Science & Technology CommitteeScience & Technology Commission of Shanghai Municipality (STCSM)); HKRGC(Hong Kong Research Grants Council)	Zengfeng Huang is partially supported by National Natural Science Foundation of China (Grant No. 61802069), Shanghai Sailing Program (Grant No. 18YF1401200) and Shanghai Science and Technology Commission (Grant No. 17JC1420200). Ziyue Huang, Yilei Wang, and Ke Yi are supported by HKRGC under grants 16200415, 16202317, and 16201318.	Aji A.F., 2017, P 2017 C EMP METH NA, DOI [10.18653/v1/D17-1045, DOI 10.18653/V1/D17-1045]; Alistarh D., 2017, ADV NEURAL INF PROCE, P1709; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Braverman M, 2016, ACM S THEORY COMPUT, P1011, DOI 10.1145/2897518.2897582; Chi-Chih Yao A., 1977, 18th Annual Symposium on Foundations of Computer Science, P222; CHILIMBI TM, 2014, P OSDI, V14, P571; Cover TM, 2006, ELEMENTS INFORM THEO; Garg A., 2014, ADV NEURAL INFORM PR, V27, P2726, DOI [https://doi.org/10.1109/hipc.1997.634533, DOI 10.1109/HIPC.1997.634533]; Hoyer PO, 2004, J MACH LEARN RES, V5, P1457; Jaggi M., 2014, ADV NEURAL INF PROCE, V4, P3068; Konecny J., 2018, FRONT APPL MATH STAT, V4, P62; Lan G., 2018, MATH PROGRAMMING; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee J.D., 2017, J MACH LEARN RES, V18, P4404; Lin Yujun, 2018, P INT C LEARNING REP; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; RAMABADRAN TV, 1990, IEEE T COMMUN, V38, P1156, DOI 10.1109/26.58748; Scaman K, 2017, PR MACH LEARN RES, V70; Strom N., 2015, 16 ANN C INT SPEECH; Suresh A. T., 2017, ICML; Trefethen L. N., 1997, NUMERICAL LINEAR ALG, V50; Wang H., 2018, ADV NEURAL INFORM PR; Wangni JQ, 2018, ADV NEUR IN, V31; Wen W., 2017, P NIPS, P1509; Yu M, 2018, ASIA PAC CONF ANTEN, P36; Zhang Y., 2013, NEURAL INFORM PROCES, P2328	27	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306038
C	Hunziker, A; Chen, YX; Mac Aodha, O; Rodriguez, MG; Krause, A; Perona, P; Yue, YS; Singla, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hunziker, Anette; Chen, Yuxin; Mac Aodha, Oisin; Rodriguez, Manuel Gomez; Krause, Andreas; Perona, Pietro; Yue, Yisong; Singla, Adish			Teaching Multiple Concepts to a Forgetful Learner	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DISTRIBUTED PRACTICE; RETENTION; MEMORY; MODEL	How can we help a forgetful learner learn multiple concepts within a limited time frame? While there have been extensive studies in designing optimal schedules for teaching a single concept given a learner's memory model, existing approaches for teaching multiple concepts are typically based on heuristic scheduling techniques without theoretical guarantees. In this paper, we look at the problem from the perspective of discrete optimization and introduce a novel algorithmic framework for teaching multiple concepts with strong performance guarantees. Our framework is both generic, allowing the design of teaching schedules for different memory models, and also interactive, allowing the teacher to adapt the schedule to the underlying forgetting mechanisms of the learner. Furthermore, for a well-known memory model, we are able to identify a regime of model parameters where our framework is guaranteed to achieve high performance. We perform extensive evaluations using simulations along with real user studies in two concrete applications: (i) an educational app for online vocabulary teaching; and (ii) an app for teaching novices how to recognize animal species from images. Our results demonstrate the effectiveness of our algorithm compared to popular heuristic approaches.	[Hunziker, Anette] Univ Zurich, Zurich, Switzerland; [Chen, Yuxin] Univ Chicago, Chicago, IL 60637 USA; [Mac Aodha, Oisin] Univ Edinburgh, Edinburgh, Midlothian, Scotland; [Krause, Andreas] Swiss Fed Inst Technol, Zurich, Switzerland; [Perona, Pietro; Yue, Yisong] CALTECH, Pasadena, CA 91125 USA; [Rodriguez, Manuel Gomez; Singla, Adish] MPI SWS, Saarbrucken, Germany	University of Zurich; University of Chicago; University of Edinburgh; Swiss Federal Institutes of Technology Domain; ETH Zurich; California Institute of Technology	Hunziker, A (corresponding author), Univ Zurich, Zurich, Switzerland.	anette.hunziker@gmail.com; chenyuxin@uchicago.edu; oisin.macaodha@ed.ac.uk; manuelgr@mpi-sws.org; krausea@ethz.ch; perona@caltech.edu; yyue@caltech.edu; adishs@mpi-sws.org	Rodriguez, Manuel Gomez/AAB-5005-2021; Singla, Adish/ABG-8960-2021	Singla, Adish/0000-0001-9922-0668; Krause, Andreas/0000-0001-7260-9673	NSF [1645832]; Northrop Grumman; Bloomberg; AWS Research Credits; Google as part of the Visipedia project; Swiss NSF Early Mobility Postdoctoral Fellowship	NSF(National Science Foundation (NSF)); Northrop Grumman; Bloomberg; AWS Research Credits; Google as part of the Visipedia project(Google Incorporated); Swiss NSF Early Mobility Postdoctoral Fellowship	This work was done when Yuxin Chen and Oisin Mac Aodha were at Caltech. This work was supported in part by NSF Award #1645832, Northrop Grumman, Bloomberg, AWS Research Credits, Google as part of the Visipedia project, and a Swiss NSF Early Mobility Postdoctoral Fellowship.	Balota D. A., 2007, IS EXPANDED RETRIEVA; BLOOM KC, 1981, J EDUC RES, V74, P245, DOI 10.1080/00220671.1981.10885317; Chen Yuxin, 2018, NEURIPS; COCHRAN WG, 1952, ANN MATH STAT, V23, P315, DOI 10.1214/aoms/1177729380; Ebbinghaus H., 1885, BER GED CHTNIS UNTER; GOLDMAN SA, 1995, J COMPUT SYST SCI, V50, P20, DOI 10.1006/jcss.1995.1003; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; Haug Luis, 2018, NEURIPS; Kamalaruban P, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2692; Khajah Mohammad M, 2014, TOPICS COGNITIVE SCI; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Krause A, 2014, TRACTABILITY, P71; Leitner S., 1972, ANGEW LERNPSYCHOLOGI; Lindsey RV, 2014, PSYCHOL SCI, V25, P639, DOI 10.1177/0956797613504302; Liu WY, 2017, PR MACH LEARN RES, V70; McCloskey M., 1989, PSYCHOL LEARNING MOT; Mozer M.C., 2009, ADV NEURAL INFORM PR, V22, P1321; NOSOFSKY RM, 1986, J EXP PSYCHOL GEN, V115, P39, DOI 10.1037/0096-3445.115.1.39; Nosofsky RM, 2019, PSYCHON B REV, V26, P48, DOI 10.3758/s13423-018-1508-8; Patil K. R., 2014, NIPS, P2465; Pavlik PI, 2005, COGNITIVE SCI, V29, P559, DOI 10.1207/s15516709cog0000_14; PIMSLEUR P, 1967, MOD LANG J, V51, P73, DOI 10.2307/321812; Reddy S, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1815, DOI 10.1145/2939672.2939850; Rubin David C, 1996, PSYCHOL REV; Settles B, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1848; Shebilske WL, 1999, J EXP PSYCHOL-APPL, V5, P413, DOI 10.1037/1076-898X.5.4.413; Simmons AL, 2012, J RES MUSIC EDUC, V59, P357, DOI 10.1177/0022429411424798; Singla A., 2013, NIPS WORKSH DAT DRIV; Singla A, 2014, PR MACH LEARN RES, V32, P154; Spruit EN, 2015, SURG ENDOSC, V29, P2235, DOI 10.1007/s00464-014-3931-x; Sullivan BL, 2009, BIOL CONSERV, V142, P2282, DOI 10.1016/j.biocon.2009.05.006; Tabibian B, 2019, P NATL ACAD SCI USA, V116, P3988, DOI 10.1073/pnas.1815156116; Tschiatschek Sebastian, 2017, AAAI; TZENG OJL, 1973, J EXP PSYCHOL, V99, P162, DOI 10.1037/h0034642; Van Horn G., 2018, CVPR; Verdaasdonk EGG, 2007, SURG ENDOSC, V21, P214, DOI 10.1007/s00464-005-0852-8; Walsh MM, 2018, J EXP PSYCHOL GEN, V147, P1325, DOI 10.1037/xge0000416; Wickens Thomas D, 1999, MEASURING TIME COURS; Yeo T, 2019, AAAI CONF ARTIF INTE, P5684; Zhang ZL, 2016, IEEE T AUTOMAT CONTR, V61, P601, DOI 10.1109/TAC.2015.2440566; Zhou Y, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2817; Zhu XJ, 2015, AAAI CONF ARTIF INTE, P4083; Zhu Xiaojin, 2018, ABS180105927 CORR	43	2	2	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304009
C	Ibragimov, B; Gusev, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ibragimov, Bulat; Gusev, Gleb			Minimal Variance Sampling in Stochastic Gradient Boosting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Stochastic Gradient Boosting (SGB) is a widely used approach to regularization of boosting models based on decision trees. It was shown that, in many cases, random sampling at each iteration can lead to better generalization performance of the model and can also decrease the learning time. Different sampling approaches were proposed, where probabilities are not uniform, and it is not currently clear which approach is the most effective. In this paper, we formulate the problem of randomization in SGB in terms of optimization of sampling probabilities to maximize the estimation accuracy of split scoring used to train decision trees. This optimization problem has a closed-form nearly optimal solution, and it leads to a new sampling technique, which we call Minimal Variance Sampling (MVS). The method both decreases the number of examples needed for each iteration of boosting and increases the quality of the model significantly as compared to the state-of-the art sampling methods. The superiority of the algorithm was confirmed by introducing MVS as a new default option for subsampling in CatBoost, a gradient boosting library achieving state-of-the-art quality on various machine learning tasks.	[Ibragimov, Bulat; Gusev, Gleb] Yandex, Moscow, Russia; [Ibragimov, Bulat] Moscow Inst Phys & Technol, Moscow, Russia; [Gusev, Gleb] Sberbank, Moscow, Russia	Moscow Institute of Physics & Technology; Sberbank	Ibragimov, B (corresponding author), Yandex, Moscow, Russia.; Ibragimov, B (corresponding author), Moscow Inst Phys & Technol, Moscow, Russia.	ibrbulat@yandex.ru; gusev.g.g@sberbank.ru	Gusev, Gleb G./C-8263-2014					Alafate J., 2019, ARXIV190109047; Breiman Leo, 1999, 547 STAT DEPT; Bulat lbragimov, 2019, MVS IMPLEMENTATION; Burges C.J., 2010, LEARNING, V11, P81; Caruana R., 2006, P 23 INT C MACH LEAR, P161; Catboost, 2018, DATA PREPROCESSING; CatBoost, 2019, CATBOOST GITHUB; Chen T., 2016, KDD16 P 22 ACM, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]; Culp M, 2011, J COMPUT GRAPH STAT, V20, P937, DOI 10.1198/jcgs.2010.09076; Daning C., 2018, ARXIV180404659; Dorogush A. V., 2017, WORKSH ML SYST NIPS; Fleuret F, 2008, J MACH LEARN RES, V9, P2549; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Friedman JH, 2002, COMPUT STAT DATA AN, V38, P367, DOI 10.1016/S0167-9473(01)00065-2; HORVITZ DG, 1952, J AM STAT ASSOC, V47, P663, DOI 10.2307/2280784; Johnson T. B., 2018, ADV NEURAL INFORM PR, V31; *KAGGL, 2013, AM DAT; *KAGGL, 2012, KICK PRED DAT; Kalal Z., 2008, P BRIT MACH VIS C, P1, DOI DOI 10.5244/C.22.42; Ke G., 2017, P ADV NEURAL INFORM, V30, P3146; KOHAVI R, ADULT DATASET; Lugosi G, 2004, ANN STAT, V32, P30; MAHMOUD HM, 1995, RAIRO-INF THEOR APPL, V29, P255, DOI 10.1051/ita/1995290402551; Mease D, 2008, J MACH LEARN RES, V9, P131; Prokhorenkova L., 2018, ADV NEURAL INF PROCE, V2018, P6638; Richardson Matthew, 2007, P 16 INT C WORLD WID, P521, DOI DOI 10.1145/1242572.1242643; Roe BP, 2005, NUCL INSTRUM METH A, V543, P577, DOI 10.1016/j.nima.2004.12.018; Shih YS, 1999, STAT COMPUT, V9, P309, DOI 10.1023/A:1008920224518; Wu QA, 2010, INFORM RETRIEVAL, V13, P254, DOI 10.1007/s10791-009-9112-1; Zhang YR, 2015, TRANSPORT RES C-EMER, V58, P308, DOI 10.1016/j.trc.2015.02.019; Zhao PL, 2015, PR MACH LEARN RES, V37, P1; 2015, RECSYS DATASET; 2012, CLICK PREDICTION DAT; 2009, KDD CHURN DATASET; 2009, KDD UPSELLING DATASE	35	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906071
C	Ida, Y; Fujiwara, Y; Kashima, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ida, Yasutoshi; Fujiwara, Yasuhiro; Kashima, Hisashi			Fast Sparse Group Lasso	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Sparse Group Lasso is a method of linear regression analysis that finds sparse parameters in terms of both feature groups and individual features. Block Coordinate Descent is a standard approach to obtain the parameters of Sparse Group Lasso, and iteratively updates the parameters for each parameter group. However, as an update of only one parameter group depends on all the parameter groups or data points, the computation cost is high when the number of the parameters or data points is large. This paper proposes a fast Block Coordinate Descent for Sparse Group Lasso. It efficiently skips the updates of the groups whose parameters must be zeros by using the parameters in one group. In addition, it preferentially updates parameters in a candidate group set, which contains groups whose parameters must not be zeros. Theoretically, our approach guarantees the same results as the original Block Coordinate Descent. Experiments show that our algorithm enhances the efficiency of the original algorithm without any loss of accuracy.	[Ida, Yasutoshi] NTT Software Innovat Ctr, Tokyo, Japan; [Fujiwara, Yasuhiro] NTT Commun Sci Labs, Tokyo, Japan; [Ida, Yasutoshi; Kashima, Hisashi] Kyoto Univ, Kyoto, Japan; [Kashima, Hisashi] RIKEN, AIP, Wako, Saitama, Japan	Nippon Telegraph & Telephone Corporation; Kyoto University; RIKEN	Ida, Y (corresponding author), NTT Software Innovat Ctr, Tokyo, Japan.; Ida, Y (corresponding author), Kyoto Univ, Kyoto, Japan.	yasutoshi.ida@ieee.org; yasuhiro.fujiwara.kh@hco.ntt.co.jp; kashima@i.kyoto-u.ac.jp						[Anonymous], 2016, NEURAL INFORM PROCES; Bonnefoy A, 2015, IEEE T SIGNAL PROCES, V63, P5121, DOI 10.1109/TSP.2015.2447503; Bonnefoy A, 2014, EUR SIGNAL PR CONF, P6; Cui DL, 2001, CONF P INDIUM PHOSPH, P224, DOI 10.1109/ICIPRM.2001.929098; El Ghaoui L, 2012, PAC J OPTIM, V8, P667; Friedman J., 2010, ARXIV; Fujiwara Y, 2016, AAAI CONF ARTIF INTE, P1561; Fujiwara Y, 2016, PROC VLDB ENDOW, V10, P229; Huang J, 2012, STAT SCI, V27, P481, DOI 10.1214/12-STS392; Jacob L., 2009, P 26 INT C MACH LEAR, P433, DOI DOI 10.1145/1553374.1553431; Johnson T. B., 2016, P ADV NEUR INF PROC, P4754; Johnson TB, 2017, PR MACH LEARN RES, V70; Nam D, 2008, BRIEF BIOINFORM, V9, P189, DOI 10.1093/bib/bbn001; Ndiaye E, 2017, J MACH LEARN RES, V18; Ndiaye E, 2015, ADV NEUR IN, V28; Roth V., 2008, P 25 INT C MACH LEAR, P848; Simon N, 2013, J COMPUT GRAPH STAT, V22, P231, DOI 10.1080/10618600.2012.681250; Wang J., 2014, ADV NEURAL INFORM PR, P2132	18	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301066
C	Jagadeesan, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jagadeesan, Meena			Understanding Sparse JL for Feature Hashing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				JOHNSON-LINDENSTRAUSS	Feature hashing and other random projection schemes are commonly used to reduce the dimensionality of feature vectors. The goal is to efficiently project a high-dimensional feature vector living in R-n into a much lower-dimensional space R-m, while approximately preserving Euclidean norm. These schemes can be constructed using sparse random projections, for example using a sparse Johnson-Lindenstrauss (JL) transform. A line of work introduced by Weinberger et. al (ICML '09) analyzes the accuracy of sparse JL with sparsity 1 on feature vectors with small l(infinity)-to-l(2) norm ratio. Recently, Freksen, Kamma, and Larsen (NeurIPS '18) closed this line of work by proving a tight tradeoff between l(infinity)-to-l(2) norm ratio and accuracy for sparse JL with sparsity 1. In this paper, we demonstrate the benefits of using sparsity s greater than 1 in sparse JL on feature vectors. Our main result is a tight tradeoff between l(infinity)-to-l(2) norm ratio and accuracy for a general sparsity s, that significantly generalizes the result of Freksen et. al. Our result theoretically demonstrates that sparse JL with s > 1 can have significantly better norm-preservation properties on feature vectors than sparse JL with s = 1; we also empirically demonstrate this finding.	[Jagadeesan, Meena] Harvard Univ, Cambridge, MA 02138 USA	Harvard University	Jagadeesan, M (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	mjagadeesan@college.harvard.edu						Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4; Allen-Zhu Z, 2014, P NATL ACAD SCI USA, V111, P16872, DOI 10.1073/pnas.1419100111; Bai B, 2010, INFORM RETRIEVAL, V13, P291, DOI 10.1007/s10791-009-9117-9; Caragea C, 2012, PROTEOME SCI, V10, DOI 10.1186/1477-5956-10-S1-S14; Chen C., 2018, SOFT COMPUT, V22; Cohen M. B., 2018, 1 S SIMPL ALG SOSA 2, P1; Cohen Michael B, 2016, P 27 ANN ACM SIAM S, P278; Dahlgaard S., 2017, P 31 INT C NEURAL IN, P6618; Dalessandro B, 2013, BIG DATA-US, V1, P110, DOI 10.1089/big.2013.0010; Dasgupta A, 2010, ACM S THEORY COMPUT, P341; Freksen C., 2018, ADV NEURAL INFORM PR, P5394; Jagadeesan M., 2019, P 23 INT C 24 INT C; Jayram TS, 2013, ACM T ALGORITHMS, V9, DOI 10.1145/2483699.2483706; Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400; Kane Daniel, 2011, Approximation, Randomization, and Combinatorial Optimization Algorithms and Techniques. Proceedings 14th International Workshop, APPROX 2011 and 15th International Workshop, RANDOM 2011, P628, DOI 10.1007/978-3-642-22935-0_53; Kane D. M., 2010, CORR; Kane D. M., 2012, P 23 ANN ACM SIAM S, P16872; Latala R, 1997, ANN PROBAB, V25, P1502; Latala R, 1999, STUD MATH, V135, P39; Li P., 2006, P 12 ACM SIGKDD INT, P287, DOI [10.1145/1150402.1150436, DOI 10.1145/1150402.1150436]; Lyu HL, 2015, CHIN CONT DECIS CONF, P2885; Ma CF, 2015, NEUROCOMPUTING, V149, P1232, DOI 10.1016/j.neucom.2014.09.004; Nelson J, 2013, ANN IEEE SYMP FOUND, P117, DOI 10.1109/FOCS.2013.21; Nelson J, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P101; Newman D., 2008, BAG WORDS DATA SET; Song HH, 2014, ELECTRON LETT, V50, P1931, DOI 10.1049/el.2014.1911; Suthaharan S, 2016, INTEGR SER INFORM SY, V36, P1, DOI 10.1007/978-1-4899-7641-3; Weinberger K., 2009, P 26 ANN INT C MACH, P1113, DOI DOI 10.1145/1553374.1553516	28	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906082
C	Jambulapati, A; Sidford, A; Tian, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jambulapati, Arun; Sidford, Aaron; Tian, Kevin			A Direct O(1/) Iteration Parallel Algorithm for Optimal Transport	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				VARIATIONAL-INEQUALITIES	Optimal transportation. or computing the Wasserstein or "earth mover's" distance between two n-dimensional distributions, is a fundamental primitive which arises in many learning and statistical settings. We give an algorithm which solves the problem to additive c accuracy with 6(1/c) parallel depth and 6 (n2/c) work. [BIK,S18, Qua191 obtained this runtime through reductions to positive linear programming and matrix scaling. However, these reduction-based algorithms use subroutines which may be impractical due to requiring solvers for second -order iterations (matrix scaling) or non-parallelizability (positive LP). Our methods match the previous-best work bounds by [131 KS 18, Qua 19] while either improving parallelization or removing the need for linear system solves, and improve upon the previous best first-order methods running in time 0 (rnin(r12/E2, n2'5/0) [DCiK1S. 191. We obtain our results by a primal -dual extragradient method, motivated by recent theoretical improvements to maximum flow [She] 7].	[Jambulapati, Arun; Sidford, Aaron; Tian, Kevin] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Jambulapati, A (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	jmblpati@stanford.edu; sidford@stanford.edu; kjtian@stanford.edu			NSF Graduate Fellowship [DGE-114747, DGE-1656518]; NSF CAREER Award [CCF-1844855]	NSF Graduate Fellowship(National Science Foundation (NSF)); NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD))	We thank Jose Blanchet and Carson Kent for helpful conversations. AJ was supported by NSF Graduate Fellowship DGE-114747. AS was supported by NSF CAREER Award CCF-1844855. KT was supported by NSF Graduate Fellowship DGE-1656518.	Agarwal PK, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P555, DOI 10.1145/2591796.2591844; Allen-Zhu Z, 2015, ACM S THEORY COMPUT, P229, DOI 10.1145/2746539.2746573; Allen-Zhu Z, 2017, ANN IEEE SYMP FOUND, P890, DOI 10.1109/FOCS.2017.87; Altschuler J., 2017, ADV NEURAL INFORM PR, P1961; Altschuler Jason, 2018, ABS181010046 CORR; Andoni A, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P574, DOI 10.1145/2591796.2591805; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bhattacharya S, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1, DOI 10.1109/TNSE.2018.2821598; Blanchet J., 2017, P MACH LEARN RES, V77, P97; Blanchet Jose, 2018, ABS181007717 CORR; Bonneel N, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024192; Bubeck S., 2015, FDN TRENDS MACHINE L; Cohen MB, 2017, ANN IEEE SYMP FOUND, P902, DOI 10.1109/FOCS.2017.88; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Diakonikolas J, 2019, SIAM J OPTIMIZ, V29, P660, DOI 10.1137/18M1172314; Dvurechensky P, 2018, PR MACH LEARN RES, V80; Esfahani PM, 2018, MATH PROGRAM, V171, P115, DOI 10.1007/s10107-017-1172-1; Hopcroft J. E., 1973, SIAM Journal on Computing, V2, P225, DOI 10.1137/0202019; Kelner J. A., 2014, P 25 ANN ACM SIAM S, P217; Lahn Nathaniel, 2019, ABS190511830 CORR; Lee YT, 2015, ANN IEEE SYMP FOUND, P230, DOI 10.1109/FOCS.2015.23; Lee YT, 2014, ANN IEEE SYMP FOUND, P424, DOI 10.1109/FOCS.2014.52; Lin Tianyi, 2019, ABS190106482 CORR; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y, 2007, MATH PROGRAM, V109, P319, DOI 10.1007/s10107-006-0034-z; Panaretos VM, 2016, ANN STAT, V44, P771, DOI 10.1214/15-AOS1387; Quanrud K., 2019, S SIMPL ALG; Sankowski P, 2009, THEOR COMPUT SCI, V410, P4480, DOI 10.1016/j.tcs.2009.07.028; Sharathkumar R, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P385; Sherman J, 2017, ACM S THEORY COMPUT, P452, DOI 10.1145/3055399.3055501; Sherman J, 2013, ANN IEEE SYMP FOUND, P263, DOI 10.1109/FOCS.2013.36; Sidford Aaron, 2018, 59 ANN IEEE S FDN CO; Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963; Young NE, 2001, ANN IEEE SYMP FOUND, P538, DOI 10.1109/SFCS.2001.959930	36	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903004
C	Janzing, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Janzing, Dominik			Causal Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				IDENTIFICATION	We argue that regularizing terms in standard regression methods not only help against overfitting finite data, but sometimes also help in getting better causal models. We first consider a multi-dimensional variable linearly influencing a target variable with some multi-dimensional unobserved common cause, where the confounding effect can be decreased by keeping the penalizing term in Ridge and Lasso regression even in the population limit. The reason is a close analogy between overfitting and confounding observed for our toy model. In the case of overfitting, we can choose regularization constants via cross validation, but here we choose the regularization constant by first estimating the strength of confounding, which yielded reasonable results for simulated and real data. Further, we show a 'causal generalization bound' which states (subject to our particular model of confounding) that the error made by interpreting any non-linear regression as causal model can be bounded from above whenever functions are taken from a not too rich class.	[Janzing, Dominik] Amazon Res Tubingen, Tubingen, Germany		Janzing, D (corresponding author), Amazon Res Tubingen, Tubingen, Germany.	janzind@amazon.com						[Anonymous], 2002, LEARNING KERNELS; [Anonymous], 2000, CAUSALITY; Bahadori M. T., 2017, ARXIV170202604; BEUTLER FJ, 1965, J MATH ANAL APPL, V10, P451, DOI 10.1016/0022-247X(65)90108-3; Chernozhukov V, 2018, ECONOMET J, V21, pC1, DOI 10.1111/ectj.12097; Dua D., 2017, UCI MACHINE LEARNING; HASTIE T, 2001, ELEMENTS STATISTICAL; Heinze-Deml C., 2017, ARXIV171011469; Heinze-Deml C, 2018, J CAUSAL INFERENCE, V6, DOI 10.1515/jci-2017-0016; Hoerl AE, 2000, TECHNOMETRICS, V42, P80, DOI 10.2307/1271436; Hoyer PO, 2008, INT J APPROX REASON, V49, P362, DOI 10.1016/j.ijar.2008.02.006; IMBENS GW, 1994, ECONOMETRICA, V62, P467, DOI 10.2307/2951620; Janzing D., 2009, PROC 25 C UNCERTAINT, P249; Janzing D., 2018, P 35 INT C MACH LEAR; Janzing D, 2018, J CAUSAL INFERENCE, V6, DOI 10.1515/jci-2017-0013; Lopez-Paz D, 2015, PR MACH LEARN RES, V37, P1452; Peters J, 2016, J R STAT SOC B, V78, P947, DOI 10.1111/rssb.12167; Raskutti Garvesh, 2011, 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1318; Rothenhausler D., 2018, ARXIV PREPRINT ARXIV; Rubin DB, 2004, SCAND J STAT, V31, P161, DOI 10.1111/j.1467-9469.2004.02-123.x; Shen Z., 2017, ARXIV170806656; Tibshirani R., 2015, COURSE STAT MACHINE; Vapnik V., 1998, STATISTICALLEARNING	23	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904036
C	Jezequel, R; Gaillard, P; Rudi, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jezequel, Remi; Gaillard, Pierre; Rudi, Alessandro			Efficient online learning with Kernels for adversarial large scale problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We are interested in a framework of online learning with kernels for low-dimensional, but large-scale and potentially adversarial datasets. We study the computational and theoretical performance of online variations of kernel Ridge regression. Despite its simplicity, the algorithm we study is the first to achieve the optimal regret for a wide range of kernels with a per-round complexity of order n(alpha) with alpha < 2. The algorithm we consider is based on approximating the kernel with the linear span of basis functions. Our contributions are twofold: 1) For the Gaussian kernel, we propose to build the basis beforehand (independently of the data) through Taylor expansion. For d-dimensional inputs, we provide a (close to) optimal regret of order O((log n)(d+1)) with per-round time complexity and space complexity O ((log n)(2d)). This makes the algorithm a suitable choice as soon as n >> e(d) which is likely to happen in a scenario with small dimensional and large-scale dataset; 2) For general kernels with low effective dimension, the basis functions are updated sequentially, adapting to the data, by sampling Nystrom points. In this case, our algorithm improves the computational trade-off known for online kernel regression.	[Jezequel, Remi; Gaillard, Pierre; Rudi, Alessandro] PSL Res Univ, Ecole Normale Super, INRIA, Dept Informat, Paris, France	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Jezequel, R (corresponding author), PSL Res Univ, Ecole Normale Super, INRIA, Dept Informat, Paris, France.	remi.jezequel@inria.fr; pierre.gaillard@inria.fr; alessandro.rudi@inria.fr						Altschuler J., 2018, ARXIV181205189; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Azoury KS, 2001, MACH LEARN, V43, P211, DOI 10.1023/A:1010896012157; Bach F., 2017, J MACH LEARN RES, V18, p1 629 681; Bartlett Peter L., 2015, JMLR WORKSHOP C P, V40, P1; Berlinet A., 2011, REPRODUCING KERNEL H; Calandriello Daniele, 2017, INT C MACH LEARN; Calandriello Daniele, 2017, NEURAL INFORM PROCES; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Cotter A., 2011, ARXIV11094603; Derezinski Michal, 2018, ARXIV181002453; FOSTER DP, 1991, ANN STAT, V19, P1084, DOI 10.1214/aos/1176348140; Gaillard Pierre, 2018, ARXIV180511386; Hazan E, 2007, LECT NOTES COMPUT SC, V4539, P499, DOI 10.1007/978-3-540-72927-3_36; Lu J, 2016, J MACH LEARN RES, V17; Olver FWJ., 2010, NIST HDB MATH FUNCTI; Rakhlin A., 2013, BERNOULLI; Rudi A., 2015, ADV NEURAL INFORM PR, V28, P1657, DOI DOI 10.5555/2969239.2969424; Rudi A., 2017, ADV NEURAL INFORM PR, V30, P3888; Rudi A., 2017, ADV NEURAL INFORM PR, V30, P3215; Smola A. J., 2000, P 17 INT C MACH LEAR, P911; Steinwart I, 2006, IEEE T INFORM THEORY, V52, P4635, DOI 10.1109/TIT.2006.881713; Vovk V, 2001, INT STAT REV, V69, P213; Vovk V., 2006, METRIC ENTROPY COMPE; Vovk V., 2005, ON LINE REGRESSION C; Williams CKI, 2001, ADV NEUR IN, V13, P682; Zhang X, 2019, PR MACH LEARN RES, V97; Zhdanov F, 2010, LECT NOTES ARTIF INT, V6331, P405	30	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901010
C	Khalvati, K; Mirbagheri, S; Park, SA; Dreher, JC; Rao, RPN		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Khalvati, Koosha; Mirbagheri, Saghar; Park, Seongmin A.; Dreher, Jean-Claude; Rao, Rajesh P. N.			A Bayesian Theory of Conformity in Collective Decision Making	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In collective decision making, members of a group need to coordinate their actions in order to achieve a desirable outcome. When there is no direct communication between group members, one must decide based on inferring others' intentions from their actions. The inference of others' intentions is called "theory of mind" and can involve different levels of reasoning, from a single inference of a hidden variable to considering others partially or fully optimal and reasoning about their actions conditioned on one's own actions (levels of "theory of mind"). In this paper, we present a new Bayesian theory of collective decision making based on a simple yet most commonly observed behavior: conformity. We show that such a Bayesian framework allows one to achieve any level of theory of mind in collective decision making. The viability of our framework is demonstrated on two different experiments, a consensus task with 120 subjects and a volunteer's dilemma task with 29 subjects, each with multiple conditions.	[Khalvati, Koosha; Rao, Rajesh P. N.] Univ Washington, Paul G Allen Sch CSE, Seattle, WA 98195 USA; [Mirbagheri, Saghar] NYU, Dept Psychol, New York, NY 10003 USA; [Park, Seongmin A.] Univ Calif Davis, Ctr Mind & Brain, Davis, CA 95616 USA; [Dreher, Jean-Claude] Inst Sci Cognit Marc Jeannerod, Neuroecon Lab, Bron, France; [Rao, Rajesh P. N.] Univ Washington, Ctr Neurotechnol, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle; New York University; University of California System; University of California Davis; University of Washington; University of Washington Seattle	Khalvati, K (corresponding author), Univ Washington, Paul G Allen Sch CSE, Seattle, WA 98195 USA.	koosha@cs.washington.edu; sm7369@nyu.edu; park@isc.cnrs.fr; dreher@isc.cnrs.fr; rao@cs.washington.edu		Rao, Rajesh P. N./0000-0003-0682-8952; Park, Seongmin/0000-0002-8361-2277	Templeton World Charity Foundation grant, CRCNS NIMH grant [5R01MH112166-03]; NSF [EEC-1028725]; NSF-ANR Collaborative Research in Computational Neuroscience 'CRCNS SOCIAL POMDP' [16-NEUC]	Templeton World Charity Foundation grant, CRCNS NIMH grant; NSF(National Science Foundation (NSF)); NSF-ANR Collaborative Research in Computational Neuroscience 'CRCNS SOCIAL POMDP'	This work was funded by a Templeton World Charity Foundation grant, CRCNS NIMH grant no. 5R01MH112166-03, and NSF grant no. EEC-1028725 and an NSF-ANR Collaborative Research in Computational Neuroscience 'CRCNS SOCIAL POMDP' no.16-NEUC grant. We thank Shinsuke Suzuki and John P. O'Doherty for providing us their data from the consensus task.	[Anonymous], 1991, CAMB MASS; Bahrami B, 2010, SCIENCE, V329, P1081, DOI 10.1126/science.1185718; Baker CL, 2017, NAT HUM BEHAV, V1, DOI 10.1038/s41562-017-0064; BARONCOHEN S, 1985, COGNITION, V21, P37, DOI 10.1016/0010-0277(85)90022-8; BECKER GM, 1964, BEHAV SCI, V9, P226, DOI 10.1002/bs.3830090304; Cialdini R. B., 1998, HDB SOCIAL PSYCHOL, V1-2, P151; CLARK CW, 1986, THEOR POPUL BIOL, V30, P45, DOI 10.1016/0040-5809(86)90024-9; Devaine M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0087619; Fehr E, 2003, NATURE, V425, P785, DOI 10.1038/nature02043; Gmytrasiewicz PJ, 2005, J ARTIF INTELL RES, V24, P49, DOI 10.1613/jair.1579; Hula A, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004254; Johnson Allen, 2000, EVOLUTION HUMAN SOC; Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X; Kendal RL, 2004, BEHAV ECOL, V15, P269, DOI 10.1093/beheco/arh008; Khalvati K., 2016, P ADV NEURAL INFORM, P2901; Khalvati K, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aax8783; McAllister P. H., 1991, Annals of Operations Research, V30, P45, DOI 10.1007/BF02204808; Mookherjee D, 1997, GAME ECON BEHAV, V19, P97, DOI 10.1006/game.1997.0540; Murphy Kevin P., 2012, ADAPTIVE COMPUTATION; Naber M, 2013, P NATL ACAD SCI USA, V110, P20046, DOI 10.1073/pnas.1305996110; Park SA, 2017, PLOS BIOL, V15, DOI 10.1371/journal.pbio.2001958; Ross Stephane, 2008, J ARTIFICIAL INTELLI, V32; Sherif M., 1936, PSYCHOL SOCIAL NORMS; Shum Michael, 2019, CORR; Suzuki S, 2015, NEURON, V86, P591, DOI 10.1016/j.neuron.2015.03.019; Szolnoki A, 2015, J R SOC INTERFACE, V12, DOI 10.1098/rsif.2014.1299; Thornton MA, 2019, J NEUROSCI, V39, P140, DOI 10.1523/JNEUROSCI.1431-18.2018; Thrun S., 2005, PROBABILISTIC ROBOTI; TSITSIKLIS JN, 1994, MACH LEARN, V16, P185, DOI 10.1023/A:1022689125041; van de Waal E, 2013, SCIENCE, V340, P483, DOI 10.1126/science.1232769; Whiten A, 2005, NATURE, V437, P737, DOI 10.1038/nature04047; Yoshida W., 2008, PLOS COMPUT BIOL, V4, DOI [DOI 10.1371/journal.pcbi.1000254, 10.1371/journal.pcbi.1000254]	32	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901034
C	Kim, B; Tewari, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kim, Baekjin; Tewari, Ambuj			On the Optimality of Perturbations in Stochastic and Adversarial Multi-armed Bandit Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We investigate the optimality of perturbation based algorithms in the stochastic and adversarial multi-armed bandit problems. For the stochastic case, we provide a unified regret analysis for both sub-Weibull and bounded perturbations when rewards are sub-Gaussian. Our bounds are instance optimal for sub-Weibull perturbations with parameter 2 that also have a matching lower tail bound, and all bounded support perturbations where there is sufficient probability mass at the extremes of the support. For the adversarial setting, we prove rigorous barriers against two natural solution approaches using tools from discrete choice theory and extreme value theory. Our results suggest that the optimal perturbation, if it exists, will be of Frechet-type.	[Kim, Baekjin; Tewari, Ambuj] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Kim, B (corresponding author), Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.	baekjin@umich.edu; tewaria@umich.edu			NSF CAREER [IIS-1452099]; UM-LSA Associate Professor Support Fund; Sloan Research Fellowship	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); UM-LSA Associate Professor Support Fund; Sloan Research Fellowship(Alfred P. Sloan Foundation)	We acknowledge the support of NSF CAREER grant IIS-1452099 and the UM-LSA Associate Professor Support Fund. AT was also supported by a Sloan Research Fellowship.	Abernethy J. D., 2015, ADV NEURAL INFORM PR, P2197; Abernethy J. D., 2014, COLT, P807; Agrawal S., 2013, ARTIF INTELL, P99; Audibert J.-Y., 2009, P COLT; Audibert JY, 2010, J MACH LEARN RES, V11, P2785; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Coles S., 2001, INTRO STAT MODELING, V208, P208, DOI [10.1007/978-1-4471-3675-0, DOI 10.1007/978-1-4471-3675-0]; Devroye L., 2013, P 25 ANN C LEARN THE, P460; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Hazan T, 2017, PERTURBATIONS OPTIMI; Hoeffding W., 1962, COLLECTED WORKS WASS, DOI [10.1007/978-1-4612-0865-5_26, DOI 10.1007/978-1-4612-0865-5_26]; Hofbauer J, 2002, ECONOMETRICA, V70, P2265, DOI 10.1111/j.1468-0262.2002.00440.x; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Kujala J, 2005, LECT NOTES ARTIF INT, V3734, P371; Kuleshov Volodymyr, 2014, ARXIV14026028; Lattimore Tor, 2018, PREPRINT; Leadbetter M. R., 2012, EXTREMES RELATED PRO; McFadden D., 1981, ECONOMETRIC MODELS P; Resnick S.I., 2013, EXTREME VALUES REGUL; Scott SL, 2010, APPL STOCH MODEL BUS, V26, P639, DOI 10.1002/asmb.874; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Tossou A. C., 2017, P 31 AAAI C ART INT; Tossou ACY, 2016, AAAI CONF ARTIF INTE, P2087; van Erven T., 2014, COLT, P949; Wong Kam Chung, 2019, ANN STAT	29	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302066
C	Kim, GS; Paik, MC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kim, Gi-Soo; Paik, Myunghee Cho			Doubly-Robust Lasso Bandit	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SELECTION	Contextual multi-armed bandit algorithms are widely used in sequential decision tasks such as news article recommendation systems, web page ad placement algorithms, and mobile health. Most of the existing algorithms have regret proportional to a polynomial function of the context dimension, d. In many applications however, it is often the case that contexts are high-dimensional with only a sparse subset of size s(0) (<< d) being correlated with the reward. We consider the stochastic linear contextual bandit problem and propose a novel algorithm, namely the Doubly-Robust Lasso Bandit algorithm, which exploits the sparse structure of the regression parameter as in Lasso, while blending the doubly-robust technique used in missing data literature. The high-probability upper bound of the regret incurred by the proposed algorithm does not depend on the number of arms and scales with log(d) instead of a polynomial function of d. The proposed algorithm shows good performance when contexts of different arms are correlated and requires less tuning parameters than existing methods.	[Kim, Gi-Soo; Paik, Myunghee Cho] Seoul Natl Univ, Dept Stat, Seoul, South Korea	Seoul National University (SNU)	Kim, GS (corresponding author), Seoul Natl Univ, Dept Stat, Seoul, South Korea.	gisoo1989@snu.ac.kr; myungheechopaik@snu.ac.kr						Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Abbasi-Yadkori Yasin, 2012, JMLR WORKSHOP C P, P1; Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Bang H, 2005, BIOMETRICS, V61, P962, DOI 10.1111/j.1541-0420.2005.00377.x; Bastani H, 2015, ONLINE DECISION MAKI; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Carpentier Alexandra, 2012, ARTIF INTELL, P190; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Chu W, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1097; Dani V, 2008, P C LEARN THEOR COLT, P355; Dimakopoulou M., 2018, ARXIV181206227; Farajtabar M., 2018, ICML 18, P1446; Ferreira KJ, 2018, OPER RES, V66, P1586, DOI 10.1287/opre.2018.1755; Gilton D, 2017, 2017 INTERNATIONAL CONFERENCE ON SAMPLING THEORY AND APPLICATIONS (SAMPTA), P518, DOI 10.1109/SAMPTA.2017.8024429; He J, 2010, NINTH WUHAN INTERNATIONAL CONFERENCE ON E-BUSINESS, VOLS I-III, P1061; Jiang N, 2016, PR MACH LEARN RES, V48; Kawale J., 2015, ADV NEURAL INFORM PR, V28, P1297; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Langford J., 2008, P 25 INT C MACHINE L, P528; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; ROBINS JM, 1994, J AM STAT ASSOC, V89, P846, DOI 10.2307/2290910; Schwartz EM, 2017, MARKET SCI, V36, P500, DOI 10.1287/mksc.2016.1023; Tewari A., 2017, MOBILE HLTH, P495; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; van de Geer S., 2007, JSM P; Wang XY, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON SMART CITY AND SYSTEMS ENGINEERING (ICSCSE), P187, DOI 10.1109/ICSCSE.2018.00044; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729	31	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305082
C	Kim, T; Ghosh, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kim, Taewan; Ghosh, Joydeep			On Single Source Robustness in Deep Fusion Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Algorithms that fuse multiple input sources benefit from both complementary and shared information. Shared information may provide robustness against faulty or noisy inputs, which is indispensable for safety-critical applications like self-driving cars. We investigate learning fusion algorithms that are robust against noise added to a single source. We first demonstrate that robustness against single source noise is not guaranteed in a linear fusion model. Motivated by this discovery, two possible approaches are proposed to increase robustness: a carefully designed loss with corresponding training algorithms for deep fusion models, and a simple convolutional fusion layer that has a structural advantage in dealing with noise. Experimental results show that both training algorithms and our fusion layer make a deep fusion-based 3D object detector robust against noise applied to a single source, while preserving the original performance on clean data.	[Kim, Taewan; Ghosh, Joydeep] Univ Texas Austin, Austin, TX 78712 USA; [Kim, Taewan] Amazon, Seattle, WA USA	University of Texas System; University of Texas Austin; Amazon.com	Kim, T (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	twankim@utexas.edu; jghosh@utexas.edu						Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Braun M, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P1546, DOI 10.1109/ITSC.2016.7795763; Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621; Chen Xiaozhi, 2017, P IEEE C COMP VIS PA, DOI 10.1109/cvpr.2017.691.2017; Chiu CC, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4774; Chorowski I. K., 2015, ADV NEURAL INFORM PR, V28, P577, DOI DOI 10.1016/0167-739X(94)90007-8; Chung JS, 2017, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2017.367; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Du XX, 2017, IEEE INT C INT ROBOT, P749; Feng D, 2021, IEEE T INTELL TRANSP, V22, P1341, DOI 10.1109/TITS.2020.2972974; Geiger A., 2012, P IEEE COMP SOC C CO; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang J, 2013, INT CONF ACOUST SPEE, P7596, DOI 10.1109/ICASSP.2013.6639140; Kim Jaekyum, 2018, AS C COMP VIS ACCV; Kim T, 2018, IEEE INT C INTELL TR, P2712, DOI 10.1109/ITSC.2018.8569987; Kim T, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P271, DOI 10.1109/ITSC.2016.7795566; Kiros R, 2014, LECT NOTES COMPUT SC, V8679, P25, DOI 10.1007/978-3-319-10581-9_4; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049; Liang M, 2019, PROC CVPR IEEE, P7337, DOI 10.1109/CVPR.2019.00752; Liang M, 2018, LECT NOTES COMPUT SC, V11220, P663, DOI 10.1007/978-3-030-01270-0_39; Liu SQ, 2015, IEEE T BIO-MED ENG, V62, P1132, DOI 10.1109/TBME.2014.2372011; Mees O, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P151, DOI 10.1109/IROS.2016.7759048; Mroueh Y, 2015, INT CONF ACOUST SPEE, P2130, DOI 10.1109/ICASSP.2015.7178347; Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102; Ramachandram D, 2017, IEEE SIGNAL PROC MAG, V34, P96, DOI 10.1109/MSP.2017.2738401; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707; Sainath TN, 2013, INT CONF ACOUST SPEE, P8614, DOI 10.1109/ICASSP.2013.6639347; Simonovsky Martin, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9902, P10, DOI 10.1007/978-3-319-46726-9_2; Sui C, 2015, IEEE I CONF COMP VIS, P154, DOI 10.1109/ICCV.2015.26; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tsipras Dimitris, 2019, ROBUSTNESS MAY BE OD, V1, P2; Tumer K., 1996, Connection Science, V8, P385, DOI 10.1080/095400996116839; Tumer K, 1996, PATTERN RECOGN, V29, P341, DOI 10.1016/0031-3203(95)00085-2; Valada Abhinav, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4644, DOI 10.1109/ICRA.2017.7989540; Wang ZC, 2018, ROCK MECH ROCK ENG, V51, P1457, DOI 10.1007/s00603-018-1422-4; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wu P., 2013, PROC 21 ACM INT C MU, P153, DOI [10.1145/2502081.2502112, DOI 10.1145/2502081.2502112]	48	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304078
C	Kim, Y; Nam, S; Cho, I; Kim, SJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kim, Yunji; Nam, Seonghyeon; Cho, In; Kim, Seon Joo			Unsupervised Keypoint Learning for Guiding Class-Conditional Video Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a deep video prediction model conditioned on a single image and an action class. To generate future frames, we first detect keypoints of a moving object and predict future motion as a sequence of keypoints. The input image is then translated following the predicted keypoints sequence to compose future frames. Detecting the keypoints is central to our algorithm, and our method is trained to detect the keypoints of arbitrary objects in an unsupervised manner. Moreover, the detected keypoints of the original videos are used as pseudo-labels to learn the motion of objects. Experimental results show that our method is successfully applied to various datasets without the cost of labeling keypoints in videos. The detected keypoints are similar to human-annotated labels, and prediction results are more realistic compared to the previous methods.	[Kim, Yunji; Nam, Seonghyeon; Cho, In; Kim, Seon Joo] Yonsei Univ, Seoul, South Korea; [Kim, Seon Joo] Facebook, Menlo Pk, CA USA	Yonsei University; Facebook Inc	Kim, Y (corresponding author), Yonsei Univ, Seoul, South Korea.	kim_yunji@yonsei.ac.kr; shnnam@yonsei.ac.kr; join@yonsei.ac.kr; seonjookim@yonsei.ac.kr			Samsung Research Funding Center of Samsung Electronics [SRFC-IT1701-01]	Samsung Research Funding Center of Samsung Electronics(Samsung)	This work was supported by Samsung Research Funding Center of Samsung Electronics under Project Number SRFC-IT1701-01.	[Anonymous], 2014, ICLR; Babaeizadeh M., 2018, ICLR; Balakrishnan Guha, 2018, CVPR; Cai H., 2018, ECCV; Carreira J., 2017, CVPR; de Brabandere B., 2016, NEURIPS; Denton Emily, 2018, ICML; Denton Emily L, 2017, NEURIPS; Dibeklioglu Hamdi, 2012, ECCV; Ebert F., 2017, CORL; Finn Chelsea, 2016, NEURIPS; Goodfellow Ian, 2014, 27 INT C NEURAL INFO; Hochreiter S., 1997, NEURAL COMPUT; Jakab Tomas, 2018, NEURIPS; Johnson J, 2016, ECCV; Kalchbrenner N., 2017, ICML; Kingma D.P, P 3 INT C LEARNING R; Lee Alex X, 2018, ARXIV180401523; Li Y., 2018, ECCV; Ma L., 2017, NEURIPS NEURIPS; Mathieu Michael, 2016, P INT C LEARN REPR I; Mejjati Y.A., 2018, NEURIPS; Newell A., 2016, ECCV; Reed S., 2017, ICML; Reed Scott E., 2015, NEURIPS; Russakovsky O., 2015, INT J COMPUT VISION, V115, P211; Schuldt C, 2004, ICPR; Siarohin Aliaksandr, 2019, CVPR, P2; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Sohn K., 2015, NEURIPS; Soomro K., 2012, ARXIVPREPRINT, P2556; Srivastava Nitish, 2015, ICML; Thewlis J., 2017, ICCV; Tulyakov S., 2018, CVPR; Unterthiner Thomas, 2018, ARXIV181201717; Villegas Ruben, 2017, ICML; Villegas Ruben, 2017, ICLR; Wang Wei, 2018, CVPR; Wichers Nevan, 2018, ICML; Zhang W., 2013, ICCV; Zhang Y., 2018, CVPR; Zhou Tinghui, 2018, ARXIV180807371; Zisserman Andrew, 2014, NEURIPS	44	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303076
C	Kolobov, A; Peres, Y; Lu, C; Horvitz, E		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kolobov, Andrey; Peres, Yuval; Lu, Cheng; Horvitz, Eric			Staying up to Date with Online Content Changes Using Reinforcement Learning for Scheduling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				POLICIES	From traditional Web search engines to virtual assistants and Web accelerators, services that rely on online information need to continually keep track of remote content changes by explicitly requesting content updates from remote sources (e.g., web pages). We propose a novel optimization objective for this setting that has several practically desirable properties, and efficient algorithms for it with optimality guarantees even in the face of mixed content change observability and initially unknown change model parameters. Experiments on 18.5M URLs crawled daily for 14 weeks show significant advantages of this approach over prior art.	[Kolobov, Andrey; Horvitz, Eric] Microsoft Res, Redmond, WA 98052 USA; [Lu, Cheng] Microsoft Bing, Bellevue, WA 98004 USA	Microsoft	Kolobov, A (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	akolobov@microsoft.com; yperes@gmail.com; Cheng.Lu@microsoft.com; horvitz@microsoft.com						Anily S, 1998, DISCRETE APPL MATH, V82, P27, DOI 10.1016/S0166-218X(97)00119-4; [Anonymous], 2020, REINFORCEMENT LEARNI; Azar Y., 2018, P NATL ACAD SCI USA; Bar-Noy A., 1998, P 9 ANN ACM SIAM S D, P11; Broder Andrei Z., 1997, P 6 INT WORLD WID WE, p1157 , DOI [DOI 10.1016/S0169-7552(97)00031-7, 10.1016/S0169-7552(97)00031-7]; Bubeck S., 2015, FDN TRENDS MACHINE L; Burden R.L., 1985, NUMERICAL ANAL, Vthird; Charikar M.S., 2002, P 34 ANN ACM S THEOR, V34, P380, DOI DOI 10.1145/509907.509965; Cho J, 2003, ACM T DATABASE SYST, V28, P390, DOI 10.1145/958942.958945; Cho J., 2000, VLDB; Cho J., 2002, VLDB; Cho J., 2003, ACM T INTERNET TECHN, V3, P256, DOI [10.1145/857166.857170, DOI 10.1145/857166.857170]; Cho J., 2000, ACM SIGMOD INT C MAN; Coffman E. G., 1998, J SCHEDULING, V1; Gal A, 2001, J ACM, V48, P1141, DOI 10.1145/504794.504797; Glazebrook KD, 2005, EUR J OPER RES, V165, P267, DOI 10.1016/j.ejor.2004.01.036; Glazebrook KD, 2002, NAV RES LOG, V49, P706, DOI 10.1002/nav.10036; Ibaraki Toshihide, 1988, RESOURCE ALLOCATION; Immorlica N., 2018, FOCS; Karimi M. R., 2016, ACM KDD; KIEFER J, 1953, P AM MATH SOC, V4, P502, DOI 10.2307/2032161; Kolobov A., 2019, SIGIR; Olston Christopher, 2010, Foundations and Trends in Information Retrieval, V4, P175, DOI 10.1561/1500000017; Olston C., 2008, PROCEEDING WWW 2008, P437; Page L., 1999, TECHNICAL REPORT 199; Pandey S., 2004, VLDB; Pandey S., 2005, WWW; Pandey S., 2003, WWW; Radinsky Kira, 2013, P 6 ACM INT C WEB SE, P415; Taylor H., 1998, INTRO STOCHASTIC MOD, V3rd; Upadhyay U., 2018, NEURIPS; van den Bosch A., 2015, ISSI; Wang YC, 2017, PR MACH LEARN RES, V70; Whittle P., 1988, J APPL PROBAB, V25, P287, DOI DOI 10.2307/3214163; Wolf J. L., 2002, WWW; Zarezade A., 2017, ACM KDD	36	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300053
C	Kuditipudi, R; Wang, X; Lee, H; Zhang, Y; Li, ZY; Hu, W; Arora, S; Ge, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kuditipudi, Rohith; Wang, Xiang; Lee, Holden; Zhang, Yi; Li, Zhiyuan; Hu, Wei; Arora, Sanjeev; Ge, Rong			Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Mode connectivity (Garipov et al., 2018; Draxler et al., 2018) is a surprising phenomenon in the loss landscape of deep nets. Optima-at least those discovered by gradient-based optimization-turn out to be connected by simple paths on which the loss function is almost constant. Often, these paths can be chosen to be piece-wise linear, with as few as two segments. We give mathematical explanations for this phenomenon, assuming generic properties (such as dropout stability and noise stability) of well-trained deep nets, which have previously been identified as part of understanding the generalization properties of deep nets. Our explanation holds for realistic multilayer nets, and experiments are presented to verify the theory.	[Kuditipudi, Rohith; Wang, Xiang; Ge, Rong] Duke Univ, Durham, NC 27706 USA; [Lee, Holden; Zhang, Yi; Li, Zhiyuan; Hu, Wei; Arora, Sanjeev] Princeton Univ, Princeton, NJ 08544 USA; [Arora, Sanjeev] Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA	Duke University; Princeton University; Institute for Advanced Study - USA	Kuditipudi, R (corresponding author), Duke Univ, Durham, NC 27706 USA.	rohith.kuditipudi@duke.edu; xwwang@cs.duke.edu; holdenl@princeton.edu; y.zhang@cs.princeton.edu; zhiyuanli@cs.princeton.edu; huwei@cs.princeton.edu; arora@cs.princeton.edu; rongge@cs.duke.edu	li, zhiyuan/HGD-9581-2022		NSF [CCF-1704656, CCF-1845171]; Sloan Fellowship; Google Faculty Research Award; ONR; Simons Foundation; Schmidt Foundation; Amazon Research; DARPA; SRC	NSF(National Science Foundation (NSF)); Sloan Fellowship(Alfred P. Sloan Foundation); Google Faculty Research Award(Google Incorporated); ONR(Office of Naval Research); Simons Foundation; Schmidt Foundation; Amazon Research; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); SRC	Rong Ge acknowledges funding from NSF CCF-1704656, NSF CCF-1845171 (CAREER), the Sloan Fellowship and Google Faculty Research Award. Sanjeev Arora acknowledges funding from the NSF, ONR, Simons Foundation, Schmidt Foundation, Amazon Research, DARPA and SRC.	Arora Sanjeev, 2018, ARXIV180205296; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Draxler Felix, 2018, ARXIV180300885; Freeman C.D., 2016, ARXIV161101540; Garipov T, 2018, ADV NEUR IN, V31; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Keshari R., 2018, ARXIV181203965; Liang S., 2018, INT C MACH LEARN, P2840; Morcos A. S., 2018, ICLR POSTER; Nguyen Q., 2018, INT C LEARN REPR; Nguyen Q, 2019, PR MACH LEARN RES, V97; Safran I, 2018, PR MACH LEARN RES, V80; Simonyan K., 2015, ICLR; Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; Venturi L., 2018, ARXIV PREPRINT ARXIV; Yun C., 2018, ARXIV180203487	18	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906028
C	Kumor, D; Chen, B; Bareinboim, E		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kumor, Daniel; Chen, Bryant; Bareinboim, Elias			Efficient Identification in Linear Structural Causal Models with Instrumental Cutsets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					One of the most common mistakes made when performing data analysis is attributing causal meaning to regression coefficients. Formally, a causal effect can only be computed if it is identifiable from a combination of observational data and structural knowledge about the domain under investigation (Pearl, 2000, Ch. 5). Building on the literature of instrumental variables (IVs), a plethora of methods has been developed to identify causal effects in linear systems. Almost invariably, however, the most powerful such methods rely on exponential-time procedures. In this paper, we investigate graphical conditions to allow efficient identification in arbitrary linear structural causal models (SCMs). In particular, we develop a method to efficiently find unconditioned instrumentalsubsets, which are generalizations of IVs that can be used to tame the complexity of many canonical algorithms found in the literature. Further, we prove that determining whether an effect can be identified with TSID (Weihs et al., 2017), a method more powerful than unconditioned instrumental sets and other efficient identification algorithms, is NP-Complete. Finally, building on the idea of flow constraints, we introduce a new and efficient criterion called Instrumental Cutsets (IC), which is able to solve for parameters missed by all other existing polynomial-time algorithms.	[Kumor, Daniel] Purdue Univ, W Lafayette, IN 47907 USA; [Chen, Bryant] Brex Inc, San Francisco, CA USA; [Bareinboim, Elias] Columbia Univ, New York, NY 10027 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus; Columbia University	Kumor, D (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.	dkumor@purdue.edu; bryant@brex.com; eb@cs.columbia.edu			NSF [IIS-1704352, IIS-1750807]; IBM Research, and Adobe Research	NSF(National Science Foundation (NSF)); IBM Research, and Adobe Research(International Business Machines (IBM))	Bareinboim and Kumor are supported in parts by grants from NSF IIS-1704352, IIS-1750807 (CAREER), IBM Research, and Adobe Research. Part of Chen's contributions were made while at IBM Research.	Bardet Magali, 2002, COMPLEXITY GR6BNER B, P8; Bareinboim E, 2016, P NATL ACAD SCI USA, V113, P7345, DOI 10.1073/pnas.1510507113; Bekker Paul A., 1994, IDENTIFICATION EQUIV; Bowden Roger J, 1984, INSTRUMENTAL VARIABL, V8; Brito C., 2002, P 18 C UNC ART INT, P85; Brito C., 2004, THESIS; Chen B., 2016, IJCAI, P3577; Chen Bryant, 2016, ADV NEURAL INFORM PR, V29, P1587; Chen Bryant, 2014, R432 UCLA COGN SYST; Chen Bryant, 2017, P 34 INT C MACH LEAR, V70, P757; Draisma J, 2013, ADV APPL MATH, V50, P661, DOI 10.1016/j.aam.2013.03.001; Duff I. S., 1978, ACM Transactions on Mathematical Software, V4, P137, DOI 10.1145/355780.355785; Fisher F.M., 1966, IDENTIFICATION PROBL; Foygel R, 2012, ANN STAT, V40, P1682, DOI 10.1214/12-AOS1012; Gessel I.M., 1989, DETERMINANTS PATHS P; Goncalves B, 2016, ARTIF INTELL, V238, P154, DOI 10.1016/j.artint.2016.06.004; Koller D., 2009, PROBABILISTIC GRAPHI; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; PICARD JC, 1982, MATH PROGRAM, V22, P121, DOI 10.1007/BF01581031; Schaefer T.J., 1978, P 10 ANN ACM S THEOR, P216, DOI DOI 10.1145/800133.804350; Spielvogel S., 2010, P 26 C UNC ART INT A, P193; Sridhar N, 1996, COMPUT AIDED DESIGN, V28, P237, DOI 10.1016/0010-4485(96)88488-0; Sullivant S, 2010, ANN STAT, V38, P1665, DOI 10.1214/09-AOS760; Van der Zander Benito, 2016, P 19 INT C ART INT S; Van der Zander Benito, 2015, IJCAI 2015 P 24 INT; Weihs L, 2018, J CAUSAL INFERENCE, V6, DOI 10.1515/jci-2017-0009; Wright PG., 1928, TARIFF ANIMAL VEGETA; Wright S, 1920, J AGRIC RES, V20, P0557	28	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904018
C	Kusumoto, M; Inoue, T; Watanabe, G; Akiba, T; Koyama, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kusumoto, Mitsuru; Inoue, Takuya; Watanabe, Gentaro; Akiba, Takuya; Koyama, Masanori			A Graph Theoretic Framework of Recomputation Algorithms for Memory-Efficient Backpropagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recomputation algorithms collectively refer to a family of methods that aims to reduce the memory consumption of the backpropagation by selectively discarding the intermediate results of the forward propagation and recomputing the discarded results as needed. In this paper, we will propose a novel and efficient recomputation method that can be applied to a wider range of neural nets than previous methods. We use the language of graph theory to formalize the general recomputation problem of minimizing the computational overhead under a fixed memory budget constraint, and provide a dynamic programming solution to the problem. Our method can reduce the peak memory consumption on various benchmark networks by 36% similar to 81%, which outperforms the reduction achieved by other methods.	[Kusumoto, Mitsuru; Watanabe, Gentaro; Akiba, Takuya; Koyama, Masanori] Preferred Networks Inc, Tokyo, Japan; [Inoue, Takuya] Univ Tokyo, Tokyo, Japan	University of Tokyo	Kusumoto, M (corresponding author), Preferred Networks Inc, Tokyo, Japan.	mkusumoto@preferred.jp; inoue-takuya57@g.ecc.u-tokyo.ac.jp; g.wtnb@preferred.jp; akiba@preferred.jp; masomatics@preferred.jp						Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Appel A.W., 2002, MODERN COMPILER IMPL; Chao Peng, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6181, DOI 10.1109/CVPR.2018.00647; Chen Tianqi, 2016, TRAINING DEEP NETS S, V6, P6; Davey B. A., 2002, INTRO LATTICES ORDER, V2nd, DOI DOI 10.1017/CBO9780511809088; Gruslys A, 2016, ADV NEUR IN, V29; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jin H, 2018, ACM T ARCHIT CODE OP, V15, DOI 10.1145/3243904; Kumar R, 2019, ADV NEUR IN, V32; Micikevicius Paulius, 2018, P 6 INT C LEARN REPR, DOI [10.48550/arXiv.1710.03740, DOI 10.1109/CAMAD.2018.8514963]; Pleiss G., 2017, ABS170706990 CORR; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Rhu Minsoo, 2016, 2016 49 ANN IEEE ACM, P1, DOI DOI 10.1109/MICRO.2016.7783721; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tokui S, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2002, DOI 10.1145/3292500.3330756; Wang LN, 2018, ACM SIGPLAN NOTICES, V53, P41, DOI 10.1145/3200691.3178491	20	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301019
C	Laflaquiere, A; Ortiz, MG		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Laflaquiere, Alban; Ortiz, Michael Garcia			Unsupervised Emergence of Egocentric Spatial Structure from Sensorimotor Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LOCALIZATION; PERCEPTION; SPACE	Despite its omnipresence in robotics application, the nature of spatial knowledge and the mechanisms that underlie its emergence in autonomous agents are still poorly understood. Recent theoretical works suggest that the Euclidean structure of space induces invariants in an agent's raw sensorimotor experience. We hypothesize that capturing these invariants is beneficial for sensorimotor prediction and that, under certain exploratory conditions, a motor representation capturing the structure of the external space should emerge as a byproduct of learning to predict future sensory experiences. We propose a simple sensorimotor predictive scheme, apply it to different agents and types of exploration, and evaluate the pertinence of these hypotheses. We show that a naive agent can capture the topology and metric regularity of its sensor's position in an egocentric spatial frame without any a priori knowledge, nor extraneous supervision.	[Laflaquiere, Alban; Ortiz, Michael Garcia] SoftBank Robot Europe, AI Lab, Paris, France		Laflaquiere, A (corresponding author), SoftBank Robot Europe, AI Lab, Paris, France.	alaflaquiere@softbankrobotics.com; mgarciaortiz@softbankrobotics.com						Antonelo E, 2009, IEEE SYS MAN CYBERN, P3818, DOI 10.1109/ICSMC.2009.5346617; Arleo A, 2001, ADV NEUR IN, V13, P89; Banino A, 2018, NATURE, V557, P429, DOI 10.1038/s41586-018-0102-6; Best PJ, 2001, ANNU REV NEUROSCI, V24, P459, DOI 10.1146/annurev.neuro.24.1.459; Bowling M., 2005, P 22 INT C MACHINE L, P65, DOI [10.1145/1102351.1102360, DOI 10.1145/1102351.1102360]; Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754; Cobbe K., 2018, ARXIV181202341, P1; Cueva Christopher J, 2018, ARXIV180307770; Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170; Gustafson NJ, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002235; Ha D., 2018, ARXIV180310122; Higgins I., 2018, ARXIV PREPRINT ARXIV; Higgins Irina, 2016, BETAVAE LEARNING BAS; Hoffmann M, 2010, IEEE T AUTON MENT DE, V2, P304, DOI 10.1109/TAMD.2010.2086454; Jonschkowski R, 2015, AUTON ROBOT, V39, P407, DOI 10.1007/s10514-015-9459-7; Kahn G., 2017, ARXIV170910489; Kant I., 1781, KRITIK REINEN VERNUN; Kingma D.P, P 3 INT C LEARNING R; Klambauer Gnter, 2017, SELF NORMALIZING NEU; Kuipers B, 2000, ARTIF INTELL, V119, P191, DOI 10.1016/S0004-3702(00)00017-5; Kwiatkowski R, 2019, SCI ROBOT, V4, DOI 10.1126/scirobotics.aau9354; Laflaquiere A, 2015, ROBOT AUTON SYST, V71, P49, DOI 10.1016/j.robot.2015.01.003; Laflaquiere A, 2013, IEEE INT C INT ROBOT, P1230, DOI 10.1109/IROS.2013.6696507; Laflaquiere A, 2012, IEEE INT C INT ROBOT, P3255; Laflaquiere Alban, 2018, NEURAL NETWORKS; Marcel Valentin, DO I MOVE MY SENSORS; Mirowski P., 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/j.neuroscience.2018.04.006; Mnih V., 2013, ARXIV PREPRINT ARXIV; Nair V., 2010, ICML, P807; Nicod Jean, 1924, GEOMETRIE MONDE SENS; O'Regan JK, 2001, BEHAV BRAIN SCI, V24, P939, DOI 10.1017/S0140525X01000115; Ortiz Michael Garcia, 2018, ARXIV180506250; Pathak D., 2017, INT C MACH LEARN ICM, V2017; Philipona D, 2003, NEURAL COMPUT, V15, P2029, DOI 10.1162/089976603322297278; POINCARE H, 1895, REV METAPHYS MORALE, V3, P631; Song D., 2018, ARXIV181012282; Stachenfeld KL, 2017, NAT NEUROSCI, V20, P1643, DOI 10.1038/nn.4650; Terekhov AV, 2016, FRONT ROBOT AI, V3, DOI 10.3389/frobt.2016.00004; Thomas Valentin, 2017, ARXIV170801289; Watter Manuel, 2015, ADV NEURAL INFORM PR, V2, P2746; Wayne G., 2018, UNSUPERVISED PREDICT; Weiller D, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0010377	42	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307020
C	Lang, H; Zhang, PC; Xiao, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lang, Hunter; Zhang, Pengchuan; Xiao, Lin			Using Statistics to Automate Stochastic Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				FALSE DISCOVERY RATE	Despite the development of numerous adaptive optimizers, tuning the learning rate of stochastic gradient methods remains a major roadblock to obtaining good practical performance in machine learning. Rather than changing the learning rate at each iteration, we propose an approach that automates the most common hand-tuning heuristic: use a constant learning rate until "progress stops", then drop. We design an explicit statistical test that determines when the dynamics of stochastic gradient descent reach a stationary distribution. This test can be performed easily during training, and when it fires, we decrease the learning rate by a constant multiplicative factor. Our experiments on several deep learning tasks demonstrate that this statistical adaptive stochastic approximation (SASA) method can automatically find good learning rate schedules and match the performance of hand-tuned methods using default settings of its parameters. The statistical testing helps to control the variance of this procedure and improves its robustness.	[Lang, Hunter; Zhang, Pengchuan; Xiao, Lin] Microsoft Res AI, Redmond, WA 98052 USA		Lang, H (corresponding author), Microsoft Res AI, Redmond, WA 98052 USA.	hunter.lang@microsoft.com; penzhan@microsoft.com; lin.xiao@microsoft.com	Zhang, Pengchuan/AAR-3769-2021					Amodei D, 2016, PR MACH LEARN RES, V48; Arvin AM, 2009, LIVE VARIOLA VIRUS: CONSIDERATIONS FOR CONTINUING RESEARCH, P9; Bach F., 2013, ADV NEURAL INFORM PR, V26, P773; Baydin Atilim Gunes, 2018, P 6 INT C LEARN REPR; BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x; Blanchard G, 2009, J MACH LEARN RES, V10, P2837; Dieuleveut Aymeric, 2017, ARXIV170706386; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Flegal JM, 2010, ANN STAT, V38, P1034, DOI 10.1214/09-AOS735; Ge R., 2019, ADV NEURAL INFORM PR, P14977; Gehring J., 2017, P ICML; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Jones GL, 2006, J AM STAT ASSOC, V101, P1537, DOI 10.1198/016214506000000492; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kushner HJ., 2003, STOCHASTIC APPROXIMA; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lindquist MA, 2015, PSYCHOSOM MED, V77, P114, DOI 10.1097/PSY.0000000000000148; McDonald J.H., 2009, HDB BIOL STAT, V2; Pflug G.C, 1983, CP83025 IIASA; PFLUG GC, 1990, MONATSH MATH, V110, P297, DOI 10.1007/BF01301683; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; Schaul T., 2013, INT C MACHINE LEARNI, P343; Streiner DL, 2003, CAN J PSYCHIAT, V48, P756, DOI 10.1177/070674370304801108; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tieleman Tijmen, 2012, LECT 65 RMSPROP DIVI, V4; WASAN M., 1969, STOCHASTIC APPROXIMA; Wilson AC, 2017, ADV NEUR IN, V30; Yaida S., 2018, ARXIV181000004; Zhang J., 2017, CORR	37	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901020
C	Law, HCL; Zhao, PL; Chan, LC; Huang, JZ; Sejdinovic, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Law, Ho Chung Leon; Zhao, Peilin; Chan, Lucian; Huang, Junzhou; Sejdinovic, Dino			Hyperparameter Learning via Distributional Transfer	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Bayesian optimisation is a popular technique for hyperparameter learning but typically requires initial exploration even in cases where similar prior tasks have been solved. We propose to transfer information across tasks using learnt representations of training datasets used in those tasks. This results in a joint Gaussian process model on hyperparameters and data representations. Representations make use of the framework of distribution embeddings into reproducing kernel Hilbert spaces. The developed method has a faster convergence compared to existing baselines, in some cases requiring only a few evaluations of the target objective.	[Law, Ho Chung Leon; Chan, Lucian; Sejdinovic, Dino] Univ Oxford, Oxford OX1 2JD, England; [Zhao, Peilin; Huang, Junzhou] Tencent AI Lap, Bellevue, WA 98004 USA	University of Oxford	Law, HCL; Sejdinovic, D (corresponding author), Univ Oxford, Oxford OX1 2JD, England.; Zhao, PL (corresponding author), Tencent AI Lap, Bellevue, WA 98004 USA.	ho.law@stats.ox.ac.uk; masonzhao@tencent.com; leung.chan@stats.ox.ac.uk; joehhuang@tencent.com; dino.sejdinovic@stats.ox.ac.uk		Sejdinovic, Dino/0000-0001-5547-9213	EPSRC; MRC through the OxWaSP CDT programme [EP/L016710/1]; ERC [FP7/617071]; Alan Turing Institute [EP/N510129/1]; Oxford-Tencent Collaboration on Large Scale Machine Learning	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); MRC through the OxWaSP CDT programme(UK Research & Innovation (UKRI)Medical Research Council UK (MRC)); ERC(European Research Council (ERC)European Commission); Alan Turing Institute; Oxford-Tencent Collaboration on Large Scale Machine Learning	We thank Kaspar Martens, Jin Xu, Wittawat Jitkrittum and Jean-Francois Ton for useful discussions. HCLL is supported by the EPSRC and MRC through the OxWaSP CDT programme (EP/L016710/1). DS is supported in part by the ERC (FP7/617071) and by The Alan Turing Institute (EP/N510129/1). HCLL partially completed this work at Tencent AI Lab, and HCLL and DS are supported in part by the Oxford-Tencent Collaboration on Large Scale Machine Learning.	Abadi M., TENSORFLOW SYSTEM LA; Bardenet Remi, 2013, INT C MACHINE LEARNI, P199; Bishop CM, 2006, PATTERN RECOGNITION; Blanchard Gilles, 2017, ARXIV171107910; Bouchard M, 2013, INT J APPROX REASON, V54, P615, DOI 10.1016/j.ijar.2013.01.006; Feurer M., 2014, ECAI WORKSH MET ALG, V1201, P3; Feurer Matthias, 2015, INITIALIZING BAYESIA; Feurer Matthias, 2018, AUTOML WORKSH ICML, V7; Gaulton A, 2017, NUCLEIC ACIDS RES, V45, pD945, DOI 10.1093/nar/gkw1074; Gomes TAF, 2012, NEUROCOMPUTING, V75, P3, DOI 10.1016/j.neucom.2011.07.005; Gretton Arthur, 2015, NOTES MEAN EMBEDDING; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Hutter F., 2019, AUTOMATIC MACHINE LE; Jones E., 2001, SCIPY OPEN SOURCE SC; Kim Jungtaek, 2017, ARXIV171006219; Kingma D.P, P 3 INT C LEARNING R; Klein A., 2016, FAST BAYESIAN OPTIMI; Law H. C. L., 2018, INT C ART INT STAT A, P1167; McLeod Mark, 2018, P INT C MACH LEARN I; Michie Donald, 1994, MACHINE LEARNING NEU, P2; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Mokus J., 1975, LECT NOTES COMPUTER, P400, DOI [10.1007/3-540-07165-2_55, DOI 10.1007/3-540-07165-2_55, DOI 10.1007/978-3-662-38527-2_55, 10.1007/3-540-07165-2, DOI 10.1007/3-540-07165-2]; Muandet K, 2017, FOUND TRENDS MACH LE, V10, P1, DOI 10.1561/2200000060; Oh C., 2018, ARXIV180601619; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Perrone V, 2018, ADV NEUR IN, V31; Pfahringer Bernhard, METALEARNING LANDMAR; Poloczek M, 2016, WINT SIMUL C PROC, P770, DOI 10.1109/WSC.2016.7822140; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Ralaivola L, 2005, NEURAL NETWORKS, V18, P1093, DOI 10.1016/j.neunet.2005.07.009; Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4; Reif M, 2012, MACH LEARN, V87, P357, DOI 10.1007/s10994-012-5286-7; Ross GA, 2013, J CHEM THEORY COMPUT, V9, P4266, DOI 10.1021/ct4004228; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Song L, 2013, IEEE SIGNAL PROC MAG, V30, P98, DOI 10.1109/MSP.2013.2252713; Springenberg J.T., 2016, P ADV NEURAL INFORM, P4134; Srinivas N., 2009, P 27 INT C MACHINE L, P1015; Swersky K., 2013, ADV NEURAL INFORM PR, P2004, DOI DOI 10.1038/S41598-021-83582-6; Todorovski Ljupco, 2000, P PKDD 00 WORKSH DAT; Vanschoren J., 2013, ACM SIGKDD EXPLOR NE, V15, P49, DOI [10.1145/2641190.2641198, DOI 10.1145/2641190.2641198]; Wang J., 2016, ARXIV160205149; Wilson AG, 2016, JMLR WORKSH CONF PRO, V51, P370; Wistuba M, 2018, MACH LEARN, V107, P43, DOI 10.1007/s10994-017-5684-y; Zaheer M., 2017, ADV NEURAL INFORM PR, P3391	44	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306077
C	Lee, J; Xiao, LC; Schoenholz, SS; Bahri, Y; Novak, R; Sohl-Dickstein, J; Pennington, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lee, Jaehoon; Xiao, Lechao; Schoenholz, Samuel S.; Bahri, Yasaman; Novak, Roman; Sohl-Dickstein, Jascha; Pennington, Jeffrey			Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.	[Lee, Jaehoon; Xiao, Lechao; Schoenholz, Samuel S.; Bahri, Yasaman; Novak, Roman; Sohl-Dickstein, Jascha; Pennington, Jeffrey] Google Brain, Mountain View, CA 94043 USA	Google Incorporated	Lee, J (corresponding author), Google Brain, Mountain View, CA 94043 USA.	jaehlee@google.com; xlc@google.com; schsam@google.com; yasamanb@google.com; romann@google.com; jaschasd@google.com; jpennin@google.com		/0000-0002-8910-0950				Abadi Martin, 2016, arXiv; Allen-Zhu Z, 2019, PR MACH LEARN RES, V97; Allen-Zhu Zeyuan, 2018, ARXIV181012065; Chizat L., 2018, NOTE LAZY TRAINING S; Cho Youngmin, 2009, P ADV NEUR INF PROC, V22, P342; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Dragomir Sever Silvestru, 2003, SOME GRONWALL TYPE I; Du SS, 2019, PR MACH LEARN RES, V97; Ganguli S., 2014, INT C LEARN REPR; Garriga-Alonso Adria, 2019, ICLR, V3; Glorot X., 2010, PROC MACH LEARN RES, P249; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee Jooyoung, 2018, INT C LEARN REPR; Matthews Alexander G. de G., 2018, INT C LEARN REPR, V4; Matthews Alexander G de G, 2018, ARXIV180411271; Matthews Alexander G. de G., 2017, NEURIPS WORKSH ADV A; Mei S, 2018, P NATL ACAD SCI USA, V115, pE7665, DOI 10.1073/pnas.1806579115; Neal R.M., 1994, CRGTR941 U TOR; Neyshabur B., 2015, INT C LEARN REPR WOR; Neyshabur B., 2019, INT C LEARN REPR; Novak R, 2019, INT C LEARNING REPRE; Novak R., 2019, INT C LEARN REPR OPE; Novak R., 2018, INT C LEARN REPR; Park DS, 2019, PR MACH LEARN RES, V97; Poole B, 2016, ADV NEUR IN, V29; Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6; Rotskoff GM, 2018, ADV NEUR IN, V31; Sirignano J., 2018, MEAN FIELD ANAL NEUR; Vershynin R., 2010, ARXIV10113027; Xiao LC, 2018, PR MACH LEARN RES, V80; Yang G., 2019, ARXIV190204760; Yang G., 2019, INT C LEARN REPR; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang Chiyuan, 2019, ARXIV190201996; Zou DF, 2020, MACH LEARN, V109, P467, DOI 10.1007/s10994-019-05839-6	48	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900019
C	Lee, J; Dabagia, M; Dyer, EL; Rozell, CJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lee, John; Dabagia, Max; Dyer, Eva L.; Rozell, Christopher J.			Hierarchical Optimal Transport for Multimodal Distribution Alignment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHM	In many machine learning applications, it is necessary to meaningfully aggregate, through alignment, different but related datasets. Optimal transport (OT)-based approaches pose alignment as a divergence minimization problem: the aim is to transform a source dataset to match a target dataset using the Wasserstein distance as a divergence measure under alignment constraints. We introduce a hierarchical formulation of OT which leverages clustered structure in data to improve alignment in noisy, ambiguous, or multimodal settings. To solve this numerically, we propose a distributed ADMM algorithm that exploits the Sinkhorn distance, thus it has an efficient computational complexity that scales quadratically with the size of the largest cluster. When the transformation between two datasets is unitary, we provide performance guarantees that describe when and how well cluster correspondences can be recovered with our formulation, and then describe the worst-case dataset geometry for such a strategy. We apply this method to synthetic datasets that model data as mixtures of low-rank Gaussians and study the impact that different geometric properties of the data have on alignment. Next, we applied our approach to a neural decoding application where the goal is to predict movement directions and instantaneous velocities from populations of neurons in the macaque primary motor cortex. Our results demonstrate that when clustered structure exists in datasets, and is consistent across trials or time points, a hierarchical alignment strategy that leverages such structure can provide significant improvements in cross-domain alignment.	[Lee, John; Dabagia, Max; Dyer, Eva L.; Rozell, Christopher J.] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA; [Dyer, Eva L.] Georgia Inst Technol, Coulter Dept Biomed Engn, Atlanta, GA 30332 USA; [Lee, John] DSO Natl Labs Singapore, Singapore, Singapore	University System of Georgia; Georgia Institute of Technology; University System of Georgia; Georgia Institute of Technology	Lee, J (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.; Lee, J (corresponding author), DSO Natl Labs Singapore, Singapore, Singapore.	john.lee@gatech.edu; maxdabagia@gatech.edu; evadyer@gatech.edu; crozell@gatech.edu		Lee, John/0000-0002-0303-5343; Rozell, Christopher/0000-0001-5173-1661	DSO National Laboratories of Singapore; NSF [CCF-1409422, IIS-1755871]; CAREER award [CCF-1350954]	DSO National Laboratories of Singapore(DSO National Laboratories); NSF(National Science Foundation (NSF)); CAREER award	JL was supported by DSO National Laboratories of Singapore, ED and MD were supported by NSF grant IIS-1755871, and CR was supported by NSF grant CCF-1409422 and CAREER award CCF-1350954.	Alvarez-Melis D., 2018, ARXIV180609277; Alvarez-Melis David, 2017, ARXIV171206199; Arias-Castro Ery, 2018, ARXIV181009569; Baktashmotlagh M, 2013, IEEE I CONF COMP VIS, P769, DOI 10.1109/ICCV.2013.100; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405; Bronstein AM, 2006, P NATL ACAD SCI USA, V103, P1168, DOI 10.1073/pnas.0508601103; Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2; Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921; Cui ., 2014, P NEURAL INFORM PROC, P2429; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Das D, 2018, IEEE IMAGE PROC, P3758, DOI 10.1109/ICIP.2018.8451152; DUDLEY RM, 1969, ANN MATH STAT, V40, P40, DOI 10.1214/aoms/1177697802; Dyer EL, 2017, NAT BIOMED ENG, V1, P967, DOI 10.1038/s41551-017-0169-7; Dyer EL, 2013, J MACH LEARN RES, V14, P2487; ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204; Eldar YC, 2009, IEEE T INFORM THEORY, V55, P5302, DOI 10.1109/TIT.2009.2030471; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368; Ferradans S, 2014, SIAM J IMAGING SCI, V7, P1853, DOI 10.1137/130929886; Genevay Aude, 2018, ARXIV181002733; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gong MM, 2016, PR MACH LEARN RES, V48; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; Grave Edouard, 2018, ARXIV180511222; Han YH, 2012, IEEE T CIRC SYST VID, V22, P1485, DOI 10.1109/TCSVT.2012.2202075; KANTOROVICH L. V., 2006, J MATH SCI-U TOKYO, V133, P1383, DOI [DOI 10.1007/S10958-006-0050-9, 10.1007/s10958-006-0050-9]; Long MS, 2014, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2014.183; MIFDAL J, 2017, ADV NEURAL INFORM PR, P3373; Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46; Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526; Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pandarinath C, 2018, J NEUROSCI, V38, P9390, DOI 10.1523/JNEUROSCI.1669-18.2018; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Rangarajan A, 1997, LECT NOTES COMPUT SC, V1230, P29; Schmitzer Bernhard, 2013, INT C SCALE SPACE VA, V7893, P5; Shekhar S, 2013, PROC CVPR IEEE, P361, DOI 10.1109/CVPR.2013.53; Sugiyama M., 2008, NIPS, P1433; SUN B, 2015, BMVC, P24; Sun BC, 2016, AAAI CONF ARTIF INTE, P2058; Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310; Thopalli Kowshik, 2018, ARXIV181104491; Wang C, 2008, P 25 INT C MACH LEAR, P1120, DOI DOI 10.1145/1390156.1390297; Wang Chang, 2009, 2009 AAAI FALL S SER; Wang Y, 2019, J SCI COMPUT, V78, P29, DOI 10.1007/s10915-018-0757-z; Weed J, 2017, ARXIV170700087; Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6; Xiaoxiao Shi, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P1049, DOI 10.1109/ICDM.2010.65; Yurochkin M, 2019, ADV NEUR IN, V32; Zhang Meng, 2017, P 2017 C EMP METH NA, P1934	52	2	2	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905016
C	Lei, Q; Jalal, A; Dhillon, IS; Dimakis, AG		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lei, Qi; Jalal, Ajil; Dhillon, Inderjit S.; Dimakis, Alexandros G.			Inverting Deep Generative models, One layer at a time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the problem of inverting a deep generative model with ReLU activations. Inversion corresponds to finding a latent code vector that explains observed measurements as much as possible. In most prior works this is performed by attempting to solve a non-convex optimization problem involving the generator. In this paper we obtain several novel theoretical results for the inversion problem. We show that for the realizable case, single layer inversion can be performed exactly in polynomial time, by solving a linear program. Further, we show that for multiple layers, inversion is NP-hard and the pre-image set can be non-convex. For generative models of arbitrary depth, we show that exact recovery is possible in polynomial time with high probability, if the layers are expanding and the weights are randomly selected. Very recent work analyzed the same problem for gradient descent inversion. Their analysis requires significantly higher expansion (logarithmic in the latent dimension) while our proposed algorithm can provably reconstruct even with constant factor expansion. We also provide provable error bounds for different norms for reconstructing noisy observations. Our empirical validation demonstrates that we obtain better reconstructions when the latent dimension is large.	[Lei, Qi; Jalal, Ajil; Dhillon, Inderjit S.; Dimakis, Alexandros G.] UT Austin, Austin, TX 78712 USA; [Dhillon, Inderjit S.] Amazon, Seattle, WA USA	University of Texas System; University of Texas Austin; Amazon.com	Lei, Q (corresponding author), UT Austin, Austin, TX 78712 USA.	leiqi@oden.utexas.edu; ajiljalal@utexas.edu; inderjit@cs.utexas.edu; dimakis@austin.utexas.edu			NSF [1618689, IIS-1546452, CCF-1564000, DMS 1723052, CCF 1763702, AF 1901292]	NSF(National Science Foundation (NSF))	This research has been supported by NSF Grants 1618689, IIS-1546452, CCF-1564000, DMS 1723052, CCF 1763702, AF 1901292 and research gifts by Google, Western Digital and NVIDIA.	Allen-Zhu Z, 2016, IEEE T INFORM THEORY, V62, P5839, DOI 10.1109/TIT.2016.2598296; Aubin Benjamin, 2019, ARXIV190512385; Berinde R, 2008, ANN ALLERTON CONF, P798, DOI 10.1109/ALLERTON.2008.4797639; Bora A, 2017, PR MACH LEARN RES, V70; Dhar Manik, 2018, INT C MACH LEARN ICM, P1214; Fletcher A. K., 2017, ARXIV170606549; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grover Aditya, 2018, ARXIV181210539; Gupta S, 2018, ARXIV180511718; Hand P., 2017, ARXIV170507576; Hand Paul, 2018, C NEUR INF PROC SYS, P9154; Heckel R., 2018, P ICLR, P1; Heckel R., 2018, ARXIV180508855; Huang Wen, 2018, ARXIV181204176; Indyk P, 2013, LECT NOTES COMPUT SC, V7965, P564, DOI 10.1007/978-3-642-39206-1_48; Mardani M, 2018, ADV NEUR IN, V31; Mixon Dustin G, 2018, ARXIV180309319; Nachin Mergen, 2010, THESIS; Pandit P., 2019, ARXIV190301293; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Radford Alec, 2015, ARXIV151106434; Romano Y, 2017, SIAM J IMAGING SCI, V10, P1804, DOI 10.1137/16M1102884; Rudelson M, 2009, COMMUN PUR APPL MATH, V62, P1707, DOI 10.1002/cpa.20294; Shah V, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4609; Tripathi S., 2018, ARXIV180304477	26	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905057
C	Li, K; Zhang, TH; Malik, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Ke; Zhang, Tianhao; Malik, Jitendra			Approximate Feature Collisions in Neural Nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Work on adversarial examples has shown that neural nets are surprisingly sensitive to adversarially chosen changes of small magnitude. In this paper, we show the opposite: neural nets could be surprisingly insensitive to adversarially chosen changes of large magnitude. We observe that this phenomenon can arise from the intrinsic properties of the ReLU activation function. As a result, two very different examples could share the same feature activation and therefore the same classification decision. We refer to this phenomenon as feature collision and the corresponding examples as colliding examples. We find that colliding examples are quite abundant: we empirically demonstrate the existence of polytopes of approximately colliding examples in the neighbourhood of practically any example.	[Li, Ke; Malik, Jitendra] Univ Calif Berkeley, Berkeley, CA 94704 USA; [Zhang, Tianhao] Nanjing Univ, Nanjing, Peoples R China	University of California System; University of California Berkeley; Nanjing University	Li, K (corresponding author), Univ Calif Berkeley, Berkeley, CA 94704 USA.	ke.li@eecs.berkeley.edu; bryanzhang@smail.nju.edu.cn; malik@eecs.berkeley.edu			ONR MURI [N00014-14-1-0671]; Natural Sciences and Engineering Research Council of Canada (NSERC)	ONR MURI(MURIOffice of Naval Research); Natural Sciences and Engineering Research Council of Canada (NSERC)(Natural Sciences and Engineering Research Council of Canada (NSERC))	This work was supported by ONR MURI N00014-14-1-0671. Ke Li thanks the Natural Sciences and Engineering Research Council of Canada (NSERC) for fellowship support.	Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; Athalye Anish, 2017, ARXIV PREPRINT ARXIV; Biggio Battista, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P387, DOI 10.1007/978-3-642-40994-3_25; Dalvi N, 2004, 10 ACM SIGKDD INT C, DOI DOI 10.1145/1014052.1014066; Dosovitskiy A, 2016, PROC CVPR IEEE, P4829, DOI 10.1109/CVPR.2016.522; Erhan D, 2009, BERNOULLI, V1341, P1; Jacobsen J.H., 2018, ARXIV181100401; Kurakin Alexey, 2016, WORKSHOP TRACK P, DOI DOI 10.48550/ARXIV.1607.02533; LeCun Y., 2011, P AISTATS 2011 FORT; Li K., 2017, ARXIV170300440; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; Nguyen A., 2016, VIS DEEP LEARN WORKS; Shafahi Ali, 2018, ARXIV180400792; Simonyan K., 2015, ICLR; Simonyan K., 2013, DEEP INSIDE CONVOLUT; Szegedy Christian, 2013, INT C LEARN REPR; Tatu A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1410, DOI 10.1109/ICCVW.2011.6130416; Yosinski J., 2015, INT C MACH LEARN WOR	18	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907049
C	Li, X; Wang, Y; Basu, S; Kumbier, K; Yu, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Xiao; Wang, Yu; Basu, Sumanta; Kumbier, Karl; Yu, Bin			A Debiased MDI Feature Importance Measure for Random Forests	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Tree ensembles such as Random Forests have achieved impressive empirical success across a wide variety of applications. To understand how these models make predictions, people routinely turn to feature importance measures calculated from tree ensembles. It has long been known that Mean Decrease Impurity (MDI), one of the most widely used measures of feature importance, incorrectly assigns high importance to noisy features, leading to systematic bias in feature selection. In this paper, we address the feature selection bias of MDI from both theoretical and methodological perspectives. Based on the original definition of MDI by Breiman et al. [3] for a single tree, we derive a tight non-asymptotic bound on the expected bias of MDI importance of noisy features, showing that deep trees have higher (expected) feature selection bias than shallow ones. However, it is not clear how to reduce the bias of MDI using its existing analytical expression. We derive a new analytical expression for MDI, and based on this new expression, we are able to propose a new MDI feature importance measure using out-of-bag samples, called MDI-oob. For both the simulated data and a genomic ChIP dataset, MDI-oob achieves state-of-the-art performance in feature selection from Random Forests for both deep and shallow trees.	[Li, Xiao; Wang, Yu; Kumbier, Karl] Univ Calif Berkeley, Stat Dept, Berkeley, CA 94720 USA; [Basu, Sumanta] Cornell Univ, Computat Biol Dept, Stat & Data Sci Dept, Ithaca, NY 14853 USA; [Yu, Bin] Univ Calif Berkeley, Stat Dept, EECS, Berkeley, CA USA	University of California System; University of California Berkeley; Cornell University; University of California System; University of California Berkeley	Li, X (corresponding author), Univ Calif Berkeley, Stat Dept, Berkeley, CA 94720 USA.	sxli@berkeley.edu; wang.yu@berkeley.edu; sumbose@cornell.edu; kkumbier@berkeley.edu; binyu@berkeley.edu	Wang, Yu/U-8482-2019	Wang, Yu/0000-0002-5329-7739	ARO [W911NF1710005]; ONR [N00014-16-1-2664]; NSF [DMS-1613002, IIS 1741340]; Center for Science of Information (CSoI), a US NSF Science and Technology Center [CCF-0939370]	ARO; ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); Center for Science of Information (CSoI), a US NSF Science and Technology Center	The authors would like to thank Merle Behr and Raaz Dwivedi from University of California, Berkeley for their very helpful comments of this paper that greatly improve its presentation. Partial supports are gratefully acknowledged from ARO grant W911NF1710005, ONR grant N00014-16-1-2664, NSF grants DMS-1613002 and IIS 1741340, and the Center for Science of Information (CSoI), a US NSF Science and Technology Center, under grant agreement CCF-0939370.	Basu S, 2018, P NATL ACAD SCI USA, V115, P1943, DOI 10.1073/pnas.1711236115; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breiman L., 2017, CLASSIFICATION REGRE; Celniker SE, 2009, NATURE, V459, P927, DOI 10.1038/459927a; Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785; Diaz-Uriarte R, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-3; Hothorn T, 2006, J COMPUT GRAPH STAT, V15, P651, DOI 10.1198/106186006X133933; Huynh-Thu VA, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0012776; Janitza S, 2018, ADV DATA ANAL CLASSI, V12, P885, DOI 10.1007/s11634-016-0276-4; Kazemitabar J, 2017, ADV NEURAL INFORM PR, P426; Kumbier K., 2018, ARXIV181007287; Lee JW, 2005, COMPUT STAT DATA AN, V48, P869, DOI 10.1016/j.csda.2004.03.017; Loh WY, 2014, INT STAT REV, V82, P329, DOI 10.1111/insr.12016; Louppe G., 2013, ADV NEURAL INFORM PR, V26, P431, DOI DOI 10.5555/2999611.2999660; Louppe G, 2014, THESIS, DOI DOI 10.13140/2.1.1570.5928; Lundberg SM, 2018, CONSISTENT INDIVIDUA; MacArthur S, 2009, GENOME BIOL, V10, DOI 10.1186/gb-2009-10-7-r80; Murdoch WJ, 2019, INTERPRETABLE MACHIN, P1; Nembrini S, 2018, BIOINFORMATICS, V34, P3711, DOI 10.1093/bioinformatics/bty373; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Ribeiro Marco Tulio, 2016, P KDD, P97, DOI [10.18653/v1/n16-3020, DOI 10.1145/2939672.2939778]; Rodenburg Wendy, 2008, PHYSL GENOMICS, V33; Saabas A., 2014, INTERPRETING RANDOM; Sandri M, 2008, J COMPUT GRAPH STAT, V17, P611, DOI 10.1198/106186008X344522; Scornet E, 2015, ANN STAT, V43, P1716, DOI 10.1214/15-AOS1321; Strobl C, 2007, COMPUTATIONAL STAT D, V52; Strobl C, 2007, BMC BIOINFORMATICS, V8, DOI 10.1186/1471-2105-8-25; Strobl C, 2009, R J, V1, P14; Strobl C, 2008, BMC BIOINFORMATICS, V9, DOI 10.1186/1471-2105-9-307; Strumbelj E, 2014, KNOWL INF SYST, V41, P647, DOI 10.1007/s10115-013-0679-x; Wager Stefan, 2018, J AM STAT ASSOC, V1459, P1; Wei PF, 2015, RELIAB ENG SYST SAFE, V142, P399, DOI 10.1016/j.ress.2015.05.018; Wright MN, 2017, J STAT SOFTW, V77, P1, DOI 10.18637/jss.v077.i01; Zhou Z., 2019, ARXIV190305179	34	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308011
C	Lim, JN; Yamada, M; Scholkopf, B; Jitkrittum, W		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lim, Jen Ning; Yamada, Makoto; Scholkopf, Bernhard; Jitkrittum, Wittawat			Kernel Stein Tests for Multiple Model Comparison	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SELECTION; INFERENCE	We address the problem of non-parametric multiple model comparison: given l candidate models, decide whether each candidate is as good as the best one(s) or worse than it. We propose two statistical tests, each controlling a different notion of decision errors. The first test, building on the post selection inference framework, provably controls the number of best models that are wrongly declared worse (false positive rate). The second test is based on multiple correction, and controls the proportion of the models declared worse but are in fact as good as the best (false discovery rate). We prove that under appropriate conditions the first test can yield a higher true positive rate than the second. Experimental results on toy and real (CelebA, Chicago Crime data) problems show that the two tests have high true positive rates with well-controlled error rates. By contrast, the naive approach of choosing the model with the lowest score without correction leads to more false positives.	[Lim, Jen Ning; Scholkopf, Bernhard; Jitkrittum, Wittawat] Max Planck Inst Intelligent Syst, Stuttgart, Germany; [Yamada, Makoto] Kyoto Univ, RIKEN AIP, Kyoto, Japan	Max Planck Society; Kyoto University; RIKEN	Lim, JN (corresponding author), Max Planck Inst Intelligent Syst, Stuttgart, Germany.	jlim@tuebingen.mpg.de; makoto.yamada@riken.jp; bs@tuebingen.mpg.de; wittawat@tuebingen.mpg.de			JST PRESTO program [JPMJPR165A]; MEXT KAKENHI [16H06299]; RIKEN engineering network	JST PRESTO program; MEXT KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); RIKEN engineering network	M.Y. was supported by the JST PRESTO program JPMJPR165A and partly supported by MEXT KAKENHI 16H06299 and the RIKEN engineering network funding.	Benjamini Y, 2001, ANN STAT, V29, P1165; Berlinet A., 2004, REPRODUCING KERNEL H, DOI [10.1007/978-1-4419-9096-9, DOI 10.1007/978-1-4419-9096-9]; Bounliphone W., 2016, INT C LEARN REPR; Burkardt J., 2014, TRUNCATED NORMAL DIS; Chwialkowski K. P., 2015, ADV NEURAL INFORM PR, P1981; Chwialkowski Kacper, 2016, INT C MACH LEARN PML; Dougal J., 2018, ICLR; Fithian W, 2014, OPTIMAL INFERENCE MO; Fromont M., 2012, C LEARN THEOR, P23; FUKUMIZU K, 2008, ADV NEURAL INFORM PR, V20, P489; Germain M, 2015, PR MACH LEARN RES, V37, P881; Gorham J, 2017, PR MACH LEARN RES, V70; Gretton A, 2012, J MACH LEARN RES, V13, P723; Harchaoui Z., 2008, ADV NEURAL INFORM PR, P609; Heusel M., 2017, 31 C NEUR INF PROC S, P6626; Jitkrittum W, 2017, ADV NEURAL INFORM PR, P262; Jitkrittum W., 2016, ADV NEURAL INFORM PR, P181; Jitkrittum Wittawat, 2018, ADV NEURAL INFORM PR; Lee JD, 2016, ANN STAT, V44, P907, DOI 10.1214/15-AOS1371; Leeb H, 2005, ECONOMET THEOR, V21, P21, DOI 10.1017/S0266466605050036; Liu Q, 2016, PR MACH LEARN RES, V48; Liu Z., 2018, LARGE SCALE CELEBFAC, V15, P11; Papamakarios George, 2017, ADV NEURAL INFORM PR, P2338; Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389; Steinwart I, 2002, J MACH LEARN RES, V2, P67, DOI 10.1162/153244302760185252; Strathmann H., 2012, P ADV NEUR INF PROC, P1205, DOI DOI 10.5555/2999134.2999269; Suzumura S, 2017, PR MACH LEARN RES, V70; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Yamada M., 2019, INT C LEARN REPR; Yang J., 2018, INT C MACH LEARN, P5557; ZAREMBA W, 2013, ADV NEURAL INFORM PR, V26, P755	33	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302026
C	Liu, H; Han, ZZ; Liu, YS; Gu, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Han; Han, Zhizhong; Liu, Yu-Shen; Gu, Ming			Fast Low-rank Metric Learning for Large-scale and High-dimensional Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Low-rank metric learning aims to learn better discrimination of data subject to low-rank constraints. It keeps the intrinsic low-rank structure of datasets and reduces the time cost and memory usage in metric learning. However, it is still a challenge for current methods to handle datasets with both high dimensions and large numbers of samples. To address this issue, we present a novel fast low-rank metric learning (FLRML) method. FLRML casts the low-rank metric learning problem into an unconstrained optimization on the Stiefel manifold, which can be efficiently solved by searching along the descent curves of the manifold. FLRML significantly reduces the complexity and memory usage in optimization, which makes the method scalable to both high dimensions and large numbers of samples. Furthermore, we introduce a mini-batch version of FLRML to make the method scalable to larger datasets which are hard to be loaded and decomposed in limited memory. The outperforming experimental results show that our method is with high accuracy and much faster than the state-of-the-art methods under several benchmarks with large numbers of high-dimensional data. Code has been made available at https://github.com/highan911/FLRML.	[Liu, Han; Liu, Yu-Shen; Gu, Ming] Tsinghua Univ, Sch Software, Beijing, Peoples R China; [Liu, Han; Liu, Yu-Shen; Gu, Ming] BNRist & KLISS, Beijing, Peoples R China; [Han, Zhizhong] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA	Tsinghua University; University System of Maryland; University of Maryland College Park	Liu, H (corresponding author), Tsinghua Univ, Sch Software, Beijing, Peoples R China.; Liu, H (corresponding author), BNRist & KLISS, Beijing, Peoples R China.	liuhan15@mails.tsinghua.edu.cn; h312h@umd.edu; liuyushen@tsinghua.edu.cn; guming@tsinghua.edu.cn	Han, Zhizhong/AAW-4044-2021		National Key R&D Program of China [2018YF-B0505400, 2016QY07X1402]; National Science and Technology Major Project of China [2016ZX 01038101]; NSFC Program [61527812]	National Key R&D Program of China; National Science and Technology Major Project of China; NSFC Program(National Natural Science Foundation of China (NSFC))	This research is sponsored in part by the National Key R&D Program of China (No. 2018YF-B0505400, 2016QY07X1402), the National Science and Technology Major Project of China (No. 2016ZX 01038101), and the NSFC Program (No. 61527812).	Absil PA, 2012, SIAM J OPTIMIZ, V22, P135, DOI 10.1137/100802529; BARZILAI J, 1988, IMA J NUMER ANAL, V8, P141, DOI 10.1093/imanum/8.1.141; Bellet A., 2012, P 29 INT C MACH LEAR, P1871; Bellet A., ARXIV13066709; Cai D, 2012, IEEE T KNOWL DATA EN, V24, P707, DOI 10.1109/TKDE.2011.104; Chechik G, 2009, ADV NEURAL INFORM PR, P306; Ding Zhengming, 2017, IEEE T IMAGE PROCESS, V1-1, P99; Dong Linsong, 2019, Aquaculture and Fisheries, V4, P3, DOI 10.1016/j.aaf.2018.05.001; Gillen S., 2018, ADV NEURAL INFORM PR, P2600; Han Z., 2019, ICCV; Han Z., 2019, AAAI; Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P3986, DOI 10.1109/TIP.2019.2904460; Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P658, DOI 10.1109/TIP.2018.2868426; Han ZZ, 2019, IEEE T CYBERNETICS, V49, P481, DOI 10.1109/TCYB.2017.2778764; Han ZZ, 2018, IEEE T IMAGE PROCESS, V27, P3049, DOI 10.1109/TIP.2018.2816821; Han ZZ, 2017, IEEE T IMAGE PROCESS, V26, P3707, DOI 10.1109/TIP.2017.2704426; Han Zhizhong, 2019, IJCAI; Harandi M., 2017, ICML; Hegde C, 2015, IEEE T SIGNAL PROCES, V63, P6109, DOI 10.1109/TSP.2015.2452228; Huang ZW, 2015, PROC CVPR IEEE, P140, DOI 10.1109/CVPR.2015.7298609; Jain L., 2017, ADV NEURAL INFORM PR, V30, P4139; Kostinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939; KULIS B, 2013, MACH LEARN, V5, P287, DOI DOI 10.1561/2200000019; Kwok J. T., 2003, P 20 INT C MACH LEAR, P400; Lang K., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P331; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li WB, 2018, PATTERN RECOGN, V75, P302, DOI 10.1016/j.patcog.2017.03.016; Liu K., 2015, AISTATS; Liu W., 2010, P 27 INT C MACH LEAR, P679; Liu W, 2015, AAAI CONF ARTIF INTE, P2792; Liu X., 2019, ACMMM; Liu X, 2019, AAAI; Lu ZY, 2013, INT CONF MACH LEARN, P583, DOI 10.1109/ICMLC.2013.6890359; Luo L., 2018, AAAI; Mu YD, 2016, AAAI CONF ARTIF INTE, P1941; Qi G.-J., 2009, P INT C MACH LEARN J, P841; Qian Q, 2015, MACH LEARN, V99, P353, DOI 10.1007/s10994-014-5456-x; Schultz M, 2004, ADV NEUR IN, V16, P41; Shalit U, 2012, J MACH LEARN RES, V13, P429; Shi Y., 2014, AAAI; Shukla A., 2015, INT WORKSH DIFF GEOM; Su YX, 2016, IEEE IJCNN, P3306, DOI 10.1109/IJCNN.2016.7727622; Wang F, 2015, DATA MIN KNOWL DISC, V29, P534, DOI 10.1007/s10618-014-0356-z; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Wen ZW, 2013, MATH PROGRAM, V142, P397, DOI 10.1007/s10107-012-0584-1; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Xing Eric P., 2015, IEEE Transactions on Big Data, V1, P49, DOI 10.1109/TBDATA.2015.2472014; Zhang HC, 2004, SIAM J OPTIMIZ, V14, P1043, DOI 10.1137/S1052623403428208; Zhang Junbo, 2017, AAAI; Zhang LJ, 2016, LECT NOTES ARTIF INT, V9925, P83, DOI 10.1007/978-3-319-46379-7_6; Zhu PF, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3235	52	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300074
C	Loaiza-Ganem, G; Perkins, SM; Schroeder, KE; Churchland, MM; Cunningham, JP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Loaiza-Ganem, Gabriel; Perkins, Sean M.; Schroeder, Karen E.; Churchland, Mark M.; Cunningham, John P.			Deep Random Splines for Point Process Intensity Estimation of Neural Population Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GAUSSIAN-PROCESSES; REGRESSION; INFERENCE	Gaussian processes are the leading class of distributions on random functions, but they suffer from well known issues including difficulty scaling and inflexibility with respect to certain shape constraints (such as nonnegativity). Here we propose Deep Random Splines, a flexible class of random functions obtained by transforming Gaussian noise through a deep neural network whose output are the parameters of a spline. Unlike Gaussian processes, Deep Random Splines allow us to readily enforce shape constraints while inheriting the richness and tractability of deep generative models. We also present an observational model for point process data which uses Deep Random Splines to model the intensity function of each point process and apply it to neural population data to obtain a low-dimensional representation of spiking activity. Inference is performed via a variational autoencoder that uses a novel recurrent encoder architecture that can handle multiple point processes as input. We use a newly collected dataset where a primate completes a pedaling task, and observe better dimensionality reduction with our model than with competing alternatives.(1)	[Loaiza-Ganem, Gabriel; Cunningham, John P.] Columbia Univ, Dept Stat, New York, NY 10027 USA; [Perkins, Sean M.] Columbia Univ, Dept Biomed Engn, New York, NY 10027 USA; [Schroeder, Karen E.; Churchland, Mark M.] Columbia Univ, Dept Neurosci, New York, NY 10027 USA	Columbia University; Columbia University; Columbia University	Loaiza-Ganem, G (corresponding author), Columbia Univ, Dept Stat, New York, NY 10027 USA.	g12480@columbia.edu; sp3222@columbia.edu; ks3381@columbia.edu; mc3502@columbia.edu; jpc2181@columbia.edu		Perkins, Sean/0000-0001-9456-4648	Simons Foundation; Sloan Foundation; McKnight Endowment Fund; Gatsby Charitable Foundation; NIH NINDS [5R01NS100066]; NSF [1707398]	Simons Foundation; Sloan Foundation(Alfred P. Sloan Foundation); McKnight Endowment Fund; Gatsby Charitable Foundation; NIH NINDS(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Neurological Disorders & Stroke (NINDS)); NSF(National Science Foundation (NSF))	We thank the Simons Foundation, Sloan Foundation, McKnight Endowment Fund, NIH NINDS 5R01NS100066, NSF 1707398, and the Gatsby Charitable Foundation for support.	Abadi M, 2015, P 12 USENIX S OPERAT; Adams R. P., 2009, P 26 ANN INT C MACH, P9, DOI DOI 10.1145/1553374.1553376; Agrawal A., 2019, ARXIV PREPRINT ARXIV; Amos B, 2017, PR MACH LEARN RES, V70; Bauschke HH, 1996, SIAM REV, V38, P367, DOI 10.1137/S0036144593251710; Boyle J. P., 1986, ADV ORDER RESTRICTED, P28, DOI DOI 10.1007/978-1-4613-9940-7_3; Brown EN, 2002, NEURAL COMPUT, V14, P325, DOI 10.1162/08997660252741149; Churchland MM, 2012, NATURE, V487, P51, DOI 10.1038/nature11129; Cunningham J. P., 2008, P 25 INT C MACH LEAR, P192, DOI DOI 10.1145/1390156.1390181; DiMatteo I, 2001, BIOMETRIKA, V88, P1055, DOI 10.1093/biomet/88.4.1055; Du N., 2012, ADV NEURAL INFORM PR, V4, P2780; Du N, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1555, DOI 10.1145/2939672.2939875; Duncker L, 2018, ADV NEUR IN, V31; DYKSTRA RL, 1983, J AM STAT ASSOC, V78, P837, DOI 10.2307/2288193; Gao YJ, 2016, ADV NEUR IN, V29; Gilboa E, 2015, IEEE T PATTERN ANAL, V37, P424, DOI 10.1109/TPAMI.2013.192; Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kingma D.P, P 3 INT C LEARNING R; Kingman J., 1992, POISSON PROCESSES, V3; Lasserre J. B., 2010, MOMENTS POSITIVE POL; Lin LZ, 2014, BIOMETRIKA, V101, P303, DOI 10.1093/biomet/ast063; Lloyd C, 2015, PR MACH LEARN RES, V37, P1814; MAMMEN E, 1991, ANN STAT, V19, P724, DOI 10.1214/aos/1176348117; Mei HY, 2017, ADV NEUR IN, V30; Mohamed S., 2017, INT C LEARN REPR; Moller J, 1998, SCAND J STAT, V25, P451, DOI 10.1111/1467-9469.00115; Ramsay J., 1988, STAT SCI, V3, P425, DOI DOI 10.1214/SS/1177012761; Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; SCHMIDT JW, 1988, BIT, V28, P340, DOI 10.1007/BF01934097; Shen WN, 2016, BERNOULLI, V22, P396, DOI 10.3150/14-BEJ663; Tibshirani RJ., 2017, P 31 INT C NEUR INF, P517; von Neumann J., 1950, ANN MATH STUDIES, V22; Wahba G., 1990, SPLINE MODELS OBSERV, V59, DOI [10.1137/1.9781611970128, DOI 10.1137/1.9781611970128]; Yang J., 2017, UAI; Yu BM, 2009, J NEUROPHYSIOL, V102, P614, DOI 10.1152/jn.90941.2008; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x	40	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905006
C	Lyu, H; Sha, NY; Qin, SY; Yan, M; Xie, YY; Wang, RR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lyu, He; Sha, Ningyu; Qin, Shuyang; Yan, Ming; Xie, Yuying; Wang, Rongrong			Manifold Denoising by Nonlinear Robust Principal Component Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CURVATURE	This paper extends robust principal component analysis (RPCA) to nonlinear manifolds. Suppose that the observed data matrix is the sum of a sparse component and a component drawn from some low dimensional manifold. Is it possible to separate them by using similar ideas as RPCA? Is there any benefit in treating the manifold as a whole as opposed to treating each local region independently? We answer these two questions affirmatively by proposing and analyzing an optimization framework that separates the sparse component from the manifold under noisy data. Theoretical error bounds are provided when the tangent spaces of the manifold satisfy certain incoherence conditions. We also provide a near optimal choice of the tuning parameters for the proposed optimization formulation with the help of a new curvature estimation method. The efficacy of our method is demonstrated on both synthetic and real datasets.	[Lyu, He; Sha, Ningyu; Qin, Shuyang; Yan, Ming; Xie, Yuying; Wang, Rongrong] Michigan State Univ, Dept Computat Math Sci & Engn, E Lansing, MI 48824 USA	Michigan State University	Lyu, H (corresponding author), Michigan State Univ, Dept Computat Math Sci & Engn, E Lansing, MI 48824 USA.	lyuhe@msu.edu; shaningy@msu.edu; qinshuya@msu.edu; myan@msu.edu; xyy@msu.edu; wangron6@msu.edu	Yan, Ming/V-3072-2017	Yan, Ming/0000-0002-8686-3530; Lyu, He/0000-0003-1349-5201	NIH [U01DE029255, 5RO3DE027399]; NSF [DMS-1902906, DMS-1621798, DMS-1715178, CCF-1909523, NCS-1630982]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF))	The authors would like to thank Shuai Yuan, Hongbo Lu, Changxiong Liu, Jonathan Fleck, Yichen Lou, and Lijun Cheng for useful discussions. This work was supported in part by the NIH grants U01DE029255, 5RO3DE027399 and the NSF grants DMS-1902906, DMS-1621798, DMS-1715178, CCF-1909523 and NCS-1630982.	Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Du  C., 2013, P 8 INT C BIOINSP CO, P353; Eppel Sagi, 2006, ARXIV160200177; Flynn P. J., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P110, DOI 10.1109/CVPR.1989.37837; Gavish M, 2014, IEEE T INFORM THEORY, V60, P5040, DOI 10.1109/TIT.2014.2323359; Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005; Jiang B, 2013, PROC CVPR IEEE, P3492, DOI 10.1109/CVPR.2013.448; Kobayashi, 1996, FDN DIFFERENTIAL GEO, V2; Li Xiang-Ru, 2009, Acta Automatica Sinica, V35, P17, DOI 10.3724/SP.J.1004.2009.00017; Little A., 2018, ARXIV181211954; MARTIN GR, 1975, P NATL ACAD SCI USA, V72, P1441, DOI 10.1073/pnas.72.4.1441; Meek DS, 2000, COMPUT AIDED GEOM D, V17, P521, DOI 10.1016/S0167-8396(00)00006-6; Meila M, 2001, ADV NEUR IN, V13, P873; Moon Kevin, 2019, BIORXIV, P120378; Pottmann H, 2007, COMPUT AIDED GEOM D, V24, P428, DOI 10.1016/j.cagd.2007.07.004; Sha N., 2019, P 89 ANN INT M SEG, P2543; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Tang ZG, 2010, CHIN CONT DECIS CONF, P452, DOI 10.1109/CCDC.2010.5499017; Tong WS, 2005, IEEE T PATTERN ANAL, V27, P434, DOI 10.1109/TPAMI.2005.62; Vershynin R., 2018, HIGH DIMENSIONAL PRO, V47	21	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE	34305366				2022-12-19	WOS:000535866905010
C	Lyu, SH; Yang, L; Zhou, ZH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lyu, Shen-Huan; Yang, Liang; Zhou, Zhi-Hua			A Refined Margin Distribution Analysis for Forest Representation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we formulate the forest representation learning approach named casForest as an additive model, and show that the generalization error can be bounded by O (ln m/m), when the margin ratio related to the margin standard deviation against the margin mean is sufficiently small. This inspires us to optimize the ratio. To this end, we design a margin distribution reweighting approach for the deep forest model to attain a small margin ratio. Experiments confirm the relation between the margin distribution and generalization performance. We remark that this study offers a novel understanding of casForest from the perspective of the margin theory and further guides the layer-by-layer forest representation learning.	[Lyu, Shen-Huan; Yang, Liang; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China	Nanjing University	Lyu, SH (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.	lvsh@lamda.nju.edu.cn; yangl@lamda.nju.edu.cn; zhouzh@lamda.nju.edu.cn		Lyu, Shen-Huan/0000-0002-0173-8408	NSFC [61751306]; National Key R&D Program of China [2018YFB1004300]; Collaborative Innovation Center of Novel Software Technology and Industrialization	NSFC(National Natural Science Foundation of China (NSFC)); National Key R&D Program of China; Collaborative Innovation Center of Novel Software Technology and Industrialization	This research was supported by the NSFC (61751306), National Key R&D Program of China (2018YFB1004300), and the Collaborative Innovation Center of Novel Software Technology and Industrialization. The authors would like to thank the anonymous reviewers for constructive suggestions, as well as Wei Gao, Lijun Zhang, Shengjun Huang, Xizhu Wu, Lu Wang, Peng Zhao, Ming Pang and Kangle Zhao for helpful discussions.	Breiman L, 1996, MACH LEARN, V24, P49; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breiman L, 1999, NEURAL COMPUT, V11, P1493, DOI 10.1162/089976699300016106; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chen T., 2016, KDD16 P 22 ACM, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Cortes C, 2017, PR MACH LEARN RES, V70; Cortes C, 2014, PR MACH LEARN RES, V32, P1179; CRAMMER K, 2001, J MACHINE LEARNING R, V2, P265, DOI DOI 10.1162/15324430260185628; Dheeru D., 2019, UCI MACHINE LEARNING; Elsayed G., 2018, ADV NEURAL INFORM PR, P842; Feng J, 2018, AAAI CONF ARTIF INTE, P2967; Feng J, 2018, ADV NEUR IN, V31; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Gao W, 2013, ARTIF INTELL, V203, P1, DOI 10.1016/j.artint.2013.07.002; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lyu S.-H., 2018, CORR; Pang M, 2018, IEEE DATA MINING, P1194, DOI 10.1109/ICDM.2018.00158; Reyzin L., 2006, PROC 23 INT C MACH L, P753, DOI DOI 10.1145/1143844.1143939; Schapire RE, 1998, ANN STAT, V26, P1651; Schapire Robert E., 2012, BOOSTING FDN ALGORIT; Utkin LV, 2018, KNOWL-BASED SYST, V139, P13, DOI 10.1016/j.knosys.2017.10.006; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang KL, 2018, IEEE WINT CONF APPL, P2058, DOI 10.1109/WACV.2018.00227; WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1; Zhang T, 2020, IEEE T KNOWL DATA EN, V32, P1143, DOI 10.1109/TKDE.2019.2897662; Zhang T, 2017, PR MACH LEARN RES, V70; Zhi-Hua Zhou, 2014, Artificial Neural Networks in Pattern Recognition. 6th IAPR TC 3 International Workshop, ANNPR 2014. Proceedings: LNCS 8774, P1, DOI 10.1007/978-3-319-11656-3_1; Zhou ZH., 2012, ENSEMBLE METHODS FDN, DOI [10.1201/b12207, DOI 10.1201/B12207]; Zhou ZH, 2019, NATL SCI REV, V6, P74, DOI 10.1093/nsr/nwy108; Zhou ZH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3553	33	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305051
C	Marom, Y; Feldman, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Marom, Yair; Feldman, Dan			k-Means Clustering of Lines for Big Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The input to the k-meanfor lines problem is a set L of n lines in R-d, and the goal is to compute a set of k centers (points) in R-d that minimizes the sum of squared distances over every line in L and its nearest center. This is a straightforward generalization of the k-mean problem where the input is a set of n points instead of lines. We suggest the first PTAS that computes a (1 + epsilon)-approximation to this problem in time O(n log n) for any constant approximation error epsilon is an element of (0, 1), and constant integers k, d >= 1. This is by proving that there is always a weighted subset (called coreset) of dk(O(k)) log(n)/epsilon(2) lines in L that approximates the sum of squared distances from L to any given set of k points. Using traditional merge-and-reduce technique, this coreset implies results for a streaming set (possibly infinite) of lines to M machines in one pass (e.g. cloud) using memory, update time and communication that is near-logarithmic in n, as well as deletion of any line but using linear space. These results generalized for other distance functions such as k-median (sum of distances) or ignoring farthest m lines from the given centers to handle outliers. Experimental results on 10 machines on Amazon EC2 cloud show that the algorithm performs well in practice. Open source code for all the algorithms and experiments is also provided.	[Marom, Yair; Feldman, Dan] Univ Haifa, Dept Comp Sci, Haifa, Israel	University of Haifa	Marom, Y (corresponding author), Univ Haifa, Dept Comp Sci, Haifa, Israel.	yairmrm@gmail.com; dannyf.post@gmail.com						Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Asuncion A, 2007, UCI MACHINE LEARNING; Bach FR, 2004, ADV NEUR IN, V16, P305; Bachem O., 2017, ARXIV171109649; Charikar M, 2002, J COMPUT SYST SCI, V65, P129, DOI 10.1006/jcss.2002.1882; Danzer L., 1963, P S PURE MATH, VVII, P101, DOI [10.1090/pspum/007/0157289, DOI 10.1090/PSPUM/007/0157289, 10.1090/pspum/007, DOI 10.1090/PSPUM/007]; Feldman Dan, 2012, P 23 ANN ACM SIAM S, P1343; Gao J, 2008, DISCRETE COMPUT GEOM, V40, P537, DOI 10.1007/s00454-008-9107-5; Gao J, 2010, ACM T ALGORITHMS, V7, DOI 10.1145/1868237.1868246; Haklay M, 2008, IEEE PERVAS COMPUT, V7, P12, DOI 10.1109/MPRV.2008.80; Har-Peled S, 2006, LECT NOTES COMPUT SC, V4337, P33; Jie B, 2016, PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON BIOINFORMATICS, COMPUTATIONAL BIOLOGY, AND HEALTH INFORMATICS, P622, DOI 10.1145/2975167.2985687; Kfir, 2018, CORESETS BIG DATA LE; Krause Andreas, 2010, ADV NEURAL INFORM PR, V23, P5; Lee E, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P810; Liu Y., 2001, P 18 INT C MACH LEAR, P329; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Nadler B., 2006, ADV NEURAL INFORM PR, P955; Ommer B, 2009, IEEE I CONF COMP VIS, P484, DOI 10.1109/ICCV.2009.5459200; Ostrovsky R, 2006, ANN IEEE SYMP FOUND, P165; Perets T., 2011, CLUSTERING LINES; Ren Huamin, 2015, ICML 15 WORKSH; Wang E., 2014, HIGH PERFORMANCE COM, V5, P2; Williams D., 2005, P INT C MACH LEARN I, P80; Xu L., 2005, NEURAL INFORM PROCES, P1537	25	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904045
C	Mastakouri, AA; Scholkopf, B; Janzing, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mastakouri, Atalanti A.; Schoelkopf, Bernhard; Janzing, Dominik			Selecting causal brain features with a single conditional independence test per feature	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				STIMULATION; OSCILLATIONS	We propose a constraint-based causal feature selection method for identifying causes of a given target variable, selecting from a set of candidate variables, while there can also be hidden variables acting as common causes with the target. We prove that if we observe a cause for each candidate cause, then a single conditional independence test with one conditioning variable is sufficient to decide whether a candidate associated with the target is indeed causing it. We thus improve upon existing methods by significantly simplifying statistical testing and requiring a weaker version of causal faithfulness. Our main assumption is inspired by neuroscience paradigms where the activity of a single neuron is considered to be also caused by its own previous state. We demonstrate successful application of our method to simulated, as well as encephalographic data of twenty-one participants, recorded in Max Planck Institute for intelligent Systems. The detected causes of motor performance are in accordance with the latest consensus about the neurophysiological pathways, and can provide new insights into personalised brain stimulation.	[Mastakouri, Atalanti A.; Schoelkopf, Bernhard] Max Planck Inst Intelligent Syst, Empir Inference Dept, D-72076 Tubingen, Germany; [Janzing, Dominik] Amazon Res, D-72076 Tubingen, Germany	Max Planck Society	Mastakouri, AA (corresponding author), Max Planck Inst Intelligent Syst, Empir Inference Dept, D-72076 Tubingen, Germany.	amastakouri@tue.mpg.de; bs@tue.mpg.de; janzind@amazon.com						Belouchrani A., 1993, PROC INT C DIGITAL S, P346; Brinkman L, 2014, J NEUROSCI, V34, P14783, DOI 10.1523/JNEUROSCI.2039-14.2014; Brown P, 2007, CURR OPIN NEUROBIOL, V17, P656, DOI 10.1016/j.conb.2007.12.001; Brown P, 2003, MOVEMENT DISORD, V18, P357, DOI 10.1002/mds.10358; Chen LS, 2007, GENOME BIOL, V8, DOI 10.1186/gb-2007-8-10-r219; Davis NJ, 2013, FRONT SYST NEUROSCI, V7, DOI 10.3389/fnsys.2013.00076; Entner D., 2013, P 16 INT C ART INT S, V31, P256; Espenhahn Svenja, 2018, THESIS; Frolich Laura, 2018, Brain Inform, V5, P13, DOI 10.1007/s40708-017-0074-6; Fukumizu Kenji, 2008, KERNEL MEASURES COND, P489; Greenland S, 2000, INT J EPIDEMIOL, V29, P722, DOI 10.1093/ije/29.4.722; Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63; Grosse-Wentrup M, 2012, J NEURAL ENG, V9, DOI 10.1088/1741-2560/9/4/046001; Gulberti A, 2015, NEUROIMAGE-CLIN, V9, P436, DOI 10.1016/j.nicl.2015.09.013; Helfrich RF, 2016, NEUROIMAGE, V140, P76, DOI 10.1016/j.neuroimage.2015.11.035; Judea Pearl, 2009, ECONOMETRIC THEORY; Khanna P, 2017, ELIFE, V6, DOI 10.7554/eLife.24573; Koller D., 2009, PROBABILISTIC GRAPHI; Mastakouri Atalanti A., 2017, P IEEE INT C SYST MA; Mastakouri Atalanti A., 2019, ENG MED BIOL C EMBC; McAllister CJ, 2013, J NEUROSCI, V33, P7919, DOI 10.1523/JNEUROSCI.5624-12.2013; McMenamin BW, 2010, NEUROIMAGE, V49, P2416, DOI 10.1016/j.neuroimage.2009.10.010; Muthukumaraswamy SD, 2010, J NEUROPHYSIOL, V104, P2873, DOI 10.1152/jn.00607.2010; Nowak Magdalena, 2018, Curr Behav Neurosci Rep, V5, P136, DOI 10.1007/s40473-018-0151-z; Pearl J., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P435; Perl J., 2000, CAUSALITY MODELS REA; Peters C, 2017, TLS-TIMES LIT SUPPL, P6; Ramsey Joseph, 2006, CORR; Rubenstein PK, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Seth AK, 2015, J NEUROSCI, V35, P3293, DOI 10.1523/JNEUROSCI.4399-14.2015; Shah R.D., 2018, ARXIV180407203; Song Le, 2007, P 24 INT C MACH LEAR; SPIRTES P, 1993, LECT NOTES STAT, V81, P1; Uhler C, 2013, ANN STAT, V41, P436, DOI 10.1214/12-AOS1080; Wiethoff S, 2014, BRAIN STIMUL, V7, P468, DOI 10.1016/j.brs.2014.02.003; Yamada M, 2014, NEURAL COMPUT, V26, P185, DOI 10.1162/NECO_a_00537; Zhang K., 2011, P C UNCERTAINTY ARTI, P804	39	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904023
C	McDuff, D; Song, YL; Kapoor, A; Ma, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		McDuff, Daniel; Song, Yale; Kapoor, Ashish; Ma, Shuang			Characterizing Bias in Classifiers using Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Models that are learned from real-world data are often biased because the data used to train them is biased. This can propagate systemic human biases that exist and ultimately lead to inequitable treatment of people, especially minorities. To characterize bias in learned classifiers, existing approaches rely on human oracles labeling real-world examples to identify the "blind spots" of the classifiers; these are ultimately limited due to the human labor required and the finite nature of existing image examples. We propose a simulation-based approach for interrogating classifiers using generative adversarial models in a systematic manner. We incorporate a progressive conditional generative model for synthesizing photo-realistic facial images and Bayesian Optimization for an efficient interrogation of independent facial image classification systems. We show how this approach can be used to efficiently characterize racial and gender biases in commercial systems.	[McDuff, Daniel; Song, Yale; Kapoor, Ashish] Microsoft, Redmond, WA 98052 USA; [Ma, Shuang] SUNY Buffalo, Buffalo, NY USA	Microsoft; State University of New York (SUNY) System; State University of New York (SUNY) Buffalo	McDuff, D (corresponding author), Microsoft, Redmond, WA 98052 USA.	damcduff@microsoft.com; yalesong@microsoft.com; akapoor@microsoft.com; shuangma@buffalo.edu						[Anonymous], 2014, CORR; [Anonymous], P AAAI; Athalye Anish, 2017, ARXIV PREPRINT ARXIV; Baeza-Yates R, 2016, PROCEEDINGS OF THE 2016 ACM WEB SCIENCE CONFERENCE (WEBSCI'16), P1, DOI 10.1145/2908131.2908135; Bareinboim E, 2016, P NATL ACAD SCI USA, V113, P7345, DOI 10.1073/pnas.1510507113; Brochu E, 2010, ARXIV PREPRINT ARXIV; Buolamwini J., 2018, C FAIRN ACC TRANSP, P77; Buolamwini J.A., 2017, THESIS; Caliskan A, 2017, SCIENCE, V356, DOI 10.1126/science.aal4230; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Dong H, 2017, ARXIV170102676; Dwork C., 2012, P 3 INN THEOR COMP S, P214; FITZPATRICK TB, 1988, ARCH DERMATOL, V124, P869, DOI 10.1001/archderm.124.6.869; Frazier P.I., 2018, ARXIV, DOI 10.1287/educ.2018.0188; Garvie Clare, 2016, PERPETUAL LINE UP UN; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6; Hajian S, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2125, DOI 10.1145/2939672.2945386; Haralick R. M., 1992, BMVC92. Proceedings of the British Machine Vision Conference, P1; Karras Tero, 2018, INT C LEARN REPR; Klare BF, 2015, PROC CVPR IEEE, P1931, DOI 10.1109/CVPR.2015.7298803; Klare BF, 2012, IEEE T INF FOREN SEC, V7, P1789, DOI 10.1109/TIFS.2012.2214212; Kortylewski Adam, 2018, P IEEE C COMP VIS PA, P2093; Kortylewski Adam, 2019, P IEEE C COMP VIS PA; Lakkaraju H., 2017, AAAI, V1, P2; Lakkaraju Himabindu, 2016, CORR; Lepri B., 2018, PHILOS TECHNOL, V31, P611, DOI [10.1007/s13347-017-0279-x, DOI 10.1007/S13347-017-0279-X]; McDuff D., 2018, ARXIV181000471; Qiu WC, 2016, LECT NOTES COMPUT SC, V9915, P909, DOI 10.1007/978-3-319-49409-8_75; Raji I.D., 2019, AAAI ACM C AI ETH SO; Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4; Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P252, DOI 10.1109/ICCVW.2015.41; Ryu H.J., 2017, ARXIV171200193; Shrivastava A., 2017, CVPR, V2, P5; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Thekumparampil K.K., 2018, ADV NEURAL INFORM PR, P10271; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Tramer F., 2017, ARXIV170403453; Vazquez D, 2014, IEEE T PATTERN ANAL, V36, P797, DOI 10.1109/TPAMI.2013.163; Veeravasarapu V., 2015, ARXIV151201401; Veeravasarapu V. S. R., 2015, ARXIV151201030; Veeravasarapu V.S.R., 2016, ARXIV160509582; Wilson Benjamin, 2019, ARXIV190211097; Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143; Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47; Yang H., 2017, ARXIV171110352; Yang J., 2017, CORR; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629	48	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305040
C	Meng, ZQ; Liang, SS; Fang, JY; Xiao, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Meng, Zaiqiao; Liang, Shangsong; Fang, Jinyuan; Xiao, Teng			Semi-supervisedly Co-embedding Attributed Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep generative models (DGMs) have achieved remarkable advances. Semi-supervised variational auto-encoders (SVAE) as a classical DGM offer a principled framework to effectively generalize from small labelled data to large unlabelled ones, but it is difficult to incorporate rich unstructured relationships within the multiple heterogeneous entities. In this paper, to deal with the problem, we present a semi-supervised co-embedding model for attributed networks (SCAN) based on the generalized SVAE for heterogeneous data, which collaboratively learns low-dimensional vector representations of both nodes and attributes for partially labelled attributed networks semi-supervisedly. The node and attribute embeddings obtained in a unified manner by our SCAN can benefit for capturing not only the proximities between nodes but also the affinities between nodes and attributes. Moreover, our model also trains a discriminative network to learn the label predictive distribution of nodes. Experimental results on real-world networks demonstrate that our model yields excellent performance in a number of applications such as attribute inference, user profiling and node classification compared to the state-of-the-art baselines.	[Meng, Zaiqiao] Sun Yat Sen Univ, Dept Comp Sci, Guangzhou, Peoples R China; [Meng, Zaiqiao] Univ Glasgow, Glasgow, Lanark, Scotland; [Liang, Shangsong; Fang, Jinyuan; Xiao, Teng] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou, Peoples R China; [Meng, Zaiqiao] Sun Yat Sen Univ, Guangzhou, Peoples R China	Sun Yat Sen University; University of Glasgow; Sun Yat Sen University; Sun Yat Sen University	Liang, SS (corresponding author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou, Peoples R China.	zaiqiao.meng@gmail.com; liangshangsong@gmail.com; fangjy6@gmail.com; tengxiao01@gmail.com	Liang, Shangsong/AAB-5514-2021	Liang, Shangsong/0000-0003-1625-2168	National Natural Science Foundation of China [61906219]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This research was partially supported by the National Natural Science Foundation of China (Grant No. 61906219).	[Anonymous], 2014, ICLR; Bojchevski A., 2018, ICLR; Cao S., 2015, P 24 ACM INT C INF K, P891, DOI DOI 10.1145/2806416.2806512; Chakrabarti Deepayan, 2014, ICML; Cheng Y, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1965; Corro C., 2019, ICLR; Ding M, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P913, DOI 10.1145/3269206.3271768; Dos Santos L., 2016, P JOINT EUROPEAN C M, P606; Gao HC, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3364; GONG NZ, 2014, TIST, V5, DOI DOI 10.1145/2594455; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; He S., 2015, PROC 24 ACM INT C IN, P623, DOI [DOI 10.1145/2806416.2806502, 10.1145/2806416.2806502, 10.1145]; Huang X, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P270, DOI 10.1145/3159652.3159655; Huang X, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P731, DOI 10.1145/3018661.3018667; Jang Eric, 2017, ICLR 2017; Kingma DP, 2014, PR MACH LEARN RES, V32, P1782; Kingma DP, 2014, ADV NEUR IN, P3581, DOI DOI 10.5555/2969033.2969226; Kipf T.N., 2017, 5 INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1609.02907; Kipf Thomas N, 2016, NIPS WORKSHOP BAYESI; Liang DW, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P689, DOI 10.1145/3178876.3186150; Liang J., 2018, P 2018 SIAM INT C DA, P153; Liang SS, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1764, DOI 10.1145/3219819.3220043; Meng ZQ, 2019, PROCEEDINGS OF THE TWELFTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'19), P393, DOI 10.1145/3289600.3291015; Perozzi Bryan, 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732; Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093; Teh Y. W., 2017, ICLR; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225, DOI 10.1145/2939672.2939753; Wang SH, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P137, DOI 10.1145/3132847.3132905; Wang Xiao, 2017, AAAI; Xiao Teng, 2019, AAAI; Xu Weidi, 2017, AAAI; Yang C, 2017, WWW'17 COMPANION: PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P564, DOI 10.1145/3041021.3054181; Yang C, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2111; Yang H., 2018, P 27 INT JOIINT C AR, P3155; Yang Z, 2016, PR MACH LEARN RES, V48; ZHANG B, 2017, SDM, P633, DOI DOI 10.1007/978-3-319-52132-9_63	38	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306050
C	Mhammedi, Z; Grunwald, PD; Guedj, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mhammedi, Zakaria; Grunwald, Peter D.; Guedj, Benjamin			PAC-Bayes Un-Expected Bernstein Inequality	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				RISK BOUNDS; STABILITY	We present a new PAC-Bayesian generalization bound. Standard bounds contain a root L-n center dot KL/n complexity term which dominates unless L-n, the empirical error of the learning algorithm's randomized predictions, vanishes. We manage to replace L-n by a term which vanishes in many more situations, essentially whenever the employed learning algorithm is sufficiently stable on the dataset at hand. Our new bound consistently beats state-of-the-art bounds both on a toy example and on UCI datasets (with large enough n). Theoretically, unlike existing bounds, our new bound can be expected to converge to 0 faster whenever a Bemstein/Tsybakov condition holds, thus connecting PAC-Bayesian generalization and excess risk bounds-for the latter it has long been known that faster convergence can be obtained under Bernstein conditions. Our main technical tool is a new concentration inequality which is like Bernstein's but with X-2 taken outside its expectation.	[Mhammedi, Zakaria] Australian Natl Univ, Canberra, ACT, Australia; [Mhammedi, Zakaria] Data61, Canberra, ACT, Australia; [Grunwald, Peter D.] CWI, Amsterdam, Netherlands; [Grunwald, Peter D.] Leiden Univ, Leiden, Netherlands; [Guedj, Benjamin] INRIA, Rocquencourt, France; [Guedj, Benjamin] UCL, London, England	Australian National University; Commonwealth Scientific & Industrial Research Organisation (CSIRO); Leiden University; Leiden University - Excl LUMC; Inria; University of London; University College London	Mhammedi, Z (corresponding author), Australian Natl Univ, Canberra, ACT, Australia.; Mhammedi, Z (corresponding author), Data61, Canberra, ACT, Australia.	zak.mhammedi@anu.edu.au; pdg@cwi.nl; benjamin.guedj@inria.fr		Guedj, Benjamin/0000-0003-1237-7430	Australian Research Council; Data61	Australian Research Council(Australian Research Council); Data61	An anonymous referee made some highly informed remarks on our paper, which led us to substantially rewrite the paper and made us understand our own work much better. Part of this work was performed while Zakaria Mhammedi was interning at the Centrum Wiskunde & Informatica (CWI). This work was also supported by the Australian Research Council and Data61.	Alquier P, 2018, MACH LEARN, V107, P887, DOI 10.1007/s10994-017-5690-0; Ambroladze Amiran, 2007, ADV NEURAL INFORM PR, P9; [Anonymous], 2007, THERMODYN STAT LEARN; Audibert JY, 2007, LECT NOTES ARTIF INT, V4754, P150; Audibert Jean-Yves, 2004, THESIS, V6, P29; Bartlett PL, 2006, PROBAB THEORY REL, V135, P311, DOI 10.1007/s00440-005-0462-3; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Catoni Olivier, 2003, PREPRINT; Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Dua D., 2017, UCI MACHINE LEARNING; Dziugaite G. K., 2017, UAI; Fan XQ, 2015, ELECTRON J PROBAB, V20, DOI 10.1214/EJP.v20-3496; Germain P, 2015, J MACH LEARN RES, V16, P787; Germain Pascal, 2009, INT C MACH LEARN; Grinwald Peter D., 2019, J MACHINE LEARNING R; Guedj B, 2019, P 2 C FRENCH MATH SO; Howard Steven R, 2018, ARXIV181008240; Koolen Wouter M., 2016, ADV NEURAL INFORM PR, V29, P4457; Langford J, 2002, ADV NEUR IN, V14, P809; Massart P, 2006, ANN STAT, V34, P2326, DOI 10.1214/009053606000000786; Maurer Andreas, 2004, CS0411099 ARXIV; Maurer Andreas, 2009, P COLT 2009; McAllester D. A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P164, DOI 10.1145/307400.307435; McAllester D. A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P230, DOI 10.1145/279943.279989; McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064; Mehta NA, 2017, PR MACH LEARN RES, V54, P1085; Mnih V., 2008, P 25 INT C MACH LEAR; Neyshabur Behnam, 2018, P 6 INT C LEARN REPR; Rivasplata O., 2018, ADV NEURAL INFORM PR, P9214; Seeger M, 2003, J MACH LEARN RES, V3, P233, DOI 10.1162/153244303765208386; Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635; Tolstikhin Ilya O., 2013, ADV NEURAL INFORM PR, P109; Tsybakov AB, 2004, ANN STAT, V32, P135; van Erven T, 2015, J MACH LEARN RES, V16, P1793; Wintenberger O, 2017, MACH LEARN, V106, P119, DOI 10.1007/s10994-016-5592-6; ZADROZNY B, 2003, ADV NEURAL INFORM PR, P435; Zhou Wenda, 2019, ICLR	40	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903078
C	Mo, S; Kim, C; Kim, S; Cho, M; Shin, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mo, Sangwoo; Kim, Chiheon; Kim, Sungwoong; Cho, Minsu; Shin, Jinwoo			Mining GOLD Samples for Conditional GANs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Conditional generative adversarial networks (cGANs) have gained a considerable attention in recent years due to its class-wise controllability and superior quality for complex generation tasks. We introduce a simple yet effective approach to improving cGANs by measuring the discrepancy between the data distribution and the model distribution on given samples. The proposed measure, coined the gap of log-densities (GOLD), provides an effective self-diagnosis for cGANs while being efficiently computed from the discriminator. We propose three applications of the GOLD: example re-weighting, rejection sampling, and active learning, which improve the training, inference, and data selection of cGANs, respectively. Our experimental results demonstrate that the proposed methods outperform corresponding baselines for all three applications on different image datasets.	[Mo, Sangwoo] Korea Adv Inst Sci & Technol, Daejeon, South Korea; [Kim, Chiheon; Kim, Sungwoong] Kakao Brain, Jeju City, South Korea; [Cho, Minsu] POSTECH, Pohang, South Korea; [Shin, Jinwoo] Korea Adv Inst Sci & Technol, AItrics, Daejeon, South Korea	Korea Advanced Institute of Science & Technology (KAIST); Pohang University of Science & Technology (POSTECH); Korea Advanced Institute of Science & Technology (KAIST)	Mo, S (corresponding author), Korea Adv Inst Sci & Technol, Daejeon, South Korea.	swmo@kaist.ac.kr; chiheon.kim@kakaobrain.com; swkim@kakaobrain.com; mscho@postech.ac.kr; jinwoos@kaist.ac.kr	Cho, Minsu/AAR-6323-2020		Information Technology Research Center (ITRC) support program [IITP-2019-2016-0-00288]; Next-Generation Information Computing Development Program [NRF2017M3C4A7069369]; Institute of Information & communications Technology Planning & Evaluation (IITP) grant - Ministry of Science and ICT, Korea (MSIT) [2017-0-01779]; Brain Cloud team at Kakao Brain	Information Technology Research Center (ITRC) support program; Next-Generation Information Computing Development Program; Institute of Information & communications Technology Planning & Evaluation (IITP) grant - Ministry of Science and ICT, Korea (MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); Brain Cloud team at Kakao Brain	This research was supported by the Information Technology Research Center (ITRC) support program (IITP-2019-2016-0-00288), Next-Generation Information Computing Development Program (NRF2017M3C4A7069369), and Institute of Information & communications Technology Planning & Evaluation (IITP) grant (No.2017-0-01779, A machine learning and statistical inference framework for explainable artificial intelligence), funded by the Ministry of Science and ICT, Korea (MSIT). We also appreciate GPU support from Brain Cloud team at Kakao Brain.	Arjovsky M, 2017, PR MACH LEARN RES, V70; Azadi Samaneh, 2018, ARXIV181006758; Beluch WH, 2018, PROC CVPR IEEE, P9368, DOI 10.1109/CVPR.2018.00976; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Brock Andrew, 2018, ARXIV180911096; Chang H. -S., 2017, ADV NEURAL INFORM PR, V30, P1002; Chen T., 2018, ARXIV PREPRINT ARXIV; Chen T., 2018, ARXIV181001365; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Gissin Daniel, 2019, ARXIV190706347; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Hensel M, 2017, ADV NEUR IN, V30; Hosseini-Asl E., 2018, ARXIV180700374; Huang SJ, 2014, IEEE T PATTERN ANAL, V36, P1936, DOI 10.1109/TPAMI.2014.2307881; Jolicoeur-Martineau Alexia, 2018, ARXIV180700734; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Katharopoulos A, 2018, PR MACH LEARN RES, V80; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kumar A, 2010, ASIA PACIF MICROWAVE, P1189; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee K., 2017, ARXIV171109325; Lesort Timoth6e, 2018, ARXIV180610840; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lucic M, 2019, PR MACH LEARN RES, V97; Mirza M., 2014, ARXIV; Miyato Takeru, 2018, ARXIV180205637; Mohamed Shakir, 2016, ARXIV161003483; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Odena A, 2018, PR MACH LEARN RES, V80; Odena A, 2017, PR MACH LEARN RES, V70; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Ravuri S., 2019, SEEING IS NOT NECESS; Reed S. E., 2016, ADV NEURAL INFORM PR, P217; Ren K., 2017, P INT C LEARNING REP; Ren Mengye, 2018, ARXIV PREPRINT ARXIV; Robert C., 2013, MONTE CARLO STAT MET; Sajjadi Mehdi SM, 2018, ADV NEURAL INFORM PR, P5234; Salimans T, 2016, ADV NEUR IN, V29; Sener Ozan, 2018, ARXIV170800489; Settles B., 2009, ACTIVE LEARNING LIT; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Sinha S, 2019, IEEE I CONF COMP VIS, P5971, DOI 10.1109/ICCV.2019.00607; Sonderby C. K., 2016, ARXIV PREPRINT ARXIV; Sricharan K., 2017, ARXIV170805789; Turner R., 2018, P INT C MACH LEARN P; Xiao H., 2017, FASHION MNIST NOVEL; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7; Zhang Han, 2018, ARXIV180508318; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	58	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306020
C	Muzellec, B; Cuturi, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Muzellec, Boris; Cuturi, Marco			Subspace Detours: Building Transport Plans that are Optimal on Subspace Projections	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Computing optimal transport (OT) between measures in high dimensions is doomed by the curse of dimensionality. A popular approach to avoid this curse is to project input measures on lower-dimensional subspaces (1D lines in the case of sliced Wasserstein distances), solve the OT problem between these reduced measures, and settle for the Wasserstein distance between these reductions, rather than that between the original measures. This approach is however difficult to extend to the case in which one wants to compute an OT map (a Monge map) between the original measures. Since computations are carried out on lower-dimensional projections, classical map estimation techniques can only produce maps operating in these reduced dimensions. We propose in this work two methods to extrapolate, from an transport map that is optimal on a subspace, one that is nearly optimal in the entire space. We prove that the best optimal transport plan that takes such "subspace detours" is a generalization of the Knothe-Rosenblatt transport. We show that these plans can be explicitly formulated when comparing Gaussian measures (between which the Wasserstein distance is commonly referred to as the Bures or Frechet distance). We provide an algorithm to select optimal subspaces given pairs of Gaussian measures, and study scenarios in which that mediating subspace can be selected using prior information. We consider applications to semantic mediation between elliptic word embeddings and domain adaptation with Gaussian mixture models.	[Muzellec, Boris; Cuturi, Marco] ENSAE, CREST, Palaiseau, France; [Cuturi, Marco] Google Brain, Mountain View, CA USA	Institut Polytechnique de Paris; Google Incorporated	Muzellec, B (corresponding author), ENSAE, CREST, Palaiseau, France.	boris.muzellec@ensae.fr; cuturi@google.com						Ambrosio L, 2005, LEC MATH; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bhatia R., 2018, EXPO MATH; Bonneel N, 2015, J MATH IMAGING VIS, V51, P22, DOI 10.1007/s10851-014-0506-3; Carlier Guillaume, 2009, SIAM J MATH ANAL; Chen YX, 2019, IEEE ACCESS, V7, P6269, DOI 10.1109/ACCESS.2018.2889838; Courty Nicolas, 2014, NIPS WORKSH OPT TRAN; Courty Nicolas, 2017, NIPS, P3730; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Deshpande I, 2018, PROC CVPR IEEE, P3483, DOI 10.1109/CVPR.2018.00367; DUDLEY RM, 1969, ANN MATH STAT, V40, P40, DOI 10.1214/aoms/1177697802; Ferradans S, 2014, SIAM J IMAGING SCI, V7, P1853, DOI 10.1137/130929886; GELBRICH M, 1990, MATH NACHR, V147, P185, DOI 10.1002/mana.19901470121; Genevay A, 2019, PR MACH LEARN RES, V89; Genevay A, 2018, PR MACH LEARN RES, V84; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Heusel M., 2017, ADV NEURAL INFORM PR, V30, P6626; Kingma D. P, 2014, ARXIV13126114; Monge G., 1781, MEM MATH PHYS ACAD R, P666; Muzellec Boris, 2018, ADV NEURAL INFORM PR, P10237; Paty FP, 2019, PR MACH LEARN RES, V97; Paty Francois-Pierre, 2019, ARXIV190510812; Rabin J, 2014, IEEE IMAGE PROC, P4852, DOI 10.1109/ICIP.2014.7025983; Rabin J, 2012, LECT NOTES COMPUT SC, V6667, P435, DOI 10.1007/978-3-642-24785-9_37; Schiebinger G, 2019, CELL, V176, P928, DOI 10.1016/j.cell.2019.01.006; Seguy V., 2018, ICLR; Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Weed J, 2017, ARXIV170700087; Wu J., 2019, CVPR	34	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306087
C	Nakkiran, P; Kaplun, G; Kalimeris, D; Yang, T; Edelman, BL; Zhang, F; Barak, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Nakkiran, Preetum; Kaplun, Gal; Kalimeris, Dimitris; Yang, Tristan; Edelman, Benjamin L.; Zhang, Fred; Barak, Boaz			SGD on Neural Networks Learns Functions of Increasing Complexity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We perform an experimental study of the dynamics of Stochastic Gradient Descent (SGD) in learning deep neural networks for several real and synthetic classification tasks. We show that in the initial epochs, almost all of the performance improvement of the classifier obtained by SGD can be explained by a linear classifier. More generally, we give evidence for the hypothesis that, as iterations progress, SGD learns functions of increasing complexity. This hypothesis can be helpful in explaining why SGD-learned classifiers tend to generalize well even in the over-parameterized regime. We also show that the linear classifier learned in the initial stages is "retained" throughout the execution even if training is continued to the point of zero training error, and complement this with a theoretical result in a simplified model. Key to our work is a new measure of how well one classifier explains the performance of another, based on conditional mutual information.	[Nakkiran, Preetum; Kaplun, Gal; Kalimeris, Dimitris; Yang, Tristan; Edelman, Benjamin L.; Zhang, Fred; Barak, Boaz] Harvard Univ, Cambridge, MA 02138 USA	Harvard University	Nakkiran, P (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	preetum@cs.harvard.edu; galkaplun@g.harvard.edu; kalimeris@g.harvard.edu; tristanyang@college.harvard.edu; bedelman@g.harvard.edu; hzhang@g.harvard.edu; b@boazbarak.org			NSF [CCF 1565264, CNS 1618026, CCF 1565641, CCF 1715187]; NSF GRFP [DGE1144152]; Simons Investigator Fellowship; Investigator Award	NSF(National Science Foundation (NSF)); NSF GRFP(National Science Foundation (NSF)NSF - Office of the Director (OD)); Simons Investigator Fellowship; Investigator Award	This work was supported by NSF awards CCF 1565264, CNS 1618026, CCF 1565641, CCF 1715187, NSF GRFP Grant No. DGE1144152, a Simons Investigator Fellowship, and Investigator Award.	Allen-Zhu Zeyuan, 2018, ABS181104918 ARXIV; [Anonymous], 1993, P 6 ANN C COMPUTATIO, DOI DOI 10.1145/168304.168306; Arora S, 2018, PR MACH LEARN RES, V80; Arpit D., 2017, IMCL, P233; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; BARTLETT P. L., 2017, ADV NEURAL INFORM PR, V30, P6240; Bell A.J., 2003, P 5 INT WORKSH IND C, V2003; Brutzkus A., 2018, INT C LEARN REPR ICL; Brutzkus Alon, 2017, SGD LEARNS OVERPARAM; Dziugaite G. K., 2017, C UNC ART INT; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Golowich N., 2018, PROC C LEARN THEORY, P297; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Gunasekar S., 2017, ADV NEURAL INFORM PR, P6151; Gunasekar Suriya, 2018, ADV NEURAL INFORM PR, P9461; Hardt M, 2016, PR MACH LEARN RES, V48; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Ji Ziwei, 2019, C LEARN THEOR COLT; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Keskar N.S., 2017, ICLR; Komendantskaya E, 2018, ELECTRON P THEOR COM, P27, DOI 10.4204/EPTCS.278.5; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kuzborskij I., 2018, P MACHINE LEARNING R, P2820; Liu S., 2020, ADV NEURAL INFORM PR, Vvol 33, P8543; McGill WJ, 1954, T IRE PROF GROUP INF, V4, P93, DOI [10.1109/TIT.1954.1057469, DOI 10.1109/TIT.1954.1057469)]; Nagarajan Vaishnavh, 2019, ARXIV190204742; Neyshabur B., 2017, ADV NEURAL INFORM PR, P5947; Neyshabur Behnam, 2018, INT C LEARN REPR ICL; Novak R., 2018, P INT C LEARN REPR, P1; Perez Guillermo Valle, 2019, INT C LEARN REPR ICL; Rahaman N, 2018, ARXIV180608734; Shwartz-Ziv Ravid, 2018, INT C LEARN REPR ICL; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Soudry D., 2018, INT C LEARN REPR ICL; Wan WF, 2018, IEEE IND ELEC, P3815, DOI 10.1109/IECON.2018.8591522; Wu Yifan, 2019, INT C ART INT STAT A, P1070; Xu Z.-Q. John., 2018, ARXIV181110146; Zhang Chiyuan, 2017, INT C LEARN REPR ICL; Zhou W., 2019, INT C LEARN REPR	42	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303048
C	Ness, R; Paneri, K; Vitek, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ness, Robert; Paneri, Kaushal; Vitek, Olga			Integrating Markov processes with structural causal modeling enables counterfactual inference in complex systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SIGNALING PATHWAYS	This manuscript contributes a general and practical framework for casting a Markov process model of a system at equilibrium as a structural causal model, and carrying out counterfactual inference. Markov processes mathematically describe the mechanisms in the system, and predict the system's equilibrium behavior upon intervention, but do not support counterfactual inference. In contrast, structural causal models support counterfactual inference, but do not identify the mechanisms. This manuscript leverages the benefits of both approaches. We define the structural causal models in terms of the parameters and the equilibrium dynamics of the Markov process models, and counterfactual inference flows from these settings. The proposed approach alleviates the identifiability drawback of the structural causal models, in that the counterfactual inference is consistent with the counterfactual trajectories simulated from the Markov process model. We showcase the benefits of this framework in case studies of complex biomolecular systems with nonlinear dynamics. We illustrate that, in presence of Markov process model mis-specification, counterfactual inference leverages prior data, and therefore estimates the outcome of an intervention more accurately than a direct simulation.	[Ness, Robert] Gamalon Inc, Boston, MA 02110 USA; [Paneri, Kaushal; Vitek, Olga] Northeastern Univ, Boston, MA 02115 USA	Northeastern University	Ness, R (corresponding author), Gamalon Inc, Boston, MA 02110 USA.	robert.ness@gamalon.com; kaushalpaneri@gmail.com; o.vitek@northeastern.edu						ALON U, 2006, INTRO SYSTEMS BIOL; [Anonymous], 2014, ARXIV 1401 4082; Balke A., 1994, P C UNC ART INT; Bianconi F, 2012, BIOTECHNOL ADV, V30, P142, DOI 10.1016/j.biotechadv.2011.05.010; Bingham Eli, 2018, J MACHINE LEARNING R; Blom T., 2019, P C UNC ART INT; Bongers S., 2018, ARXIV180308784; Bottou L, 2013, J MACH LEARN RES, V14, P3207; Buesing L., 2018, 181106272 ARXIV; DUBINS LE, 1966, ANN MATH STAT, V37, P837, DOI 10.1214/aoms/1177699364; Eberhardt F, 2007, PHILOS SCI, V74, P981, DOI 10.1086/525638; Hilborn Ray, 1997, V28; Hoffman M., 2012, ARXIV12067051; HORN F, 1972, ARCH RATION MECH AN, V47, P81; Huang CYF, 1996, P NATL ACAD SCI USA, V93, P10078, DOI 10.1073/pnas.93.19.10078; Jahnke T, 2007, J MATH BIOL, V54, P1, DOI 10.1007/s00285-006-0034-x; Joachims T, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1199, DOI 10.1145/2911451.2914803; Kim EK, 2010, BBA-MOL BASIS DIS, V1802, P396, DOI 10.1016/j.bbadis.2009.12.009; Mooij J. M., 2013, ARXIV13047920; Oberst Michael, 2019, ARXIV190505824; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Pearl J, 2019, J CAUSAL INFERENCE, V7, DOI 10.1515/jci-2019-2002; Pearl J, 2011, ANN MATH ARTIF INTEL, V61, P29, DOI 10.1007/s10472-011-9247-9; Peters J, 2017, ADAPT COMPUT MACH LE; Qiao L, 2007, PLOS COMPUT BIOL, V3, P1819, DOI 10.1371/journal.pcbi.0030184; Roese NJ, 1997, PSYCHOL BULL, V121, P133, DOI 10.1037/0033-2909.121.1.133; Tyson JJ, 2003, CURR OPIN CELL BIOL, V15, P221, DOI 10.1016/S0955-0674(03)00017-6; Wilkinso D.J., 2006, STOCHASTIC MODELLING; Wilkinson DJ, 2009, NAT REV GENET, V10, P122, DOI 10.1038/nrg2509	29	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905084
C	Obeid, D; Ramambason, H; Pehlevan, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Obeid, Dina; Ramambason, Hugo; Pehlevan, Cengiz			Structured and Deep Similarity Matching via Structured and Deep Hebbian Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				REPRESENTATIONS; BACKPROPAGATION	Synaptic plasticity is widely accepted to be the mechanism behind learning in the brain's neural networks. A central question is how synapses, with access to only local information about the network, can still organize collectively and perform circuit-wide learning in an efficient manner. In single-layered and all-to-all connected neural networks, local plasticity has been shown to implement gradient-based learning on a class of cost functions that contain a term that aligns the similarity of outputs to the similarity of inputs. Whether such cost functions exist for networks with other architectures is not known. In this paper, we introduce structured and deep similarity matching cost functions, and show how they can be optimized in a gradient-based manner by neural networks with local learning rules. These networks extend Foldiak's Hebbian/Anti-Hebbian network to deep architectures and structured feedforward, lateral and feedback connections. Credit assignment problem is solved elegantly by a factorization of the dual learning objective to synapse specific local objectives. Simulations show that our networks learn meaningful features.	[Obeid, Dina; Ramambason, Hugo; Pehlevan, Cengiz] Harvard Univ, John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA	Harvard University	Obeid, D (corresponding author), Harvard Univ, John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA.	dinaobeid@seas.harvard.edu; hugo_ramambason@g.harvard.edu; cpehlevan@seas.harvard.edu						[Anonymous], 2015, PROC C LEARN THEORY; Bahroun Y, 2017, LECT NOTES COMPUT SC, V10613, P354, DOI 10.1007/978-3-319-68600-4_41; Boutin Victor, 2019, ARXIV190207651; FOLDIAK P, 1990, BIOL CYBERN, V64, P165, DOI 10.1007/BF02331346; Freeman J, 2011, NAT NEUROSCI, V14, P1195, DOI 10.1038/nn.2889; Gregor Karol, 2011, ADV NEURAL INFORM PR, V24; GROSSBERG S, 1987, COGNITIVE SCI, V11, P23, DOI 10.1111/j.1551-6708.1987.tb00862.x; Guerguiev J, 2017, ELIFE, V6, DOI 10.7554/eLife.22901; Hertz J., 1991, INTRO THEORY NEURAL, DOI DOI 10.1201/9780429499661; HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088; Hu T, 2014, CONF REC ASILOMAR C, P613, DOI 10.1109/ACSSC.2014.7094519; Jiang L, 2014, 2014 INTERNATIONAL CONFERENCE ON PLANARIZATION/CMP TECHNOLOGY (ICPT), P209, DOI 10.1109/ICPT.2014.7017282; Kawulok M., 2016, ADV FACE DETECTION F, P189, DOI DOI 10.1007/978-3-319-25958-1_8; Kim E, 2018, PROC CVPR IEEE, P1111, DOI 10.1109/CVPR.2018.00122; KOLEN JF, 1994, 1994 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOL 1-7, P1375, DOI 10.1109/ICNN.1994.374486; Krotov D, 2019, P NATL ACAD SCI USA, V116, P7723, DOI 10.1073/pnas.1820458116; Kuang J, 2012, INT CONF CLOUD COMPU, P1013, DOI 10.1109/CCIS.2012.6664534; Lee DH, 2015, LECT NOTES ARTIF INT, V9284, P498, DOI 10.1007/978-3-319-23528-8_31; Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276; Minden V, 2018, CONF REC ASILOMAR C, P104, DOI 10.1109/ACSSC.2018.8645109; Nokland, 2016, ADV NEURAL INFORM PR, V29, P1037; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Pehlevan C, 2018, NEURAL COMPUT, V30, P84, DOI [10.1162/neco_a_01018, 10.1162/NECO_a_01018]; Pehlevan C, 2014, CONF REC ASILOMAR C, P769, DOI 10.1109/ACSSC.2014.7094553; Pehlevan C, 2015, NEURAL COMPUT, V27, P1461, DOI 10.1162/NECO_a_00745; Pehlevan Cengiz, 2019, ARXIV190801867; Pehlevan Cengiz, 2019, ARXIV190201429; Richards BA, 2019, CURR OPIN NEUROBIOL, V54, P28, DOI 10.1016/j.conb.2018.08.003; Rozell CJ, 2008, NEURAL COMPUT, V20, P2526, DOI 10.1162/neco.2008.03-07-486; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Sacramento J, 2018, ADV NEUR IN, V31; Scellier B, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00024; Schultz W, 1997, SCIENCE, V275, P1593, DOI 10.1126/science.275.5306.1593; Sengupta A., 2018, ADV NEURAL INFORM PR, V31, P7080; Whittington J. C., 2019, TRENDS COGNITIVE SCI; Whittington JCR, 2017, NEURAL COMPUT, V29, P1229, DOI 10.1162/NECO_a_00949; Xie XH, 2003, NEURAL COMPUT, V15, P441, DOI 10.1162/089976603762552988	37	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907010
C	Pagliana, N; Rosasco, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Pagliana, Nicolo; Rosasco, Lorenzo			Implicit Regularization of Accelerated Methods in Hilbert Spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study learning properties of accelerated gradient descent methods for linear least-squares in Hilbert spaces. We analyze the implicit regularization properties of Nesterov acceleration and a variant of heavy-ball in terms of corresponding learning error bounds. Our results show that acceleration can provides faster bias decay than gradient descent, but also suffers of a more unstable behavior. As a result acceleration cannot be in general expected to improve learning accuracy with respect to gradient descent, but rather to achieve the same accuracy with reduced computations. Our theoretical results are validated by numerical simulations. Our analysis is based on studying suitable polynomials induced by the accelerated dynamics and combining spectral techniques with concentration inequalities.	[Pagliana, Nicolo; Rosasco, Lorenzo] Univ Genoa, Genoa, Italy; [Pagliana, Nicolo] DIMA, Modena, Italy; [Pagliana, Nicolo; Rosasco, Lorenzo] MaLGa, Genoa, Italy; [Rosasco, Lorenzo] DIBRIS, Genoa, Italy; [Rosasco, Lorenzo] IIT, Genoa, Italy; [Rosasco, Lorenzo] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	University of Genoa; Istituto Italiano di Tecnologia - IIT; Massachusetts Institute of Technology (MIT)	Pagliana, N (corresponding author), Univ Genoa, Genoa, Italy.; Pagliana, N (corresponding author), DIMA, Modena, Italy.; Pagliana, N (corresponding author), MaLGa, Genoa, Italy.	pagliana@dima.unige.it; lrosasco@mit.edu			Center for Brains, Minds and Machines (CBMM) - NSF STC award [CCF-1231216]; Italian Institute of Technology; AFOSR [FA9550-17-1-0390, BAA-AFRL-AFOSR-2016-0007]; EU H2020-MSCA-RISE project [NoMADS -DLV-777826]	Center for Brains, Minds and Machines (CBMM) - NSF STC award; Italian Institute of Technology(Istituto Italiano di Tecnologia - IIT); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); EU H2020-MSCA-RISE project	This material is based upon work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216, and the Italian Institute of Technology. We gratefully acknowledge the support of NVIDIA Corporation for the donation of the Titan Xp GPUs and the Tesla k40 GPU used for this research. L. R. acknowledges the financial support of the AFOSR projects FA9550-17-1-0390 and BAA-AFRL-AFOSR-2016-0007 (European Office of Aerospace Research and Development), and the EU H2020-MSCA-RISE project NoMADS -DLV-777826. N.P. would like to thank Murata Tomoya for the useful observations.	[Anonymous], 2015, ADV NEURAL INFORM PR; Attouch H, 2019, ESAIM CONTR OPTIM CA, V25, DOI 10.1051/cocv/2017083; Baldassarre L, 2012, MACH LEARN, V87, P259, DOI 10.1007/s10994-012-5282-y; Blanchard G, 2018, FOUND COMPUT MATH, V18, P971, DOI 10.1007/s10208-017-9359-7; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Charles Z., 2017, ARXIV171008402; Chen Y., 2018, ARXIV180401619; Engl HW, 1996, REGULARIZATION INVER, V375; FUJII J, 1993, P AM MATH SOC, V118, P827, DOI 10.2307/2160128; Gunasekar Suriya, 2018, ADV NEURAL INFORM PR, P9461; Hardt M, 2015, ARXIV150901240; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Lin J., 2018, APPL COMPUTATIONAL H, DOI 10.1016/j.acha.2018.09.009.; Lin JH, 2016, PR MACH LEARN RES, V48; Lin Junhong, 2018, 105022 STAT, V1050, P22; Lin Junhong, 2016, ADV NEURAL INFORM PR, P4556; Mathe P, 2002, NUMER FUNC ANAL OPT, V23, P623, DOI 10.1081/NFA-120014755; Mathe P, 2006, MATH COMPUT, V75, P1913, DOI 10.1090/S0025-5718-06-01873-4; NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543; Neubauer A, 2017, J INVERSE ILL-POSE P, V25, P381, DOI 10.1515/jiip-2016-0060; Polyak Boris T, 1987, TECHNICAL REPORT; Raskutti G, 2014, J MACH LEARN RES, V15, P335; Soudry D, 2018, J MACH LEARN RES, V19; Steinwart I., 2008, SUPPORT VECTOR MACHI; Szeg G., 1939, ORTHOGONAL POLYNOMIA, V23; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2; Zavriev S., 1993, COMPUTATIONAL MATH M, V4, P336, DOI DOI 10.1007/BF01128757; Zhang Chiyuan, 2016, ARXIV161103530	33	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906017
C	Peltola, T; Celikok, MM; Daee, P; Kaski, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Peltola, Tomi; Celikok, Mustafa Mert; Daee, Pedram; Kaski, Samuel			Machine Teaching of Active Sequential Learners	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Machine teaching addresses the problem of finding the best training data that can guide a learning algorithm to a target model with minimal effort. In conventional settings, a teacher provides data that are consistent with the true data distribution. However, for sequential learners which actively choose their queries, such as multi-armed bandits and active learners, the teacher can only provide responses to the learner's queries, not design the full data. In this setting, consistent teachers can be sub-optimal for finite horizons. We formulate this sequential teaching problem, which current techniques in machine teaching do not address, as a Markov decision process, with the dynamics nesting a model of the learner and the actions being the teacher's responses. Furthermore, we address the complementary problem of learning from a teacher that plans: to recognise the teaching intent of the responses, the learner is endowed with a model of the teacher. We test the formulation with multi-armed bandit learners in simulated experiments and a user study. The results show that learning is improved by (i) planning teaching and (ii) the learner having a model of the teacher. The approach gives tools to taking into account strategic (planning) behaviour of users of interactive intelligent systems, such as recommendation engines, by considering them as boundedly optimal teachers.	[Peltola, Tomi; Celikok, Mustafa Mert; Daee, Pedram; Kaski, Samuel] Aalto Univ, Dept Comp Sci, Helsinki Inst Informat Technol HIIT, Helsinki, Finland	Aalto University; University of Helsinki	Peltola, T (corresponding author), Aalto Univ, Dept Comp Sci, Helsinki Inst Informat Technol HIIT, Helsinki, Finland.	tomi.peltola@aalto.fi; mustafa.celikok@aalto.fi; pedram.daee@aalto.fi; samuel.kaski@aalto.fi	Kaski, Samuel/B-6684-2008	Kaski, Samuel/0000-0003-1925-9154	Academy of Finland (Flagship programme: Finnish Center for Artificial Intelligence, FCAI) [319264, 313195, 305780, 292334]; Finnish Science Foundation for Technology and Economics KAUTE	Academy of Finland (Flagship programme: Finnish Center for Artificial Intelligence, FCAI); Finnish Science Foundation for Technology and Economics KAUTE	This work was financially supported by the Academy of Finland (Flagship programme: Finnish Center for Artificial Intelligence, FCAI; grants 319264, 313195, 305780, 292334). Mustafa Mert Celikok is partially funded by the Finnish Science Foundation for Technology and Economics KAUTE. We acknowledge the computational resources provided by the Aalto Science-IT Project. We thank Antti Oulasvirta and Marta Soare for comments that improved the article.	Albrecht SV, 2018, ARTIF INTELL, V258, P66, DOI 10.1016/j.artint.2018.01.002; [Anonymous], 2018, ARXIV180105927; Baker CL, 2017, NAT HUM BEHAV, V1, DOI 10.1038/s41562-017-0064; Bingham E, 2019, J MACH LEARN RES, V20; Brochu Eric, 2010, P 2010 ACM SIGGRAPHE, P103, DOI [10.5555/1921427.1921443, DOI 10.5555/1921427.1921443]; Brown Daniel S., 2019, P 33 AAAI C ART INT; Cakmak Maya, 2012, P 26 AAAI C ART INT, P1536; Chandrasekaran Arjun, 2017, ARXIV170400717; Choi J, 2011, INT GEOSCI REMOTE SE, P1989, DOI 10.1109/IGARSS.2011.6049518; Daee P, 2018, IUI 2018: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P305, DOI 10.1145/3172944.3172989; Doliwa T, 2014, J MACH LEARN RES, V15, P3107; Fisac Jaime F., 2017, INT S ROB RES ISRR; Fischer G, 2001, USER MODEL USER-ADAP, V11, P65, DOI 10.1023/A:1011145532042; Frans A. OliehoekA., 2016, CONCISE INTRO DECENT; Gelman A., 2021, BAYESIAN DATA ANAL; Gershman SJ, 2015, SCIENCE, V349, P273, DOI 10.1126/science.aac6076; Gmytrasiewicz PJ, 2005, J ARTIF INTELL RES, V24, P49, DOI 10.1613/jair.1579; GOLDMAN SA, 1995, J COMPUT SYST SCI, V50, P20, DOI 10.1006/jcss.1995.1003; Guez A, 2013, J ARTIF INTELL RES, V48, P841, DOI 10.1613/jair.4117; Hadfield-Menell D, 2016, ADV NEURAL INFORM PR, V29, P3909; Holyoak K. J., 1995, INVITATION COGNITIVE, P267; Jun Kwang-Sung, 2018, ADV NEURAL INFORM PR, P3640; Lessard L., 2019, 22 INT CON ART INT S, P2495; Liu WY, 2017, PR MACH LEARN RES, V70; Liu Weiyang, 2018, P 35 INT C MACH LEAR, V80, P3141; Ma YZ, 2018, LECT NOTES COMPUT SC, V11199, P186, DOI 10.1007/978-3-030-01554-1_11; Marchionini G, 2006, COMMUN ACM, V49, P41, DOI 10.1145/1121949.1121979; Mei SK, 2015, AAAI CONF ARTIF INTE, P2871; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Newell A, 1972, HUMAN PROBLEM SOLVIN; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Pynadath DV, 2002, J ARTIF INTELL RES, V16, P389, DOI 10.1613/jair.1024; Rafferty AN, 2016, COGNITIVE SCI, V40, P1290, DOI 10.1111/cogs.12290; Ramachandran D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2586; Ruotsalo T, 2018, ACM T INFORM SYST, V36, DOI 10.1145/3231593; Ruotsalo T, 2015, COMMUN ACM, V58, P86, DOI 10.1145/2656334; Russo DJ, 2018, FOUND TRENDS MACH LE, V11, P1, DOI 10.1561/2200000070; Schmit S, 2018, INT C ART INT STAT A, P862; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Whitehill J, 2018, IEEE T LEARN TECHNOL, V11, P152, DOI 10.1109/TLT.2017.2692761; Williams RB, 2019, INT J GAMING COMPUT-, V11, P1, DOI 10.4018/IJGCMS.2019010101; Woodward Mark P., 2012, ARXIV12040274; Zhu XJ, 2015, AAAI CONF ARTIF INTE, P4083; Zilles S, 2011, J MACH LEARN RES, V12, P349	44	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902079
C	Penedones, H; Riquelme, C; Vincent, D; Maennel, H; Mann, T; Barreto, A; Gelly, S; Neu, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Penedones, Hugo; Riquelme, Carlos; Vincent, Damien; Maennel, Hartmut; Mann, Timothy; Barreto, Andre; Gelly, Sylvain; Neu, Gergely			Adaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty Estimates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider the core reinforcement-learning problem of on-policy value function approximation from a batch of trajectory data, and focus on various issues of Temporal Difference (TD) learning and Monte Carlo (MC) policy evaluation. The two methods are known to achieve complementary bias-variance trade-off properties, with TD tending to achieve lower variance but potentially higher bias. In this paper, we argue that the larger bias of TD can be a result of the amplification of local approximation errors. We address this by proposing an algorithm that adaptively switches between TD and MC in each state, thus mitigating the propagation of errors. Our method is based on learned confidence intervals that detect biases of TD estimates. We demonstrate in a variety of policy evaluation tasks that this simple adaptive algorithm performs competitively with the best approach in hindsight, suggesting that learned confidence intervals are a powerful technique for adapting policy evaluation to use TD or MC returns in a data-driven way.	[Penedones, Hugo; Mann, Timothy; Barreto, Andre] DeepMind, London, England; [Riquelme, Carlos; Vincent, Damien; Maennel, Hartmut; Gelly, Sylvain] Google Brain, Mountain View, CA 94043 USA; [Neu, Gergely] Univ Pompeu Fabra, Barcelona, Spain	Google Incorporated; Pompeu Fabra University	Riquelme, C (corresponding author), Google Brain, Mountain View, CA 94043 USA.	rikel@google.com			"La Caixa" Banking Foundation; Google Faculty Research Award	"La Caixa" Banking Foundation(La Caixa Foundation); Google Faculty Research Award(Google Incorporated)	The authors thank Matthieu Geist for his comments and suggestions, and the anonymous reviewers for their valuable feedback. G. Neu was supported by "La Caixa" Banking Foundation through the Junior Leader Postdoctoral Fellowship Programme and a Google Faculty Research Award.	Amiranashvili Artemij, 2018, INT C LEARN REPR; Azizzadenesheli  Kamyar, 2018, ARXIV180204412; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Bellman RE, 1957, DYNAMIC PROGRAMMING; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Blundell C., 2015, ARXIV PREPRINT ARXIV; Downey C., 2010, ICML, P311; Efron B., 1982, JACKKNIFE BOOTSTRAP; Gal Y, 2016, PR MACH LEARN RES, V48; Geist M, 2014, J MACH LEARN RES, V15, P289; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402; Mandt S, 2016, PR MACH LEARN RES, V48; Mann T A, 2016, ARXIV161209465; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; RASMUSSEN CE, 2004, SUMMER SCH MACHINE L, V3176, P00063; Riquelme Carlos, 2018, ARXIV180209127; Scherrer B., 2010, 27 INT C MACH LEARN; Snoek J, 2015, PR MACH LEARN RES, V37, P2171; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton Rich, 2014, P MACHINE LEARNING R, P568; Sutton Richard, 2015, J MACHINE LEARNING R, V17; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Thomas Philip S, 2015, ADV NEURAL INFORM PR, V28, P334; Tsitsiklis JN, 1997, ADV NEUR IN, V9, P1075; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; White M., 2016, AAMAS	28	2	2	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903050
C	Poon, C; Liang, JW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Poon, Clarice; Liang, Jingwei			Trajectory of Alternating Direction Method of Multipliers and Adaptive Acceleration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CONVERGENCE; ALGORITHM	The alternating direction method of multipliers (ADMM) is one of the most widely used first-order methods in the literature owing to its simplicity, flexibility and efficiency. Over the years, numerous efforts are made to improve the performance of ADMM, such as the inertial technique. By studying the geometric properties of ADMM, we discuss the limitations of current inertial accelerated ADMM, then present and analyze an adaptive acceleration scheme for the method. Numerical experiments on problems arising from image processing, statistics and machine learning demonstrate the advantages of the proposed acceleration approach.	[Poon, Clarice] Univ Bath, Bath, Avon, England; [Liang, Jingwei] Univ Cambridge, Cambridge, England	University of Bath; University of Cambridge	Poon, C (corresponding author), Univ Bath, Bath, Avon, England.	cmhsp20@bath.ac.uk; jl993@cam.ac.uk			Leverhulme trust and Newton trust	Leverhulme trust and Newton trust(Leverhulme Trust)	We would like to thank Arieh Iserles for pointing out the connection between trajectory following adaptive acceleration and vector extrapolation. We also like to thank the reviewers whose comments helped to improve the paper. JL was partly supported by Leverhulme trust and Newton trust.	Aitken AC, 1926, P R SOC EDINB, V46, P289, DOI DOI 10.1017/S0370164600022070; Alvarez F, 2001, SET-VALUED ANAL, V9, P3, DOI 10.1023/A:1011253113155; ANDERSON DG, 1965, J ACM, V12, P547, DOI 10.1145/321296.321305; Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Brezinski C., 2001, NUMERICAL ANAL HIST, P113; Brezinski C., 2013, EXTRAPOLATION METHOD, V2; CABAY S, 1976, SIAM J NUMER ANAL, V13, P734, DOI 10.1137/0713060; Chambolle A, 2015, J OPTIMIZ THEORY APP, V166, P968, DOI 10.1007/s10957-015-0746-4; ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204; Eddy R., 1979, INFORM LINKAGE APPL, P387; Franca G., 2018, ARXIV180804048; Franca G, 2018, PR MACH LEARN RES, V80; Gabay D., 1975, DUAL ALGORITHM SOLUT; Gabay D., 1983, STUD MATH APPL, V15, P299, DOI [DOI 10.1016/S0168-2024(08)70034-1, 10.1016/S0168-2024(08)70034-1]; GLOWINSKI R, 1975, REV FR AUTOMAT INFOR, V9, P41; Kadkhodaie M, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P497, DOI 10.1145/2783258.2783400; Lewis AS, 2003, SIAM J OPTIMIZ, V13, P702, DOI 10.1137/S1052623401387623; Liang J., 2016, THESIS NORMANDIE U; Mesina M., 1977, Computer Methods in Applied Mechanics and Engineering, V10, P165, DOI 10.1016/0045-7825(77)90004-4; Nesterov Yu. E., 1983, Doklady Akademii Nauk SSSR, V269, P543; Pejcic I, 2016, 2016 EUROPEAN CONTROL CONFERENCE (ECC), P1952, DOI 10.1109/ECC.2016.7810577; Richardson LF, 1927, PHILOS T R SOC LOND, V226, P299, DOI 10.1098/rsta.1927.0008; Scieur D., 2016, ADV NEURAL INFORM PR, P712; Shanks Daniel, 1955, J MATH PHYS, V34, P1, DOI DOI 10.1080/00207167308803075; Sidi A., 2003, CAMBRIDGE MONOGRAPHS, V10; Sidi A., 2017, VECTOR EXTRAPOLATION, V17; Wynn P., 1962, MATH COMPUT, V16, P301, DOI DOI 10.1090/S0025-5718-1962-0145647-X	29	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307038
C	Prabhakar, P; Afzal, ZR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Prabhakar, Pavithra; Afzal, Zahra Rahimi			Abstraction based Output Range Analysis for Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we consider the problem of output range analysis for feed-forward neural networks with ReLU activation functions. The existing approaches reduce the output range analysis problem to satisfiability and optimization solving, which are NP-hard problems, and whose computational complexity increases with the number of neurons in the network. To tackle the computational complexity, we present a novel abstraction technique that constructs a simpler neural network with fewer neurons, albeit with interval weights called interval neural network (INN), which over-approximates the output range of the given neural network. We reduce the output range analysis on the INNs to solving a mixed integer linear programming problem. Our experimental results highlight the trade-off between the computation time and the precision of the computed output range.	[Prabhakar, Pavithra; Afzal, Zahra Rahimi] Kansas State Univ, Dept Comp Sci, Manhattan, KS 66506 USA	Kansas State University	Prabhakar, P (corresponding author), Kansas State Univ, Dept Comp Sci, Manhattan, KS 66506 USA.	pprabhakar@ksu.edu; zrahimi@ksu.edu			NSF CAREER Award [1552668]; ONR YIP Award [N000141712577]	NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); ONR YIP Award	Pavithra Prabhakar was partially supported by NSF CAREER Award No. 1552668 and ONR YIP Award No. N000141712577.	Bengtson V, 2000, SOC AGING, P115; Bojarski M., 2016, CORR; Bunel Rudy, 2017, ABS171100455 CORR; Cheng C., 2017, ABS170501040 CORR; Dutta S, 2019, PROCEEDINGS OF THE 2019 22ND ACM INTERNATIONAL CONFERENCE ON HYBRID SYSTEMS: COMPUTATION AND CONTROL (HSCC '19), P157, DOI 10.1145/3302504.3311807; Dutta S, 2018, LECT N BIOINFORMAT, V11095, P183, DOI 10.1007/978-3-319-99429-1_11; Dutta S, 2018, IFAC PAPERSONLINE, V51, P151, DOI 10.1016/j.ifacol.2018.08.026; Dvijotham K., 2018, ABS180306567 CORR; Ehlers R, 2017, LECT NOTES COMPUT SC, V10482, P269, DOI 10.1007/978-3-319-68167-2_19; Gehr T, 2018, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2018.00058; Graf S, 1997, LECT NOTES COMPUT SC, V1254, P72; Huang C, 2019, ACM T EMBED COMPUT S, V18, DOI 10.1145/3358228; Huang XW, 2017, LECT NOTES COMPUT SC, V10426, P3, DOI 10.1007/978-3-319-63387-9_1; Julian K. D., 2016, DAS 16; Julian K. D., 2018, ARXIV E PRINTS; Kahn G., 2016, CORR; Katz G., 2017, CORR; Maganti Lalit, 2017, ABS170607351 CORR; Narodytska N., 2017, VERIFYING PROPERTIES; Pulina L, 2010, LECT NOTES COMPUT SC, V6174, P243, DOI 10.1007/978-3-642-14295-6_24; Ruan W., 2018, ABS180502242 CORR; Singh G., 2019, PACMPL, V3; Sun X., 2018, ABS181013072 CORR; Tran D., 2019, SAFETY VERIFICATION; Wang S., 2018, ABS180410829 CORR; Xiang W., 2018, ABS181001989 CORR; Xiang W., 2017, ABS171208163 CORR; Xiang W., 2017, ABS170803322 CORR; Xiang W., 2018, ABS181206161 CORR; Xiang W., 2018, ABS180509944 CORR	31	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907044
C	Qi, Y; Liu, B; Wang, YM; Pan, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Qi, Yu; Liu, Bin; Wang, Yueming; Pan, Gang			Dynamic Ensemble Modeling Approach to Nonstationary Neural Decoding in Brain-Computer Interfaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				TRACKING	Brain-computer interfaces (BCIs) have enabled prosthetic device control by decoding motor movements from neural activities. Neural signals recorded from cortex exhibit nonstationary property due to abrupt noises and neuroplastic changes in brain activities during motor control. Current state-of-the-art neural signal decoders such as Kalman filter assume fixed relationship between neural activities and motor movements, thus will fail if this assumption is not satisfied. We propose a dynamic ensemble modeling (DyEnsemble) approach that is capable of adapting to changes in neural signals by employing a proper combination of decoding functions. The DyEnsemble method firstly learns a set of diverse candidate models. Then, it dynamically selects and combines these models online according to Bayesian updating mechanism. Our method can mitigate the effect of noises and cope with different task behaviors by automatic model switching, thus gives more accurate predictions. Experiments with neural data demonstrate that the DyEnsemble method outperforms Kalman filters remarkably, and its advantage is more obvious with noisy signals.	[Qi, Yu; Pan, Gang] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China; [Liu, Bin] Nanjing Univ Posts & Telecommun, Sch Comp Sci, Nanjing, Peoples R China; [Wang, Yueming] Zhejiang Univ, Qiushi Acad Adv Studies, Hangzhou, Peoples R China; [Pan, Gang] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Peoples R China	Zhejiang University; Nanjing University of Posts & Telecommunications; Zhejiang University; Zhejiang University	Pan, G (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.; Wang, YM (corresponding author), Zhejiang Univ, Qiushi Acad Adv Studies, Hangzhou, Peoples R China.; Pan, G (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Peoples R China.	qiyu@zju.edu.cn; bins@ieee.org; ymingwang@zju.edu.cn; gpan@zju.edu.cn			National Key Research and Development Program of China [2018YFA0701400, 2017YFB1002503]; National Natural Science Foundation of China [61906166, 61571238, 61906099]; Zhejiang Provincial Natural Science Foundation of China [LZ17F030001]; Zhejiang Lab [2018EB0ZX01]	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Zhejiang Provincial Natural Science Foundation of China(Natural Science Foundation of Zhejiang Province); Zhejiang Lab	This work was partly supported by grants from the National Key Research and Development Program of China (2018YFA0701400, 2017YFB1002503), National Natural Science Foundation of China (61906166, 61571238, 61906099), Zhejiang Provincial Natural Science Foundation of China (LZ17F030001), and the Zhejiang Lab (2018EB0ZX01).	Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374; Brandman DM, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aa9ee7; Chaudhary U, 2016, NAT REV NEUROL, V12, P513, DOI 10.1038/nrneurol.2016.113; Collinger JL, 2013, LANCET, V381, P557, DOI 10.1016/S0140-6736(12)61816-9; Dai Y, 2016, OPT ENG, V55, DOI 10.1117/1.OE.55.8.083102; Eden UT, 2004, NEURAL COMPUT, V16, P971, DOI 10.1162/089976604773135069; Gilja V, 2015, NAT MED, V21, P1142, DOI 10.1038/nm.3953; Hochberg LR, 2006, NATURE, V442, P164, DOI 10.1038/nature04970; Hochberg LR, 2012, NATURE, V485, P372, DOI 10.1038/nature11076; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hoeting JA, 1999, STAT SCI, V14, P382, DOI 10.1214/ss/1009212519; Kim S-P, 2006, 1 IEEE RAS EMBS INT, P811; Kipke DR, 2003, IEEE T NEUR SYS REH, V11, P151, DOI 10.1109/TNSRE.2003.814443; Liu B, 2017, INT CONF ACOUST SPEE, P4034, DOI 10.1109/ICASSP.2017.7952914; Liu B, 2011, IEEE T WIREL COMMUN, V10, P1810, DOI 10.1109/TWC.2011.042211.100639; Qi Y, 2019, IEEE T NEUR SYS REH, V27, P1942, DOI 10.1109/TNSRE.2019.2939010; Qian CL, 2018, NEURAL COMPUT, V30, P3189, DOI 10.1162/neco_a_01137; Raftery AE, 1997, J AM STAT ASSOC, V92, P179, DOI 10.2307/2291462; Sanes JN, 2000, ANNU REV NEUROSCI, V23, P393, DOI 10.1146/annurev.neuro.23.1.393; Schwemmer MA, 2018, NAT MED, V24, P1669, DOI 10.1038/s41591-018-0171-y; Shanechi MM, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004730; Shenoy KV, 2014, NEURON, V84, P665, DOI 10.1016/j.neuron.2014.08.038; Taylor DM, 2002, SCIENCE, V296, P1829, DOI 10.1126/science.1070291; Todorova S, 2017, NEURAL COMPUT, V29, P3290, DOI [10.1162/NECO_a_01020, 10.1162/neco_a_01020]; van der Merwe R, 2001, ADV NEUR IN, V13, P584; Vaskov AK, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00751; Wang Y, 2015, INT J CLIN EXP PATHO, V8, P11772; Wang YW, 2008, IEEE ENG MED BIO, P1720, DOI 10.1109/IEMBS.2008.4649508; Wang YM, 2018, IEEE T BIO-MED ENG, V65, P1953, DOI 10.1109/TBME.2018.2842769; Wolpaw JR, 2007, J PHYSIOL-LONDON, V579, P613, DOI 10.1113/jphysiol.2006.125948; Yu BM, 2007, J NEUROPHYSIOL, V97, P3763, DOI 10.1152/jn.00482.2006; Zhang SM, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-36885-0	32	2	2	1	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306013
C	Ray, K; Szabo, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ray, Kolyan; Szabo, Botond			Debiased Bayesian inference for average treatment effects	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				VON MISES THEOREM; CAUSAL INFERENCE; REGRESSION; MODELS; FUNCTIONALS	Bayesian approaches have become increasingly popular in causal inference problems due to their conceptual simplicity, excellent performance and in-built uncertainty quantification ('posterior credible sets'). We investigate Bayesian inference for average treatment effects from observational data, which is a challenging problem due to the missing counterfactuals and selection bias. Working in the standard potential outcomes framework, we propose a data-driven modification to an arbitrary (nonparametric) prior based on the propensity score that corrects for the first-order posterior bias, thereby improving performance. We illustrate our method for Gaussian process (GP) priors using (semi-)synthetic data. Our experiments demonstrate significant improvement in both estimation accuracy and uncertainty quantification compared to the unmodified GP, rendering our approach highly competitive with the state-of-the-art.	[Ray, Kolyan] Kings Coll London, Dept Math, London, England; [Szabo, Botond] Leiden Univ, Math Inst, Leiden, Netherlands	University of London; King's College London; Leiden University; Leiden University - Excl LUMC	Ray, K (corresponding author), Kings Coll London, Dept Math, London, England.	kolyan.ray@kcl.ac.uk; b.t.szabo@math.leidenuniv.nl	Szabo, Botond/AAC-7236-2021	Szabo, Botond/0000-0002-5526-8747	Netherlands Organization for Scientific Research (NWO) [639.031.654]	Netherlands Organization for Scientific Research (NWO)(Netherlands Organization for Scientific Research (NWO))	Botond Szabo received funding from the Netherlands Organization for Scientific Research (NWO) under Project number: 639.031.654. We thank 3 reviewers for their useful comments that helped improve the presentation of this work.	ALAA A. M., 2017, ADV NEURAL INFORM PR, V30, P2329; Alaa AM, 2018, PR MACH LEARN RES, V80; Alaa Ahmed M., 2017, ADV NEURAL INFORM PR, V30, P3424; [Anonymous], 2017, ADV NEURAL INFORM PR, DOI DOI 10.13275/J.CNKI.LYKXYJ.2017.06.007; ANTONELLI J., 2018, ARXIV E PRINTS; Athey S, 2017, AM ECON REV, V107, P278, DOI 10.1257/aer.p20171042; Brodersen KH, 2015, ANN APPL STAT, V9, P247, DOI 10.1214/14-AOAS788; Castillo I, 2015, ANN STAT, V43, P2353, DOI 10.1214/15-AOS1336; Castillo I, 2012, SANKHYA SER A, V74, P194, DOI 10.1007/s13171-012-0008-6; Chan KCG, 2016, J R STAT SOC B, V78, P673, DOI 10.1111/rssb.12129; Chipman HA, 2010, ANN APPL STAT, V4, P266, DOI 10.1214/09-AOAS285; Futoma J, 2017, PR MACH LEARN RES, V70; Green DP, 2012, PUBLIC OPIN QUART, V76, P491, DOI 10.1093/poq/nfs036; HAHN P. R., 2017, ARXIV E PRINTS; Hahn PR, 2018, BAYESIAN ANAL, V13, P163, DOI 10.1214/16-BA1044; Heckman JJ, 1998, REV ECON STUD, V65, P261, DOI 10.1111/1467-937X.00044; Hill JL, 2011, J COMPUT GRAPH STAT, V20, P217, DOI 10.1198/jcgs.2010.08162; Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751; Kern HL, 2016, J RES EDUC EFF, V9, P103, DOI 10.1080/19345747.2015.1060282; Li S., 2016, IJCAI, P3768; MUELLER JA, 2017, APPL POWER ELECT CO, P1039; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; RAY K., ANN STAT; Ritov Y, 2014, STAT SCI, V29, P619, DOI 10.1214/14-STS483; Rivoirard V, 2012, ANN STAT, V40, P1489, DOI 10.1214/12-AOS1004; Robins J. M., 2008, PROBABILITY STAT ESS, V2, P335, DOI [10.1214/193940307000000527, DOI 10.1214/193940307000000527]; Robins JM, 2017, ANN STAT, V45, P1951, DOI 10.1214/16-AOS1515; Robins JM, 1997, STAT MED, V16, P285, DOI 10.1002/(SICI)1097-0258(19970215)16:3<285::AID-SIM535>3.0.CO;2-#; ROSENBAUM PR, 1983, BIOMETRIKA, V70, P41, DOI 10.1093/biomet/70.1.41; ROTNITZKY A, 1995, SCAND J STAT, V22, P323; RUBIN DB, 1978, ANN STAT, V6, P34, DOI 10.1214/aos/1176344064; Sivaganesan S, 2017, STAT MED, V36, P2391, DOI 10.1002/sim.7276; Stuart EA, 2010, STAT SCI, V25, P1, DOI 10.1214/09-STS313; SUN W., 2015, 29 AAAI C ART INT; URTEAGA I., 2017, NIPS 2017 WORKSH MAC; Wager S, 2018, J AM STAT ASSOC, V113, P1228, DOI 10.1080/01621459.2017.1319839	39	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903055
C	Rubenstein, PK; Bousquet, O; Djolonga, J; Riquelme, C; Tolstikhin, I		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Rubenstein, Paul K.; Bousquet, Olivier; Djolonga, Josip; Riquelme, Carlos; Tolstikhin, Ilya			Practical and Consistent Estimation of f-Divergences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The estimation of an f-divergence between two probability distributions based on samples is a fundamental problem in statistics and machine learning. Most works study this problem under very weak assumptions, in which case it is provably hard. We consider the case of stronger structural assumptions that are commonly satisfied in modern machine learning, including representation learning and generative modelling with autoencoder architectures. Under these assumptions we propose and study an estimator that can be easily implemented, works well in high dimensions, and enjoys faster rates of convergence. We verify the behavior of our estimator empirically in both synthetic and real-data experiments, and discuss its direct implications for total correlation, entropy, and mutual information estimation.	[Rubenstein, Paul K.] Univ Cambridge, Max Planck Inst Intelligent Syst, Tubingen & Machine Learning Grp, Cambridge, England; [Bousquet, Olivier; Djolonga, Josip; Riquelme, Carlos; Tolstikhin, Ilya] Google Res, Brain Team, Zurich, Switzerland	Max Planck Society; University of Cambridge; Google Incorporated	Rubenstein, PK (corresponding author), Univ Cambridge, Max Planck Inst Intelligent Syst, Tubingen & Machine Learning Grp, Cambridge, England.	paul.rubenstein@tuebingen.mpg.de; obousquet@google.com; josipd@google.com; rikel@google.com; tolstikhin@google.com						Alemi AA, 2018, PR MACH LEARN RES, V80; BAI J, 2016, ADV NEURAL INFORM PR, P1073; Ben Poole Sherjil Ozair, 2018, ICML; Ben-David S., 2007, ADV NEURAL INFORM PR, V19, P137; Burda Yuri, 2015, ARXIV150900519; Chen LJ, 2018, PR MACH LEARN RES, V80; Chen T.Q., 2018, NEURIPS, P2610; Dieng A. B., 2018, ARXIV180704863; Dieng A. B., 2017, ADV NEURAL INFORM PR, P2732; DURRIEU JL, 2012, INT CONF ACOUST SPEE, P4833; Ganin Y., 2016, JMLR, V17, P2096; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A, 2012, J MACH LEARN RES, V13, P723; Hein M., 2005, AISTATS; Hero A. O., 2002, IEEE SIGNAL PROCESSI; Hershey JR, 2007, INT CONF ACOUST SPEE, P317, DOI 10.1109/icassp.2007.366913; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Hoffman Matthew D, 2016, ELBO SURG YET ANOTHE; Kanamori T, 2012, IEEE T INFORM THEORY, V58, P708, DOI 10.1109/TIT.2011.2163380; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krishnamurthy Akshay, 2014, ICML; Liese F, 2006, IEEE T INFORM THEORY, V52, P4394, DOI 10.1109/TIT.2006.881731; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Moon K., 2014, NEURIPS; Moon KR, 2014, IEEE INT SYMP INFO, P356, DOI 10.1109/ISIT.2014.6874854; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Nielsen F, 2014, IEEE SIGNAL PROC LET, V21, P10, DOI 10.1109/LSP.2013.2288355; Nowozin S, 2016, ADV NEUR IN, V29; Osterreicher F, 2003, ANN I STAT MATH, V55, P639, DOI 10.1007/BF02517812; Pardo L, 2005, STAT INFERENCE BASED; Perez-Cruz F, 2008, IEEE INT SYMP INFO, P1666, DOI 10.1109/ISIT.2008.4595271; Poczos Barnabas, 2011, AISTATS; Singh S., 2014, ICML; Tsybakov A. B., 2009, INTRO NONPARAMETRIC, DOI 10.1007/b13794; van den Oord Aaron, 2018, ARXIV180703748; Wang Q, 2009, IEEE T INFORM THEORY, V55, P2392, DOI 10.1109/TIT.2009.2016060; [No title captured]	39	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304011
C	Ryffel, T; Dufour-Sans, E; Gay, R; Bach, F; Pointcheval, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ryffel, Theo; Dufour-Sans, Edouard; Gay, Romain; Bach, Francis; Pointcheval, David			Partially Encrypted Machine Learning using Functional Encryption	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Machine learning on encrypted data has received a lot of attention thanks to recent breakthroughs in homomorphic encryption and secure multi-party computation. It allows outsourcing computation to untrusted servers without sacrificing privacy of sensitive data. We propose a practical framework to perform partially encrypted and privacy-preserving predictions which combines adversarial training and functional encryption. We first present a new functional encryption scheme to efficiently compute quadratic functions so that the data owner controls what can be computed but is not involved in the calculation: it provides a decryption key which allows one to learn a specific function evaluation of some encrypted data. We then show how to use it in machine learning to partially encrypt neural networks with quadratic activation functions at evaluation time, and we provide a thorough analysis of the information leaks based on indistinguishability of data items of the same label. Last, since most encryption schemes cannot deal with the last thresholding operation used for classification, we propose a training method to prevent selected sensitive features from leaking, which adversarially optimizes the network against an adversary trying to identify these features. This is interesting for several existing works using partially encrypted machine learning as it comes with little reduction on the model's accuracy and significantly improves data privacy.	[Ryffel, Theo; Dufour-Sans, Edouard; Gay, Romain; Bach, Francis; Pointcheval, David] PSL Univ, CNRS, ENS, Dept Informat ENS, Paris, France; [Ryffel, Theo; Bach, Francis; Pointcheval, David] INRIA, Paris, France; [Gay, Romain] Univ Calif Berkeley, Berkeley, CA 94720 USA	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite; Inria; University of California System; University of California Berkeley	Ryffel, T (corresponding author), PSL Univ, CNRS, ENS, Dept Informat ENS, Paris, France.; Ryffel, T (corresponding author), INRIA, Paris, France.	theo.ryffel@ens.fr; edufoursans@ens.fr; romain.gay@ens.fr; francis.bach@ens.fr; david.pointcheval@ens.fr			European Community [339563 -CryptoCloud, 780108]; Google PhD fellowship; French FUI ANBLIC Project	European Community(European Commission); Google PhD fellowship(Google Incorporated); French FUI ANBLIC Project	This work was supported in part by the European Community's Seventh Framework Programme (FP7/2007-2013 Grant Agreement no. 339563 -CryptoCloud), the European Community's Horizon 2020 Project FENTEC (Grant Agreement no. 780108), the Google PhD fellowship, and the French FUI ANBLIC Project.	Abdalla M, 2015, LECT NOTES COMPUT SC, V9020, P733, DOI 10.1007/978-3-662-46447-2_33; Agrawal S, 2016, LECT NOTES COMPUT SC, V9816, P333, DOI 10.1007/978-3-662-53015-3_12; Akinyele JA, 2013, J CRYPTOGR ENG, V3, P111, DOI 10.1007/s13389-013-0057-3; Al Badawi Ahmad, 2018, 20181056 CRYPT EPRIN; Ambrona M, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P647, DOI 10.1145/3133956.3134088; Baltico CEZ, 2017, LECT NOTES COMPUT SC, V10401, P67, DOI 10.1007/978-3-319-63688-7_3; Barak B., 2001, LNCS, V2139, P1, DOI [10.1007/3-540-44647-8_1, DOI 10.1007/3-540-44647-8_1]; Barni M, 2011, IEEE T INF FOREN SEC, V6, P452, DOI 10.1109/TIFS.2011.2108650; Barocas Solon, 2018, FAIRNESS MACHINE LEA; Barthe G, 2014, LECT NOTES COMPUT SC, V8616, P95, DOI 10.1007/978-3-662-44371-2_6; Boneh D, 2003, SIAM J COMPUT, V32, P586, DOI 10.1137/S0097539701398521; Boneh D, 2011, LECT NOTES COMPUT SC, V6597, P253, DOI 10.1007/978-3-642-19571-6_16; Bourse F., 2017, 20171114 CRYPT EPRIN; Bourse F, 2018, LECT NOTES COMPUT SC, V10993, P483, DOI 10.1007/978-3-319-96878-0_17; Brakerski Z, 2011, ANN IEEE SYMP FOUND, P97, DOI 10.1109/FOCS.2011.12; Carpov Sergiu, 2018, IACR CRYPTOLOGY EPRI, V2018, P1001; Chillotti I, 2016, LECT NOTES COMPUT SC, V10031, P3, DOI 10.1007/978-3-662-53887-6_1; Cranmer K, 2017, ADV NEURAL INFORM PR, V30, P981; Damgard I, 2012, LECT NOTES COMPUT SC, V7417, P643; Dowlin N, 2016, PR MACH LEARN RES, V48; Garg S, 2013, ANN IEEE SYMP FOUND, P40, DOI 10.1109/FOCS.2013.13; Gentry C, 2013, LECT NOTES COMPUT SC, V8042, P75, DOI 10.1007/978-3-642-40041-4_5; GOLDWASSER S, 1984, J COMPUT SYST SCI, V28, P270, DOI 10.1016/0022-0000(84)90070-9; Joux A, 2004, J CRYPTOL, V17, P263, DOI 10.1007/s00145-004-0312-y; Kim S, 2018, LECT NOTES COMPUT SC, V11035, P544, DOI 10.1007/978-3-319-98113-0_29; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y., 2010, MNIST HANDWRITTEN DI; Livni Roi, 2014, ABS14101141 CORR; Maurer U, 2005, LECT NOTES COMPUT SC, V3796, P1; NECHAEV VI, 1994, MATH NOTES+, V55, P165, DOI 10.1007/BF02113297; ONeill A, 2010, 2010556 CRYPT EPRINT; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Riazi MS, 2018, PROCEEDINGS OF THE 2018 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIACCS'18), P707, DOI 10.1145/3196494.3196522; Ryffel T., 2018, ABS181104017 CORR; Shoup V., 1997, LECT NOTES COMPUTER, P256, DOI DOI 10.1007/3-540-69053-0; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Wagh Sameer, 2019, SECURENN EFFICIENT P	38	2	2	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304051
C	Sabbagh, D; Ablin, P; Varoquaux, G; Gramfort, A; Engemann, DA		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sabbagh, David; Ablin, Pierre; Varoquaux, Gael; Gramfort, Alexandre; Engemann, Denis A.			Manifold-regression to predict from MEG/EEG brain signals without source modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COMPUTER INTERFACES; EEG; MEG; AGE; MAGNETOENCEPHALOGRAPHY; OPTIMIZATION; GEOMETRY; SPACE	Magnetoencephalography and electroencephalography (M/EEG) can reveal neuronal dynamics non-invasively in real-time and are therefore appreciated methods in medicine and neuroscience. Recent advances in modeling brain-behavior relationships have highlighted the effectiveness of Riemannian geometry for summarizing the spatially correlated time-series from M/EEG in terms of their covariance. However, after artefact-suppression, M/EEG data is often rank deficient which limits the application of Riemannian concepts. In this article, we focus on the task of regression with rank-reduced covariance matrices. We study two Riemannian approaches that vectorize the M/EEG covariance between-sensors through projection into a tangent space. The Wasserstein distance readily applies to rank-reduced data but lacks affine-invariance. This can be overcome by finding a common subspace in which the covariance matrices are full rank, enabling the affine-invariant geometric distance. We investigated the implications of these two approaches in synthetic generative models, which allowed us to control estimation bias of a linear model for prediction. We show that Wasserstein and geometric distances allow perfect out-of-sample prediction on the generative models. We then evaluated the methods on real data with regard to their effectiveness in predicting age from M/EEG covariance matrices. The findings suggest that the data-driven Riemannian methods outperform different sensor-space estimators and that they get close to the performance of biophysics-driven source-localization model that requires MRI acquisitions and tedious data processing. Our study suggests that the proposed Riemannian methods can serve as fundamental building-blocks for automated large-scale analysis of M/EEG.	[Sabbagh, David; Ablin, Pierre; Varoquaux, Gael; Gramfort, Alexandre; Engemann, Denis A.] Univ Paris Saclay, INRIA, CEA, F-91120 Palaiseau, France; [Sabbagh, David] Paris Diderot Univ, INSERM, UMRS 942, UMRS-942 Paris, France; [Sabbagh, David] Lariboisiere Hosp, AP HP, Dept Anaesthesiol & Crit Care, Paris, France	CEA; Inria; UDICE-French Research Universities; Universite Paris Saclay; Institut National de la Sante et de la Recherche Medicale (Inserm); Assistance Publique Hopitaux Paris (APHP); Hopital Universitaire Lariboisiere-Fernand-Widal - APHP; UDICE-French Research Universities; Universite Paris Cite	Sabbagh, D (corresponding author), Univ Paris Saclay, INRIA, CEA, F-91120 Palaiseau, France.; Sabbagh, D (corresponding author), Paris Diderot Univ, INSERM, UMRS 942, UMRS-942 Paris, France.; Sabbagh, D (corresponding author), Lariboisiere Hosp, AP HP, Dept Anaesthesiol & Crit Care, Paris, France.	dav.sabbagh@gmail.com; denis-alexander.engemann@inria.fr	Engemann, Denis-Alexander/AAC-2846-2021	Engemann, Denis-Alexander/0000-0002-7223-1014	2018 "medecine numerique"; Inria (French national research institute for the digital sciences); European Research Council Starting Grant [SLAB ERC-YStG-676943]	2018 "medecine numerique"; Inria (French national research institute for the digital sciences); European Research Council Starting Grant(European Research Council (ERC))	This work was supported by a 2018 "medecine numerique" (for digital Medicine) thesis grant issued by Inserm (French national institute of health and medical research) and Inria (French national research institute for the digital sciences). It was also partly supported by the European Research Council Starting Grant SLAB ERC-YStG-676943.	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Babayan A, 2019, SCI DATA, V6, DOI 10.1038/sdata.2018.308; Baillet S, 2017, NAT NEUROSCI, V20, P327, DOI 10.1038/nn.4504; Berthouze L, 2010, CLIN NEUROPHYSIOL, V121, P1187, DOI 10.1016/j.clinph.2010.02.163; Bhatia R, 2007, PRINC SER APPL MATH, P1; Bhatia R., 2018, EXPO MATH; Blankertz B, 2008, IEEE SIGNAL PROC MAG, V25, P41, DOI 10.1109/MSP.2008.4408441; Bonnabel S, 2009, SIAM J MATRIX ANAL A, V31, P1055, DOI 10.1137/080731347; Boumal N, 2014, J MACH LEARN RES, V15, P1455; Buzsaki G, 2014, NAT REV NEUROSCI, V15, P264, DOI 10.1038/nrn3687; Buzsaki G, 2017, SCIENCE, V358, P482, DOI 10.1126/science.aan8869; Clark CR, 2004, INT J PSYCHOPHYSIOL, V53, P1, DOI 10.1016/j.ijpsycho.2003.12.011; Congedo M., 2013, NEW GENERATION BRAIN; Congedo M, 2017, BRAIN-COMPUT INTERFA, V4, P155, DOI 10.1080/2326263X.2017.1297192; Dahne S, 2014, NEUROIMAGE, V86, P111, DOI 10.1016/j.neuroimage.2013.07.079; Dmochowski JP, 2012, FRONT HUM NEUROSCI, V6, DOI 10.3389/fnhum.2012.00112; Engemann DA, 2015, NEUROIMAGE, V108, P328, DOI 10.1016/j.neuroimage.2014.12.040; Forstner W, 2003, GEODESY THE CHALLENG, P299, DOI DOI 10.1007/978-3-662-05296-9_31; Garces P, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17122926; GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751; Gramfort A, 2014, NEUROIMAGE, V86, P446, DOI 10.1016/j.neuroimage.2013.10.027; Grosse-Wentrup M, 2008, IEEE T BIO-MED ENG, V55, P1991, DOI 10.1109/TBME.2008.921154; HAMALAINEN M, 1993, REV MOD PHYS, V65, P413, DOI 10.1103/RevModPhys.65.413; Hamalainen M. S., 1984, TKKFA559 HELS U TECH; Harandi M, 2018, IEEE T PATTERN ANAL, V40, P48, DOI 10.1109/TPAMI.2017.2655048; Hari R, 2017, MEG EEG PRIMER; Haufe S, 2014, NEUROIMAGE, V87, P96, DOI 10.1016/j.neuroimage.2013.10.067; Horev Inbal, 2016, MACH LEARN, V106, P11; Jas M, 2017, NEUROIMAGE, V159, P417, DOI 10.1016/j.neuroimage.2017.06.030; Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359; Khan S, 2018, NEUROIMAGE, V174, P57, DOI 10.1016/j.neuroimage.2018.02.018; Larson-Prior LJ, 2013, NEUROIMAGE, V80, P190, DOI 10.1016/j.neuroimage.2013.05.056; Liem F, 2017, NEUROIMAGE, V148, P179, DOI 10.1016/j.neuroimage.2016.11.005; Makeig S., 1995, ADV NEURAL INFORM PR, V8, P145; Massart Estelle, 2018, TECHNICAL REPORT; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Rodrigues PLC, 2018, 2018 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP), P573; Shafto MA, 2014, BMC NEUROL, V14, DOI 10.1186/s12883-014-0204-1; Smith SM, 2018, NEURON, V97, P263, DOI 10.1016/j.neuron.2017.12.018; Taulu S, 2005, J APPL PHYS, V97, DOI 10.1063/1.1935742; Taylor JR, 2017, NEUROIMAGE, V144, P262, DOI 10.1016/j.neuroimage.2015.09.018; Uusitalo MA, 1997, MED BIOL ENG COMPUT, V35, P135, DOI 10.1007/BF02534144; Vandereycken B, 2009, 2009 IEEE/SP 15TH WORKSHOP ON STATISTICAL SIGNAL PROCESSING, VOLS 1 AND 2, P389, DOI 10.1109/SSP.2009.5278558; Voytek B, 2015, J NEUROSCI, V35, P13257, DOI 10.1523/JNEUROSCI.2332-14.2015; Yger F, 2017, IEEE T NEUR SYS REH, V25, P1753, DOI 10.1109/TNSRE.2016.2627016	45	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307035
C	Sala, F; Varma, P; Fries, J; Fu, DY; Sagawa, S; Khattar, S; Ramamoorthy, A; Xiao, K; Fatahalian, K; Priest, J; Re, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sala, Frederic; Varma, Paroma; Fries, Jason; Fu, Daniel Y.; Sagawa, Shiori; Khattar, Saelig; Ramamoorthy, Ashwini; Xiao, Ke; Fatahalian, Kayvon; Priest, James; Re, Christopher			Multi-Resolution Weak Supervision for Sequential Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Since manually labeling training data is slow and expensive, recent industrial and scientific research efforts have turned to weaker or noisier forms of supervision sources. However, existing weak supervision approaches fail to model multi-resolution sources for sequential data, like video, that can assign labels to individual elements or collections of elements in a sequence. A key challenge in weak supervision is estimating the unknown accuracies and correlations of these sources without using labeled data. Multi-resolution sources exacerbate this challenge due to complex correlations and sample complexity that scales in the length of the sequence. We propose Dugong, the first framework to model multi-resolution weak supervision sources with complex correlations to assign probabilistic labels to training data. Theoretically, we prove that Dugong, under mild conditions, can uniquely recover the unobserved accuracy and correlation parameters and use parameter sharing to improve sample complexity. Our method assigns clinician-validated labels to population-scale biomedical video repositories, helping outperform traditional supervision by 36.8 F1 points and addressing a key use case where machine learning has been severely limited by the lack of expert labeled data. On average, Dugong improves over traditional supervision by 16.0 F1 points and existing weak supervision approaches by 24.2 F1 points across several video and sensor classification tasks.				fredsala@stanford.edu; paroma@stanford.edu; jfries@stanford.edu; danfu@stanford.edu; sagawas@stanford.edu; saelig@stanford.edu; ashwinir@stanford.edu; kexiao@cs.umass.edu; kayvonf@stanford.edu; jpriest@stanford.edu; chrismre@stanford.edu		Fries, Jason/0000-0001-9316-5768	DARPA [FA87501720095, FA86501827865, FA86501827882]; NIH [U54EB020405]; NSF [CCF1763315, CCF1563078, 1937301]; ONR [N000141712266]; Moore Foundation; NXP; Xilinx; LETI-CEA; Intel; IBM; Microsoft; NEC; Toshiba; TSMC; ARM; Hitachi; BASF; Accenture; Ericsson; Qualcomm; Analog Devices; Okawa Foundation; American Family Insurance; Google Cloud; Swiss Re; Brown Institute for Media Innovation; National Science Foundation (NSF) Graduate Research Fellowship [DGE-114747]; JosephW. and Hon Mai Goodman Stanford Graduate Fellowship; Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG); Teradata; Facebook; Google; Ant Financial; VMWare; Infosys	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); Moore Foundation(Gordon and Betty Moore Foundation); NXP; Xilinx; LETI-CEA; Intel(Intel Corporation); IBM(International Business Machines (IBM)); Microsoft(Microsoft); NEC; Toshiba; TSMC; ARM; Hitachi; BASF(BASF); Accenture; Ericsson(Ericsson); Qualcomm; Analog Devices; Okawa Foundation; American Family Insurance; Google Cloud(Google Incorporated); Swiss Re; Brown Institute for Media Innovation; National Science Foundation (NSF) Graduate Research Fellowship(National Science Foundation (NSF)); JosephW. and Hon Mai Goodman Stanford Graduate Fellowship; Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG); Teradata; Facebook(Facebook Inc); Google(Google Incorporated); Ant Financial; VMWare; Infosys	We gratefully acknowledge the support of DARPA under Nos. FA87501720095 (D3M), FA86501827865 (SDH), and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (UnifyingWeak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, Brown Institute for Media Innovation, the National Science Foundation (NSF) Graduate Research Fellowship under No. DGE-114747, JosephW. and Hon Mai Goodman Stanford Graduate Fellowship, Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.	Bach Stephen H, 2018, ARXIV181200417; Bach Stephen H, 2017, ICML; Bae, 2017, ARXIV170503281; Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698; Dehghani M., 2017, ARXIV171111383; Fries JA, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-11012-3; Fu D. Y., 2019, ARXIV191002993; Guan M. Y., 2018, 32 AAAI C ART INT; Hearst MA, 1992, P 14 C COMP LING, V2, P539, DOI DOI 10.3115/992133.992154; Jia ZP, 2017, IEEE T MED IMAGING, V36, P2376, DOI 10.1109/TMI.2017.2724070; Khattar Saelig, 2019, MULTIFRAME WEAK SUPE; Khetan A., 2017, P 6 INT C LEARN REPR; Koller D., 2009, PROBABILISTIC GRAPHI; Liang Chen, 2016, ARXIV161100020; Mahajan Dhruv, 2018, P EUR C COMP VIS ECC, P181; Masalov A, 2018, IEEE INT VEH SYM, P2143; Mintz M., 2009, P ACL, P1003, DOI DOI 10.3115/1690219.1690287; Niu F., 2012, VLDS CEUR WORKSHOP P, V884, P25; Ratner A. J., 2016, P 29 C NEUR INF PROC; Ratner A. J., 2019, P AAAI C ART INT HON; Ratner A, 2016, ADV NEUR IN, V29; Sudlow C, 2015, PLOS MED, V12, DOI 10.1371/journal.pmed.1001779; Takamatsu Shingo, 2012, LONG PAPERS, V1, P721; Tang Shitao, 2018, P AS C COMP VIS ACCV; Varma P., 2019, P 36 INT C MACH LEAR; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885; Zhan Eric, 2019, 7 INT C LEARN REPR I	29	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300018
C	Serrino, J; Kleiman-Weiner, M; Parkes, DC; Tenenbaum, JB		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Serrino, Jack; Kleiman-Weiner, Max; Parkes, David C.; Tenenbaum, Joshua B.			Finding Friend and Foe in Multi-Agent Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GO	Recent breakthroughs in AI for multi-agent games like Go, Poker, and Dota, have seen great strides in recent years. Yet none of these games address the real-life challenge of cooperation in the presence of unknown and uncertain teammates. This challenge is a key game mechanism in hidden role games. Here we develop the DeepRole algorithm, a multi-agent reinforcement learning agent that we test on The Resistance: Avalon, the most popular hidden role game. DeepRole combines counterfactual regret minimization (CFR) with deep value networks trained through self-play. Our algorithm integrates deductive reasoning into vector-form CFR to reason about joint beliefs and deduce partially observable actions. We augment deep value networks with constraints that yield interpretable representations of win probabilities. These innovations enable DeepRole to scale to the full Avalon game. Empirical game-theoretic methods show that DeepRole outperforms other hand-crafted and learned agents in five-player Avalon. DeepRole played with and against human players on the web in hybrid human-agent teams. We find that DeepRole outperforms human players as both a cooperator and a competitor.	[Serrino, Jack; Kleiman-Weiner, Max; Tenenbaum, Joshua B.] MIT, Cambridge, MA 02139 USA; [Kleiman-Weiner, Max] Harvard, Cambridge, MA USA; [Kleiman-Weiner, Max] Diffeo, Cambridge, MA USA; [Parkes, David C.] Harvard Univ, Cambridge, MA 02138 USA; [Tenenbaum, Joshua B.] CBMM, Araxa, MG, Brazil	Massachusetts Institute of Technology (MIT); Harvard University	Serrino, J (corresponding author), MIT, Cambridge, MA 02139 USA.	jserrino@mit.edu; maxkleimanweiner@fas.harvard.edu; parkes@eecs.harvard.edu; jbt@mit.edu	Smith, Jack/HHC-2369-2022		Harvard Data Science Initiative; CRCS; MBB; Future of Life Institute; DARPA Ground Truth; Center for Brains, Minds and Machines (NSF STC award) [CCF-1231216]	Harvard Data Science Initiative; CRCS; MBB; Future of Life Institute; DARPA Ground Truth; Center for Brains, Minds and Machines (NSF STC award)	We thank Victor Kuo and ProAvalon.com for help integrating DeepRole with human players online. We also thank Noam Brown, Murray Campbell, and Michael Wellman for helpful discussions and comments. This work was supported by Harvard Data Science Initiative, CRCS, and MBB, The Future of Life Institute, DARPA Ground Truth, and the Center for Brains, Minds and Machines (NSF STC award CCF-1231216).	Albrecht S. V., 2017, ARXIV170908071; [Anonymous], 2017, ARXIV170701068; [Anonymous], [No title captured]; Axelrod R, 1985, EVOLUTION COOPERATIO; Baker CL, 2017, NAT HUM BEHAV, V1, DOI 10.1038/s41562-017-0064; Bowling M, 2015, SCIENCE, V347, P145, DOI 10.1126/science.1259433; Brown N., 2017, SCIENCE; Brown N., 2018, ARXIV181100164; Camerer C.F., 2003, BEHAV GAME THEORY EX, DOI 10.1257/jep.9.2.209; Camerer CF, 2004, Q J ECON, V119, P861, DOI 10.1162/0033553041502225; Canaan R., 2019, ARXIV190307008; Chittaranjan G, 2010, INT CONF ACOUST SPEE, P5334, DOI 10.1109/ICASSP.2010.5494961; Cowling PI, 2015, IEEE CONF COMPU INTE, P114, DOI 10.1109/CIG.2015.7317927; Cowling PI, 2012, IEEE T COMP INTEL AI, V4, P120, DOI 10.1109/TCIAIG.2012.2200894; Eskridge Don, 2012, RESISTANCE AVALON; Farrell J, 1996, J ECON PERSPECT, V10, P103, DOI 10.1257/jep.10.3.103; Galinsky A., 2016, CURRENCY; Henrich J, 2016, SECRET OF OUR SUCCESS: HOW CULTURE IS DRIVING HUMAN EVOLUTION, DOMESTICATING OUR SPECIES, AND MAKING US SMARTER, P1; Jaderberg, 2018, ARXIV180701281; Johanson M., 2013, ARXIV13027008; Johanson M, 2012, P 11 INT C AUT AG MU, V2, P837; Johanson Michael, 2011, 22 INT JOINT C ART I; Kleiman-Weiner M., 2016, P 38 ANN C COGNITIVE; Lanctot M., 2009, ADV NEURAL INFORM PR, P1078; Lanctot M, 2017, ADV NEUR IN, V30; Leibo JZ, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P464; Littman M.L, 1994, P 11 INT C INT C MAC, P157; Moravcik M, 2017, SCIENCE, V356, P508, DOI 10.1126/science.aam6960; Nowak MA, 2006, SCIENCE, V314, P1560, DOI 10.1126/science.1133755; Perolat J., 2017, ADV NEURAL INFORM PR, P3646; Rand DG, 2013, TRENDS COGN SCI, V17, P413, DOI 10.1016/j.tics.2013.06.003; Shum Michael, 2019, P 33 AAAI C ART INT; Silver D., 2010, NIPS, P2164; Silver D, 2018, SCIENCE, V362, P1140, DOI 10.1126/science.aar6404; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Strouse D., 2018, P ADV NEUR INF PROC, P10270; Tammelin Oskari, 2015, 24 INT JOINT C ART I; Tomasello M., 2014, BECOMING HUMAN; Tuyls K, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P77; Ullman T.D., 2009, 22 INT C NEUR INF PR, P1874; Wellman M. P., 2006, AAAI, P1552; Whitehouse D., 2014, THESIS; Wunder M, 2011, P 10 INT C AUT AG MU, P593; Zinkevich M., 2008, ADV NEURAL INFORM PR, P1729	44	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301027
C	Sharifnassab, A; Salehkaleybar, S; Golestani, SJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sharifnassab, Arsalan; Salehkaleybar, Saber; Golestani, S. Jamaloddin			Order Optimal One-Shot Distributed Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider distributed statistical optimization in one-shot setting, where there are m machines each observing n i.i.d. samples. Based on its observed samples, each machine then sends an O (log(mn))-length message to a server, at which a parameter minimizing an expected loss is to be estimated. We propose an algorithm called Multi-Resolution Estimator (MRE) whose expected error is no larger than (O) over tilde (m(-1/max(d,2))n(-1/2)), where d is the dimension of the parameter space. This error bound meets existing lower bounds up to poly-logarithmic factors, and is thereby order optimal. The expected error of MRE, unlike existing algorithms, tends to zero as the number of machines (m) goes to infinity, even when the number of samples per machine (n) remains upper bounded by a constant. This property of the MRE algorithm makes it applicable in new machine learning paradigms where m is much larger than n	[Sharifnassab, Arsalan; Salehkaleybar, Saber; Golestani, S. Jamaloddin] Sharif Univ Technol, Dept Elect Engn, Tehran, Iran	Sharif University of Technology	Sharifnassab, A (corresponding author), Sharif Univ Technol, Dept Elect Engn, Tehran, Iran.	a.sharifnassab@gmail.com; saleh@sharif.edu; golestani@sharif.edu	Salehkaleybar, Saber/AAQ-3268-2021	Salehkaleybar, Saber/0000-0003-3934-9931	Iran National Science Foundation (INSF) [97012846]	Iran National Science Foundation (INSF)(Iran National Science Foundation (INSF))	This research was supported by Iran National Science Foundation (INSF) under contract No. 97012846.	Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Braverman M, 2016, ACM S THEORY COMPUT, P1011, DOI 10.1145/2897518.2897582; Chang X., 2017, J MACH LEARN RES, V18, P1493; DIAKONIKOLAS I., 2017, ADV NEURAL INFORM PR, P6391; Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027; Jordan Michael I, 2018, J AM STAT ASSOC, P1; LEE JD, 2017, J MACH LEARN RES, V18; Lehmann E.L., 2006, THEORY POINT ESTIMAT; McMahan Brendan, 2015, ARXIV151103575; McMahan HB, 2017, PR MACH LEARN RES, V54, P1273; Motwani R., 1995, RANDOMIZED ALGORITHM; Naz A, 2017, P IEEE VIRT REAL ANN, P3, DOI 10.1109/VR.2017.7892225; Salehkaleybar Saber, 2019, ARXIV190504634V1; Yudong Chen, 2017, Proceedings of the ACM on Measurement and Analysis of Computing Systems, V1, DOI 10.1145/3154503; Zhang S., 2015, NEURAL INFORM PROCES, P685; Zhang Y., 2012, ADV NEURAL INFORM PR, P1502; ZHANG YC, 2013, ADV NEURAL INFORM PR, P328, DOI DOI 10.1109/CLOUDCOM-ASIA.2013.57	18	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302019
C	Shin, J; Ramdas, A; Rinaldo, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shin, Jaehyeok; Ramdas, Aaditya; Rinaldo, Alessandro			Are sample means in multi-armed bandits positively or negatively biased?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					It is well known that in stochastic multi-armed bandits (MAB), the sample mean of an arm is typically not an unbiased estimator of its true mean. In this paper, we decouple three different sources of this selection bias: adaptive sampling of arms, adaptive stopping of the experiment, and adaptively choosing which arm to study. Through a new notion called "optimism" that captures certain natural monotonic behaviors of algorithms, we provide a clean and unified analysis of how optimistic rules affect the sign of the bias. The main takeaway message is that optimistic sampling induces a negative bias, but optimistic stopping and optimistic choosing both induce a positive bias. These results are derived in a general stochastic MAB setup that is entirely agnostic to the final aim of the experiment (regret minimization or best-arm identification or anything else). We provide examples of optimistic rules of each type, demonstrate that simulations confirm our theoretical predictions, and pose some natural but hard open problems.	[Shin, Jaehyeok; Ramdas, Aaditya; Rinaldo, Alessandro] Carnegie Mellon Univ, Dept Stat & Data Sci, Pittsburgh, PA 15213 USA; [Ramdas, Aaditya] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	Shin, J (corresponding author), Carnegie Mellon Univ, Dept Stat & Data Sci, Pittsburgh, PA 15213 USA.	shinjaehyeok@cmu.edu; aramdas@cmu.edu; arinaldo@cmu.edu						Agrawal S., 2012, C LEARN THEOR, P39; Audibert J.-Y., 2009, P 22 ANN C LEARN THE, P217; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bowden J, 2017, STAT METHODS MED RES, V26, P2376, DOI 10.1177/0962280215597716; Garivier A., 2011, P 24 ANN C LEARNING, V24, P359; Garivier A., 2016, C LEARN THEOR, P998; Gut A, 2009, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-87835-5; Howard Steven R, 2018, ARXIV181008240; Jamieson K., 2014, C LEARN THEOR, P423; Kalyanakrishnan S, 2012, P 29 INT C MACH LEAR, P655; Kaufmann Emilie, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P199, DOI 10.1007/978-3-642-34106-9_18; Lattimore Tor, 2019, BANDIT ALGORITHMS; Nie X, 2018, PROC 21 INTERNAT C A, V84, P1261; ROBBINS H, 1970, ANN MATH STAT, V41, P1397, DOI 10.1214/aoms/1177696786; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; Russo D., 2016, C LEARNING THEORY, P1417; Shin J., 2019, ARXIV190200746; SIEGMUND D, 1978, BIOMETRIKA, V65, P341; STARR N, 1968, P NATL ACAD SCI USA, V61, P1215, DOI 10.1073/pnas.61.4.1215; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Villar SS, 2015, STAT SCI, V30, P199, DOI 10.1214/14-STS504	23	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307015
C	Sihag, S; Tajer, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sihag, Saurabh; Tajer, Ali			Structure Learning with Side Information: Sample Complexity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GRAPHICAL MODELS; SELECTION	Graphical models encode the stochastic dependencies among random variables (RVs). The vertices represent the RVs, and the edges signify the conditional dependencies among the RVs. Structure learning is the process of inferring the edges by observing realizations of the RVs, and it has applications in a wide range of technological, social, and biological networks. Learning the structure of graphs when the vertices are treated in isolation from inferential information known about them is well-investigated. In a wide range of domains, however, often there exist additional inferred knowledge about the structure, which can serve as valuable side information. For instance, the gene networks that represent different subtypes of the same cancer share similar edges across all subtypes and also have exclusive edges corresponding to each subtype, rendering partially similar graphical models for gene expression in different cancer subtypes. Hence, an inferential decision regarding a gene network can serve as side information for inferring other related gene networks. When such side information is leveraged judiciously, it can translate to significant improvement in structure learning. Leveraging such side information can be abstracted as inferring structures of distinct graphical models that are partially similar. This paper focuses on Ising graphical models, and considers the problem of simultaneously learning the structures of two partiallysimilar graphs, where any inference about the structure of one graph offers side information for the other graph. The bounded edge subclass of Ising models is considered, and necessary conditions (information-theoretic), as well as sufficient conditions (algorithmic) for the sample complexity for achieving a bounded probability of error, are established. Furthermore, specific regimes are identified in which the necessary and sufficient conditions coincide, rendering the optimal sample complexity.	[Sihag, Saurabh; Tajer, Ali] Rensselaer Polytech Inst, Elect Comp & Syst Engn Dept, Troy, NY 12181 USA	Rensselaer Polytechnic Institute	Sihag, S (corresponding author), Rensselaer Polytech Inst, Elect Comp & Syst Engn Dept, Troy, NY 12181 USA.			Sihag, Saurabh/0000-0001-9209-7943				Banerjee O, 2008, J MACH LEARN RES, V9, P485; Chen X., BIOINFORMATICS, V29, P2137; Danaher P, 2014, J R STAT SOC B, V76, P373, DOI 10.1111/rssb.12033; Das A. K., IEEE INT S INF THEOR, P2731; Devroye L., 2018, ARXIV180606887; Dobra A, 2004, J MULTIVARIATE ANAL, V90, P196, DOI 10.1016/j.jmva.2004.02.009; Dvijotham K, 2017, CONSTRAINTS, V22, P24, DOI 10.1007/s10601-016-9253-y; Fang J, 2016, BIOINFORMATICS, V32, P3480, DOI 10.1093/bioinformatics/btw485; Fishkind DE, 2019, PATTERN RECOGN, V87, P203, DOI 10.1016/j.patcog.2018.09.014; Gangrade A, 2017, ANN ALLERTON CONF, P1016; Guo J, 2015, ANN APPL STAT, V9, P821, DOI 10.1214/13-AOAS700; Guo JA, 2011, BIOMETRIKA, V98, P1, DOI 10.1093/biomet/asq060; Jacob Y, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P373, DOI 10.1145/2556195.2556225; Lauritzen Steffen L., 1996, OXFORD STAT SCI SERI, V17; Lyzinski V, 2014, J MACH LEARN RES, V15, P3513; Mohan K, 2014, J MACH LEARN RES, V15, P445; Neykov M., 2017, ARXIV170906688; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Peterson C, 2015, J AM STAT ASSOC, V110, P159, DOI 10.1080/01621459.2014.896806; Qiu HT, 2016, J R STAT SOC B, V78, P487, DOI 10.1111/rssb.12123; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; Rothman AJ, 2008, ELECTRON J STAT, V2, P494, DOI 10.1214/08-EJS176; Scarlett J, 2016, IEEE T SIGNAL INF PR, V2, P625, DOI 10.1109/TSIPN.2016.2596439; Sihag S, 2019, INT CONF ACOUST SPEE, P5292, DOI 10.1109/ICASSP.2019.8682199; Tandon R., 2014, ADV NEURAL INFORM PR, V27, P2303; Tandon R., P IEEE INT S INF THE, P2493; Vats D, 2011, IEEE INT SYMP INFO, P303, DOI 10.1109/ISIT.2011.6034133; Wang W., 2010, P IEEE INT S INF THE; WON CS, 1992, CVGIP-GRAPH MODEL IM, V54, P308, DOI 10.1016/1049-9652(92)90078-C; Yang S, 2015, SIAM J OPTIMIZ, V25, P916, DOI 10.1137/130936397; Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018	33	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906008
C	Singh, G; Yoon, J; Son, Y; Ahn, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Singh, Gautam; Yoon, Jaesik; Son, Youngsung; Ahn, Sungjin			Sequential Neural Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Neural Processes combine the strengths of neural networks and Gaussian processes to achieve both flexible learning and fast prediction in stochastic processes. However, a large class of problems comprises underlying temporal dependency structures in a sequence of stochastic processes that Neural Processes (NP) do not explicitly consider. In this paper, we propose Sequential Neural Processes (SNP) which incorporates a temporal state-transition model of stochastic processes and thus extends its modeling capabilities to dynamic stochastic processes. In applying SNP to dynamic 3D scene modeling, we introduce the Temporal Generative Query Networks. To our knowledge, this is the first 4D model that can deal with the temporal dynamics of 3D scenes. In experiments, we evaluate the proposed methods in dynamic (non-stationary) regression and 4D scene inference and rendering.	[Singh, Gautam; Ahn, Sungjin] Rutgers State Univ, New Brunswick, NJ 08854 USA; [Yoon, Jaesik] SAP, New Brunswick, NJ USA; [Son, Youngsung] ETRI, New Brunswick, NJ USA	Rutgers State University New Brunswick	Singh, G (corresponding author), Rutgers State Univ, New Brunswick, NJ 08854 USA.	singh.gautam@rutgers.edu; jaesik.yoon01@sap.com; ysson@etri.re.kr; sungjin.ahn@rutgers.edu			Electronics and Telecommunications Research Institute (ETRI) - Korean government [19ZH1100]; Kakao Brain, Center for Super Intelligence (CSI); Element AI; SAP	Electronics and Telecommunications Research Institute (ETRI) - Korean government; Kakao Brain, Center for Super Intelligence (CSI); Element AI; SAP	This work was supported by Electronics and Telecommunications Research Institute (ETRI) grant funded by the Korean government. [19ZH1100, Distributed Intelligence Core Technology of Hyper-Connected Space]. SA thanks to Kakao Brain, Center for Super Intelligence (CSI), and Element AI for their support. JY thanks to Kakao Brain and SAP for their support.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; [Anonymous], 2015, NIPS 15 P 28 INT C N; Auger-Methe M, 2016, SCI REP-UK, V6, DOI 10.1038/srep26677; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Bowman S. R., 2016, ARXIV, P10; Brockman G., 2016, OPENAI GYM; Chung J, 2015, ADV NEURAL INFORM PR, P2980; Eleftheriadis S., 2017, ADV NEURAL INFORM PR, P5309; Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170; Fraccaro M., 2017, ADV NEURAL INFORM PR, V30, P3601; Fraccaro M., 2018, INT C MACH LEARN, P1544; Garnelo M, 2018, ARXIV180701622; Garnelo M., 2018, ARXIV180701613; Gemici M., 2017, ARXIV PREPRINT ARXIV; Goyal A.G.A.P., 2017, ADV NEURAL INFORM PR, P6713; Hafner D., 2018, ARXIV181104551; Hassabis Demis, 2018, ARXIV180203006; Karl M., 2016, INT C LEARN REPR; Kim Hyunjik, 2019, P INT C LEARN REPR; Kingma DP, 2 INT C LEARN REPR I, P1; Krishnan RG, 2017, AAAI CONF ARTIF INTE, P2101; Kumar Ananya, 2018, ARXIV180702033; Mordatch I., 2015 IEEE RSJ INT C, P5307; Nair V., 2010, ICML, P807; Rosenbaum Dan, 2018, ARXIV180703149; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Zheng X, 2017, ARXIV171111179CSLG	29	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901084
C	Subramanya, SJ; Devvrit; Kadekodi, R; Krishaswamy, R; Simhadri, HV		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Subramanya, Suhas Jayaram; Devvrit; Kadekodi, Rohan; Krishaswamy, Ravishankar; Simhadri, Harsha Vardhan			DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Current state-of-the-art approximate nearest neighbor search (ANNS) algorithms generate indices that must be stored in main memory for fast high-recall search. This makes them expensive and limits the size of the dataset. We present a new graph-based indexing and search system called DiskANN that can index, store, and search a billion point database on a single workstation with just 64GB RAM and an inexpensive solid-state drive (SSD). Contrary to current wisdom, we demonstrate that the SSD-based indices built by DiskANN can meet all three desiderata for large-scale ANNS: high-recall, low query latency and high density (points indexed per node). On the billion point SIFT1B bigann dataset, DiskANN serves > 5000 queries a second with < 3ms mean latency and 95%+ 1-recall@1 on a 16 core machine, where state-of-the-art billion-point ANNS algorithms with similar memory footprint like FAISS [18] and IVFOADC+G+P [8] plateau at around 50% 1-recall@1. Alternately, in the high recall regime, DiskANN can index and serve 5 10x more points per node compared to state-of-the-art graph-based methods such as HNSW [21] and NSG [13]. Finally, as part of our overall DiskANN system, we introduce Vamana, a new graph-based ANNS index that is more versatile than the existing graph indices even for in-memory indices.	[Subramanya, Suhas Jayaram] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Devvrit; Kadekodi, Rohan] Univ Texas Austin, Austin, TX 78712 USA; [Subramanya, Suhas Jayaram; Krishaswamy, Ravishankar; Simhadri, Harsha Vardhan] Microsoft Res India, Bangalore, Karnataka, India	Carnegie Mellon University; University of Texas System; University of Texas Austin; Microsoft	Subramanya, SJ (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	suhas@cmu.edu; devvrit.030@gmail.com; rak@cs.texas.edu; rakri@microsoft.com; harshasi@microsoft.com						Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494; Andoni A, 2015, ACM S THEORY COMPUT, P793, DOI 10.1145/2746539.2746553; Andoni Alexandr, 2015, ADV NEURAL INFORM PR, P1225; Arya S., 1993, P 4 ANN ACM SIAM S D, P271; Babenko A, 2016, PROC CVPR IEEE, P2055, DOI 10.1109/CVPR.2016.226; Baranchuk Dmitry, 2018, ABS180202422 CORR; Baranchuk Dmitry, 2018, EUR C COMP VIS ECCV; BENTLEY JL, 1980, COMMUN ACM, V23, P214, DOI 10.1145/358841.358850; Clarkson K. L., 1994, Proceedings of the Tenth Annual Symposium on Computational Geometry, P160, DOI 10.1145/177424.177609; Douze Matthijs, 2017, FAISS LIB EFFICIENT; Fu C, 2019, PROC VLDB ENDOW, V12, P461, DOI 10.14778/3303753.3303754; Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Jaromczyk Jerzy W., 1992, RELATIVE NEIGHBORHOO; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Johnson Jeff, 2017, BILLION SCALE SIMILA; Kalantidis Y, 2014, PROC CVPR IEEE, P2329, DOI 10.1109/CVPR.2014.298; Li W, 2020, IEEE T KNOWL DATA EN, V32, P1475, DOI 10.1109/TKDE.2019.2909204; Malkov Y. A., 2016, ABS160309320 CORR; Wang J, 2012, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.2012.6247790; Weber R., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P194; Zhang Minjia, 2018, ABS180904067 CORR	22	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905042
C	Sun, YF; Duan, YQ; Gong, H; Wang, MD		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sun, Yifan; Duan, Yaqi; Gong, Hao; Wang, Mengdi			Learning low-dimensional state embeddings and metastable clusters from time series data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				REDUCTION	This paper studies how to find compact state embeddings from high-dimensional Markov state trajectories, where the transition kernel has a small intrinsic rank. In the spirit of diffusion map, we propose an efficient method for learning a low-dimensional state embedding and capturing the process's dynamics. This idea also leads to a kernel reshaping method for more accurate nonparametric estimation of the transition function. State embedding can be used to cluster states into metastable sets, thereby identifying the slow dynamics. Sharp statistical error bounds and misclassification rate are proved. Experiment on a simulated dynamical system shows that the state clustering method indeed reveals metastable structures. We also experiment with time series generated by layers of a Deep-Q-Network when playing an Atari game. The embedding method identifies game states to be similar if they share similar future events, even though their raw data are far different.	[Sun, Yifan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Duan, Yaqi; Gong, Hao; Wang, Mengdi] Princeton Univ, Princeton, NJ 08544 USA	Carnegie Mellon University; Princeton University	Sun, YF (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	yifans@andrew.cmu.edu; yaqid@princeton.edu; hgong@princeton.edu; mengdiw@princeton.edu		Wang, Mengdi/0000-0002-2101-9507				[Anonymous], 2011, J CHEM PHYS; Berlinet A., 2004, REPRODUCING KERNEL H, DOI [10.1007/978-1-4419-9096-9, DOI 10.1007/978-1-4419-9096-9]; Bertsekas Dimitri P, 1996, NEURODYNAMIC PROGRAM, V5; Bohmer W., 2015, KI KUNSTLICHE INTELL, V29, P1, DOI [10.1007/s13218-015-0356-1, DOI 10.1007/S13218-015-0356-1]; Claire Lacour, 2007, THESIS; Coifman Ronald R., 2008, SIAM J MULTISCALE MO, V7, P852; Duan Y., 2019, ADV NEURAL INFORM PR, P4488; Garding Lars, 1954, MATH SCAND, P237; Grunewfilder Steffen, 2012, INT C MACH LEARN; Klus S, 2018, J NONLINEAR SCI, V28, P985, DOI 10.1007/s00332-017-9437-7; Klus Stefan, 2016, J COMPUT DYNAM, V3, P51; Klus Stefan, 2018, ARXIV171201572; Krishnamurthy A., 2016, ADV NEURAL INFORM PR, P1840; Lafon S, 2006, IEEE T PATTERN ANAL, V28, P1393, DOI 10.1109/TPAMI.2006.184; Li Xudong, 2018, INT C MACH LEARN; Loffler M., 2018, ARXIV180808153; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Muandet K, 2017, FOUND TRENDS MACH LE, V10, P1, DOI 10.1561/2200000060; Nadler B., 2006, ADV NEURAL INFORM PR, P955; Peres Yuval, 2017, MARKOV CHAINS MIXING, V107; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Sart M, 2014, ANN I H POINCARE-PR, V50, P1028, DOI 10.1214/13-AIHP551; Schutte C, 2011, J CHEM PHYS, V134, DOI 10.1063/1.3590108; Schutte C., 2013, METASTABILITY MARKOV, V24; Smola A., 2007, INT C ALG LEARN THEO; Song Le, 2009, INT C MACH LEARN; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Weinan E, 2008, P NATL ACAD SCI USA, V105, P7907, DOI 10.1073/pnas.0707563105; YAKOWITZ S, 1979, ANN STAT, V7, P671, DOI 10.1214/aos/1176344687; Zhang A, 2018, ARXIV180202920	31	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304055
C	Suresh, AT		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Suresh, Ananda Theertha			Differentially private anonymized histograms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ENTROPY	For a dataset of label-count pairs, an anonymized histogram is the multiset of counts. Anonymized histograms appear in various potentially sensitive contexts such as password-frequency lists, degree distribution in social networks, and estimation of symmetric properties of discrete distributions. Motivated by these applications, we propose the first differentially private mechanism to release anonymized histograms that achieves near-optimal privacy utility trade-off both in terms of number of items and the privacy parameter. Further, if the underlying histogram is given in a compact format, the proposed algorithm runs in time sub-linear in the number of items. For anonymized histograms generated from unknown discrete distributions, we show that the released histogram can be directly used for estimating symmetric properties of the underlying distribution.	[Suresh, Ananda Theertha] Google Res, New York, NY 10011 USA	Google Incorporated	Suresh, AT (corresponding author), Google Res, New York, NY 10011 USA.	theertha@google.com						Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Acharya J., 2018, P 35 INT C MACH LEAR, P30; Acharya J., 2019, AISTATS; Acharya J, 2017, PR MACH LEARN RES, V70; Acharya J, 2017, IEEE T INFORM THEORY, V63, P38, DOI 10.1109/TIT.2016.2620435; ADAM NR, 1989, COMPUT SURV, V21, P515, DOI 10.1145/76894.76895; Agrawal D., 2001, PROC 20 ACM SIGMOD S, P247, DOI DOI 10.1145/375551.375602; Alda F, 2018, INFORM PROCESS LETT, V129, P1, DOI 10.1016/j.ipl.2017.09.001; [Anonymous], 2014, P 26 ANN ACM SIAM S, DOI DOI 10.1137/1.9781611973730.124; Backstrom L., 2007, PROC 16 INT C WORLD, P181, DOI DOI 10.1145/1242572.1242598; Barlow R.E., 1972, TECHNICAL REPORT; Batu T, 2000, ANN IEEE SYMP FOUND, P259, DOI 10.1109/SFCS.2000.892113; Blocki J., 2016, PROC NDSS, V16, P153; Blocki J., 2016, DIFFERENTIALLY PRIVA; Blocki J, 2018, P IEEE S SECUR PRIV, P853, DOI 10.1109/SP.2018.00009; Bonneau J, 2012, P IEEE S SECUR PRIV, P538, DOI 10.1109/SP.2012.49; Bonneau Joseph, 2015, YAHOO PASSWORD FREQU; Dalenius Tore, 1977, STAT TIDSKRIFT, V15, P2; Day WY, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P123, DOI 10.1145/2882903.2926745; de Leeuw J, 2009, J STAT SOFTW, V32, P1; Duchi JC, 2013, ANN IEEE SYMP FOUND, P429, DOI 10.1109/FOCS.2013.53; Dwork C, 2008, LECT NOTES COMPUT SC, V4978, P1, DOI 10.1007/978-3-540-79228-4_1; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Erlingsson U, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1054, DOI 10.1145/2660267.2660348; Ghosh A, 2012, SIAM J COMPUT, V41, P1673, DOI 10.1137/09076828X; Hao Y., 2018, ADV NEURAL INFORM PR, P8834; Hao Y., 2019, ARXIV PREPRINT ARXIV; Hao Yi, 2019, ARXIV190603794; Hardy GH, 1918, P LOND MATH SOC, V17, P75; Hay M, 2010, PROC VLDB ENDOW, V3, P1021; Hay M, 2009, IEEE DATA MINING, P169, DOI 10.1109/ICDM.2009.11; Hay M, 2008, PROC VLDB ENDOW, V1, P102, DOI 10.14778/1453856.1453873; Jiao JT, 2015, IEEE T INFORM THEORY, V61, P2835, DOI [10.1109/TIT.2015.2412945, 10.1109/tit.2015.2412945]; Kairouz P., 2014, ADV NEURAL INFORM PR, V27, P2879; Karwa Vishesh, 2012, Privacy in Statistical Databases. UNESCO Chair in Data Privacy. Proceedings of the International Conference, PSD 2012, P273, DOI 10.1007/978-3-642-33627-0_21; Kasiviswanathan SP, 2013, LECT NOTES COMPUT SC, V7785, P457, DOI 10.1007/978-3-642-36594-2_26; Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090; Narayanan A, 2009, P IEEE S SECUR PRIV, P173, DOI 10.1109/SP.2009.22; Orlitsky A., 2004, PROC 20 C UNCERTAINT, P426; Orlitsky A, 2016, P NATL ACAD SCI USA, V113, P13283, DOI 10.1073/pnas.1607774113; Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272; Raskhodnikova S., 2016, FOCS; Valiant G, 2011, ACM S THEORY COMPUT, P685; Valiant G, 2011, ANN IEEE SYMP FOUND, P403, DOI 10.1109/FOCS.2011.81; WARNER SL, 1965, J AM STAT ASSOC, V60, P63, DOI 10.2307/2283137; Wu YH, 2016, IEEE T INFORM THEORY, V62, P3702, DOI 10.1109/TIT.2016.2548468; Zou J, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13293	48	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308004
C	Tobar, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tobar, Felipe			Band-Limited Gaussian Processes: The Sinc Kernel	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a novel class of Gaussian processes (GPs) whose spectra have compact support, meaning that their sample trajectories are almost-surely band limited. As a complement to the growing literature on spectral design of covariance kernels, the core of our proposal is to model power spectral densities through a rectangular function, which results in a kernel based on the sinc function with straightforward extensions to non-centred (around zero frequency) and frequency-varying cases. In addition to its use in regression, the relationship between the sinc kernel and the classic theory is illuminated, in particular, the Shannon-Nyquist theorem is interpreted as posterior reconstruction under the proposed kernel. Additionally, we show that the sinc kernel is instrumental in two fundamental signal processing applications: first, in stereo amplitude modulation, where the non-centred sinc kernel arises naturally. Second, for band-pass filtering, where the proposed kernel allows for a Bayesian treatment that is robust to observation noise and missing data. The developed theory is complemented with illustrative graphic examples and validated experimentally using real-world data.	[Tobar, Felipe] Univ Chile, Ctr Math Modeling, Santiago, Chile	Universidad de Chile	Tobar, F (corresponding author), Univ Chile, Ctr Math Modeling, Santiago, Chile.	ftobar@dim.uchile.cl			Conicyt-PIA [AFB 170001]; Fondecyt-Iniciacion [11171165]	Conicyt-PIA; Fondecyt-Iniciacion(Comision Nacional de Investigacion Cientifica y Tecnologica (CONICYT)CONICYT FONDECYT)	This work was funded by the projects Conicyt-PIA #AFB 170001 Center for Mathematical Modeling and Fondecyt-Iniciacion #11171165.	Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; [Anonymous], 2008, ADAPTIVE FILTER THEO; Bochner S., 1959, LECT FOURIER INTEGRA; Boloix-Tortosa R, 2018, IEEE T NEUR NET LEAR, V29, P5499, DOI 10.1109/TNNLS.2018.2805019; Bretthorst G.L., 1988, BAYESIAN SPECTRUM AN; Garofolo JS, 1993, DAPRPA TIMIT AC CONT; Goldberger A. L., 1991, NONLINEAR DYNAMICS B, P583; Gregory PC, 2001, AIP CONF PROC, V568, P557, DOI 10.1063/1.1381917; Hensman J, 2018, J MACH LEARN RES, V18, P1; Jaynes E. T., 1987, Maximum-Entropy and Bayesian Analysis and Estimation Problems. Proceedings of the Third Workshop on Maximum Entropy and Bayesian Methods in Applied Statistics, P1; Jaynes E.T., 2003, PROBABILITY THEORY L; Kay S.M, 1988, MODERN SPECTRAL ESTI; Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865; Ldzaro-Gredilla M., 2009, ADV NEURAL INFORM PR, V22, P1087; Melkumyan A., 2011, P INT JOINT C ART IN, V2, P1408; Nawab S.H., 1996, SIGNALS SYSTEMS, V2; Nyquist H., 1928, T AM I ELECT ENG, V47, P617, DOI [10.1109/T-AIEE.1928.5055024, DOI 10.1109/T-AIEE.1928.5055024]; Parra G., 2017, P NIPS, P6681; POWELL MJD, 1964, COMPUT J, V7, P155, DOI 10.1093/comjnl/7.2.155; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Remes S., 2017, ADV NEURAL INFORM PR, V30, P4642; SHANNON CE, 1949, P IRE, V37, P10, DOI 10.1109/JRPROC.1949.232969; Stoica P., 2005, SPECTRAL ANAL SIGNAL; Tal M, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P665; Tobar F., 2015, ADV NEURAL INFORM PR, P3501; Tobar F., 2015, NIPS 2015 TIME SERIE; Tobar F., 2018, ADV NEURAL INFORM PR, P10148; Tobar F, 2015, INT CONF ACOUST SPEE, P2209, DOI 10.1109/ICASSP.2015.7178363; Ulrich K.R., 2015, ADV NEURAL INFORM PR, P1999; Valenzuela C, 2019, INT CONF ACOUST SPEE, P3367, DOI 10.1109/ICASSP.2019.8683798; Whittaker Edmund Taylor, 1915, P R SOC EDINB B, V35, P181, DOI DOI 10.1017/S0370164600017806; Wilson A., 2013, INT C MACH LEARN, P1067; Wright S., 1999, SPRINGER SCI, V35, P7	33	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904040
C	Trott, A; Zheng, S; Xiong, CM; Socher, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Trott, Alexander; Zheng, Stephan; Xiong, Caiming; Socher, Richard			Keeping Your Distance: Solving Sparse Reward Tasks Using Self-Balancing Shaped Rewards	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					While using shaped rewards can be beneficial when solving sparse reward tasks, their successful application often requires careful engineering and is problem specific. For instance, in tasks where the agent must achieve some goal state, simple distance-to-goal reward shaping often fails, as it renders learning vulnerable to local optima. We introduce a simple and effective model-free method to learn from shaped distance-to-goal rewards on tasks where success depends on reaching a goal state. Our method introduces an auxiliary distance-based reward based on pairs of rollouts to encourage diverse exploration. This approach effectively prevents learning dynamics from stabilizing around local optima induced by the naive distance-to-goal reward shaping and enables policies to efficiently solve sparse reward tasks. Our augmented objective does not require any additional reward engineering or domain expertise to implement and converges to the original sparse objective as the agent learns to solve the task. We demonstrate that our method successfully solves a variety of hard-exploration tasks (including maze navigation and 3D construction in a Minecraft environment), where naive distance-based reward shaping otherwise fails, and intrinsic curiosity and reward relabeling strategies exhibit poor performance.	[Trott, Alexander; Zheng, Stephan; Xiong, Caiming; Socher, Richard] Salesforce Res, San Francisco, CA 94105 USA	Salesforce	Trott, A (corresponding author), Salesforce Res, San Francisco, CA 94105 USA.	atrott@salesforce.com; stephan.zheng@salesforce.com; cxiong@salesforce.com; rsocher@salesforce.com						Andrychowicz M., 2017, ADV NEURAL INFORM PR; [Anonymous], 2009, ICML; Arjona- Medina Jose A, 2019, NEURIPS; Bellemare M, 2016, NIPS; Burda Y., 2018, LARGE SCALE STUDY CU; Burda Y., 2018, EXPLORATION RANDOM N; Chou Po-Wei, 2017, ICML; Clark J., 2016, FAULTY REWARD FUNCTI; Espeholt L., 2018, ICML; Eysenbach B., 2019, ICLR; Florensa Carlos, 2018, ICML; Ghosh D, 2018, 7 INT C LEARN REPR; Haarnoja T., 2017, ICML; Johnson M., 2016, IJCAI; Kaelbling Leslie Pack, 1993, IJCAI; Lillicrap Timothy P, 2016, ICLR; Liu Hanxiao, 2019, ICLR; Mataric M. J., 1994, ICML; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Molchanov Artem, 2018, REGION GROWING CURRI; Nachum O., 2018, DATA EFFICIENT HIERA; Nair Ashvin, 2018, VISUAL REINFORCEMENT; Ng A. Y., 1999, ICML; Pathak D., 2017, ICML; Savinov Nikolay, 2019, ICLR; Schaul T., 2015, ICML; Schulman J., 2017, PROXIMAL POLICY OPTI; Sigaud Olivier, 2018, ICLR; Silver D, 2018, SCIENCE, V362, P1140, DOI 10.1126/science.aar6404; Sukhbaatar Sainbayar, 2018, ICLR; Sukhbaatar Sainbayar, 2018, LEARNING GOAL EMBEDD; Sutton R. S., 2011, AAMAS; Tang Haoran, 2017, NIPS; Todorov E., 2012, IEEE INT C INT ROB S; Warde-Farley David, 2019, INT C LEARN REPR; Zhao Rui, 2018, NEURIPS DEEP RL WORK	36	2	2	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902005
C	Umenberger, J; Ferizbegovic, M; Schon, TB; Hjalmarsson, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Umenberger, Jack; Ferizbegovic, Mina; Schon, Thomas B.; Hjalmarsson, Hakan			Robust exploration in linear quadratic reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				IDENTIFICATION; CAUTION; SAFE	This paper concerns the problem of learning control policies for an unknown linear dynamical system to minimize a quadratic cost function. We present a method, based on convex optimization, that accomplishes this task robustly: i.e., we minimize the worst-case cost, accounting for system uncertainty given the observed data. The method balances exploitation and exploration, exciting the system in such a way so as to reduce uncertainty in the model parameters to which the worst-case cost is most sensitive. Numerical simulations and application to a hardware-in-the-loop servo-mechanism demonstrate the approach, with appreciable performance and robustness gains over alternative methods observed in both.	[Umenberger, Jack; Schon, Thomas B.] Uppsala Univ, Dept Informat Technol, Uppsala, Sweden; [Ferizbegovic, Mina; Hjalmarsson, Hakan] KTH, Sch Elect Engn & Comp Sci, Stockholm, Sweden	Uppsala University; Royal Institute of Technology	Umenberger, J (corresponding author), Uppsala Univ, Dept Informat Technol, Uppsala, Sweden.	jack.umenberger@it.uu.se; minafe@kth.se; thomas.schon@it.uu.se; hjalmars@kth.se		Umenberger, Jack/0000-0001-6946-1508	Swedish Research Council; Swedish Foundation for Strategic Research (SSF); project NewLEADS - New Directions in Learning Dynamical Systems [621-2016-06079]; project ASSEMBLE [RIT15-0012]	Swedish Research Council(Swedish Research CouncilEuropean Commission); Swedish Foundation for Strategic Research (SSF)(Swedish Foundation for Strategic Research); project NewLEADS - New Directions in Learning Dynamical Systems; project ASSEMBLE	This research was financially supported by the project NewLEADS - New Directions in Learning Dynamical Systems (contract number: 621-2016-06079), funded by the Swedish Research Council and by the project ASSEMBLE (contract number: RIT15-0012), funded by the Swedish Foundation for Strategic Research (SSF).	Abbasi-Yadkori Y., 2011, P 24 ANN C LEARNING, P1; Abbeel P, 2005, ICML ACM INT C PROCE, V119, P1, DOI 10.1145/1102351.1102352; Abeille M, 2017, PR MACH LEARN RES, V54, P1246; Abeille M, 2018, PR MACH LEARN RES, V80; Annergren M, 2017, IEEE CONTR SYST MAG, V37, P31, DOI 10.1109/MCS.2016.2643243; ASTROM KJ, 1971, J MATH ANAL APPL, V34, P90, DOI 10.1016/0022-247X(71)90161-2; Aswani A, 2013, AUTOMATICA, V49, P1216, DOI 10.1016/j.automatica.2013.02.003; BARSHALOM Y, 1976, ANN ECON SOC MEAS, V5, P323; BARSHALOM Y, 1981, IEEE T AUTOMAT CONTR, V26, P1184, DOI 10.1109/TAC.1981.1102793; Berkenkamp Felix, 2017, ADV NEURAL INFORM PR, P908; Dean S, 2019, P AMER CONTR CONF, P5582; Dean S, 2018, ADV NEUR IN, V31; Doshi-Velez F, 2017, ARXIV PREPRINT ARXIV; Faradonbeh M. K. S., 2019, IEEE T AUTOMATIC CON; Feldbaum A. A., 1960, AUTOMAT REMOTE CONTR, V21, P1453; Feldbaum A.A., 1960, AVTOMATIKA TELEMEKHA, V21, P1240; Garcia J, 2015, J MACH LEARN RES, V16, P1437; Geibel P, 2005, J ARTIF INTELL RES, V24, P81, DOI 10.1613/jair.1666; Hazan E, 2018, ADV NEUR IN, V31; Hazan E, 2017, ADV NEUR IN, V30; Ibrahimi M., 2012, ADV NEURAL INFORM PR, P2636; Larsson CA, 2016, EUR J CONTROL, V29, P1, DOI 10.1016/j.ejcon.2016.03.001; Larsson CA, 2015, J PROCESS CONTR, V31, P1, DOI 10.1016/j.jprocont.2015.03.011; Liu Z, 2009, SIAM J MATRIX ANAL A, V31, P1235, DOI 10.1137/090755436; Lobo M. S., 1999, Proceedings of the 1999 American Control Conference (Cat. No. 99CH36251), P958, DOI 10.1109/ACC.1999.783182; Luo ZQ, 2004, SIAM J OPTIMIZ, V14, P1140, DOI 10.1137/S1052623403421498; Malik D., 2018, ARXIV181208305; Mania Horia, 2019, ARXIV190207826; Matni N, 2019, IEEE DECIS CONTR P, P3724, DOI 10.1109/CDC40024.2019.9029916; Mihatsch O, 2002, MACH LEARN, V49, P267, DOI 10.1023/A:1017940631555; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Ostafew CJ, 2016, INT J ROBOT RES, V35, P1547, DOI 10.1177/0278364916645661; Ouyang Yi, 2017, ARXIV170904047; Petersen K.B., 2012, MATRIX COOKBOOK; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Simchowitz M., 2018, C LEARNING THEORY, P439; Wainwright M.J., 2019, HIGH DIMENSIONAL STA, V48	40	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907004
C	Vaskevicius, T; Kanade, V; Rebeschini, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Vaskevicius, Tomas; Kanade, Varun; Rebeschini, Patrick			Implicit Regularization for Optimal Sparse Recovery	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LINEAR CONVERGENCE; REGRESSION; LASSO; OPTIMIZATION; SELECTION	We investigate implicit regularization schemes for gradient descent methods applied to unpenalized least squares regression to solve the problem of reconstructing a sparse signal from an underdetermined system of linear measurements under the restricted isometry assumption. For a given parametrization yielding a non-convex optimization problem, we show that prescribed choices of initialization, step size and stopping time yield a statistically and computationally optimal algorithm that achieves the minimax rate with the same cost required to read the data up to poly-logarithmic factors. Beyond minimax optimality, we show that our algorithm adapts to instance difficulty and yields a dimension-independent rate when the signal-to-noise ratio is high enough. Key to the computational efficiency of our method is an increasing step size scheme that adapts to refined estimates of the true solution. We validate our findings with numerical experiments and compare our algorithm against explicit l(1) penalization. Going from hard instances to easy ones, our algorithm is seen to undergo a phase transition, eventually matching least squares with an oracle knowledge of the true support.	[Vaskevicius, Tomas; Rebeschini, Patrick] Univ Oxford, Dept Stat, Oxford, England; [Kanade, Varun] Univ Oxford, Dept Comp Sci, Oxford, England	University of Oxford; University of Oxford	Vaskevicius, T (corresponding author), Univ Oxford, Dept Stat, Oxford, England.	tomas.vaskevicius@stats.ox.ac.uk; varunk@cs.ox.ac.uk; patrick.rebeschini@stats.ox.ac.uk		Rebeschini, Patrick/0000-0001-7772-4160	EPSRC; MRC through the OxWaSP CDT programme [EP/L016710/1]; Alan Turing Institute under the EPSRC [EP/N510129/1]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); MRC through the OxWaSP CDT programme(UK Research & Innovation (UKRI)Medical Research Council UK (MRC)); Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	Tomas Vaskevicius is supported by the EPSRC and MRC through the OxWaSP CDT programme (EP/L016710/1). Varun Kanade and Patrick Rebeschini are supported in part by the Alan Turing Institute under the EPSRC grant EP/N510129/1.	Adamczak R, 2011, CONSTR APPROX, V34, P61, DOI 10.1007/s00365-010-9117-4; Akkaram S, 2010, PROCEEDINGS OF THE ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE, DETC 2010, VOL 3, A AND B, P37; Ali A., 2018, ARXIV181010082; Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015; Bandeira AS, 2013, IEEE T INFORM THEORY, V59, P3448, DOI 10.1109/TIT.2013.2248414; Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x; Bertsimas D, 2016, ANN STAT, V44, P813, DOI 10.1214/15-AOS1388; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Bredies K, 2008, J FOURIER ANAL APPL, V14, P813, DOI 10.1007/s00041-008-9041-1; Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Cohen A, 2009, J AM MATH SOC, V22, P211; Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Feuer A, 2003, IEEE T INFORM THEORY, V49, P1579, DOI 10.1109/TIT.2003.811926; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Friedman Jerome, 2004, WORKING PAPER; Friedman S, 2001, IND REL RES, P1, DOI 10.1097/00054725-200102000-00001; Guedon O, 2007, POSITIVITY, V11, P269, DOI 10.1007/s11117-006-2059-1; Guedon O, 2008, REV MAT IBEROAM, V24, P1075; Gunasekar S., 2017, ADV NEURAL INFORM PR, P6151; Gunasekar S, 2018, PR MACH LEARN RES, V80; Gunasekar S, 2018, ADV NEUR IN, V31; Hale ET, 2008, SIAM J OPTIMIZ, V19, P1107, DOI 10.1137/070698920; Hastie T., 2017, ARXIV PREPRINT ARXIV; Jordan M.I., 2014, C LEARN THEOR, P921; Li Y., 2018, C LEARN THEOR, V75, P2; Meinshausen N, 2009, ANN STAT, V37, P246, DOI 10.1214/07-AOS582; Mendelson S, 2008, CONSTR APPROX, V28, P277, DOI 10.1007/s00365-007-9005-8; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Neu G, 2018, P MACHINE LEARNING R, V75, P3222; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Raskutti G, 2014, J MACH LEARN RES, V15, P335; Raskutti G, 2011, IEEE T INFORM THEORY, V57, P6976, DOI 10.1109/TIT.2011.2165799; Raskutti G, 2010, J MACH LEARN RES, V11, P2241; Romberg J, 2009, SIAM J IMAGING SCI, V2, P1098, DOI 10.1137/08072975X; Rosset S, 2004, J MACH LEARN RES, V5, P941; Rudelson M., 2012, C LEARNING THEORY, p10.1; Rudelson M, 2008, COMMUN PUR APPL MATH, V61, P1025, DOI 10.1002/cpa.20227; Soudry D, 2018, J MACH LEARN RES, V19; Suggala AS, 2018, ADV NEUR IN, V31; Tao SZ, 2016, SIAM J OPTIMIZ, V26, P313, DOI 10.1137/151004549; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani R., 2015, STAT LEARNING SPARSI; vandeGeer Sara, 2007, JSM P; Wainwright M.J., 2019, HIGH DIMENSIONAL STA, V48; Wei Y., 2017, 31 C NEUR INF PROC S, V30, P6065; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2; Zhang Chiyuan, 2016, ARXIV161103530; Zhang T, 2005, ANN STAT, V33, P1538, DOI 10.1214/009053605000000255; Zhao Peng, 2019, ARXIV190309367	56	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303001
C	Wang, P; Vasconcelos, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Pei; Vasconcelos, Nuno			Deliberative Explanations: visualizing network insecurities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BACKPROPAGATION	A new approach to explainable AI, denoted deliberative explanations, is proposed. Deliberative explanations are a visualization technique that aims to go beyond the simple visualization of the image regions (or, more generally, input variables) responsible for a network prediction. Instead, they aim to expose the deliberations carried by the network to arrive at that prediction, by uncovering the insecurities of the network about the latter. The explanation consists of a list of insecurities, each composed of 1) an image region (more generally, a set of input variables), and 2) an ambiguity formed by the pair of classes responsible for the network uncertainty about the region. Since insecurity detection requires quantifying the difficulty of network predictions, deliberative explanations combine ideas from the literature on visual explanations and assessment of classification difficulty. More specifically, the proposed implementation combines attributions with respect to both class predictions and a difficulty score. An evaluation protocol that leverages object recognition (CUB200) and scene classification (ADE20K) datasets that combine part and attribute annotations is also introduced to evaluate the accuracy of deliberative explanations. Finally, an experimental evaluation shows that the most accurate explanations are achieved by combining non self-referential difficulty scores and second-order attributions. The resulting insecurities are shown to correlate with regions of attributes shared by different classes. Since these regions are also ambiguous for humans, deliberative explanations are intuitive, suggesting that the deliberative process of modern networks correlates with human reasoning.	[Wang, Pei; Vasconcelos, Nuno] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Wang, P (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.	pew062@ucsd.edu; nvasconcelos@ucsd.edu		Vasconcelos, Nuno/0000-0002-9024-4302	NSF [IIS-1546305, IIS-1637941, IIS-1924937]; NVIDIA GPU	NSF(National Science Foundation (NSF)); NVIDIA GPU	This work was partially funded by NSF awards IIS-1546305, IIS-1637941, IIS-1924937, and NVIDIA GPU donations.	Alvarez-Melis D, 2018, ADV NEUR IN, V31; Ancona M, 2017, NIPS 2017 WORKSH INT; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Daftry S, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1743, DOI 10.1109/IROS.2016.7759279; DeVries Terrance, 2018, ARXIV180204865; Dhurandhar A., 2018, ADV NEURAL INFORM PR, P592; Endres DM, 2003, IEEE T INFORM THEORY, V49, P1858, DOI 10.1109/TIT.2003.813506; Fong R, 2018, PROC CVPR IEEE, P8730, DOI 10.1109/CVPR.2018.00910; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Goyal Yash, 2019, INT C MACH LEARN, P2376; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Harada T., 2018, ARXIV181201280; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hendricks L.A., 2018, ICML WHI; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1861; Jiang H, 2018, ADV NEUR IN, V31; Johns E, 2015, PROC CVPR IEEE, P2616, DOI 10.1109/CVPR.2015.7298877; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li KP, 2018, PROC CVPR IEEE, P9215, DOI 10.1109/CVPR.2018.00960; Lundberg SM, 2017, ADV NEUR IN, V30; Mac Aodha O, 2018, PROC CVPR IEEE, P3820, DOI 10.1109/CVPR.2018.00402; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Mandelbaum A., 2017, ARXIV PREPRINT ARXIV; Miller T., 2018, ARXIV181103163; Montavon G, 2017, PATTERN RECOGN, V65, P211, DOI 10.1016/j.patcog.2016.11.008; Pei Wang, 2018, EUR C COMP VIS; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Saxena Dhruv Mauria, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5824, DOI 10.1109/ICRA.2017.7989684; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Sensoy Murat, 2018, NIPS, P3179; Shrikumar Avanti, 2017, PMLR, P3145, DOI DOI 10.1145/3292500.3330701; Shrikumar Avanti, 2016, ARXIV160501713; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Singla A., 2014, ICML, P3; Singla Sahil, 2019, INT C MACH LEARN; Sundararajan M, 2017, PR MACH LEARN RES, V70; van der Maaten L, 2014, J MACH LEARN RES, V15, P3221; van der Maaten L, 2012, MACH LEARN, V87, P33, DOI 10.1007/s10994-011-5273-4; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Van Der Maaten Laurens, 2009, ARTIF INTELL, P384; Wachter S., 2017, HARV J LAW TECHNOL, V31, P841, DOI DOI 10.2139/SSRN.3063289; Wang Xin, 2017, ARXIV PREPRINT ARXIV; Welinder P., 2010, CNSTR2010001 CALTECH; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319; Zhou Bolei, 2015, OBJECT DETECTORS EME, P2	53	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301037
C	Werpachowski, R; Gyorgy, A; Szepesvari, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Werpachowski, Roman; Gyorgy, Andras; Szepesvari, Csaba			Detecting Overfitting via Adversarial Examples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The frequent reuse of test sets in popular benchmark problems raises doubts about the credibility of reported test-error rates. Verifying whether a learned model is overfitted to a test set is challenging as independent test sets drawn from the same data distribution are usually unavailable, while other test sets may introduce a distribution shift. We propose a new hypothesis test that uses only the original test data to detect overfitting. It utilizes a new unbiased error estimate that is based on adversarial examples generated from the test data and importance weighting. Overfitting is detected if this error estimate is sufficiently different from the original test error rate. We develop a specialized variant of our test for multiclass image classification, and apply it to testing overfitting of recent models to the popular ImageNet benchmark. Our method correctly indicates overfitting of the trained model to the training set, but is not able to detect any overfitting to the test set, in line with other recent work on this topic.	[Werpachowski, Roman; Gyorgy, Andras; Szepesvari, Csaba] DeepMind, London, England		Werpachowski, R (corresponding author), DeepMind, London, England.	romanw@google.com; agyorgy@google.com; szepi@google.com						Azulay A., 2018, WHY DO DEEP CONVOLUT; Bahdanau D., 2015, INT C LEARN REPR ICL; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Bucklew J.A., 2004, INTRO RARE EVENT SIM; Carlini N., 2018, 2018 IEEE SEC PRIV W; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng L, 2013, INT CONF ACOUST SPEE, P8599, DOI 10.1109/ICASSP.2013.6639344; Dwork C, 2015, SCIENCE, V349, P636, DOI 10.1126/science.aaa9375; Ebrahimi J., 2018, P 27 INT C COMP LING, P653; Ebrahimi J, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P31; Goodfellow I., 2015, P INT C LEARN REPR I; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Hardt M, 2019, P INT C MACH LEARN, P1892; Kahn H, 1951, NATL BUR STAND APPL, V12, P27; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 2010, ATT LAB; Mania Horia, 2019, ARXIV190512580; Mnih V., 2008, P 25 INT C MACH LEAR; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Recht B, 2018, ARXIV180600451; Recht Benjamin, 2019, ARXIV190210811; Simonyan Karen, 2015, INT C LEARN REPR; Szegedy C, 2015, 2015 IEEE C COMP VIS, P1; Tieleman Tijmen, 2012, LECT 65 RMSPROP DIVI, V4; Uesato J, 2018, PR MACH LEARN RES, V80; Wu Y., 2016, ADV NEURAL INFORM PR, V1609, DOI 10.48550/arXiv.1609.08144; Yadav Chhavi, 2019, ARXIV190510498	32	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307083
C	Widmann, D; Lindsten, F; Zachariah, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Widmann, David; Lindsten, Fredrik; Zachariah, Dave			Calibration tests in multi-class classification: A unifying framework	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DECOMPOSITION; RELIABILITY	In safety-critical applications a probabilistic model is usually required to be calibrated, i.e., to capture the uncertainty of its predictions accurately. In multi-class classification, calibration of the most confident predictions only is often not sufficient. We propose and study calibration measures for multi-class classification that generalize existing measures such as the expected calibration error, the maximum calibration error, and the maximum mean calibration error. We propose and evaluate empirically different consistent and unbiased estimators for a specific class of measures based on matrix-valued kernels. Importantly, these estimators can be interpreted as test statistics associated with well-defined bounds and approximations of the p-value under the null hypothesis that the model is calibrated, significantly improving the interpretability of calibration measures, which otherwise lack any meaningful unit or scale.	[Widmann, David; Zachariah, Dave] Uppsala Univ, Dept Informat Technol, Uppsala, Sweden; [Lindsten, Fredrik] Linkoping Univ, Div Stat & Machine Learning, Linkoping, Sweden	Uppsala University; Linkoping University	Widmann, D (corresponding author), Uppsala Univ, Dept Informat Technol, Uppsala, Sweden.	david.widmann@it.uu.se; fredrik.lindsten@liu.se; dave.zachariah@it.uu.se	Widmann, David/HCJ-0034-2022	Widmann, David/0000-0001-9282-053X	Swedish Research Council via the project Learning of Large-Scale Probabilistic Dynamical Models [2016-04278]; Swedish Research Council via the project Counterfactual Prediction Methods for Heterogeneous Populations [2018-05040]; Swedish Foundation for Strategic Research via the project Probabilistic Modeling and Inference for Machine Learning [ICA16-0015]; Wallenberg Al, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation	Swedish Research Council via the project Learning of Large-Scale Probabilistic Dynamical Models; Swedish Research Council via the project Counterfactual Prediction Methods for Heterogeneous Populations; Swedish Foundation for Strategic Research via the project Probabilistic Modeling and Inference for Machine Learning; Wallenberg Al, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation	We thank the reviewers for all the constructive feedback on our paper. This research is financially supported by the Swedish Research Council via the projects Learning of Large-Scale Probabilistic Dynamical Models (contract number: 2016-04278) and Counterfactual Prediction Methods for Heterogeneous Populations (contract number: 2018-05040), by the Swedish Foundation for Strategic Research via the project Probabilistic Modeling and Inference for Machine Learning (contract number: ICA16-0015), and by the Wallenberg Al, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.	ARCONES MA, 1992, ANN STAT, V20, P655, DOI 10.1214/aos/1176348650; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Berlinet A., 2004, REPRODUCING KERNEL H, DOI [10.1007/978-1-4419-9096-9, DOI 10.1007/978-1-4419-9096-9]; Brocker J, 2007, WEATHER FORECAST, V22, P651, DOI 10.1175/WAF993.1; Brocker J, 2012, CLIM DYNAM, V39, P655, DOI 10.1007/s00382-011-1191-1; Caponnetto A, 2008, J MACH LEARN RES, V9, P1615; Carmeli C, 2010, ANAL APPL, V8, P19, DOI 10.1142/S0219530510001503; Christmann A., 2008, SUPPORT VECTOR MACHI; DEGROOT MH, 1983, J ROY STAT SOC D-STA, V32, P12; Ferro CAT, 2012, Q J ROY METEOR SOC, V138, P1954, DOI 10.1002/qj.1924; Gretton A, 2012, J MACH LEARN RES, V13, P723; Guo CA, 2017, PR MACH LEARN RES, V70; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; HOEFFDING W, 1948, ANN MATH STAT, V19, P293, DOI 10.1214/aoms/1177730196; Kiureghian AD, 2009, STRUCT SAF, V31, P105, DOI 10.1016/j.strusafe.2008.06.020; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kumar A, 2018, IEEE ENER CONV, P2805, DOI 10.1109/ECCE.2018.8557649; Kumar A, 2019, ADV NEUR IN, V32; Micchelli CA, 2005, NEURAL COMPUT, V17, P177, DOI 10.1162/0899766052530802; Murphy Allan H, 1977, J ROYAL STAT SOC C, V26, P41, DOI [DOI 10.2307/2346866, 10.2307/2346866]; Naeini M. P., 2015, P 20 9 AAAI C ART IN; Pastell M., 2017, J OPEN SOURCE SOFTW, V2, P204, DOI [10.21105/joss.00204, DOI 10.21105/JOSS.00204]; Phan H., 2019, PYTORCH CIFAR10; Rudin W., 1986, REAL COMPLEX ANAL, V3; Serfling R. J., 1980, APPROXIMATION THEORE, P11; Vaart A. W., 1998, ASYMPTOTIC STAT; Van Der VaartJon A., 1996, WEAK CONVERGENCE EMP; Widmann D., 2019, DEVMOTION CONSISTENC, DOI [10.5281/zenodo.3232854, DOI 10.5281/ZENODO.3232854]; Zadrozny B., 2002, INT C KNOWLEDGE DISC	32	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903083
C	Williams, F; Trager, M; Silva, C; Panozzo, D; Zorin, D; Bruna, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Williams, Francis; Trager, Matthew; Silva, Claudio; Panozzo, Daniele; Zorin, Denis; Bruna, Joan			Gradient Dynamics of Shallow Univariate ReLU Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present a theoretical and empirical study of the gradient dynamics of overparameterized shallow ReLU networks with one-dimensional input, solving least-squares interpolation. We show that the gradient dynamics of such networks are determined by the gradient flow in a non-redundant parameterization of the network function. We examine the principal qualitative features of this gradient flow. In particular, we determine conditions for two learning regimes: kernel and adaptive, which depend both on the relative magnitude of initialization of weights in different layers and the asymptotic behavior of initialization coefficients in the limit of large network widths. We show that learning in the kernel regime yields smooth interpolants, minimizing curvature, and reduces to cubic splines for uniform initializations. Learning in the adaptive regime favors instead linear splines, where knots cluster adaptively at the sample points.	[Williams, Francis; Trager, Matthew; Silva, Claudio; Panozzo, Daniele; Zorin, Denis; Bruna, Joan] NYU, New York, NY 10003 USA	New York University	Williams, F (corresponding author), NYU, New York, NY 10003 USA.				Alfred P. Sloan Foundation; NSF [DMS-1835712, RI-1816753, CIF 1845360, 1652515, IIS-1320635, DMS-1436591]; Samsung Electronics; SNSF [P2TIP2_175859]; Moore-Sloan Data Science Environment; DARPA D3M program; NVIDIA; Labex DigiCosme; DOA [W911NF-17-1-0438]	Alfred P. Sloan Foundation(Alfred P. Sloan Foundation); NSF(National Science Foundation (NSF)); Samsung Electronics(Samsung); SNSF(Swiss National Science Foundation (SNSF)); Moore-Sloan Data Science Environment; DARPA D3M program; NVIDIA; Labex DigiCosme; DOA	This work was partially supported by the Alfred P. Sloan Foundation, NSF RI-1816753, NSF CAREER CIF 1845360, Samsung Electronics, the NSF CAREER award 1652515, the NSF grant IIS-1320635, the NSF grant DMS-1436591, the NSF grant DMS-1835712, the SNSF grant P2TIP2_175859, the Moore-Sloan Data Science Environment, the DARPA D3M program, NVIDIA, Labex DigiCosme, DOA W911NF-17-1-0438, a gift from Adobe Research, and a gift from nTopology. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.	ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Arora Sanjeev, 2019, ARXIV190108584; Bach Francis., 2017, J MACHINE LEARNING R, V18, P629; Basri R., 2016, ARXIV160204723; Bietti Alberto, 2019, ARXIV190512173; CAO Y., 2019, ARXIV190201384; Chizat L., 2018, NOTE LAZY TRAINING S; Chizat Lenaic, 2018, ADV NEURAL INFORM PR, P3036; CLARKE FH, 1975, T AM MATH SOC, V205, P247, DOI 10.1090/s0002-9947-1975-0367131-6; Daniely Amit, 2017, ARXIV PREPRINT ARXIV; Daubechies I, 2019, ARXIV190502199; Du Simon S, 2018, GRADIENT DESCENT FIN; Du SS., 2019, P 7 INT C LEARN REPR; Ghorbani B., 2019, ARXIV190412191; Gunasekar S, 2018, ARXIV180208246; Hanin B., 2019, ARXIV190109021; Hastie Trevor, 2019, ARXIV190308560; Hotz Thomas, 2012, ARXIV12024443; Jacot Arthur, 2018, ADV NEURAL INFORM PR, P8571; Ma C., 2019, ARXIV190404326; Maennel  Hartmut, 2018, ARXIV180308367; Mei S, 2018, P NATL ACAD SCI USA, V115, pE7665, DOI 10.1073/pnas.1806579115; Oymak Samet, 2018, ARXIV181210004; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rotskoff Grant, 2019, P INT C MACH LEARN L; Rotskoff Grant M, 2018, STAT-US; SAFRAN I., 2017, ARXIV171208968; Savarese Pedro, 2019, ARXIV190205040; Sirignano J, 2018, ARXIV180809372; Soudry D, 2018, J MACH LEARN RES, V19; Venturi L., 2018, ARXIV PREPRINT ARXIV; Williams Francis, 2018, ARXIV181110943	32	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900002
C	Wu, QT; Zhang, ZX; Gao, XF; Yan, JC; Chen, GH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wu, Qitian; Zhang, Zixuan; Gao, Xiaofeng; Yan, Junchi; Chen, Guihai			Learning Latent Process from High-Dimensional Event Sequences via Efficient Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We target modeling latent dynamics in high-dimension marked event sequences without any prior knowledge about marker relations. Such problem has been rarely studied by previous works which would have fundamental difficulty to handle the arisen challenges: 1) the high-dimensional markers and unknown relation network among them pose intractable obstacles for modeling the latent dynamic process; 2) one observed event sequence may concurrently contain several different chains of interdependent events; 3) it is hard to well define the distance between two high-dimension event sequences. To these ends, in this paper, we propose a seminal adversarial imitation learning framework for high-dimension event sequence generation which could be decomposed into: 1) a latent structural intensity model that estimates the adjacent nodes without explicit networks and learns to capture the temporal dynamics in the latent space of markers over observed sequence; 2) an efficient random walk based generation model that aims at imitating the generation process of high-dimension event sequences from a bottom-up view; 3) a discriminator specified as a seq2seq network optimizing the rewards to help the generator output event sequences as real as possible. Experimental results on both synthetic and real-world datasets demonstrate that the proposed method could effectively detect the hidden network among markers and make decent prediction for future marked events, even when the number of markers scales to million level.	[Wu, Qitian; Zhang, Zixuan; Gao, Xiaofeng] Shanghai Key Lab Scalable Comp & Syst, Shanghai, Peoples R China; [Wu, Qitian; Zhang, Zixuan; Gao, Xiaofeng; Yan, Junchi] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China; [Yan, Junchi] Shanghai Jiao Tong Univ, MoE Key Lab Artificial Intelligence, Shanghai, Peoples R China; [Chen, Guihai] Nanjing Univ, State Key Labrotary Novel Software Technol, Nanjing, Peoples R China	Shanghai Jiao Tong University; Shanghai Jiao Tong University; Nanjing University	Gao, XF (corresponding author), Shanghai Key Lab Scalable Comp & Syst, Shanghai, Peoples R China.; Gao, XF (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.	echo740@sjtu.edu.cn; zzx_gongshi117@sjtu.edu.cn; gao-xf@cs.sjtu.edu.cn; yanjunchi@sjtu.edu.cn; gchen@nju.edu.cn		Chen, Guihai/0000-0002-6934-1685	National Key RD Program of China [2018YFB1004703]; National Natural Science Foundation of China [61872238, 61672353, 61972250]; Shanghai Science and Technology Fund [17510740200]; CCF-Huawei Database System Innovation Research Plan [CCF-Huawei DBIR2019002A]; Huawei Innovation Research Program [HO2018085286]; State Key Laboratory of Air Traffic Management System and Technology [SKLATM20180X]; Tencent Social Ads Rhino-Bird Focused Research Program	National Key RD Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shanghai Science and Technology Fund; CCF-Huawei Database System Innovation Research Plan; Huawei Innovation Research Program(Huawei Technologies); State Key Laboratory of Air Traffic Management System and Technology; Tencent Social Ads Rhino-Bird Focused Research Program	This work was supported by the National Key RD Program of China [2018YFB1004703]; the National Natural Science Foundation of China [61872238, 61672353, 61972250]; the Shanghai Science and Technology Fund [17510740200]; the CCF-Huawei Database System Innovation Research Plan [CCF-Huawei DBIR2019002A]; the Huawei Innovation Research Program [HO2018085286]; the State Key Laboratory of Air Traffic Management System and Technology [SKLATM20180X], and the Tencent Social Ads Rhino-Bird Focused Research Program.	Ahmed A, 2009, P NATL ACAD SCI USA, V106, P11878, DOI 10.1073/pnas.0901910106; [Anonymous], 2010, PROC 16 ACM SIGKDD I, DOI DOI 10.1145/1835804.1835933; Bourigault S, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P573, DOI 10.1145/2835776.2835817; COX DR, 1955, J ROY STAT SOC B, V17, P129; Daley D. J., 2007, INTRO THEORY POINT P, V2; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Didelez V, 2008, J ROY STAT SOC B, V70, P245, DOI 10.1111/j.1467-9868.2007.00634.x; DMichael Eichler R.D., 2016, ARXIV160506759; Du N., 2012, ADV NEURAL INFORM PR, V4, P2780; Du Nan, 2013, Adv Neural Inf Process Syst, V26, P3147; FOTHERINGHAM AS, 1991, ENVIRON PLANN A, V23, P1025, DOI 10.1068/a231025; Gomez-Rodriguez Manuel, 2011, ICML, P561; Hawkes A. G., 1971, J ROYAL STAT SOC B; Ho J., 2016, NIPS; Isham V, 1979, ADV APPL PROBAB, V37, P629; Lewis E., 2011, J NONPARAMETR STAT, V1, P1; Li CL, 2017, 2017 IEEE SECOND INTERNATIONAL CONFERENCE ON DC MICROGRIDS (ICDCM), P577, DOI 10.1109/ICDCM.2017.8001105; Li S., 2018, NIPS; Upadhyay U., 2018, ADV NEURAL INFORM PR, P3168; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Velickovic P., 2018, P 6 INT C LEARN REPR; Wang YQ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2985; Xiao S., 2017, NIPS; Xiao S., 2017, AAAI; Xu H, 2016, INT C MACH LEARN, P1717; Zhang J, 2015, ACM T KNOWL DISCOV D, V9, DOI 10.1145/2700398; Zhou K, 2013, ICML; Zhou K., 2013, ARTIF INTELL, P641	28	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303079
C	Xu, J; Hsu, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xu, Ji; Hsu, Daniel			On the number of variables to use in principal component regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHMS	We study least squares linear regression over N uncorrelated Gaussian features that are selected in order of decreasing variance. When the number of selected features p is at most the sample size n, the estimator under consideration coincides with the principal component regression estimator; when p > n, the estimator is the least l(2) norm solution over the selected features. We give an average-case analysis of the out-of-sample prediction error as p, n, N -> infinity with p/N -> alpha and n/N -> beta, for some constants alpha is an element of[0, 1] and beta is an element of(0, 1). In this average-case setting, the prediction error exhibits a "double descent" shape as a function of p. We also establish conditions under which the minimum risk is achieved in the interpolating (p > n) regime.	[Xu, Ji; Hsu, Daniel] Columbia Univ, New York, NY 10027 USA	Columbia University	Xu, J (corresponding author), Columbia Univ, New York, NY 10027 USA.	jixu@cs.columbia.edu; djhsu@cs.columbia.edu			NSF [CCF-1740833]; Sloan Research Fellowship; Google Faculty Award; Cheung-Kong Graduate School of Business Fellowship	NSF(National Science Foundation (NSF)); Sloan Research Fellowship(Alfred P. Sloan Foundation); Google Faculty Award(Google Incorporated); Cheung-Kong Graduate School of Business Fellowship	This research was supported by NSF CCF-1740833, a Sloan Research Fellowship, a Google Faculty Award, and a Cheung-Kong Graduate School of Business Fellowship.	[Anonymous], 2011, PRINCIPAL COMPONENT; Belkin M., 2018, ARXIV181211118; Belkin M., 2019, ARXIV190307571; BREIMAN L, 1983, J AM STAT ASSOC, V78, P131, DOI 10.2307/2287119; Dicker LH, 2016, BERNOULLI, V22, P1, DOI 10.3150/14-BEJ609; Dobriban E, 2018, ANN STAT, V46, P247, DOI 10.1214/17-AOS1549; Hastie Trevor, 2019, ARXIV190308560; Laurent B, 2000, ANN STAT, V28, P1302; Ledoit O, 2011, PROBAB THEORY REL, V151, P233, DOI 10.1007/s00440-010-0298-3; Lo Gerfo L, 2008, NEURAL COMPUT, V20, P1873, DOI 10.1162/neco.2008.05-07-517; Mathe P, 2004, SIAM J NUMER ANAL, V42, P968, DOI 10.1137/S0036142903420947; Muthukumar V., 2019, ARXIV190309139; Neal Brady, 2018, ARXIV181008591; Rad, 2019, ARXIV190201753; Rahimi A, 2007, NEURAL INFORM PROCES; Spigler S., 2018, ARXIV181009665; Tulino A. M., 2004, Foundations and Trends in Communications and Information Theory, V1, P1, DOI 10.1561/0100000001; Xu Ji, 2019, ARXIV190601139	20	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305013
C	Yan, Z; Guo, YW; Zhang, CS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yan, Ziang; Guo, Yiwen; Zhang, Changshui			Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Unlike the white-box counterparts that are widely studied and readily accessible, adversarial examples in black-box settings are generally more Herculean on account of the difficulty of estimating gradients. Many methods achieve the task by issuing numerous queries to target classification systems, which makes the whole procedure costly and suspicious to the systems. In this paper, we aim at reducing the query complexity of black-box attacks in this category. We propose to exploit gradients of a few reference models which arguably span some promising search subspaces. Experimental results show that, in comparison with the state-of-the-arts, our method can gain up to 2x and 4x reductions in the requisite mean and medium numbers of queries with much lower failure rates even if the reference models are trained on a small and inadequate dataset disjoint to the one for training the victim model. Code and models for reproducing our results are available at https://github.com/ZiangYan/subspace-attack.pytorch.	[Yan, Ziang; Zhang, Changshui] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, State Key Lab Intelligent Technol & Syst, Inst Artificial Intelligence,THUAI,Dept Automat, Beijing, Peoples R China; [Guo, Yiwen] Bytedance AI Lab, Beijing, Peoples R China; [Yan, Ziang; Guo, Yiwen] Intel Labs China, Beijing, Peoples R China	Tsinghua University; Intel Corporation	Yan, Z (corresponding author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, State Key Lab Intelligent Technol & Syst, Inst Artificial Intelligence,THUAI,Dept Automat, Beijing, Peoples R China.; Yan, Z (corresponding author), Intel Labs China, Beijing, Peoples R China.	yza18@mails.tsinghua.edu.cn; guoyiwen.ai@bytedance.com; zcs@mail.tsinghua.edu.cn			NSFC [61876095]; Beijing Academy of Artificial Intelligence (BAAI)	NSFC(National Natural Science Foundation of China (NSFC)); Beijing Academy of Artificial Intelligence (BAAI)	This work is funded by NSFC (Grant No. 61876095) and Beijing Academy of Artificial Intelligence (BAAI).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bhagoji AN, 2018, LECT NOTES COMPUT SC, V11216, P158, DOI 10.1007/978-3-030-01258-8_10; Brendel W., 2018, PROC 6 INT C LEARN R; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chen P.-Y., 2017, P AISEC CCS 2017, P15, DOI DOI 10.1145/3128572.3140448; Cheng M., 2019, P ICLR; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020; Dong XY, 2019, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2019.00186; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Guo C, 2019, PR MACH LEARN RES, V97; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; Ilyas A, 2018, PR MACH LEARN RES, V80; Ilyas Andrew, 2019, ICLR; Jegelka S., 2018, ICLR; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kurakin A, 2018, ICLR, P99, DOI DOI 10.1201/9781351251389-8; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Liu C, 2018, LECT NOTES COMPUT SC, V11210, P203, DOI 10.1007/978-3-030-01231-1_13; Liu Yanpei, 2017, ICLR; Madry Aleksander, 2017, ARXIV; Mao HZ, 2017, IEEE COMPUT SOC CONF, P1927, DOI 10.1109/CVPRW.2017.241; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Narodytska N, 2017, IEEE COMPUT SOC CONF, P1310, DOI 10.1109/CVPRW.2017.172; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Recht B, 2018, ARXIV180600451; Recht B, 2019, PR MACH LEARN RES, V97; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T., 2017, ARXIV170303864; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sun G., 2018, P IEEE CVF C COMP VI, P7132, DOI DOI 10.1109/CVPR.2018.00745; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tu Chun-Chen, 2019, AAAI; Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3; Yamada Y., 2018, ABS180202375 CORR; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87	41	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303077
C	Yasodharan, S; Loiseau, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yasodharan, Sarath; Loiseau, Patrick			Nonzero-sum Adversarial Hypothesis Testing Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SECURITY	We study nonzero-sum hypothesis testing games that arise in the context of adversarial classification, in both the Bayesian as well as the Neyman-Pearson frameworks. We first show that these games admit mixed strategy Nash equilibria, and then we examine some interesting concentration phenomena of these equilibria. Our main results are on the exponential rates of convergence of classification errors at equilibrium, which are analogous to the well-known Chernoff-Stein lemma and Chernoff information that describe the error exponents in the classical binary hypothesis testing problem, but with parameters derived from the adversarial model. The results are validated through numerical experiments.	[Yasodharan, Sarath] Indian Inst Sci, Dept Elect Commun Engn, Bangalore 560012, Karnataka, India; [Loiseau, Patrick] Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,LIG, 700 Ave Cent,Domaine Univ, F-38400 St Martin Dheres, France; [Loiseau, Patrick] MPI SWS, 700 Ave Cent,Domaine Univ, F-38400 St Martin Dheres, France	Indian Institute of Science (IISC) - Bangalore; Inria; UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS); Communaute Universite Grenoble Alpes; UDICE-French Research Universities; Universite Grenoble Alpes (UGA)	Yasodharan, S (corresponding author), Indian Inst Sci, Dept Elect Commun Engn, Bangalore 560012, Karnataka, India.	sarath@iisc.ac.in; patrick.loiseau@inria.fr			French National Research Agency (ANR) through the "Investissements d'avenir" program [ANR-15-IDEX-02, ANR-16-TERC0012]; Alexander von Humboldt Foundation	French National Research Agency (ANR) through the "Investissements d'avenir" program(French National Research Agency (ANR)); Alexander von Humboldt Foundation(Alexander von Humboldt Foundation)	The first author is partially supported by the Cisco-IISc Research Fellowship grant. The work of the second author was supported in part by the French National Research Agency (ANR) through the "Investissements d'avenir" program (ANR-15-IDEX-02) and through grant ANR-16-TERC0012; and by the Alexander von Humboldt Foundation.	Bao N, 2011, P 2 INT C GAM THEOR, P265; Barni M, 2014, IEEE T INFORM THEORY, V60, P4848, DOI 10.1109/TIT.2014.2325571; Barni M, 2013, IEEE T INF FOREN SEC, V8, P450, DOI 10.1109/TIFS.2012.2237397; Barreno M, 2010, MACH LEARN, V81, P121, DOI 10.1007/s10994-010-5188-5; Brandao F. G., 2014, P 5 C INN THEOR COMP, P183; Bruckner M., 2011, P 17 ACM SIGKDD INT, P547, DOI DOI 10.1145/2020408.2020495; Bruckner M, 2012, J MACH LEARN RES, V13, P2617; Chen L, 2009, IEEE T INF FOREN SEC, V4, P165, DOI 10.1109/TIFS.2009.2019154; Dalvi N, 2004, 10 ACM SIGKDD INT C, DOI DOI 10.1145/1014052.1014066; Dembo A., 2010, LARGE DEVIATIONS TEC; Dong JS, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P55, DOI 10.1145/3219166.3219193; Dritsoula L, 2017, IEEE T INF FOREN SEC, V12, P3094, DOI 10.1109/TIFS.2017.2718494; Hardt M, 2016, ITCS'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INNOVATIONS IN THEORETICAL COMPUTER SCIENCE, P111, DOI 10.1145/2840728.2840730; Huang L., 2011, PROC AISEC, P43, DOI DOI 10.1145/2046684.2046692; INGSTER YI, 2003, LECT NOTES STAT, V169; Kantarcioglu M, 2011, DATA MIN KNOWL DISC, V22, P291, DOI 10.1007/s10618-010-0197-3; Korzhyk D, 2011, J ARTIF INTELL RES, V41, P297, DOI 10.1613/jair.3269; Li B, 2015, JMLR WORKSH CONF PRO, V38, P599; Lisy Viliam, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P240, DOI 10.1007/978-3-662-44851-9_16; Lowd D., 2005, P 11 ACM SIGKDD INT, P641, DOI DOI 10.1145/1081870.1081950; Lye K.-W., 2005, INT J INF SECUR, V4, P71; Papernot N., 2018, P 3 IEEE EUR S SEC P; Poor, 2013, INTRO SIGNAL DETECTI; Reny P. J., 2005, NEW PALGRAVE DICT EC; Soper B, 2015, ANN ALLERTON CONF, P361, DOI 10.1109/ALLERTON.2015.7447027; Tondi B, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21010023; Yasodharan S., 2019, NONZERO SUM ADVERSAR; Zhou Y., 2012, KDD 12, DOI [10.1145/2339530.2339697, DOI 10.1145/2339530.2339697]; Zhou Y., 2014, P SIAM INT C DAT MIN, P929, DOI DOI 10.1137/1.9781611973440	29	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307034
C	Yu, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yu, Hao			A Communication Efficient Stochastic Multi-Block Alternating Direction Method of Multipliers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CONVERGENCE RATE; ALGORITHM	The alternating direction method of multipliers (ADMM) has recently received tremendous interests for distributed large scale optimization in machine learning, statistics, multi-agent networks and related applications. In this paper, we propose a new parallel multi-block stochastic ADMM for distributed stochastic optimization, where each node is only required to perform simple stochastic gradient descent updates. The proposed ADMM is fully parallel, can solve problems with arbitrary block structures, and has a convergence rate comparable to or better than existing state-of-the-art ADMM methods for stochastic optimization. Existing stochastic (or deterministic) ADMMs require each node to exchange its updated primal variables across nodes at each iteration and hence cause significant amounts of communication overhead. Existing ADMMs require roughly the same number of inter-node communication rounds as the number of in-node computation rounds. In contrast, the number of communication rounds required by our new ADMM is only the square root of the number of computation rounds.	[Yu, Hao] Amazon, Seattle, WA 98109 USA	Amazon.com	Yu, H (corresponding author), Amazon, Seattle, WA 98109 USA.	eeyuhao@gmail.com						Azadi S, 2014, PR MACH LEARN RES, V32; Bertsekas D., 2003, CONVEX ANAL OPTIMIZA; Bertsekas D.P., NONLINEAR PROGRAMMIN; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Chang TH, 2016, IEEE T SIGNAL PROCES, V64, P3118, DOI 10.1109/TSP.2016.2537271; Chen CH, 2016, MATH PROGRAM, V155, P57, DOI 10.1007/s10107-014-0826-5; Deng W, 2017, J SCI COMPUT, V71, P712, DOI 10.1007/s10915-016-0318-2; Deng W, 2016, J SCI COMPUT, V66, P889, DOI 10.1007/s10915-015-0048-x; Diamond S, 2016, J MACH LEARN RES, V17; ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204; Fang B, 2014, 2014 9TH INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND NETWORKING IN CHINA (CHINACOM), P462, DOI 10.1109/CHINACOM.2014.7054339; Gao Xiang, 2016, ARXIV160505969; He BS, 2016, J SCI COMPUT, V66, P1204, DOI 10.1007/s10915-015-0060-1; He BS, 2012, SIAM J NUMER ANAL, V50, P700, DOI 10.1137/110836936; Jaggi Martin, 2014, ADV NEURAL INFORM PR; Konecn`y J., 2016, ARXIV161002527; Lacoste-Julien S., 2002, ARXIV12122002; Lan G., 2017, ARXIV170103961; Li H., 2017, ARXIV171110802; Nedic A, 2018, P IEEE, V106, P953, DOI 10.1109/JPROC.2018.2817461; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Ouyang Hua, 2013, INT C MACH LEARN ICM; Pu S., 2018, ARXIV180511454; Scaman K., 2018, ADV NEURAL INFORM PR; Smith V., 2016, J MACH LEARN RES, V18, P1; Suzuki Taiji, 2013, INT C MACH LEARN ICM; Uribe C. A., 2017, ARXIV171200232; Wang H., 2014, ADV NEURAL INFORM PR; Wei E, 2012, IEEE DECIS CONTR P, P5445, DOI 10.1109/CDC.2012.6425904; Yang T., 2013, ADV NEURAL INFORM PR, P629; Yu H, 2017, SIAM J OPTIMIZ, V27, P759, DOI 10.1137/16M1059011	31	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900024
C	Zanette, A; Lazaric, A; Kochenderfer, MJ; Brunskill, E		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zanette, Andrea; Lazaric, Alessandro; Kochenderfer, Mykel J.; Brunskill, Emma			Limiting Extrapolation in Linear Approximate Value Iteration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GAME; GO	We study linear approximate value iteration (LAVI) with a generative model. While linear models may accurately represent the optimal value function using a few parameters, several empirical and theoretical studies show the combination of least-squares projection with the Bellman operator may be expansive, thus leading LAVI to amplify errors over iterations and eventually diverge. We introduce an algorithm that approximates value functions by combining Q-values estimated at a set of anchor states. Our algorithm tries to balance the generalization and compactness of linear methods with the small amplification of errors typical of interpolation methods. We prove that if the features at any state can be represented as a convex combination of features at the anchor points, then errors are propagated linearly over iterations (instead of exponentially) and our method achieves a polynomial sample complexity bound in the horizon and the number of anchor points. These findings are confirmed in preliminary simulations in a number of simple problems where a traditional least-square LAVI method diverges.	[Zanette, Andrea] Stanford Univ, Inst Computat & Math Engn, Stanford, CA 94305 USA; [Lazaric, Alessandro] Facebook AI Res, New York, NY USA; [Kochenderfer, Mykel J.] Stanford Univ, Dept Aeronaut & Astronaut, Stanford, CA 94305 USA; [Brunskill, Emma] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University; Facebook Inc; Stanford University; Stanford University	Zanette, A (corresponding author), Stanford Univ, Inst Computat & Math Engn, Stanford, CA 94305 USA.	zanette@stanford.edu; lazaric@fb.com; mykel@stanford.edu; ebrun@cs.stanford.edu			Total Innovation Fellowship	Total Innovation Fellowship	This was was partially supported by a Total Innovation Fellowship. The authors are grateful to the reviewers for the high quality reviews and helpful suggestions.	Amin M., 2013, AM J COMPUTATIONAL M, V3, P11, DOI [10.4236/ajcm.2013.31A003, DOI 10.4236/AJCM.2013.31A003]; [Anonymous], 2018, REINFORCEMENT LEARNI, DOI 10.1016/S1364-6613(99)01331-5; [Anonymous], 1995, INT C MACH LEARN ICM; Antos A, 2008, MACH LEARN, V71, P89, DOI 10.1007/s10994-007-5038-2; Bellemare Marc G., 2019, ABS190111530 CORR; Blum Avrim, 2017, ARXIV171204564CSCG; Boyd S, 2004, CONVEX OPTIMIZATION; Chen Jinglin, 2019, ARXIV190500360; Farahmand Amir-massoud, 2010, ADV NEURAL INFORM PR; Gordon Geoffrey J, 1996, ADV NEURAL INFORM PR; Graham Robert, 2017, ARXIV170301350CSCG; Hoeffding W., 1963, J AM STAT ASS; Jiang N., 2018, C LEARN THEOR, P3395; Kakade Sham M., 2003, INT C MACH LEARN ICM; Kumaraswamy Raksha, 2018, ADV NEURAL INFORM PR; Mahadevan S, 2007, J MACH LEARN RES, V8, P2169; Mnih Volodymyr, 2013, ADV NEURAL INFORM PR; Munos R., 2005, AAAI C ART INT AAAI; Munos R, 2008, J MACH LEARN RES, V9, P815; Munos Remi, 1999, ADV NEURAL INFORM PR; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Ormoneit D, 2002, MACH LEARN, V49, P161, DOI 10.1023/A:1017928328829; Osband Ian, 2016, INT C MACH LEARN ICM; Pazis Jason, 2013, P ANN AAAI C ARTIFIC, P774; Petrik M, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2574; Sartipizadeh Hossein, 2016, ARXIV160304422CSCG; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Soare M., 2014, ARXIV14096110; Tsitsiklis JN, 1996, MACH LEARN, V22, P59, DOI 10.1007/BF00114724; Tsitsiklis John N, 1997, ADV NEURAL INFORM PR; Yang Lin F, 2019, ARXIV190204779; Yang Zhuoran, 2019, CORR	36	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305059
C	Zanette, A; Kochenderfer, MJ; Brunskill, E		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zanette, Andrea; Kochenderfer, Mykel J.; Brunskill, Emma			Almost Horizon-Free Structure-Aware Best Policy Identification with a Generative Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PAC BOUNDS	This paper focuses on the problem of computing an o-optimal policy in a discounted Markov Decision Process (MDP) provided that we can access the reward and transition function through a generative model. We propose an algorithm that is initially agnostic to the MDP but that can leverage the specific MDP structure, expressed in terms of variances of the rewards and next-state value function, and gaps in the optimal action-value function to reduce the sample complexity needed to find a good policy, precisely highlighting the contribution of each state-action pair to the final sample complexity. A key feature of our analysis is that it removes all horizon dependencies in the sample complexity of suboptimal actions except for the intrinsic scaling of the value function and a constant additive term.	[Zanette, Andrea] Stanford Univ, Inst Computat & Math Engn, Stanford, CA 94305 USA; [Kochenderfer, Mykel J.] Stanford Univ, Dept Aeronaut & Astronaut, Stanford, CA 94305 USA; [Brunskill, Emma] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University; Stanford University; Stanford University	Zanette, A (corresponding author), Stanford Univ, Inst Computat & Math Engn, Stanford, CA 94305 USA.	zanette@stanford.edu; mykel@stanford.edu; ebrun@cs.stanford.edu			Total Innovation Fellowship program, an NSF CAREER award; Office of Naval Research Young Investigator Award	Total Innovation Fellowship program, an NSF CAREER award; Office of Naval Research Young Investigator Award(Office of Naval Research)	This work is partially supported by a Total Innovation Fellowship program, an NSF CAREER award and an Office of Naval Research Young Investigator Award. The authors are grateful to the reviewers for the high-quality reviews and suggestions.	Agarwal A., 2019, ARXIV190800261; [Anonymous], 2018, REINFORCEMENT LEARNI, DOI 10.1016/S1364-6613(99)01331-5; Audibert Jean-Yves, 2010, C LEARN THEOR COLT; AUDIBERT JY, 2009, THEORETICAL COMPUTER; Auer P., 2002, MACHINE LEARNING; Azar M., 2012, INT C MACH LEARN ICM; Azar Mohammad Gheshlaghi, 2017, INT C MACH LEARN ICM; Boyd S, 2004, CONVEX OPTIMIZATION; Brunskill Emma, 2010, INT C AUT PLANN SCHE, P218; Bubeck Sebastien, 2009, INT C ALG LEARN THEO; Burnetas AN, 1997, MATH OPER RES, V22, P222, DOI 10.1287/moor.22.1.222; Dann Christoph, 2017, ADV NEURAL INFORM PR; Dann Christoph, 2015, ADV NEURAL INFORM PR; Even-Dar Eyal, 2006, J MACHINE LEARNING R; Farahmand Amir-massoud, 2010, ADV NEURAL INFORM PR; Gabillon V., 2012, ADV NEURAL INFORM PR, P3212; Jaksch Thomas, 2010, J MACHINE LEARNING R; Jamieson K., 2014, C LEARN THEOR, P423; Kakade Sham M., 2003, THESIS; Karnin Z., 2013, ICML; Kearns M, 2002, MACH LEARN, V49, P193, DOI 10.1023/A:1017932429737; Lattimore T, 2014, THEOR COMPUT SCI, V558, P125, DOI 10.1016/j.tcs.2014.09.029; Maillard O.-A., 2014, ADV NEURAL INFORM PR; Maron O., 1994, ADV NEURAL INFORM PR, V6, P59; Maurer Andreas, 2009, C LEARN THEOR COLT; Mnih V., 2008, P 25 INT C MACH LEAR; Ok J., 2018, ADV NEURAL INFORM PR, P8874; Osband Ian, 2013, ADV NEURAL INFORM PR; Sidford Aaron, 2018, ADV NEURAL INFORM PR; Simchowitz Max, 2019, ARXIV190503814; Strehl AL, 2009, J MACH LEARN RES, V10, P2413; TEWARI A, 2008, ADV NEURAL INFORM PR, P1505; Wang T, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON APPROXIMATE DYNAMIC PROGRAMMING AND REINFORCEMENT LEARNING, P44; Weissman Tsachy, 2003, TECH REP; Zanette Andrea, 2019, INT C MACH LEARN ICM	36	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305060
C	Zhang, JZ; Bareinboim, E		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Junzhe; Bareinboim, Elias			Near-Optimal Reinforcement Learning in Dynamic Treatment Regimes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				2-STAGE RANDOMIZATION DESIGNS; SURVIVAL DISTRIBUTIONS; TREATMENT POLICIES; STRATEGIES; INFERENCE; BOUNDS	A dynamic treatment regime (DTR) consists of a sequence of decision rules, one per stage of intervention, that dictates how to determine the treatment assignment to patients based on evolving treatments and covariates' history. These regimes are particularly effective for managing chronic disorders and is arguably one of the key aspects towards more personalized decision-making. In this paper, we investigate the online reinforcement learning (RL) problem for selecting optimal DTRs provided that observational data is available. We develop the first adaptive algorithm that achieves near-optimal regret in DTRs in online settings, without any access to historical data. We further derive informative bounds on the system dynamics of the underlying DTR from confounded, observational data. Finally, we combine these results and develop a novel RL algorithm that efficiently learns the optimal DTR while leveraging the abundant, yet imperfect confounded observations.	[Zhang, Junzhe; Bareinboim, Elias] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA	Columbia University	Zhang, JZ (corresponding author), Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.	junzhez@cs.columbia.edu; eb@cs.columbia.edu	Zhang, Junzhe/GON-5782-2022	Zhang, Junzhe/0000-0002-1309-0151	IBM Research; Adobe Research; NSF [IIS-1704352, IIS-1750807]	IBM Research(International Business Machines (IBM)); Adobe Research; NSF(National Science Foundation (NSF))	This research is supported in parts by grants from IBM Research, Adobe Research, NSF IIS-1704352, and IIS-1750807 (CAREER).	Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Azizzadenesheli  K., 2016, COLT; Balke A., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P11; Bareinboim E, 2016, P NATL ACAD SCI USA, V113, P7345, DOI 10.1073/pnas.1510507113; BELLMAN R, 1966, SCIENCE, V153, P34, DOI 10.1126/science.153.3731.34; Chakraborty B, 2013, STAT METHODS DYNAMIC; Chakraborty B, 2014, ANNU REV STAT APPL, V1, P447, DOI 10.1146/annurev-statistics-022513-115553; Chakraborty B, 2011, AM J PUBLIC HEALTH, V101, P40, DOI 10.2105/AJPH.2010.198937; Frangakis CE, 2002, BIOMETRICS, V58, P21, DOI 10.1111/j.0006-341X.2002.00021.x; Guo ZD, 2016, JMLR WORKSH CONF PRO, V51, P510; Ihler A, 2012, P 28 C UNC ART INT U, P523; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Lavori PW, 2008, ANNU REV MED, V59, P443, DOI 10.1146/annurev.med.59.062606.122232; Lavori PW, 2000, J ROY STAT SOC A STA, V163, P29, DOI 10.1111/1467-985X.00154; Lee S., 2019, P 35 C UNC ART INT U; Lunceford JK, 2002, BIOMETRICS, V58, P48, DOI 10.1111/j.0006-341X.2002.00048.x; MANSKI CF, 1990, AM ECON REV, V80, P319; Murphy SA, 2005, J MACH LEARN RES, V6, P1073; Murphy SA, 2005, STAT MED, V24, P1455, DOI 10.1002/sim.2022; Murphy SA, 2003, J R STAT SOC B, V65, P331, DOI 10.1111/1467-9868.00389; Murphy SA, 2001, J AM STAT ASSOC, V96, P1410, DOI 10.1198/016214501753382327; Osband I., 2014, ADV NEURAL INFORM PR, V27, P604; Pearl J., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P444; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Robins J, 2008, STAT MED, V27, P4678, DOI 10.1002/sim.3301; ROSENBAUM PR, 1983, BIOMETRIKA, V70, P41, DOI 10.1093/biomet/70.1.41; RUBIN DB, 1978, ANN STAT, V6, P34, DOI 10.1214/aos/1176344064; Spirtes P., 2000, CAUSATION PREDICTION; STONE RM, 1995, NEW ENGL J MED, V332, P1671, DOI 10.1056/NEJM199506223322503; Strehl A.L., 2006, ICML, P881, DOI [10.1145/1143844.1143955, DOI 10.1145/1143844.1143955]; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Szita Istvan, 2010, INT C MACH LEARN ICM, P1031; Thall PF, 2000, STAT MED, V19, P1011, DOI 10.1002/(SICI)1097-0258(20000430)19:8<1011::AID-SIM414>3.0.CO;2-M; Thall PF, 2002, J AM STAT ASSOC, V97, P29, DOI 10.1198/016214502753479202; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Wagner EH, 2001, HEALTH AFFAIR, V20, P64, DOI 10.1377/hlthaff.20.6.64; Wahed AS, 2006, BIOMETRIKA, V93, P163, DOI 10.1093/biomet/93.1.163; Wahed AS, 2004, BIOMETRICS, V60, P124, DOI 10.1111/j.0006-341X.2004.00160.x; Zhang J., 2019, R48 COL U CAUS AI LA; Zhang JZ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1340	40	2	2	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905011
C	Zheng, H; Fang, FM; Zhang, GX		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zheng, Hao; Fang, Faming; Zhang, Guixu			Cascaded Dilated Dense Network with Two-step Data Consistency for MRI Reconstruction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Compressed Sensing MRI (CS-MRI) aims at reconstrcuting de-aliased images from sub-Nyquist sampling k-space data to accelerate MR Imaging. Inspired by recent deep learning methods, we propose a Cascaded Dilated Dense Network (CDDN) for MRI reconstruction. Dense blocks with residual connection are used to restore clear images step by step and dilated convolution is introduced for expanding receptive field without taking more network parameters. After each sub-network, we use a novel Two-step Data Consistency (TDC) operation in k-space. We convert the complex result from first DC operation to real-valued images and applied another replacement with sampled k-space data. Extensive experiments demonstrate that the proposed CDDN with TDC achieves state-of-art result.	[Fang, Faming] East China Normal Univ, Shanghai Key Lab Multidimens Informat Proc, Shanghai, Peoples R China; East China Normal Univ, Sch Comp Sci & Technol, Shanghai, Peoples R China	East China Normal University; East China Normal University	Fang, FM (corresponding author), East China Normal Univ, Shanghai Key Lab Multidimens Informat Proc, Shanghai, Peoples R China.	wsnbzh@hotmail.com; fmfang@cs.ecnu.edu.cn; gxzhang@cs.ecnu.edu.cn			Key Project of the National Natural Science Foundation of China [61731009]; National Natural Science Foundation of China [61871185]; "Chenguang Program" - Shanghai Education Development Foundation; Shanghai Municipal Education Commission [17CG25]	Key Project of the National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); "Chenguang Program" - Shanghai Education Development Foundation; Shanghai Municipal Education Commission(Shanghai Municipal Education Commission (SHMEC))	This work is sponsored in part by the Key Project of the National Natural Science Foundation of China under Grant 61731009, and in part by the National Natural Science Foundation of China under Grant 61871185, and in part by "Chenguang Program" supported by Shanghai Education Development Foundation and Shanghai Municipal Education Commission under Grant 17CG25.	Andreopoulos A, 2008, MED IMAGE ANAL, V12, P335, DOI 10.1016/j.media.2007.12.003; [Anonymous], 2018, ARXIV181108839; Candes E. J., 2004, MATH0409186 ARXIV; Dong WS, 2014, IEEE T IMAGE PROCESS, V23, P3618, DOI 10.1109/TIP.2014.2329449; Glorot X., 2011, P 14 INT C ART INT S, P315; Hot E, 2015, 2015 4TH MEDITERRANEAN CONFERENCE ON EMBEDDED COMPUTING (MECO), P323, DOI 10.1109/MECO.2015.7181934; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kingma D.P, P 3 INT C LEARNING R; Lai ZY, 2016, MED IMAGE ANAL, V27, P93, DOI 10.1016/j.media.2015.05.012; Lauterbur P. C, 1974, NATURE, V246, P469; Li XM, 2018, IEEE T MED IMAGING, V37, P2663, DOI 10.1109/TMI.2018.2845918; Lingala SG, 2013, IEEE T MED IMAGING, V32, P1132, DOI 10.1109/TMI.2013.2255133; Moeskops P, 2017, LECT NOTES COMPUT SC, V10553, P56, DOI 10.1007/978-3-319-67558-9_7; Nyquist H, 2002, P IEEE, V90, P280, DOI 10.1109/5.989875; Perone CS, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-24304-3; Qu XB, 2012, MAGN RESON IMAGING, V30, P964, DOI 10.1016/j.mri.2012.02.019; Quan Tran Minh, 2017, IEEE T MED IMAGING; Ravishankar S, 2011, IEEE T MED IMAGING, V30, P1028, DOI 10.1109/TMI.2010.2090538; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Schlemper J, 2017, LECT NOTES COMPUT SC, V10265, P647, DOI 10.1007/978-3-319-59050-9_51; Sun LY, 2018, AAAI CONF ARTIF INTE, P2444; Tong T., 2017, P IEEE INT C COMP VI, P4799; Wang SS, 2016, I S BIOMED IMAGING, P514, DOI 10.1109/ISBI.2016.7493320; Wang YH, 2014, IEEE ENG MED BIO, P1533, DOI 10.1109/EMBC.2014.6943894; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50; Yan RM, 2016, IEEE T IMAGE PROCESS, V25, P1910, DOI 10.1109/TIP.2016.2535273; Yang G, 2018, IEEE T MED IMAGING, V37, P1310, DOI 10.1109/TMI.2017.2785879; Yang Yan, 2018, IEEE T PATTERN ANAL; Yu F., 2016, ABS151107122 CORR; Zhan ZF, 2016, IEEE T BIO-MED ENG, V63, P1850, DOI 10.1109/TBME.2015.2503756; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zhang Q, 2017, LECT NOTES COMPUT SC, V10635, P364, DOI 10.1007/978-3-319-70096-0_38	35	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301070
C	Zhong, K; Song, Z; Jain, P; Dhillon, IS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhong, Kai; Song, Zhao; Jain, Prateek; Dhillon, Inderjit S.			Provable Non-linear Inductive Matrix Completion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Consider a standard recommendation/retrieval problem where given a query, the goal is to retrieve the most relevant items. Inductive matrix completion (IMC) method is a standard approach for this problem where the given query as well as the items are embedded in a common low-dimensional space. The inner product between a query embedding and an item embedding reflects relevance of the (query, item) pair. Non-linear IMC (NIMC) uses non-linear networks to embed the query as well as items, and is known to be highly effective for a variety of tasks, such as video recommendations for users, semantic web search, etc. Despite its wide usage, existing literature lacks rigorous understanding of NIMC models. A key challenge in analyzing such models is to deal with the non-convexity arising out of non-linear embeddings in addition to the non-convexity arising out of the low-dimensional restriction of the embedding space, which is akin to the low-rank restriction in the standard matrix completion problem. In this paper, we provide the first theoretical analysis for a simple NIMC model in the realizable setting, where the relevance score of a (query, item) pair is formulated as the inner product between their single-layer neural representations. Our results show that under mild assumptions we can recover the ground truth parameters of the NIMC model using standard (stochastic) gradient descent methods if the methods are initialized within a small distance to the optimal parameters. We show that a standard tensor method can be used to initialize the solution within the required distance to the optimal parameters. Furthermore, we show that the number of query-item relevance observations required, a key parameter in learning such models, scales nearly linearly with the input dimensionality thus matching existing results for the standard linear inductive matrix completion.	[Zhong, Kai; Dhillon, Inderjit S.] Amazon, Seattle, WA 98109 USA; [Song, Zhao] Univ Washington, Seattle, WA 98195 USA; [Jain, Prateek] Microsoft, Redmond, WA USA; [Dhillon, Inderjit S.] Univ Texas Austin, Austin, TX 78712 USA	Amazon.com; University of Washington; University of Washington Seattle; Microsoft; University of Texas System; University of Texas Austin	Zhong, K (corresponding author), Amazon, Seattle, WA 98109 USA.	kaizhong@amazon.com; magic.linuxkde@gmail.com; prajain@microsoft.com; isd@amazon.com						Abernethy J, 2006, CS0611124 ARXIV; [Anonymous], 2018, ICML; Brutzkus A., 2018, ICLR; Brutzkus Alon, 2017, ICML; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chiang K Y, 2015, ADV NEURAL INFORM PR, P3447; Covington P, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P191, DOI 10.1145/2959100.2959190; Du S. S., 2018, ADV NEURAL INFORM PR, P373; Du Simon S, 2018, ICLR; Du Simon S, 2018, ICML; GE R., 2018, P 6 INT C LEARN REPR; Ge R, 2017, PR MACH LEARN RES, V70; Goel Surbhi, 2018, ICML; Goel Surbhi, 2017, 30 ANN C LEARN THEOR; Gomez-Uribe CA, 2016, ACM TRANS MANAG INF, V6, DOI 10.1145/2843948; GroupLens, 1997, MOV LENS DAT; Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75; HSU D, 2012, ELECTRON COMMUN PROB, V17, P1; Huang PS, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P2333; Jain P., 2013, ABS13060626 CORR; Janzamin Majid, 2015, 150608473 ARXIV; Lei CY, 2016, PROC CVPR IEEE, P2545, DOI 10.1109/CVPR.2016.279; Li Y., 2017, ADV NEURAL INFORM PR, P597; Lin Ming, 2016, ADV NEURAL INFORM PR, P1633; Natarajan N, 2014, BIOINFORMATICS, V30, P60, DOI 10.1093/bioinformatics/btu269; Nigam Priyanka, 2019, P 25 ACM SIGKDD INT; Razenshteyn I, 2016, ACM S THEORY COMPUT, P250, DOI 10.1145/2897518.2897639; Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127; Shin D., 2015, P 24 ACM INT C INF K, P203; Si S, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1165, DOI 10.1145/2939672.2939809; Song Z., 2017, P 49 ANN S THEOR COM; Song Z., 2017, ARXIV170408246; Song Zhao, 2018, ZERO ONE LAW ENTRY W; Tian Yuandong, 2017, ICML; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; Vempala S., 2018, ARXIV180502677; Wang XX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P627, DOI 10.1145/2647868.2654940; Xu Miao, 2013, ADV NEURAL INFORM PR, P2301, DOI DOI 10.5555/2999792.2999869; Yu H.-F., 2014, INT C MACH LEARN, P593; Zhang Xiao, 2018, ARXIV180607808; Zhong K., 2017, ARXIV171103440; Zhong Kai, 2017, ICML; Zhong KD, 2015, LECT N EDUC TECHNOL, P9, DOI 10.1007/978-3-662-44188-6_2	44	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903011
C	Zhou, YC; Qi, HZ; Huang, JW; Ma, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhou, Yichao; Qi, Haozhi; Huang, Jingwei; Ma, Yi			NeurVPS: Neural Vanishing Point Scanning via Conic Convolution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present a simple yet effective end-to-end trainable deep network with geometry-inspired convolutional operators for detecting vanishing points in images. Traditional convolutional neural networks rely on aggregating edge features and do not have mechanisms to directly exploit the geometric properties of vanishing points as the intersections of parallel lines. In this work, we identify a canonical conic space in which the neural network can effectively compute the global geometric information of vanishing points locally, and we propose a novel operator named conic convolution that can be implemented as regular convolutions in this space. This new operator explicitly enforces feature extractions and aggregations along the structural lines and yet has the same number of parameters as the regular 2D convolution. Our extensive experiments on both synthetic and real-world datasets show that the proposed operator significantly improves the performance of vanishing point detection over traditional methods. The code and dataset have been made publicly available at https://github.com/zhou13/neurvps.	[Zhou, Yichao; Qi, Haozhi; Ma, Yi] Univ Calif Berkeley, Berkeley, CA 94701 USA; [Huang, Jingwei] Stanford Univ, Stanford, CA 94305 USA	University of California System; University of California Berkeley; Stanford University	Zhou, YC (corresponding author), Univ Calif Berkeley, Berkeley, CA 94701 USA.	zyc@berkeley.edu; hqi@berkeley.edu; jingweih@stanford.edu; yima@eecs.berkeley.edu	Qi, Haozhi/ABD-9753-2021		Berkeley EECS Startup fund; Berkeley FHL Vive Center for Enhanced Reality; Sony Research; Bytedance Research Lab (Silicon Valley)	Berkeley EECS Startup fund; Berkeley FHL Vive Center for Enhanced Reality; Sony Research; Bytedance Research Lab (Silicon Valley)	This work is partially supported by the funding from Berkeley EECS Startup fund, Berkeley FHL Vive Center for Enhanced Reality, research grants from Sony Research, and Bytedance Research Lab (Silicon Valley).	[Anonymous], 2015, P 28 INT C NEUR INF; Antunes Michel, 2013, CVPR; Barinova O., 2010, ECCV; Barnard S. T., 1983, ARTIF INTELL; Bazin Jean-Charles, 2012, CVPR; Bolles Robert C, 1981, IJCAI; Borji A., 2016, VANISHING POINT DETE; Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; Canny John, 1987, COMPUTATIONAL APPROA, P184; Chang Chin-Kai, 2018, ICRA; Cipolla Roberto, 1999, BMVC; Cohen Taco S, 2018, ICLR 2018; Coughlan JM, 1999, ICCV; Dai A., 2017, CVPR; Dai J., 2017, ICCV; Denis P., 2008, ECCV; Feng Chen, 2010, SEMIAUTOMATIC 3D REC; Gonzalez Alvaro, 2010, MATH GEOSCIENCES; Guillou Erwan, 2000, VISUAL COMPUTER; Hoiem D., 2008, IJCV; Hough P., 1959, INT C HIGH EN ACC IN; Huang Jingwei, 2019, CVPR; Jeon Yunho, 2017, CVPR; Jiang Chiyu, 2019, ICLR 2019; Kingma D. P., 2015, 3 INT C LEARN REPR I, P1; Kluger Florian, 2017, GCPR; Kosecka J., 2002, ECCV; Lee Seokju, 2017, ICCV; Lezama J., 2014, CVPR; Magee Michael J, 1984, COMPUTER VISION GRAP; Masci J., 2015, ICCV WORKSH; McLean GF, 1995, PAMI; Mirzaei Faraz, 2011, ICCV; Newell A., 2016, ECCV; OBrien James F, 2012, TOG; QUAN L, 1989, PATTERN RECOGN LETT, V9, P279, DOI 10.1016/0167-8655(89)90006-8; Schindler Grant, 2004, CVPR; Shelhamer Evan, 2019, BLURRING LINE STRUCT; Sifre L., 2013, CVPR; Straforini Marco, 1993, IMAGE VISION COMPUTI; Tardif J.-P., 2009, ICCV; Von Gioi Rafael Grompone, 2008, PAMI; Wildenauer H., 2012, CVPR; Workman Scott, 2016, BMVC; Xu C, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/4160652; Zhai M., 2016, CVPR; Zhang W., 2002, ICRA; Zhou Yichao, 2019, ICCV; Zhou Zihan, 2017, IEEE T MULTIMEDIA	49	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300078
C	Zimmert, J; Lattimore, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zimmert, Julian; Lattimore, Tor			Connections Between Mirror Descent, Thompson Sampling and the Information Ratio	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				REGRET	The information-theoretic analysis by Russo and Van Roy [25] in combination with minimax duality has proved a powerful tool for the analysis of online learning algorithms in full and partial information settings. In most applications there is a tantalising similarity to the classical analysis based on mirror descent. We make a formal connection, showing that the information-theoretic bounds in most applications can be derived from existing techniques for online convex optimisation. Besides this, for k-armed adversarial bandits we provide an efficient algorithm with regret that matches the best information-theoretic upper bound and improve best known regret guarantees for online linear optimisation on l(p)-balls and bandits with graph feedback.	[Zimmert, Julian; Lattimore, Tor] DeepMind, London, England; [Zimmert, Julian] Univ Copenhagen, Copenhagen, Denmark	University of Copenhagen	Zimmert, J (corresponding author), DeepMind, London, England.; Zimmert, J (corresponding author), Univ Copenhagen, Copenhagen, Denmark.	zimmert@di.ku.dk; lattimore@google.com						Abernethy J, 2008, 21 ANN C LEARNING TH, P263; Abernethy J., 2009, P 22 ANN C LEARN THE; Alon N., 2015, PMLR, V40, P23; Alon N, 2017, SIAM J COMPUT, V46, P1785, DOI 10.1137/140989455; [Anonymous], 2016, FDN TRENDS IN OPTIMI; Antos A, 2013, THEOR COMPUT SCI, V473, P77, DOI 10.1016/j.tcs.2012.10.008; Audibert J.-Y., 2009, P 22 ANN C LEARN THE, P217; Bartok G, 2014, MATH OPER RES, V39, P967, DOI 10.1287/moor.2014.0663; Bubeck S., 2019, ARXIV190200681; Bubeck S, 2012, REGRET ANAL STOCHAST; Bubeck S., 2018, P 29 INT C ALG LEARN, P111; Bubeck Sebastien, 2015, C LEARN THEOR, P266; Cesa-Bianchi N, 2006, MATH OPER RES, V31, P562, DOI 10.1287/moor.1060.0206; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Cohen A, 2016, PR MACH LEARN RES, V48; Dong S., 2018, P ADV NEUR INF PROC, P4157; Foster D., 2012, J MACHINE LEARN RES, P382; Gravin N, 2016, PROCEED INGS 27 ANN, P528; Lattimore T., 2019, PREPRINT; Lattimore T., 2019, INT C ALG LEARN THEO; Lattimore T., 2019, C LEARN THEOR; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Nemirovsky A. S., 1979, EKONOMIKA MATEMATICH, V15; Russo D, 2016, J MACH LEARN RES, V17; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Rustichini A, 1999, GAME ECON BEHAV, V29, P224, DOI 10.1006/game.1999.0690; Valko M., 2016, BANDITS GRAPHS STRUC; Van Roy, 2014, ADV NEURAL INFORM PR, P1583	28	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903057
C	Acharya, J; Sun, ZT; Zhang, HY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Acharya, Jayadev; Sun, Ziteng; Zhang, Huanyu			Differentially Private Testing of Identity and Closeness of Discrete Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study the fundamental problems of identity testing (goodness of fit), and closeness testing (two sample test) of distributions over k elements, under differential privacy. While the problems have a long history in statistics, finite sample bounds for these problems have only been established recently. In this work, we derive upper and lower bounds on the sample complexity of both the problems under (epsilon, delta)-differential privacy. We provide sample optimal algorithms for identity testing problem for all parameter ranges, and the first results for closeness testing. Our closeness testing bounds are optimal in the sparse regime where the number of samples is at most k. Our upper bounds are obtained by privatizing non-private estimators for these problems. The non-private estimators are chosen to have small sensitivity. We propose a general framework to establish lower bounds on the sample complexity of statistical tasks under differential privacy. We show a bound on differentially private algorithms in terms of a coupling between the two hypothesis classes we aim to test. By carefully constructing chosen priors over the hypothesis classes, and using Le Cam's two point theorem we provide a general mechanism for proving lower bounds. We believe that the framework can be used to obtain strong lower bounds for other statistical tasks under privacy.	[Acharya, Jayadev; Sun, Ziteng; Zhang, Huanyu] Cornell Univ, Ithaca, NY 14853 USA	Cornell University	Acharya, J (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	acharya@cornell.edu; zs335@cornell.edu; hz388@cornell.edu			NSF-CCF-CRII [1657471]; Cornell University	NSF-CCF-CRII; Cornell University	The authors are listed in alphabetical order. This research was supported by NSF-CCF-CRII 1657471, and a grant from Cornell University.	Acharya J., 2018, P 35 INT C MACH LEAR, P30; Acharya J, 2017, IEEE T INFORM THEORY, V63, P38, DOI 10.1109/TIT.2016.2620435; Acharya Jayadev, 2015, ADV NEURAL INFORM PR, P3591; Acharya Jayadev, 2018, ARXIV180802174; Acharya Jayadev, 2018, ABS180204705 CORR; Acharya Jayadev, 2013, P 16 INT C ART INT S; Acharya S, 2014, 2014 IEEE 2ND INTERNATIONAL CONFERENCE ON EMERGING ELECTRONICS (ICEE); Aliakbarpour Maryam, 2018, P 35 INT C MACH LEAR, P169; [Anonymous], [No title captured]; Barthe G, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P55, DOI 10.1145/2976749.2978391; Barthe G, 2016, PROCEEDINGS OF THE 31ST ANNUAL ACM-IEEE SYMPOSIUM ON LOGIC IN COMPUTER SCIENCE (LICS 2016), P749, DOI 10.1145/2933575.2934554; Batu T, 2001, ANN IEEE SYMP FOUND, P442, DOI 10.1109/SFCS.2001.959920; Batu T, 2000, ANN IEEE SYMP FOUND, P259, DOI 10.1109/SFCS.2000.892113; Batu T, 2017, ANN IEEE SYMP FOUND, P880, DOI 10.1109/FOCS.2017.86; Blum A, 2013, J ACM, V60, DOI 10.1145/2450142.2450148; Cai B, 2017, PR MACH LEARN RES, V70; Canonne CL, 2016, LEIBNIZ INT PR INFOR, V47, DOI 10.4230/LIPIcs.STACS.2016.25; Canonne Clement L., 2015, ELECT C COMPUTATIONA, V22, P63; Chan S., 2014, P 25 ANN ACM SIAM S, P1193, DOI [10.1137/1.9781611973402.88, DOI 10.1137/1.9781611973402.88]; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Cuff P., 2016, P 2016 ACM SIGSAC C, P43, DOI DOI 10.1145/2976749.2978308; Dalenius T, 1977, STAT TIDSKRIFT, V15, P2; Den Hollander F, 2012, LECT NOTES; Diakonikolas I., 2018, P 45 INT C AUT LANG; Diakonikolas I., 2015, ADV NEURAL INFORM PR, P2566; Diakonikolas I, 2016, ANN IEEE SYMP FOUND, P685, DOI 10.1109/FOCS.2016.78; Diakonikolas Ilias, 2015, P TWENTYSIXTH ANN AC, P1841, DOI [10.1137/1.9781611973730.123, DOI 10.1137/1.9781611973730.123]; Dinur I., 2003, P 22 ACM SIGMOD SIGA, P202, DOI DOI 10.1145/773153.773173; Duchi J.C., 2012, ADV NEURAL INFORM PR, V25, P1430; Duchi JC, 2013, ANN IEEE SYMP FOUND, P429, DOI 10.1109/FOCS.2013.53; Dwork C, 2008, LECT NOTES COMPUT SC, V4890, P1; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2015, LECT NOTES COMPUT SC, V9453, P735, DOI 10.1007/978-3-662-48800-3_30; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12; Erlingsson U, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1054, DOI 10.1145/2660267.2660348; Gaboardi Marco, 2016, ICML, P1395; Goldreich O., 2016, ELECT C COMPUTATIONA, V23; Goldreich O., 2011, STUDIES COMPLEXITY C, V6650, P68, DOI [10.1007/978-3-642-22670-0_9, DOI 10.1007/978-3-642-22670-0_9, 10.1007/978-3-642-22670-09, DOI 10.1007/978-3-642-22670-09]; Hardt M, 2010, ACM S THEORY COMPUT, P705; Issa Ibrahim, 2017, P 2017 IEEE INT S IN; Kairouz P, 2016, PR MACH LEARN RES, V48; Kairouz P, 2017, IEEE T INFORM THEORY, V63, P4037, DOI 10.1109/TIT.2017.2685505; Karwa V., 2018, P 9 INN THEOR COMP S, V94, p44:1; Kifer D, 2017, PR MACH LEARN RES, V54, P991; Knoblauch A, 2008, SIAM J APPL MATH, V69, P197, DOI 10.1137/070700024; Lehmann Erich Leo, 2006, THEORY POINT ESTIMAT, V31; Li C, 2015, VLDB J, V24, P757, DOI 10.1007/s00778-015-0398-x; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Mir D.J., 2012, INT S FDN PRACT SEC, P374; Narayanan A, 2008, P IEEE S SECUR PRIV, P111, DOI 10.1109/SP.2008.33; Neyman J, 1933, PHILOS T R SOC LOND, V231, P289, DOI 10.1098/rsta.1933.0009; Paninski L, 2008, IEEE T INFORM THEORY, V54, P4750, DOI 10.1109/TIT.2008.928987; Pastore A, 2016, IEEE INT SYMP INFO, P2694, DOI 10.1109/ISIT.2016.7541788; Sankar L, 2013, IEEE T INF FOREN SEC, V8, P838, DOI 10.1109/TIFS.2013.2253320; Sheffet Or, 2018, P INT C MACH LEARN, V80, P4612; Sweeney L, 2002, INT J UNCERTAIN FUZZ, V10, P557, DOI 10.1142/S0218488502001648; Vadhan S, 2017, INFORM SEC CRYPT TEX, P347, DOI 10.1007/978-3-319-57048-8_7; Valiant G, 2014, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2014.14; Wang S., 2016, ARXIV160708025; Wang WN, 2016, IEEE T INFORM THEORY, V62, P5018, DOI 10.1109/TIT.2016.2584610; Wang Y., 2015, ARXIV151103376; WARNER SL, 1965, J AM STAT ASSOC, V60, P63, DOI 10.2307/2283137; Wasserman L, 2010, J AM STAT ASSOC, V105, P375, DOI 10.1198/jasa.2009.tm08651; Ye M, 2018, IEEE T INFORM THEORY, V64, P5662, DOI 10.1109/TIT.2018.2809790; Yu B., 1997, FESTSCHRIFT LUCIEN C, P423	67	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													14	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001042
C	Aghasi, A; Ahmed, A; Hand, P; Joshi, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Aghasi, Alireza; Ahmed, Ali; Hand, Paul; Joshi, Babhru			A convex program for bilinear inversion of sparse vectors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				BLIND DECONVOLUTION; SIGNAL RECOVERY; ALGORITHMS	We consider the bilinear inverse problem of recovering two vectors, x is an element of R-L and w is an element of R-L, from their entrywise product. We consider the case where x and w have known signs and are sparse with respect to known dictionaries of size K and N, respectively. Here, K and N may be larger than, smaller than, or equal to L. We introduce l(1)-BranchHull, which is a convex program posed in the natural parameter space and does not require an approximate solution or initialization in order to be stated or solved. We study the case where x and w are S-1- and S-2-sparse with respect to a random dictionary, with the sparse vectors satisfying an effective sparsity condition, and present a recovery guarantee that depends on the number of measurements as L >= Omega( S-1 + S-2) log(2) (K + N). Numerical experiments verify that the scaling constant in the theorem is not too large. One application of this problem is the sweep distortion removal task in dielectric imaging, where one of the signals is a nonnegative reflectivity, and the other signal lives in a known subspace, for example that given by dominant wavelet coefficients. We also introduce a variants of l(1)-BranchHull for the purposes of tolerating noise and outliers, and for the purpose of recovering piecewise constant signals. We provide an ADMM implementation of these variants and show they can extract piecewise constant behavior from real images.	[Aghasi, Alireza] GSU, Georgia State Business Sch, Atlanta, GA 30302 USA; [Ahmed, Ali] ITU, Dept Elect Engn, Lahore, Pakistan; [Hand, Paul] Northeastern Univ, Dept Math, Boston, MA 02115 USA; [Hand, Paul] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA; [Joshi, Babhru] Rice Univ, Dept Computat & Appl Math, Houston, TX 77251 USA	University System of Georgia; Georgia State University; Northeastern University; Northeastern University; Rice University	Aghasi, A (corresponding author), GSU, Georgia State Business Sch, Atlanta, GA 30302 USA.	aaghasi@gsu.edu; ali.ahmed@itu.edu.pk; p.hand@northeastern.edu; babhru.joshi@rice.edu			HEC, Pakistan; NSF [DMS-1464525]	HEC, Pakistan(Higher Education Commission of Pakistan); NSF(National Science Foundation (NSF))	Ali Ahmed would like to acknowledge the partial support through the grant for the National center of cyber security (NCCS) from HEC, Pakistan. Paul Hand would like to acknowledge funding by the grant NSF DMS-1464525.	Aghasi A, 2016, OPTICA, V3, P754, DOI 10.1364/OPTICA.3.000754; Aghasi  Alireza, 2016, ARXIV13120525V2; Akritas M. G., 2016, TOPICS NONPARAMETRIC; [Anonymous], 2015, ADV NEURAL INFORM PR; Bahmani S., 2016, P INT C ART INT STAT; Bahmani S., 2017, ARXIV170205327; Cand~s E., 2012, FOUND COMPUT MATH, P1; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Chen T, 2006, IEEE T PATTERN ANAL, V28, P1519, DOI 10.1109/TPAMI.2006.195; FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758; Goldstein Tom, 2016, ARXIV161007531; He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168; Hold Berthold K. P., 1986, ROBOT VISION; Hoyer PO, 2004, J MACH LEARN RES, V5, P1457; Koltchinskii V, 2015, INT MATH RES NOTICES, V2015, P12991, DOI 10.1093/imrn/rnv096; Kundur D, 1996, IEEE SIGNAL PROC MAG, V13, P43, DOI 10.1109/79.489268; Lecue Guillaume, 2017, J MACHINE LEARNING R, V18, P5356; Ledoux M., 2013, PROBABILITY BANACH S, P86; Lee DD, 2001, ADV NEUR IN, V13, P556; Lee Kiryung, 2017, ARXIV170204342; Li X., 2016, ARXIV160604933; Li XD, 2013, SIAM J MATH ANAL, V45, P3019, DOI 10.1137/120893707; Ling SY, 2015, INVERSE PROBL, V31, DOI 10.1088/0266-5611/31/11/115002; McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148, DOI DOI 10.1017/CBO9781107359949.008; Mendelson S., 2014, C LEARN THEOR, P25; Netrapalli P., 2013, P ADV NEUR INF PROC, P2796; O'Grady PD, 2005, INT J IMAG SYST TECH, V15, P18, DOI 10.1002/ima.20035; Oymak S, 2015, IEEE T INFORM THEORY, V61, P2886, DOI 10.1109/TIT.2015.2401574; STOCKHAM TG, 1975, P IEEE, V63, P678, DOI 10.1109/PROC.1975.9800; Sun J, 2016, IEEE INT SYMP INFO, P2379, DOI 10.1109/ISIT.2016.7541725; Tu S., 2015, ARXIV150703566; van de Geer S, 2013, PROBAB THEORY REL, V157, P225, DOI 10.1007/s00440-012-0455-y	37	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003014
C	Ahn, K; Lee, K; Cha, H; Suh, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ahn, Kwangjun; Lee, Kangwook; Cha, Hyunseung; Suh, Changho			Binary Rating Estimation with Graph Side Information	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Rich experimental evidences show that one can better estimate users' unknown ratings with the aid of graph side information such as social graphs. However, the gain is not theoretically quantified. In this work, we study the binary rating estimation problem to understand the fundamental value of graph side information. Considering a simple correlation model between a rating matrix and a graph, we characterize the sharp threshold on the number of observed entries required to recover the rating matrix (called the optimal sample complexity) as a function of the quality of graph side information (to be detailed). To the best of our knowledge, we are the first to reveal how much the graph side information reduces sample complexity. Further, we propose a computationally efficient algorithm that achieves the limit Our experimental results demonstrate that the algorithm performs well even with real-world graphs.	[Ahn, Kwangjun] 142nd Mil Police Co, Korean Augmentat US Army, Seoul, South Korea; [Lee, Kangwook; Suh, Changho] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea; [Cha, Hyunseung] Kakao Brain, Jeju, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Ahn, K (corresponding author), 142nd Mil Police Co, Korean Augmentat US Army, Seoul, South Korea.	kjahnkorea@kaist.ac.kr; kw1jjang@kaist.ac.kr; tony.cha@kakaobrain.com; chsuh@kaist.ac.kr		Suh, Changho/0000-0002-3101-4291	National Research Foundation of Korea (NRF) - Korea government (MSIP) [2018R1A1A1A05022889]	National Research Foundation of Korea (NRF) - Korea government (MSIP)	The work was jointly supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No. 2018R1A1A1A05022889) and Kakao Brain Corp.	Abbe E., 2017, COMMUNICATIONS PURE; Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670; Abbe E, 2015, ANN IEEE SYMP FOUND, P670, DOI 10.1109/FOCS.2015.47; Adamic LA, 2005, INT WORKSHOP LINK DI, P36, DOI DOI 10.1145/1134271.1134277; Agarwal D., 2010, P 3 ACM INT C WEB SE, P91, DOI DOI 10.1145/1718487.1718499; [Anonymous], 2008, P 25 INT C MACH LEAR; Ashtiani H., 2016, ADV NEURAL INFORM PR, P3216; Balakrishnan Sivaraman, 2017, ANN STAT; Bell RM, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P95; Boutsidis C, 2015, PR MACH LEARN RES, V37, P40; Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Chen YX, 2015, PR MACH LEARN RES, V37, P371; Chen Yuxin, 2016, ICML; Chiang K Y, 2015, ADV NEURAL INFORM PR, P3447; Chin Peter, 2015, STOCHASTIC BLOCK MOD, P391; Chouvardas S, 2017, INT CONF ACOUST SPEE, P4019, DOI 10.1109/ICASSP.2017.7952911; Gao Chao, 2017, JMLR; Golbeck J, 2006, CONSUM COMM NETWORK, P282; Guo G., 2015, AAAI; Guo Guibing, 2015, UMAP WORKSHOPS, V4; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Jahrer M, 2010, P 16 ACM SIGKDD INT, V25, P693, DOI DOI 10.1145/1835804.1835893; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; JAMALI M., 2009, P 3 ACM C REC SYST, P181; Jamali M., 2010, P 4 ACM C RECOMMENDE, P135, DOI [10.1145/1864708.1864736, DOI 10.1145/1864708.1864736]; Jamali M, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P397; Javanmard A, 2016, P NATL ACAD SCI USA, V113, pE2218, DOI 10.1073/pnas.1523097113; Jog V., 2015, ARXIV150906418; Kalofolias V., 2014, ARXIV14081717, DOI DOI 10.1016/j.compchemeng.2009.01.019; Keshavan R. H., 2010, IEEE T INFORM THEORY; Koren Y., 2008, SIGKDD; Krzakala F., 2013, PNAS; Lei J, 2015, ANN STAT, V43, P215, DOI 10.1214/14-AOS1274; Ma H., 2008, NEUROCOMPUTING, P931; Ma H, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P203, DOI 10.1145/1571941.1571978; Ma Hao, 2011, WSDM, P287; Massa P., 2005, CONTROVERSIAL USERS, P121; Mazumdar A., 2017, PROC INT C NEURAL IN, P4685; McPherson M, 2001, ANNU REV SOCIOL, V27, P415, DOI 10.1146/annurev.soc.27.1.415; Monti F., 2017, NIPS, P3700; Mossel Elchanan, 2015, ARXIV150903281, P7; Netrapalli P., 2013, P ADV NEUR INF PROC, P2796; Rao N., 2015, PROC 28 INT C NEURAL, P2107, DOI DOI 10.5555/2969442.2969475; Rennie J. D., 2005, P 22 INT C MACHINE L, P713; Rui Wu, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P449, DOI 10.1145/2745844.2745887; Saad H., 2018, IEEE J SELECTED TOPI; Salakhutdinov R., 2007, ADV NEURAL INF PROCE, V20, P1257; Shah Devavrat, 2017, ARXIV170308085; Si L., 2003, MACHINE LEARNING INT, V20, P704; Traud AL, 2012, PHYSICA A, V391, P4165, DOI 10.1016/j.physa.2011.12.021; van den Berg Rianne, 2017, STAT, V1050, P7; Yang X., 2012, P 6 ACM C REC SYST, P67; Yang XW, 2013, IEEE T PARALL DISTR, V24, P642, DOI 10.1109/TPDS.2012.192; Yi X., 2016, NIPS, P4152; Yun S., 2014, ARXIV14127335; Zhang AY, 2016, ANN STAT, V44, P2252, DOI 10.1214/15-AOS1428; Zhang Y., 2015, ARXIV150901173; Zhao H, 2017, IEEE DATA MINING, P645, DOI 10.1109/ICDM.2017.74	60	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304030
C	Antognini, JM; Sohl-Dickstein, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Antognini, Joseph M.; Sohl-Dickstein, Jascha			PCA of high dimensional random walks with comparison to neural network training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					One technique to visualize the training of neural networks is to perform PCA on the parameters over the course of training and to project to the subspace spanned by the first few PCA components. In this paper we compare this technique to the PCA of a high dimensional random walk. We compute the eigenvalues and eigenvectors of the covariance of the trajectory and prove that in the long trajectory and high dimensional limit most of the variance is in the first few PCA components, and that the projection of the trajectory onto any subspace spanned by PCA components is a Lissajous curve. We generalize these results to a random walk with momentum and to an Ornstein-Uhlenbeck processes (i.e., a random walk in a quadratic potential) and show that in high dimensions the walk is not mean reverting, but will instead be trapped at a fixed distance from the minimum. We finally analyze PCA projected training trajectories for: a linear model trained on CIFAR-10; a fully connected model trained on MNIST; and ResNet-50-v2 trained on Imagenet. In all cases, both the distribution of PCA eigenvalues and the projected trajectories resemble those of a random walk with drift.	[Antognini, Joseph M.] Whisper AI, San Francisco, CA 94110 USA; [Sohl-Dickstein, Jascha] Google Brain, Mountain View, CA USA	Google Incorporated	Antognini, JM (corresponding author), Whisper AI, San Francisco, CA 94110 USA.	joe.antognini@gmail.com; jaschasd@google.com						Ahn S., 2012, P 29 INT C MACH LEAR; Baity-Jesi M., 2018, INT C MACH LEARN PML, P314; Bottcher A, 2003, MATH COMPUT, V72, P1329, DOI 10.1090/S0025-5718-03-01505-9; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Dinh L, 2017, PR MACH LEARN RES, V70; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Gray RM, 2006, FOUND TRENDS COMMUN, V2, DOI 10.1561/0100000006; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jozefowicz Rafal, 2016, ARXIV160202410; Keskar N.S., 2017, ICLR; Li H., 2018, INT C LEARN REPR; Lipton Z. C., 2016, ARXIV160207320; Lorch E., 2016, 33 INT C MACH LEARN, V48; Mandt S, 2016, PR MACH LEARN RES, V48; Moore J, 2018, J THEOR BIOL, V447, P56, DOI 10.1016/j.jtbi.2018.03.022; Novak R., 2018, INT C LEARN REPR; Rump SM, 2006, LINEAR ALGEBRA APPL, V413, P567, DOI 10.1016/j.laa.2005.06.009; Smith Samuel L., 2018, INT C LEARN REPR; Uhlenbeck GE, 1930, PHYS REV, V36, P0823, DOI 10.1103/PhysRev.36.823; Zhu ZH, 2017, IEEE T INFORM THEORY, V63, P2975, DOI 10.1109/TIT.2017.2676808	22	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004082
C	Awan, J; Slavkovic, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Awan, Jordan; Slavkovic, Aleksandra			Differentially Private Uniformly Most Powerful Tests for Binomial Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We derive uniformly most powerful (UMP) tests for simple and one-sided hypotheses for a population proportion within the framework of Differential Privacy (DP), optimizing finite sample performance. We show that in general, DP hypothesis tests can be written in terms of linear constraints, and for exchangeable data can always be expressed as a function of the empirical distribution. Using this structure, we prove a 'Neyman-Pearson lemma' for binomial data under DP, where the DP-UMP only depends on the sample sum. Our tests can also be stated as a post-processing of a random variable, whose distribution we coin "Truncated-Uniform-Laplace" (Tulap), a generalization of the Staircase and discrete Laplace distributions. Furthermore, we obtain exact p-values, which are easily computed in terms of the Tulap random variable. We show that our results also apply to distribution-free hypothesis tests for continuous data. Our simulation results demonstrate that our tests have exact type I error, and are more powerful than current techniques.	[Awan, Jordan; Slavkovic, Aleksandra] Penn State Univ, Dept Stat, University Pk, PA 16802 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park	Awan, J (corresponding author), Penn State Univ, Dept Stat, University Pk, PA 16802 USA.	awan@psu.edu; sesa@psu.edu		Awan, Jordan/0000-0001-9404-7499; Slavkovic, Aleksandra/0000-0003-0497-1771	NSF [SES-1534433]	NSF(National Science Foundation (NSF))	We would like to thank Vishesh Karwa and Matthew Reimherr for helpful discussions and feedback on previous drafts. We also thank the reviewers for their helpful comments and suggestions, which have contributed to many improvements in the presentation of this work. This work is supported in part by NSF Award No. SES-1534433 to The Pennsylvania State University.	Awan J., 2018, ARXIV E PRINTS; Barrientos A., 2017, ARXIV E PRINTS; Bishop C.M, 2006, PATTERN RECOGN; Casella G, 2002, STAT INFERENCE, V2nd; Duchi JC, 2018, J AM STAT ASSOC, V113, P182, DOI 10.1080/01621459.2017.1389735; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Gaboardi M, 2016, PR MACH LEARN RES, V48; Gaboardi Marco, 2018, P MACHINE LEARNING R, V80, P1626; Geng Q, 2016, IEEE T INFORM THEORY, V62, P925, DOI 10.1109/TIT.2015.2504967; Geng Q, 2016, IEEE T INFORM THEORY, V62, P952, DOI 10.1109/TIT.2015.2504972; Ghosh A, 2009, ACM S THEORY COMPUT, P351; Gibbons J.D., 2014, NONPARAMETRIC STAT I; Inusah S, 2006, J STAT PLAN INFER, V136, P1090, DOI 10.1016/j.jspi.2004.08.014; Karwa Vishesh, 2017, CORR; Schervish M., 1996, SPRINGER SERIES STAT; Sheffet O, 2017, PR MACH LEARN RES, V70; Solea Eftychia, 2014, THESIS; Steinke Thomas, 2018, PRIVATE CORRES; Uhler C, 2013, J PRIVACY CONFIDENTI, V5; Van der Vaart A.W., 2000, CAMBRIDGE SERIES STA, DOI DOI 10.1017/CBO9780511802256; Vu D, 2009, INT CONF DAT MIN WOR, P138, DOI 10.1109/ICDMW.2009.52; Wang Y., 2015, ARXIV E PRINTS; Wasserman L, 2010, J AM STAT ASSOC, V105, P375, DOI 10.1198/jasa.2009.tm08651; Xie MG, 2013, INT STAT REV, V81, P3, DOI 10.1111/insr.12000	25	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304024
C	Bello, K; Honorio, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bello, Kevin; Honorio, Jean			Computationally and statistically efficient learning of causal Bayes nets using path queries	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DISCOVERY	Causal discovery from empirical data is a fundamental problem in many scientific domains. Observational data allows for identifiability only up to Markov equivalence class. In this paper we first propose a polynomial time algorithm for learning the exact correctly-oriented structure of the transitive reduction of any causal Bayesian network with high probability, by using interventional path queries. Each path query takes as input an origin node and a target node, and answers whether there is a directed path from the origin to the target. This is done by intervening on the origin node and observing samples from the target node. We theoretically show the logarithmic sample complexity for the size of interventional data per path query, for continuous and discrete networks. We then show how to learn the transitive edges using also logarithmic sample complexity (albeit in time exponential in the maximum number of parents for discrete networks), which allows us to learn the full network. We further extend our work by reducing the number of interventional path queries for learning rooted trees. We also provide an analysis of imperfect interventions.	[Bello, Kevin; Honorio, Jean] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus	Bello, K (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.	kbellome@purdue.edu; jhonorio@purdue.edu						Aho A. V., 1972, SIAM Journal on Computing, V1, P131, DOI 10.1137/0201008; Brenner E., 2013, UAI; Buhlmann P, 2012, P 6 EUR WORKSH PROB; Cheng J., 2002, ARTIFICIAL INTELLIGE; Chickering D., 2002, UAI; DVORETZKY A, 1956, ANN MATH STAT, V27, P642, DOI 10.1214/aoms/1177728174; Eaton D, 2007, P MACHINE LEARNING R, V2, P107; Eberhardt F., 2005, P 21 C UNC ART INT, P178; Harbison Christopher T, 2004, NATURE; He YB, 2008, J MACHINE LEARNING R, V9; Hoffgen K, 1993, COLT; Kocaoglu M., 2017, ADV NEURAL INFORM PR, P7021, DOI [10.5555/3295222.3295445, DOI 10.5555/3295222.3295445]; Koller D., 2009, PROBABILISTIC GRAPHI; Legall Francois, 2014, P 39 INT S SYMBOLIC, P296, DOI [DOI 10.1145/2608628.2608664, 10.1145/2608628.2608664]; Louizos C., 2017, NIPS; Murphy K. P., 2001, TECHNICAL REPORT; Obozinski G., 2009, ADV NEURAL INFORM PR; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Peters J., 2010, P 13 INT C ART INT S, P597; Peters J, 2014, J MACH LEARN RES, V15, P2009; Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631; Shanmugam K., 2015, ADV NEURAL INFORM PR, P3195; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Spirtes P., 2000, CAUSATION PREDICTION; Triantafillou S, 2015, J MACH LEARN RES, V16, P2147; Tsamardinos Ioannis, 2006, MACHINE LEARNING; Verma T., 1991, P 6 ANN C UNC ART IN; Wang Z., 2016, ARXIV160605183; Xiao Yun, 2015, SCI REPORTS; Zuk O., 2006, UAI; [No title captured]	34	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005051
C	Bertsimas, D; McCord, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bertsimas, Dimitris; McCord, Christopher			Optimization over Continuous and Multi-dimensional Decisions with Observational Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider the optimization of an uncertain objective over continuous and multi-dimensional decision spaces in problems in which we are only provided with observational data. We propose a novel algorithmic framework that is tractable, asymptotically consistent, and superior to comparable methods on example problems. Our approach leverages predictive machine learning methods and incorporates information on the uncertainty of the predicted outcomes for the purpose of prescribing decisions. We demonstrate the efficacy of our method on examples involving both synthetic and real data sets.	[Bertsimas, Dimitris] MIT, Sloan Sch Management, Cambridge, MA 02142 USA; [McCord, Christopher] MIT, Operat Res Ctr, Cambridge, MA 02142 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Bertsimas, D (corresponding author), MIT, Sloan Sch Management, Cambridge, MA 02142 USA.	dbertsim@mit.edu; mccord@mit.edu						[Anonymous], 2014, ARXIV14025481; Athey S., 2017, ARXIV PREPRINT ARXIV; Bertsimas D, 2018, MATH PROGRAM, V167, P235, DOI 10.1007/s10107-017-1125-8; Bertsimas Dimitris, 2017, POWER LIMITS PREDICT; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Chen T., 2016, KDD16 P 22 ACM, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]; Dunn Jack, 2018, OPTIMAL PRESCRIPTIVE; Elmachtoub A. N., 2017, ARXIV171008005; Flores Carlos A, 2005, ESTIMATION DOSE RESP; Hirano K., 2004, APPL BAYESIAN MODELI, P73, DOI DOI 10.1002/0470090456.CH7; Kallus N., 2017, ARXIV170507384; Kallus N, 2017, PR MACH LEARN RES, V70; Klein TE, 2009, NEW ENGL J MED, V360, P753, DOI 10.1056/NEJMoa0809329; Maurer A., 2009, COLT; Misic, 2017, PREPRINT; Olshen R., 1984, CLASSIFICATION REGRE; Rosenbaum PR, 2010, SPRINGER SER STAT, P1, DOI 10.1007/978-1-4419-1213-8; Swaminathan A, 2015, WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P939, DOI 10.1145/2740908.2742564; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wager Stefan, 2017, J AM STAT ASS; Xiang Y, 2009, ADV NEURAL INFORM PR, V22, P889; Zhou A., 2018, ARXIV180206037	23	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302094
C	Brunel, VE		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Brunel, Victor-Emmanuel			Learning Signed Determinantal Point Processes through the Principal Minor Assignment Problem	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CYCLE BASIS	Symmetric determinantal point processes (DPP) are a class of probabilistic models that encode the random selection of items that have a repulsive behavior. They have attracted a lot of attention in machine learning, where returning diverse sets of items is sought for. Sampling and learning these symmetric DPP's is pretty well understood. In this work, we consider a new class of DPP's, which we call signed DPP's, where we break the symmetry and allow attractive behaviors. We set the ground for learning signed DPP's through a method of moments, by solving the so called principal assignment problem for a class of matrices K that satisfy K-i,K-j = +/- K-j,K-i, i not equal j, in polynomial time.	[Brunel, Victor-Emmanuel] MIT, Dept Math, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Brunel, VE (corresponding author), MIT, Dept Math, Cambridge, MA 02139 USA.	vebrunel@mit.edu						Affandi RH, 2014, PR MACH LEARN RES, V32, P1224; Amaldi E, 2010, LECT NOTES COMPUT SC, V6080, P397, DOI 10.1007/978-3-642-13036-6_30; Anari N., 2016, JMLR WORKSHOP C P, V49, P103; BARDENET R, 2015, ADV NEURAL INFORM PR, V28, P3393; Batmanghelich Nematollah Kayhan, 2014, CORR; Borcea J, 2009, J AM MATH SOC, V22, P521; Brunel Victor-Emmanuel, 2017, C LEARN THEOR; Donghoon Lee, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9910, P330, DOI 10.1007/978-3-319-46466-4_20; Dupuy C., 2016, ARXIV161005925; Gartrell M., 2016, ARXIV160205436; Gartrell M, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P349, DOI 10.1145/2959100.2959178; Gillenwater J., 2014, ADV NEURAL INFORM PR, V2, P3149; HORTON JD, 1987, SIAM J COMPUT, V16, P358, DOI 10.1137/0216026; Johnson C.R., 1995, LINEAR MULTILINEAR A, V38, P233, DOI DOI 10.1080/03081089508818359; Kulesza Alex, 2011, ICML; Kulesza Alex, 2012, DETERMINANTAL POINT; Lin H., 2012, P 28 C UNC ART INT U, P479; MACCHI O, 1975, ADV APPL PROBAB, V7, P83, DOI 10.2307/1425855; Mariet Z., 2016, ADV NEURAL INFORM PR, V29, P2694; Mariet Z, 2015, PR MACH LEARN RES, V37, P2389; Oeding L, 2011, ALGEBR NUMBER THEORY, V5, P75, DOI 10.2140/ant.2011.5.75; Rising J, 2015, LINEAR ALGEBRA APPL, V473, P126, DOI 10.1016/j.laa.2014.04.019; Snoek J, 2013, P ADV NEUR INF PROC, V26, P1932; Urschel John, 2017, ICML; Xu HT, 2016, IEEE-ACM T AUDIO SPE, V24, P978, DOI 10.1109/TASLP.2016.2537203; Yao JG, 2016, AAAI CONF ARTIF INTE, P3080	26	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001088
C	Burkov, E; Lempitsky, V		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Burkov, Egor; Lempitsky, Victor			Deep Neural Networks with Box Convolutions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Box filters computed using integral images have been part of the computer vision toolset for a long time. Here, we show that a convolutional layer that computes box filter responses in a sliding manner can be used within deep architectures, whereas the dimensions and the offsets of the sliding boxes in such a layer can be learned as a part of an end-to-end loss minimization. Crucially, the training process can make the size of the boxes in such a layer arbitrarily large without incurring extra computational cost and without the need to increase the number of learnable parameters. Due to its ability to integrate information over large boxes, the new layer facilitates long-range propagation of information and leads to the efficient increase of the receptive fields of network units. By incorporating the new layer into existing architectures for semantic segmentation, we are able to achieve both the increase in segmentation accuracy as well as the decrease in the computational cost and the number of learnable parameters.	[Burkov, Egor; Lempitsky, Victor] Samsung AI Ctr, Moscow, Russia; [Burkov, Egor; Lempitsky, Victor] Skolkovo Inst Sci & Technol Skoltech, Moscow, Russia	Skolkovo Institute of Science & Technology	Burkov, E (corresponding author), Samsung AI Ctr, Moscow, Russia.; Burkov, E (corresponding author), Skolkovo Inst Sci & Technol Skoltech, Moscow, Russia.				Skolkovo Institute of Science and Technology; Ministry of Science of Russian Federation [14.756.31.0001]	Skolkovo Institute of Science and Technology; Ministry of Science of Russian Federation	Most of the work was done when both authors were full-time with Skolkovo Institute of Science and Technology. The work was supported by the Ministry of Science of Russian Federation grant 14.756.31.0001.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 1995, VISION INTERFACE; Chandra S, 2016, LECT NOTES COMPUT SC, V9911, P402, DOI 10.1007/978-3-319-46478-7_25; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LC, 2015, PR MACH LEARN RES, V37, P1785; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Cipoll Roberto, 2008, PROC CVPR IEEE, P1; Collobert R., 2011, BIGLEARN NIPS WORKSH; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Criminisil A, 2011, FOUND TRENDS COMPUT, V7, P81, DOI [10.1561/0600000035, 10.1501/0000000035]; Dollar P., 2009, P BMVC; Ghodrati A, 2017, INT J COMPUT VISION, V124, P115, DOI 10.1007/s11263-017-1006-x; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Holschneider M., 1990, WAVELETS, P286, DOI [10.1007/978-3-642-75988-8_28, 10.1007/978-3-642-75988-828, DOI 10.1007/978-3-642-75988-8_28]; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kingma D.P, P 3 INT C LEARNING R; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Paszke A., 2016, ARXIV160602147; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1; Song S., 2015, CVPR, V5, P6; Springenberg J.T., 2014, ARXIV14126806; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2017, P 31 AAAI C ART INT, V4, P12; Viola P, 2005, INT J COMPUT VISION, V63, P153, DOI 10.1007/s11263-005-6644-8; Viola Paul, 2001, P CVPR; Yandex Artem Babenko, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1269, DOI 10.1109/ICCV.2015.150; Yu  F., 2015, P ICLR; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179	35	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000069
C	Calandriello, D; Rosasco, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Calandriello, Daniele; Rosasco, Lorenzo			Statistical and Computational Trade-Offs in Kernel K-Means	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				QUANTIZATION	We investigate the efficiency of k-means in terms of both statistical and computational requirements. More precisely, we study a Nystrom approach to kernel k-means. We analyze the statistical properties of the proposed method and show that it achieves the same accuracy of exact kernel k-means with only a fraction of computations. Indeed, we prove under basic assumptions that sampling \root pn Nystrom landmarks allows to greatly reduce computational costs without incurring in any loss of accuracy. To the best of our knowledge this is the first result of this kind for unsupervised learning.	[Calandriello, Daniele] IIT, LCSL, Genoa, Italy; [Calandriello, Daniele; Rosasco, Lorenzo] MIT, Genoa, Italy; [Rosasco, Lorenzo] Univ Genoa, IIT, LCSL, Genoa, Italy	Istituto Italiano di Tecnologia - IIT; Istituto Italiano di Tecnologia - IIT; University of Genoa	Calandriello, D (corresponding author), IIT, LCSL, Genoa, Italy.; Calandriello, D (corresponding author), MIT, Genoa, Italy.				Center for Brains, Minds and Machines (CBMM) - NSF STC [CCF-1231216]; Italian Institute of Technology; NVIDIA Corporation; AFOSR (European Office of Aerospace Research and Development) [FA9550-17-1-0390, BAA-AFRL-AFOSR-2016-0007]; EU H2020-MSCA-RISE project [NoMADS - DLV-777826]; European Research Council [SEQUOIA 724063]	Center for Brains, Minds and Machines (CBMM) - NSF STC; Italian Institute of Technology(Istituto Italiano di Tecnologia - IIT); NVIDIA Corporation; AFOSR (European Office of Aerospace Research and Development); EU H2020-MSCA-RISE project; European Research Council(European Research Council (ERC)European Commission)	This material is based upon work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216, and the Italian Institute of Technology. We gratefully acknowledge the support of NVIDIA Corporation for the donation of the Titan Xp GPUs and the Tesla k40 GPU used for this research. L. R. acknowledges the support of the AFOSR projects FA9550-17-1-0390 and BAA-AFRL-AFOSR-2016-0007 (European Office of Aerospace Research and Development), and the EU H2020-MSCA-RISE project NoMADS - DLV-777826. A. R. acknowledges the support of the European Research Council (grant SEQUOIA 724063).	Ailon N., 2009, PROC 22 ADV NEURAL I, P10; Aizerman M. A., 1964, AUTOMAT REM CONTR, V25, P821, DOI DOI 10.1234/12345678; Aloise D, 2009, MACH LEARN, V75, P245, DOI 10.1007/s10994-009-5103-0; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Avron H, 2017, PR MACH LEARN RES, V70; Bach F.R., 2005, P 22 INT C MACH LEAR, P33, DOI DOI 10.1145/1102351.1102356; Bach Francis, 2013, C LEARN THEOR; Backurs Arturs, 2017, ADV NEURAL INFORM PR; Biau G, 2008, IEEE T INFORM THEORY, V54, P781, DOI 10.1109/TIT.2007.913516; Calandriello D, 2017, PR MACH LEARN RES, V70; Calandriello D, 2017, ADV NEUR IN, V30; Calandriello D, 2017, PR MACH LEARN RES, V54, P1421; Calandriello Daniele, 2017, THESIS; Canas G., 2012, ADV NEURAL INF PROCE, P2465; Chitta R., 2011, PROC 17 ACM SIGKDD I, P895, DOI DOI 10.1145/2020408.2020558; Dhillon Inderjit S., 2004, TR0425 UTCS U TEX AU; El Alaoui A, 2015, ADV NEUR IN, V28; Graf S, 2000, LECT NOTES MATH, V1730, P1; Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400; LeCun Y., 2010, MNIST HANDWRITTEN DI; Levrard C, 2015, ANN STAT, V43, P592, DOI 10.1214/14-AOS1293; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Loosli Gaelle, 2007, LARGE SCALE KERNEL M, V6; Maurer A, 2010, IEEE T INFORM THEORY, V56, P5839, DOI 10.1109/TIT.2010.2069250; Musco Cameron, 2017, ADV NEURAL INFORM PR, V30; Musco Cameron, 2017, NIPS; Oglic Dino, 2017, J MACHINE LEARNING R; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Rudi A, 2017, ADV NEUR IN, V30; Rudi A, 2015, ADV NEUR IN, V28; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Scholkopf B., 2001, LEARNING KERNELS SUP; Tropp JA, 2017, ADV NEUR IN, V30	34	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003087
C	Chakraborty, R; Yang, CH; Zhen, XJ; Banerjee, M; Archer, D; Vaillancourt, D; Singh, V; Vemuri, BC		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chakraborty, Rudrasis; Yang, Chun-Hao; Zhen, Xingjian; Banerjee, Monami; Archer, Derek; Vaillancourt, David; Singh, Vikas; Vemuri, Baba C.			A Statistical Recurrent Model on the Manifold of Symmetric Positive Definite Matrices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				TENSOR-BASED MORPHOMETRY; DTI SEGMENTATION; TRACTOGRAPHY	In a number of disciplines, the data (e.g., graphs, manifolds) to be analyzed are non-Euclidean in nature. Geometric deep learning corresponds to techniques that generalize deep neural network models to such non-Euclidean spaces. Several recent papers have shown how convolutional neural networks (CNNs) can be extended to learn with graph-based data. In this work, we study the setting where the data (or measurements) are ordered, longitudinal or temporal in nature and live on a Riemannian manifold - this setting is common in a variety of problems in statistical machine learning, vision and medical imaging. We show how recurrent statistical network models can be defined in such spaces. Then, we present an efficient algorithm and conduct a rigorous analysis of its statistical properties. We perform numerical experiments demonstrating competitive performance with state of the art methods but with significantly fewer parameters. We also show applications to a statistical analysis task in brain imaging, a regime where deep neural network models have only been utilized in limited ways.	[Chakraborty, Rudrasis; Yang, Chun-Hao; Banerjee, Monami; Archer, Derek; Vaillancourt, David; Vemuri, Baba C.] Univ Florida, Gainesville, FL 32611 USA; [Zhen, Xingjian; Singh, Vikas] Univ Wisconsin, Madison, WI USA	State University System of Florida; University of Florida; University of Wisconsin System; University of Wisconsin Madison	Chakraborty, R (corresponding author), Univ Florida, Gainesville, FL 32611 USA.			Yang, Chun-Hao/0000-0002-2522-5957; SINGH, VIKAS/0000-0002-8355-6519	NSF [IIS-1525431, IIS-1724174, R01 NS052318]; NSF CAREER award [1252725, R01 EB022883]; UW CPCP [U54 AI117924]	NSF(National Science Foundation (NSF)); NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); UW CPCP	This research was funded in part by the NSF grant IIS-1525431 and IIS-1724174 to BCV, R01 NS052318 to DV and NSF CAREER award 1252725 and R01 EB022883 to VS. XZ and VS were also supported by UW CPCP (U54 AI117924).	Afsari B, 2013, IEEE DECIS CONTR P, P1162, DOI 10.1109/CDC.2013.6760039; Afsari B, 2012, PROC CVPR IEEE, P2208, DOI 10.1109/CVPR.2012.6247929; Afsari B, 2011, P AM MATH SOC, V139, P655, DOI 10.1090/S0002-9939-2010-10541-5; Aganj I, 2009, I S BIOMED IMAGING, P1398, DOI 10.1109/ISBI.2009.5193327; Archer Derek B, 2017, CEREB CORTEX, P1; Arjovsky M, 2016, PR MACH LEARN RES, V48; BASSER PJ, 1994, BIOPHYS J, V66, P259, DOI 10.1016/S0006-3495(94)80775-1; Behrens TEJ, 2007, NEUROIMAGE, V34, P144, DOI 10.1016/j.neuroimage.2006.09.018; Bissacco Alessandro, 2001, COMP VIS PATT REC 20, V2, pII; Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chakraborty Rudrasis, 2018, ARXIV180505487; Cheng G, 2015, IEEE T MED IMAGING, V34, P298, DOI 10.1109/TMI.2014.2355138; Cheng G, 2012, LECT NOTES COMPUT SC, V7578, P390, DOI 10.1007/978-3-642-33786-4_29; Cherian A, 2011, IEEE I CONF COMP VIS, P2399, DOI 10.1109/ICCV.2011.6126523; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Cohen Taco S, 2016, ARXIV161208498; Cohen Taco S, 2018, ICLR; Dominici F, 2002, AM J EPIDEMIOL, V156, P193, DOI 10.1093/aje/kwf062; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Doretto G, 2003, INT J COMPUT VISION, V51, P91, DOI 10.1023/A:1021669406132; Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547; Fr~echet M. R., 1948, ANN LINSTITUT HENRI, V10, P215; Hall B., 2015, LIE GROUPS LIE ALGEB, V222, DOI [DOI 10.1007/978-3-319-13467-3, 10.1017/, DOI 10.1017/, 10.1007/978-3-319-13467-3]; Helgason S., 1962, DIFFERENTIAL GEOMETR; Henaff M, 2016, PR MACH LEARN RES, V48; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hua X, 2008, NEUROIMAGE, V43, P458, DOI 10.1016/j.neuroimage.2008.07.013; Huang Z., 2016, ARXIV161105742; Huang Zhiwu, 2017, ASS ADV ARTIF INTELL, V2, P6; Jian B, 2007, NEUROIMAGE, V37, P164, DOI 10.1016/j.neuroimage.2007.03.074; Jing L, 2017, ARXIV170602761; Liu Jiaomin, 2009, Proceedings of the 2009 Second International Conference on Intelligent Networks and Intelligent Systems (ICINIS 2009), P15, DOI [10.1109/CVPRW.2009.5206744, 10.1109/ICINIS.2009.13]; KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81; Kim HJ, 2017, PROC CVPR IEEE, P3251, DOI 10.1109/CVPR.2017.346; Kim HJ, 2014, PROC CVPR IEEE, P2705, DOI 10.1109/CVPR.2014.352; Kondor R, 2018, PR MACH LEARN RES, V80; Koutnik Jan, 2014, ARXIV14023511; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lebanon Guy, 2015, RIEMANNIAN GEOMETRY; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lenglet C, 2006, IEEE T MED IMAGING, V25, P685, DOI 10.1109/TMI.2006.873299; Mammasis K, 2010, EURASIP J WIREL COMM, DOI 10.1155/2010/307265; Moakher M, 2006, VISUALIZATION AND PROCESSING OF TENSOR FIELDS, P285, DOI 10.1007/3-540-31272-2_17; Oliva Junier B, 2017, ARXIV170300381; PARK FC, 1995, J MECH DESIGN, V117, P36, DOI 10.1115/1.2826114; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; Peters J, 2017, ADAPT COMPUT MACH LE; Pujol S, 2015, J NEUROIMAGING, V25, P875, DOI 10.1111/jon.12283; Quirk C., 2005, P 43 ANN M ASS COMPU, P271; Salehian H, 2013, IEEE I CONF COMP VIS, P1793, DOI 10.1109/ICCV.2013.225; Sharma Shikhar, 2015, ARXIV151104119, P1; Sra Suvrit, 2011, TECHNICAL REPORT; Srivastava A., 2007, IEEE C COMP VIS PATT, P1; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Straub J, 2015, JMLR WORKSH CONF PRO, V38, P930; Triacca U, 2016, ECONOMETRICS, V4, DOI 10.3390/econometrics4030032; Tsay R.S., 2005, ANAL FINANCIAL TIME, V543; Turaga P., 2008, PROC IEEE C COMPUT V, P1; Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589; Vandereycken B, 2013, SIAM J OPTIMIZ, V23, P1214, DOI 10.1137/110845768; Wang ZZ, 2005, IEEE T MED IMAGING, V24, P1267, DOI 10.1109/TMI.2005.854516; Xu J, 2013, IEEE I CONF COMP VIS, P3376, DOI 10.1109/ICCV.2013.419; Yang YC, 2017, PR MACH LEARN RES, V70; Yu K., 2017, ARXIV170306817; Zacur E, 2014, J MATH IMAGING VIS, V50, P18, DOI 10.1007/s10851-013-0479-7	68	2	2	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003044
C	Chen, YB; Paiton, DM; Olshausen, BA		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Yubei; Paiton, Dylan M.; Olshausen, Bruno A.			The Sparse Manifold Transform	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				INDEPENDENT COMPONENT ANALYSIS; NATURAL IMAGES; REPRESENTATIONS; STATISTICS; EMERGENCE; FIELD; MODEL	We present a signal representation framework called the sparse manifold transform that combines key ideas from sparse coding, manifold learning, and slow feature analysis. It turns non-linear transformations in the primary sensory signal space into linear interpolations in a representational embedding space while maintaining approximate invertibility. The sparse manifold transform is an unsupervised and generative framework that explicitly and simultaneously models the sparse discreteness and low-dimensional manifold structure found in natural scenes. When stacked, it also models hierarchical composition. We provide a theoretical description of the transform and demonstrate properties of the learned representation on both synthetic data and natural videos.	[Chen, Yubei; Paiton, Dylan M.; Olshausen, Bruno A.] Univ Calif Berkeley, Redwood Ctr Theoret Neurosci, Berkeley, CA 94720 USA; [Chen, Yubei] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA; [Paiton, Dylan M.; Olshausen, Bruno A.] Univ Calif Berkeley, Vis Sci Grad Grp, Berkeley, CA 94720 USA; [Olshausen, Bruno A.] Univ Calif Berkeley, Helen Wills Neurosci Inst, Berkeley, CA 94720 USA; [Olshausen, Bruno A.] Univ Calif Berkeley, Sch Optometry, Berkeley, CA 94720 USA	University of California System; University of California Berkeley; University of California System; University of California Berkeley; University of California System; University of California Berkeley; University of California System; University of California Berkeley; University of California System; University of California Berkeley	Chen, YB (corresponding author), Univ Calif Berkeley, Redwood Ctr Theoret Neurosci, Berkeley, CA 94720 USA.; Chen, YB (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	yubeic@eecs.berkeley.edu	Paiton, Dylan/AIB-2839-2022		NIH/NEI [T32 EY007043];  [NSF-IIS-1718991];  [NSF-DGE-1106400]	NIH/NEI(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Eye Institute (NEI)); ; 	We thank Joan Bruna, Fritz Sommer, Ryan Zarcone, Alex Anderson, Brian Cheung and Charles Frye for many fruitful discussions; Karl Zipser for sharing computing resources; Eero Simoncelli and Chris Rozell for pointing us to some valuable references. This work is supported by NSF-IIS-1718991, NSF-DGE-1106400, and NIH/NEI T32 EY007043.	[Anonymous], 2010, PATTERN THEORY STOCH; [Anonymous], 2004, SPBG 04 S POINT BASE, DOI DOI 10.2312/SPBG/SPBG04/157-166; ATICK JJ, 1992, NEURAL COMPUT, V4, P196, DOI 10.1162/neco.1992.4.2.196; Atick JJ, 1990, NEURAL COMPUT, V2, P308, DOI 10.1162/neco.1990.2.3.308; Balle J., 2015, ARXIV151106281; Belkin M, 2002, ADV NEUR IN, V14, P585; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Berkes P, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000495; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Bruna J., 2013, ARXIV13114025; Cadieu CF, 2012, NEURAL COMPUT, V24, P827, DOI 10.1162/NECO_a_00247; Carlsson G, 2008, INT J COMPUT VISION, V76, P1, DOI 10.1007/s11263-007-0056-x; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; De Silva V., 2004, TECHNICAL REPORT; DiCarlo JJ, 2007, TRENDS COGN SCI, V11, P333, DOI 10.1016/j.tics.2007.06.010; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Dosovitskiy A, 2016, PROC CVPR IEEE, P4829, DOI 10.1109/CVPR.2016.522; Elhamifar Ehsan, 2011, ADV NEURAL INF PROCE, V24, P3; FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379; Foldiak P, 1991, NEURAL COMPUT, V3, P194, DOI 10.1162/neco.1991.3.2.194; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; Gilbert Anna C, 2017, ARXIV170508664; Henaff  OJ, 2018, COMPUTATIONAL SYSTEM; Hosoya  Haruo, 2016, NEURAL COMPUTATION; Huo X., 2007, RECENT ADV DATA MINI, P691, DOI 10.1142/9789812779861_0015; Hyvarinen A, 2000, NEURAL COMPUT, V12, P1705, DOI 10.1162/089976600300015312; Hyvarinen A, 2003, J OPT SOC AM A, V20, P1237, DOI 10.1364/JOSAA.20.001237; Hyvarinen A, 2001, NEURAL COMPUT, V13, P1527, DOI 10.1162/089976601750264992; Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5; Karhunen J, 1997, IEEE T NEURAL NETWOR, V8, P486, DOI 10.1109/72.572090; Koster U, 2010, NEURAL COMPUT, V22, P2308, DOI 10.1162/NECO_a_00010; Le Q. V., 2012, P 29 INT C MACH LEAR, P507; Lee AB, 2003, INT J COMPUT VISION, V54, P83, DOI 10.1023/A:1023705401078; Lee J., 2012, GRADUATE TEXTS MATH, DOI 10.1007/978-1-4419-9982-5; Lee TS, 2003, J OPT SOC AM A, V20, P1434, DOI 10.1364/JOSAA.20.001434; Lyu S., 2008, COMPUTER VISION PATT; Malo J, 2006, NETWORK-COMP NEURAL, V17, P85, DOI 10.1080/09548980500439602; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Olshausen BA, 2003, IEEE IMAGE PROC, P41; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Olshausen BA, 2013, PROC SPIE, V8651, DOI 10.1117/12.2013504; Osindero S, 2006, NEURAL COMPUT, V18, P381, DOI 10.1162/089976606775093936; Paiton Dylan M, 2016, P 9 EAI INT C BIOINS, P535; PERONA P, 1995, IEEE T PATTERN ANAL, V17, P488, DOI 10.1109/34.391394; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Rozell CJ, 2008, NEURAL COMPUT, V20, P2526, DOI 10.1162/neco.2008.03-07-486; Sabour Sara, 2017, PROC 31 INT C NEURAL; Shan  Honghao, 2013, ARXIV13126077; Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192; Silva J., 2006, ADV NEURAL INF PROCE, V18, P1241; SIMONCELLI EP, 1992, IEEE T INFORM THEORY, V38, P587, DOI 10.1109/18.119725; Simoncelli EP, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC444; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P2315, DOI 10.1098/rspb.1998.0577; Vladymyrov Max, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P256, DOI 10.1007/978-3-642-40994-3_17; Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938; Wu YN, 2010, INT J COMPUT VISION, V90, P198, DOI 10.1007/s11263-009-0287-0; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474	62	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005012
C	Ciccone, M; Gallieri, M; Masci, J; Osendorfer, C; Gomez, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ciccone, Marco; Gallieri, Marco; Masci, Jonathan; Osendorfer, Christian; Gomez, Faustino			NAIS-NET: Stable Deep Networks from Non-Autonomous Differential Equations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					This paper introduces Non-Autonomous Input-Output Stable Network (NAIS-Net), a very deep architecture where each stacked processing block is derived from a time-invariant non-autonomous dynamical system. Non-autonomy is implemented by skip connections from the block input to each of the unrolled processing stages and allows stability to be enforced so that blocks can be unrolled adaptively to a pattern-dependent processing depth. NAIS-Net induces non-trivial, Lipschitz input-output maps, even for an infinite unroll length. We prove that the network is globally asymptotically stable so that for every initial condition there is exactly one input-dependent equilibrium assuming tank units, and multiple stable equilibria for ReL units. An efficient implementation that enforces the stability under derived conditions for both fully-connected and convolutional layers is also presented. Experimental results show how NAIS-Net exhibits stability in practice, yielding a significant reduction in generalization gap compared to ResNets.	[Ciccone, Marco] Politecn Milan, NNAISENSE SA, Milan, Italy; [Gallieri, Marco; Masci, Jonathan; Osendorfer, Christian; Gomez, Faustino] NNAISENSE SA, Lugano, Switzerland	Polytechnic University of Milan	Ciccone, M (corresponding author), Politecn Milan, NNAISENSE SA, Milan, Italy.	marco.ciccone@polimi.it; marco@nnaisense.com; jonathan@nnaisense.com; christian@nnaisense.com; tino@nnaisense.com						Ascher U.M., 1998, SIAM; Baldi P, 1996, ADV NEUR IN, V8, P451; Battenberg E., 2017, CORR; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bengio Y., 2014, ARXIV14061078; Chang B, 2017, ARXIV171010348; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cisse M, 2017, PR MACH LEARN RES, V70; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; DOYA K, 1992, 1992 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS 1-6, P2777, DOI 10.1109/ISCAS.1992.230622; Figurnov M., 2017, CORR; Gomez Aidan N, 2017, ADV NEURAL INFORM PR, P2214; Graves Alex, 2016, CORR; Greff K, 2016, ARXIV161207771; Gregor K., 2010, P 27 INT C INT C MAC, P399, DOI DOI 10.5555/3104322.3104374; Haschke R, 2005, NEUROCOMPUTING, V64, P25, DOI 10.1016/j.neucom.2004.11.030; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter S, 1991, UNTERSUCHUNGEN DYNAM, V91; Horn R.A., 2012, MATRIX ANAL, DOI [DOI 10.1017/CBO9780511810817, 10.1017/CBO9780511810817]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jastrzebski S., 2017, ARXIV PREPRINT ARXIV; Kanai S, 2017, ADV NEUR IN, V30; Khalil HK., 2002, NONLINEAR SYSTEM, V3rd; Knight J. N., 2008, STABILITY ANAL RECUR; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lang KJ., 1988, P 1988 CONN MOD SUMM, P52; Larsson G., 2016, ARXIV PREPRINT ARXIV; LAURENT T, 2016, ARXIV PREPRINT ARXIV; Liao Q., 2016, CORR; Lu Y., 2018, FINITE LAYER NEURAL; Monti F., 2017, CVPR2017; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Ruthotto L, 2017, ARXIV170503341; Singh J, 2016, NEURAL NETWORKS, V74, P58, DOI 10.1016/j.neunet.2015.10.013; Sontag ED., 1998, MATH CONTROL THEORY, DOI [10.1007/978-1-4612-0577-7, DOI 10.1007/978-1-4612-0577-7]; Steil Jochen J, 1999, INPUT OUTPUT STABILI; Strogatz S.H., 2015, NONLINEAR DYNAMICS C, DOI DOI 10.1201/9780429492563; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Tallec C., 2018, ARXIV; Veit A., 2017, CORR; Vorontsov E, 2017, PR MACH LEARN RES, V70; Yann LeCun, 1998, MNIST DATABASE HANDW; Yoshida Y, 2017, ARXIV PREPRINT ARXIV; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179	52	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303006
C	Efroni, Y; Dalal, G; Scherrer, B; Mannor, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Efroni, Yonathan; Dalal, Gal; Scherrer, Bruno; Mannor, Shie			Multiple-Step Greedy Policies in Online and Approximate Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Multiple-step lookahead policies have demonstrated high empirical competence in Reinforcement Learning, via the use of Monte Carlo Tree Search or Model Predictive Control. In a recent work [5], multiple-step greedy policies and their use in vanilla Policy Iteration algorithms were proposed and analyzed. In this work, we study multiple-step greedy algorithms in more practical setups. We begin by highlighting a counter-intuitive difficulty, arising with soft-policy updates: even in the absence of approximations, and contrary to the 1-step-greedy case, monotonic policy improvement is not guaranteed unless the update stepsize is sufficiently large. Taking particular care about this difficulty, we formulate and analyze online and approximate algorithms that use such a multi-step greedy operator.	[Efroni, Yonathan; Dalal, Gal; Mannor, Shie] Technion Israel Inst Technol, Dept Elect Engn, Haifa, Israel; [Scherrer, Bruno] INRIA, Villers Les Nancy, France	Technion Israel Institute of Technology; Inria	Efroni, Y (corresponding author), Technion Israel Inst Technol, Dept Elect Engn, Haifa, Israel.	jonathan.efroni@gmail.com; gald@campus.technion.ac.il; bruno.scherrer@inria.fr; shie@ee.technion.ac.il		Mannor, Shie/0000-0003-4439-7647	Israel Science Foundation [1380/16]	Israel Science Foundation(Israel Science Foundation)	This work was partially funded by the Israel Science Foundation under contract 1380/16.	Bagnell JA, 2004, ADV NEUR IN, V16, P831; Bertsekas DP, 1995, PROCEEDINGS OF THE 34TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-4, P560, DOI 10.1109/CDC.1995.478953; Bouzy B, 2004, INT FED INFO PROC, V135, P159; Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810; Culotta A., 2010, ADV NEURAL INFORM PR, V23, P568; Efroni  Yonathan, 2018, ARXIV180203654; Ernst D, 2009, IEEE T SYST MAN CY B, V39, P517, DOI 10.1109/TSMCB.2008.2007630; Hellendoorn, 2005, IFAC P VOLUMES, V38, P354, DOI DOI 10.3182/20050703-6-CZ-1902.00280; Jiang N, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1181; Kakade S., 2002, P 19 INT C MACH LEAR; Konda VR, 1999, SIAM J CONTROL OPTIM, V38, P94, DOI 10.1137/S036301299731669X; Lazaric A., 2016, J MACHINE LEARNING R, V17, P583; Mnih V, 2016, PR MACH LEARN RES, V48; Munos R, 2007, SIAM J CONTROL OPTIM, V46, P541, DOI 10.1137/040614384; Munos  Remi, 2003, P 20 INT C INT C MAC, P560; Perkins  Steven, 2013, STOCHASTIC SYSTEMS, V2, P409; Petrik M., 2009, ADV NEURAL INFORM PR, P1265; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Scherrer Bruno, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8726, P35, DOI 10.1007/978-3-662-44845-8_3; Scherrer B, 2014, PR MACH LEARN RES, V32, P1314; Scherrer  Bruno, 2016, MARKOV DECISION PROC; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Sheppard B, 2002, ARTIF INTELL, V134, P241, DOI 10.1016/S0004-3702(01)00166-7; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Silver David, 2017, CORR; Strehl AL, 2009, J MACH LEARN RES, V10, P2413; Tamar Aviv, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P336, DOI 10.1109/ICRA.2017.7989043; Tesauro G, 1997, ADV NEUR IN, V9, P1068; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698	29	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305027
C	Flennerhag, S; Yin, HJ; Keane, J; Elliot, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Flennerhag, Sebastian; Yin, Hujun; Keane, John; Elliot, Mark			Breaking the Activation Function Bottleneck through Adaptive Parameterization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Standard neural network architectures are non-linear only by virtue of a simple element-wise activation function, making them both brittle and excessively large. In this paper, we consider methods for making the feed-forward layer more flexible while preserving its basic structure. We develop simple drop-in replacements that learn to adapt their parameterization conditional on the input, thereby increasing statistical efficiency significantly. We present an adaptive LSTM that advances the state of the art for the Penn Treebank and WikiText-2 word-modeling tasks while using fewer parameters and converging in less than half the number of iterations.	[Flennerhag, Sebastian; Yin, Hujun; Keane, John; Elliot, Mark] Univ Manchester, Manchester, Lancs, England; [Flennerhag, Sebastian; Yin, Hujun] Alan Turing Inst, London, England	University of Manchester	Flennerhag, S (corresponding author), Univ Manchester, Manchester, Lancs, England.; Flennerhag, S (corresponding author), Alan Turing Inst, London, England.	sflennerhag@turing.ac.uk; hujun.yin@manchester.ac.uk; john.keane@manchester.ac.uk; mark.elliot@manchester.ac.uk		Yin, Hujun/0000-0002-9198-5401	ESRC via the North West Doctoral Training Centre [ES/J500094/1]	ESRC via the North West Doctoral Training Centre	The authors would like to thank anonymous reviewers for their comments. This work was supported by ESRC via the North West Doctoral Training Centre, grant number ES/J500094/1.	Al-Shedivat M., 2018, INT C LEARN REPR, P1; Andrychowicz M, 2016, ADV NEUR IN, V29; Bengio Samy, 1995, OPTIMALITY BIOL ARTI, P6; Bengio Y., 2014, ARXIV14061078; Bengio Yoshua, 1991, LEARNING SYNAPTIC LE; Bertinetto Luca, 2016, NIPS; Brock A., 2018, ICLR, P1; Canziani A, 2016, ARXIV; Chung J., 2014, ARXIV14123555; Cooijmans Tim, 2016, INT C LEARN REPR; Cybenko George, 1989, MCSS; Dauphin YN, 2017, PR MACH LEARN RES, V70; Denil M., 2013, ADV NEURAL INFORM PR, P2148, DOI DOI 10.5555/2999792.2999852; Djork-Arn, ICLR 2016; Fernando Chrisantha, 2016, GECCO; Finn C, 2017, PR MACH LEARN RES, V70; Frankle Jonathan, 2018, ARXIV180303635; Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015; Gomez F, 2005, LECT NOTES COMPUT SC, V3697, P383; Grave Edouard, 2017, 5 INT C LEARN REPR I; Graves A, 2013, ARXIV13080850; Ha David, 2017, ICLR; Ha David, 2018, ICLR; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; Inan Hakan, 2017, P 5 INT C LEARN REPR; Jaderberg M, 2017, PR MACH LEARN RES, V70; Kingma D.P, P 3 INT C LEARNING R; Klambauer Gnter, 2017, SELF NORMALIZING NEU; Krause B., 2017, ARXIV170907432; Krause B., 2016, ARXIV PREPRINT ARXIV; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Lee Yoonho, 2017, INT C MACH LEARN; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Melis G, 2018, INT C LEARN REPR ICL; Merity Stephen, 2017, ICLR; Merity Stephen, 2018, INT C LEARN REPR; Mikolov T., 2012, GOOGLE; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Novak R., 2018, INT C LEARN REPR; Press Ofir, 2017, P 15 C EUR CH ASS CO, P157, DOI DOI 10.18653/V1/E17-2025; Radford A., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1704.01444; Ravi S., 2017, INT C LEARN REPR, P12; Saxe Andrew M, 2013, ARXIV13126120; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131; Stanley KO, 2009, ARTIF LIFE, V15, P185, DOI 10.1162/artl.2009.15.2.15202; Suarez Joseph, 2017, ADV NEURAL INFORM PR, P3269; Sutskever Ilya, 2011, P 28 INT C MACH LEAR; Wu Y., 2016, ADV NEURAL INFORM PR, P2864; Yang Z., 2018, BREAKING SOFTMAX BOT; Zaremba W., 2015, P INT C LEARN REPR S, P1310; Zilly J.G., 2016, ARXIV PREPRINT ARXIV; Zoph B., 2017, P1	55	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002030
C	Fu, J; Singh, A; Ghosh, D; Yang, L; Levine, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Fu, Justin; Singh, Avi; Ghosh, Dibya; Yang, Larry; Levine, Sergey			Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The design of a reward function often poses a major practical challenge to real-world applications of reinforcement learning. Approaches such as inverse reinforcement learning attempt to overcome this challenge, but require expert demonstrations, which can be difficult or expensive to obtain in practice. We propose variational inverse control with events (VICE), which generalizes inverse reinforcement learning methods to cases where full demonstrations are not needed, such as when only samples of desired goal states are available. Our method is grounded in an alternative perspective on control and reinforcement learning, where an agent's goal is to maximize the probability that one or more events will happen at some point in the future, rather than maximizing cumulative rewards. We demonstrate the effectiveness of our methods on continuous control tasks, with a focus on high-dimensional observations like images where rewards are hard or even impossible to specify.	[Fu, Justin; Singh, Avi; Ghosh, Dibya; Yang, Larry; Levine, Sergey] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Fu, J (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	justinfu@berkeley.edu; avisingh@berkeley.edu; dibyaghosh@berkeley.edu; larrywyang@berkeley.edu; svlevine@berkeley.edu			ONR Young Investigator Program award; National Science Foundation [IIS-1651843, IIS-1614653, IIS-1700696]; Berkeley DeepDrive	ONR Young Investigator Program award; National Science Foundation(National Science Foundation (NSF)); Berkeley DeepDrive	This research was supported by an ONR Young Investigator Program award, the National Science Foundation through IIS-1651843, IIS-1614653, and IIS-1700696, Berkeley DeepDrive, and donations from Google, Amazon, and NVIDIA.	Amodei Dario, 2016, ABS160606565 ARXIV; [Anonymous], ADV NEURAL INFORM PR; Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024; Ermon S., 2016, ADV NEURAL INFORM PR; Finn C., 2016, ICRA; Finn Chelsea, 2016, ABS161103852; Fu Justin, 2018, INT C LEARN REPR ICL; Haarnoja T., 2017, INT C MACH LEARN ICM; Hadfield-Menell Dylan, 2017, NIPS; Hadsell R., 2017, C ROB LEARN CORL; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Kappen H. J., 2009, OPTIMAL CONTROL GRAP; Levine Sergey, 2016, J MACHINE LEARNING J; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Ng A. Y., 2000, INT C MACH LEARN; O'Donoghue B., 2016, COMBINING POLICY GRA; Peng X.B., 2017, ABS171006537 CORR; Pinto L, 2016, IEEE INT CONF ROBOT, P3406, DOI 10.1109/ICRA.2016.7487517; Rawlik K, 2012, ROBOTICS SCI SYSTEMS; Russell SJ, 1995, ARTIF INTELL, V4th; Schulman J., 2015, INT C MACH LEARN ICM; Schulman J., 2017, EQUIVALENCE POLICY G; Singh S, 2010, P INT S INSP BIOL S; Sorg Jonathan, 2010, NIPS; Todorov E., 2007, ADV NEURAL INFORM PR; Todorov Emo, 2008, IEEE C DEC CONTR CDC; Toussaint Marc, 2009, INT C MACH LEARN ICM; Tung Hsiao-Yu Fish, 2018, C COMP VIS PATT REC; Ziebart B. D., 2008, AAAI C ART INT AAAI; Ziebart B. D., 2010, THESIS	30	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003013
C	Gao, H; Shou, Z; Zareian, A; Zhang, HW; Chang, SF		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gao, Hang; Shou, Zheng; Zareian, Alireza; Zhang, Hanwang; Chang, Shih-Fu			Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep neural networks suffer from over-fitting and catastrophic forgetting when trained with small data. One natural remedy for this problem is data augmentation, which has been recently shown to be effective. However, previous works either assume that intra-class variances can always be generalized to new classes, or employ naive generation methods to hallucinate finite examples without modeling their latent distributions. In this work, we propose Covariance-Preserving Adversarial Augmentation Networks to overcome existing limits of low-shot learning. Specifically, a novel Generative Adversarial Network is designed to model the latent distribution of each novel class given its related base counterparts. Since direct estimation of novel classes can be inductively biased, we explicitly preserve covariance information as the "variability" of base examples during the generation process. Empirical results show that our model can generate realistic yet diverse examples, leading to substantial improvements on the ImageNet benchmark over the state of the art.	[Gao, Hang; Shou, Zheng; Zareian, Alireza; Chang, Shih-Fu] Columbia Univ, New York, NY 10027 USA; [Zhang, Hanwang] Nanyang Technol Univ, Singapore, Singapore	Columbia University; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Gao, H (corresponding author), Columbia Univ, New York, NY 10027 USA.	hg2469@columbia.edu; zs2262@columbia.edu; az2407@columbia.edu; hanwangzhang@ntu.edu.sg; sc250@columbia.edu			U.S. DARPA AIDA Program [FA8750-18-2-0014]	U.S. DARPA AIDA Program	This work was supported by the U.S. DARPA AIDA Program No. FA8750-18-2-0014. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.	[Anonymous], 2016, ARXIV160305106; [Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Chen YC, 2018, IEEE T PATTERN ANAL, V40, P392, DOI 10.1109/TPAMI.2017.2666805; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Finn C, 2017, ARXIV170303400; Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459; Goldberger Jacob, 2005, ADV NEURAL INFORM PR, V17, P8, DOI DOI 10.1109/TCSVT.2013.2242640; Goodfellow I. J., 2013, ARXIV13126211, DOI DOI 10.1109/ISIE.2007; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gurumurthy S, 2017, IEEE C COMP VIS PATT, V1; Hariharan B, 2016, ARXIV160602819; Huang X., 2018, ARXIV180404732; Kingma D.P, P 3 INT C LEARNING R; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Koch Gregory, 2015, P ICML DEEP LEARN WO, V2; Lei D, 1996, J MANAGE, V22, P549, DOI 10.1177/014920639602200402; Mirza M., 2014, ARXIV PREPRINT ARXIV; Mroueh Y, 2017, ARXIV170208398; Mroueh Y., 2017, P ADV NEURAL INFORM, P2510, DOI DOI 10.5555/3294996.3295012; Odena A., 2016, ARXIV161009585; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Salakhutdinov R, 2012, JMLR WORKSHOP C P, P195; Salimans T., 2016, ADV NEUR IN, P2234; Santoro A., 2016, ARXIV160506065; Shmelkov K., 2017, ARXIV170806977; Snell J., 2017, ADV NEURAL INFORM PR, P4080; Sung Flood, 2017, ARXIV171106025; TOBIN J, 1958, ECONOMETRICA, V26, P24, DOI 10.2307/1907382; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vinyals Oriol, 2016, ARXIV160604080, P3630; Wang Y., 2018, ARXIV180105401; Xian Y., 2017, ARXIV171200981; Xu PL, 1998, GEOPHYS J INT, V135, P505, DOI 10.1046/j.1365-246X.1998.00652.x; Zhu J.-Y., 2017, ARXIV170310593	37	2	2	2	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301001
C	Garg, S; Sharan, V; Zhang, BH; Valiant, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Garg, Shivam; Sharan, Vatsal; Zhang, Brian Hu; Valiant, Gregory			A Spectral View of Adversarially Robust Features	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Given the apparent difficulty of learning models that are robust to adversarial perturbations, we propose tackling the simpler problem of developing adversarially robust features. Specifically, given a dataset and metric of interest, the goal is to return a function (or multiple functions) that 1) is robust to adversarial perturbations, and 2) has significant variation across the datapoints. We establish strong connections between adversarially robust features and a natural spectral property of the geometry of the dataset and metric of interest. This connection can be leveraged to provide both robust features, and a lower bound on the robustness of any function that has significant variance across the dataset. Finally, we provide empirical evidence that the adversarially robust features given by this spectral approach can be fruitfully leveraged to learn a robust (and accurate) model.	[Garg, Shivam; Sharan, Vatsal; Zhang, Brian Hu; Valiant, Gregory] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Garg, S (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	shivamgarg@stanford.edu; vsharan@stanford.edu; bhz@stanford.edu; gvaliant@stanford.edu			NSF [CCF-1704417, 1813049]; ONR Young Investigator Award [N00014-18-1-2295]	NSF(National Science Foundation (NSF)); ONR Young Investigator Award	This work was supported by NSF awards CCF-1704417 and 1813049, and an ONR Young Investigator Award (N00014-18-1-2295).	Athalye A., 2018, P 35 INT C MACH LEAR; Athalye Anish, 2017, ARXIV PREPRINT ARXIV; Ba J., 2017, P 3 INT C LEARN REPR; Brown TB, 2017, CSCV171209665 ARXIV; Bubeck S., 2018, ARXIV180510204; Chung F., 1997, AM MATH SOC, DOI 10.1090/cbms/092; Evtimov I., 2017, ARXIV PREPRINT ARXIV, V2, P4; Fawzi Alhussein, 2018, ARXIV180208686; Gilmer J., 2018, INT C LEARN REPR WOR; Goodfellow I. J., 2014, ARXIV14126572; Hein M, 2017, NIPS 17; KOLTER JZ, 2017, ARXIV171100851; Kurakin Alexey, 2016, WORKSHOP TRACK P, DOI DOI 10.48550/ARXIV.1607.02533; Madry A., 2018, ARXIV PREPRINT ARXIV; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Peck J, 2017, ADV NEURAL INFORM PR, P804; Raghunathan Aditi, 2018, INT C LEARN REPR; Schmidt Ludwig, 2018, ARXIV180411285; Sinha Aman, 2017, ARXIV PREPRINT ARXIV; Spielman DA, 2007, ANN IEEE SYMP FOUND, P29, DOI 10.1109/FOCS.2007.56; Szegedy C, 2013, 2 INT C LEARNING REP	21	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004067
C	Garg, VK; Kalai, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Garg, Vikas K.; Kalai, Adam			Supervising Unsupervised Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce a framework to transfer knowledge acquired from a repository of (heterogeneous) supervised datasets to new unsupervised datasets. Our perspective avoids the subjectivity inherent in unsupervised learning by reducing it to supervised learning, and provides a principled way to evaluate unsupervised algorithms. We demonstrate the versatility of our framework via rigorous agnostic bounds on a variety of unsupervised problems. In the context of clustering, our approach helps choose the number of clusters and the clustering algorithm, remove the outliers, and provably circumvent Kleinberg's impossibility result. Experiments across hundreds of problems demonstrate improvements in performance on unsupervised data with simple algorithms despite the fact our problems come from heterogeneous domains. Additionally, our framework lets us leverage deep networks to learn common features across many small datasets, and perform zero shot learning.	[Garg, Vikas K.] MIT, CSAIL, Cambridge, MA 02139 USA; [Kalai, Adam] Microsoft Res, Redmond, WA USA	Massachusetts Institute of Technology (MIT); Microsoft	Garg, VK (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	vgarg@csail.mit.edu; noreply@microsoft.com		Kalai, Adam Tauman/0000-0002-4559-8574				Ackerman  Margareta, 2008, NIPS; Balcan, 2015, C LEARN THEOR, P191; Balcan M., 2017, C LEARN THEOR, P213; Bousmalis Konstantinos, 2016, ADV NEURAL INFORM PR, P343; Chen WL, 2015, PR MACH LEARN RES, V37, P2285; Fusi  Nicolo, 2018, PROBABILISTIC MATRIX; Ganin Yaroslav, 2015, ICML; Gupta C, 2017, PR MACH LEARN RES, V70; Han S., 2016, P 4 INT C LEARN REPR, P1; HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075; KEARNS MJ, 1994, MACH LEARN, V17, P115, DOI 10.1007/BF00993468; Kingma D. P, 2014, ARXIV13126114; Kleinberg J. M., 2003, ADV NEURAL INFORM PR, V15, P463; Long MS, 2016, ADV NEUR IN, V29; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sinno J.P., 2009, IEEE T KNOWL DATA EN, V22, P1345, DOI [10.1109/TKDE.2009.191, DOI 10.1109/TKDE.2009.191]; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Thornton C, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P847, DOI 10.1145/2487575.2487629; Thrun S., 2012, LEARNING LEARN; Thrun Sebastian, 1995, ROBOTICS AUTONOMOUS; Tseng GC, 2007, BIOINFORMATICS, V23, P2247, DOI 10.1093/bioinformatics/btm320; Zadeh R.B., 2009, P 25 C UNCERTAINTY A, P639; Zeiler Matthew D, 2012, ARXIV12125701	28	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305004
C	Ghalebi, E; Mirzasoleiman, B; Grosu, R; Leskovec, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ghalebi, Elahe; Mirzasoleiman, Baharan; Grosu, Radu; Leskovec, Jure			Dynamic Network Model from Partial Observations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				STOCHASTIC BLOCKMODELS	Can evolving networks be inferred and modeled without directly observing their nodes and edges? In many applications, the edges of a dynamic network might not be observed, but one can observe the dynamics of stochastic cascading processes (e.g., information diffusion, virus propagation) occurring over the unobserved network. While there have been efforts to infer networks based on such data, providing a generative probabilistic model that is able to identify the underlying time-varying network remains an open question. Here we consider the problem of inferring generative dynamic network models based on network cascade diffusion data. We propose a novel framework for providing a non-parametric dynamic network model-based on a mixture of coupled hierarchical Dirichlet processes- based on data capturing cascade node infection times. Our approach allows us to infer the evolving community structure in networks and to obtain an explicit predictive distribution over the edges of the underlying network-including those that were not involved in transmission of any cascade, or are likely to appear in the future. We show the effectiveness of our approach using extensive experiments on synthetic as well as real-world networks.	[Ghalebi, Elahe; Grosu, Radu] TU Wien, Vienna, Austria; [Mirzasoleiman, Baharan; Leskovec, Jure] Stanford Univ, Stanford, CA 94305 USA	Technische Universitat Wien; Stanford University	Ghalebi, E (corresponding author), TU Wien, Vienna, Austria.	eghalebi@cps.tuwien.ac.at; baharanm@cs.stanford.edu; radu.grosu@tuwien.ac.at; jure@cs.stanford.edu			SNSF [P2EZP2_172187]	SNSF(Swiss National Science Foundation (SNSF))	This research was partially supported by SNSF P2EZP2_172187.	Ahmed A, 2009, P NATL ACAD SCI USA, V106, P11878, DOI 10.1073/pnas.0901910106; Airoldi E. M., 2009, 1 HARV U DEP STAT; Airoldi EM, 2008, J MACH LEARN RES, V9, P1981; ALDOUS DJ, 1981, J MULTIVARIATE ANAL, V11, P581, DOI 10.1016/0047-259X(81)90099-3; [Anonymous], 2010, PROC 16 ACM SIGKDD I, DOI DOI 10.1145/1835804.1835933; [Anonymous], [No title captured]; [Anonymous], P 29 INT C MACH LEAR; [Anonymous], 2011, ARXIV11050697; Bourigault S, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P573, DOI 10.1145/2835776.2835817; Cai D., 2016, ADV NEURAL INFORM PR, P4249; Dempsey, 2016, ARXIV160304571; Djolonga Josip, 2017, ARXIV170901006; Fox Emily B, STICKY HDP HMM BAYES; Gomez-Rodriguez M, 2012, ACM T KNOWL DISCOV D, V5, DOI 10.1145/2086737.2086741; Gomez-Rodriguez Manuel, 2012, P 29 INT COFERENCE I, P1587; Gomez-Rodriguez Manuel, 2011, ABS11050697 CORR; Herlau T., 2016, P 30 INT C NEUR INF, P4260; Hodas Nathan Oken, 2013, CORRABS13085015; Hoff PD, 2002, J AM STAT ASSOC, V97, P1090, DOI 10.1198/016214502388618906; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Ishiguro K., 2010, ADV NEURAL INFORM PR, P919; Kemp C., 2006, AAAI, V3, P5; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Leskovec J., 2007, P 24 INT C MACHINE L, P497, DOI [http://doi.acm.org/10.1145/1273496.1273559, DOI 10.1145/1273496.1273559]; Leskovec J., 2005, P 11 ACM SIGKDD INT, P177, DOI DOI 10.1145/1081870.1081893; Leskovec J, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P497; Li C, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P577, DOI 10.1145/3038912.3052643; Lloyd J., 2012, ADV NEURAL INFORM PR, P998; Lyons R, 2003, PUBL MATH-PARIS, P167; Miller Kurt T., 2009, NONPARAMETRIC LATENT, P1276; Myers Seth, 2010, P INT C ADV NEURAL I, P1741; Namaki A, 2011, PHYSICA A, V390, P3835, DOI 10.1016/j.physa.2011.06.033; Palla K, 2016, BAYESIAN NONPARAMETR; Rodriguez Manuel Gomez, 2013, P 6 ACM INT C WEB SE, P23, DOI DOI 10.1145/2433396.2433402; Rodriguez MG, 2012, ARXIV12051671; Snijders TAB, 1997, J CLASSIF, V14, P75, DOI 10.1007/s003579900004; Spielman DA, 2014, SIAM J MATRIX ANAL A, V35, P835, DOI 10.1137/090771430; Wang Jia, 2017, ARXIV171110162; Williamson RC, 2016, J MACH LEARN RES, V17, P1	39	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004042
C	Gower, RM; Hanzely, F; Richtarik, P; Stich, SU		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gower, Robert M.; Hanzely, Filip; Richtarik, Peter; Stich, Sebastian U.			Accelerated Stochastic Matrix Inversion: General Theory and Speeding up BFGS Rules for Faster Second-Order Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				QUASI-NEWTON METHODS; ALGORITHM; EFFICIENCY	We present the first accelerated randomized algorithm for solving linear systems in Euclidean spaces. One essential problem of this type is the matrix inversion problem. In particular, our algorithm can be specialized to invert positive definite matrices in such a way that all iterates (approximate solutions) generated by the algorithm are positive definite matrices themselves. This opens the way for many applications in the field of optimization and machine learning. As an application of our general theory, we develop the first accelerated (deterministic and stochastic) quasi-Newton updates. Our updates lead to provably more aggressive approximations of the inverse Hessian, and lead to speed-ups over classical non-accelerated rules in numerical experiments. Experiments with empirical risk minimization show that our rules can accelerate training of machine learning models.	[Gower, Robert M.] Telecom ParisTech, Paris, France; [Hanzely, Filip; Richtarik, Peter] KAUST, Thuwal, Saudi Arabia; [Stich, Sebastian U.] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Richtarik, Peter] Univ Edinburgh, Moscow Inst Phys & Technol, Edinburgh, Midlothian, Scotland	IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; King Abdullah University of Science & Technology; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; University of Edinburgh	Gower, RM (corresponding author), Telecom ParisTech, Paris, France.	robert.gower@telecom-paristech.fr; filip.hanzely@kaust.edu.sa; peter.richtarik@kaust.edu.sa; sebastian.stich@epfl.ch	Gower, Robert Mansel/Y-8838-2019; Richtarik, Peter/O-5797-2018	Gower, Robert Mansel/0000-0002-2268-9780; 				Agarwal N, 2017, J MACH LEARN RES, V18; Berahas A.L., 2016, ADV NEURAL INFORM PR, P1055; Berahas Albert S, 2017, ABS170506211 CORR; BROYDEN CG, 1967, MATH COMPUT, V21, P368, DOI 10.2307/2003239; Byrd RH, 2016, SIAM J OPTIMIZ, V26, P1008, DOI 10.1137/140954362; Byrd RH, 2011, SIAM J OPTIMIZ, V21, P977, DOI 10.1137/10079923X; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Curtis FE, 2016, PR MACH LEARN RES, V48; DESOER CA, 1963, J SOC IND APPL MATH, V11, P442, DOI 10.1137/0111031; FLETCHER R, 1970, COMPUT J, V13, P317, DOI 10.1093/comjnl/13.3.317; GOLDFARB D, 1970, MATH COMPUT, V24, P23, DOI 10.2307/2004873; Gower R.M., 2015, ARXIV151206890; Gower RM, 2016, PR MACH LEARN RES, V48; Gower RM, 2017, SIAM J MATRIX ANAL A, V38, P1380, DOI 10.1137/16M1062053; Gower RM, 2015, SIAM J MATRIX ANAL A, V36, P1660, DOI 10.1137/15M1025487; Kaczmarz S., 1937, B INT ACAD POL SCI L, V35, P355; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Liu J, 2015, MATH COMPUT, V85, P153, DOI 10.1090/mcom/2971; Loizou N., 2017, ARXIV171209677; Mokhtari A, 2015, J MACH LEARN RES, V16, P3151; Moritz P, 2016, JMLR WORKSH CONF PRO, V51, P249; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nesterov Y, 2017, SIAM J OPTIMIZ, V27, P110, DOI 10.1137/16M1060182; Pedersen Gert K, 1996, GRADUATE TEXTS MATH; Pilanci M, 2017, SIAM J OPTIMIZ, V27, P205, DOI 10.1137/15M1021106; RICHTARIK P, 2017, ARXIV170601108; Richtarik  Peter, 2017, STOCHASTIC REF UNPUB; Schraudolph N. N., 2007, PROC 11 INT C ARTIF, P436; SHANNO DF, 1970, MATH COMPUT, V24, P647, DOI 10.2307/2004840; Stich SU, 2016, MATH PROGRAM, V156, P549, DOI 10.1007/s10107-015-0908-z; Stich Sebastian U, 2014, THESIS; Strohmer T, 2009, J FOURIER ANAL APPL, V15, P262, DOI 10.1007/s00041-008-9030-4; Tu S, 2017, PR MACH LEARN RES, V70; Wang X, 2017, SIAM J OPTIMIZ, V27, P927, DOI 10.1137/15M1053141; Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3; Xu P., 2016, PROC INT C NEURAL IN, P3000; Xu  Peng, 2017, ARXIV170807164	38	2	2	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301059
C	Han, YL; Gmytrasiewicz, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Han, Yanlin; Gmytrasiewicz, Piotr			Learning Others' Intentional Models in Multi-Agent Settings Using Interactive POMDPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Interactive partially observable Markov decision processes (I-POMDPs) provide a principled framework for planning and acting in a partially observable, stochastic and multi-agent environment. It extends POMDPs to multi-agent settings by including models of other agents in the state space and forming a hierarchical belief structure. In order to predict other agents' actions using I-POMDPs, we propose an approach that effectively uses Bayesian inference and sequential Monte Carlo sampling to learn others' intentional models which ascribe to them beliefs, preferences and rationality in action selection. Empirical results show that our algorithm accurately learns models of the other agent and has superior performance than methods that use subintentional models. Our approach serves as a generalized Bayesian learning algorithm that learns other agents' beliefs, strategy levels, and transition, observation and reward functions. It also effectively mitigates the belief space complexity due to the nested belief hierarchy.	[Han, Yanlin; Gmytrasiewicz, Piotr] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA	University of Illinois System; University of Illinois Chicago; University of Illinois Chicago Hospital	Han, YL (corresponding author), Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.	yhan37@uic.edu; piotr@uic.edu						[Anonymous], 1996, THESIS; Del Moral P., 1996, MARKOV PROCESSES REL, V2, P555; Doshi P., 2009, J ARTIFICIAL INTELLI, V34, P297; Doshi-Velez F, 2015, IEEE T PATTERN ANAL, V37, P394, DOI 10.1109/TPAMI.2013.191; Doucet A, 2001, STAT ENG IN, P3; Fudenberg D., 1998, THEORY LEARNING GAME, V2; Gilks W. R., 1996, MARKOV CHAIN MONTE C, V1, P19; Gmytrasiewicz PJ, 2005, J ARTIF INTELL RES, V24, P49, DOI 10.1613/jair.1579; Gmytrasiewicz PJ, 2000, AUTON AGENT MULTI-AG, V3, P319, DOI 10.1023/A:1010028119149; Harsanyi John, 1967, MANAGE SCI, V50, P159, DOI [10.1287/mnsc.14.3.159, DOI 10.1287/MNSC.14.3.159]; Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X; Karkus P., 2017, ADV NEURAL INFORM PR, P4697; Liu M., 2011, P 28 INT C MACH LEAR, P769; Panella A, 2016, AAAI CONF ARTIF INTE, P2530; PAPADIMITRIOU CH, 1987, MATH OPER RES, V12, P441, DOI 10.1287/moor.12.3.441	16	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000016
C	Hannah, R; Liu, YL; O'Connor, D; Yin, W		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hannah, Robert; Liu, Yanli; O'Connor, Daniel; Yin, Wotao			Breaking the Span Assumption Yields Fast Finite-Sum Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we show that SVRG and SARAH can be modified to be fundamentally faster than all of the other standard algorithms that minimize the sum of n smooth functions, such as SAGA, SAG, SDCA, and SDCA without duality. Most finite sum algorithms follow what we call the "span assumption": Their updates are in the span of a sequence of component gradients chosen in a random IID fashion. In the big data regime, where the condition number kappa = O(n), the span assumption prevents algorithms from converging to an approximate solution of accuracy epsilon in less than n ln(1/epsilon) iterations. SVRG and SARAH do not follow the span assumption since they are updated with a hybrid of full-gradient and component-gradient information. We show that because of this, they can be up to Omega (1 + (ln(n/kappa))+) times faster. In particular, to obtain an accuracy epsilon = 1/n(alpha) for kappa = n(beta) and alpha, beta is an element of (0, 1), modified SVRG requires O(n) iterations, whereas algorithms that follow the span assumption require O(n ln(n)) iterations. Moreover, we present lower bound results that show this speedup is optimal, and provide analysis to help explain why this speedup exists. With the understanding that the span assumption is a point of weakness of finite sum algorithms, future work may purposefully exploit this to yield faster algorithms in the big data regime.	[Hannah, Robert] Univ Calif Los Angeles, Dept Math, Los Angeles, CA 90024 USA; Univ San Francisco, Dept Math, San Francisco, CA 94117 USA	University of California System; University of California Los Angeles; University of San Francisco	Hannah, R (corresponding author), Univ Calif Los Angeles, Dept Math, Los Angeles, CA 90024 USA.	RobertHannah89@gmail.com; yanli@math.ucla.edu; daniel.v.oconnor@gmail.com; WotaoYin@math.ucla.edu			AFOSR MURI [FA9550-18-1-0502]; NSF [DMS-1720237]; ONR [N000141712162]	AFOSR MURI(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)MURI); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	This work was supported in part by grants: AFOSR MURI FA9550-18-1-0502, NSF DMS-1720237, and ONR N000141712162.	Allen-Zhu Z, 2018, PR MACH LEARN RES, V80; Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448; [Anonymous], 2013, INTRO LECT CONVEX OP; [Anonymous], ARXIV150702000; Arjevani Y., 2014, ARXIV14106387; Arjevani Y., 2016, ARXIV160503529CSMATH; Arjevani Y., 2016, ARXIV160609333CSMATH; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Defazio A, 2016, ADV NEUR IN, V29; Defazio AJ, 2014, PR MACH LEARN RES, V32, P1125; Eberts M., 2011, ADV NEURAL INFORM PR, V24, P1539; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Lan GH, 2018, MATH PROGRAM, V171, P167, DOI 10.1007/s10107-017-1173-0; Lin H., 2015, ARXIV150602186; LIN Q., 2014, ARXIV14071296; Mairal J., 2013, P 30 INT C INT C MAC, P783; Nguyen L. M., 2017, ARXIV170300102CSMATH; Roux N.L., 2012, ADV NEURAL INFORM PR, V25, P2663; Shalev-Shwartz S, 2016, MATH PROGRAM, V155, P105, DOI 10.1007/s10107-014-0839-0; Shalev-Shwartz Shai, 2015, ARXIV PREPRINT ARXIV; Sridharan K., 2009, ADV NEURAL INFORM PR, P1545; Woodworth Blake, 2016, ARXIV160508003; Xiao L., 2014, ARXIV14034699	24	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302033
C	Havens, AJ; Jiang, ZH; Sarkar, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Havens, Aaron J.; Jiang, Zhanhong; Sarkar, Soumik			Online Robust Policy Learning in the Presence of Unknown Adversaries	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The growing prospect of deep reinforcement learning (DRL) being used in cyber-physical systems has raised concerns around safety and robustness of autonomous agents. Recent work on generating adversarial attacks have shown that it is computationally feasible for a bad actor to fool a DRL policy into behaving sub optimally. Although certain adversarial attacks with specific attack models have been addressed, most studies are only interested in off-line optimization in the data space (e.g., example fitting, distillation). This paper introduces a Meta-Learned Advantage Hierarchy (MLAH) framework that is attack model-agnostic and more suited to reinforcement learning, via handling the attacks in the decision space (as opposed to data space) and directly mitigating learned bias introduced by the adversary. In MLAH, we learn separate sub-policies (nominal and adversarial) in an online manner, as guided by a supervisory master agent that detects the presence of the adversary by leveraging the advantage function for the sub-policies. We demonstrate that the proposed algorithm enables policy learning with significantly lower bias as compared to the state-of-the-art policy learning approaches even in the presence of heavy state information attacks. We present algorithm analysis and simulation results using popular OpenAI Gym environments.	[Havens, Aaron J.; Jiang, Zhanhong; Sarkar, Soumik] Iowa State Univ, Dept Mech Engn, Ames, IA 50011 USA	Iowa State University	Havens, AJ (corresponding author), Iowa State Univ, Dept Mech Engn, Ames, IA 50011 USA.	ajhavens@iastate.edu; zhjiang@iastate.edu; soumiks@iastate.edu	SARKAR, SOUMIK/T-9707-2018		U.S. AFOSR under the YIP [FA9550-17-1-0220]	U.S. AFOSR under the YIP	This work has been supported in part by the U.S. AFOSR under the YIP grant FA9550-17-1-0220. Any opinions, findings and conclusions or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the sponsoring agencies.	Antoniou A, 2016, IEEE IJCNN, P2879, DOI 10.1109/IJCNN.2016.7727563; Brockman G., 2016, OPENAI GYM; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chi-Sheng Shih, 2016, IET Cyber-Physical Systems: Theory & Applications, V1, P3, DOI 10.1049/iet-cps.2016.0025; Frans K., 2017, ABS170408834 CORR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow Ian, 2017, 5 INT C LEARN REPR I; Havens Aaron J, 2018, ARXIV180706064; Kos Jernej, 2017, ARXIV PREPRINT ARXIV; Levine S, 2016, ARXIV161001283; Levine S, 2016, J MACH LEARN RES, V17; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Lin Y.-C., 2017, ARXIV E PRINTS; Madry Aleksander, 2017, ARXIV; Mandlekar Ajay, 2017, IEEE INT C INT ROB S; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Pattanaik A., 2017, ARXIV E PRINTS; Pinto L., 2017, ARXIV E PRINTS; Rawat D.B., 2015, SOUTHEASTCON 2015 IE, P1; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Schulman John, 2015, ARXIV150602438; Sutton R. S., 1992, IEEE Control Systems Magazine, V12, P19, DOI 10.1109/37.126844; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Van Hasselt Hado, 2016, P AAAI C ART INT, V30	27	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004047
C	Hong, S; Yan, XC; Huang, T; Lee, H		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hong, Seunghoon; Yan, Xinchen; Huang, Thomas; Lee, Honglak			Learning Hierarchical Semantic Image Manipulation through Structured Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Understanding, reasoning, and manipulating semantic concepts of images have been a fundamental research problem for decades. Previous work mainly focused on direct manipulation on natural image manifold through color strokes, key-points, textures, and holes-to-fill. In this work, we present a novel hierarchical framework for semantic image manipulation. Key to our hierarchical framework is that we employ structured semantic layout as our intermediate representation for manipulation. Initialized with coarse-level bounding boxes, our structure generator first creates pixel-wise semantic layout capturing the object shape, object-object interactions, and object-scene relations. Then our image generator fills in the pixel-level textures guided by the semantic layout. Such framework allows a user to manipulate images at object-level by adding, removing, and moving one bounding box at a time. Experimental evaluations demonstrate the advantages of the hierarchical manipulation framework over existing image generation and context hole-filing models, both qualitatively and quantitatively. Benefits of the hierarchical framework are further demonstrated in applications such as semantic object manipulation, interactive image editing, and data-driven image manipulation.	[Hong, Seunghoon; Yan, Xinchen; Huang, Thomas; Lee, Honglak] Univ Michigan, Ann Arbor, MI 48109 USA; [Lee, Honglak] Google Brain, Mountain View, CA USA	University of Michigan System; University of Michigan; Google Incorporated	Hong, S (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	hongseu@umich.edu; xcyan@umich.edu; thomaseh@umich.edu; honglak@umich.edu	Yan, Xinchen/AAY-4481-2020; Hong, Seunghoon/AAF-9628-2019	Yan, Xinchen/0000-0003-1019-5537; 	ONR [N00014-13-1-0762]; NSF CAREER [IIS-1453651]; DARPA Explainable AI (XAI) program [313498]; Sloan Research Fellowship; Adobe Research Fellowship; Google PhD Fellowship	ONR(Office of Naval Research); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); DARPA Explainable AI (XAI) program; Sloan Research Fellowship(Alfred P. Sloan Foundation); Adobe Research Fellowship; Google PhD Fellowship(Google Incorporated)	This work was supported in part by ONR N00014-13-1-0762, NSF CAREER IIS-1453651, DARPA Explainable AI (XAI) program #313498, Sloan Research Fellowship, and Adobe Research Fellowship and Google PhD Fellowship to X. Yan.	[Anonymous], 2017, ICCV; [Anonymous], 2016, NIPS; [Anonymous], 2017, ICCV; Ba J., 2017, P 3 INT C LEARN REPR; Chen L.-C., 2018, ARXIV 1802 02611; Cordts M., 2016, CVPR; Denton E. L., 2016, ARXIV161106430; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gupta A., 2010, ECCV; Hoiem D, 2008, INT J COMPUT VISION, V80, P3, DOI 10.1007/s11263-008-0137-5; Hoiem Derek, 2005, CVPR; Hong S., 2018, CVPR; Isola P., 2017, CVPR; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Li Y., 2017, CVPR; Liu C., 2016, DENSE IMAGE CORRES C, V33, P15, DOI DOI 10.1007/978-3-319-23048-1_2; Liu M. -Y., 2017, NIPS; Pathak D., 2016, CVPR; Radford A., 2016, ICLR; Reed S., 2016, NEURAL INFORM PROCES; Sangkloy P., 2017, CVPR; Simonyan K., 2015, ICLR; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; Wang T.-C., 2017, ICCV; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xian Wenqi, 2018, CVPR; Yan X, 2016, ECCV; Yang Jianwei, 2017, INT C LEARN REPR; Yang Y, 2012, IEEE T PATTERN ANAL, V34, P1731, DOI 10.1109/TPAMI.2011.208; Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728; Zhou B., 2017, CVPR; Zhu J. -Y., 2016, ECCV	33	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302070
C	Hoskins, JG; Musco, C; Musco, C; Tsourakakis, CE		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hoskins, Jeremy G.; Musco, Cameron; Musco, Christopher; Tsourakakis, Charalampos E.			Inferring Networks From Random Walk-Based Node Similarities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				GRAPH; ALGORITHM; TREES; RESISTANCE	Digital presence in the world of online social media entails significant privacy risks [31, 56]. In this work we consider a privacy threat to a social network in which an attacker has access to a subset of random walk-based node similarities, such as effective resistances (i.e., commute times) or personalized PageRank scores. Using these similarities, the attacker seeks to infer as much information as possible about the network, including unknown pairwise node similarities and edges. For the effective resistance metric, we show that with just a small subset of measurements, one can learn a large fraction of edges in a social network. We also show that it is possible to learn a graph which accurately matches the underlying network on all other effective resistances. This second observation is interesting from a data mining perspective, since it can be expensive to compute all effective resistances or other random walk-based similarities. As an alternative, our graphs learned from just a subset of effective resistances can be used as surrogates in a range of applications that use effective resistances to probe graph structure, including for graph clustering, node centrality evaluation, and anomaly detection. We obtain our results by formalizing the graph learning objective mathematically, using two optimization problems. One formulation is convex and can be solved provably in polynomial time. The other is not, but we solve it efficiently with projected gradient and coordinate descent. We demonstrate the effectiveness of these methods on a number of social networks obtained from Facebook. We also discuss how our methods can be generalized to other random walk-based similarities, such as personalized PageRank scores. Our code is available at https://github.com/cnmusco/graph-similarity-learning.	[Hoskins, Jeremy G.] Yale Univ, Dept Math, New Haven, CT 06520 USA; [Musco, Cameron] Microsoft Res, Cambridge, MA USA; [Musco, Christopher] Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA; [Tsourakakis, Charalampos E.] Boston Univ, Dept Comp Sci, 111 Cummington St, Boston, MA 02215 USA; [Tsourakakis, Charalampos E.] Harvard Univ, Boston, MA 02115 USA	Yale University; Microsoft; Princeton University; Boston University; Harvard University	Hoskins, JG (corresponding author), Yale Univ, Dept Math, New Haven, CT 06520 USA.	jeremy.hoskins@yale.edu; camusco@microsoft.com; cmusco@cs.princeton.edu; ctsourak@bu.edu		Musco, Christopher/0000-0002-3118-4848				Abebe Rediet, 2014, PRIVATE LINK PREDICT; Adamic LA, 2003, SOC NETWORKS, V25, P211, DOI 10.1016/S0378-8733(03)00009-1; Al Hasan M, 2011, SOCIAL NETWORK DATA ANALYTICS, P243; Andersen R., 2006, 47 ANN S FDN COMP SC; Angluin D, 2008, J COMPUT SYST SCI, V74, P546, DOI 10.1016/j.jcss.2007.06.006; Backstrom L., 2007, PROC 16 INT C WORLD, P181, DOI DOI 10.1145/1242572.1242598; BATAGELJ V, 1990, INT J COMPUT MATH, V34, P171, DOI 10.1080/00207169008803874; Ben-Hamou Anna, 2018, P ACM SIAM S DISCR A; Blondel VD, 2004, SIAM REV, V46, P647, DOI 10.1137/S0036144502415960; Boyd S., 2004, SIAM REV; Brin Sergey, 1999, TECHNICAL REPORT; Castro R, 2004, STAT SCI, V19, P499, DOI 10.1214/088342304000000422; Chandra A. K., 1996, Computational Complexity, V6, P312, DOI 10.1007/BF01270385; Chen D, 2010, PROC APPL MATH, V135, P1309; Cooper C, 2014, SOC NETW ANAL MIN, V4, DOI 10.1007/s13278-014-0168-6; CULBERSON JC, 1989, INFORM PROCESS LETT, V30, P215, DOI 10.1016/0020-0190(89)90216-0; Desper R, 1999, J COMPUT BIOL, V6, P37, DOI 10.1089/cmb.1999.6.37; FELSENSTEIN J, 1985, EVOLUTION, V39, P783, DOI 10.1111/j.1558-5646.1985.tb00420.x; Fouss F, 2007, IEEE T KNOWL DATA EN, V19, P355, DOI 10.1109/TKDE.2007.46; Ghosh A, 2008, SIAM REV, V50, P37, DOI 10.1137/050645452; Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233; Grant M., 2014, CVX MATLAB SOFTWARE; Haveliwala T., 2003, IEEE T KNOWLEDGE DAT; Jeh G, 2002, P 8 ACM SIGKDD INT C, V02, P538; Jeh G., 2003, P 12 INT C WORLD WID, P271, DOI DOI 10.1145/775152.775191; Kalofolias V., 2016, J MACHINE LEARNING R; Kannan Sampath, 2015, INT C AUT LANG PROGR; Katz L., 1953, PSYCHOMETRIKA, V18, P39, DOI [10.1007/BF02289026, DOI 10.1007/BF02289026, DOI 10.1016/j.clinph.2016.12.016]; Katzir L., 2011, P 20 INT C WORLD WID; KLEIN DJ, 1993, J MATH CHEM, V12, P81, DOI 10.1007/BF01164627; Kleinberg J. M., 1999, Computing and Combinatorics. 5th Annual International Conference, COCOON'99. Proceedings (Lecture Notes in Computer Science Vol.1627), P1; Korolova A., 2008, P 17 ACM C INF KNOWL, P289; Leskovec J, 2010, P 17 INT C WORLD WID; Leskovec J, 2014, SNAP DATASETS STANFO; Leskovec J., 2012, P 25 INT C NEUR INF, P539, DOI DOI 10.1109/ICDM.2012.159; Liben-Nowell D, 2007, J AM SOC INF SCI TEC, V58, P1019, DOI 10.1002/asi.20591; Mcrae BH, 2008, ECOLOGY, V89, P2712, DOI 10.1890/07-1861.1; McRae BH, 2006, EVOLUTION, V60, P1551, DOI 10.1111/j.0014-3820.2006.tb00500.x; MOSEK ApS, 2017, MOSEK OPT SUIT; Perozzi Bryan, 2014, P 20 ACM INT C KNOWL; Pilliod David, 2015, EFFECTS CHANGING CLI, V5; Rattigan M., 2005, SIGKDD EXPLORATIONS, V7, P41, DOI DOI 10.1145/1117454.1117460; Reyzin L, 2007, INFORM PROCESS LETT, V101, P98, DOI 10.1016/j.ipl.2006.08.013; Saerens M, 2004, LECT NOTES COMPUT SC, V3201, P371; Sarkar P., 2007, UAI, P335; Spielman DA, 2011, SIAM J COMPUT, V40, P1913, DOI 10.1137/080734029; Spielman Daniel A., 2012, U LECT; Stone EA, 2009, LINEAR ALGEBRA APPL, V431, P1869, DOI 10.1016/j.laa.2009.06.024; Sun J, 2006, SIAM REV, V48, P681, DOI 10.1137/S0036144504443821; Tong H, 2006, FAST RANDOM WALK RES; Tsourakakis C. E., 2017, ARXIV170907308; von Luxburg U, 2014, J MACH LEARN RES, V15, P1751; Von Luxburg Ulrike, 2010, ADV NEURAL INFORM PR; Wittmann Dominik M., 2009, THEORETICAL COMPUTER; Yen Luh, 2007, ADV KNOWLEDGE DISCOV; Zheleva Elena, 2012, SYNTHESIS LECT DATA, V3, P1	56	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303068
C	Hsieh, YP; Kavis, A; Rolland, P; Cevher, V		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hsieh, Ya-Ping; Kavis, Ali; Rolland, Paul; Cevher, Volkan			Mirrored Langevin Dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider the problem of sampling from constrained distributions, which has posed significant challenges to both non-asymptotic analysis and algorithmic design. We propose a unified framework, which is inspired by the classical mirror descent, to derive novel first-order sampling schemes. We prove that, for a general target distribution with strongly convex potential, our framework implies the existence of a first-order algorithm achieving (O) over tilde (epsilon(-2)d) convergence, suggesting that the state-of-the-art (O) over tilde (epsilon(-6)d(5)) can be vastly improved. With the important Latent Dirichlet Allocation (LDA) application in mind, we specialize our algorithm to sample from Dirichlet posteriors, and derive the first non-asymptotic (O) over tilde (epsilon(-2)d(2)) rate for first-order sampling. We further extend our framework to the mini-batch setting and prove convergence rates when only stochastic gradients are available. Finally, we report promising experimental results for LDA on real datasets.	[Hsieh, Ya-Ping; Kavis, Ali; Rolland, Paul; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Hsieh, YP (corresponding author), Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland.	ya-ping.hsieh@epfl.ch; ali.kavis@epfl.ch; paul.rolland@epfl.ch; volkan.cevher@epfl.ch			European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [725594]	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC))	This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement no 725594 - time-data).	Ahn S., 2012, P 29 INT COF INT C M, P1771; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Brosse Nicolas, 2017, P MACHINE LEARNING R, V65, P319; BUBECK S, 2015, ARXIV150702564; Chen C., 2015, NIPS; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Cheng X., 2017, ARXIV170703663; Cheng X., 2018, P 29 INT C ALG LEARN, P186; Dai B, 2016, JMLR WORKSH CONF PRO, V51, P985; Dalalyan A. S., 2017, ARXIV171000095; Dalalyan AS, 2017, J R STAT SOC B, V79, P651, DOI 10.1111/rssb.12183; Ding N., 2014, ADV NEURAL INFORM PR, V2, P3203; DURMUS A., 2016, ADV NEURAL INFORM PR, P2047; Durmus  Alain, 2018, ARXIV180209188; Dwivedi R., 2018, ARXIV180102309; Krichene Walid, 2017, ADV NEURAL INFORM PR, P6799; Lan SW, 2016, ADV COMPUT VIS PATT, P25, DOI 10.1007/978-3-319-45026-1_2; Liu C., 2016, ADV NEURAL INFORM PR, P3009; Luu T.D., 2017, SAMPLING NONSMOOTH D; Ma Y.-A., 2015, P 28 INT C NEURAL IN, P2917; Mandelbrot B. B., 1983, FRACTAL GEOMETRY NAT, V173; McCann RJ, 1995, DUKE MATH J, V80, P309, DOI 10.1215/S0012-7094-95-08013-2; Mertikopoulos P, 2018, SIAM J OPTIMIZ, V28, P163, DOI 10.1137/16M1105682; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Patterson S., 2013, P 26 INT C NEUR INF, P3102; Raginsky M, 2012, IEEE DECIS CONTR P, P6793, DOI 10.1109/CDC.2012.6426639; Rockafellar R. T., 1970, CONVEX ANAL; Simsekli U, 2016, PR MACH LEARN RES, V48; Villani C., 2003, TOPICS OPTIMAL TRANS, V58; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Xu Pan, 2018, INT C ART INT STAT, P1087	33	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302086
C	Hu, ZH; Liang, YT; Zhang, J; Li, Z; Liu, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hu, Zehong; Liang, Yitao; Zhang, Jie; Li, Zhao; Liu, Yang			Inference Aided Reinforcement Learning for Incentive Mechanism Design in Crowdsourcing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Incentive mechanisms for crowdsourcing are designed to incentivize financially self-interested workers to generate and report high-quality labels. Existing mechanisms are often developed as one-shot static solutions, assuming a certain level of knowledge about worker models (expertise levels, costs of exerting efforts, etc.). In this paper, we propose a novel inference aided reinforcement mechanism that learns to incentivize high-quality data sequentially and requires no such prior assumptions. Specifically, we first design a Gibbs sampling augmented Bayesian inference algorithm to estimate workers' labeling strategies from the collected labels at each step. Then we propose a reinforcement incentive learning (RIL) method, building on top of the above estimates, to uncover how workers respond to different payments. RIL dynamically determines the payment without accessing any ground-truth labels. We theoretically prove that RIL is able to incentivize rational workers to provide high-quality labels. Empirical results show that our mechanism performs consistently well under both rational and non-fully rational (adaptive learning) worker models. Besides, the payments offered by RIL are more robust and have lower variances compared to the existing one-shot mechanisms.	[Hu, Zehong; Li, Zhao] Alibaba Grp, Hangzhou, Zhejiang, Peoples R China; [Liang, Yitao] Univ Calif Los Angeles, Los Angeles, CA USA; [Zhang, Jie] Nanyang Technol Univ, Singapore, Singapore; [Liu, Yang] Univ Calif Santa Cruz, Santa Cruz, CA 95064 USA; [Liu, Yang] Harvard Univ, Cambridge, MA 02138 USA	Alibaba Group; University of California System; University of California Los Angeles; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; University of California System; University of California Santa Cruz; Harvard University	Hu, ZH (corresponding author), Alibaba Grp, Hangzhou, Zhejiang, Peoples R China.	HUZE0004@e.ntu.edu.sg; yliang@cs.ucla.edu; ZhangJ@ntu.edu.sg; lizhao.lz@alibaba-inc.com; yangliu@ucsc.edu		Li, Zhao/0000-0002-5056-0351	National Research Foundation (NRF) Singapore under the Corp Lab@University Scheme; NSF [IIS-1657613, IIS-1633857]; DARPA XAI [N66001-17-2-4032]; NSF CCF [1718549]	National Research Foundation (NRF) Singapore under the Corp Lab@University Scheme(National Research Foundation, Singapore); NSF(National Science Foundation (NSF)); DARPA XAI; NSF CCF	This work started when Zehong Hu was at the Rolls-Royce@NTU Corporate Lab with support from the National Research Foundation (NRF) Singapore under the Corp Lab@UniversityScheme.Yitao is partially supported by NSF grants #IIS-1657613, #IIS-1633857 and DARPA XAI grant #N66001-17-2-4032. Yang Liu acknowledges supports from NSF CCF #1718549. The authors also thank Anxiang Zeng from Alibaba Group for valuable discussions.	Chen X, 2015, J MACH LEARN RES, V16, P1; Dasgupta  Anirban, 2013, P WWW; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Difallah Djellel Eddine, 2015, P WWW; Engel Y., 2005, P ICML; Gasic M, 2014, IEEE-ACM T AUDIO SPE, V22, P28, DOI 10.1109/TASL.2013.2282190; HOWE J, 2006, WIRED MAG, V14, P14; Jurca R, 2009, J ARTIFICIAL INTELLI, V34, P209; Liang  Yitao, 2016, P AAMAS; Liu Q., 2012, P NIPS; Liu Y, 2017, AAAI CONF ARTIF INTE, P607; MCKELVEY RD, 1995, GAME ECON BEHAV, V10, P6, DOI 10.1006/game.1995.1023; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Prelec D, 2004, SCIENCE, V306, P462, DOI 10.1126/science.1102081; Provost  Foster, 2008, P SIGKDD; Raykar VC, 2010, J MACH LEARN RES, V11, P1297; Simpson Edwin D, 2015, P WWW; Slivkins A, 2013, ACM SIGECOM EXCH, V12, P4; Snow  Rion, 2008, P EMNLP; Witkowski  Jens, 2012, P ACM EC; Zhang Y., 2014, P NIPS; Zheng YD, 2017, PROC VLDB ENDOW, V10, P541, DOI 10.14778/3055540.3055547; Zhou  Dengyong, 2014, P ICML	24	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000005
C	Le, H; Tran, T; Nguyen, T; Venkatesh, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hung Le; Truyen Tran; Thin Nguyen; Venkatesh, Svetha			Variational Memory Encoder-Decoder	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Introducing variability while maintaining coherence is a core task in learning to generate utterances in conversation. Standard neural encoder-decoder models and their extensions using conditional variational autoencoder often result in either trivial or digressive responses. To overcome this, we explore a novel approach that injects variability into neural encoder-decoder via the use of external memory as a mixture model, namely Variational Memory Encoder-Decoder (VMED). By associating each memory read with a mode in the latent mixture distribution at each timestep, our model can capture the variability observed in sequential data such as natural conversations. We empirically compare the proposed model against other recent approaches on various conversational datasets. The results show that VMED consistently achieves significant improvement over others in both metric-based and qualitative evaluations.	[Hung Le; Truyen Tran; Thin Nguyen; Venkatesh, Svetha] Deakin Univ, Appl AI Inst, Geelong, Vic, Australia	Deakin University	Le, H (corresponding author), Deakin Univ, Appl AI Inst, Geelong, Vic, Australia.	lethai@deakin.edu.au; truyen.tran@deakin.edu.au; thin.nguyen@deakin.edu.au; svetha.venkatesh@deakin.edu.au		Tran, Truyen/0000-0001-6531-8907; Le, Hung/0000-0002-3126-184X; Nguyen, Thin/0000-0003-3467-8963; venkatesh, svetha/0000-0001-8675-6631				[Anonymous], 2015, P INT C MACH LEARN; [Anonymous], 2016, ARXIV; Bacharoglou AG, 2010, P AM MATH SOC, V138, P2619, DOI 10.1090/S0002-9939-10-10340-2; Bahdanau D., 2015, P 3 INT C LEARN REPR; Bornschein J., 2017, ADV NEURAL INFORM PR, P3923; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Britz Denny, 2017, EMNLP, P392, DOI DOI 10.18653/V1/D17-1040; Chen Boxing, 2014, WMT ACL, P362; Chen HS, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1653, DOI 10.1145/3178876.3186077; Chung J, 2015, ADV NEURAL INFORM PR, P2980; Forgues G., 2014, NIPS MOD MACH LEARN, V2; Gemici M., 2017, ARXIV PREPRINT ARXIV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Graves Alex, 2013, ARXIV13080850 CORR; HERSHEY J. R., 2007, IEEE INT C AC SPEECH; Le H, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1637, DOI 10.1145/3219819.3219981; Jiang ZX, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1965; Kalchbrenner Nal, 2013, P 2013 C EMP METH NA, P1700, DOI DOI 10.1146/ANNUREV.NEURO.26.041002.131047; Kingma DP, 2014, ADV NEUR IN, P3581, DOI DOI 10.5555/2969033.2969226; Le H., 2018, PAC AS C KNOWL DISC, P273; Levy O, 2014, ADV NEUR IN, V27; Li J., 2016, P C N AM CHAPT ASS C, P110; Lison P, 2017, 18TH ANNUAL MEETING OF THE SPECIAL INTEREST GROUP ON DISCOURSE AND DIALOGUE (SIGDIAL 2017), P384; Mazya V, 1996, IMA J NUMER ANAL, V16, P13, DOI 10.1093/imanum/16.1.13; Nalisnick Eric, 2016, NIPS WORKSH BAYES DE, V2; Prakash A, 2017, AAAI CONF ARTIF INTE, P3274; Rezende D. J., 2014, P INT C INT C MACH L, pII; Serban IV, 2017, AAAI CONF ARTIF INTE, P3295; Shen XY, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 2, P504, DOI 10.18653/v1/P17-2080; Shu Rui, 2016, ECCV WORKSH ACT ANT, V2; Sukhbaatar S., 2015, P 28 INT C NEURAL IN, V28, P2440; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Wang Liwei, 2017, P ADV NEUR INF PROC, P5756; Wang  M., 2016, P 2016 C EMP METH NA, P278; Welling M, 2014, AUTOENCODING VARIATI; Wen TH, 2017, PR MACH LEARN RES, V70; Zhang Biao, 2016, P 2016 C EMP METH NA, P521, DOI [DOI 10.18653/V1/D16-1050, 10.18653/v1/D16-1050]; Zhao TC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P654, DOI 10.18653/v1/P17-1061	40	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301049
C	Ji, Y; Liang, L; Deng, L; Zhang, YY; Zhang, YH; Xie, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ji, Yu; Liang, Ling; Deng, Lei; Zhang, Youyang; Zhang, Youhui; Xie, Yuan			TETRIS: TilE-matching the TRemendous Irregular Sparsity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Compressing neural networks by pruning weights with small magnitudes can significantly reduce the computation and storage cost. Although pruning makes the model smaller, it is difficult to get a practical speedup in modern computing platforms such as CPU and GPU due to the irregularity. Structural pruning has attracted a lot of research interest to make sparsity hardware-friendly. Increasing the sparsity granularity can lead to better hardware utilization, but it will compromise the sparsity for maintaining accuracy. In this work, we propose a novel method, TETRIS, to achieve both better hardware utilization and higher sparsity. Just like a tile-matching game(2), we cluster the irregularly distributed weights with small value into structured groups by reordering the input/output dimension and structurally prune them. Results show that it can achieve comparable sparsity with the irregular element-wise pruning and demonstrate negligible accuracy loss. The experiments also show ideal speedup, which is proportional to the sparsity, on GPU platforms. Our proposed method provides a new solution toward algorithm and architecture co-optimization for accuracy-efficiency trade-off.	[Ji, Yu; Zhang, Youyang; Zhang, Youhui] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China; [Ji, Yu; Zhang, Youhui] Beijing Innovat Ctr Future Chip, Beijing, Peoples R China; [Ji, Yu; Liang, Ling; Deng, Lei; Xie, Yuan] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA	Tsinghua University; University of California System; University of California Santa Barbara	Zhang, YH (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.; Zhang, YH (corresponding author), Beijing Innovat Ctr Future Chip, Beijing, Peoples R China.	jiy15@mails.tsinghua.edu.cn; lingliang@ece.ucsb.edu; leideng@ece.ucsb.edu; zhang-yy15@mails.tsinghua.edu.cn; zyh02@tsinghua.edu.cn; yuanxie@ece.ucsb.edu			Beijing Innovation Center for Future Chip; National Science Foundations (NSF) [1725447, 1730309]; Science and Technology Innovation Special Zone project	Beijing Innovation Center for Future Chip; National Science Foundations (NSF)(National Science Foundation (NSF)); Science and Technology Innovation Special Zone project	This research was collaborative work of Tsinghua University and University of California, Santa Barbara. Thanks for the support from Beijing Innovation Center for Future Chip, Science and Technology Innovation Special Zone project, and the National Science Foundations (NSF) under grant numbers 1725447 and 1730309. We also thank OpenAI for their open-source library, blocksparse.	Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736; Amodei D, 2016, PR MACH LEARN RES, V48; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, ARXIV170606197; [Anonymous], 2016, 1612 ARXIV; [Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; Ardakani A., 2016, ARXIV161101427; HAN S., 2015, ARXIV151000149; Han S, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P75, DOI 10.1145/3020078.3021745; Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; Li H., 2016, INT C LEARNING REPRE; Li K., 2009, CVPR09; Lin CY, 2018, ASIA S PACIF DES AUT, P105; Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298; Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681; Lu Y., 2017, P 2017 INT C COMP AR, P12; Luo J.-H., 2017, ARXIV170706342; Marcel Sebastien, 2010, P 18 ACM INT C MULTI, P1485, DOI DOI 10.1145/1873951.1874254; Narang Sharan, 2017, ARXIV171102782; Page A, 2017, ACM J EMERG TECH COM, V13, DOI 10.1145/3005448; Parashar A, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P27, DOI 10.1145/3079856.3080254; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Scott G., 2016, GPU KERNEL BLOCK SPA; Wen W., 2016, ADV NEURAL INFORM PR, P2074; Wen W., 2017, CORR; Wen W., 2017, ARXIV170905027; Wu Y., 2016, GOOGLES NEURAL MACHI; Yu JC, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P548, DOI 10.1145/3079856.3080215	29	2	3	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304015
C	Jin, YYZ; Zhang, WR; Li, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jin, Yingyezhe; Zhang, Wenrui; Li, Peng			Hybrid Macro/Micro Level Backpropagation for Training Deep Spiking Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Spiking neural networks (SNNs) are positioned to enable spatio-temporal information processing and ultra-low power event-driven neuromorphic hardware. However, SNNs are yet to reach the same performances of conventional deep artificial neural networks (ANNs), a long-standing challenge due to complex dynamics and non-differentiable spike events encountered in training. The existing SNN error backpropagation (BP) methods are limited in terms of scalability, lack of proper handling of spiking discontinuities, and/or mismatch between the rate-coded loss function and computed gradient. We present a hybrid macro/micro level backpropagation (HM2-BP) algorithm for training multi-layer SNNs. The temporal effects are precisely captured by the proposed spike-train level post-synaptic potential (S-PSP) at the microscopic level. The rate-coded errors are defined at the macroscopic level, computed and back-propagated across both macroscopic and microscopic levels. Different from existing BP methods, HM2-BP directly computes the gradient of the rate-coded loss function w.r.t tunable parameters. We evaluate the proposed HM2-BP algorithm by training deep fully connected and convolutional SNNs based on the static MNIST [14] and dynamic neuromorphic N-MNIST [26]. HM2-BP achieves an accuracy level of 99.49% and 98.88% for MNIST and N-MNIST, respectively, outperforming the best reported performances obtained from the existing SNN BP algorithms. Furthermore, the HM2-BP produces the highest accuracies based on SNNs for the EMNIST [3] dataset, and leads to high recognition accuracy for the 16-speaker spoken English letters of TI46 Corpus [16], a challenging spatio-temporal speech recognition benchmark for which no prior success based on SNNs was reported. It also achieves competitive performances surpassing those of conventional deep learning models when dealing with asynchronous spiking streams.	[Jin, Yingyezhe; Zhang, Wenrui; Li, Peng] Texas A&M Univ, College Stn, TX 77843 USA	Texas A&M University System; Texas A&M University College Station	Jin, YYZ (corresponding author), Texas A&M Univ, College Stn, TX 77843 USA.	jyyz@tamu.edu; zhangwenrui@tamu.edu; pli@tamu.edu	Zhang, Wenrui/AAB-2428-2020		National Science Foundation [CCF-1639995]; Semiconductor Research Corporation (SRC) [2692.001]	National Science Foundation(National Science Foundation (NSF)); Semiconductor Research Corporation (SRC)	This material is based upon work supported by the National Science Foundation under Grant No.CCF-1639995 and the Semiconductor Research Corporation (SRC) under Task 2692.001. The authors would like to thank High Performance Research Computing (HPRC) at Texas A&M University for providing computing support. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of NSF, SRC, Texas A&M University, and their contractors.	Ba J., 2017, P 3 INT C LEARN REPR; Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565; Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0; Cohen Gregory, 2017, ARXIV170205373 EMNIS; Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184; Collobert R., 2008, AUNIFIED ARCHITECTUR, P160; DIEHL PU, 2015, IEEE IJCNN; Esser S. K., 2015, ADV NEURAL INFORM PR, P1117; Gerstner W., 2002, SPIKING NEURON MODEL; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Hunsberger Eric, 2015, ARXIV151008829; Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508; Liberman Mark, TI46 SPEECH CORPUS; Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337; Lyon R. F., 1982, Proceedings of ICASSP 82. IEEE International Conference on Acoustics, Speech and Signal Processing, P1282; Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7; Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642; Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324; Neil D, 2016, ADV NEUR IN, V29; Neil D, 2016, IEEE INT SYMP CIRC S, P2282, DOI 10.1109/ISCAS.2016.7539039; O'Connor P., 2016, ARXIV160208323; O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178; Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]; Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Schrauwen B, 2003, IEEE IJCNN, P2825; Simard PY, 2003, PROC INT CONF DOC, P958; WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337; Wu Y., 2017, ARXIV170602609; Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086	33	2	2	2	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001054
C	Khan, H; Yener, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Khan, Haidar; Yener, Bulent			Learning filter widths of spectral decompositions with wavelets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DEEP NEURAL-NETWORKS; SIGNAL	Time series classification using deep neural networks, such as convolutional neural networks (CNN), operate on the spectral decomposition of the time series computed using a preprocessing step. This step can include a large number of hyperparameters, such as window length, filter widths, and filter shapes, each with a range of possible values that must be chosen using time and data intensive cross-validation procedures. We propose the wavelet deconvolution (WD) layer as an efficient alternative to this preprocessing step that eliminates a significant number of hyperparameters. The WD layer uses wavelet functions with adjustable scale parameters to learn the spectral decomposition directly from the signal. Using backpropagation, we show the scale parameters can be optimized with gradient descent. Furthermore, the WD layer adds interpretability to the learned time series classifier by exploiting the properties of the wavelet transform. In our experiments, we show that the WD layer can automatically extract the frequency content used to generate a dataset. The WD layer combined with a CNN applied to the phone recognition task on the TIMIT database achieves a phone error rate of 18.1%, a relative improvement of 4% over the baseline CNN. Experiments on a dataset where engineered features are not available showed WD+CNN is the best performing method. Our results show that the WD layer can improve neural network based time series classifiers both in accuracy and interpretability by learning directly from the input signal.	[Khan, Haidar; Yener, Bulent] Rensselaer Polytech Inst, Dept Comp Sci, Troy, NY 12180 USA	Rensselaer Polytechnic Institute	Khan, H (corresponding author), Rensselaer Polytech Inst, Dept Comp Sci, Troy, NY 12180 USA.	khanh2@rpi.edu; yener@rpi.edu			NSF [1302231]	NSF(National Science Foundation (NSF))	This work was supported in part by NSF Award #1302231.	Abadi M., TENSORFLOW LARGE SCA; Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736; Abdel-Hamid O, 2012, INT CONF ACOUST SPEE, P4277, DOI 10.1109/ICASSP.2012.6288864; Acar E, 2007, BIOINFORMATICS, V23, pI10, DOI 10.1093/bioinformatics/btm210; Al-Naymat G., 2009, AUSTRALASIAN DATA MI, V101, P117, DOI DOI 10.1007/s10115-004-0154-9; Bagnall A, 2016, PROC INT CONF DATA, P1548, DOI 10.1109/ICDE.2016.7498418; Brockwell P.J., 2002, INTRO TIME SERIES FO, V2nd Edn; Chen Y, 2015, UCR TIME SERIES CLAS; Chollet F., 2015, KERAS; DAUBECHIES I, 1990, IEEE T INFORM THEORY, V36, P961, DOI 10.1109/18.57199; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Gosztolyal G, 2015, INT CONF ACOUST SPEE, P4570, DOI 10.1109/ICASSP.2015.7178836; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Huang NE, 2008, REV GEOPHYS, V46, DOI 10.1029/2007RG000228; Jaitly N, 2011, INT CONF ACOUST SPEE, P5884; Khan H., 2017, IEEE T BIOMED ENG, V99, P1, DOI DOI 10.1109/TBME.2017.2785401; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Mirowski P, 2009, CLIN NEUROPHYSIOL, V120, P1927, DOI 10.1016/j.clinph.2009.09.002; Mirowski PW, 2008, MACHINE LEARN SIGN P, P244, DOI 10.1109/MLSP.2008.4685487; Mohamed Abdel-rahman, 2009, P NIPS WORKSH DEEP L, V1, P39; Nair V., 2010, ICML, P807; Oppenheim A.V., 2014, DISCRETE TIME SIGNAL; Palaz D, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P11; Palaz Dimitri, 2013, ARXIV13122137, P2; Po-Sen Huang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P205, DOI 10.1109/ICASSP.2014.6853587; Polikar Robi, 1996, ENG ULTIMATE GUIDE W, V14, P81; Sainath TN, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1; Sainath TN, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P297, DOI 10.1109/ASRU.2013.6707746; Schafer P, 2015, DATA MIN KNOWL DISC, V29, P1505, DOI 10.1007/s10618-014-0377-7; Simard PY, 2003, PROC INT CONF DOC, P958; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Stober Sebastian, 2015, DEEP FEATURE LEARNIN, P1; Toth L, 2014, INTERSPEECH, P1078; Toth L, 2013, INT CONF ACOUST SPEE, P6985, DOI 10.1109/ICASSP.2013.6639016; Tuske Z, 2014, INTERSPEECH, P890; Wang ZG, 2017, IEEE IJCNN, P1578, DOI 10.1109/IJCNN.2017.7966039; Yu Dong, 2015, AUTOMATIC SPEECH REC, V1976; Zeiler M. D., 2012, ARXIV 12125701, V1212.5701	42	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304060
C	Khoshaman, AH; Amin, MH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Khoshaman, Amir H.; Amin, Mohammad H.			GumBolt: Extending Gumbel trick to Boltzmann priors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Boltzmann machines (BMs) are appealing candidates for powerful priors in variational autoencoders (VAEs), as they are capable of capturing nontrivial and multi-modal distributions over discrete variables. However, non-differentiability of the discrete units prohibits using the reparameterization trick, essential for low-noise back propagation. The Gumbel trick resolves this problem in a consistent way by relaxing the variables and distributions, but it is incompatible with BM priors. Here, we propose the GumBolt, a model that extends the Gumbel trick to BM priors in VAEs. GumBolt is significantly simpler than the recently proposed methods with BM prior and outperforms them by a considerable margin. It achieves state-of-theart performance on permutation invariant MNIST and OMNIGLOT datasets in the scope of models with only discrete latent variables. Moreover, the performance can be further improved by allowing multi-sampled (importance-weighted) estimation of log-likelihood in training, which was not possible with previous models.	[Khoshaman, Amir H.] D Wave Syst Inc, Burnaby, BC, Canada; [Amin, Mohammad H.] Simon Fraser Univ, D Wave Syst Inc, Burnaby, BC, Canada; [Khoshaman, Amir H.] Borealis AI, Edmonton, AB, Canada	Simon Fraser University; Borealis AG	Khoshaman, AH (corresponding author), D Wave Syst Inc, Burnaby, BC, Canada.; Khoshaman, AH (corresponding author), Borealis AI, Edmonton, AB, Canada.	khoshaman@gmail.com; mhsamin@dwavesys.com	Amin, Mohammad/AAM-9571-2020					Amin M. H., 2016, QUANTUM BOLTZMANN MA; [Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; Ba J., 2017, P 3 INT C LEARN REPR; Bengio Yoshua, 2013, ARXIV; BENNETT CH, 1976, J COMPUT PHYS, V22, P245, DOI 10.1016/0021-9991(76)90078-4; Bishop C. M., 2011, PATTERN RECOGN, DOI DOI 10.1038/nature11129; Burda Yuri, 2015, ICLR; Chen Xi, 2016, ARXIV161102731; Desjardins G., 2010, P 13 INT C ART INT S, P145; Germain M, 2015, PR MACH LEARN RES, V37, P881; Goumas Georgios, 2008, ARXIV151105176, P283; Goyal A., 2017, ADV NEURAL INFORM PR, P6716; Grathwohl W., 2017, ARXIV171100123; Gregor K., 2013, ARXIV PREPRINT ARXIV, V2; Gregor K., 2015, ARXIV150204623; Gulrajani I., 2016, P INT C LEARN REPR; Jang Eric, 2017, P 5 INT C LEARN REPR; Khoshaman A, 2019, QUANTUM SCI TECHNOL, V4, DOI 10.1088/2058-9565/aada1f; Kingma DP, 2014, ADV NEUR IN, P3581, DOI DOI 10.5555/2969033.2969226; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kingma DP, 2 INT C LEARN REPR I, P1; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510; Maaloe L., 2017, ARXIV170400637; Maddison Chris J, 2016, ARXIV161100712; Miron M., 2018, ARXIV180101423; Mnih A, 2016, PR MACH LEARN RES, V48; Mnih Andriy, 2014, INT C MACH LEARN; Oord A.V.D., 2016, ARXIV160106759; Raiko Tapani, 2014, ARXIV14062989; Rezende Danilo Jimenez, 2015, ARXIV150505770; Rolfe J. T., 2016, ARXIV160902200; Ross S. M., 2013, APPL PROBABILITY MOD; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Salimans T, 2015, PR MACH LEARN RES, V37, P1218; Shirts MR, 2008, J CHEM PHYS, V129, DOI 10.1063/1.2978177; Sonderby CK, 2016, ADV NEUR IN, V29; Tieleman T., 2008, P 25 INT C MACHINE L, P1064, DOI DOI 10.1145/1390156.1390290; Tucker G., 2017, ADV NEURAL INFORM PR, P2624; Vahdat Arash, 2018, ARXIV180204920; Williams R.J., 1992, REINFORCEMENT LEARNI, V173, P5, DOI [10.1007/978-1-4615-3618-5, DOI 10.1007/978-1-4615-3618-5]; Yeung S., 2017, ICML WORK PRINC APPR	42	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304010
C	Knoblauch, J; Jewson, J; Damoulas, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Knoblauch, Jeremias; Jewson, Jack; Damoulas, Theodoros			Doubly Robust Bayesian Inference for Non-Stationary Streaming Data with beta-Divergences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present the first robust Bayesian Online Changepoint Detection algorithm through General Bayesian Inference (GBI) with beta-divergences. The resulting inference procedure is doubly robust for both the parameter and the changepoint (CP) posterior, with linear time and constant space complexity. We provide a construction for exponential models and demonstrate it on the Bayesian Linear Regression model. In so doing, we make two additional contributions: Firstly, we make GBI scalable using Structural Variational approximations that are exact as beta -> 0. Secondly, we give a principled way of choosing the divergence parameter beta by minimizing expected predictive loss on-line. Reducing False Discovery Rates of CPs from over 90% to 0% on real world data, this offers the state of the art.	[Knoblauch, Jeremias; Damoulas, Theodoros] Univ Warwick, Alan Turing Inst, Dept Stat, Coventry CV4 7AL, W Midlands, England; [Jewson, Jack] Univ Warwick, Dept Stat, Coventry CV4 7AL, W Midlands, England; [Damoulas, Theodoros] Univ Warwick, Alan Turing Inst, Dept Comp Sci, Coventry CV4 7AL, W Midlands, England	University of Warwick; University of Warwick; University of Warwick	Knoblauch, J (corresponding author), Univ Warwick, Alan Turing Inst, Dept Stat, Coventry CV4 7AL, W Midlands, England.	j.knoblauch@warwick.ac.uk; j.e.jewson@warwick.ac.uk; t.damoulas@warwick.ac.uk	Jewson, Jack/AAB-5109-2021		EPSRC as part of the Oxford-Warwick Statistics Programme (OXWASP) [EP/L016710/1]; Lloyds Register Foundation programme on Data Centric Engineering through the London Air Quality project; Alan Turing Institute for Data Science and AI under EPSRC [EP/N510129/1]	EPSRC as part of the Oxford-Warwick Statistics Programme (OXWASP)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Lloyds Register Foundation programme on Data Centric Engineering through the London Air Quality project; Alan Turing Institute for Data Science and AI under EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We would like to cordially thank both Jim Smith and Chris Holmes for fruitful discussions and help with some of the theoretical results. JK and JJ are funded by EPSRC grant EP/L016710/1 as part of the Oxford-Warwick Statistics Programme (OXWASP). TD is funded by the Lloyds Register Foundation programme on Data Centric Engineering through the London Air Quality project. This work was supported by The Alan Turing Institute for Data Science and AI under EPSRC grant EP/N510129/1. In collaboration with the Greater London Authority.	Adams R. Prescott, 2007, ARXIV07103742; Alvarez M., 2010, NEURAL INFORM PROCES, P55; [Anonymous], P 25 ANN C NEUR INF; BARRY D, 1993, J AM STAT ASSOC, V88, P309, DOI 10.1080/01621459.1993.10594323; Basu A, 1998, BIOMETRIKA, V85, P549, DOI 10.1093/biomet/85.3.549; Bernardo JM., 2001, BAYESIAN THEORY; Bissiri PG, 2016, J R STAT SOC B, V78, P1103, DOI 10.1111/rssb.12158; Cao Y, 2017, IEEE INT SYMP INFO, P1287, DOI 10.1109/ISIT.2017.8006736; Caron F, 2012, STAT COMPUT, V22, P579, DOI 10.1007/s11222-011-9248-x; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; DIENG AB, 2017, ADV NEURAL INFORM PR, P02729; Fearnhead P, 2007, J ROY STAT SOC B, V69, P589, DOI 10.1111/j.1467-9868.2007.00601.x; Fearnhead Paul, 2017, J AM STAT ASS; Fox E.B., 2012, P 25 INT C NEURAL IN, V25, P737, DOI DOI 10.5555/2999134.2999217; Futami Futoshi, 2018, ARTIFICIAL INTELLIGE; Ghosh A, 2016, ANN I STAT MATH, V68, P413, DOI 10.1007/s10463-014-0499-0; Grzegorczyk M., 2009, P ADV NEUR INF PROC, P682; Hall P, 2005, J R STAT SOC B, V67, P427, DOI 10.1111/j.1467-9868.2005.00510.x; Harikandeh R., 2015, PROC NEURAL INF PROC, P2251; Hernandez-Lobato J., 2016, 33 INT C INT C MACH, P1511; Huang He, 2016, NEURAL INFORM PROCES, P2730; Jewson J, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20060442; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Khaleghi Azadeh, 2012, P ADV NEUR INF PROC, P3086; Killick R, 2010, OCEAN ENG, V37, P1120, DOI 10.1016/j.oceaneng.2010.04.009; Knoblauch Jeremias, 2018, P 27 INT C MACH LEAR; Konidaris G. D., 2010, P ADV NEUR INF PROC, V23, P1162; Kummerfeld E., 2013, ADV NEURAL INFORM PR, P1205; Kurtek S, 2015, BIOMETRIKA, V102, P601, DOI 10.1093/biomet/asv026; Lei LH, 2017, ADV NEUR IN, V30; Levy-leduc C., 2008, P ADV NEURAL INFORM, P617; Li Y., 2016, ADV NEURAL INFORM PR, P1073; Lin K., 2017, ADV NEURAL INFORM PR, P6887; Niekum S., 2014, CMURITR1410; Nitanda A., 2014, ADV NEURAL INFORM PR, P1574; Pollak M., 2010, SEQUENTIAL ANAL, V29, P146; Polunchenko AS, 2012, SEQUENTIAL ANAL, V31, P409, DOI 10.1080/07474946.2012.694351; Ranganath R, 2016, ADV NEUR IN, V29; Ruanaidh O, 1996, NUMERICAL BAYESIAN M; Ruggieri E, 2016, COMPUT STAT DATA AN, V97, P71, DOI 10.1016/j.csda.2015.11.010; Saatci Y, 2010, P 27 INT C MACH LEAR, P927; Stimberg F., 2011, ADV NEURAL INFORM PR, V24, P2717; Turner R., 2009, TEMP SEGM WORKSH NIP; Turner R., 2013, ADV NEURAL INFORM PR, P306; Turner R.D., 2012, THESIS; Wilson RC, 2010, NEURAL COMPUT, V22, P2452, DOI 10.1162/NECO_a_00007; Xuan X., 2007, P 24 INT C MACH LEAR, P1055, DOI DOI 10.1145/1273496.1273629; Yao Yuling, 2018, ARXIV180202538; Yilmaz KY, 2011, ADV NEURAL INFORM PR, P2151	50	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300007
C	Kroer, C; Sandholm, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kroer, Christian; Sandholm, Tuomas			A Unified Framework for Extensive-Form Game Abstraction with Bounds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The Abstraction has long been a key component in the practical solving of large-scale extensive-form games. Despite this, abstraction remains poorly understood. There have been some recent theoretical results but they have been confined to specific assumptions on abstraction structure and are specific to various disjoint types of abstraction, and specific solution concepts, for example, exact Nash equilibria or strategies with bounded immediate regret. In this paper we present a unified framework for analyzing abstractions that can express all types of abstractions and solution concepts used in prior papers with performance guarantees-while maintaining comparable bounds on abstraction quality. Moreover, our framework gives an exact decomposition of abstraction error in a much broader class of games, albeit only in an ex-post sense, as our results depend on the specific strategy chosen. Nonetheless, we use this ex-post decomposition along with slightly weaker assumptions than prior work to derive generalizations of prior bounds on abstraction quality. We also show, via counterexample, that such assumptions are necessary for some games. Finally, we prove the first bounds for how is an element of-Nash equilibria computed in abstractions perform in the original game. This is important because often one cannot afford to compute an exact Nash equilibrium in the abstraction. All our results apply to general-sum n-player games.	[Kroer, Christian; Sandholm, Tuomas] Dept Comp Sci, Pittsburgh, PA 15213 USA		Kroer, C (corresponding author), Dept Comp Sci, Pittsburgh, PA 15213 USA.	ckroer@cs.cmu.edu; sandholm@cs.cmu.edu		Kroer, Christian/0000-0002-9009-8683	National Science Foundation [IIS-1718457, IIS-1617590, CCF-1733556]; ARO [W911NF-17-1-0082]; Facebook Fellowship	National Science Foundation(National Science Foundation (NSF)); ARO; Facebook Fellowship(Facebook Inc)	This material is based on work supported by the National Science Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and the ARO under award W911NF-17-1-0082. Christian Kroer is supported by a Facebook Fellowship.	[Anonymous], 2009, THESIS; Basak A, 2016, LECT NOTES COMPUT SC, V9996, P251, DOI 10.1007/978-3-319-47413-7_15; Basak A, 2016, LECT NOTES ARTIF INT, V10003, P13, DOI 10.1007/978-3-319-46840-2_2; Basilico N., 2011, AAAI C ART INT AAAI; Billings D., 2003, P INT JOINT C ARTIFI; Bowling M, 2015, SCIENCE, V347, P145, DOI 10.1126/science.1259433; Brown N., 2017, SCIENCE; Brown N., 2015, INT C AUT AG MULT SY; Brown N., 2017, ADV NEURAL INFORM PR, P689; Brown N, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P489; Brown Noam, 2014, AAAI C ART INT AAAI; Burch N, 2012, ADV NEURAL INFORM PR, P1880; Cermak J, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P936; Farina G, 2017, PR MACH LEARN RES, V70; Ganzfried Sam, 2014, AAAI C ART INT AAAI; Gilpin A., 2007, P 6 AAMAS, P1168; Gilpin A., 2008, INT C AUT AG MULT SY; Gilpin A., 2007, P AAAI C ART INT AAA; Gilpin A., 2006, P NAT C ART INT AAAI, P1007; Gilpin A, 2007, J ACM, V54, DOI 10.1145/1284320.1284324; Gilpin  Andrew, 2008, P AAAI C ART INT AAA; Hawkin John, 2011, AAAI C ART INT AAAI; Hawkin John, 2012, AAAI C ART INT AAAI; Hoda S, 2010, MATH OPER RES, V35, P494, DOI 10.1287/moor.1100.0452; Johanson  Michael, 2013, INT C AUT AG MULT SY; Kroer C., 2017, P ACM C EC COMP EC; Kroer C., 2016, P ACM C EC COMP EC; Kroer C., 2015, INT C AUT AG MULT SY; Kroer C., 2015, P ACM C EC COMP EC; Kroer  Christian, 2014, P ACM C EC COMP EC; Lanctot M., 2009, ADV NEURAL INFORM PR, P1078; Lanctot  Marc, 2012, INT C MACH LEARN ICM; Littman Michael L., 2000, INT C COMP GAM, P333; Moravcik M, 2017, SCIENCE, V356, P508, DOI 10.1126/science.aam6960; NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48; Sandholm T., 2015, AAAI C ART INT AAAI; Sandholm  Tuomas, 2012, P ACM C EL COMM EC; Waugh Kevin, 2015, AAAI C ART INT AAAI; Wellman M. P., 2005, P NAT C ART INT AAAI; Zinkevich M., 2007, ADV NEURAL INFORM PR, V7, P1729	40	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300057
C	Lange-Hegermann, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lange-Hegermann, Markus			Algorithmic Linearly Constrained Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				COMPLEXITY	We algorithmically construct multi-output Gaussian process priors which satisfy linear differential equations. Our approach attempts to parametrize all solutions of the equations using Grobner bases. If successful, a push forward Gaussian process along the paramerization is the desired prior. We consider several examples from physics, geomathematics and control, among them the full inhomogeneous system of Maxwell's equations. By bringing together stochastic learning and computer algebra in a novel way, we combine noisy observations with precise algebraic computations.	[Lange-Hegermann, Markus] Ostwestfalen Lippe Univ Appl Sci, Dept Elect Engn & Comp Sci, Lemgo, Germany		Lange-Hegermann, M (corresponding author), Ostwestfalen Lippe Univ Appl Sci, Dept Elect Engn & Comp Sci, Lemgo, Germany.	markus.lange-hegermann@hs-owl.de		Lange-Hegermann, Markus/0000-0002-5327-4529				[Anonymous], 2016, SINGULAR 4 1 0 COMPU; [Anonymous], 1994, GRADUATE STUDIES MAT; Barakat M., 2010, P 19 INT S MATH THEO, P1657; BAYER D, 1988, J SYMB COMPUT, V6, P135, DOI 10.1016/S0747-7171(88)80039-7; Bertinet A., 2004, REPRODUCING KERNEL H; Buchberger B, 2006, J SYMB COMPUT, V41, P475, DOI 10.1016/j.jsc.2005.09.007; Calandra R, 2016, IEEE IJCNN, P3338, DOI 10.1109/IJCNN.2016.7727626; Chyzak F, 2005, APPL ALGEBR ENG COMM, V16, P319, DOI 10.1007/s00200-005-0188-6; Deisenroth MP, 2015, IEEE T PATTERN ANAL, V37, P408, DOI 10.1109/TPAMI.2013.218; Dong Kun, 2017, ARXIV171103481; Duvenaud D., 2014, AUTOMATIC MODEL CONS; Eisenbud David, 1995, COMMUTATIVE ALGEBRA, V150; Garnett R, 2015, PR MACH LEARN RES, V37, P1025; Gerdt VP, 2005, NATO SC S SS III C S, V196, P199; Graepel T., 2003, P 20 INT C MACHINE L, P234; Grayson D. R., MACAULAY2 SOFTWARE S; Greuel G.-M., 2002, SINGULAR INTRO COMMU; Hensman J., 2013, P 29 C UNC ART INT; Honkela A, 2015, P NATL ACAD SCI USA, V112, P13115, DOI 10.1073/pnas.1420404112; Izmailov  Pavel, 2017, ARXIVMATH171007324; Jang Phillip A, 2017, ADV NEURAL INFORM PR, V30, P3940; Jaynes E.T., 2003, PROBABILITY THEORY L; JAYNES ET, 1968, IEEE T SYST SCI CYB, VSSC4, P227, DOI 10.1109/TSSC.1968.300117; Jidling Carl, 2017, ARXIV170300787; Lee Jaehoon, 2017, ARXIV171100165; Levandovskyy V., 2003, P INT S SYMBOLIC ALG, P176; LEVANDOVSKYY V, 2005, THESIS; Macedo I., 2008, TECH REP; MAYR E, 1989, LECT NOTES COMPUT SC, V349, P400; MAYR EW, 1982, ADV MATH, V46, P305, DOI 10.1016/0001-8708(82)90048-2; OBERST U, 1990, ACTA APPL MATH, V20, P1, DOI 10.1007/BF00046908; Osborne M, 2009, LEARNING INTELLIGENT, P1; Quadrat A., 2010, THESIS; Quadrat A, 2013, ACTA APPL MATH, V127, P27, DOI 10.1007/s10440-012-9791-2; Quadrat  Alban, 2010, COURS CIRM, V1, P279; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Robertz D, 2015, MULTIDIM SYST SIGN P, V26, P349, DOI 10.1007/s11045-014-0280-9; Robertz D., 2006, THESIS; Robertz D., 2003, JANETORE MAPLE PACKA; Scheuerer M, 2012, STOCH MODELS, V28, P433, DOI 10.1080/15326349.2012.699756; Seiler Werner M., 2010, PAMM, V10, P633; Simon-Gabriel C.-J., 2016, ARXIV160405251; Snelson E, 2004, ADV NEUR IN, V16, P337; Solin Arno, 2015, ARXIV150904634; STURMFELS B, 2005, NOT AM MATH SOC, V52, P2; Thewes S., 2015, DESIGN EXPT DOE POWE; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Treves F., 1967, DOVER BOOKS MATH; Wahlstrom N., 2013, P 38 INT C AC SPEECH; Wilson A., 2013, INT C MACH LEARN, P1067; Wilson A.G., 2015, ARXIV151101870; Wilson AG, 2015, ARXIV151102222; Zerz E., 2000, LECT NOTES CONTROL I, V256; Zerz E, 2010, COMMUN ALGEBRA, V38, P2037, DOI 10.1080/00927870903015226	54	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302017
C	Lee, HB; Lee, J; Kim, S; Yang, E; Hwang, SJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lee, Hae Beom; Lee, Juho; Kim, Saehoon; Yang, Eunho; Hwang, Sung Ju			DropMax: Adaptive Variational Softmax	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose DropMax, a stochastic version of softmax classifier which at each iteration drops non-target classes according to dropout probabilities adaptively decided for each instance. Specifically, we overlay binary masking variables over class output probabilities, which are input-adaptively learned via variational inference. This stochastic regularization has an effect of building an ensemble classifier out of exponentially many classifiers with different decision boundaries. Moreover, the learning of dropout rates for non-target classes on each instance allows the classifier to focus more on classification against the most confusing classes. We validate our model on multiple public datasets for classification, on which it obtains significantly improved accuracy over the regular softmax classifier and other baselines. Further analysis of the learned dropout probabilities shows that our model indeed selects confusing classes more often when it performs classification.	[Lee, Hae Beom; Yang, Eunho; Hwang, Sung Ju] Korea Adv Inst Sci & Technol, Daejeon, South Korea; [Lee, Hae Beom; Lee, Juho; Kim, Saehoon; Yang, Eunho; Hwang, Sung Ju] AItrics, Seoul, South Korea; [Lee, Juho] Univ Oxford, Oxford, England	Korea Advanced Institute of Science & Technology (KAIST); University of Oxford	Lee, HB (corresponding author), Korea Adv Inst Sci & Technol, Daejeon, South Korea.; Lee, HB (corresponding author), AItrics, Seoul, South Korea.	haebeom.lee@kaist.ac.kr; juho.lee@stats.ox.ac.uk; shkim@aitrics.com; eunhoy@kaist.ac.kr; sjhwang82@kaist.ac.kr	Lee, Juho/AAA-2901-2022; Yang, Eunho/K-8395-2016		Engineering Research Center Program through the National Research Foundation of Korea (NRF) - Korean Government MSIT [NRF-2018R1A5A1059921]; Samsung Research Funding Center of Samsung Electronics [SRFC-IT150203]; Machine Learning and Statistical Inference Framework for Explainable Artificial Intelligence [2017-0-01779]; Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Education [2015R1D1A1A01061019]; European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant [617071]	Engineering Research Center Program through the National Research Foundation of Korea (NRF) - Korean Government MSIT(National Research Foundation of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); Samsung Research Funding Center of Samsung Electronics(Samsung); Machine Learning and Statistical Inference Framework for Explainable Artificial Intelligence; Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Education(National Research Foundation of KoreaMinistry of Education (MOE), Republic of KoreaNational Research Council for Economics, Humanities & Social Sciences, Republic of Korea); European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant(European Research Council (ERC))	This research was supported by the Engineering Research Center Program through the National Research Foundation of Korea (NRF) funded by the Korean Government MSIT (NRF-2018R1A5A1059921), Samsung Research Funding Center of Samsung Electronics (SRFC-IT150203), Machine Learning and Statistical Inference Framework for Explainable Artificial Intelligence (No. 2017-0-01779), and Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (2015R1D1A1A01061019). Juho Lee's research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617071.	Abadi M, 2015, P 12 USENIX S OPERAT; Ba J., 2013, ADV NEURAL INFORM PR, P3084; Bouthillier X., 2015, ARXIV150608700; de Brebisson Alexandre, 2016, ICLR; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Gal Y., 2017, NIPS; Gal Y, 2016, PR MACH LEARN RES, V48; Gal Yarin, 2016, ADV NEURAL INFORM PR, P1019, DOI DOI 10.5555/3157096.3157211; Gal Yarin, 2015, ARXIV150602158; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jean S, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1; Kendall Alex, 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295309; Kingma D.P, P 3 INT C LEARNING R; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Luong M.-T., 2015, ACL; Maddison Chris J, 2017, ICLR; Martins AFT, 2016, PR MACH LEARN RES, V48; Molchanov D, 2017, PR MACH LEARN RES, V70; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Wah C., 2011, TECH REP; Xu K, 2015, PR MACH LEARN RES, V37, P2048	28	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300085
C	Li, J; Mantiuk, RK; Wang, JL; Ling, S; Le Callet, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Jing; Mantiuk, Rafal K.; Wang, Junle; Ling, Suiyi; Le Callet, Patrick			Hybrid-MST: A Hybrid Active Sampling Strategy for Pairwise Preference Aggregation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper we present a hybrid active sampling strategy for pairwise preference aggregation, which aims at recovering the underlying rating of the test candidates from sparse and noisy pairwise labelling. Our method employs Bayesian optimization framework and Bradley-Terry model to construct the utility function, then to obtain the Expected Information Gain (EIG) of each pair. For computational efficiency, Gaussian-Hermite quadrature is used for estimation of EIG. In this work, a hybrid active sampling strategy is proposed, either using Global Maximum (GM) EIG sampling or Minimum Spanning Tree (MST) sampling in each trial, which is determined by the test budget. The proposed method has been validated on both simulated and real-world datasets, where it shows higher preference aggregation ability than the state-of-the-art methods.	[Li, Jing; Ling, Suiyi; Le Callet, Patrick] Univ Nantes, IPI Lab, LS2N, Nantes, France; [Mantiuk, Rafal K.] Univ Cambridge, Comp Lab, Cambridge, England; [Wang, Junle] Tencent Games, Turing Lab, Shenzhen, Peoples R China	Nantes Universite; University of Cambridge	Li, J (corresponding author), Univ Nantes, IPI Lab, LS2N, Nantes, France.	jingli.univ@gmail.com; rkm38@cam.ac.uk; wangjunle@gmail.com; suiyi.ling@univ-nantes.fr; patrick.lecallet@univ-nantes.fr	Mantiuk, Rafał K./AAP-9514-2020; Le Callet, Patrick/F-5772-2010	Mantiuk, Rafał K./0000-0003-2353-0349; 				Ailon N., 2009, ADV NEURAL INFORM PR, P25; [Anonymous], 2012, BT50013 ITUR; [Anonymous], 2008, P910 ITUT; [Anonymous], 2012, AAAI; Azari H, 2012, NIPS 12, P126; Bradley R.A., 1955, BIOMETRIKA, V42, P450; BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029; Chen XF, 2013, KEY ENG MATER, V538, P193, DOI 10.4028/www.scientific.net/KEM.538.193; Cortes Corinna, 2007, P 24 INT C MACH LEAR, P169; Crammer K, 2002, ADV NEUR IN, V14, P641; Dangauthier Pierre, 2008, ADV NEURAL INFORM PR, P337; Davis P.J., 2007, METHODS NUMERICAL IN; DYKSTRA O, 1960, BIOMETRICS, V16, P176, DOI 10.2307/2527550; Emerson P, 2013, SOC CHOICE WELFARE, V40, P353, DOI 10.1007/s00355-011-0603-9; Freund Y., 2003, J MACHINE LEARNING R, V4, P933, DOI DOI 10.1162/JMLR.2003.4.6.933; Herbrich Ralf, 2007, ADV NEURAL INFORM PR; Jamieson Kevin G, 2011, ADV NEURAL INFORM PR, P2240; Le Callet P., 2005, SUBJECTIVE QUALITY A; Le Callet Patrick, 2012, EUROPEAN NETWORK QUA, P3; Li J., 2018, IS T ELECT IMAGING H; Li J., 2013, IS T SPIE ELECT IMAG; Li JY, 2013, INT CONF MEAS, P1, DOI 10.1109/ICMTMA.2013.11; Li J, 2012, IEEE IMAGE PROC, P629, DOI 10.1109/ICIP.2012.6466938; LINDLEY DV, 1956, ANN MATH STAT, V27, P986, DOI 10.1214/aoms/1177728069; Lu T., 2011, ICML, P145; Luce R. D., 2005, INDIVIDUAL CHOICE BE; MALLOWS CL, 1957, BIOMETRIKA, V44, P114, DOI 10.2307/2333244; Negahban S., 2012, ADV NEURAL INFORM PR, P2474; Plackett R. L., 1975, Applied Statistics, V24, P193, DOI 10.2307/2346567; PRIM RC, 1957, AT&T TECH J, V36, P1389, DOI 10.1002/j.1538-7305.1957.tb01515.x; Qin T., 2010, ADV NEURAL INFORM PR; Shah Nihar B., 2016, J MACH LEARN RES, V17, P2049; Sheikh H., LIVE IMAGE QUALITY A; Silverstein DA, 1998, IMAGE PROCESSING IMAGE QUALITY IMAGE CAPTURE SYSTEMS CONFERENCE, P242; Soufiani Hossein Azari, 2013, P 26 INT C NEUR INF, P2706; Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288; Wauthier Fabian, 2013, INT C MACH LEARN, P109; Xu, 2011, P 19 ACM INT C MULT, P393, DOI DOI 10.1145/2072298.2072350; Xu Q., 2018, AAAI; Xu Q, 2012, IEEE T MULTIMEDIA, V14, P844, DOI 10.1109/TMM.2012.2190924; Ye P, 2014, PROC CVPR IEEE, P4249, DOI 10.1109/CVPR.2014.541	41	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303047
C	Lindsten, F; Helske, J; Vihola, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lindsten, Fredrik; Helske, Jouni; Vihola, Matti			Graphical model inference: Sequential Monte Carlo meets deterministic approximations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Approximate inference in probabilistic graphical models (PGMs) can be grouped into deterministic methods and Monte-Carlo-based methods. The former can often provide accurate and rapid inferences, but are typically associated with biases that are hard to quantify. The latter enjoy asymptotic consistency, but can suffer from high computational costs. In this paper we present a way of bridging the gap between deterministic and stochastic inference. Specifically, we suggest an efficient sequential Monte Carlo (SMC) algorithm for PGMs which can leverage the output from deterministic inference methods. While generally applicable, we show explicitly how this can be done with loopy belief propagation, expectation propagation, and Laplace approximations. The resulting algorithm can be viewed as a post-correction of the biases associated with these methods and, indeed, numerical results show clear improvements over the baseline deterministic methods as well as over "plain" SMC.	[Lindsten, Fredrik] Uppsala Univ, Dept Informat Technol, Uppsala, Sweden; [Helske, Jouni] Linkoping Univ, Dept Sci & Technol, Norrkoping, Sweden; [Vihola, Matti] Univ Jyvaskyla, Dept Math & Stat, Jyvaskyla, Finland	Uppsala University; Linkoping University; University of Jyvaskyla	Lindsten, F (corresponding author), Uppsala Univ, Dept Informat Technol, Uppsala, Sweden.	fredrik.lindsten@it.uu.se; jouni.helske@liu.se; matti.s.vihola@jyu.fi	Vihola, Matti S/H-7742-2016; Helske, Jouni/J-1186-2019	Vihola, Matti S/0000-0002-8041-7222; Helske, Jouni/0000-0001-7130-793X	Swedish Foundation for Strategic Research (SSF) via the project Probabilistic Modeling and Inference for Machine Learning [ICA16-0015]; Swedish Research Council (VR) via the projects Learning of Large-Scale Probabilistic Dynamical Models [2016-04278]; NewLEADS - New Directions in Learning Dynamical Systems [621-2016-06079]; Academy of Finland [274740, 284513, 312605]	Swedish Foundation for Strategic Research (SSF) via the project Probabilistic Modeling and Inference for Machine Learning; Swedish Research Council (VR) via the projects Learning of Large-Scale Probabilistic Dynamical Models(Swedish Research Council); NewLEADS - New Directions in Learning Dynamical Systems; Academy of Finland(Academy of Finland)	FL has received support from the Swedish Foundation for Strategic Research (SSF) via the project Probabilistic Modeling and Inference for Machine Learning (contract number: ICA16-0015) and from the Swedish Research Council (VR) via the projects Learning of Large-Scale Probabilistic Dynamical Models (contract number: 2016-04278) and NewLEADS - New Directions in Learning Dynamical Systems (contract number: 621-2016-06079). JH and MV have received support from the Academy of Finland (grants 274740, 284513 and 312605).	Andrieu C, 2015, ANN APPL PROBAB, V25, P1030, DOI 10.1214/14-AAP1022; Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; Andrieu C, 2009, ANN STAT, V37, P697, DOI 10.1214/07-AOS574; Bivand R, 2015, J STAT SOFTW, V63, P1; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Buntine W., 2009, P 1 AS C MACH LEARN; Carbonetto Peter, 2006, ADV NEURAL INFORM PR, V19, P201; Cuthill E., 1969, P 1969 24 NAT C; Del Moral P., 2004, PROB APPL S; Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x; Doucet A., 2009, OXFORD HDB NONLINEAR, P656; Durbin J, 1997, BIOMETRIKA, V84, P669, DOI 10.1093/biomet/84.3.669; Ghahramani Z, 1999, ADV NEUR IN, V11, P431; Guarniero P, 2017, J AM STAT ASSOC, V112, P1636, DOI 10.1080/01621459.2016.1222291; Hamze F., 2005, ADV NEURAL INFORM PR, V18, P491; Heng J., 2018, ARXIV170808396; Jacob PE, 2015, STAT COMPUT, V25, P487, DOI 10.1007/s11222-013-9445-x; Jordan MI, 2004, STAT SCI, V19, P140, DOI 10.1214/088342304000000026; KONG A, 1994, J AM STAT ASSOC, V89, P278, DOI 10.2307/2291224; Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572; Minka T., 2002, P 18 C UNC ART INT U; Minka Thomas P., 2001, P 17 C UNC ART INT U; Naesseth C. A., 2015, NIPS WORKSH BLACK BO; Naesseth C.A., 2014, ADV NEURAL INFORM PR, P1862; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Pitt MK, 1999, J AM STAT ASSOC, V94, P590, DOI 10.2307/2670179; Robert C, 2004, MONTE CARLO STAT MET, DOI DOI 10.1007/978-1-4757-4145-2; RUE H, 2005, MONOGRAPHS STAT APPL; Rue H, 2009, J R STAT SOC B, V71, P319, DOI 10.1111/j.1467-9868.2008.00700.x; Ruiz HC, 2017, IEEE T SIGNAL PROCES, V65, P3191, DOI 10.1109/TSP.2017.2686340; Russell S., 2001, P 17 C UNC ART INT, P120; Scott G. S., 2009, P 16 INT C ART INT S; Shephard N, 1997, BIOMETRIKA, V84, P653, DOI 10.1093/biomet/84.3.653; Vihola M., 2018, ARXIV160902541; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P2313, DOI 10.1109/TIT.2005.850091; Wallach H.M., 2009, P 26 ANN INT C MACH; Whiteley N, 2016, BERNOULLI, V22, P494, DOI 10.3150/14-BEJ666	38	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002071
C	Linzner, D; Koeppl, H		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Linzner, Dominik; Koeppl, Heinz			Cluster Variational Approximations for Structure Learning of Continuous-Time Bayesian Networks from Incomplete Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				GENE-EXPRESSION; INFERENCE	Continuous-time Bayesian networks (CTBNs) constitute a general and powerful framework for modeling continuous-time stochastic processes on networks. This makes them particularly attractive for learning the directed structures among interacting entities. However, if the available data is incomplete, one needs to simulate the prohibitively complex CTBN dynamics. Existing approximation techniques, such as sampling and low-order variational methods, either scale unfavorably in system size, or are unsatisfactory in terms of accuracy. Inspired by recent advances in statistical physics, we present a new approximation scheme based on cluster variational methods that significantly improves upon existing variational approximations. We can analytically marginalize the parameters of the approximate CTBN, as these are of secondary importance for structure learning. This recovers a scalable scheme for direct structure learning from incomplete and noisy time-series data. Our approach outperforms existing methods in terms of scalability.	[Linzner, Dominik; Koeppl, Heinz] Tech Univ Darmstadt, Dept Elect Engn & Informat Technol, Darmstadt, Germany; [Koeppl, Heinz] Tech Univ Darmstadt, Dept Biol, Darmstadt, Germany	Technical University of Darmstadt; Technical University of Darmstadt	Linzner, D (corresponding author), Tech Univ Darmstadt, Dept Elect Engn & Informat Technol, Darmstadt, Germany.	dominik.linzner@bcs.tu-darmstadt.de; heinz.koeppl@bcs.tu-darmstadt.de			European Union's Horizon 2020 research and innovation programme [668858]	European Union's Horizon 2020 research and innovation programme	We thank the anonymous reviewers for helpful comments on the previous version of this manuscript. Dominik Linzner is funded by the European Union's Horizon 2020 research and innovation programme under grant agreement 668858.	Acerbi E, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/s12859-014-0387-x; Bansal M, 2006, BIOINFORMATICS, V22, P815, DOI 10.1093/bioinformatics/btl003; Bansal M, 2007, MOL SYST BIOL, V3, DOI 10.1038/msb4100120; Cantone I, 2009, CELL, V137, P172, DOI 10.1016/j.cell.2009.01.055; Cohn I, 2010, J MACH LEARN RES, V11, P2745; Vazquez ED, 2017, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/aa5d22; El-Hay T., 2010, P 27 INT C MACH LEAR, P343; El-Hay T., 2011, P 22 C UNC ART INT; Fan Yu, 2008, AI AND MATH; Friedman Nir, 1999, P 16 INT JOINT C ART; GLAUBER RJ, 1963, J MATH PHYS, V4, P294, DOI 10.1063/1.1703954; KIKUCHI R, 1951, PHYS REV, V81, P988, DOI 10.1103/PhysRev.81.988; Klann M, 2012, INT J MOL SCI, V13, P7798, DOI 10.3390/ijms13067798; Koller D., 2005, P 21 C UNC ART INT U, P421; Nodelman U., 1995, P 18 C UNC ART INT, P378; Nodelman U., 2003, P 19 C UNC ART INT, P451; Opper M., 2008, ADV NEURAL INFORM PR, P1105; Pelizzola Alessandro, 2017, J STAT MECH-THEORY E, V2017, P1; Penfold CA, 2011, INTERFACE FOCUS, V1, P857, DOI 10.1098/rsfs.2011.0053; Rao Vinayak, 2012, J MACHINE LEARNING R, V14, P3295; Schadt EE, 2005, NAT GENET, V37, P710, DOI 10.1038/ng1589; Studer L, 2016, AAAI CONF ARTIF INTE, P2051; Yedidia Jonathan S, 2000, ADV NEURAL INFORM, V13, P657; Yu J, 2004, BIOINFORMATICS, V20, P3594, DOI 10.1093/bioinformatics/bth448	24	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002043
C	Liu, MM; Cheng, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Meimei; Cheng, Guang			Early Stopping for Nonparametric Testing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				REGRESSION; INFERENCE	Early stopping of iterative algorithms is an algorithmic regularization method to avoid over-fitting in estimation and classification. In this paper, we show that early stopping can also be applied to obtain the minimax optimal testing in a general non-parametric setup. Specifically, a Wald-type test statistic is obtained based on an iterated estimate produced by functional gradient descent algorithms in a reproducing kernel Hilbert space. A notable contribution is to establish a "sharp" stopping rule: when the number of iterations achieves an optimal order, testing optimality is achievable; otherwise, testing optimality becomes impossible. As a by-product, a similar sharpness result is also derived for minimax optimal estimation under early stopping. All obtained results hold for various kernel classes, including Sobolev smoothness classes and Gaussian kernel classes.	[Liu, Meimei] Duke Univ, Dept Stat Sci, Durham, NC 27705 USA; [Cheng, Guang] Purdue Univ, Dept Stat, W Lafayette, IN 47907 USA	Duke University; Purdue University System; Purdue University; Purdue University West Lafayette Campus	Liu, MM (corresponding author), Duke Univ, Dept Stat Sci, Durham, NC 27705 USA.	meimei.liu@duke.edu; chengg@purdue.edu						Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Fan  Jianqing, 2007, TEST, V16, P409; Fan JQ, 2001, ANN STAT, V29, P153, DOI 10.1214/aos/996986505; GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751; Guo WS, 2002, J ROY STAT SOC B, V64, P887, DOI 10.1111/1467-9868.00367; Liu M, 2018, ARXIV180206308; Lu Junwei, 2016, ARXIV160106212; MA S, 2017, ADV NEURAL INFORM PR, P3781; Raskutti G, 2014, J MACH LEARN RES, V15, P335; Rudelson M, 2013, ELECTRON COMMUN PROB, V18, P1, DOI 10.1214/ECP.v18-2865; Scholkopf B., 1999, ADV KERNEL METHODS S; Shang ZF, 2013, ANN STAT, V41, P2608, DOI 10.1214/13-AOS1164; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Stewart GW, 2002, SIAM J MATRIX ANAL A, V23, P601, DOI 10.1137/S0895479800371529; Wahba G., 1990, SPLINE MODELS OBSERV; Wei Y., 2017, P ADV NEUR INF PROC, P6067; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2; Zhang T, 2005, ANN STAT, V33, P1538, DOI 10.1214/009053605000000255; [No title captured]	21	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304003
C	Liu, Y; De Brabanter, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Yu; De Brabanter, Kris			Derivative Estimation in Random Design	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CONFIDENCE BANDS; REGRESSION; SIZER; CHOICE	We propose a nonparametric derivative estimation method for random design without having to estimate the regression function. The method is based on a variance-reducing linear combination of symmetric difference quotients. First, we discuss the special case of uniform random design and establish the estimator's asymptotic properties. Secondly, we generalize these results for any distribution of the dependent variable and compare the proposed estimator with popular estimators for derivative estimation such as local polynomial regression and smoothing splines.	[Liu, Yu; De Brabanter, Kris] Iowa State Univ, Dept Comp Sci, Ames, IA 50011 USA; [De Brabanter, Kris] Iowa State Univ, Dept Stat, Ames, IA 50011 USA	Iowa State University; Iowa State University	Liu, Y; De Brabanter, K (corresponding author), Iowa State Univ, Dept Comp Sci, Ames, IA 50011 USA.; De Brabanter, K (corresponding author), Iowa State Univ, Dept Stat, Ames, IA 50011 USA.	yuliu@iastate.edu; kbrabant@iastate.edu						Cabrera J. L. O., 2012, LOCPOL KERNEL LOCAL; Charnigo R, 2007, J OPT SOC AM A, V24, P2578, DOI 10.1364/JOSAA.24.002578; Charnigo R, 2011, TECHNOMETRICS, V53, P238, DOI 10.1198/TECH.2011.09147; Chaudhuri P, 1999, J AM STAT ASSOC, V94, P807, DOI 10.2307/2669996; Dai WL, 2016, J MACH LEARN RES, V17; David H. A., 2003, ORDER STAT, V3rd; De Brabanter K., 2018, BIOMETRIKA; De Brabanter K, 2013, J MACH LEARN RES, V14, P281; Duong T., 2018, KS KERNEL SMOOTHING; EUBANK RL, 1993, J AM STAT ASSOC, V88, P1287, DOI 10.2307/2291269; Fan J.Q., 1996, LOCAL POLYNOMIAL MOD; Gijbels I, 2004, COMMUN STAT-THEOR M, V33, P851, DOI 10.1081/STA-120028730; HALL P, 1990, BIOMETRIKA, V77, P521, DOI 10.2307/2336990; Hardle W, 1990, APPL NONPARAMETRIC R; Heckman NE, 2000, CAN J STAT, V28, P241, DOI 10.2307/3315976; M?ller HG., 2012, NONPARAMETRIC REGRES, V46; MULLER HG, 1987, BIOMETRIKA, V74, P743, DOI 10.2307/2336468; Park C, 2008, COMPUT STAT DATA AN, V52, P3954, DOI 10.1016/j.csda.2008.01.006; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; RAMSAY J. O., 2007, APPL FUNCTIONAL DATA; Ripley B., 2017, PSPLINE PENALIZED SM; Rondonotti V, 2007, ELECTRON J STAT, V1, P268, DOI 10.1214/07-EJS006; ROSENBLATT M, 1956, ANN MATH STAT, V27, P832, DOI 10.1214/aoms/1177728190; RUPPERT D, 1994, ANN STAT, V22, P1346, DOI 10.1214/aos/1176325632; STONE CJ, 1985, ANN STAT, V13, P689, DOI 10.1214/aos/1176349548; Stork D.G., 1993, ADV NEURAL INF PROCE, P164; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; WAHBA G, 1990, COMMUN STAT THEORY, V19, P1685, DOI 10.1080/03610929008830285; Wang WW, 2015, J MACH LEARN RES, V16, P2617; Xia YC, 1998, J ROY STAT SOC B, V60, P797, DOI 10.1111/1467-9868.00155; Zhou SG, 2000, STAT SINICA, V10, P93	32	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303044
C	Luise, G; Rudi, A; Pontil, M; Ciliberto, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Luise, Giulia; Rudi, Alessandro; Pontil, Massimiliano; Ciliberto, Carlo			Differential Properties of Sinkhorn Approximation for Learning with Wasserstein Distance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Applications of optimal transport have recently gained remarkable attention as a result of the computational advantages of entropic regularization. However, in most situations the Sinkhorn approximation to the Wasserstein distance is replaced by a regularized version that is less accurate but easy to differentiate. In this work we characterize the differential properties of the original Sinkhorn approximation, proving that it enjoys the same smoothness of its regularized version and we explicitly provide an efficient algorithm to compute its gradient. We show that this result benefits both theory and applications: on one hand, high order smoothness confers statistical guarantees to learning with Wasserstein approximations. On the other hand, the gradient formula is used to efficiently solve learning and optimization problems in practice. Promising preliminary experiments complement our analysis.	[Luise, Giulia; Pontil, Massimiliano; Ciliberto, Carlo] UCL, Dept Comp Sci, London, England; [Rudi, Alessandro] PSL Res Univ, Ecole Normale Super, Dept Informat, INRIA, Paris, France; [Pontil, Massimiliano] Ist Italiano Tecnol, Genoa, Italy; [Ciliberto, Carlo] Imperial Coll, Dept Elect & Elect Engn, London, England	University of London; University College London; Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Istituto Italiano di Tecnologia - IIT; Imperial College London	Luise, G (corresponding author), UCL, Dept Comp Sci, London, England.	g.luise.16@ucl.ac.uk; alessandro.rudi@inria.fr; m.pontil@ucl.ac.uk; c.ciliberto@imperial.ac.uk			EPSRC [EP/P009069/1]; European Research Council [SEQUOIA 724063]; Engineering and Physical Research Council (EPSRC) [EP/P009069/1]; UK Defence Science and Technology Laboratory (Dstl)	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); European Research Council(European Research Council (ERC)European Commission); Engineering and Physical Research Council (EPSRC)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); UK Defence Science and Technology Laboratory (Dstl)	This work was supported in part by EPSRC Grant N. EP/P009069/1, by the European Research Council (grant SEQUOIA 724063), UK Defence Science and Technology Laboratory (Dstl) and Engineering and Physical Research Council (EPSRC) under grant EP/P009069/1. This is part of the collaboration between US DOD, UK MOD and UK EPSRC under the Multidisciplinary University Research Initiative.	Altschuler J., 2017, ADV NEURAL INFORM PR, P1961; Araya M., 2015, P ADV NEUR INF PROC, P2053; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; BakIr G., 2007, PREDICTING STRUCTURE; Benamou J. D., 2015, SIAM J SCI COMPUTING, V37; Bengio Yoshua, 2000, NEURAL COMPUTATION, V12; Berlinet A., 2011, REPRODUCING KERNEL H; Bertsimas D., 1997, INTRO LINEAR OPTIMIZ; Bonneel N, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925918; Brezis H., 2011, FUNCTION ANAL SOBOLE, pxiv+599, DOI [10.1007/978-0-387-70914-7, DOI 10.1007/978-0-387-70914-7]; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Chapelle O, 2002, MACH LEARN, V46, P131, DOI 10.1023/A:1012450327387; Ciliberto Carlo, 2017, ADV NEURAL INFORM PR; Ciliberto Carlo, 2016, ADV NEURAL INFORM PR, V29, P4412; COMINETTI R, 1994, MATH PROGRAM, V67, P169, DOI 10.1007/BF01582220; Courty N., 2014, LNCS, P1; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi M, 2016, SIAM J IMAGING SCI, V9, P320, DOI 10.1137/15M1032600; Cuturi M, 2014, PR MACH LEARN RES, V32, P685; Dempe S., 2002, FDN BILEVEL PROGRAMM; Edwards C. H., 2012, DOVER BOOKS MATH; Feydy J., 2018, ARXIV E PRINTS; Flamary Remi, 2018, MACHINE LEARNING; Genevay A, 2018, PR MACH LEARN RES, V84; Inc Google, QUICKDRAW DAT; Kollo T., 2006, ADV MULTIVARIATE STA; Korba Anna, 2018, ARXIV180702374; Lin J., 2018, APPL COMPUTATIONAL H, DOI 10.1016/j.acha.2018.09.009.; Moretti V., 2013, UNITEXT; Osokin Anton, 2017, ADV NEURAL INFORM PR, P302; Pedregosa F., 2016, ARXIV160202355; Peyre Gabriel, 2017, TECHNICAL REPORT; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rudi A., 2017, ADV NEURAL INFORM PR, V30, P3888; Rudi A., 2017, ADV NEURAL INFORM PR, V30, P3215; Rudi Alessandro, 2018, ADV NEURAL INFORM PR, P5615; Schmitz MA, 2018, SIAM J IMAGING SCI, V11, P643, DOI 10.1137/17M1140431; SINKHORN R, 1967, PAC J MATH, V21, P343, DOI 10.2140/pjm.1967.21.343; Smola Alex J, 2000, SPARSE GREEDY MATRIX; Steinwart I., 2008, SUPPORT VECTOR MACHI; Villani C, 2008, GRUNDLEHREN MATH WIS; WESTON J, 2003, ADV NEURAL INFORM PR, V15, P873; Ye JB, 2017, IEEE T SIGNAL PROCES, V65, P2317, DOI 10.1109/TSP.2017.2659647; [No title captured]	45	2	2	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000037
C	Luo, R; Wang, JH; Yang, YD; Zhu, ZX; Wang, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Luo, Rui; Wang, Jianhong; Yang, Yaodong; Zhu, Zhanxing; Wang, Jun			Thermostat-assisted continuously-tempered Hamiltonian Monte Carlo for Bayesian learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MOLECULAR-DYNAMICS	We propose a new sampling method, the thermostat-assisted continuously-tempered Hamiltonian Monte Carlo, for Bayesian learning on large datasets and multimodal distributions. It simulates the Nose-Hoover dynamics of a continuously-tempered Hamiltonian system built on the distribution of interest. A significant advantage of this method is that it is not only able to efficiently draw representative i.i.d. samples when the distribution contains multiple isolated modes, but capable of adaptively neutralising the noise arising from mini-batches and maintaining accurate sampling. While the properties of this method have been studied using synthetic distributions, experiments on three real datasets also demonstrated the gain of performance over several strong baselines with various types of neural networks plunged in.	[Luo, Rui; Wang, Jianhong; Yang, Yaodong; Wang, Jun] UCL, London, England; [Zhu, Zhanxing] Peking Univ, Beijing, Peoples R China	University of London; University College London; Peking University	Wang, J (corresponding author), UCL, London, England.	j.wang@cs.ucl.ac.uk	wang, jian/GVS-0711-2022; Zhu, Zhanxing/GQA-7335-2022					Brooks S, 2011, CH CRC HANDB MOD STA, pXIX; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Comer J, 2015, J PHYS CHEM B, V119, P1129, DOI 10.1021/jp506633n; Darve E, 2001, J CHEM PHYS, V115, P9169, DOI 10.1063/1.1410978; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Feroz F, 2008, MON NOT R ASTRON SOC, V384, P449, DOI 10.1111/j.1365-2966.2007.12353.x; Gobbo G, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.061301; Graham MM, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; HOOVER WG, 1985, PHYS REV A, V31, P1695, DOI 10.1103/PhysRevA.31.1695; Jones A, 2011, J CHEM PHYS, V135, DOI 10.1063/1.3626941; Kingma D.P, P 3 INT C LEARNING R; Lelievre T, 2008, NONLINEARITY, V21, P1155, DOI 10.1088/0951-7715/21/6/001; Lenner N, 2016, J CHEM THEORY COMPUT, V12, P486, DOI 10.1021/acs.jctc.5b00751; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Neven, 2014, ADV NEURAL INFORM PR, P3203; NOSE S, 1984, J CHEM PHYS, V81, P511, DOI 10.1063/1.447334; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; Risken H, 1989, FOKKER PLANCK EQUATI, V2nd; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roberts G.O., 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Ye N., 2017, ADV NEURAL INFORM PR, P618	25	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005027
C	Miscouridou, X; Caron, F; Teh, YW		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Miscouridou, Xenia; Caron, Francois; Teh, Yee Whye			Modelling sparsity, heterogeneity, reciprocity and community structure in temporal interaction data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose a novel class of network models for temporal dyadic interaction data. Our objective is to capture important features often observed in social interactions: sparsity, degree heterogeneity, community structure and reciprocity. We use mutually-exciting Hawkes processes to model the interactions between each (directed) pair of individuals. The intensity of each process allows interactions to arise as responses to opposite interactions (reciprocity), or due to shared interests between individuals (community structure). For sparsity and degree heterogeneity, we build the non time dependent part of the intensity function on compound random measures following (Todeschini et al., 2016). We conduct experiments on real-world temporal interaction data and show that the proposed model outperforms competing approaches for link prediction, and leads to interpretable parameters.	[Miscouridou, Xenia; Caron, Francois; Teh, Yee Whye] Univ Oxford, Dept Stat, Oxford, England; [Teh, Yee Whye] DeepMind, London, England	University of Oxford	Miscouridou, X (corresponding author), Univ Oxford, Dept Stat, Oxford, England.	miscouri@stats.ox.ac.uk; caron@stats.ox.ac.uk; y.w.teh@stats.ox.ac.uk			ERC under the European Union [617071]; EPSRC [EP/P026753/1]; Alan Turing Institute under EPSRC [EP/N510129/1]; A. G. Leventis Foundation	ERC under the European Union; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Alan Turing Institute under EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); A. G. Leventis Foundation	The authors thank the reviewers and area chair for their constructive comments. XM, FC and YWT acknowledge funding from the ERC under the European Union's 7th Framework programme (FP7/2007-2013) ERC grant agreement no. 617071. FC acknowledges support from EPSRC under grant EP/P026753/1 and from the Alan Turing Institute under EPSRC grant EP/N510129/1. XM acknowledges support from the A. G. Leventis Foundation.	[Anonymous], 2017, ARXIV170808719; [Anonymous], 2014, ICML; [Anonymous], 2013, ADV NEURAL INFORM PR; [Anonymous], 2016, BAYESIAN NONPARAMETR; Blundell  C., 2012, ADV NEURAL INFORM PR, V15, P5249; Borgs C, 2018, J MACH LEARN RES, V18, P1; Cai D., 2016, ADV NEURAL INFORM PR, P4249; Caron  F., 2017, J ROYAL STAT SOC B, V79; Crane H., 2015, ARXIV150908185; Crane H, 2018, J AM STAT ASSOC, V113, P1311, DOI 10.1080/01621459.2017.1341413; DuBois C., 2013, P 16 INT C ARTIFICIA, P238; Griffin JE, 2017, J R STAT SOC B, V79, P525; Hawkes A. G., 1971, J ROYAL STAT SOC B, V15, P5249; Herlau T, 2016, ADV NEUR IN, V29; Janson  S., 2017, J STAT PHYS; Janson  S., 2017, ARXIV170206389; Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107; Kingman J. F. C, 1993, POISSON PROCESSES, V3; KINGMAN JFC, 1967, PAC J MATH, V21, P59, DOI 10.2140/pjm.1967.21.59; Leskovec J, 2014, SNAP DATASETS STANFO; Ng Y. C., 2017, ARXIV171004008; Rasmussen JG, 2013, METHODOL COMPUT APPL, V15, P623, DOI 10.1007/s11009-011-9272-5; Todeschini  A., 2016, ARXIV160202114; Veitch V., 2015, ARXIV151203099; Williamson RC, 2016, J MACH LEARN RES, V17, P1; Zhou MY, 2015, JMLR WORKSH CONF PRO, V38, P1135	26	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302036
C	Nar, K; Sastry, SS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Nar, Kamil; Sastry, S. Shankar			Step Size Matters in Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NEURAL NETWORKS	Training a neural network with the gradient descent algorithm gives rise to a discrete-time nonlinear dynamical system. Consequently, behaviors that are typically observed in these systems emerge during training, such as convergence to an orbit but not to a fixed point or dependence of convergence on the initialization. Step size of the algorithm plays a critical role in these behaviors: it determines the subset of the local optima that the algorithm can converge to, and it specifies the magnitude of the oscillations if the algorithm converges to an orbit. To elucidate the effects of the step size on training of neural networks, we study the gradient descent algorithm as a discrete-time dynamical system, and by analyzing the Lyapunov stability of different solutions, we show the relationship between the step size of the algorithm and the solutions that can be obtained with this algorithm. The results provide an explanation for several phenomena observed in practice, including the deterioration in the training error with increased depth, the hardness of estimating linear mappings with large singular values, and the distinct performance of deep residual networks.	[Nar, Kamil; Sastry, S. Shankar] Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Nar, K (corresponding author), Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	nar@eecs.berkeley.edu; sastry@eecs.berkeley.edu			U.S. Office of Naval Research (ONR) MURI [N00014-16-1-2710]	U.S. Office of Naval Research (ONR) MURI(MURIOffice of Naval Research)	This research was supported by the U.S. Office of Naval Research (ONR) MURI grant N00014-16-1-2710.	[Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; Barabanov NE, 2002, IEEE T NEURAL NETWOR, V13, P292, DOI 10.1109/72.991416; Bartlett P. L., 2018, ARXIV180405012CSLG; Bartlett P. L., 2017, ADV NEURAL INFORM PR; Bartlett P. L., 2018, ARXIV180206093CSLG; Daniel C, 2016, P AAAI C ART INT; Gunasekar S., 2017, ADV NEURAL INFORM PR, P6151; Hardt M., 2016, ARXIV161104231CSLG; JACOBS RA, 1988, NEURAL NETWORKS, V1, P295, DOI 10.1016/0893-6080(88)90003-2; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Khalil HK., 2002, NONLINEAR SYSTEM, V3rd; Magoulas GD, 1997, NEURAL NETWORKS, V10, P69, DOI 10.1016/S0893-6080(96)00052-4; MATSUOKA K, 1992, NEURAL NETWORKS, V5, P495, DOI 10.1016/0893-6080(92)90011-7; Michel A. N., 1988, NEURAL INFORMATION P, P554; Nar K., 2018, ARXIV180308203CSLG; Neyshabur B., 2017, ARXIV170503071CSLG; Rolinek M., 2018, ARXIV180205074CSLG; Sastry S., 2013, NONLINEAR SYSTEMS AN, V10; Saxe A. M., 2013, ARXIV13126120CSNE	20	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303043
C	Neitz, A; Parascandolo, G; Bauer, S; Scholkopf, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Neitz, Alexander; Parascandolo, Giambattista; Bauer, Stefan; Schoelkopf, Bernhard			Adaptive Skip Intervals: Temporal Abstraction for Recurrent Dynamical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce a method which enables a recurrent dynamics model to be temporally abstract. Our approach, which we call Adaptive Skip Intervals (ASI), is based on the observation that in many sequential prediction tasks, the exact time at which events occur is irrelevant to the underlying objective. Moreover, in many situations, there exist prediction intervals which result in particularly easy-to-predict transitions. We show that there are prediction tasks for which we gain both computational efficiency and prediction accuracy by allowing the model to make predictions at a sampling rate which it can choose itself.	[Neitz, Alexander; Parascandolo, Giambattista; Bauer, Stefan; Schoelkopf, Bernhard] Max Planck Inst Intelligent Syst, Stuttgart, Germany; [Parascandolo, Giambattista; Bauer, Stefan; Schoelkopf, Bernhard] Max Planck ETH Ctr Learning Syst, Tubingen, Germany	Max Planck Society	Neitz, A (corresponding author), Max Planck Inst Intelligent Syst, Stuttgart, Germany.	aneitz@tue.mpg.de			International Max Planck Research School for Intelligent Systems; Max Planck ETH Center for Learning Systems	International Max Planck Research School for Intelligent Systems; Max Planck ETH Center for Learning Systems	This work is partially supported by the International Max Planck Research School for Intelligent Systems and the Max Planck ETH Center for Learning Systems.	Arulkumaran K., 2017, ARXIV PREPRINT ARXIV; Ba J., 2017, P 3 INT C LEARN REPR; Barto AG, 2003, DISCRETE EVENT DYN S, V13, P41, DOI 10.1023/A:1025696116075; Belzner L., 2016, P 31 ANN ACM S APPL, P254; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Braylan A., 2000, SPACE, V1600, P1800; Daw N. D., 2012, MODEL BASED REINFORC; Ebert Frederik, 2017, P P MACHINE LEARNING; Finn Chelsea, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2786, DOI 10.1109/ICRA.2017.7989324; Glorot X, 2011, P 14 INT C ART INT S, V15, P315, DOI DOI 10.1177/1753193410395357; Hassabis Demis, 2018, ARXIV180203006; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Heess N., 2016, ARXIV161005182; Jayaraman D., 2018, ARXIV180807784; Ke N. R., 2017, ARXIV171102326; Legg S, 2007, MIND MACH, V17, P391, DOI 10.1007/s11023-007-9079-x; Muller M., 2007, INFORM RETRIEVAL MUS, V3, P69, DOI [10.1007/978-3-540- 74048-3_4, DOI 10.1007/978-3-540-74048-3]; Oh J., 2015, P ADV NEUR INF PROC, P2863; Oh J, 2017, ADV NEUR IN, V30; Parascandolo Giambattista, 2017, ARXIV171200961; Pearl J, 2009, CAUSALITY, DOI 10.1017/CBO9780511803161; Peters J, 2017, ADAPT COMPUT MACH LE; Peters J, 2016, J R STAT SOC B, V78, P947, DOI 10.1111/rssb.12167; Pong V., 2018, ARXIV180209081; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Racaniere S., 2017, P 31 INT C NEUR INF, P5694; Racaniere Sebastien, 2017, ARXIV170402254; Scholkopf Bernhard, 2012, ICML; Silver D., 2016, ARXIV161208810; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Veness J., 2017, ARXIV170906009; Watter Manuel, 2015, ADV NEURAL INFORM PR, V2, P2746	32	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004038
C	Sachan, M; Dubey, A; Mitchell, T; Roth, D; Xing, EP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sachan, Mrinmaya; Dubey, Avinava; Mitchell, Tom; Roth, Dan; Xing, Eric P.			Learning Pipelines with Limited Data and Domain Knowledge: A Study in Parsing Physics Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					As machine learning becomes more widely used in practice, we need new methods to build complex intelligent systems that integrate learning with existing software, and with domain knowledge encoded as rules. As a case study, we present such a system that learns to parse Newtonian physics problems in textbooks. This system, Nuts&Bolt s, learns a pipeline process that incorporates existing code, pre-learned machine learning models, and human engineered rules. It jointly trains the entire pipeline to prevent propagation of errors, using a combination of labelled and unlabelled data. Our approach achieves a good performance on the parsing task, outperforming the simple pipeline and its variants. Finally, we also show how Nuts&Bolt s can be used to achieve improvements on a relation extraction task and on the end task of answering Newtonian physics problems.	[Sachan, Mrinmaya; Dubey, Avinava; Mitchell, Tom; Xing, Eric P.] Carnegie Mellon Univ, Sch Comp Sci, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Roth, Dan] Univ Penn, Dept Comp & Informat Sci, Philadelphia, PA 19104 USA; [Xing, Eric P.] Petuum Inc, Pittsburgh, PA 15222 USA	Carnegie Mellon University; University of Pennsylvania	Sachan, M (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Machine Learning Dept, Pittsburgh, PA 15213 USA.	mrinmays@cs.cmu.edu; akdubey@cs.cmu.edu; tom.mitchell@cs.cmu.edu; danroth@seas.upenn.edu; epxing@cs.cmu.edu			ONR [N000141712463]; NIH [R01GM114311]	ONR(Office of Naval Research); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This work is supported by the ONR grant N000141712463 and the NIH grant R01GM114311. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of ONR or NIH.	Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2015, P 28 INT C NEUR INF; [Anonymous], 2016, ARXIV160306318; Bach S., 2017, J MACH LEARN RES, V18, P1; Brocheler M., 2010, P 26 C UNCERTAINTY A, P73; Carreira J, 2012, INT J COMPUT VISION, V98, P243, DOI 10.1007/s11263-011-0507-2; Chang MW, 2012, MACH LEARN, V88, P399, DOI 10.1007/s10994-012-5296-5; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; de Campos T. E., 2009, P INT C COMP VIS THE; Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1; DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242; Finkel J. R., 2006, P 2006 C EMP METH NA, P618; Futrelle RP, 2003, PROC INT CONF DOC, P1007; Gu CH, 2009, PROC CVPR IEEE, P1030, DOI 10.1109/CVPRW.2009.5206727; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; HOLLINGSHEAD K, 2007, P 45 ANN M ASS COMP, P952; Kembhavi A, 2016, LECT NOTES COMPUT SC, V9908, P235, DOI 10.1007/978-3-319-46493-0_15; Kingma DP, 2014, ADV NEUR IN, P3581, DOI DOI 10.5555/2969033.2969226; Kokkinos I, 2010, PROC CVPR IEEE, P2520, DOI 10.1109/CVPR.2010.5539956; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LEVANDOWSKY M, 1971, NATURE, V234, P34, DOI 10.1038/234034a0; LI Y., 2017, ARXIV; Lukasiewicz J., 1988, STUDIA FILOZOFICZNE, V270; Marciniak T., 2005, P 9 C COMP NAT LANG, P136; Mintz M., 2009, P ACL, P1003, DOI DOI 10.3115/1690219.1690287; OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076; Park E, 2016, IEEE WINT CONF APPL; Platanios E. A., 2016, INT C MACH LEARN, P1416; Platanios EA, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P682; Platanios Emmanouil Antonios, 2017, ABS170507086 CORR; Punyakanok V, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1124; Raman  Karthik, 2013, P 19 ACM SIGKDD INT, P86; Rantalankila P, 2014, PROC CVPR IEEE, P2417, DOI 10.1109/CVPR.2014.310; Ratner A., 2017, ARXIV171110160; Ratner A, 2016, ADV NEUR IN, V29; Sachan M., 2017, P 2017 C EMP METH NA, P773; Sachan  Mrinmaya, 2018, P 24 SIGKDD C KNOWL; Sachan  Mrinmaya, 2018, PARSING PROGRAMS FRA; Seo Min Joon, 2014, P AAAI; Seo Min Joon, 2015, P EMNLP; Shet V, 2011, INT J COMPUT VISION, V93, P141, DOI 10.1007/s11263-010-0343-9; Socher R., 2011, P 28 INT C INT C MAC, P129; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; TOWELL GG, 1994, ARTIF INTELL, V70, P119, DOI 10.1016/0004-3702(94)90105-8; Tu ZW, 2005, INT J COMPUT VISION, V63, P113, DOI 10.1007/s11263-005-6642-x; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wachsmuth  Henning, 2015, TEXT ANAL PIPELINES, V9383; Wu Y., 2016, GOOGLES NEURAL MACHI; Zhang Chiyuan, 2016, ARXIV161103530; Zhang ZM, 2011, PROC CVPR IEEE, P1497, DOI 10.1109/CVPR.2011.5995411; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	56	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300014
C	Shahrampour, S; Tarokh, V		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shahrampour, Shahin; Tarokh, Vahid			Learning Bounds for Greedy Approximation with Explicit Feature Maps from Multiple Kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MATCHING PURSUITS; MATRIX	Nonlinear kernels can be approximated using finite-dimensional feature maps for efficient risk minimization. Due to the inherent trade-off between the dimension of the (mapped) feature space and the approximation accuracy, the key problem is to identify promising (explicit) features leading to a satisfactory out-of-sample performance. In this work, we tackle this problem by efficiently choosing such features from multiple kernels in a greedy fashion. Our method sequentially selects these explicit features from a set of candidate features using a correlation metric. We establish an out-of-sample error bound capturing the trade-off between the error in terms of explicit features (approximation error) and the error due to spectral properties of the best model in the Hilbert space associated to the combined kernel (spectral error). The result verifies that when the (best) underlying data model is sparse enough, i.e., the spectral error is negligible, one can control the test error with a small number of explicit features, that can scale poly-logarithmically with data. Our empirical results show that given a fixed number of explicit features, the method can achieve a lower test error with a smaller time cost, compared to the state-of-the-art in data-dependent random features.	[Shahrampour, Shahin] Texas A&M Univ, Dept Ind & Syst Engn, College Stn, TX 77843 USA; [Tarokh, Vahid] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA	Texas A&M University System; Texas A&M University College Station; Duke University	Shahrampour, S (corresponding author), Texas A&M Univ, Dept Ind & Syst Engn, College Stn, TX 77843 USA.	shahin@tamu.edu; vahid.tarokh@duke.edu	Poor, H. Vincent/S-5027-2016	Poor, H. Vincent/0000-0002-2062-131X	DARPA [W911NF1810134]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	We gratefully acknowledge the support of DARPA Grant W911NF1810134.	Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Beirami A., 2017, ADV NEURAL INFORM PR; Belkin M., 2018, P MACHINE LEARNING R, P1348; Bullins B., 2018, PROC INT C LEARN REP, P1; Chang Wei-Cheng, 2017, P 26 INT JOINT C ART; Cortes, 2009, NEURAL INF PROCESS S; Cortes C., 2010, ICML 2010 P 27 INT C; Cortes C, 2017, PR MACH LEARN RES, V70; Cortes C, 2014, PR MACH LEARN RES, V32, P1179; Cortes C, 2012, J MACH LEARN RES, V13, P795; Cotter A., 2011, ARXIV11094603; DAVIS G, 1994, OPT ENG, V33, P2183, DOI 10.1117/12.173207; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619; FRIEDMAN JH, 1981, J AM STAT ASSOC, V76, P817, DOI 10.2307/2287576; Giordano Ryan, 2018, ARXIV180600550; Gonen M, 2011, J MACH LEARN RES, V12, P2211; Gribonval R, 2006, IEEE T INFORM THEORY, V52, P255, DOI 10.1109/TIT.2005.860474; Huang FR, 2018, PR MACH LEARN RES, V80; Joachims T., 2006, P 12 ACM SIGKDD INT, V06, P217, DOI DOI 10.1145/1150402.1150429; KANDOLA J, 2002, OPTIMIZING KERNEL AL; Kar P., 2012, ARTIF INTELL, P583; Kloft M, 2011, J MACH LEARN RES, V12, P953; Le Quoc, 2013, INT C MACH LEARN, V85; Locatello F, 2017, PR MACH LEARN RES, V54, P860; Lozano A. C., 2011, 14 INT C ART INT STA, P452; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Minh H.Q., 2006, INT C COMP LEARN THE, P154, DOI DOI 10.1007/11776420_14; Mohri M., 2018, FDN MACHINE LEARNING; Nair PB, 2003, J MACH LEARN RES, V3, P781, DOI 10.1162/jmlr.2003.3.4-5.781; Oglic D, 2016, ADV NEUR IN, V29; Oliva JB, 2016, JMLR WORKSH CONF PRO, V51, P1078; Oppenheim A. V., 1998, SIGNALS SYSTEMS; PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Rudi A, 2017, ADV NEUR IN, V30; Shahrampour S, 2018, AAAI CONF ARTIF INTE, P4026; Shahrampour S, 2018, IEEE DECIS CONTR P, P1168, DOI 10.1109/CDC.2018.8619558; Shalev-Shwartz S, 2010, SIAM J OPTIMIZ, V20, P2807, DOI 10.1137/090759574; SINDHWANI V, 2011, ADV NEURAL INFORM PR, P2519; Sinha A., 2016, ADV NEURAL INFORM PR, P1298; Smola A. J., 2000, P 17 INT C MACH LEAR, P911; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Vincent P, 2002, MACH LEARN, V48, P165, DOI 10.1023/A:1013955821559; Wang J, 2012, IEEE T SIGNAL PROCES, V60, P6202, DOI 10.1109/TSP.2012.2218810; Wang SW, 2018, PR MACH LEARN RES, V80; Williams CKI, 2001, ADV NEUR IN, V13, P682; Xu Jian-Wu, 2006, IEEE INT C AC SPEECH, V5; Yang C., 2004, NIPS, V17, P1561; Yang JY, 2014, PR MACH LEARN RES, V32; Yang ZC, 2015, JMLR WORKSH CONF PRO, V38, P1098; Yen I.E.-H., 2014, ADV NEURAL INF PROCE, P2456; Yu F. X., 2015, ARXIV150303893; Yu Felix X, 2016, ADV NEURAL INFORM PR, V29, P1975	57	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304068
C	Sharan, V; Gopalan, P; Wieder, U		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sharan, Vatsal; Gopalan, Parikshit; Wieder, Udi			Efficient Anomaly Detection via Matrix Sketching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHMS; BOUNDS	We consider the problem of finding anomalies in high-dimensional data using popular PCA based anomaly scores. The naive algorithms for computing these scores explicitly compute the PCA of the covariance matrix which uses space quadratic in the dimensionality of the data. We give the first streaming algorithms that use space that is linear or sublinear in the dimension. We prove general results showing that any sketch of a matrix that satisfies a certain operator norm guarantee can be used to approximate these scores. We instantiate these results with powerful matrix sketching techniques such as Frequent Directions and random projections to derive efficient and practical algorithms for these problems, which we validate over real-world data sets. Our main technical contribution is to prove matrix perturbation inequalities for operators arising in the computation of these measures.	[Sharan, Vatsal] Stanford Univ, Stanford, CA 94305 USA; [Sharan, Vatsal; Gopalan, Parikshit; Wieder, Udi] VMware Res, Stanford, CA USA	Stanford University	Sharan, V (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	vsharan@stanford.edu; pgopalan@vmware.com; uwieder@vmware.com			NSF [1813049]; ONR [N00014-18-1-2295]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	The authors thank David Woodruff for suggestions on using communication complexity tools to show lower bounds on memory usage for approximating anomaly scores and Weihao Kong for several useful discussions on estimating singular values and vectors using random projections. We also thank Steve Mussmann, Neha Gupta, Yair Carmon and the anonymous reviewers for detailed feedback on initial versions of the paper. VS's contribution was partially supported by NSF award 1813049, and ONR award N00014-18-1-2295.	Aggarwal, 2015, OUTLIER ANAL, DOI [DOI 10.1007/978-1-4614-6396-2, 10.1007/978-1-4614-6396-2]; Ailon N, 2009, SIAM J COMPUT, V39, P302, DOI 10.1137/060673096; Alaoui A., 2015, P 28 INT C NEURAL IN, P775; Breunig MM, 2000, SIGMOD REC, V29, P93, DOI 10.1145/335191.335388; Chakrabarti A, 2003, ANN IEEE CONF COMPUT, P107, DOI 10.1109/CCC.2003.1214414; Chiang L.H., 2000, FAULT DETECTION DIAG; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Cohen M. B., 2015, ARXIV150702268; Cohen MB, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1758; Cohen MB, 2015, ACM S THEORY COMPUT, P163, DOI 10.1145/2746539.2746569; Danziger S. A., 2006, IEEE ACM T COMPUTATI; Davis JJ, 2011, COMPUT SECUR, V30, P353, DOI 10.1016/j.cose.2011.05.008; Drineas P, 2006, SIAM J COMPUT, V36, P132, DOI 10.1137/S0097539704442684; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Ghashami M, 2016, SIAM J COMPUT, V45, P1762, DOI 10.1137/15M1009718; Guyon I., 2004, RESULT ANAL NIPS 200, P17; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Holgersson HET, 2012, J APPL STAT, V39, P2713, DOI 10.1080/02664763.2012.725464; Horn Roger A, 1994, TOPICS MATRIX ANAL, V1, P994; Huang Hao, 2015, P VLDB ENDOW, V9; Huang L, 2007, IEEE INFOCOM SER, P134, DOI 10.1109/INFCOM.2007.24; Jordan M.I., 2007, ADV NEURAL INFORM PR, P617; Lakhina A, 2005, ACM SIGCOMM COMP COM, V35, P217, DOI 10.1145/1090191.1080118; Lakhina A, 2004, ACM SIGCOMM COMP COM, V34, P219, DOI 10.1145/1030194.1015492; Liberty E, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P581, DOI 10.1145/2487575.2487623; Liu FT, 2008, IEEE DATA MINING, P413, DOI 10.1109/ICDM.2008.17; Magen A, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1422; Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035; Mi HB, 2013, IEEE T PARALL DISTR, V24, P1245, DOI 10.1109/TPDS.2013.21; Portnoff Rebecca, 2018, THESIS; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Qin SJ, 2003, J CHEMOMETR, V17, P480, DOI 10.1002/cem.800; Russell E, 2000, CHEMOMETRICS INTELLI, V81, P51; Schneider M, 2016, MACH LEARN, V105, P305, DOI 10.1007/s10994-016-5567-7; Shyu Mei-ling, 2003, P IEEE FDN NEW DIR D; Vadhan SP, 2011, FOUND TRENDS THEOR C, V7, P1, DOI 10.1561/0400000010; Viswanath B, 2014, PROCEEDINGS OF THE 23RD USENIX SECURITY SYMPOSIUM, P223; Wedin P.-A., 1972, BIT (Nordisk Tidskrift for Informationsbehandling), V12, P99, DOI 10.1007/BF01932678; Wei Wang, 2004, Advances in Neural Networks - ISNN 2004. International Symposium on Neural Networks. Proceedings (Lecture Notes in Comput. Sci. Vol.3174)), P657; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060; Xu W, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P117	43	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002060
C	Sherman, E; Shpitser, I		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sherman, Eli; Shpitser, Ilya			Identification and Estimation Of Causal Effects from Dependent Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				INFERENCE; DIAGRAMS	The assumption that data samples are independent and identically distributed (iid) is standard in many areas of statistics and machine learning. Nevertheless, in some settings, such as social networks, infectious disease modeling, and reasoning with spatial and temporal data, this assumption is false. An extensive literature exists on making causal inferences under the iid assumption [17, 11, 26, 21], even when unobserved confounding bias may be present. But, as pointed out in [19], causal inference in non-iid contexts is challenging due to the presence of both unobserved confounding and data dependence. In this paper we develop a general theory describing when causal inferences are possible in such scenarios. We use segregated graphs [20], a generalization of latent projection mixed graphs [28], to represent causal models of this type and provide a complete algorithm for nonparametric identification in these models. We then demonstrate how statistical inference may be performed on causal parameters identified by this algorithm. In particular, we consider cases where only a single sample is available for parts of the model due to full interference, i.e., all units are pathwise dependent and neighbors' treatments affect each others' outcomes [24]. We apply these techniques to a synthetic data set which considers users sharing fake news articles given the structure of their social network, user activity levels, and baseline demographics and socioeconomic covariates.	[Sherman, Eli; Shpitser, Ilya] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA	Johns Hopkins University	Sherman, E (corresponding author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.	esherman@jhu.edu; ilyas@cs.jhu.edu			American Institute of Mathematics via the SQuaRE program; National Institutes of Health [R01 AI127271-01 A1]; Office of Naval Research [N00014-18-1-2760]; Defense Advanced Research Projects Agency (DARPA) [HR0011-18-C-0049]	American Institute of Mathematics via the SQuaRE program; National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Office of Naval Research(Office of Naval Research); Defense Advanced Research Projects Agency (DARPA)(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	The second author would like to thank the American Institute of Mathematics for supporting this research via the SQuaRE program. This project is sponsored in part by the National Institutes of Health grant R01 AI127271-01 A1, the Office of Naval Research grant N00014-18-1-2760 and the Defense Advanced Research Projects Agency (DARPA) under contract HR0011-18-C-0049. The content of the information does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred.	Arbour D, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P715, DOI 10.1145/2939672.2939791; Drton M, 2009, BERNOULLI, V15, P736, DOI 10.3150/08-BEJ172; Hudgens MG, 2008, J AM STAT ASSOC, V103, P832, DOI 10.1198/016214508000000292; Kenny D. A., 2006, DYADIC DATA ANAL; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Maier M., 2013, ARXIV13024381; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Ogburn EL, 2014, STAT SCI, V29, P559, DOI 10.1214/14-STS501; Pearl J, 1995, BIOMETRIKA, V82, P669, DOI 10.1093/biomet/82.4.669; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Pena J.M., 2016, C PROB GRAPH MOD, P392; Pena J.M., 2018, BEHAVIORMETRIKA, P1; Richardson T, 2003, SCAND J STAT, V30, P145, DOI 10.1111/1467-9469.00323; Richardson T. S., 2017, WORKING PAPER; ROBINS J, 1986, MATH MODELLING, V7, P1393, DOI 10.1016/0270-0255(86)90088-6; Robins JM., 1999, STAT MODELS EPIDEMIO; Shalizi CR, 2011, SOCIOL METHOD RES, V40, P211, DOI 10.1177/0049124111404820; Shpitser I., 2006, P 21 NAT C ART INT A; Shpitser I., 2015, ADV NEURAL INFORM PR, P28; Sobel ME, 2006, J AM STAT ASSOC, V101, P1398, DOI 10.1198/016214506000000636; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Tchetgen E. J. Tchetgen, 2017, WORKING PAPER; Tchetgen EJT, 2012, STAT METHODS MED RES, V21, P55, DOI 10.1177/0962280210386779; Tian J., 2002, R290L U CAL DEP COMP; Tian J., 2002, P 18 C UNC ART INT, P519; Verma T, 1990, P 6 C UNC ART INT, P255	28	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004003
C	Singh, S; Uppal, A; Li, BY; Li, CL; Zaheer, M; Poczos, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Singh, Shashank; Uppal, Ananya; Li, Boyue; Li, Chun-Liang; Zaheer, Manzil; Poczos, Barnabas			Nonparametric Density Estimation with Adversarial Losses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study minimax convergence rates of nonparametric density estimation under a large class of loss functions called "adversarial losses", which, besides classical L-p losses, includes maximum mean discrepancy (MMD), Wasserstein distance, and total variation distance. These losses are closely related to the losses encoded by discriminator networks in generative adversarial networks (GANs). In a general framework, we study how the choice of loss and the assumed smoothness of the underlying density together determine the minimax rate. We also discuss implications for training GANs based on deep ReLU networks, and more general connections to learning implicit generative models in a minimax statistical sense.	[Singh, Shashank; Li, Chun-Liang; Zaheer, Manzil; Poczos, Barnabas] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Singh, Shashank] Carnegie Mellon Univ, Dept Stat & Data Sci, Pittsburgh, PA 15213 USA; [Uppal, Ananya] Carnegie Mellon Univ, Dept Math Sci, Pittsburgh, PA 15213 USA; [Li, Boyue] Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University	Singh, S (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.; Singh, S (corresponding author), Carnegie Mellon Univ, Dept Stat & Data Sci, Pittsburgh, PA 15213 USA.	sss1@cs.cmu.edu			NSF [IIS1563887, DGE-1252522]; Darpa D3M program; AFRL [FA8750-17-2-0212]	NSF(National Science Foundation (NSF)); Darpa D3M program; AFRL(United States Department of DefenseUS Air Force Research Laboratory)	This work was partly supported by NSF grant IIS1563887, the Darpa D3M program, AFRL FA8750-17-2-0212, and the NSF Graduate Research Fellowship DGE-1252522.	Abbasnejad Ehsan, 2018, DEEP LIPSCHITZ NETWO; Arjovsky M, 2017, PR MACH LEARN RES, V70; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Berlinet A., 2011, REPRODUCING KERNEL H; Biau G., 2015, LECT NEAREST NEIGHBO; Bottou L., 2017, ARXIV171207822; Brown LD, 1998, ANN STAT, V26, P279; Canas Guillermo D, 2012, ADV NEURAL INFORM PR, P2492; CHIB S, 1995, AM STAT, V49, P327, DOI 10.2307/2684568; DUDLEY RM, 1972, Z WAHRSCHEINLICHKEIT, V22, P323, DOI 10.1007/BF00532491; Efromovich S, 2010, WIRES COMPUT STAT, V2, P467, DOI 10.1002/wics.97; Endres DM, 2003, IEEE T INFORM THEORY, V49, P1858, DOI 10.1109/TIT.2003.813506; Goldenshluger A, 2014, PROBAB THEORY REL, V159, P479, DOI 10.1007/s00440-013-0512-1; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A, 2012, J MACH LEARN RES, V13, P723; Gulrajani I, 2017, P NIPS 2017; Kantorovich Leonid Vasilevich, SPACE COMPLETELY ADD; Kingma D.P, P 3 INT C LEARNING R; Kolmogorov A., 1933, GIORN I ITAL DEGLI A, V4; Lei Jing, 2018, ARXIV180410556; Leoni G., 2017, GRADUATE STUDIES MAT, V181; Liang Tengyuan, 2017, ARXIV171208244; Liang Tengyuan, 2018, ARXIV180206132; Mohamed Shakir, 2016, ARXIV161003483; Mroueh Youssef, 2017, P 6 INT C LEARN REPR; Nagarajan V, 2017, ADV NEUR IN, V30; Nowozin S, 2016, ADV NEUR IN, V29; Owen M., 2007, PRACTICAL SIGNAL PRO; Ramdas A, 2015, AAAI CONF ARTIF INTE, P3571; Ramdas A, 2017, ENTROPY-SWITZ, V19, DOI 10.3390/e19020047; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Robert C.P., 2004, MONTE CARLO METHODS; Singh Shashank, 2018, ARXIV180208855; SMIRNOV N, 1948, ANN MATH STAT, V19, P279, DOI 10.1214/aoms/1177730256; Sriperumbudur B, 2016, BERNOULLI, V22, P1839, DOI 10.3150/15-BEJ713; Sriperumbudur BK, 2012, ELECTRON J STAT, V6, P1550, DOI 10.1214/12-EJS722; Sriperumbudur BK, 2010, IEEE INT SYMP INFO, P1428, DOI 10.1109/ISIT.2010.5513626; Sutherland D. J., 2017, ICLR; Szekely GJ, 2007, ANN STAT, V35, P2769, DOI 10.1214/009053607000000505; Tolstikhin Ilya, 2017, J MACHINE LEARNING R, V18, P3002; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Weed J, 2017, ARXIV170700087; Wendland H, 2004, SCATTERED DATA APPRO, V17; Yarotsky D, 2017, NEURAL NETWORKS, V94, P103, DOI 10.1016/j.neunet.2017.07.002	52	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004075
C	Struminsky, K; Lacoste-Julien, S; Osokin, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Struminsky, Kirill; Lacoste-Julien, Simon; Osokin, Anton			Quantifying Learning Guarantees for Convex but Inconsistent Surrogates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CLASSIFICATION; CONSISTENCY	We study consistency properties of machine learning methods based on minimizing convex surrogates. We extend the recent framework of Osokin et al. [14] for the quantitative analysis of consistency properties to the case of inconsistent surrogates. Our key technical contribution consists in a new lower bound on the calibration function for the quadratic surrogate, which is non-trivial (not always zero) for inconsistent cases. The new bound allows to quantify the level of inconsistency of the setting and shows how learning with inconsistent surrogates can have guarantees on sample complexity and optimization difficulty. We apply our theory to two concrete cases: multi-class classification with the tree-structured loss and ranking with the mean average precision loss. The results show the approximation-computation trade-offs caused by inconsistent surrogates and their potential benefits.	[Struminsky, Kirill; Osokin, Anton] Natl Res Univ, Higher Sch Econ, Moscow, Russia; [Lacoste-Julien, Simon] Univ Montreal, MILA, Montreal, PQ, Canada; [Lacoste-Julien, Simon] Univ Montreal, DIRO, Montreal, PQ, Canada; [Osokin, Anton] Skolkovo Inst Sci & Technol, Moscow, Russia; [Osokin, Anton] Samsung HSE Joint Lab, Moscow, Russia	HSE University (National Research University Higher School of Economics); Universite de Montreal; Universite de Montreal; Skolkovo Institute of Science & Technology	Struminsky, K (corresponding author), Natl Res Univ, Higher Sch Econ, Moscow, Russia.		Osokin, Anton/D-7398-2012	Osokin, Anton/0000-0002-8807-5132	Samsung Research; Samsung Electronics; Ministry of Education and Science of the Russian Federation [14.756.31.0001]; NSERC [RGPIN-2017-06936]	Samsung Research(Samsung); Samsung Electronics(Samsung); Ministry of Education and Science of the Russian Federation(Ministry of Education and Science, Russian Federation); NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC))	This work was partly supported by Samsung Research, by Samsung Electronics, by the Ministry of Education and Science of the Russian Federation (grant 14.756.31.0001) and by the NSERC Discovery Grant RGPIN-2017-06936.	Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Buffoni D., 2011, ICML; Calauz`enes C., 2012, NIPS; Choromanska A., 2013, NIPS WORKSH EXTREME; CRAMMER K, 2001, J MACHINE LEARNING R, V2, P265, DOI DOI 10.1162/15324430260185628; DEFAZIO A, 2014, NIPS; Dorn W.S., 1960, Q APPL MATH, V18, P155, DOI [https://doi.org/10.1090/qam/112751, DOI 10.1090/QAM/112751, 10.1090/qam/112751]; Duchi J., 2010, ICML; Long Phil, 2013, ICML; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Osokin Anton, 2017, NIPS; Pedregosa F, 2017, J MACH LEARN RES, V18, P1; Pillaud-Vivien Loucas, 2018, COLT; Pires Avila, 2013, ICML; Ramaswamy H. G., 2013, NIPS; Ramaswamy HG, 2016, J MACH LEARN RES, V17; Rosasco Lorenzo, 2016, NIPS; Taskar Ben, 2003, NIPS; Tewari A, 2007, J MACH LEARN RES, V8, P1007; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Zhang T, 2004, ANN STAT, V32, P56; Zhang T, 2004, J MACH LEARN RES, V5, P1225	23	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300062
C	Sun, T; Sun, YJ; Yin, WT		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sun, Tao; Sun, Yuejiao; Yin, Wotao			On Markov Chain Gradient Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHMS	Stochastic gradient methods are the workhorse (algorithms) of large-scale optimization problems in machine learning, signal processing, and other computational sciences and engineering. This paper studies Markov chain gradient descent, a variant of stochastic gradient descent where the random samples are taken on the trajectory of a Markov chain. Existing results of this method assume convex objectives and a reversible Markov chain and thus have their limitations. We establish new non-ergodic convergence under wider step sizes, for nonconvex problems, and for non-reversible finite-state Markov chains. Nonconvexity makes our method applicable to broader problem classes. Non-reversible finite-state Markov chains, on the other hand, can mix substatially faster. To obtain these results, we introduce a new technique that varies the mixing levels of the Markov chains. The reported numerical results validate our contributions.	[Sun, Tao] Natl Univ Def Technol, Coll Comp, Changsha 410073, Hunan, Peoples R China; [Sun, Yuejiao; Yin, Wotao] Univ Calif Los Angeles, Dept Math, Los Angeles, CA 90095 USA	National University of Defense Technology - China; University of California System; University of California Los Angeles	Sun, T (corresponding author), Natl Univ Def Technol, Coll Comp, Changsha 410073, Hunan, Peoples R China.	nudtsuntao@163.com; sunyj@math.ucla.edu; wotaoyin@math.ucla.edu			National Key R&D Program of China [2017YFB0202902]; AFOSR MURI [FA9550-18-1-0502]; NSF [DMS-1720237]; ONR [N000141712162]; NSFC [11728105]	National Key R&D Program of China; AFOSR MURI(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)MURI); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); NSFC(National Natural Science Foundation of China (NSFC))	The work is supported in part by the National Key R&D Program of China 2017YFB0202902, China Scholarship Council. The work of Y. Sun andW. Yin was supported in part by AFOSR MURI FA9550-18-1-0502, NSF DMS-1720237, NSFC 11728105, and ONR N000141712162.	Dyer Martin, 1993, COMB PROBAB COMPUT, V2, P271, DOI [10.1017/S0963548300000675, DOI 10.1017/S0963548300000675]; Fill J.A., 1991, ANN APPL PROBAB, P62; Jerrum Mark, 1996, MARKOV CHAIN MONTE C; Johansson B., 2007, 2007 46 IEEE C DEC C, P4705, DOI [10.1109/CDC.2007.4434888, DOI 10.1109/CDC.2007.4434888]; Johansson B, 2009, SIAM J OPTIMIZ, V20, P1157, DOI 10.1137/08073038X; Montenegro R, 2006, FOUND TRENDS THEOR C, V1, P237, DOI 10.1561/0400000003; Oldenburger R., 1940, DUKE MATH J, V6, P357, DOI [10.1215/S0012-7094-40-00627-5, DOI 10.1215/S0012-7094-40-00627-5]; Ram SS, 2009, SIAM J OPTIMIZ, V20, P691, DOI 10.1137/080726380; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Robbins H., 1971, P S OH STAT U COL O, P233, DOI DOI 10.1016/B978-0-12-604550-5.50015-8; Rockafellar R.T., 2015, CONVEX ANAL; Turitsyn KS, 2011, PHYSICA D, V240, P410, DOI 10.1016/j.physd.2010.10.003; Zeng JS, 2018, IEEE T SIGNAL PROCES, V66, P2834, DOI 10.1109/TSP.2018.2818081	15	2	2	3	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004045
C	Sundaramoorthi, G; Yezzi, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sundaramoorthi, Ganesh; Yezzi, Anthony			Variational PDEs for Acceleration on Manifolds and Application to Diffeomorphisms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				VISCOSITY SOLUTIONS; MINIMIZATION; FLOWS	We consider the optimization of cost functionals on infinite dimensional manifolds and derive a variational approach to accelerated methods on manifolds. We demonstrate the methodology on the infinite-dimensional manifold of diffeomorphisms, motivated by optical flow problems in computer vision. We build on a variational approach to accelerated optimization in finite dimensions, and generalize that approach to infinite dimensional manifolds. We derive the continuum evolution equations, which are partial differential equations (PDE), and relate them to mechanical principles. A particular case of our approach can be viewed as a generalization of the L-2 optimal mass transport problem. Our approach evolves an infinite number of particles endowed with mass, represented as a mass density. The density evolves with the optimization variable, and endows the particles with dynamics. This is different than current accelerated methods where only a single particle moves and hence the dynamics does not depend on mass. We derive theory and the PDEs for acceleration, and illustrate the behavior of this new scheme.	[Sundaramoorthi, Ganesh] United Technol Res Ctr, E Hartford, CT 06118 USA; [Yezzi, Anthony] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA	Raytheon Technologies; University System of Georgia; Georgia Institute of Technology	Sundaramoorthi, G (corresponding author), United Technol Res Ctr, E Hartford, CT 06118 USA.	sundarga1@utrc.utc.com; ayezzi@ece.gatech.edu	Yezzi, Anthony/AAB-4235-2020		ARO [W911NF-18-1-0281]; NSF [CCF-1526848]	ARO; NSF(National Science Foundation (NSF))	This research was partially funded by ARO W911NF-18-1-0281 and NSF CCF-1526848.	Angenent S, 2003, SIAM J MATH ANAL, V35, P61, DOI 10.1137/S0036141002410927; [Anonymous], 2013, INTRO MECH SYMMETRY; Arnold V.I., 2013, MATH METHODS CLASSIC, V60; Beg MF, 2005, INT J COMPUT VISION, V61, P139, DOI 10.1023/B:VISI.0000043755.93987.aa; Benamou JD, 2000, NUMER MATH, V84, P375, DOI 10.1007/s002119900117; Benyamin M., 2018, ABS181000410; Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Bubeck S., 2015, CORR; Charpiat G, 2007, INT J COMPUT VISION, V73, P325, DOI 10.1007/s11263-006-9966-2; CRANDALL MG, 1983, T AM MATH SOC, V277, P1, DOI 10.2307/1999343; do Carmo M. P., 1992, RIEMANNIAN GEOMETRY; EBIN DG, 1970, ANN MATH, V92, P102, DOI 10.2307/1970699; Flammarion N., 2015, C LEARN THEOR, P658; Gangbo W, 1996, ACTA MATH-DJURSHOLM, V177, P113, DOI 10.1007/BF02392620; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Hosseini R., 2017, ARXIV170603267; Hu C., 2009, ADV NEURAL INF PROCE, P781; Ji S., 2009, P 26 ANN INT C MACH, P457, DOI DOI 10.1145/1553374.1553434; Jojic V., 2010, P 27 INT C MACH LEAR, P503; Krichene W., 2015, ADV NEURAL INFORM PR, P2845; Li H., 2015, PROC 28 INT C NEURAL, P379; Liu Y., 2017, ADV NEURAL INFORM PR, P4875; Miller MI, 2006, J MATH IMAGING VIS, V24, P209, DOI 10.1007/s10851-005-3624-0; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y, 2014, INTRO LECT CONVEX OP; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Nesterov Y, 2008, MATH PROGRAM, V112, P159, DOI 10.1007/s10107-006-0089-x; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; ROUY E, 1992, SIAM J NUMER ANAL, V29, P867, DOI 10.1137/0729053; Sethian J.A., 1999, LEVEL SET METHODS FA, V3; Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939; Sundaramoorthi G., 2018, ARXIV E PRINTS; Sundaramoorthi G, 2007, INT J COMPUT VISION, V73, P345, DOI 10.1007/s11263-006-0635-2; Villani C., 2003, TOPICS OPTIMAL TRANS; Wedel A, 2009, LECT NOTES COMPUT SC, V5604, P23, DOI 10.1007/978-3-642-03061-1_2; Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113; Yang L. F., 2016, ARXIV161202803; Yang YC, 2015, IEEE T PATTERN ANAL, V37, P1053, DOI 10.1109/TPAMI.2014.2360380; Yezzi A. J., 2017, CORR; Zhang Hongyi, 2016, ADV NEURAL INFORM PR, V29, P4592	44	2	2	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303076
C	Tan, CH; Zhang, T; Ma, SQ; Liu, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tan, Conghui; Zhang, Tong; Ma, Shiqian; Liu, Ji			Stochastic Primal-Dual Method for Empirical Risk Minimization with O(1) Per-Iteration Complexity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Regularized empirical risk minimization problem with linear predictor appears frequently in machine learning. In this paper, we propose a new stochastic primaldual method to solve this class of problems. Different from existing methods, our proposed methods only require O(1) operations in each iteration. We also develop a variance-reduction variant of the algorithm that converges linearly. Numerical experiments suggest that our methods are faster than existing ones such as proximal SGD, SVRG and SAGA on high-dimensional problems.	[Tan, Conghui] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Zhang, Tong] Tencent AI Lab, Bellevue, WA USA; [Ma, Shiqian] Univ Calif Davis, Davis, CA 95616 USA; [Liu, Ji] Univ Rochester, Tencent AI Lab, Rochester, NY 14627 USA	Chinese University of Hong Kong; University of California System; University of California Davis; University of Rochester	Tan, CH (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.	chtan@se.cuhk.edu.hk; tongzhang@tongzhang-ml.org; sqma@math.ucdavis.edu; ji.liu.uwisc@gmail.com	Zhang, Tong/HGC-1090-2022		startup package in Department of Mathematics at UC Davis; National Natural Science Foundation of China [11631013]; NSF [CCF1718513]; IBM faculty award; NEC fellowship	startup package in Department of Mathematics at UC Davis; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); NSF(National Science Foundation (NSF)); IBM faculty award(International Business Machines (IBM)); NEC fellowship	S. Ma is partly supported by a startup package in Department of Mathematics at UC Davis and the National Natural Science Foundation of China under Grant 11631013. J. Liu is in part supported by NSF CCF1718513, IBM faculty award, and NEC fellowship.	Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; DANG C., 2014, RANDOMIZED 1 ORDER M RANDOMIZED 1 ORDER M; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Koepelevich, 1976, EKONOMIKA MATEMATICH, V12, P747; Konecny J, 2017, OPTIM METHOD SOFTW, V32, P993, DOI 10.1080/10556788.2017.1298596; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nesterov Y, 2004, INTRO LECT CONVEX OP, P15; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Palaniappan B., 2016, NEURIPS, P1416; Shalev-Shwartz S, 2014, PR MACH LEARN RES, V32; Shamir O., 2013, P INT C MACH LEARN A, P71; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Yu Adams Wei, 2015, ARXIV150803390; ZHANG YC, 2017, J MACHINE LEARNING R, V0018, P02939	18	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002087
C	Tobar, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tobar, Felipe			Bayesian Nonparametric Spectral Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				FREQUENCY-ANALYSIS; ALGORITHM	Spectral estimation (SE) aims to identify how the energy of a signal (e.g., a time series) is distributed across different frequencies. This can become particularly challenging when only partial and noisy observations of the signal are available, where current methods fail to handle uncertainty appropriately. In this context, we propose a joint probabilistic model for signals, observations and spectra, where SE is addressed as an exact inference problem. Assuming a Gaussian process prior over the signal, we apply Bayes' rule to find the analytic posterior distribution of the spectrum given a set of observations. Besides its expressiveness and natural account of spectral uncertainty, the proposed model also provides a functional-form representation of the power spectral density, which can be optimised efficiently. Comparison with previous approaches, in particular against Lomb-Scargle, is addressed theoretically and also experimentally in three different scenarios. Code and demo available at github. com/GAMES-UChile.	[Tobar, Felipe] Univ Chile, Santiago, Chile	Universidad de Chile	Tobar, F (corresponding author), Univ Chile, Santiago, Chile.	ftobar@dim.uchile.cl			project Conicyt-PIA Center for Mathematical Modeling [AFB170001]; project Fondecyt-Iniciacion [11171165]	project Conicyt-PIA Center for Mathematical Modeling; project Fondecyt-Iniciacion(Comision Nacional de Investigacion Cientifica y Tecnologica (CONICYT)CONICYT FONDECYT)	This work was funded by the projects Conicyt-PIA #AFB170001 Center for Mathematical Modeling and Fondecyt-Iniciacion #11171165.	Ahrabian A., 2012, SENSOR SIGNAL PROCES, P1, DOI [10.1049/ic.2012.0119, DOI 10.1049/IC.2012.0119]; Bochner S., 1959, LECT FOURIER INTEGRA; Boloix-Tortosa R, 2018, IEEE T NEUR NET LEAR, V29, P5499, DOI 10.1109/TNNLS.2018.2805019; Bretthorst GL, 1988, LECT NOTES STAT; Choudhuri N, 2004, J AM STAT ASSOC, V99, P1050, DOI 10.1198/016214504000000557; COOLEY JW, 1965, MATH COMPUT, V19, P297, DOI 10.2307/2003354; Cuevas A., 2017, 2017 CHILEAN C ELECT, P1; DJURIC PM, 1995, IEEE SIGNAL PROC LET, V2, P213, DOI 10.1109/97.473649; Durrande N, 2016, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.50; Duvenaud D., 2014, AUTOMATIC MODEL CONS; GOLDBERGER AL, 1991, THEORY OF HEART, P583; Gregory PC, 2001, AIP CONF PROC, V568, P557, DOI 10.1063/1.1381917; Hensman J, 2018, J MACH LEARN RES, V18, P1; Huijse P, 2012, IEEE T SIGNAL PROCES, V60, P5135, DOI 10.1109/TSP.2012.2204260; Jaynes E. T., 1987, Maximum-Entropy and Bayesian Analysis and Estimation Problems. Proceedings of the Third Workshop on Maximum Entropy and Bayesian Methods in Applied Statistics, P1; Kay S.M, 1988, MODERN SPECTRAL ESTI; Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865; LOMB NR, 1976, ASTROPHYS SPACE SCI, V39, P447, DOI 10.1007/BF00648343; Mandic D.P., 2009, COMPLEX VALUED NONLI, DOI 10.1002/9780470742624; Matthews AGD, 2017, J MACH LEARN RES, V18, P1; Parra G., 2017, P NIPS, P6681; PISARENKO VF, 1973, GEOPHYS J ROY ASTR S, V33, P347, DOI 10.1111/j.1365-246X.1973.tb03424.x; POWELL MJD, 1964, COMPUT J, V7, P155, DOI 10.1093/comjnl/7.2.155; Qi Y, 2002, INT CONF ACOUST SPEE, P1473; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Royden H. L., 1968, REAL ANAL, V2; Schuster A., 1898, TERRESTRIAL MAGNETIS, V3, P13, DOI [10.1029/TM003i001p00013, DOI 10.1029/TM003I001P00013]; Ting CM, 2011, IEEE T BIO-MED ENG, V58, P321, DOI 10.1109/TBME.2010.2088396; Tobar F., 2015, ADV NEURAL INFORM PR, P3501; Tobar F., 2015, NIPS 2015; Tobar F, 2015, INT CONF ACOUST SPEE, P2209, DOI 10.1109/ICASSP.2015.7178363; Turner RE, 2010, THESIS; Turner RE, 2014, IEEE T SIGNAL PROCES, V62, P6171, DOI 10.1109/TSP.2014.2362100; Ulrich K.R., 2015, ADV NEURAL INFORM PR, P1999; Walker G, 1931, P R SOC LOND A-CONTA, V131, P518, DOI 10.1098/rspa.1931.0069; Wang YY, 2012, ASTROPHYS J, V756, DOI 10.1088/0004-637X/756/1/67; Wilson A., 2013, INT C MACH LEARN, P1067	37	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004066
C	van Merrienboer, B; Moldovan, D; Wiltschko, AB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		van Merrienboer, Bart; Moldovan, Dan; Wiltschko, Alexander B.			Tangent: Automatic differentiation using source-code transformation for dynamically typed array programming	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The need to efficiently calculate first-and higher-order derivatives of increasingly complex models expressed in Python has stressed or exceeded the capabilities of available tools. In this work, we explore techniques from the field of automatic differentiation (AD) that can give researchers expressive power, performance and strong usability. These include source-code transformation (SCT), flexible gradient surgery, efficient in-place array operations, and higher-order derivatives. We implement and demonstrate these ideas in the Tangent software library for Python, the first AD framework for a dynamic language that uses SCT.	[van Merrienboer, Bart] Google Brain, MILA, Montreal, PQ, Canada; [Moldovan, Dan; Wiltschko, Alexander B.] Google Brain, Montreal, PQ, Canada		van Merrienboer, B (corresponding author), Google Brain, MILA, Montreal, PQ, Canada.	bartvm@google.com; mdan@google.com; alexbw@google.com						Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Al-Rfou R., 2016, ARXIV160502688, V472, P473; Baydin Atilim Gunes, 2015, ABS150205767 CORR; Bengio Yoshua, 2013, ARXIV; Bischof Christian H, 2000, TECHNICAL REPORT; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Buckman Jacob, 2018, ARXIV180305071; Chen TQ, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P579; DRISCOLL JR, 1989, J COMPUT SYST SCI, V38, P86, DOI 10.1016/0022-0000(89)90034-2; Ganin Y., 2016, JMLR, V17, P2096; Griewank A, 2008, OTHER TITL APPL MATH, V105, P1, DOI 10.1137/1.9780898717761; Guelton Serge, 2015, Computational Science and Discovery, V8, DOI 10.1088/1749-4680/8/1/014001; Heess N., 2015, NIPS; Jang Eric, 2017, P 5 INT C LEARN REPR; Lam S.K., 2015, P 2 WORKSHOP LLVM CO, P1, DOI [10.1145/2833157.2833162, DOI 10.1145/2833157.2833162]; Lillicrap T. P., 2014, ARXIV14110247; Maclaurin D., 2015, ICML 2015 AUTOML WOR; Naumann U, 2012, SOFTW ENVIRON TOOLS, V24, P1; Nokland, 2016, ADV NEURAL INFORM PR, V29, P1037; Oliphant TE, 2006, A GUIDE TO NUMPY, V1; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Pearlmutter Barak A, 2005, PERTURBATION CONFUSI; Rae JW, 2016, ADV NEUR IN, V29; Tokui Seiya, 2015, NIPS 2015 LEARNINGSY, V5; van den Oord A., 2017, CORR; Williams RJ, 1990, NEURAL COMPUT, V2, P490, DOI 10.1162/neco.1990.2.4.490	26	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000073
C	Wang, YN; Chen, X; Zhou, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Yining; Chen, Xi; Zhou, Yuan			Near-Optimal Policies for Dynamic Multinomial Logit Assortment Selection Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA			dynamic assortment planning; multinomial logit choice model; trisection algorithm; regret analysis	OPTIMIZATION; CHOICE	In this paper we consider the dynamic assortment selection problem under an uncapacitated multinomial-logit (MNL) model. By carefully analyzing a revenue potential function, we show that a trisection based algorithm achieves an item-independent regret bound of O(root T log log T), which matches information theoretical lower bounds up to iterated logarithmic terms. Our proof technique draws tools from the unimodal/convex bandit literature as well as adaptive confidence parameters in minimax multi-armed bandit problems.	[Wang, Yining] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Chen, Xi] NYU, Stern Sch Business, New York, NY 10003 USA; [Zhou, Yuan] Indiana Univ, Comp Sci Dept, Bloomington, IN 47405 USA; [Zhou, Yuan] Univ Illinois, Dept Ind & Enterprise Syst Engn, Urbana, IL 61801 USA	Carnegie Mellon University; New York University; Indiana University System; Indiana University Bloomington; University of Illinois System; University of Illinois Urbana-Champaign	Wang, YN (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	yiningwa@cs.cmu.edu; xchen3@stern.nyu.edu; yuanz@illinois.edu		Wang, Yining/0000-0001-9410-0392	Alibaba Innovation Research Award; Bloomberg Data Science Research Grant	Alibaba Innovation Research Award; Bloomberg Data Science Research Grant	Xi Chen would like to thank the support from Alibaba Innovation Research Award and Bloomberg Data Science Research Grant. Part of the work was done when Yuan Zhou was visiting the Shanghai University of Finance and Economics.	Agarwal A, 2013, SIAM J OPTIMIZ, V23, P213, DOI 10.1137/110850827; Agrawal S., 2017, COLT; Agrawal S., 2016, EC; Audibert Jean-Yves, 2009, COLT; BORSCHSUPAN A, 1990, J ECONOMETRICS, V43, P373, DOI 10.1016/0304-4076(90)90126-E; Bubeck S., 2009, ALT; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Caro F, 2007, MANAGE SCI, V53, P276, DOI 10.1287/mnsc.1060.0613; Chen X., 2017, ARXIV170906192; Cohen-Addad V., 2016, ARXIV160401999; Combes R., 2014, ICML; Cope EW, 2009, IEEE T AUTOMAT CONTR, V54, P1243, DOI 10.1109/TAC.2009.2019797; Darling D., 1985, H ROBBINS SELECTED P, P254; Golrezaei N, 2014, MANAGE SCI, V60, P1532, DOI 10.1287/mnsc.2014.1939; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Jamieson K., 2014, COLT; Kok AG, 2008, RETAIL SUPPLY CHAIN, P99; MCFADDEN D, 1980, J BUS, V53, pS13, DOI 10.1086/296093; Rusmevichientong P, 2012, OPER RES, V60, P865, DOI 10.1287/opre.1120.1063; Rusmevichientong P, 2010, OPER RES, V58, P1666, DOI 10.1287/opre.1100.0866; Saure D, 2013, M&SOM-MANUF SERV OP, V15, P387, DOI 10.1287/msom.2013.0429; WILLIAMS HCWL, 1977, ENVIRON PLANN A, V9, P285, DOI 10.1068/a090285; Yu J. Y., 2011, ICML	23	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303013
C	Webb, S; Golinski, A; Zinkov, R; Siddharth, N; Rainforth, T; Teh, YW; Wood, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Webb, Stefan; Golinski, Adam; Zinkov, Robert; Siddharth, N.; Rainforth, Tom; Teh, Yee Whye; Wood, Frank			Faithful Inversion of Generative Models for Effective Amortized Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Inference amortization methods share information across multiple posterior-inference problems, allowing each to be carried out more efficiently. Generally, they require the inversion of the dependency structure in the generative model, as the modeller must learn a mapping from observations to distributions approximating the posterior. Previous approaches have involved inverting the dependency structure in a heuristic way that fails to capture these dependencies correctly, thereby limiting the achievable accuracy of the resulting approximations. We introduce an algorithm for faithfully, and minimally, inverting the graphical model structure of any generative model. Such inverses have two crucial properties: a) they do not encode any independence assertions that are absent from the model and b) they are local maxima for the number of true independencies encoded. We prove the correctness of our approach and empirically show that the resulting minimally faithful inverses lead to better inference amortization than existing heuristic approaches.	[Webb, Stefan; Golinski, Adam; Siddharth, N.; Rainforth, Tom; Teh, Yee Whye] Univ Oxford, Oxford, England; [Zinkov, Robert; Wood, Frank] Univ British Columbia, Vancouver, BC, Canada	University of Oxford; University of British Columbia	Webb, S (corresponding author), Univ Oxford, Oxford, England.	info@stefanwebb.me		Narayanaswamy, Siddharth/0000-0003-4911-7333; Rainforth, Tom/0000-0001-7939-4230	EPSRC AIMS CDT [EP/L015987/2]; DARPA D3M [FA8750-17-2-0093]; EPSRC/MURI [EP/N019474/1]; European Research Council under the European Union [617071]; ERC StG IDIU; The Alan Turing Institute under the EPSRC [EP/N510129/1]; DARPA PPAML through the U.S. AFRL [FA8750-14-2-0006]; Intel Big Data Center grant	EPSRC AIMS CDT(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); DARPA D3M; EPSRC/MURI(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); European Research Council under the European Union(European Research Council (ERC)); ERC StG IDIU; The Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); DARPA PPAML through the U.S. AFRL; Intel Big Data Center grant	SW and AG gratefully acknowledge support from the EPSRC AIMS CDT through grant EP/L015987/2. RZ acknowledges support under DARPA D3M, under Cooperative Agreement FA8750-17-2-0093. NS was supported by EPSRC/MURI grant EP/N019474/1. TR and YWT are supported in part by the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013)/ERC grant agreement no. 617071. TR further acknowledges support of the ERC StG IDIU. FW was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1, DARPA PPAML through the U.S. AFRL under Cooperative Agreement FA8750-14-2-0006, an Intel Big Data Center grant, and DARPA D3M, under Cooperative Agreement FA8750-17-2-0093.	Burda Yuri, 2016, INT C LEARN REPR; Chen R.T., 2018, ADV NEURAL INFORM PR, V31; Cremer C., 2017, INT C LEARN REPR WOR; Cremer Chris, 2018, P INT C MACH LEARN; Fishelson M, 2004, J COMPUT BIOL, V11, P263, DOI 10.1089/1066527041410409; Gan Zhe, 2015, ADV NEURAL INFORM PR; Germain Mathieu, 2015, P INT C MACH LEARN; Gershman S.J., 2014, COGSCI; Huang Chin-Wei, 2018, P INT C MACH LEARN; Jang E., 2017, ICLR; Johnson Matthew J, 2016, ARXIV160306277V2STAT; Kingma D., 2014, ADAM METHOD STOCHAST; Kingma D.P., 2016, ADV NEURAL INFORM PR; Koller D., 2009, PROBABILISTIC GRAPHI; Krishnan Rahul G, 2017, P NAT C ART INT AAAI; Le T, 2018, INT C LEARN REPR; Le Tuan Anh, 2017, P INT C ART INT STAT; Maaloe Lars, 2016, P INT C MACH LEARN; Maddison Chris J, 2017, ICLR; Maddison Chris J, 2017, ADV NEURAL INFORM PR; Naesseth Christian A, 2018, P INT C ART INT STAT; NEAL R, 1998, 9805 U TOR DEP STAT; Neal R. M., 1990, LEARNING STOCHASTIC, V64, P1577; Paige Brooks, 2016, P INT C MACH LEARN; Papamakarios George, 2015, PROB INT WORKSH NEUR; Rainforth Tom, 2018, P INT C MACH LEARN; Ranganath Rajesh, 2015, P INT C ART INT STAT; Rezende D. J., 2015, P INT C MACH LEARN; Ritchie D., 2016, ARXIV161005735; Stuhlmuller A., 2013, ADV NEURAL INFORM PR; Uria B, 2016, J MACH LEARN RES, V17; van den Oord Aaron, 2016, P INT C MACH LEARN; Vinyals O., 2016, ADV NEURAL INFORM PR; Wu Yuxin, 2017, INT C LEARN REPR	35	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303010
C	Wu, L; Ma, C; Weinan, E		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wu, Lei; Ma, Chao; Weinan, E.			How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The question of which global minima are accessible by a stochastic gradient decent (SGD) algorithm with specific learning rate and batch size is studied from the perspective of dynamical stability. The concept of non-uniformity is introduced, which, together with sharpness, characterizes the stability property of a global minimum and hence the accessibility of a particular SGD algorithm to that global minimum. In particular, this analysis shows that learning rate and batch size play different roles in minima selection. Extensive empirical results seem to correlate well with the theoretical findings and provide further support to these claims.	[Wu, Lei] Peking Univ, Sch Math Sci, Beijing 100081, Peoples R China; [Ma, Chao; Weinan, E.] Princeton Univ, Program Appl & Computat Math, Princeton, NJ 08544 USA; [Weinan, E.] Princeton Univ, Dept Math, Princeton, NJ 08544 USA; [Weinan, E.] Beijing Inst Big Data Res, Beijing 100081, Peoples R China	Peking University; Princeton University; Princeton University	Wu, L (corresponding author), Peking Univ, Sch Math Sci, Beijing 100081, Peoples R China.	leiwu@pku.edu.cn; chaom@princeton.edu; weinan@math.princeton.edu			ONR [N00014-13-1-0338]; Major Program of NNSFC [91130005]	ONR(Office of Naval Research); Major Program of NNSFC	We are grateful to Zhanxing Zhu for very helpful discussions. The worked performed here is supported in part by ONR grant N00014-13-1-0338 and the Major Program of NNSFC under grant 91130005.	Gardiner C., 2009, STOCHASTIC METHODS; Goyal Priya, 2017, ARXIV170602677; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hoffer Elad, 2017, ADV NEURAL INFORM PR, P1729; Hu Wenqing, 2017, ARXIV170507562; Jastrzebski Stanislaw, 2017, ARXIV171104623; Keskar N.S., 2017, ICLR; Ma SY, 2018, PR MACH LEARN RES, V80; Smith Samuel L., 2018, INT C LEARN REPR; Strogatz S.H., 2015, NONLINEAR DYNAMICS C, DOI DOI 10.1201/9780429492563; Wilson AC, 2017, ADV NEUR IN, V30; Wu L., 2017, ARXIV170610239; Yin Dong, 2018, P INT C ART INT STAT, P1998; Zhu Z., 2018, ARXIV180300195	15	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002079
C	Xu, J; Hsu, D; Maleki, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Xu, Ji; Hsu, Daniel; Maleki, Arian			Benefits of over-parameterization with EM	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CONVERGENCE PROPERTIES; MAXIMUM-LIKELIHOOD; ALGORITHM	Expectation Maximization (EM) is among the most popular algorithms for maximum likelihood estimation, but it is generally only guaranteed to find its stationary points of the log-likelihood objective. The goal of this article is to present theoretical and empirical evidence that over-parameterization can help EM avoid spurious local optima in the log-likelihood. We consider the problem of estimating the mean vectors of a Gaussian mixture model in a scenario where the mixing weights are known. Our study shows that the global behavior of EM, when one uses an over-parameterized model in which the mixing weights are treated as unknown, is better than that when one uses the (correct) model with the mixing weights fixed to the known values. For symmetric Gaussians mixtures with two components, we prove that introducing the (statistically redundant) weight parameters enables EM to find the global maximizer of the log-likelihood starting from almost any initial mean parameters, whereas EM without this over-parameterization may very often fail. For other Gaussian mixtures, we provide empirical evidence that shows similar behavior. Our results corroborate the value of over-parameterization in solving non-convex optimization problems, previously observed in other domains..	[Xu, Ji; Hsu, Daniel; Maleki, Arian] Columbia Univ, New York, NY 10027 USA	Columbia University	Xu, J (corresponding author), Columbia Univ, New York, NY 10027 USA.	jixu@cs.columbia.edu; djhsu@cs.columbia.edu; arian@stat.columbia.edu		Hsu, Daniel/0000-0002-3495-7113	NSF [DMREF-1534910, CCF-1740833]; Cheung-Kong Graduate School of Business Fellowship	NSF(National Science Foundation (NSF)); Cheung-Kong Graduate School of Business Fellowship	DH and JX were partially supported by NSF awards DMREF-1534910 and CCF-1740833, and JX was also partially supported by a Cheung-Kong Graduate School of Business Fellowship. We thank Jiantao Jiao for a helpful discussion about this problem.	Balakrishnan S, 2017, ANN STAT, V45, P77, DOI 10.1214/16-AOS1435; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chr?tien S., 2008, ESAIM-PROBAB STAT, V12, P308, DOI DOI 10.1051/PS:2007041; d'Aspremont A., 2005, PROC 17 INT C NEURAL, P41; DASKALAKIS C., 2017, C LEARN THEOR, V65, P704; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Du SS, 2018, PR MACH LEARN RES, V80; Haeffele B. D., 2015, ARXIV PREPRINT ARXIV; Jin Chi, 2016, ADV NEURAL INFORM PR, P4116; Klusowski J. M., 2016, STAT GUARANTEES ESTI; Koltchinskii V, 2011, ECOLE ETE PROBABILIT; Livni R., 2014, NIPS, V1, P855; Nguyen Q, 2018, PR MACH LEARN RES, V80; Nguyen Q, 2017, PR MACH LEARN RES, V70; REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034; SAFRAN I., 2017, ARXIV171208968; Soltani M., 2018, INT C ART INT STAT A, P1417; Tseng P, 2004, MATH OPER RES, V29, P27, DOI 10.1287/moor.1030.0073; WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060; Xu Ji, 2016, ADV NEURAL INFORM PR, P2676; Xu L, 1996, NEURAL COMPUT, V8, P129, DOI 10.1162/neco.1996.8.1.129; Yan B., 2017, ADV NEURAL INFORM PR, P6956	24	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005026
C	Yamane, I; Yger, F; Atif, J; Sugiyama, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yamane, Ikko; Yger, Florian; Atif, Jamal; Sugiyama, Masashi			Uplift Modeling from Separate Labels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				RATIO	Uplift modeling is aimed at estimating the incremental impact of an action on an individual's behavior, which is useful in various application domains such as targeted marketing (advertisement campaigns) and personalized medicine (medical treatments). Conventional methods of uplift modeling require every instance to be jointly equipped with two types of labels: the taken action and its outcome. However, obtaining two labels for each instance at the same time is difficult or expensive in many real-world problems. In this paper, we propose a novel method of uplift modeling that is applicable to a more practical setting where only one type of labels is available for each instance. We show a mean squared error bound for the proposed estimator and demonstrate its effectiveness through experiments.	[Yamane, Ikko; Sugiyama, Masashi] Univ Tokyo, Chiba, Japan; [Yamane, Ikko; Yger, Florian; Sugiyama, Masashi] RIKEN, Ctr Adv Intelligence Project AIP, Tokyo, Japan; [Yger, Florian; Atif, Jamal] Univ PSL, LAMSADE, CNRS, Univ Paris Dauphine, Paris, France	University of Tokyo; RIKEN; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Universite Paris-Dauphine; Universite Paris Cite	Yamane, I (corresponding author), Univ Tokyo, Chiba, Japan.; Yamane, I (corresponding author), RIKEN, Ctr Adv Intelligence Project AIP, Tokyo, Japan.	yamane@ms.k.u-tokyo.ac.jp; florian.yger@dauphine.fr; jamal.atif@dauphine.fr; sugi@k.u-tokyo.ac.jp	Sugiyama, Masashi/AEO-1176-2022	Sugiyama, Masashi/0000-0001-6658-6743	JSPS KAKENHI [16J07970]; Adway; International Research Center for Neurointelligence (WPI-IRCN) at The University of Tokyo Institutes for Advanced Study	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); Adway; International Research Center for Neurointelligence (WPI-IRCN) at The University of Tokyo Institutes for Advanced Study	We are grateful to Marthinus Christoffel du Plessis and Takeshi Teshima for their inspiring suggestions and for the meaningful discussions. We would like to thank the anonymous reviewers for their helpful comments. IY was supported by JSPS KAKENHI 16J07970. JA and FY would like to thank Adway for its support. MS was supported by the International Research Center for Neurointelligence (WPI-IRCN) at The University of Tokyo Institutes for Advanced Study.	Abrahams Edward, 2009, J Diabetes Sci Technol, V3, P680; Athey S., 2016, ARXIV161001271ECONST; Austin PC, 2011, MULTIVAR BEHAV RES, V46, P119, DOI 10.1080/00273171.2011.540480; Bishop C.M, 2006, PATTERN RECOGN; Gutierrez P., 2017, P 3 INT C PREDICTIVE, P1; Hartford J., 2017, INT C MACH LEARN, P1414; Hill JL, 2011, J COMPUT GRAPH STAT, V20, P217, DOI 10.1198/jcgs.2010.08162; Hillstrom Kevin, 2008, MINETHATDATA E MAIL; Imai K, 2013, ANN APPL STAT, V7, P443, DOI 10.1214/12-AOAS593; Imbens GW, 2014, STAT SCI, V29, P323, DOI 10.1214/14-STS480; Jaskowski M., 2012, ICML WORKSH CLIN DAT; Johansson FD, 2016, PR MACH LEARN RES, V48; Katsanis SH, 2008, SCIENCE, V320, P53, DOI 10.1126/science.1156604; Kohavi R, 2009, DATA MIN KNOWL DISC, V18, P140, DOI 10.1007/s10618-008-0114-1; Koltchinskii V, 2001, IEEE T INFORM THEORY, V47, P1902, DOI 10.1109/18.930926; Kunzel S. R., 2017, ARXIV170603461MACHST; Lefortier Damien, 2016, NIPS WORKSH INF LEAR; Lewis G., 2018, ARXIV180307164CSECON; Li Lihong, 2012, P WORKSH ON LIN TRAD, V2, P19; Mohri M., 2018, FDN MACHINE LEARNING; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Okanohara D., 2010, P 13 INT C ARTIFICIA, P781; Pearl J, 2009, CAUSALITY, DOI 10.1017/CBO9780511803161; Radcliffe N. J., 2008, TECHNICAL REPORT; Radcliffe Nicholas J, 2007, DIRECT MARKETING ANA, V1, P14; Radcliffe Nicholas J, 1999, CREDIT SCORING CREDI; Radcliffe NJ, 2011, TR20111 STOCH SOL ED; Renault R, 2015, HBK ECON, P121, DOI 10.1016/B978-0-444-62721-6.00004-4; Rockafellar R. T., 1970, CONVEX ANAL; Rubin DB, 2005, J AM STAT ASSOC, V100, P322, DOI 10.1198/016214504000001880; Rzepakowski P, 2012, KNOWL INF SYST, V32, P303, DOI 10.1007/s10115-011-0434-0; Shalit U, 2017, PR MACH LEARN RES, V70; Song L, 2017, ADV NEURAL INFORM PR, P4518; Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613; Tuffery S ., 2011, DATA MINING STAT DEC, V2; Wager S., 2015, ARXIV151004342MATHST; Yamada M, 2013, NEURAL COMPUT, V25, P1324, DOI 10.1162/NECO_a_00442	38	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004048
C	Yuan, M; Van Durme, B; Boyd-Graber, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yuan, Michelle; Van Durme, Benjamin; Boyd-Graber, Jordan			Multilingual Anchoring: Interactive Topic Modeling and Alignment Across Languages	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Multilingual topic models can reveal patterns in cross-lingual document collections. However, existing models lack speed and interactivity, which prevents adoption in everyday corpora exploration or quick moving situations (e.g., natural disasters, political instability). First, we propose a multilingual anchoring algorithm that builds an anchor-based topic model for documents in different languages. Then, we incorporate interactivity to develop MTAnchor (Multilingual Topic Anchors), a system that allows users to refine the topic model. We test our algorithms on labeled English, Chinese, and Sinhalese documents. Within minutes, our methods can produce interpretable topics that are useful for specific classification tasks.	[Yuan, Michelle; Boyd-Graber, Jordan] Univ Maryland, College Pk, MD 20742 USA; [Van Durme, Benjamin] Johns Hopkins Univ, Baltimore, MD 21218 USA	University System of Maryland; University of Maryland College Park; Johns Hopkins University	Yuan, M (corresponding author), Univ Maryland, College Pk, MD 20742 USA.	myuan@cs.umd.edu; vandurme@jhu.edu; jbg@umiacs.umd.edu		Yuan, Michelle/0000-0002-5318-8583	JHU Human Language Technology Center of Excellence (HLTCOE); DARPA [HR0011-15-C-0113]; Raytheon BBN Technologies	JHU Human Language Technology Center of Excellence (HLTCOE); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Raytheon BBN Technologies	We thank the anonymous reviewers for their insightful and constructive comments. Additionally, we thank Leah Findlater, Jeff Lund, Thang Nguyen, Shi Feng, Mozhi Zhang, Weiwei Yang, Eric Wallace, and Manasij Venkatesh for their helpful feedback. This work was supported in part by the JHU Human Language Technology Center of Excellence (HLTCOE) and Raytheon BBN Technologies, by DARPA award HR0011-15-C-0113. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsors.	Arora S, 2012, FDN COMPUTER SCI FOC; Arora S, 2013, P INT C MACH LEARN; Bengio Y., 2013, IEEE T PATTERN ANAL; Bird S., 2009, NATURAL LANGUAGE PRO; Blei D. M., 2003, J MACHINE LEARNING R; Boyd-Graber J, 2017, FDN TRENDS INFORM RE; Boyd-Graber J., 2012, MULTILINGUAL TOPIC M; Boyd-Graber Jordan, 2010, P EMP METH NAT LANG; Cettolo Mauro, 2012, P ANN C EUROPEAN ASS; Choo J, 2013, IEEE T VISUALIZATION; Constant N, 2009, SPRACHE DATENVERARBE; Fan R.-E., 2008, J MACHINE LEARNING R; Gutierrez E. D, 2016, T ASS COMPUTATIONAL; Hao S, 2018, C N AM ASS COMP LING; Hu Y, 2014, MACHINE LEARNING; Hu Yuening, 2014, P ASS COMP LING; Jagarlamudi J, 2010, P EUR C INF RETR; Landauer T. K., 1998, DISCOURSE PROCESSES; Lau J. H, 2014, P EUR ASS COMP LING; Lee M, 2014, P EMP METH NAT LANG; Lee Tak Yeon, 2017, INT J HUMAN COMPUTER; Lund Jeffrey, 2017, P ASS COMP LING; Manning Christopher D, 2014, P ASS COMP LING; Mimno D, 2009, P EMP METH NAT LANG; Morrow N., 2011, DEV INFORM SYSTEMS I; Nguyen T, 2015, C N AM ASS COMP LING; Nguyen T, 2014, P ASS COMP LING; Ni Xiaochuan, 2009, P WORLD WID WEB C; Rastogi P, 2015, C N AM ASS COMP LING; Shi B, 2016, P ASS COMP LING; Strassel S, 2016, LANG RES EV C; Strassel S, 2017, EXPLOITATION SOCIAL; Xiao M, 2013, P ADV NEUR INF PROC; Yurochkin M, 2017, P ADV NEUR INF PROC; Zhao B, 2006, P INT C COMP LING	35	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003023
C	Zhang, Q; Zhou, MY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Quan; Zhou, Mingyuan			Nonparametric Bayesian Lomax delegate racing for survival analysis with competing risks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MODELS; PERFORMANCE	We propose Lomax delegate racing (LDR) to explicitly model the mechanism of survival under competing risks and to interpret how the covariates accelerate or decelerate the time to event. LDR explains non-monotonic covariate effects by racing a potentially infinite number of sub-risks, and consequently relaxes the ubiquitous proportional-hazards assumption which may be too restrictive. Moreover, LDR is naturally able to model not only censoring, but also missing event times or event types. For inference, we develop a Gibbs sampler under data augmentation for moderately sized data, along with a stochastic gradient descent maximum a posteriori inference algorithm for big data applications. Illustrative experiments are provided on both synthetic and real datasets, and comparison with various benchmark algorithms for survival analysis with competing risks demonstrates distinguished performance of LDR.	[Zhang, Quan; Zhou, Mingyuan] Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Zhang, Q (corresponding author), Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA.	quan.zhang@mccombs.utexas.edu; mingyuan.zhou@mccombs.utexas.edu	Zhou, Mingyuan/AAE-8717-2021		U.S. National Science Foundation [IIS-1812699]	U.S. National Science Foundation(National Science Foundation (NSF))	The authors acknowledge the support of Award IIS-1812699 from the U.S. National Science Foundation, and the computational support of Texas Advanced Computing Center.	Al-Awadhi S., 2001, J APPL STAT SCI, V10, P365; Alaa A. M., 2017, NIPS; [Anonymous], 2014, R PACKAGE VERSION; Austin PC, 2016, CIRCULATION, V133, P601, DOI 10.1161/CIRCULATIONAHA.115.017719; Ballester F, 1997, INT J EPIDEMIOL, V26, P551, DOI 10.1093/ije/26.3.551; Barrett J. E., 2013, ARXIV13121591; Binder H., 2013, COXBOOST COX MODELS; Binder H, 2009, BIOINFORMATICS, V25, P890, DOI 10.1093/bioinformatics/btp088; BORG I., 2005, MODERN MULTIDIMENSIO, P207; Caron F., 2012, ADV NEURAL INFORM PR, V25, P1520; Chapfuwa P., 2018, ICML; Childs A, 2001, STAT PAP, V42, P187, DOI 10.1007/s003620100050; Cormen T.H., 2009, INTRO ALGORITHMS; Cox D. R., 1992, BREAKTHROUGHS STAT, P527, DOI DOI 10.1007/978-1-4612-4380-9_37; Cramer E, 2011, COMPUT STAT DATA AN, V55, P1285, DOI 10.1016/j.csda.2010.09.017; Crowder M., 2001, CLASSICAL COMPETING; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; Fernandez T., 2016, ADV NEURAL INFORM PR, P5021; Fine JP, 1999, J AM STAT ASSOC, V94, P496, DOI 10.2307/2670170; Flegal KM, 2007, JAMA-J AM MED ASSOC, V298, P2028, DOI 10.1001/jama.298.17.2028; Gerds, 2017, PEC PREDICTION ERROR; Gerds T. A., 2015, ISKREGRESSION RISK; Gerds TA, 2008, BIOMETRICAL J, V50, P457, DOI 10.1002/bimj.200810443; Giles DE, 2013, COMMUN STAT-THEOR M, V42, P1934, DOI 10.1080/03610926.2011.600506; Hemmati  F., 2017, COMMUNICATIONS STAT, P1; Howlader HA, 2002, COMPUT STAT DATA AN, V38, P301, DOI 10.1016/S0167-9473(01)00039-1; ISHWARAN H, 2018, RANDOM FORESTS SURVI; Ishwaran H, 2014, BIOSTATISTICS, V15, P757, DOI 10.1093/biostatistics/kxu010; Kalbfleisch J. D., 2011, STAT ANAL FAILURE TI, V360; KAPLAN EL, 1958, J AM STAT ASSOC, V53, P457, DOI 10.2307/2281868; Katzman JL, 2018, BMC MED RES METHODOL, V18, DOI 10.1186/s12874-018-0482-1; Lau B, 2009, AM J EPIDEMIOL, V170, P244, DOI 10.1093/aje/kwp107; Lee C., 2018, AAAI; Li HZ, 2005, BIOINFORMATICS, V21, P2403, DOI 10.1093/bioinformatics/bti324; Li Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1715, DOI 10.1145/2939672.2939857; LOMAX KS, 1954, J AM STAT ASSOC, V49, P847, DOI 10.2307/2281544; Lu WB, 2008, BIOSTATISTICS, V9, P658, DOI 10.1093/biostatistics/kxn005; Luck M., 2017, ARXIV170510245; Mei H., 2017, ADV NEURAL INFORM PR, P6754; Miscouridou  X., 2018, MACH LEARN HEALTHC C; MOSCHOPOULOS PG, 1985, ANN I STAT MATH, V37, P541, DOI 10.1007/BF02481123; Myhre J, 1982, LECT NOTES MONOGRAPH, V2, P166; Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001; Putter H, 2007, STAT MED, V26, P2389, DOI 10.1002/sim.2712; Ranganath R., 2016, ARXIV160802158; Rosenwald A, 2002, NEW ENGL J MED, V346, P1937, DOI 10.1056/NEJMoa012914; Ross S., 2014, INTRO PROBABILITY MO, V9; Saha P, 2010, BIOMETRICS, V66, P999, DOI 10.1111/j.1541-0420.2009.01375.x; Steyerberg EW, 2010, EPIDEMIOLOGY, V21, P128, DOI 10.1097/EDE.0b013e3181c30fb2; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; van Houwelingen HC., 2011, DYNAMIC PREDICTION C, V1; Varma R, 2014, JAMA OPHTHALMOL, V132, P1334, DOI 10.1001/jamaophthalmol.2014.2854; Wang  Y., 2016, ICML, P2226; Wolbers M, 2014, BIOSTATISTICS, V15, P526, DOI 10.1093/biostatistics/kxt059; Yu C.N., 2011, ADV NEURAL INF PROCE, V24, P1845; Zhou M., 2016, ARXIV160806383; Zhou MY, 2016, J MACH LEARN RES, V17, P1; Zhou Mingyuan, 2012, Proc Int Conf Mach Learn, V2012, P1343; Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211; Zhu XL, 2016, IEEE INT C BIOINFORM, P544, DOI 10.1109/BIBM.2016.7822579	60	2	2	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305005
C	Zhang, RY; Josz, C; Sojoudi, S; Lavaei, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Richard Y.; Josz, Cedric; Sojoudi, Somayeh; Lavaei, Javad			How Much Restricted Isometry is Needed In Nonconvex Matrix Recovery?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				COMPLETION; SIGNAL	When the linear measurements of an instance of low-rank matrix recovery satisfy a restricted isometry property (RIP)-i.e. they are approximately norm-preserving-the problem is known to contain no spurious local minima, so exact recovery is guaranteed. In this paper, we show that moderate RIP is not enough to eliminate spurious local minima, so existing results can only hold for near-perfect RIP. In fact, counterexamples are ubiquitous: we prove that every x is the spurious local minimum of a rank-1 instance of matrix recovery that satisfies RIP. One specific counterexample has RIP constant delta = 1/2, but causes randomly initialized stochastic gradient descent (SGD) to fail 12% of the time. SGD is frequently able to avoid and escape spurious local minima, but this empirical result shows that it can occasionally be defeated by their existence. Hence, while exact recovery guarantees will likely require a proof of no spurious local minima, arguments based solely on norm preservation will only be applicable to a narrow set of nearly-isotropic instances.	[Zhang, Richard Y.; Josz, Cedric; Sojoudi, Somayeh; Lavaei, Javad] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Zhang, RY (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	ryz@alum.mit.edu; cedric.josz@gmail.com; sojoudi@berkeley.edu; lavaei@berkeley.edu			ONR [N00014-17-1-2933, ONR N00014-18-1-2526]; NSF Award [1808859]; DARPA [D16AP00002]; AFOSR Award [FA9550- 17-1-0163]	ONR(Office of Naval Research); NSF Award(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); AFOSR Award(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	We thank our three NIPS reviewers for helpful comments and suggestions. In particular, we thank reviewer #2 for a key insight that allowed us to lower-bound delta<SUP>star</SUP> in Section 4. This work was supported by the ONR Awards N00014-17-1-2933 and ONR N00014-18-1-2526, NSF Award 1808859, DARPA Award D16AP00002, and AFOSR Award FA9550- 17-1-0163.	Agarwal A, 2014, J MACH LEARN RES, V15, P1111; Andersen E., 2000, HIGH PERFORMANCE OPT, P197, DOI DOI 10.1007/978-1-4757-3216-0_8; [Anonymous], 2016, ADV NEURAL INFORM PR; Bach FR, 2008, J MACH LEARN RES, V9, P1019; Bhojanapalli S., 2016, ADV NEURAL INFORM PR, P3873; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; BOUMAL N, 2016, ADV NEURAL INFORM PR, P2757, DOI DOI 10.5555/3157382.3157407; Boumal Nicolas, 2018, IMA J NUMERICAL ANAL; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Boyd S., 1994, LINEAR MATRIX INEQUA, V15; Cai TT, 2013, APPL COMPUT HARMON A, V35, P74, DOI 10.1016/j.acha.2012.07.010; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Du S.S., 2017, ADV NEURAL INFORM PR, P1067; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Ge R, 2017, PR MACH LEARN RES, V70; Hardt M, 2016, PR MACH LEARN RES, V48; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Jin C, 2017, PR MACH LEARN RES, V70; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee J. D., 2016, C LEARN THEOR, P1246; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Sturm JF, 1999, OPTIM METHOD SOFTW, V11-2, P625, DOI 10.1080/10556789908805766; Sun J, 2015, PR MACH LEARN RES, V37, P2351; Sun J, 2016, IEEE INT SYMP INFO, P2379, DOI 10.1109/ISIT.2016.7541725; Sun RY, 2016, IEEE T INFORM THEORY, V62, P6535, DOI 10.1109/TIT.2016.2598574; Toh KC, 1999, OPTIM METHOD SOFTW, V11-2, P545, DOI 10.1080/10556789908805762; Toint Philippe L., 2000, MOS-SIAM SER OPTIMIZ, DOI 10.1137/1.9780898719857; Wang HM, 2013, SCI CHINA MATH, V56, P1117, DOI 10.1007/s11425-013-4624-y; Wilson AC, 2017, ADV NEUR IN, V30; Zhang Chiyuan, 2016, ARXIV161103530; Zheng Q., 2015, ADV NEURAL INFORM PR, P109; Zhu ZH, 2018, IEEE T SIGNAL PROCES, V66, P3614, DOI 10.1109/TSP.2018.2835403	43	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000012
C	Zhu, JY; Zhang, ZT; Zhang, CK; Wu, JJ; Torralba, A; Tenenbaum, JB; Freeman, WT		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhu, Jun-Yan; Zhang, Zhoutong; Zhang, Chengkai; Wu, Jiajun; Torralba, Antonio; Tenenbaum, Joshua B.; Freeman, William T.			Visual Object Networks: Image Generation with Disentangled 3D Representation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recent progress in deep generative models has led to tremendous breakthroughs in image generation. However, while existing models can synthesize photorealistic images, they lack an understanding of our underlying 3D world. We present a new generative model, Visual Object Networks (VON), synthesizing natural images of objects with a disentangled 3D representation. Inspired by classic graphics rendering pipelines, we unravel our image formation process into three conditionally independent factors-shape, viewpoint, and texture-and present an end-to-end adversarial learning framework that jointly models 3D shapes and 2D images. Our model first learns to synthesize 3D shapes that are indistinguishable from real shapes. It then renders the object's 2.5D sketches (i.e., silhouette and depth map) from its shape under a sampled viewpoint. Finally, it learns to add realistic texture to these 2.5D sketches to generate natural images. The VON not only generates images that are more realistic than state-of-the-art 2D image synthesis methods, but also enables many 3D operations such as changing the viewpoint of a generated image, editing of shape and texture, linear interpolation in texture and shape space, and transferring appearance across different objects and viewpoints.	[Zhu, Jun-Yan; Zhang, Zhoutong; Zhang, Chengkai; Wu, Jiajun; Torralba, Antonio; Tenenbaum, Joshua B.] MIT CSAIL, Cambridge, MA 02139 USA; [Freeman, William T.] MIT CSAIL, Google, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Google Incorporated; Massachusetts Institute of Technology (MIT)	Zhu, JY (corresponding author), MIT CSAIL, Cambridge, MA 02139 USA.		Wu, JiaJun/GQH-7885-2022		NSF [1231216, 1447476, 1524817]; ONR [MURI N00014-16-1-2007]; Toyota Research Institute; Shell; Facebook	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); Toyota Research Institute; Shell(Royal Dutch Shell); Facebook(Facebook Inc)	This work is supported by NSF #1231216, NSF #1447476, NSF #1524817, ONR MURI N00014-16-1-2007, Toyota Research Institute, Shell, and Facebook. We thank Xiuming Zhang, Richard Zhang, David Bau, and Zhuang Liu for valuable discussions.	Achlioptas Panos, 2018, ICLR WORKSH; [Anonymous], 2017, NIPS; [Anonymous], 2015, P 28 INT C NEUR INF; [Anonymous], 2016, ICML; [Anonymous], 2018, ICML; [Anonymous], 2017, ICCV; Ba J., 2017, P 3 INT C LEARN REPR; Barrow H., 1978, COMPUTER VISION SYST; Bever TG, 2010, BIOLINGUISTICS, V4, P174; Blanz V., 1999, SIGGRAPH; Bottou L., 2017, ICML; Chang A. X., 2015, ARXIV PREPRINT ARXIV; Chen X, 2016, ADV NEUR IN, V29; Curless B., 1996, SIGGRAPH; Dai A., 2017, CVPR; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dosovitskiy A., 2015, CVPR; Gadelha M, 2017, INT CONF 3D VISION, P402, DOI 10.1109/3DV.2017.00053; Gadelha Matheus, 2017, BMVC; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani Ishaan, 2017, NIPS; Heess N, 2016, NIPS; Heusel Martin, 2017, NIPS170608500; Huang Xun, 2018, ECCV180404732; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Isola P., 2017, CVPR; Isola P., 2016, ICLR WORKSH; Kajiya James T., 1986, SIGGRAPH; Kanazawa Angjoo, 2018, ECCV180307549; Karras T., 2017, ICLR; Karras Tero, 2018, ICLR; Kingma D.P., 2015, INT C LEARN REPR, P1; Kulkarni T. D., 2015, CVPR; Kulkarni T. D., 2015, NIPS; Ledig C., 2017, CVPR; Lempitsky V., 2016, ARXIV160708022V3; Liu M. -Y., 2017, NIPS; Lucic Mario, 2018, NIPS; Mao X., 2017, ICCV; Marr D., 1982, VISION COMPUTATIONAL; Mathieu Michael, 2016, P INT C LEARN REPR I; Pathak D., 2016, CVPR; Radford A., 2016, ICLR; Shu Z., 2017, CVPR; Sun J, 2015, CVPR; Sun Xingyuan, 2018, P IEEE C COMP VIS PA; Sun Yongbin, 2018, ARXIV180406375; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; Tatarchenko M., 2016, ECCV; Tatarchenko M., 2017, ICCV; Tulsiani S., 2017, IEEE C COMP VIS PATT; Tung H.Y.F., 2017, ICCV; Wang T.-C., 2018, CVPR; Wang W., 2017, ICCV; WU J., 2016, NIPS; Wu J., 2017, NIPS; Wu Jiajun, 2018, EUR C COMP VIS 2018; Yang J., 2015, P ADV NEUR INF PROC; Yi Z., 2017, ICCV; Yuille A, 2006, TRENDS COGN SCI, V10, P301, DOI 10.1016/j.tics.2006.05.002; Zhang H., 2017, ICCV; Zhang X., 2018, NIPS; Zhu J. -Y., 2016, ECCV	63	2	2	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300012
C	Aghasi, A; Abdi, A; Nguyen, N; Romberg, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Aghasi, Alireza; Abdi, Afshin; Nguyen, Nam; Romberg, Justin			Net-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We introduce and analyze a new technique for model reduction for deep neural networks. While large networks are theoretically capable of learning arbitrarily complex models, overfitting and model redundancy negatively affects the prediction accuracy and model variance. Our Net-Trim algorithm prunes (sparsifies) a trained network layer-wise, removing connections at each layer by solving a convex optimization program. This program seeks a sparse set of weights at each layer that keeps the layer inputs and outputs consistent with the originally trained model. The algorithms and associated analysis are applicable to neural networks operating with the rectified linear unit (ReLU) as the nonlinear activation. We present both parallel and cascade versions of the algorithm. While the latter can achieve slightly simpler models with the same generalization performance, the former can be computed in a distributed manner. In both cases, Net-Trim significantly reduces the number of connections in the network, while also providing enough regularization to slightly reduce the generalization error. We also provide a mathematical analysis of the consistency between the initial network and the retrained model. To analyze the model sample complexity, we derive the general sufficient conditions for the recovery of a sparse transform matrix. For a single layer taking independent Gaussian random vectors of length N as inputs, we show that if the network response can be described using a maximum number of s non-zero weights per node, these weights can be learned from O (s log N) samples.	[Aghasi, Alireza] Georgia State Univ, Inst Insight, Atlanta, GA 30303 USA; [Aghasi, Alireza; Nguyen, Nam] IBM TJ Watson, Yorktown Hts, NY 10598 USA; [Abdi, Afshin; Romberg, Justin] Georgia Tech, Dept ECE, Atlanta, GA USA	University System of Georgia; Georgia State University; International Business Machines (IBM); University System of Georgia; Georgia Institute of Technology	Aghasi, A (corresponding author), Georgia State Univ, Inst Insight, Atlanta, GA 30303 USA.; Aghasi, A (corresponding author), IBM TJ Watson, Yorktown Hts, NY 10598 USA.	aaghasi@gsu.edu; abdi@gatech.edu; nnguyen@us.ibm.com; jrom@ece.gatech.edu						Arora S., 2014, P 31 INT C MACH LEAR; Aslan O., 2014, ADV NEURAL INFORM PR, P3275; Bach F., 2014, TECHNICAL REPORT; Bengio Yoshua, 2005, ADV NEURAL INFORM PR, P123; Chen WL, 2015, PR MACH LEARN RES, V37, P2285; Choromanska A., 2015, P 18 INT C ART INT S; Ghadimi E, 2015, IEEE T AUTOMAT CONTR, V60, P644, DOI 10.1109/TAC.2014.2354892; Giryes R, 2016, IEEE T SIGNAL PROCES, V64, P3444, DOI 10.1109/TSP.2016.2546221; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; HAN S., 2015, ARXIV151000149; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Kawaguchi K., 2016, PREPRINT; NOWLAN SJ, 1992, NEURAL COMPUT, V4, P473, DOI 10.1162/neco.1992.4.4.473; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Wan L., 2016, P 33 INT C MACH LEAR	17	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403024
C	Ahn, S; Chertkov, M; Shin, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ahn, Sungsoo; Chertkov, Michael; Shin, Jinwoo			Gauging Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				PROPAGATION; GRAPHS; CODES	Computing partition function is the most important statistical inference task arising in applications of Graphical Models (GM). Since it is computationally intractable, approximate methods have been used in practice, where mean-field (MF) and belief propagation (BP) are arguably the most popular and successful approaches of a variational type. In this paper, we propose two new variational schemes, coined Gauged-MF (G-MF) and Gauged-BP (G-BP), improving MF and BP, respectively. Both provide lower bounds for the partition function by utilizing the so-called gauge transformation which modifies factors of GM while keeping the partition function invariant. Moreover, we prove that both G-MF and G-BP are exact for GMs with a single loop of a special structure, even though the bare MF and BP perform badly in this case. Our extensive experiments indeed confirm that the proposed algorithms outperform and generalize MF and BR.	[Ahn, Sungsoo; Shin, Jinwoo] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea; [Chertkov, Michael] Los Alamos Natl Lab, Theoret Div, T 4, Los Alamos, NM 87545 USA; [Chertkov, Michael] Los Alamos Natl Lab, Ctr Nonlinear Studies, Los Alamos, NM 87545 USA; [Chertkov, Michael] Skolkovo Inst Sci & Technol, Moscow 143026, Russia	Korea Advanced Institute of Science & Technology (KAIST); United States Department of Energy (DOE); Los Alamos National Laboratory; United States Department of Energy (DOE); Los Alamos National Laboratory; Skolkovo Institute of Science & Technology	Ahn, S (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.	sungsoo.ahn@kaist.ac.kr; chertkov@lanl.gov; jinwoos@kaist.ac.kr	Jeong, Yongwook/N-7413-2016; Chertkov, Michael/O-8828-2015	Chertkov, Michael/0000-0002-6758-515X	National Research Council of Science & Technology (NST) - Korea government (MSIP) [CRC-15-05-ETRI]; Institute for Information & communications Technology Promotion(IITP) - Korea government(MSIT) [2017-0-01778]; ICT R&D program of MSIP/IITP [2016-0-00563]	National Research Council of Science & Technology (NST) - Korea government (MSIP); Institute for Information & communications Technology Promotion(IITP) - Korea government(MSIT); ICT R&D program of MSIP/IITP	This work was supported in part by the National Research Council of Science & Technology (NST) grant by the Korea government (MSIP) (No. CRC-15-05-ETRI), Institute for Information & communications Technology Promotion(IITP) grant funded by the Korea government(MSIT) (No. 2017-0-01778, Development of Explainable Human-level Deep Machine Learning Inference Framework) and ICT R&D program of MSIP/IITP [2016-0-00563, Research on Adaptive Machine Learning Technology Development for Intelligent Autonomous Digital Companion].	Ahn S.-S., 2016, ADV NEURAL INFORM PR, P1453; Al-Bashabsheh A, 2011, IEEE T INFORM THEORY, V57, P752, DOI 10.1109/TIT.2010.2094870; Alpaydin E, 2014, ADAPT COMPUT MACH LE, P115; [Anonymous], 1935, P R SOC LONDON A, DOI DOI 10.1098/RSPA.1935.0122; Chernyak VY, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS, VOLS 1-7, P316, DOI 10.1109/ISIT.2007.4557245; Chertkov M, 2006, PHYS REV E, V73, DOI 10.1103/PhysRevE.73.065102; Chertkov M, 2006, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2006/06/P06009; Chertkov M, 2008, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2008/05/P05003; Chertkov Michael, 2016, LECT NOTES STAT INFE; Ermon S, 2012, ADV NEURAL INFORM PR, P2762; Forney G D, 2011, P ANN WORKSH INF THE; Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075; GALLAGER RG, 1962, IRE T INFORM THEOR, V8, P21, DOI 10.1109/tit.1962.1057683; JERRUM M, 1993, SIAM J COMPUT, V22, P1087, DOI 10.1137/0222066; Kschischang FR, 1998, IEEE J SEL AREA COMM, V16, P219, DOI 10.1109/49.661110; Liu Q, 2012, ARXIV12033494; Liu Q., 2011, P 28 INT C MACH LEAR, P849; Mezard M., 1987, SPIN GLASS THEORY IN; Mezard M., 2009, INFORM PHYS COMPUTAT, pp 584, DOI [10.1093/acprof:oso/9780198570837.001.0001, DOI 10.1093/ACPROF:OSO/9780198570837.001, DOI 10.1093/ACPROF:OSO/9780198570837.001.0001]; Mori R, 2015, IEEE INT SYMP INFO, P1099, DOI 10.1109/ISIT.2015.7282625; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Pearl J, 1982, REVEREND BAYES INFER; Peierls R, 1936, P CAMB PHILOS SOC, V32, P477, DOI 10.1017/S0305004100019174; Ruozzi Nicholas, 2012, P 25 INT C NEUR INF, V1, P117; Saul L, 1998, NATO ADV SCI I D-BEH, V89, P541; Valiant LG, 2008, SIAM J COMPUT, V37, P1565, DOI 10.1137/070682575; Wachter A, 2006, MATH PROGRAM, V106, P25, DOI 10.1007/s10107-004-0559-y; Wainwright M. J., 2003, 649 UC BERK DEP STAT; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P2313, DOI 10.1109/TIT.2005.850091; Wainwright MJ, 2003, IEEE T INFORM THEORY, V49, P1120, DOI 10.1109/TIT.2003.810642; Weller A, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P868; Yedidia J S, 2001, TECHNICAL REPORT, V13; Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085	36	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402091
C	Bashiri, MA; Zhang, XH		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Bashiri, Mohammad Ali; Zhang, Xinhua			Decomposition-Invariant Conditional Gradient for General Polytopes with Line Search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ALGORITHM; WOLFE; CONVERGENCE	Frank-Wolfe (FW) algorithms with linear convergence rates have recently achieved great efficiency in many applications. Garber and Meshi (2016) designed a new decomposition-invariant pairwise FW variant with favorable dependency on the domain geometry. Unfortunately it applies only to a restricted class of polytopes and cannot achieve theoretical and practical efficiency at the same time. In this paper, we show that by employing an away-step update, similar rates can be generalized to arbitrary polytopes with strong empirical performance. A new "condition number" of the domain is introduced which allows leveraging the sparsity of the solution. We applied the method to a reformulation of SVM, and the linear convergence rate depends, for the first time, on the number of support vectors.	[Bashiri, Mohammad Ali; Zhang, Xinhua] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA	University of Illinois System; University of Illinois Chicago; University of Illinois Chicago Hospital	Bashiri, MA (corresponding author), Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.	mbashi4@uic.edu; zhangx@uic.edu	Jeong, Yongwook/N-7413-2016		NSF [RI-1526379]	NSF(National Science Foundation (NSF))	We thank Dan Garber for very helpful discussions and clarifications on [18]. Mohammad Ali Bashiri is supported in part by NSF grant RI-1526379.	Ahipasaoglu SD, 2008, OPTIM METHOD SOFTW, V23, P5, DOI 10.1080/10556780701589669; Beck A, 2004, MATH METHOD OPER RES, V59, P235, DOI 10.1007/s001860300327; Beck A, 2017, MATH PROGRAM, V164, P1, DOI 10.1007/s10107-016-1069-4; Bredensteiner Erin J., 2000, P INT C MACH LEARN; Defazio A., 2014, NEURAL INFORM PROCES; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Garber D., 2016, NEURAL INFORM PROCES; Garber D, 2016, SIAM J OPTIMIZ, V26, P1493, DOI 10.1137/140985366; GUELAT J, 1986, MATH PROGRAM, V35, P110, DOI 10.1007/BF01589445; Harchaoui Z., 2012, P IEEE C COMP VIS PA; Harchaoui Z, 2015, MATH PROGRAM, V152, P75, DOI 10.1007/s10107-014-0778-9; Jaggi Martin, 2010, P INT C MACH LEARN; Jaggi Martin, 2013, P INT C MACH LEARN; Johnson R., 2013, NEURAL INFORM PROCES; Keerthi SS, 2006, J MACH LEARN RES, V7, P1493; Kumar P, 2011, INFORMS J COMPUT, V23, P377, DOI 10.1287/ijoc.1100.0412; Lacoste-Julien S., 2013, NIPS 2013 WORKSH GRE; Lacoste-Julien S., 2015, NEURAL INFORM PROCES; Lan G., 2014, TECHNICAL REPORT; Levitin Evgeny S, 1966, USSR COMP MATH MATH, V6, P1, DOI DOI 10.1016/0041-5553(66)90114-5; Lichman M., 2013, UCI MACHINE LEARNING; List N., 2009, P ANN C COMP LEARN T; Nancuef R, 2014, INFORM SCIENCES, V285, P66, DOI 10.1016/j.ins.2014.03.059; Platt J, 1998, MSRTR9814; Prokhorov D., 2001, IJCNN, V1, P97; Robinson S. M., 1982, GEN EQUATIONS THEI 2; Wolfe P., 1970, INTEGER NONLINEAR PR; You Y., 2016, NEURAL INFORM PROCES; Zhang X., 2012, NEURAL INFORM PROCES	30	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402072
C	Basu, K; Saha, A; Chatterjee, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Basu, Kinjal; Saha, Ankan; Chatterjee, Shaunak			Large-Scale Quadratically Constrained Quadratic Program via Low-Discrepancy Sequences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				RIESZ ENERGY; QCQP	We consider the problem of solving a large-scale Quadratically Constrained Quadratic Program. Such problems occur naturally in many scientific and web applications. Although there are efficient methods which tackle this problem, they are mostly not scalable. In this paper, we develop a method that transforms the quadratic constraint into a linear form by sampling a set of low-discrepancy points [16]. The transformed problem can then be solved by applying any state-of-the-art large-scale quadratic programming solvers. We show the convergence of our approximate solution to the true solution as well as some finite sample error bounds. Experimental results are also shown to prove scalability as well as improved quality of approximation in practice.	[Basu, Kinjal; Saha, Ankan; Chatterjee, Shaunak] LinkedIn Corp, Mountain View, CA 94043 USA		Basu, K (corresponding author), LinkedIn Corp, Mountain View, CA 94043 USA.	kbasu@linkedin.com; asaha@linkedin.com; shchatte@linkedin.com	Jeong, Yongwook/N-7413-2016					Agarwal D, 2015, WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P375, DOI 10.1145/2740908.2745398; Agarwal D, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P485, DOI 10.1145/2348283.2348350; Aholt C, 2012, LECT NOTES COMPUT SC, V7572, P654, DOI 10.1007/978-3-642-33718-5_47; Anstreicher KM, 2009, J GLOBAL OPTIM, V43, P471, DOI 10.1007/s10898-008-9372-0; Bao XW, 2011, MATH PROGRAM, V129, P129, DOI 10.1007/s10107-011-0462-2; Basu K., 2016, ARXIV160204391; Basu K, 2017, FOUND COMPUT MATH, V17, P467, DOI 10.1007/s10208-015-9293-5; Basu K, 2015, SIAM J NUMER ANAL, V53, P743, DOI 10.1137/140960463; Bondarenko A, 2013, ANN MATH, V178, P443, DOI 10.4007/annals.2013.178.2.2; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Boyd S, 2004, CONVEX OPTIMIZATION; Brauchart JS, 2009, B LOND MATH SOC, V41, P621, DOI 10.1112/blms/bdp034; Brauchart JS, 2015, J COMPLEXITY, V31, P293, DOI 10.1016/j.jco.2015.02.003; Brauchart JS, 2012, NUMER MATH, V121, P473, DOI 10.1007/s00211-011-0444-6; Chapelle O., 2006, SEMISUPERVISED LEARN, P277, DOI DOI 10.7551/MITPRESS/9780262033589; De Maio A, 2011, IEEE T SIGNAL PROCES, V59, P172, DOI 10.1109/TSP.2010.2087327; Dick J., 2010, DIGITAL SEQUENCES DI; Gotz M, 2003, J APPROX THEORY, V122, P62, DOI 10.1016/S0021-9045(03)00031-5; Grabner PJ, 2014, APPLIED ALGEBRA AND NUMBER THEORY: ESSAYS IN HONOR OF HARALD NIEDERREITER ON THE OCCASION OF HIS 70TH BIRTHDAY, P109; Hardin DP, 2005, ADV MATH, V193, P174, DOI 10.1016/j.aim.2004.05.006; Huang YW, 2014, IEEE T SIGNAL PROCES, V62, P1093, DOI 10.1109/TSP.2013.2297683; Lanckriet G. R. G., 2002, 19 INT C MACH LEARN, P323, DOI [10.1023/B:JODS.0000012018.62090.a7, DOI 10.1023/B:JODS.0000012018.62090.A7]; Lasserre JB, 2002, MATH OPER RES, V27, P347, DOI 10.1287/moor.27.2.347.322; Nesterov Y., 1994, STUD APPL MATH, V13; Niederreiter H., 1992, RANDOM NUMBER GENERA, V63; O'Donoghue B, 2016, J OPTIMIZ THEORY APP, V169, P1042, DOI 10.1007/s10957-016-0892-3; Parikh N, 2014, MATH PROGRAM COMPUT, V6, P77, DOI 10.1007/s12532-013-0061-8; Rockafellar R. T., 1970, CONVEX ANAL; Ye JP, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P854	29	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402034
C	Bornschein, J; Mnih, A; Zoran, D; Rezende, DJ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Bornschein, Jorg; Mnih, Andriy; Zoran, Daniel; Rezende, Danilo J.			Variational Memory Addressing in Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Aiming to augment generative models with external memory, we interpret the output of a memory module with stochastic addressing as a conditional mixture distribution, where a read operation corresponds to sampling a discrete memory address and retrieving the corresponding content from memory. This perspective allows us to apply variational inference to memory addressing, which enables effective training of the memory module by using the target information to guide memory lookups. Stochastic addressing is particularly well-suited for generative models as it naturally encourages multimodality which is a prominent aspect of most high-dimensional datasets. Treating the chosen address as a latent variable also allows us to quantify the amount of information gained with a memory lookup and measure the contribution of the memory module to the generative process. To illustrate the advantages of this approach we incorporate it into a variational autoencoder and apply the resulting model to the task of generative few-shot learning. The intuition behind this architecture is that the memory module can pick a relevant template from memory and the continuous part of the model can concentrate on modeling remaining variations. We demonstrate empirically that our model is able to identify and access the relevant memory contents even with hundreds of unseen Omniglot characters in memory.	[Bornschein, Jorg; Mnih, Andriy; Zoran, Daniel; Rezende, Danilo J.] DeepMind, London, England		Bornschein, J (corresponding author), DeepMind, London, England.	bornschein@google.com; amnih@google.com; danielzoran@google.com; danilor@google.com	Jeong, Yongwook/N-7413-2016					[Anonymous], 2015, DRAW RECURRENT NEURA; Ba Jimmy, 2015, ADV NEURAL INFORM PR, P2; Bartunov Sergey, 2016, ARXIV161202192; Bornschein Jorg, 2014, ARXIV14062751; Burda Yuri, 2015, ARXIV150900519; DAS S, 1992, PROCEEDINGS OF THE FOURTEENTH ANNUAL CONFERENCE OF THE COGNITIVE SCIENCE SOCIETY, P791; Dilokthanakul Nat, 2016, ARXIV161102648; Edwards Harrison, 2017, NEURAL STAT, V2; Gemici M., 2017, ARXIV PREPRINT ARXIV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Jang E., 2017, ICLR; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Li CX, 2016, PR MACH LEARN RES, V48; Maddison Chris J, 2016, ARXIV161100712; Mnih A, 2016, PR MACH LEARN RES, V48; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Nalisnick Eric, 2016, P NEURIPS WORKSH BAY; Rezende DJ, 2016, PR MACH LEARN RES, V48; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Sukhbaatar S, 2015, ADV NEUR IN, V28; Zaremba Wojciech, 2015, ARXIV150500521, V362	26	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403095
C	Chen, H; Wang, XQ; Deng, C; Huang, H		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Chen, Hong; Wang, Xiaoqian; Deng, Cheng; Huang, Heng			Group Sparse Additive Machine	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SOFT MARGIN CLASSIFIERS; MINIMAX-OPTIMAL RATES; SELECTION; RISK; CLASSIFICATION; CONSISTENCY; REGRESSION	A family of learning algorithms generated from additive models have attracted much attention recently for their flexibility and interpretability in high dimensional data analysis. Among them, learning models with grouped variables have shown competitive performance for prediction and variable selection. However, the previous works mainly focus on the least squares regression problem, not the classification task. Thus, it is desired to design the new additive classification model with variable selection capability for many real-world applications which focus on high-dimensional data classification. To address this challenging problem, in this paper, we investigate the classification with group sparse additive models in reproducing kernel Hilbert spaces. A novel classification method, called as group sparse additive machine (GroupSAM), is proposed to explore and utilize the structure information among the input variables. Generalization error bound is derived and proved by integrating the sample error analysis with empirical covering numbers and the hypothesis error estimate with the stepping stone technique. Our new bound shows that GroupSAM can achieve a satisfactory learning rate with polynomial decay. Experimental results on synthetic data and seven benchmark datasets consistently show the effectiveness of our new approach.	[Chen, Hong; Wang, Xiaoqian; Huang, Heng] Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15260 USA; [Deng, Cheng] Xidian Univ, Sch Elect Engn, Xian, Shaanxi, Peoples R China	Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; Xidian University	Huang, H (corresponding author), Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15260 USA.	chenh@mail.hzau.edu.cn; xgwang1991@gmail.com; chdeng@mail.xidian.edu.cn; heng.huang@pitt.edu	Jeong, Yongwook/N-7413-2016	Deng, Cheng/0000-0003-2620-3247	U.S. NSF [IIS 1302675]; NSF [IIS 1344152, IIS 1619308, IIS 1633753]; NSF-DBI [1356628]; NIH [AG049371]; National Natural Science Foundation of China (NSFC) [11671161]	U.S. NSF(National Science Foundation (NSF)); NSF(National Science Foundation (NSF)); NSF-DBI(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	This work was partially supported by U.S. NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628, NSF-IIS 1619308, NSF-IIS 1633753, NIH AG049371. Hong Chen was partially supported by National Natural Science Foundation of China (NSFC) 11671161. We are grateful to the anonymous NIPS reviewers for the insightful comments.	Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chen DR, 2004, J MACH LEARN RES, V5, P1143; Chen H, 2013, NEURAL COMPUT, V25, P1107, DOI 10.1162/NECO_a_00421; Christmann A, 2016, ANAL APPL, V14, P449, DOI 10.1142/S0219530515500050; Christmann A, 2012, COMPUT STAT DATA AN, V56, P854, DOI 10.1016/j.csda.2011.04.006; Edmunds D.E., 1996, FUNCTION SPACES ENTR; Guo ZC, 2013, ADV COMPUT MATH, V38, P207, DOI 10.1007/s10444-011-9238-8; Huang JA, 2010, ANN STAT, V38, P2282, DOI 10.1214/09-AOS781; Kandasamy K., 2016, ICML; Lichman M, 2013, UCI MACHINE LEARNING; Lin Y, 2006, ANN STAT, V34, P2272, DOI 10.1214/009053606000000722; Lv S., 2017, ANN STAT; Meier L, 2009, ANN STAT, V37, P3779, DOI 10.1214/09-AOS692; Raskutti G, 2012, J MACH LEARN RES, V13, P389; Ravikumar P, 2009, J R STAT SOC B, V71, P1009, DOI 10.1111/j.1467-9868.2009.00718.x; Shi L, 2013, APPL COMPUT HARMON A, V34, P252, DOI 10.1016/j.acha.2012.05.001; Shi L, 2011, APPL COMPUT HARMON A, V31, P286, DOI 10.1016/j.acha.2011.01.001; Tsybakov AB, 2004, ANN STAT, V32, P135; Vapnik VN, 1998, STAT LEARNING THEORY, DOI DOI 10.1007/978-1-4419-1428-6_5864; Wu Q, 2005, NEURAL COMPUT, V17, P1160, DOI 10.1162/0899766053491896; Wu Q, 2007, J COMPLEXITY, V23, P108, DOI 10.1016/j.jco.2006.06.007; Yang L, 2016, J MACH LEARN RES, V17; Yin J., 2012, ICML; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Yuan M, 2016, ANN STAT, V44, P2564, DOI 10.1214/15-AOS1422; Zhang T, 2004, ANN STAT, V32, P56; Zhao T., 2012, AISTATS; Zhong L. W., 2011, ICML	31	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400019
C	Chen, R; Lucier, B; Singer, Y; Syrgkanis, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Chen, Robert; Lucier, Brendan; Singer, Yaron; Syrgkanis, Vasilis			Robust Optimization for Non-Convex Objectives	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We consider robust optimization problems, where the goal is to optimize in the worst case over a class of objective functions. We develop a reduction from robust improper optimization to stochastic optimization: given an oracle that returns alpha-approximate solutions for distributions over objectives, we compute a distribution over solutions that is alpha-approximate in the worst case. We show that derandomizing this solution is NP-hard in general, but can be done for a broad class of statistical learning tasks. We apply our results to robust neural network training and submodular optimization. We evaluate our approach experimentally on corrupted character classification and robust influence maximization in networks.	[Chen, Robert; Singer, Yaron] Harvard Univ, Comp Sci, Cambridge, MA 02138 USA; [Lucier, Brendan; Syrgkanis, Vasilis] Microsoft Res New England, Cambridge, MA USA	Harvard University; Microsoft	Chen, R (corresponding author), Harvard Univ, Comp Sci, Cambridge, MA 02138 USA.		Jeong, Yongwook/N-7413-2016					Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Ben-Tal A, 2015, OPER RES, V63, P628, DOI 10.1287/opre.2015.1374; Chatterjee S., 2016, ADV NEURAL INFORM PR, V29, P3423; Chen W, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P795, DOI 10.1145/2939672.2939745; Hazan E., 2015, ADV NEURAL INFORM PR, P1594; Hazan E, 2016, PR MACH LEARN RES, V48; He XR, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P885, DOI 10.1145/2939672.2939760; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Krause A, 2011, P 22 INT JOINT C ART, V22, P2133; Krause A, 2007, P ADV NEUR INF PROC, P777; Leskovec J., STANFORD NETWORK ANA; Lowalekar M, 2016, AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1395; Mansour Y., 2015, P 26 ANN ACM SIAM S, P449; Namkoong H, 2016, ADV NEURAL INFORM PR, P2208; Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Shalev-Shwartz S, 2016, PR MACH LEARN RES, V48; Steinhardt Jacob., 2015, C LEARN THEOR, P1564	19	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404075
C	Cusumano-Towner, MF; Mansinghka, VK		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Cusumano-Towner, Marco F.; Mansinghka, Vikash K.			AIDE: An algorithm for measuring the accuracy of probabilistic inference algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Approximate probabilistic inference algorithms are central to many fields. Examples include sequential Monte Carlo inference in robotics, variational inference in machine learning, and Markov chain Monte Carlo inference in statistics. A key problem faced by practitioners is measuring the accuracy of an approximate inference algorithm on a specific data set. This paper introduces the auxiliary inference divergence estimator (AIDE), an algorithm for measuring the accuracy of approximate inference algorithms. AIDE is based on the observation that inference algorithms can be treated as probabilistic models and the random variables used within the inference algorithm can be viewed as auxiliary variables. This view leads to a new estimator for the symmetric KL divergence between the approximating distributions of two inference algorithms. The paper illustrates application of AIDE to algorithms for inference in regression, hidden Markov, and Dirichlet process mixture models. The experiments show that AIDE captures the qualitative behavior of a broad class of inference algorithms and can detect failure modes of inference algorithms that are missed by standard heuristics.	[Cusumano-Towner, Marco F.; Mansinghka, Vikash K.] MIT, Probabilist Comp Project, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Cusumano-Towner, MF (corresponding author), MIT, Probabilist Comp Project, Cambridge, MA 02139 USA.	marcoct@mit.edu; vkm@mit.edu	Jeong, Yongwook/N-7413-2016		DARPA (PPAML program) [FA8750-14-2-0004]; Office of Naval Research [N000141310333]; Army Research Office [W911NF-13-1-0212]; DoD, Air Force Office of Scientific Research, National Defense Science and Engineering Graduate (NDSEG) Fellowship [32 CFR 168a]; IARPA [2015-15061000003]	DARPA (PPAML program); Office of Naval Research(Office of Naval Research); Army Research Office; DoD, Air Force Office of Scientific Research, National Defense Science and Engineering Graduate (NDSEG) Fellowship; IARPA	This research was supported by DARPA (PPAML program, contract number FA8750-14-2-0004), IARPA (under research contract 2015-15061000003), the Office of Naval Research (under research contract N000141310333), the Army Research Office (under agreement number W911NF-13-1-0212), and gifts from Analog Devices and Google. This research was conducted with Government support under and awarded by DoD, Air Force Office of Scientific Research, National Defense Science and Engineering Graduate (NDSEG) Fellowship, 32 CFR 168a.	Ackerman Nathanael L, 2010, ARXIV10053014; Agapiou S, 2017, STAT SCI, V32, P405, DOI 10.1214/17-STS611; Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; Chatterjee S, 2015, ARXIV151101437; Cowles MK, 1996, J AM STAT ASSOC, V91, P883, DOI 10.2307/2291683; Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x; Drinkwater MJ, 2004, PUBL ASTRON SOC AUST, V21, P89, DOI 10.1071/AS03057; Freer Cameron E, 2010, NIPS WORKSH ADV MONT; Gelman A, 1992, STAT SCI, V7, P136, DOI 10.1214/ss/1177011136; Geweke J, 2004, J AM STAT ASSOC, V99, P799, DOI 10.1198/016214504000001132; Goodman N. D., 2008, UAI; Gorham J, 2015, ADV NEUR IN, V28; Grosse R. B., 2015, ARXIV151102543; Grosse R. B., 2016, ADV NEURAL INFORM PR, V29, P2451; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; Huggins J.H., 2015, ARXIV150300966; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kong A., 1992, 348 U CHIC DEP STAT; Le T. A., 2017, ARXIV170510306; Maddison C. J., 2017, ARXIV170509279; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Naesseth C. A., 2017, ARXIV170511140; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Rubin D. B, 1988, BAYESIAN STATISTICS, V3, P395; Salimans T, 2015, PR MACH LEARN RES, V37, P1218; SMITH AFM, 1992, AM STAT, V46, P84, DOI 10.2307/2684170; Ulker Y., 2010, INT C ART INT STAT INT C ART INT STAT, P876	28	2	2	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403007
C	Daskalakis, C; Dikkala, N; Kamath, G		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Daskalakis, Constantinos; Dikkala, Nishanth; Kamath, Gautam			Concentration of Multilinear Functions of the Ising Model with Applications to Network Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SYSTEMS	We prove near-tight concentration of measure for polynomial functions of the Ising model under high temperature. For any degree d, we show that a degreed polynomial of a n-spin Ising model exhibits exponential tails that scale as exp(-r(2/d)) at radius r = (Omega) over tilde (d) (n(d/2)). Our concentration radius is optimal up to logarithmic factors for constant d, improving known results by polynomial factors in the number of spins. We demonstrate the efficacy of polynomial functions as statistics for testing the strength of interactions in social networks in both synthetic and real world data.	[Daskalakis, Constantinos; Dikkala, Nishanth; Kamath, Gautam] MIT, EECS, Cambridge, MA 02139 USA; [Daskalakis, Constantinos; Dikkala, Nishanth; Kamath, Gautam] MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Daskalakis, C (corresponding author), MIT, EECS, Cambridge, MA 02139 USA.; Daskalakis, C (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	costis@csail.mit.edu; nishanthd@csail.mit.edu; g@csail.mit.edu	Jeong, Yongwook/N-7413-2016		NSF [CCF-1617730, CCF-1650733]; ONR [N00014-12-1-0999]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	Research was supported by NSF CCF-1617730, CCF-1650733, and ONR N00014-12-1-0999. Part of this work was done while GK was an intern at Microsoft Research New England.	Abbeel P, 2006, J MACH LEARN RES, V7, P1743; Bhattacharya Bhaswar B., 2016, INFERENCE ISING MODE; Bhattacharya Bhaswar B., 2016, ARXIV150807530; Bresler G, 2015, ACM S THEORY COMPUT, P771, DOI 10.1145/2746539.2746631; Bresler Guy, 2014, NIPS, P2852; Bresler Guy, 2016, ARXIV160406749; Cantador I., 2011, P 5 ACM C RECOMMENDE, P387, DOI DOI 10.1145/2043932.2044016; Chatterjee S., 2005, THESIS; Chatterjee S, 2007, ANN STAT, V35, P1931, DOI 10.1214/009053607000000109; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Csiszar I, 2006, ANN STAT, V34, P123, DOI 10.1214/009053605000000912; Daskalakis C, 2011, PROBAB THEORY REL, V149, P149, DOI 10.1007/s00440-009-0246-2; Daskalakis Constantinos, 2018, P 29 ANN ACM SIAM S; del Campo Abraham Martin, 2016, SCANDINAVIAN J STAT; Deser Stanley, 1968, STAT PHYS PHASE TRAN; Easley D., 2010, NETWORKS CROWDS MARK; Ellison G, 1993, ECONOMETRICA, V61, P1047, DOI DOI 10.2307/2951493; Felsenstein J., 2004, INFERRING PHYLOGENIE; Geman S, 1987, P INT C MATH, V1, P1496; Gheissari Reza, 2017, ARXIV170600121; Hamilton L., 2017, ADV NEURAL INFORM PR; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; Ising E, 1925, Z PHYS, V31, P253, DOI 10.1007/BF02980577; Jalali A., 2011, ADV NEURAL INF PROCE, V24, P1935; Klivans Adam, 2017, P 58 ANN IEEE S FDN; Levin D. A., 2009, MARKOV CHAINS MIXING; Mackey L, 2014, ANN PROBAB, V42, P906, DOI 10.1214/13-AOP892; Montanari A, 2010, P NATL ACAD SCI USA, V107, P20196, DOI 10.1073/pnas.1004098107; Onsager L, 1944, PHYS REV, V65, P117, DOI 10.1103/PhysRev.65.117; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; Schneidman E, 2006, NATURE, V440, P1007, DOI 10.1038/nature04701; SHERRINGTON D, 1975, PHYS REV LETT, V35, P1792, DOI 10.1103/PhysRevLett.35.1792; STROOCK DW, 1992, COMMUN MATH PHYS, V149, P175, DOI 10.1007/BF02096629; Vuffray M., 2016, ADV NEURAL INFORM PR, P2595	36	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400002
C	Dieng, AB; Tran, D; Ranganath, R; Paisley, J; Blei, DM		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dieng, Adji B.; Tran, Dustin; Ranganath, Rajesh; Paisley, John; Blei, David M.			Variational Inference via chi Upper Bound Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Variational inference (VI) is widely used as an efficient alternative to Markov chain Monte Carlo. It posits a family of approximating distributions q and finds the closest member to the exact posterior p. Closeness is usually measured via a divergence D (q parallel to p) from q to p. While successful, this approach also has problems. Notably, it typically leads to underestimation of the posterior variance. In this paper we propose CHIVI, a black-box variational inference algorithm that minimizes D chi(p parallel to q), the chi-divergence from p to q. CHIVI minimizes an upper bound of the model evidence, which we term the chi upper bound (CUBO). Minimizing the CUBO leads to improved posterior uncertainty, and it can also be used with the classical VI lower bound (ELBO) to provide a sandwich estimate of the model evidence. We study CHIVI on three models: probit regression, Gaussian process classification, and a Cox process model of basketball plays. When compared to expectation propagation and classical VI, CHIVI produces better error rates and more accurate estimates of posterior variance.	[Dieng, Adji B.; Tran, Dustin; Paisley, John; Blei, David M.] Columbia Univ, New York, NY 10027 USA; [Ranganath, Rajesh] Princeton Univ, Princeton, NJ 08544 USA	Columbia University; Princeton University	Dieng, AB (corresponding author), Columbia Univ, New York, NY 10027 USA.		Paisley, John/AAF-8586-2019		NSF [IIS-1247664]; ONR [N00014-11-1-0651]; DARPA PPAML [FA8750-14-2-0009]; DARPA SIMPLEX [N66001-15-C-4032]; Alfred P. Sloan Foundation; John Simon Guggenheim Foundation	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); DARPA PPAML; DARPA SIMPLEX; Alfred P. Sloan Foundation(Alfred P. Sloan Foundation); John Simon Guggenheim Foundation	We thank Alp Kucukelbir, Francisco J. R. Ruiz, Christian A. Naesseth, Scott W. Linderman, Maja Rudolph, and Jaan Altosaar for their insightful comments. This work is supported by NSF IIS-1247664, ONR N00014-11-1-0651, DARPA PPAML FA8750-14-2-0009, DARPA SIMPLEX N66001-15-C-4032, the Alfred P. Sloan Foundation, and the John Simon Guggenheim Foundation.	[Anonymous], 2014, ICLR; Beal Matthew James, 2003, VARIATIONAL ALGORITH; Bishop C.M., 2006, MACH LEARN, V128; Burda Yuri, 2016, INT C LEARN REPR; Dehaene G., 2015, NIPS; Gelman A., 2017, ARXIV14124869; Grosse R. B., 2015, ARXIV151102543; Hensman J., 2014, JMLR; Hernandez-Lobato J., 2016, ICML; Hoffman Matthew D, 2013, JMLR; Hoffman Matthew D, 2017, INT C MACH LEARN; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; KIM HC, 2003, P WORKSH PROB GRAPH, P37; Kuleshov V., 2017, NIPS; Kuss M, 2005, J MACH LEARN RES, V6, P1679; Li Y., 2015, NIPS; Li Y., 2016, NIPS; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Miller A., 2014, ICML; Minka T., 2004, TECHNICAL REPORT; Minka T., 2001, THESIS MIT CAMBRIDGE; Minka Tom, 2005, TECHNICAL REPORT; Opper M, 2000, NEURAL COMPUT, V12, P2655, DOI 10.1162/089976600300014881; Paisley J., 2012, ICML; Raftery AE, 1995, SOCIOL METHODOL, V25, P111, DOI 10.2307/271063; Ranganath R., 2014, AISTATS; Ranganath R., 2016, NIPS; Rezende D.J., 2014, ICML; Robert C, 2004, MONTE CARLO STAT MET, DOI DOI 10.1007/978-1-4757-4145-2; Sunehag P., 2009, P INT C ART INT STAT, P560; Teh Y.W., 2015, ARXIV151209327; Tran Dustin, 2016, ARXIV161009787	34	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402076
C	Emamjomeh-Zadeh, E; Kempe, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Emamjomeh-Zadeh, Ehsan; Kempe, David			A General Framework for Robust Interactive Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ORDER	We propose a general framework for interactively learning models, such as (binary or non-binary) classifiers, orderings/rankings of items, or clusterings of data points. Our framework is based on a generalization of Angluin's equivalence query model and Littlestone's online learning model: in each iteration, the algorithm proposes a model, and the user either accepts it or reveals a specific mistake in the proposal. The feedback is correct only with probability p > 1/2 (and adversarially incorrect with probability 1 - p), i.e., the algorithm must be able to learn in the presence of arbitrary noise. The algorithm's goal is to learn the ground truth model using few iterations. Our general framework is based on a graph representation of the models and user feedback. To be able to learn efficiently, it is sufficient that there be a graph G whose nodes are the models, and (weighted) edges capture the user feedback, with the property that if s, s* are the proposed and target models, respectively, then any (correct) user feedback s' must lie on a shortest s-s* path in G. Under this one assumption, there is a natural algorithm, reminiscent of the Multiplicative Weights Update algorithm, which will efficiently learn s* even in the presence of noise in the user's feedback. From this general result, we rederive with barely any extra effort classic results on learning of classifiers and a recent result on interactive clustering; in addition, we easily obtain new interactive learning algorithms for ordering/ranking.	[Emamjomeh-Zadeh, Ehsan; Kempe, David] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA	University of Southern California	Emamjomeh-Zadeh, E (corresponding author), Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.	emamjome@usc.edu; dkempe@usc.edu	Jeong, Yongwook/N-7413-2016		NSF [1619458]	NSF(National Science Foundation (NSF))	Research supported in part by NSF grant 1619458. We would like to thank Sanjoy Dasgupta, Ilias Diakonikolas, Shaddin Dughmi, Haipeng Luo, Shanghua Teng, and anonymous reviewers for useful feedback and suggestions.	Agichtein E., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P3, DOI 10.1145/1148170.1148175; Angluin D., 1992, Proceedings of the Twenty-Fourth Annual ACM Symposium on the Theory of Computing, P351, DOI 10.1145/129712.129746; Angluin D., 1988, Machine Learning, V2, P319, DOI 10.1023/A:1022821128753; Awasthi P, 2017, J MACH LEARN RES, V18; Awasthi Pranjal, 2010, ADV NEURAL INFORM PR, P91; Balcan MF, 2008, LECT NOTES ARTIF INT, V5254, P316, DOI 10.1007/978-3-540-87987-9_27; Bubley R, 1999, DISCRETE MATH, V201, P81, DOI 10.1016/S0012-365X(98)00333-1; Bubley R., 2001, DISTINGUISHED DISSER; Crammer K, 2002, ADV NEUR IN, V14, P641; Emamjomeh-Zadeh E, 2016, ACM S THEORY COMPUT, P519, DOI 10.1145/2897518.2897656; Huber M, 2006, DISCRETE MATH, V306, P420, DOI 10.1016/j.disc.2006.01.003; Joachims T., 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]; KARZANOV A, 1991, ORDER, V8, P7, DOI 10.1007/BF00385809; Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914; MAASS W, 1994, MACH LEARN, V14, P251, DOI 10.1007/BF00993976; Maass W., 1990, Proceedings. 31st Annual Symposium on Foundations of Computer Science (Cat. No.90CH2925-6), P203, DOI 10.1109/FSCS.1990.89539; MAASS W, 1992, MACH LEARN, V9, P107, DOI 10.1007/BF00992674; Radlinski F., 2005, QUERY CHAINS LEARNIN, P239, DOI DOI 10.1145/1081870.1081899; Sauer N., 1972, J COMB THEORY A, V13, P145, DOI [10.1016/0097-3165(72)90019-2, DOI 10.1016/0097-3165(72)90019-2]; SHELAH S, 1972, PAC J MATH, V41, P247, DOI 10.2140/pjm.1972.41.247; Wagstaff KL, 2002, THESIS CORNELL U ITH	21	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407017
C	Farahmand, AM; Pourazarm, S; Nikovski, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Farahmand, Amir-massoud; Pourazarm, Sepideh; Nikovski, Daniel			Random Projection Filter Bank for Time Series Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose Random Projection Filter Bank (RPFB) as a generic and simple approach to extract features from time series data. RPFB is a set of randomly generated stable autoregressive filters that are convolved with the input time series to generate the features. These features can be used by any conventional machine learning algorithm for solving tasks such as time series prediction, classification with time series data, etc. Different filters in RPFB extract different aspects of the time series, and together they provide a reasonably good summary of the time series. RPFB is easy to implement, fast to compute, and parallelizable. We provide an error upper bound indicating that RPFB provides a reasonable approximation to a class of dynamical systems. The empirical results in a series of synthetic and real-world problems show that RPFB is an effective method to extract features from time series.	[Farahmand, Amir-massoud; Pourazarm, Sepideh; Nikovski, Daniel] MERL, Cambridge, MA 02139 USA		Farahmand, AM (corresponding author), MERL, Cambridge, MA 02139 USA.	farahmand@merl.com; sepid@bu.edu; nikovski@merl.com	Jeong, Yongwook/N-7413-2016					Baraniuk RG, 2009, FOUND COMPUT MATH, V9, P51, DOI 10.1007/s10208-007-9011-z; Bishop CM, 2006, PATTERN RECOGNITION; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Doukhan P., 1994, LECT NOTES STAT; Farahmand AM, 2012, J STAT PLAN INFER, V142, P493, DOI 10.1016/j.jspi.2011.08.007; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Hastie T, 2009, ELEMENTS STAT LEARNI; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kakade Sham, 2017, ARXIV161202526V2; Kuznetsov V., 2015, ADV NEURAL INFORM PR, P541; Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005; Meir R, 2000, MACH LEARN, V39, P5, DOI 10.1023/A:1007602715810; Mohri M., 2009, ADV NEURAL INFORM PR, V21, P1097; Mohri M, 2010, J MACH LEARN RES, V11, P789; Oliva JB, 2017, PR MACH LEARN RES, V70; Oppenheim A.V., 1999, DISCRETE TIME SIGNAL, V2 edition; Pourazarm Sepideh, 2017, ANN C PROGN HLTH MAN, P242; Rakhlin Alexander, 2014, PROBABILITY THEORY R; Rakhlin Alexander, 2010, ADV NEURAL INFORM PR; Steinwart I., 2009, ADV NEURAL INF PROCE, V22, P1768; Steinwart I, 2009, J MULTIVARIATE ANAL, V100, P175, DOI 10.1016/j.jmva.2008.04.001; Vempala Santosh S., 2004, DIMACS SERIES DISCRE; Wasserman L., 2007, SPRINGER TEXTS STAT; White M, 2015, AAAI CONF ARTIF INTE, P3080	26	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406061
C	Goel, S; Klivans, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Goel, Surbhi; Klivans, Adam			Eigenvalue Decay Implies Polynomial-Time Learnability for Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				GRAM MATRIX; COMPLEXITY; ERROR	We consider the problem of learning function classes computed by neural networks with various activations (e.g. ReLU or Sigmoid), a task believed to be computationally intractable in the worst-case. A major open problem is to understand the minimal assumptions under which these classes admit provably efficient algorithms. In this work we show that a natural distributional assumption corresponding to eigenvalue decay of the Gram matrix yields polynomial-time algorithms in the non-realizable setting for expressive classes of networks (e.g. feed-forward networks of ReLUs). We make no assumptions on the structure of the network or the labels. Given sufficiently-strong eigenvalue decay, we obtain fully-polynomial time algorithms in all the relevant parameters with respect to square-loss. This is the first purely distributional assumption that leads to polynomial-time algorithms for networks of ReLUs. Further, unlike prior distributional assumptions (e.g., the marginal distribution is Gaussian), eigenvalue decay has been observed in practice on common data sets.	[Goel, Surbhi; Klivans, Adam] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Goel, S (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	surbhi@cs.utexas.edu; klivans@cs.utexas.edu	Jeong, Yongwook/N-7413-2016		Microsoft Data Science Initiative Award	Microsoft Data Science Initiative Award(Microsoft)	Work supported by a Microsoft Data Science Initiative Award.	Auer P, 1996, ADV NEUR IN, V8, P316; Avron H, 2014, ADV NEUR IN, V27; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bartlett Peter, COMMUNICATION; Bartlett Peter L., 2005, LOCAL RADEMACHER COM, V33; Brach Pawel, 2015, CORR; Brutzkus A., 2017, CORR; Choromanska Anna, 2015, JMLR WORKSHOP C P, V38; Cotter A., 2013, PROC INT C MACH LEAR, P266; Daniely A, 2016, ACM S THEORY COMPUT, P105, DOI 10.1145/2897518.2897520; Daniely Amit, 2017, CORR; David Ofir, 2016, ARXIV161003592; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X; GOEL S., 2016, ARXIV161110258; Janzamin M., 2015, ARXIV150608473; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Klivans A.R., 2014, LIPICS, P793; Klivans AR, 2009, J COMPUT SYST SCI, V75, P2, DOI 10.1016/j.jcss.2008.07.008; Klivans Adam R., 2013, ELECT C COMPUTATIONA, V20, P8; Krohmer Anton, 2012, THESIS; Kuzmin D, 2007, J MACH LEARN RES, V8, P2047; Littlestone N., 1986, RELATING DATA COMPRE; Livni R., 2014, NIPS, V1, P855; Ma S., 2017, CORR; Musco C., 2016, ARXIV160507583; Scholkopf B., 2001, LEARNING KERNELS SUP; Scholkopf B., 1999, 99035 NEUROCOLT; SEDGHI H., 2014, ARXIV14122693; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; SHAMIR O., 2016, ARXIV160901037; Shamir O, 2015, J MACH LEARN RES, V16, P3475; Shawe-Taylor J, 2005, IEEE T INFORM THEORY, V51, P2510, DOI 10.1109/TIT.2005.850052; Song Le, 2017, ARXIV170704615; Soudry Daniel, 2016, CORR; Talwalkar Ameet, 2014, CORR; Williams CK, 2000, P 13 INT C NEUR INF, P661; Xie Bo, 2016, CORR; Zhang Q, 2017, CORR; Zhang T, 2003, ADV NEURAL INFORM PR, P471; Zhang YC, 2016, PR MACH LEARN RES, V48; Zhang Yuchen, 2015, CORR	43	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402024
C	Hauser, M; Ray, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hauser, Michael; Ray, Asok			Principles of Riemannian Geometry in Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This study deals with neural networks in the sense of geometric transformations acting on the coordinate representation of the underlying data manifold which the data is sampled from. It forms part of an attempt to construct a formalized general theory of neural networks in the setting of Riemannian geometry. From this perspective, the following theoretical results are developed and proven for feedforward networks. First it is shown that residual neural networks are finite difference approximations to dynamical systems of first order differential equations, as opposed to ordinary networks that are static. This implies that the network is learning systems of differential equations governing the coordinate transformations that represent the data. Second it is shown that a closed form solution of the metric tensor on the underlying data manifold can be found by backpropagating the coordinate representations learned by the neural network itself This is formulated in a formal abstract sense as a sequence of Lie group actions on the metric fibre space in the principal and associated bundles on the data manifold. Toy experiments were run to confirm parts of the proposed theory, as well as to provide intuitions as to how neural networks operate on data.	[Hauser, Michael; Ray, Asok] Penn State Univ, Dept Mech Engn, State Coll, PA 16801 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University	Hauser, M (corresponding author), Penn State Univ, Dept Mech Engn, State Coll, PA 16801 USA.	mzh190@psu.edu; axr2@psu.edu	Jeong, Yongwook/N-7413-2016		U.S. Air Force Of<SUP>f</SUP>ice of Scientific Research (AFOSR) [FA9550-15-1-0400]; PSU/ARL Walker Fellowship	U.S. Air Force Of<SUP>f</SUP>ice of Scientific Research (AFOSR); PSU/ARL Walker Fellowship	This work has been supported in part by the U.S. Air Force Of<SUP>f</SUP>ice of Scientific Research (AFOSR) under Grant No. FA9550-15-1-0400. The first author has been supported by PSU/ARL Walker Fellowship. Any opinions,findings and conclusions or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the sponsoring agencies.	Bastien F., 2012, DEEP LEARN UNS FEAT; Bengio Yoshua, 2013, INT C MACHINE LEARNI, P552; Boulch A, 2017, ARXIV170208782; Glorot X., 2010, PROC MACH LEARN RES, P249; Graves A., 2014, ARXIV14105401; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lee J. M, 2000, INTRO SMOOTH MANIFOL, DOI [10.1007/978-0-387-21752-9, DOI 10.1007/978-0-387-21752-9]; Mikolov T., 2013, P 2013 C N AM CHAPTE, P746, DOI DOI 10.3109/10826089109058901; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Rumelhart DE, 1985, TECHNICAL REPORT, DOI 10.1016/b978-1-4832-1446-7.50035-2; Theano Development Team, 2016, ABS160502688 ARXIV T; Veit A, 2016, ADV NEUR IN, V29; Walschap G., 2012, METRIC STRUCTURES DI, V224; Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957	19	2	2	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402083
C	Indyk, P; Razenshteyn, I; Wagner, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Indyk, Piotr; Razenshteyn, Ilya; Wagner, Tal			Practical Data-Dependent Metric Compression with Provable Guarantees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NEAREST-NEIGHBOR; SEARCH	We introduce a new distance-preserving compact representation of multi-dimensional point-sets. Given n points in a d-dimensional space where each coordinate is represented using B bits (i.e., dB bits per point), it produces a representation of size O(dlog(dB/is an element of) + log n) bits per point from which one can approximate the distances up to a factor of 1 +/- is an element of. Our algorithm almost matches the recent bound of [6] while being much simpler. We compare our algorithm to Product Quantization (PQ) [7], a state of the art heuristic metric compression method. We evaluate both algorithms on several data sets: SIFT (used in [7]), MNIST [11], New York City taxi time series [4] and a synthetic one-dimensional data set embedded in a high-dimensional space. With appropriately tuned parameters, our algorithm produces representations that are comparable to or better than those produced by PQ, while having provable guarantees on its performance.	[Indyk, Piotr; Razenshteyn, Ilya; Wagner, Tal] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Indyk, P (corresponding author), MIT, Cambridge, MA 02139 USA.		Jeong, Yongwook/N-7413-2016					Arandjelovic R, 2014, IEEE T PATTERN ANAL, V36, P2396, DOI 10.1109/TPAMI.2014.2339821; Bartal Y, 1996, AN S FDN CO, P184, DOI 10.1109/SFCS.1996.548477; Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900; Burges, 1998, MNIST DATABASE HANDW; Guha S, 2016, PR MACH LEARN RES, V48; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Indyk P, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P710; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Johnson J, 2017, IEEE I CONF COMP VIS, P3008, DOI 10.1109/ICCV.2017.325; Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400; Kushilevitz E, 2000, SIAM J COMPUT, V30, P457, DOI 10.1137/S0097539798347177; Li Ping, 2011, ADV NEURAL INFORM PR, P2672; Mohammad Norouzi, 2012, ADV NEURAL INFORM PR, P1061; Muthukrishnan S, 2005, FOUND TRENDS THEOR C, V1, P1, DOI 10.1561/0400000002; Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388; Raginsky M., 2009, ADV NEURAL INFORM PR, P1509, DOI [10.5555/2984093.2984263, DOI 10.5555/2984093.2984263]; Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006; Shrivastava A, 2014, PR MACH LEARN RES, V32; Torralba A, 2008, PROC CVPR IEEE, P2269; Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976; Weinberger K., 2009, P 26 ANN INT C MACH, P1113, DOI DOI 10.1145/1553374.1553516; Weiss Y, 2009, ADV NEURAL INFORM PR, P1753; YAU MM, 1983, COMMUN ACM, V26, P504, DOI 10.1145/358150.358158	23	2	2	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402065
