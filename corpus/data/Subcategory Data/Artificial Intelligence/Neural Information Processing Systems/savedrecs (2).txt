PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
C	Fisher, JW; Darrell, T; Freeman, WT; Viola, P		Leen, TK; Dietterich, TG; Tresp, V		Fisher, JW; Darrell, T; Freeman, WT; Viola, P			Learning joint statistical models for audio-visual fusion and segregation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					People can understand complex auditory and visual information, often using one to disambiguate the other. Automated analysis, even at a low-level, faces severe challenges, including the lack of accurate statistical models for the signals, and their high-dimensionality and varied sampling rates. Previous approaches [6] assumed simple parametric models for the joint distribution which, while tractable, cannot capture the complex signal relationships. We team the joint distribution of the visual and auditory signals using a non-Parametric approach. First, we project the data into a maximally informative, low-dimensional subspace, suitable for density estimation. We then model the complicated stochastic relationships between the signals using a nonparametric density estimator. These learned densities allow processing across signal modalities. We demonstrate, on synthetic and real signals, localization in video of the face that is speaking in audio, and, conversely, audio enhancement of a particular speaker selected from the video.	MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Fisher, JW (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	fisher@ai.mit.edu; trevor@ai.mit.edu; freeman@merl.com; viola@ai.mit-edu						ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; Fisher JW, 1996, PROC SPIE, V2752, P2, DOI 10.1117/12.235636; FISHER JW, 1999, P 1999 C ADV NEUR IN, V12; FISHER JW, 1998, P IEEE INT JOINT C N; HERSHEY J, 1999, P 1999 C ADV NEUR IN, V12; Viola P, 1996, ADV NEUR IN, V8, P851	7	28	30	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						772	778						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800109
C	Hyvarinen, A; Hoyer, P		Solla, SA; Leen, TK; Muller, KR		Hyvarinen, A; Hoyer, P			Emergence of topography and complex cell properties from natural images using extensions of ICA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				INDEPENDENT COMPONENT ANALYSIS; ALGORITHM; FILTERS	Independent component analysis of natural images leads to emergence of simple cell properties, i.e. linear filters that resemble wavelets or Gabor functions. In this paper, we extend ICA to explain further properties of V1 cells. First, we decompose natural images into independent subspaces instead of scalar components. This model leads to emergence of phase and shift invariant features, similar to those in V1 complex cells. Second, we define a topography between the linear components obtained by ICA. The topographic distance between two components is defined by their higher-order correlations, so that two components are close to each other in the topography if they are strongly dependent on each other. This leads to simultaneous emergence of both topography and invariances similar to complex cell properties.	Helsinki Univ Technol, Neural Networks Res Ctr, FIN-02015 Helsinki, Finland	Aalto University	Hyvarinen, A (corresponding author), Helsinki Univ Technol, Neural Networks Res Ctr, POB 5400, FIN-02015 Helsinki, Finland.							Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Cardoso J.F., 1998, P IEEE INT C AC SPEE; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; Foldiak P, 1991, NEURAL COMPUT, V3, P194, DOI 10.1162/neco.1991.3.2.194; Hyvarinen A, 1997, NEURAL COMPUT, V9, P1483, DOI 10.1162/neco.1997.9.7.1483; HYVARINEN A, 1999, UNPUB TOPOGRAPHIC IN; HYVARINEN A, 2000, IN PRESS ADV INDEPEN; HYVARINEN A, 2000, IN PRESS NEURAL COMP; JUTTEN C, 1991, SIGNAL PROCESS, V24, P1, DOI 10.1016/0165-1684(91)90079-X; Kohonen T, 1996, BIOL CYBERN, V75, P281, DOI 10.1007/s004220050295; Kohonen T., 1995, SELF ORG MAPS, V30, DOI 10.1007/978-3-642-97610-0; Lin JK, 1998, ADV NEUR IN, V10, P563; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Simoncelli EP, 1999, ADV NEUR IN, V11, P153; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P359, DOI 10.1098/rspb.1998.0303	15	28	28	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						827	833						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700117
C	Kempter, R; Gerstner, W; van Hemmen, JL		Kearns, MS; Solla, SA; Cohn, DA		Kempter, R; Gerstner, W; van Hemmen, JL			Spike-based compared to rate-based Hebbian learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				COINCIDENCE; CELLS	A correlation-based learning rule at the spike level is formulated, mathematically analyzed, and compared to learning in a firing-rate description. A differential equation for the learning dynamics is derived under the assumption that the time scales of learning and spiking can be separated. For a linear Poissonian neuron model which receives time-dependent stochastic input we show that spike correlations on a millisecond time scale play indeed a role. Correlations between input and output spikes tend to stabilize structure formation, provided that the form of the learning window is in accordance with Hebb's principle. Conditions for an intrinsic normalization of the average synaptic weight are discussed.	Tech Univ Munich, Inst Theoret Phys, D-85747 Garching, Germany	Technical University of Munich	Kempter, R (corresponding author), Tech Univ Munich, Inst Theoret Phys, D-85747 Garching, Germany.							Abeles M, 1994, MODELS NEURAL NETWOR, P121, DOI [10.1007/978-1-4612-4320-5_3, DOI 10.1007/978-1-4612-4320-5_3]; BIALEK W, 1991, SCIENCE, V252, P1855; CARR CE, 1993, ANNU REV NEUROSCI, V16, P223, DOI 10.1146/annurev.ne.16.030193.001255; Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0; Gerstner W, 1998, PULSED NEURAL NETWORKS, P353; Hebb D., 1949, ORG BEHAV; Kempter R, 1998, NEURAL COMPUT, V10, P1987, DOI 10.1162/089976698300016945; KEMPTER R, 1999, IN PRESS PHYS REV E; LINSKER R, 1986, P NATL ACAD SCI USA, V83, P7508, DOI 10.1073/pnas.83.19.7508; MacKay DJC, 1990, NETWORK-COMP NEURAL, V1, P257, DOI 10.1088/0954-898X/1/3/001; Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213; SENN W, 1999, PREPRINT U BERN; Wimbauer S, 1997, BIOL CYBERN, V77, P453, DOI 10.1007/s004220050405; Zhang LI, 1998, NATURE, V395, P37, DOI 10.1038/25665	14	28	30	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						125	131						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700018
C	Singh, S; Cohn, D		Jordan, MI; Kearns, MJ; Solla, SA		Singh, S; Cohn, D			How to dynamically merge Markov decision processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We are frequently called upon to perform multiple tasks that compete for our attention and resource. Often we know the optimal solution to each task in isolation; in this paper, we describe how this knowledge can be exploited to efficiently find good solutions for doing the tasks in parallel. We formulate this problem as that of dynamically merging multiple Markov decision processes (MDPs) into a composite MDP, and present a new theoretically-sound dynamic programming algorithm for finding an optimal policy for the composite MDP. We analyze various aspects of our algorithm and illustrate its use on a simple merging problem.	Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Singh, S (corresponding author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.								0	28	28	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						1057	1063						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700149
C	Wan, EA; Nelson, AT		Mozer, MC; Jordan, MI; Petsche, T		Wan, EA; Nelson, AT			Dual Kalman filtering methods for nonlinear prediction, smoothing, and estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Prediction, estimation, and smoothing are fundamental to signal processing. To perform these interrelated tasks given noisy data, we form a time series model of the process that generates the data. Taking noise in the system explicitly into account, maximum-likelihood and Kalman frameworks are discussed which involve the dual process of estimating both the model parameters and the underlying state of the system. We review several established methods in the linear case, and propose several extensions utilizing dual Kalman filters (DKF) and forward-backward (FB) filters that are applicable to neural networks. Methods are compared on several simulations of noisy time series. We also include an example of nonlinear noise reduction in speech.			Wan, EA (corresponding author), OREGON GRAD INST,DEPT ELECT ENGN,POB 91000,PORTLAND,OR 97291, USA.								0	28	28	1	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						793	799						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00112
C	Ding, YM; Florensa, C; Phielipp, M; Abbeel, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ding, Yiming; Florensa, Carlos; Phielipp, Mariano; Abbeel, Pieter			Goal-conditioned Imitation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might require many samples to discover how to reach certain areas of the state-space. In this work we propose a novel algorithm goalGAIL, which incorporates demonstrations to drastically speed up the convergence to a policy able to reach any goal, surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, we show our method can also be used when the available expert trajectories do not contain the actions or when the expert is suboptimal, which makes it applicable when only kinesthetic, third-person or noisy demonstrations are available. Our code is open-source(2).	[Ding, Yiming; Florensa, Carlos; Abbeel, Pieter] Univ Calif Berkeley, Dept Comp Sci, Berkeley, CA 94720 USA; [Phielipp, Mariano] Intel AI Labs, Santa Clara, CA USA	University of California System; University of California Berkeley	Ding, YM (corresponding author), Univ Calif Berkeley, Dept Comp Sci, Berkeley, CA 94720 USA.	dingyiming0427@berkeley.edu; florensa@berkeley.edu; mariano.j.phielipp@intel.com; pabbeel@berkeley.edu						Andrychowicz Marcin, 2017, ADV NEURAL INFORM PR, DOI [10.1016/j.surfcoat.2018.06.018, DOI 10.1016/J.SURFCOAT.2018.06.018]; Andrychowicz Marcin, LEARNING DEXTEROUS I, P1; Bojarski Mariusz, 2016, arXiv; Finn C, 2016, PR MACH LEARN RES, V48; Florensa C., 2017, PROC 1 C ROBOT LEARN, P482; Florensa C., 2017, PROC INT C MACH LEAR, P1; Florensa C, 2018, PR MACH LEARN RES, V80; Florensa Carlos, 2018, WORKSH INF CONTR NEU; Fu J, 2018, UNCERTAIN OPER RES, P1, DOI 10.1007/978-981-10-7817-0_1; Goodfellow I. J, GENERATIVE ADVERSARI, P1, DOI DOI 10.3156/JSOFT.29.5_177_2; Haarnoja T, 2018, PR MACH LEARN RES, V80; Heess Nicolas, 2017, ABS170702286 CORR, P3; Ho Jonathan, 2016, ADV NEURAL INFORM PR; KAELBLING LP, 1993, IJCAI-93, VOLS 1 AND 2, P1094; Kalakrishnan M, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P167, DOI 10.1109/IROS.2009.5354701; Kalousis, 2019, AISTATS, P3138; Kostrikov Ilya, 2019, INT C LEARN REPR; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Lin Xingyu, 2019, INT C ROB AUT; Manschitz S, 2015, ROBOT AUTON SYST, V74, P97, DOI 10.1016/j.robot.2015.07.005; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nair A, 2018, ADV NEUR IN, V31; Nair Ashvin, 2018, INT C ROB AUT, DOI [10.1080/09692290.2013., DOI 10.1080/09692290.2013]; Peng Xue Bin, 2018, ACM T GRAPHIC, V37, DOI DOI 10.1145/3197517.3201311; Pomerleau D.A., 1989, ALVINN AUTONOMOUS LA; Rajeswaran A, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Riedmiller M, 2018, PR MACH LEARN RES, V80; Ross St<prime>ephane, 2011, AISTATS; Sasaki Fumihiro, 2019, INT C LEARN REPR, P1; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Schroecker Y., 2019, INT C LEARN REPR ICL; Schulman J., 2015, TRUSTREGIONPOLICYOPT; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Stadie BC, 2017, INT C LEARN REPR; Sun Wen, 2018, THESIS CHINESE CTR D, P1, DOI [10.1051/0004- 6361/201527329, DOI 10.1051/0004-6361/201527329]; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Vecerik M., 2017, LEVERAGING DEMONSTRA, P1; Vinyals Oriol, 2019, TECHNICAL REPORT; Wei Y, 2018, 2018 IEEE SYMPOSIUM ON PRODUCT COMPLIANCE ENGINEERING - ASIA 2018 (IEEE ISPCE-CN 2018), P100; Yang G, 2018, INT C MACH LEARN; Zhu Y., 2018, ROBOTICS SCI SYSTEMS; Ziebart Brian D., 2008, MAXIMUM ENTROPY INVE, P1433	42	27	27	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907003
C	Fan, LX; Ng, KW; Chan, CS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fan, Lixin; Ng, Kam Woh; Chan, Chee Seng			Rethinking Deep Neural Network Ownership Verification: Embedding Passports to Defeat Ambiguity Attacks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					With substantial amount of time, resources and human (team) efforts invested to explore and develop successful deep neural networks (DNN), there emerges an urgent need to protect these inventions from being illegally copied, redistributed, or abused without respecting the intellectual properties of legitimate owners. Following recent progresses along this line, we investigate a number of watermark-based DNN ownership verification methods in the face of ambiguity attacks, which aim to cast doubts on the ownership verification by forging counterfeit watermarks. It is shown that ambiguity attacks pose serious threats to existing DNN watermarking methods. As remedies to the above-mentioned loophole, this paper proposes novel passport-based DNN ownership verification schemes which are both robust to network modifications and resilient to ambiguity attacks. The gist of embedding digital passports is to design and train DNN models in a way such that, the DNN inference performance of an original task will be significantly deteriorateddue to forged passports. In other words, genuine passports are not only verified by looking for the predefined signatures, but also reasserted by the unyielding DNN model inference performances. Extensive experimental results justify the effectiveness of the proposed passport-based DNN ownership verification schemes.	[Fan, Lixin] WeBank AI Lab, Shenzhen, Peoples R China; [Ng, Kam Woh; Chan, Chee Seng] Univ Malaya, Fac Comp Sci & Info Tech, Ctr Image & Signal Proc, Kuala Lumpur, Malaysia	Universiti Malaya	Fan, LX (corresponding author), WeBank AI Lab, Shenzhen, Peoples R China.	lixinfan@webank.com; kamwoh@siswa.um.edu.my; cs.chan@um.edu.my	Chan, Chee Seng/B-9754-2011	Chan, Chee Seng/0000-0001-7677-2865	Fundamental Research Grant Scheme (FRGS) MoHE Grant from the Ministry of Education Malaysia [FP021-2018A]	Fundamental Research Grant Scheme (FRGS) MoHE Grant from the Ministry of Education Malaysia	This research is partly supported by the Fundamental Research Grant Scheme (FRGS) MoHE Grant FP021-2018A, from the Ministry of Education Malaysia. Also, we gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan V GPU used for this research.	Adi Y, 2018, PROCEEDINGS OF THE 27TH USENIX SECURITY SYMPOSIUM, P1615; Allen-Zhu Z, 2019, PR MACH LEARN RES, V97; Chen H., 2018, ARXIV180403648; Craver S, 1998, IEEE J SEL AREA COMM, V16, P573, DOI 10.1109/49.668979; Fan L., 2019, ARXIV190907830; Guo J., 2018, IEEE ACM INT C COMP, P1; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Le Merrer Erwan, 2017, ARXIV171101894; Li Q., 2006, P IEEE INT C NETW PR, P158; Luong A, 2016, P 20 SIGNLL C COMP N, P291, DOI DOI 10.18653/V1/K16-1029; Rouhani Bita Darvish, 2018, ARXIV180400750; Sencar HT, 2007, IEEE T INF FOREN SEC, V2, P664, DOI 10.1109/TIFS.2007.908211; Uchida Y, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P274, DOI 10.1145/3078971.3078974; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Zhang Chiyuan, 2016, ARXIV161103530; Zhang JL, 2018, PROCEEDINGS OF THE 2018 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIACCS'18), P159, DOI 10.1145/3196494.3196550	16	27	28	0	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304069
C	Kumar, A; Liang, P; Ma, TY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kumar, Ananya; Liang, Percy; Ma, Tengyu			Verified Uncertainty Calibration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DECOMPOSITION; RELIABILITY; FORECASTS	Applications such as weather forecasting and personalized medicine demand models that output calibrated probability estimates-those representative of the true likelihood of a prediction. Most models are not calibrated out of the box but are recalibrated by post-processing model outputs. We find in this work that popular recalibration methods like Platt scaling and temperature scaling are (i) less calibrated than reported, and (ii) current techniques cannot estimate how miscalibrated they are. An alternative method, histogram binning, has measurable calibration error but is sample inefficient-it requires O(B/epsilon(2)) samples, compared to O(1/epsilon(2)) for scaling methods, where B is the number of distinct probabilities the model can output. To get the best of both worlds, we introduce the scaling-binning calibrator, which first fits a parametric function to reduce variance and then bins the function values to actually ensure calibration. This requires only O(1/epsilon(2) + B) samples. Next, we show that we can estimate a model's calibration error more accurately using an estimator from the meteorological community-or equivalently measure its calibration error with fewer samples (O(root B) instead of O(B)). We validate our approach with multiclass calibration experiments on CIFAR-10 and ImageNet, where we obtain a 35% lower calibration error than histogram binning and, unlike scaling methods, guarantees on true calibration. We implement all these methods in a Python library: https://pypi.org/project/uncertainty-calibartion	[Kumar, Ananya; Liang, Percy; Ma, Tengyu] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Kumar, A (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	ananya@cs.stanford.edu; pliang@cs.stanford.edu; tengyuma@cs.stanford.edu			Toyota Research Institute; Toyota Research Institute ("TRI"); Open Philantropy Project; Stanford Graduate Fellowship	Toyota Research Institute; Toyota Research Institute ("TRI"); Open Philantropy Project; Stanford Graduate Fellowship(Stanford University)	The authors would like to thank the Open Philantropy Project, Stanford Graduate Fellowship, and Toyota Research Institute for funding. Toyota Research Institute ("TRI") provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity.	Abadi M, 2015, P 12 USENIX S OPERAT; Brocker J, 2007, WEATHER FORECAST, V22, P651, DOI 10.1175/WAF993.1; Brocker J, 2012, CLIM DYNAM, V39, P655, DOI 10.1007/s00382-011-1191-1; Card Dallas, 2018, P 2018 C N AM CHAPT, P1636; Chollet F., 2015, KERAS; Crowson CS, 2017, STAT METHODS MED RES, V26, P1992, DOI 10.1177/0962280217719818; DEGROOT MH, 1983, J ROY STAT SOC D-STA, V32, P12; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Ferro CAT, 2012, Q J ROY METEOR SOC, V138, P1954, DOI 10.1002/qj.1924; Fischhoff B., 1982, JUDGEMENT UNCERTAINT; Gelman A, 2013, BAYESIAN DATA ANAL, P16; Gneiting T, 2005, SCIENCE, V310, P248, DOI 10.1126/science.1115255; Gneiting T, 2007, J R STAT SOC B, V69, P243, DOI 10.1111/j.1467-9868.2007.00587.x; Harrell FE, 1996, STAT MED, V15, P361, DOI 10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4; Hebert-Johnson U, 2018, PR MACH LEARN RES, V80; Hendrycks D., 2019, ICLR, P1; Hendrycks D, 2019, PR MACH LEARN RES, V97; HOSMER DW, 1980, COMMUN STAT A-THEOR, V9, P1043, DOI 10.1080/03610928008827941; Hubbard J. H., 1998, VECTOR CALCULUS LINE; Jiang XQ, 2012, J AM MED INFORM ASSN, V19, P263, DOI 10.1136/amiajnl-2011-000291; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kuleshov V, 2018, PR MACH LEARN RES, V80; Kuleshov V, 2015, ADV NEUR IN, V28; Lei J., 2016, J AM STAT ASSOC, V113, P1094; Liu LT, 2019, PR MACH LEARN RES, V97; Malik A, 2019, PR MACH LEARN RES, V97; Murphy A. H., 1973, Journal of Applied Meteorology, V12, P595, DOI 10.1175/1520-0450(1973)012<0595:ANVPOT>2.0.CO;2; Murphy Allan H, 1977, J ROYAL STAT SOC C, V26, P41, DOI [DOI 10.2307/2346866, 10.2307/2346866]; Naeini M. P., 2014, BINARY CLASSIFIER CA; Naeini MP, 2015, AAAI CONF ARTIF INTE, P2901; Nguyen Khanh, 2015, P C EMP METH NAT LAN; Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; Platt JC, 2000, ADV NEUR IN, P61; Shafer G, 2008, J MACH LEARN RES, V9, P371; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Vaart A. W., 1998, ASYMPTOTIC STAT; Widmann D, 2019, ADV NEUR IN, V32; Yadlowsky S., 2019, MACHINE LEARNING HEA; Yonatan Geifman, 2015, CIFAR VGG; Yu D, 2011, IEEE T AUDIO SPEECH, V19, P2461, DOI 10.1109/TASL.2011.2141988; Zadrozny Bianca, 2001, ICML; Zhu LH, 2017, 2017 IEEE SECOND INTERNATIONAL CONFERENCE ON DATA SCIENCE IN CYBERSPACE (DSC), P213, DOI 10.1109/DSC.2017.89	49	27	27	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303074
C	Liang, JB; Lin, MC; Koltun, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liang, Junbang; Lin, Ming C.; Koltun, Vladlen			Differentiable Cloth Simulation for Inverse Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a differentiable cloth simulator that can be embedded as a layer in deep neural networks. This approach provides an effective, robust framework for modeling cloth dynamics, self-collisions, and contacts. Due to the high dimensionality of the dynamical system in modeling cloth, traditional gradient computation for collision response can become impractical. To address this problem, we propose to compute the gradient directly using QR decomposition of a much smaller matrix. Experimental results indicate that our method can speed up backpropagation by two orders of magnitude. We demonstrate the presented approach on a number of inverse problems, including parameter estimation and motion control for cloth.	[Liang, Junbang; Lin, Ming C.] Univ Maryland, College Pk, MD 20742 USA; [Koltun, Vladlen] Intel Labs, Santa Clara, CA USA	University System of Maryland; University of Maryland College Park; Intel Corporation	Liang, JB (corresponding author), Univ Maryland, College Pk, MD 20742 USA.							Amos B, 2017, PR MACH LEARN RES, V70; Baraff D., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P43, DOI 10.1145/280814.280821; Belbute-Peres FD, 2018, ADV NEUR IN, V31; Bouman Katherine L., 2013, INT C COMP VIS ICCV; Cline Michael Bradley, 2002, THESIS; Cottle R. W., 2009, LINEAR COMPLEMENTARI; Cusumano-Towner M, 2011, IEEE INT CONF ROBOT; Degrave J, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00006; Etzmuss Olaf, 2003, PAC C COMP GRAPH APP; Grinspun Eitan, 2003, S COMP AN; Gundogdu Erhan, 2019, INT C COMP VIS ICCV; Harmon D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360622; Hu YM, 2019, IEEE INT CONF ROBOT, P6265, DOI 10.1109/ICRA.2019.8794333; Ingraham J., 2019, INT C LEARN REPR; Kutz JN, 2017, J FLUID MECH, V814, P1, DOI 10.1017/jfm.2016.803; Lahner Z, 2018, LECT NOTES COMPUT SC, V11208, P698, DOI 10.1007/978-3-030-01225-0_41; Li Yunzhu, 2019, ICLR; Liang E, 2018, PR MACH LEARN RES, V80; Liu F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899408; Miller Stephen, 2012, I J ROBOTICS RES, V31; Morton J, 2018, ADV NEUR IN, V31; Mrowca Damian, 2018, ARXIV180608047; Narain R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366171; Paszke A, 2019, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.1912.01703; PONSMOLL G, 2017, ACM T GRAPHIC, V36, DOI DOI 10.1145/3072959.3073711; Santesteban Igor, 2019, EUROGRAPHICS; Schenck Connor, 2018, P MACHINE LEARNING R, P317; Tang Min, 2010, S INT 3D GRAPH GAM; Toussaint M, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Yang Shan, 2017, INT C COMP VIS ICCV	31	27	27	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300070
C	Meng, Y; Huang, JX; Wang, GY; Zhang, C; Zhuang, HL; Kaplan, L; Han, JW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Meng, Yu; Huang, Jiaxin; Wang, Guangyuan; Zhang, Chao; Zhuang, Honglei; Kaplan, Lance; Han, Jiawei			Spherical Text Embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Unsupervised text embedding has shown great power in a wide range of NLP tasks. While text embeddings are typically learned in the Euclidean space, directional similarity is often more effective in tasks such as word similarity and document clustering, which creates a gap between the training stage and usage stage of text embedding. To close this gap, we propose a spherical generative model based on which unsupervised word and paragraph embeddings are jointly learned. To learn text embeddings in the spherical space, we develop an efficient optimization algorithm with convergence guarantee based on Riemannian optimization. Our model enjoys high efficiency and achieves state-of-the-art performances on various text embedding tasks including word similarity and document clustering.	[Meng, Yu; Huang, Jiaxin; Wang, Guangyuan; Zhuang, Honglei; Han, Jiawei] Univ Illinois, Dept Comp Sci, Champaign, IL 61820 USA; [Zhang, Chao] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA; [Kaplan, Lance] US Army, Res Lab, Washington, DC 20310 USA; [Zhuang, Honglei] Google Res, Beijing, Peoples R China	University of Illinois System; University of Illinois Urbana-Champaign; University System of Georgia; Georgia Institute of Technology; United States Department of Defense; US Army Research, Development & Engineering Command (RDECOM); US Army Research Laboratory (ARL); Google Incorporated	Meng, Y (corresponding author), Univ Illinois, Dept Comp Sci, Champaign, IL 61820 USA.	yumeng5@illinois.edu; jiaxinh3@illinois.edu; gwang10@illinois.edu; chaozhang@gatech.edu; hzhuang3@illinois.edu; lance.m.kaplan.civ@mail.mil; hanj@illinois.edu	Zhang, Chao/AAR-7251-2020; Meng, Yu/ABH-2615-2020	Zhang, Chao/0000-0003-3009-598X; 	U.S. Army Research Lab [W911NF-09-2-0053]; DARPA [W911NF-17-C-0099, FA875019-2-1004]; National Science Foundation [IIS 16-18481, IIS 17-04532, IIS 17-41317]; NIGMS through trans-NIH Big Data to Knowledge (BD2K) initiative [1U54GM114838]; DTRA [HDTRA11810026]	U.S. Army Research Lab(United States Department of DefenseUS Army Research Laboratory (ARL)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); National Science Foundation(National Science Foundation (NSF)); NIGMS through trans-NIH Big Data to Knowledge (BD2K) initiative; DTRA(United States Department of DefenseDefense Threat Reduction Agency)	Research was sponsored in part by U.S. Army Research Lab. under Cooperative Agreement No. W911NF-09-2-0053 (NSCTA), DARPA under Agreements No. W911NF-17-C-0099 and FA875019-2-1004, National Science Foundation IIS 16-18481, IIS 17-04532, and IIS 17-41317, DTRA HDTRA11810026, and grant 1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov).Any opinions, findings, and conclusions or recommendations expressed in this document are those of the author(s) and should not be interpreted as the views of any U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon. We thank anonymous reviewers for valuable and insightful feedback.	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Arora S., 2017, INT C LEARNING REPRE; Banerjee A, 2005, J MACH LEARN RES, V6, P1345; Batmanghelich Kayhan, 2016, ACL; Bojanowski Piotr., 2017, TACL, V5, P135, DOI [10.1162/tacl_a_00051, DOI 10.1162/TACL_A_00051]; Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619; Bruni E, 2014, J ARTIF INTELL RES, V49, P1, DOI 10.1613/jair.4135; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Dhingra B., 2018, TEXTGRAPHS NAACL HLT; Ferreira O. P., 2014, CONCEPTS TECHNIQUES; Finkelstein Lev, 2001, P 10 INT C WORLD WID, P406, DOI DOI 10.1145/371920.372094; Fisk D. L., 1965, T AM MATH SOC; Ganea OE, 2018, PR MACH LEARN RES, V80; Gopal S, 2014, PR MACH LEARN RES, V32; Hill F, 2015, COMPUT LINGUIST, V41, P665, DOI 10.1162/COLI_a_00237; Kim Y., 2014, P 2014 C EMPIRICAL M, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]; Kiros R., 2015, ADV NEURAL INFORM PR, V28, P3294; Kumar Sachin, 2019, ICLR; Lample G., 2016, C N AM CHAPTER ASS C, P260, DOI [10.18653/v1/N16-1030, 10.18653/v1/n16-1030, DOI 10.18653/V1/N16-1030]; Levy Omer, 2014, CONLL; Liu Weiyang, 2017, NIPS; Manning C.D., 2008, INTRO INFORM RETRIEV; Mardia K.V., 2000, DIRECTIONAL STAT, P15, DOI [10.1002/9780470316979, DOI 10.1002/9780470316979]; Meng Y, 2019, AAAI CONF ARTIF INTE, P6826; Meng Yu, 2018, CIKM; Mikolov T., 2013, ARXIV; Nickel M, 2018, PR MACH LEARN RES, V80; Nickel M, 2017, ADV NEUR IN, V30; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Quoc Le, 2014, P 31 INT C MACHINE L, V32, P1188; Smith S. T., 2014, ABS14075965 CORR; Sra S., 2007, THESIS; Steinley D, 2004, PSYCHOL METHODS, V9, P386, DOI 10.1037/1082-989X.9.3.386; Tang J, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1165, DOI 10.1145/2783258.2783307; Tao F., 2018, ICDM; Tifrea Alexandru, 2019, ICLR; Vendrov I., 2016, P INT C LEARN REPR; Vilnis Luke, 2015, INT C LEARN REPR; Wieting J., 2016, ICLR; Zhang C., 2017, KDD; Zhuang H., 2017, EMNLP	43	27	27	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308025
C	Wu, JL; Mooney, RJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wu, Jialin; Mooney, Raymond J.			Self-Critical Reasoning for Robust Visual Question Answering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Visual Question Answering (VQA) deep-learning systems tend to capture superficial statistical correlations in the training data because of strong language priors and fail to generalize to test data with a significantly different question-answer (QA) distribution [1]. To address this issue, we introduce a self-critical training objective that ensures that visual explanations of correct answers match the most influential image regions more than other competitive answer candidates. The influential regions are either determined from human visual/textual explanations or automatically from just significant words in the question and answer. We evaluate our approach on the VQA generalization task using the VQA-CP dataset, achieving a new state-of-the-art i.e.; 49.5% using textual explanations and 48.5% using automatically annotated regions.	[Wu, Jialin; Mooney, Raymond J.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Wu, JL (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	jialinwu@utexas.edu; mooney@cs.utexas.edu			DARPA XAI program under a grant from AFRL	DARPA XAI program under a grant from AFRL	This research was supported by the DARPA XAI program under a grant from AFRL.	Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522; Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285; Cho K., 2014, P 2014 C EMP METH NA, P1724; Das Abhishek, 2017, COMPUTER VISION IMAG; Fukui Akira, 2016, ARXIV160601847; Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Honnibal M., 2017, APPEAR; Hu Ronghang, 2018, ECCV; Jain Sarthak, 2019, ARXIV190210186, DOI [10.18653/v1/N19-1357, DOI 10.18653/V1/N19-1357]; Jiang Yu, 2018, ARXIV180709956; Kim JH, 2018, ADV NEUR IN, V31; Kingma D.P, P 3 INT C LEARNING R; Krishna R., IJCV; Park Dong Huk, 2018, P IEEE C COMP VIS PA; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Qiao Tingting, 2018, AAAI; Ramakrishnan S., 2018, NEURIPS; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ross A. S., 2017, ARXIV PREPRINT ARXIV; Selvaraju R. R., 2019, P INT C COMP VIS ICC; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shah M., 2019, IEEE C COMP VIS PATT; Teney D., 2017, ARXIV170802711; Trott A., 2018, INTERPRETABLE COUNTI; Wu J., 2018, ARXIV171201026V4; Wu JL, 2019, BLACKBOXNLP WORKSHOP ON ANALYZING AND INTERPRETING NEURAL NETWORKS FOR NLP AT ACL 2019, P103; Wu JL, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3585; Xie Ning, 2019, ABS190106706 ARXIV, P1; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Zhang J., IJCV; Zhang Y., 2019, WACV	35	27	27	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900022
C	Zheng, HL; Fu, JL; Zha, ZJ; Luo, JB		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zheng, Heliang; Fu, Jianlong; Zha, Zheng-Jun; Luo, Jiebo			Learning Deep Bilinear Transformation for Fine-grained Image Representation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Bilinear feature transformation has shown the state-of-the-art performance in learning fine-grained image representations. However, the computational cost to learn pairwise interactions between deep feature channels is prohibitively expensive, which restricts this powerful transformation to be used in deep neural networks. In this paper, we propose a deep bilinear transformation (DBT) block, which can be deeply stacked in convolutional neural networks to learn fine-grained image representations. The DBT block can uniformly divide input channels into several semantic groups. As bilinear transformation can be represented by calculating pairwise interactions within each group, the computational cost can be heavily relieved. The output of each block is further obtained by aggregating intra-group bilinear features, with residuals from the entire input features. We found that the proposed network achieves new state-of-the-art in several fine-grained image recognition benchmarks, including CUB-Bird, Stanford-Car, and FGVC-Aircraft.	[Zheng, Heliang; Zha, Zheng-Jun] Univ Sci & Technol China, Hefei, Peoples R China; [Zheng, Heliang; Fu, Jianlong] Microsoft Res, Beijing, Peoples R China; [Luo, Jiebo] Univ Rochester, Rochester, NY USA	Chinese Academy of Sciences; University of Science & Technology of China, CAS; Microsoft; University of Rochester	Zheng, HL (corresponding author), Univ Sci & Technol China, Hefei, Peoples R China.	zhenghl@mail.ustc.edu.cn; jianf@microsoft.com; zhazj@ustc.edu.cn; jluo@cs.rochester.edu	Zha, Zheng-Jun/AAF-8667-2020; Zha, Zheng-Jun/AAE-8408-2020	Zha, Zheng-Jun/0000-0003-2510-8993; Luo, Jiebo/0000-0002-4516-9729	National Key R&D Program of China [2017YFB 1300201]; National Natural Science Foundation of China (NSFC) [61622211, 61620106009]; Fundamental Research Funds for the Central Universities [WK2100100030]	National Key R&D Program of China; National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	This work was supported by the National Key R&D Program of China under Grant 2017YFB 1300201, the National Natural Science Foundation of China (NSFC) under Grants 61622211 and 61620106009 as well as the Fundamental Research Funds for the Central Universities under Grant WK2100100030.	[Anonymous], 2015, ARXIV151201274; Berg T, 2014, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2014.259; Branson S., 2014, BMVC; Cui  Y., 2017, IEEE INT C COMP VIS, V1, P7; Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476; Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41; Gao ZL., 2019, P IEEE C COMP VIS PA, P3024; Howard A. G., 2017, MOBILENETS EFFICIENT; Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kim J, 2017, ICLR; Kim JH, 2018, ADV NEUR IN, V31; Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743; Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194; Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lam M., 2017, CVPR; Li P., 2018, ICPR; Li PH., 2017, ICCV, P2070; Li YH, 2017, IEEE I CONF COMP VIS, P2098, DOI 10.1109/ICCV.2017.229; Lin M., 2014, ICLR; Lin T.Y., 2017, BRIT MACH VIS C; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Maji S., 2013, TECHNICAL REPORT, P6; R Welinder, 2010, CNSTR2010001; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136; Van Horn G., 2018, INATURALIST SPECIES; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002; Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128; Zhao Bo, 2016, ABS160608572 CORR; Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557	37	27	27	0	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304030
C	Haber, N; Mrowca, D; Wang, S; Li, FF; Yamins, DLK		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Haber, Nick; Mrowca, Damian; Wang, Stephanie; Li Fei-Fei; Yamins, Daniel L. K.			Learning to Play With Intrinsically-Motivated, Self-Aware Agents	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				INFANTS; ATTENTION; EXPERIENCE; CURIOSITY	Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to mathematically formalize these abilities using a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulated environment in which an agent can move and interact with objects it sees, we propose a "world-model" network that learns to predict the dynamic consequences of the agent's actions. Simultaneously, we train a separate explicit "self-model" that allows the agent to track the error map of its world-model. It then uses the self-model to adversarially challenge the developing world-model. We demonstrate that this policy causes the agent to explore novel and informative interactions with its environment, leading to the generation of a spectrum of complex behaviors, including ego-motion prediction, object attention, and object gathering. Moreover, the world-model that the agent learns supports improved performance on object dynamics prediction, detection, localization and recognition tasks. Taken together, our results are initial steps toward creating flexible autonomous agents that self-supervise in realistic physical environments.	[Haber, Nick; Yamins, Daniel L. K.] Dept Psychol, Stanford, CA 94305 USA; [Haber, Nick] Dept Pediat, Stanford, CA 94305 USA; [Haber, Nick] Dept Biomed Data Sci, Stanford, CA 94305 USA; [Mrowca, Damian; Wang, Stephanie; Li Fei-Fei; Yamins, Daniel L. K.] Dept Comp Sci, Stanford, CA 94305 USA; [Yamins, Daniel L. K.] Wu Tsai Neurosci Inst, Stanford, CA 94305 USA		Haber, N (corresponding author), Dept Psychol, Stanford, CA 94305 USA.; Haber, N (corresponding author), Dept Pediat, Stanford, CA 94305 USA.; Haber, N (corresponding author), Dept Biomed Data Sci, Stanford, CA 94305 USA.	nhaber@stanford.edu; mrowca@stanford.edu			James S. McDonnell Foundation; Simons Foundation; Sloan Foundation; Berry Foundation postdoctoral fellowship; Stanford Department of Biomedical Data Science [NLM T-15 LM007033-35]; ONR - MURI (Stanford Lead) [N00014-16-1-2127]; ONR - MURI (UCLA Lead) [1015 G TA275]	James S. McDonnell Foundation; Simons Foundation; Sloan Foundation(Alfred P. Sloan Foundation); Berry Foundation postdoctoral fellowship; Stanford Department of Biomedical Data Science; ONR - MURI (Stanford Lead); ONR - MURI (UCLA Lead)	This work was supported by grants from the James S. McDonnell Foundation, Simons Foundation, and Sloan Foundation (DLKY), a Berry Foundation postdoctoral fellowship and Stanford Department of Biomedical Data Science NLM T-15 LM007033-35 (NH), ONR - MURI (Stanford Lead) N00014-16-1-2127 and ONR - MURI (UCLA Lead) 1015 G TA275 (LF).	Achiam J., 2017, ARXIV170301732; Agrawal P., 2016, ADV NEURAL INFORM PR, P5074; Baranes A, 2013, ROBOT AUTON SYST, V61, P49, DOI 10.1016/j.robot.2012.05.008; Begus K, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108817; Boeing A, 2007, GRAPHITE 2007: 5TH INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES IN AUSTRALASIA AND SOUTHERN ASIA, PROCEEDINGS, P281; Chentanez, 2005, ADV NEURAL INFORM PR, P1281, DOI DOI 10.21236/ADA440280; Ebert Frederik, 2017, ARXIV171005268; Elhamifar E, 2013, IEEE I CONF COMP VIS, P209, DOI 10.1109/ICCV.2013.33; FANTZ RL, 1964, SCIENCE, V146, P668, DOI 10.1126/science.146.3644.668; Finn Chelsea, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2786, DOI 10.1109/ICRA.2017.7989324; Frank Mikhail, 2014, Front Neurorobot, V7, P25, DOI 10.3389/fnbot.2013.00025; Gidaris N. K. Spyros, 2018, ICLR; Gilad-Bachrach R, 2005, ADV NEURAL INFORM PR, P443; Gopnik A., 2009, SCI CRIB MINDS BRAIN; Gottlieb J, 2013, TRENDS COGN SCI, V17, P585, DOI 10.1016/j.tics.2013.09.001; Goupil L, 2016, P NATL ACAD SCI USA, V113, P3492, DOI 10.1073/pnas.1515129113; Held D., 2017, CORR; Ho Jonathan, 2016, ADV NEURAL INFORM PR; HONG S, 2017, CVPR, P2224, DOI DOI 10.1109/CVPR.2017.239; Houthooft R., 2016, ADV NEURAL INFORM PR, P1109; Hurley KB, 2015, J COGN DEV, V16, P11, DOI 10.1080/15248372.2013.833922; Hurley KB, 2010, INFANT BEHAV DEV, V33, P619, DOI 10.1016/j.infbeh.2010.07.015; Kalchbrenner N, 2017, PR MACH LEARN RES, V70; Kidd C, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0036399; Kingma D.P, P 3 INT C LEARNING R; Kulkarni T. D., 2016, PROC 30 INT C NEURAL, P3675; Lin L, 1992, THESIS; Machado MC, 2017, PR MACH LEARN RES, V70; Mitash C, 2017, IEEE INT C INT ROBOT, P545; Mnih V, 2016, PR MACH LEARN RES, V48; Mrowca Damian, 2018, ARXIV180608047; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Oudeyer PY, 2007, IEEE T EVOLUT COMPUT, V11, P265, DOI 10.1109/TEVC.2006.890271; Oudeyer PY, 2016, TOP COGN SCI, V8, P492, DOI 10.1111/tops.12196; Pathak D, 2017, P 34 INT C MACH LEAR, P2778, DOI DOI 10.5555/3305890.3305968; Popov I., 2017, DATA EFFICIENT DEEP; Saxe R, 2003, NEUROIMAGE, V19, P1835, DOI 10.1016/S1053-8119(03)00230-1; Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368; Sener O., 2017, ARXIV PREPRINT ARXIV, P1; Settles B., 2011, ACTIVE LEARNING, V18; Singh S, 2010, IEEE T AUTON MENT DE, V2, P70, DOI 10.1109/TAMD.2010.2051031; Sokolov E.N., 1963, PERCEPTION CONDITION; Twomey KE, 2017, DEV SCI, V21, P1; Wang KZ, 2017, IEEE T CIRC SYST VID, V27, P2591, DOI 10.1109/TCSVT.2016.2589879; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40	46	27	27	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002089
C	Palm, RB; Paquet, U; Winther, O		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Palm, Rasmus Berg; Paquet, Ulrich; Winther, Ole			Recurrent Relational Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					This paper is concerned with learning to solve tasks that require a chain of interdependent steps of relational inference, like answering complex questions about the relationships between objects, or solving puzzles where the smaller elements of a solution mutually constrain each other. We introduce the recurrent relational network, a general purpose module that operates on a graph representation of objects. As a generalization of Santoro et al. [2017]'s relational network, it can augment any neural network model with the capacity to do many-step relational reasoning. We achieve state of the art results on the bAbI textual question-answering dataset with the recurrent relational network, consistently solving 20/20 tasks. As bAbI is not particularly challenging from a relational reasoning point of view, we introduce Pretty-CLEVR, a new diagnostic dataset for relational reasoning. In the Pretty-CLEVR set-up, we can vary the question to control for the number of relational reasoning steps that are required to obtain the answer. Using Pretty-CLEVR, we probe the limitations of multi-layer perceptrons, relational and recurrent relational networks. Finally, we show how recurrent relational networks can learn to solve Sudoku puzzles from supervised training data, a challenging task requiring upwards of 64 steps of relational reasoning. We achieve state-of-the-art results amongst comparable methods by solving 96.6% of the hardest Sudoku puzzles.	[Palm, Rasmus Berg; Winther, Ole] Tech Univ Denmark, Lyngby, Denmark; [Palm, Rasmus Berg] Tradeshift, San Francisco, CA 94105 USA; [Paquet, Ulrich] DeepMind, London, England	Technical University of Denmark	Palm, RB (corresponding author), Tech Univ Denmark, Lyngby, Denmark.; Palm, RB (corresponding author), Tradeshift, San Francisco, CA 94105 USA.	rapal@dtu.dk; upaq@google.com; olwi@dtu.dk		Winther, Ole/0000-0002-1966-3205	NVIDIA Corporation	NVIDIA Corporation	We'd like to thank the anonymous reviewers for the valuable comments and suggestions, especially reviewer 2 who suggested the age arithmetic task. This research was supported by the NVIDIA Corporation with the donation of TITAN X GPUs.	Amos B, 2017, PR MACH LEARN RES, V70; Battaglia Peter W, 2016, ARXIV161200222; Bauke H, 2008, COMPUT SCI ENG, V10, P32, DOI 10.1109/MCSE.2008.60; Besold T. R., 2021, ARXIV171103902, P1; de Raedt L., 2016, SYNTHESIS LECT ARTIF, DOI DOI 10.2200/S00692ED1V01Y201601AIM032; Deng ZW, 2016, PROC CVPR IEEE, P4772, DOI 10.1109/CVPR.2016.516; Gilmer J, 2017, PR MACH LEARN RES, V70; HEESS N, 2013, ADV NEURAL INFORM PR, V26, P3219; Henaff Mikael, 2016, ARXIV161203969; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Khan Sheehan, 2014, SOLVING SUDOKU USING; Knuth Donald E, 2000, CS0011047 ARXIV; Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837; Lin G., 2015, ADV NEURAL INFORM PR, P361; McCulloch W., 1943, B MATH BIOPHYS, V5, P115, DOI [10.1007/BF02478259, DOI 10.1007/BF02478259]; Mikolov T., 2015, ARXIV150205698, V1502, P05698; Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467; Norvig Peter, 2006, SOLVING EVERY SUDOKU; Park Kyubyong, 2016, CAN NEURAL NETWORKS; Rae JW, 2016, ADV NEUR IN, V29; Ross S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2737, DOI 10.1109/CVPR.2011.5995724; Santoro A, 2017, ADV NEUR IN, V30; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Serafini L, 2016, LECT NOTES COMPUT SC, V10037, P334, DOI 10.1007/978-3-319-49130-1_25; Sourek G., 2015, P 2015 INT C COGN CO, V1583, P52; Spelke ES, 2007, DEVELOPMENTAL SCI, V10, P89, DOI 10.1111/j.1467-7687.2007.00569.x; Spelke Elizabeth S, 1995, DEV OBJECT PERCEPTIO; Sukhbaatar S., 2016, ADV NEURAL INF PROCE, V29, P2244, DOI DOI 10.5555/3157096.3157348; Sukhbaatar S, 2015, ADV NEUR IN, V28; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Yang Hyochang, 2018, ARXIV180108459	31	27	27	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303037
C	Wang, SS; Roosta-Khorasani, F; Xu, P; Mahoney, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Shusen; Roosta-Khorasani, Farbod; Xu, Peng; Mahoney, MichaelW.			GIANT: Globally Improved Approximate Newton Method for Distributed Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PARALLEL	For distributed computing environment, we consider the empirical risk minimization problem and propose a distributed and communication-efficient Newton-type optimization method. At every iteration, each worker locally finds an Approximate NewTon (ANT) direction, which is sent to the main driver. The main driver, then, averages all the ANT directions received from workers to form a Globally Improved ANT (GIANT) direction. GIANT is highly communication efficient and naturally exploits the trade-offs between local computations and global communications in that more local computations result in fewer overall rounds of communications. Theoretically, we show that GIANT enjoys an improved convergence rate as compared with first-order methods and existing distributed Newton-type methods. Further, and in sharp contrast with many existing distributed Newton-type methods, as well as popular first-order methods, a highly advantageous practical feature of GIANT is that it only involves one tuning parameter. We conduct large-scale experiments on a computer cluster and, empirically, demonstrate the superior performance of GIANT.	[Wang, Shusen] Stevens Inst Technol, Hoboken, NJ 07030 USA; [Roosta-Khorasani, Farbod] Univ Queensland, Brisbane, Qld, Australia; [Xu, Peng] Stanford Univ, Stanford, CA 94305 USA; [Mahoney, MichaelW.] Univ Calif Berkeley, Berkeley, CA 94720 USA	Stevens Institute of Technology; University of Queensland; Stanford University; University of California System; University of California Berkeley	Wang, SS (corresponding author), Stevens Inst Technol, Hoboken, NJ 07030 USA.	shusen.wang@stevens.edu; fred.roosta@uq.edu.au; pengxu@stanford.edu; mmahoney@stat.berkeley.edu			ARO; DARPA; NSF; Australian Research Council through a Discovery Early Career Researcher Award [DE180100923]; Cray	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); Australian Research Council through a Discovery Early Career Researcher Award(Australian Research Council); Cray	We thank Kimon Fountoulakis, Alex Gittens, Jey Kottalam, Zirui Liu, Hao Ren, Sathiya Selvaraj, Zebang Shen, and Haishan Ye for their helpful suggestions. The four authors would like to acknowledge ARO, DARPA, Cray, and NSF for providing partial support of this work. Farbod Roosta-Khorasani was partially supported by the Australian Research Council through a Discovery Early Career Researcher Award (DE180100923).	[Anonymous], 2015, ARXIV150707595; Bonawitz K, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1175, DOI 10.1145/3133956.3133982; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137; Fercoq O, 2016, SIAM REV, V58, P739, DOI 10.1137/16M1085905; Gittens A, 2016, J MACH LEARN RES, V17; Jaggi M., 2014, ADV NEURAL INF PROCE, V4, P3068; Konecn J., 2016, ARXIV161005492; Konecn Jakub, 2016, ARXIV161002527; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Liu J., 2015, J MACHINE LEARNING R, V16, P1; Low Yucheng, 2012, P VLDB ENDOWMENT; Ma Chenxin, 2015, INT C MACH LEARN ICM; Mahajan Dhruv, 2013, ARXIV13108418; Mahajan Dhruv, 2013, ARXIV13110636; McMahan Brendan, 2017, INT C ART INT STAT A; Meng X., 2016, J MACH LEARN RES, V17, P1235, DOI DOI 10.1145/2882903.2912565; Mu Li, 2014, USENIX S OP SYST DES; Necoara I, 2016, SIAM J OPTIMIZ, V26, P197, DOI 10.1137/130950288; Nesterov Y., 2018, APPL OPTIMIZATION; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Pilanci M, 2017, SIAM J OPTIMIZ, V27, P205, DOI 10.1137/15M1021106; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Recht B, 2011, ADV NEURAL INFORM PR, V2011, P693; Reddi S.J., 2016, FAST; Reddi Sashank J, 2015, ADV NEURAL INFORM PR; Richtarik P, 2016, J MACH LEARN RES, V17; Richtarik P, 2016, MATH PROGRAM, V156, P433, DOI 10.1007/s10107-015-0901-6; Roosta-Khorasani, 2016, ARXIV PREPRINT ARXIV; Roosta-Khorasani Farbod, 2016, ARXIV160104737; Shalev-Shwartz S, 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shamir Ohad, 2014, ANN ALL C COMM CONTR; Shamir Ohad, 2014, INT C MACH LEARN ICM; Shun Zheng, 2016, ARXIV160403763; Smith V., 2016, J MACH LEARN RES, V18, P1; Smith V, 2017, ADV NEUR IN, V30; Wang SS, 2016, J MACH LEARN RES, V17; Wang Shusen, 2017, INT C MACH LEARN ICM; Wang Shusen, 2018, ARXIV170903528; Xu Peng, 2016, ADV NEURAL INFORM PR; Yang T., 2013, ADV NEURAL INFORM PR, P629; Zaharia M, 2010, HOTCLOUD, P10, DOI DOI 10.HTTP://DL.ACM.0RG/CITATI0N.CFM?; Zhang Yuchen, 2015, INT C MACH LEARN ICM; Zinkevich M., 2010, P ADV NEUR INF PROC, P2595	47	27	28	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302035
C	Yan, Z; Guo, YW; Zhang, CS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yan, Ziang; Guo, Yiwen; Zhang, Changshui			Deep Defense: Training DNNs with Improved Adversarial Robustness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Despite the efficacy on a variety of computer vision tasks, deep neural networks (DNNs) are vulnerable to adversarial attacks, limiting their applications in security-critical systems. Recent works have shown the possibility of generating imperceptibly perturbed image inputs (a.k.a., adversarial examples) to fool well-trained DNN classifiers into making arbitrary predictions. To address this problem, we propose a training recipe named "deep defense". Our core idea is to integrate an adversarial perturbation-based regularizer into the classification objective, such that the obtained models learn to resist potential attacks, directly and precisely. The whole optimization problem is solved just like training a recursive network. Experimental results demonstrate that our method outperforms training with adversarial/Parseval regularizations by large margins on various datasets (including MNIST, CIFAR-10 and ImageNet) and different DNN architectures. Code and models for reproducing our results are available at https://github.com/ZiangYan/deepdefense.pytorch.	[Yan, Ziang; Guo, Yiwen; Zhang, Changshui] Tsinghua Univ, Inst Artificial Intelligence,Tsinghua Univ THUAI, Beijing Natl Res Ctr Informat Sci & Technol BNRis, State Key Lab Intelligent Technol & Syst,Dept Aut, Beijing, Peoples R China; [Guo, Yiwen] Intel Labs China, Beijing, Peoples R China	Tsinghua University; Intel Corporation	Yan, Z (corresponding author), Tsinghua Univ, Inst Artificial Intelligence,Tsinghua Univ THUAI, Beijing Natl Res Ctr Informat Sci & Technol BNRis, State Key Lab Intelligent Technol & Syst,Dept Aut, Beijing, Peoples R China.	yza18@mails.tsinghua.edu.cn; yiwen.guo@intel.com; zcs@mail.tsinghua.edu.cn			NSFC [61876095, 61751308, 61473167]; Beijing Natural Science Foundation [L172037]	NSFC(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation)	This work is supported by NSFC (Grant No. 61876095, No. 61751308 and No. 61473167) and Beijing Natural Science Foundation (Grant No. L172037).	Alemi Alex, 2017, ICLR; Buckman Jacob, 2018, INT C LEARN REPR, DOI DOI 10.1109/TCYB.2016.2523000; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cisse M, 2017, PR MACH LEARN RES, V70; Dhillon G.S., 2018, ARXIV180301442; Fawzi Alhussein, 2016, ADV NEURAL INFORM PR; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Gu Shixiang, 2015, ICLR WORKSH; Guo YW, 2016, ADV NEUR IN, V29; Han Song, 2015, ARXIV PREPRINT ARXIV, P1135; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hein M, 2017, NIPS 17; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kurakin A, 2018, ICLR, P99, DOI DOI 10.1201/9781351251389-8; LeCun Y, 1999, LECT NOTES COMPUT SC, V1681, P319, DOI 10.1007/3-540-46805-6_19; Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8; Lu JJ, 2017, IEEE I CONF COMP VIS, P446, DOI 10.1109/ICCV.2017.56; Madry A., 2018, P ICLR VANC BC CAN; Metzen J. H., 2017, 5 INT C LEARNING REP, DOI DOI 10.1109/ICCV.2017.300; Miyato T., 2017, ABS170403976 CORR; Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Papernot Nicolas, 2018, IEEE EUR S SEC PRIV; Ros AS, 2018, AAAI CONF ARTIF INTE, P1660; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sokolic Jure, 2017, IEEE T SIGNAL PROCES; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Xie CY, 2018, IEEE PHOTONICS J, V10, DOI 10.1109/JPHOT.2018.2809731; Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153; Xu H, 2012, MACH LEARN, V86, P391, DOI 10.1007/s10994-011-5268-1; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53	38	27	27	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300039
C	Berkenkamp, F; Turchetta, M; Schoellig, AP; Krause, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Berkenkamp, Felix; Turchetta, Matteo; Schoellig, Angela P.; Krause, Andreas			Safe Model-based Reinforcement Learning with Stability Guarantees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DISCRETE-TIME	Reinforcement learning is a powerful paradigm for learning optimal policies from experimental data. However, to find optimal policies, most reinforcement learning algorithms explore all possible actions, which may be harmful for real-world systems. As a consequence, learning algorithms are rarely applied on safety-critical systems in the real world. In this paper, we present a learning algorithm that explicitly considers safety, defined in terms of stability guarantees. Specifically, we extend control-theoretic results on Lyapunov stability verification and show how to use statistical models of the dynamics to obtain high-performance control policies with provable stability certificates. Moreover, under additional regularity assumptions in terms of a Gaussian process prior, we prove that one can effectively and safely collect data in order to learn about the dynamics and thus both improve control performance and expand the safe region of the state space. In our experiments, we show how the resulting algorithm can safely optimize a neural network policy on a simulated inverted pendulum, without the pendulum ever falling down.	[Berkenkamp, Felix; Turchetta, Matteo; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland; [Schoellig, Angela P.] Univ Toronto, Inst Aerosp Studies, Toronto, ON, Canada	Swiss Federal Institutes of Technology Domain; ETH Zurich; University of Toronto	Berkenkamp, F (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	befelix@inf.ethz.ch; matteotu@inf.ethz.ch; schoellig@utias.utoronto.ca; krausea@ethz.ch	Jeong, Yongwook/N-7413-2016		SNSF [200020_159557]; Max Planck ETH Center for Learning Systems; NSERC [RGPIN-2014-04634]; Ontario Early Researcher Award	SNSF(Swiss National Science Foundation (SNSF)); Max Planck ETH Center for Learning Systems; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Ontario Early Researcher Award(Ministry of Research and Innovation, Ontario)	This research was supported by SNSF grant 200020_159557, the Max Planck ETH Center for Learning Systems, NSERC grant RGPIN-2014-04634, and the Ontario Early Researcher Award.	Achiam Joshua, 2017, P INT C MACH LEARN I; Akametalu AK, 2014, IEEE DECIS CONTR P, P1424, DOI 10.1109/CDC.2014.7039601; [Anonymous], 2016, ARXIV160304467CS; Aswani A, 2013, AUTOMATICA, V49, P1216, DOI 10.1016/j.automatica.2013.02.003; Berkenkamp F, 2016, IEEE DECIS CONTR P, P4661, DOI 10.1109/CDC.2016.7798979; Berkenkamp F, 2016, IEEE INT CONF ROBOT, P491, DOI 10.1109/ICRA.2016.7487170; Bobiti R, 2016, 2016 EUROPEAN CONTROL CONFERENCE (ECC), P561, DOI 10.1109/ECC.2016.7810344; Chowdhury SR, 2017, PR MACH LEARN RES, V70; Christmann A., 2008, SUPPORT VECTOR MACHI; Davies Scott, 1996, NIPS, P1005; Garcia J, 2015, J MACH LEARN RES, V16, P1437; Geibel P, 2005, J ARTIF INTELL RES, V24, P81, DOI 10.1613/jair.1666; Giesl P, 2015, DISCRETE CONT DYN-B, V20, P2291, DOI 10.3934/dcdsb.2015.20.2291; Hans A., 2008, ESANN, P143; Kapoor A., 2016, P ROB SCI SYST; Khalil H. K., 1996, NONLINEAR SYSTEM, V3; Li HJ, 2016, J MATH ANAL APPL, V438, P701, DOI 10.1016/j.jmaa.2016.01.045; Mane D, 2016, ARXIV160606565CS; Matthews AGD, 2017, J MACH LEARN RES, V18, P1; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mockus Jonas, 1989, MATH ITS APPL, V37; Moldovan T.M., 2012, P 29 INT C MACH LEAR, P1451, DOI DOI 10.5555/3042573.3042759; Ostafew CJ, 2016, INT J ROBOT RES, V35, P1547, DOI 10.1177/0278364916645661; Pecka M, 2014, LECT NOTES COMPUT SC, V8906, P357; Perkins TJ, 2003, J MACH LEARN RES, V3, P803, DOI 10.1162/jmlr.2003.3.4-5.803; Peters J, 2017, ADAPT COMPUT MACH LE; Peters J, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P2219, DOI 10.1109/IROS.2006.282564; Powell WB, 2007, APPROXIMATE DYNAMIC PROGRAMMING: SOLVING THE CURSES OF DIMENSIONALITY, P1, DOI 10.1002/9780470182963; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Schreiter J, 2015, LECT NOTES ARTIF INT, V9286, P133, DOI 10.1007/978-3-319-23461-8_9; Srinivas N, 2012, IEEE T INFORM THEORY, V58, P3250, DOI 10.1109/TIT.2011.2182033; Sui YA, 2015, PR MACH LEARN RES, V37, P997; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Szegedy C, 2014, INT C LEARN REPR ICL; Tamar Aviv, 2014, P INT C MACH LEARN I; Turchetta Matteo, 2016, SAFE EXPLORATION FIN, P4305; Vinogradska J, 2016, PR MACH LEARN RES, V48; Wiesemann W, 2013, MATH OPER RES, V38, P153, DOI 10.1287/moor.1120.0566	40	27	27	3	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400087
C	Makhzani, A; Frey, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Makhzani, Alireza; Frey, Brendan			PixelGAN Autoencoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this paper, we describe the "PixelGAN autoencoder", a generative autoencoder in which the generative path is a convolutional autoregressive neural network on pixels (PixelCNN) that is conditioned on a latent code, and the recognition path uses a generative adversarial network (GAN) to impose a prior distribution on the latent code. We show that different priors result in different decompositions of information between the latent code and the autoregressive decoder. For example, by imposing a Gaussian distribution as the prior, we can achieve a global vs. local decomposition, or by imposing a categorical distribution as the prior, we can disentangle the style and content information of images in an unsupervised fashion. We further show how the PixelGAN autoencoder with a categorical prior can be directly used in semi-supervised settings and achieve competitive semi-supervised classification results on the MNIST, SVHN and NORB datasets.	[Makhzani, Alireza; Frey, Brendan] Univ Toronto, Toronto, ON, Canada	University of Toronto	Makhzani, A (corresponding author), Univ Toronto, Toronto, ON, Canada.	makhzani@psi.toronto.edu; frey@psi.toronto.edu	Jeong, Yongwook/N-7413-2016					Abadi M, 2015, P 12 USENIX S OPERAT; Barone Antonio Valerio Miceli, 2016, P 1 WORKSH REPR LEAR, P121; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Chen Xi, 2016, ARXIV161102731; Donahue J., 2016, ARXIV160509782; Dumoulin Vincent, 2016, ARXIV E PRINTS; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I., 2016, P INT C LEARN REPR; Hoffman Matthew D., 2016, NIPS 2016 WORKSH ADV; Huszar F., 2017, ARXIV170208235; Huszar Ferenc, IS MAXIMUM LIKELIHOO; Im D.J., 2015, ARXIV151106406; Kalchbrenner N, 2016, ARXIV161000527; Kim Taeksoo, 2017, P 34 INT C MACH LEAR, P1857, DOI [10.5555/3305381.3305573, DOI 10.5555/3305381.3305573]; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Laine Samuli, 2016, ARXIV161002242; Maaloe L, 2016, PR MACH LEARN RES, V48; Makhzani A., 2015, ARXIV151105644; Mescheder L, 2017, PR MACH LEARN RES, V70; Miyato Takeru, 2015, STAT, V1050, P25; Mohamed Shakir, 2016, ARXIV161003483; Ranganath R, 2016, ADV NEUR IN, V29; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Salimans T, 2016, ADV NEUR IN, V29; Salimans Tim, 2017, ARXIV170105517; Sonderby C. K., 2016, ARXIV PREPRINT ARXIV; Springenberg Jost Tobias, 2015, ARXIV151106390; Sutskever I., 2015, ARXIV PREPRINT ARXIV; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Theis Lucas, 2015, ARXIV151101844; Tran D, 2017, ADV NEUR IN, V30; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; Zhang Meng, ADVERSARIAL TRAINING; Zhu Jun-Yan, 2017, ICCV	39	27	28	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402003
C	You, S; Ding, D; Canini, K; Pfeifer, J; Gupta, MR		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		You, Seungil; Ding, David; Canini, Kevin; Pfeifer, Jan; Gupta, Maya R.			Deep Lattice Networks and Partial Monotonic Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose learning deep models that are monotonic with respect to a user-specified set of inputs by alternating layers of linear embeddings, ensembles of lattices, and calibrators (piecewise linear functions), with appropriate constraints for monotonicity, and jointly training the resulting network. We implement the layers and projections with new computational graph nodes in TensorFlow and use the Adam optimizer and batched stochastic gradients. Experiments on benchmark and real-world datasets show that six-layer monotonic deep lattice networks achieve state-of-the art performance for classification and regression with monotonicity guarantees.	[You, Seungil; Ding, David; Canini, Kevin; Pfeifer, Jan; Gupta, Maya R.] Google Res, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA	Google Incorporated	You, S (corresponding author), Google Res, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.	siyou@google.com; dwding@google.com; canini@google.com; janpf@google.com; mayagupta@google.com	Ding, David/GRY-0167-2022; Jeong, Yongwook/N-7413-2016					Abadi M, 2015, P 12 USENIX S OPERAT; ARCHER NP, 1993, DECISION SCI, V24, P60, DOI 10.1111/j.1540-5915.1993.tb00462.x; Armstrong W. W., 1996, HDB NEURAL COMPUTATI; AYER M, 1955, ANN MATH STAT, V26, P641, DOI 10.1214/aoms/1177728423; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Canini K, 2016, ADV NEUR IN, V29; Daniels H, 2010, IEEE T NEURAL NETWOR, V21, P906, DOI 10.1109/TNN.2010.2044803; Dugas C., 2009, J MACHINE LEARNING R; Garcia E. K., 2009, ADV NEURAL INFORM PR; Groeneboom P., 2014, NONPARAMETRIC ESTIMA; Gupta M, 2016, J MACH LEARN RES, V17; Howard A., 2007, ADV NEURAL INFORM PR; Kay H, 2000, AICHE J, V46, P2426, DOI 10.1002/aic.690461211; Kingma D.P, P 3 INT C LEARNING R; Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10; Minin A, 2010, NEURAL NETWORKS, V23, P471, DOI 10.1016/j.neunet.2009.09.002; Newman C. B. D., 1998, UCI REPOSITORY MACHI; Sill J., 1998, ADV NEURAL INFORM PR; Wang S., 1994, NEURAL COMPUT APPL, V2, P160	19	27	27	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403005
C	Ranganath, R; Altosaar, J; Tran, D; Blei, DM		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ranganath, Rajesh; Altosaar, Jaan; Tran, Dustin; Blei, David M.			Operator Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (opvi), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling-allowing inference to scale to massive data-as well as objectives that admit variational programs-a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of opvi on a mixture model and a generative model of images.	[Ranganath, Rajesh; Altosaar, Jaan] Princeton Univ, Princeton, NJ 08544 USA; [Tran, Dustin; Blei, David M.] Columbia Univ, New York, NY 10027 USA	Princeton University; Columbia University	Ranganath, R (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.		Altosaar, Jaan/AAN-6289-2020	Altosaar, Jaan/0000-0003-1294-4159	NSF [IIS-1247664]; ONR [N00014-11-1-0651]; DARPA [FA8750-14-2-0009, N66001-15-C-4032]; Adobe; NSERC PGS-D; Porter Ogden Jacobus Fellowship; Seibel Foundation; Sloan Foundation	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Adobe; NSERC PGS-D(Natural Sciences and Engineering Research Council of Canada (NSERC)); Porter Ogden Jacobus Fellowship; Seibel Foundation; Sloan Foundation(Alfred P. Sloan Foundation)	This work is supported by NSF IIS-1247664, ONR N00014-11-1-0651, DARPA FA8750-14-2-0009, DARPA N66001-15-C-4032, Adobe, NSERC PGS-D, Porter Ogden Jacobus Fellowship, Seibel Foundation, and the Sloan Foundation. The authors would like to thank Dawen Liang, Ben Poole, Stephan Mandt, Kevin Murphy, Christian Naesseth, and the anonymous reviews for their helpful feedback and comments.	Assaraf R, 1999, PHYS REV LETT, V83, P4682, DOI 10.1103/PhysRevLett.83.4682; Barbour, 1988, J APPL PROBAB A, V25, P175; Carpenter B., 2015, ARXIV PREPRINT ARXIV; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Ghahramani Z, 2001, ADV NEUR IN, V13, P507; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hernandez-Lobato J. M., 2015, ARXIV ORG; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Hyvarinen A, 2005, J MACH LEARN RES, V6, P695; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kushner H. J., 1997, STOCHASTIC APPROXIMA; Maaloe L, 2016, PR MACH LEARN RES, V48; Minka T., 2004, TECHNICAL REPORT; Minka T.P., 2001, P 17 C UNC ART INT, P362; Nielsen F, 2014, IEEE SIGNAL PROC LET, V21, P10, DOI 10.1109/LSP.2013.2288355; Ranganath R., 2014, ARTIFICIAL INTELLIGE, P814; Ranganath R, 2016, PR MACH LEARN RES, V48; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Theis Lucas, 2016, ICLR; Titsias MK, 2014, PR MACH LEARN RES, V32, P1971; Wingate D., 2013, ARXIV13011299	27	27	27	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700055
C	Flach, PA; Kull, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Flach, Peter A.; Kull, Meelis			Precision-Recall-Gain Curves: PR Analysis Done Right	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				CLASSIFICATION	Precision-Recall analysis abounds in applications of binary classification where true negatives do not add value and hence should not affect assessment of the classifier's performance. Perhaps inspired by the many advantages of receiver operating characteristic (ROC) curves and the area under such curves for accuracy-based performance assessment, many researchers have taken to report PrecisionRecall (PR) curves and associated areas as performance metric. We demonstrate in this paper that this practice is fraught with difficulties, mainly because of incoherent scale assumptions - e.g., the area under a PR curve takes the arithmetic mean of precision values whereas the F-beta score applies the harmonic mean. We show how to fix this by plotting PR curves in a different coordinate system, and demonstrate that the new Precision-Recall-Gain curves inherit all key advantages of ROC curves. In particular, the area under Precision-Recall-Gain curves conveys an expected F-1 score on a harmonic scale, and the convex hull of a Precision-Recall-Gain curve allows us to calibrate the classifier's scores so as to determine, for each operating point on the convex hull, the interval of b values for which the point optimises F-beta. We demonstrate experimentally that the area under traditional PR curves can easily favour models with lower expected F-1 score than others, and so the use of Precision-Recall-Gain curves will result in better model selection.	[Flach, Peter A.; Kull, Meelis] Univ Bristol, Intelligent Syst Lab, Bristol, Avon, England	University of Bristol	Flach, PA (corresponding author), Univ Bristol, Intelligent Syst Lab, Bristol, Avon, England.	Peter.Flach@bristol.ac.uk; Meelis.Kull@bristol.ac.uk			REFRAME project - European Coordinated Research on Long-Term Challenges in Information and Communication Sciences & Technologies ERA-Net (CHIST-ERA); Engineering and Physical Sciences Research Council in the UK [EP/K018728/1]	REFRAME project - European Coordinated Research on Long-Term Challenges in Information and Communication Sciences & Technologies ERA-Net (CHIST-ERA); Engineering and Physical Sciences Research Council in the UK(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported by the REFRAME project granted by the European Coordinated Research on Long-Term Challenges in Information and Communication Sciences & Technologies ERA-Net (CHIST-ERA), and funded by the Engineering and Physical Sciences Research Council in the UK under grant EP/K018728/1. Discussions with Hendrik Blockeel helped to clarify the intuitions underlying this work.	Boyd Kendrick, 2012, Proc Int Conf Mach Learn, V2012, P349; Chieu H. L, 2012, P 29 INT C MACH LEAR, P289; Davis J., 2006, 23 INT C MACH LEARN, P233, DOI [10.1145/1143844.1143874, DOI 10.1145/1143844.1143874]; Fawcett T, 2007, MACH LEARN, V68, P97, DOI 10.1007/s10994-007-5011-0; Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010; Flach P., 2010, ENCY MACHINE LEARNIN, P869, DOI DOI 10.1007/978-0-387-30164-8_733; Flach P., 2003, P 20 INT C MACH LEAR, V20, P194; Hand DJ, 2001, MACH LEARN, V45, P171, DOI 10.1023/A:1010920819831; Hernandez-Orallo J, 2012, J MACH LEARN RES, V13, P2813; Koyejo O., 2014, ADV NEURAL INFORM PR, V3, P2744; Lipton Zachary C, 2014, Mach Learn Knowl Discov Databases, V8725, P225, DOI 10.1007/978-3-662-44851-9_15; Narasimhan H., 2014, ADV NEURAL INF PROC, V27, P1493; Parambath S., 2014, ADV NEURAL INF PROCE, V27, P2123; Platt JC, 2000, ADV NEUR IN, P61; Provost F, 2001, MACH LEARN, V42, P203, DOI 10.1023/A:1007601015854; Sluban Borut, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P650, DOI 10.1007/978-3-642-40994-3_47; Van Rijsbergen CJ, 1979, INFORM RETRIEVAL; Vanschoren J., 2013, ACM SIGKDD EXPLOR NE, V15, P49, DOI [10.1145/2641190.2641198, DOI 10.1145/2641190.2641198]; Zadrozny Bianca, 2001, ICML; Zhao MJ, 2013, J MACH LEARN RES, V14, P1033	20	27	27	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102016
C	Courville, AC; Daw, ND; Gordon, GJ; Touretzky, DS		Thrun, S; Saul, K; Scholkopf, B		Courville, AC; Daw, ND; Gordon, GJ; Touretzky, DS			Model uncertainty in classical conditioning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We develop a framework based on Bayesian model averaging to explain how animals cope with uncertainty about contingencies in classical conditioning experiments. Traditional accounts of conditioning fit parameters within a fixed generative model of reinforcer delivery; uncertainty over the model structure is not considered. We apply the theory to explain the puzzling relationship between second-order conditioning and conditioned inhibition, two similar conditioning regimes that nonetheless result in strongly divergent behavioral outcomes. According to the theory, second-order conditioning results when limited experience leads animals to prefer a simpler world model that produces spurious correlations; conditioned inhibition results when a more complex model is justified by additional experience.	Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Courville, AC (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.							Courville AC, 2002, ADV NEUR IN, V14, P3; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; Iba Y, 2001, INT J MOD PHYS C, V12, P623, DOI 10.1142/S0129183101001912; Kakade S, 2002, PSYCHOL REV, V109, P533, DOI 10.1037//0033-295X.109.3.533; MACKAY DJC, 1991, ADV NEURAL INFORMATI, V4; PEARCE JM, 1994, PSYCHOL REV, V101, P587, DOI 10.1037/0033-295X.101.4.587; Rescorla R. A., 1972, CLASSICAL CONDITIONI; Sutton R.S., 1990, LEARNING COMPUTATION, P497; Tenenbaum JB, 2001, ADV NEUR IN, V13, P59; WILLIAMS DA, 1988, LEARN MOTIV, V19, P345, DOI 10.1016/0023-9690(88)90045-8; YIN H, 1994, J EXP PSYCHOL ANIM B, V20, P419, DOI 10.1037/0097-7403.20.4.419	11	27	27	1	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						977	984						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500122
C	Roth, V; Lange, T		Thrun, S; Saul, K; Scholkopf, B		Roth, V; Lange, T			Feature selection in clustering problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				DISCRIMINANT-ANALYSIS; LASSO	A novel approach to combining clustering and feature selection is presented. It implements a wrapper strategy for feature selection, in the sense that the features are directly selected by optimizing the discriminative power of the used partitioning algorithm. On the technical side, we present an efficient optimization algorithm with guaranteed local convergence property. The only free parameter of this method is selected by a resampling-based stability analysis. Experiments with real-world datasets demonstrate that our method is able to infer both meaningful partitions and meaningful subsets of features.	ETH, Inst Computat Sci, CH-8092 Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich; Universita della Svizzera Italiana	Roth, V (corresponding author), ETH, Inst Computat Sci, Hirschengraben 84, CH-8092 Zurich, Switzerland.		Roth, Volker/Q-4025-2017	Roth, Volker/0000-0003-0991-0273				Ben-Dor A, 2001, P 5 ANN INT C COMP M, P31, DOI DOI 10.1145/369133.369167; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; FIGUEIREDO M, 2001, CVPR2001, P35; HASTIE T, 1994, J AM STAT ASSOC, V89, P1255, DOI 10.2307/2290989; HASTIE T, 1995, ANN STAT, V23, P73, DOI 10.1214/aos/1176324456; HASTIE T, 1996, J R STAT SOC B, V58, P158; Hofmann T, 1997, IEEE T PATTERN ANAL, V19, P1, DOI 10.1109/34.566806; LANGE T, 2003, IN PRESS ADV NEURAL, V15; LAW MH, 2003, IN PRESS ADV NEURAL, V15; MEINECKE F, 2002, ADV NEURAL INFORMATI, V14; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; ROTH V, 2003, IN PRESS ADV NEURAL, V15; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; VONHEYDEBRECK A, 2001, BIOINFORMATICS, V17	14	27	27	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						473	480						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500060
C	Platt, JC; Burges, CJC; Swenson, S; Weare, C; Zheng, A		Dietterich, TG; Becker, S; Ghahramani, Z		Platt, JC; Burges, CJC; Swenson, S; Weare, C; Zheng, A			Learning a Gaussian process prior for automatically generating music playlists	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					This paper presents AutoDJ: a system for automatically generating music playlists based on one or more seed songs selected by a user. AutoDJ uses Gaussian Process Regression to learn a user preference function over songs. This function takes music metadata as inputs. This paper further introduces Kernel Meta-Training, which is a method of learning a Gaussian Process kernel from a distribution of functions that generates the learned function. For playlist generation, AutoDJ learns a kernel from a large set of albums. This learned kernel is shown to be more effective at predicting users' playlists than a reasonable hand-designed kernel.	Microsoft Corp, Redmond, WA 98052 USA	Microsoft	Platt, JC (corresponding author), Microsoft Corp, 1 Microsoft Way, Redmond, WA 98052 USA.	jplatt@microsoft.com; cburges@microsoft.com; sswenson@microsoft.com; chriswea@microsoft.com; alicez@cs.berkeley.edu	Platt, John/GOH-2678-2022	Platt, John/0000-0002-5652-5303				Barber D, 1997, ADV NEUR IN, V9, P340; Baxter J, 1997, MACH LEARN, V28, P7, DOI 10.1023/A:1007327622663; BENNETT KP, 1998, NIPS, V11, P368; Breese J.S., 2013, EMPIRICAL ANAL PREDI; CARUANA R, 1995, NIPS, V7, P657; CASTELLI V, 1996, IEEE T INFORM THEORY, V42, P75; Cressie N., 2011, STAT SPATIO TEMPORAL; CRISTIANINI N, 2001, NCTR01087 NEUR; GOLDBERG D, 1992, COMMUN ACM, V35, P61, DOI 10.1145/138859.138867; Minka T. P., 1997, LEARNING LEARN IS LE; Pazzani M, 1997, MACH LEARN, V27, P313, DOI 10.1023/A:1007369909943; RAO PSR, 1997, VARIANCE COMPONENTS; Williams CKI, 1996, ADV NEUR IN, V8, P514	14	27	30	0	3	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1425	1432						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100177
C	Seeger, M		Dietterich, TG; Becker, S; Ghahramani, Z		Seeger, M			Covariance kernels from Bayesian generative models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We propose the framework of mutual information kernels for learning covariance kernels, as used in Support Vector machines and Gaussian process classifiers, from unlabeled task data using Bayesian techniques. We describe an implementation of this framework which uses variational Bayesian mixtures of factor analyzers in order to attack classification problems in high-dimensional spaces where labeled data is sparse, but unlabeled data is abundant.	Univ Edinburgh, Inst Adapt & Neural Computat, Edinburgh EH1 2QL, Midlothian, Scotland	University of Edinburgh	Seeger, M (corresponding author), Univ Edinburgh, Inst Adapt & Neural Computat, 5 Forrest Hill, Edinburgh EH1 2QL, Midlothian, Scotland.							Blum A., 1998, P COLT; GHAHRAMANI Z, 1999, ADV NIPS, V12; Haussler D., 1999, CONVOLUTION KERNELS; JAAKKOLA T, 1998, ADV NEURAL INFORMATI, V11; SEEGER M, 2000, COVARIANCE KERNELS B; Seeger M., 2000, LEARNING LABELED UNL; SZUMMER M, 2001, ADV NIPS, V14; TSUDA K, 2001, ADV NIPS, V14; WATKINS C, 1999, CSDTR9811 U LOND ROY; Williams Christopher K. I., 1998, IEEE T PAMI, V20, P1342; Yianilos P., 1995, METRIC LEARNING VIA	11	27	27	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						905	912						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100113
C	Warmuth, MK; Ratsch, G; Mathieson, M; Liao, J; Lemmen, C		Dietterich, TG; Becker, S; Ghahramani, Z		Warmuth, MK; Ratsch, G; Mathieson, M; Liao, J; Lemmen, C			Active learning in the drug discovery process	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We investigate the following data mining problem from Computational Chemistry: From a large data set of compounds, find those that bind to a target molecule in as few iterations of biological testing as possible. In each iteration a comparatively small batch of compounds is screened for binding to the target. We apply active learning techniques for selecting the successive batches. One selection strategy picks unlabeled examples closest to the maximum margin hyperplane. Another produces many weight vectors by running perceptrons over multiple permutations of the data. Each weight vector votes with its +/- prediction and we pick the unlabeled examples for which the prediction is most evenly split between + and -. For a third selection strategy note that each unlabeled example bisects the version space of consistent weight vectors. We estimate the volume on both sides of the split by bouncing a billiard through the version space and select unlabeled examples that cause the most even split of the version space. We demonstrate that on two data sets provided by DuPont Pharmaceuticals that all three selection strategies perform comparably well and are much better than selecting random batches for testing.	Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA	University of California System; University of California Santa Cruz	Warmuth, MK (corresponding author), Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA.	manfred@cse.ucsc.edu; Gunnar.Raetsch@anu.edu.au; mathiesm@cse.ucsc.edu; liaojun@cse.ucsc.edu; clemmen@biosolveit.de		Ratsch, Gunnar/0000-0001-5486-8532				Angluin D., 1988, Machine Learning, V2, P319, DOI 10.1023/A:1022821128753; Atlas L. E., 1990, ADV NEURAL INFORM PR, P566; Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; CAMPBELL C, 2000, P ICML2000 STANF CA, P8; Freund Y., 1998, P 11 ANN C COMP LEAR; HERBRICH R, 1999, P IJCAI WORKSH SUPP, P23; Joachims T, 1999, ADVANCES IN KERNEL METHODS, P169; MYERS PL, 1997, TODAYS CHEM WORK, V6, P46; Rujan P, 1997, NEURAL COMPUT, V9, P99, DOI 10.1162/neco.1997.9.1.99; RUJAN P, 2000, ADV LARGE MARGIN CLA, V12, P329; Seung H. S., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P287, DOI 10.1145/130385.130417; Tong S., 2000, P 17 INT C MACH LEAR	12	27	27	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1449	1456						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100180
C	Zadrozny, B		Dietterich, TG; Becker, S; Ghahramani, Z		Zadrozny, B			Reducing multiclass to binary by coupling probability estimates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					This paper presents a method for obtaining class membership probability estimates for multiclass classification problems by coupling the probability estimates produced by binary classifiers. This is an extension for arbitrary code matrices of a method due to Hastie and Tibshirani for pairwise coupling of probability estimates. Experimental results with Boosted Naive Bayes show that our method produces calibrated class membership probability estimates, while having similar classification accuracy as loss-based decoding, a method for obtaining the most likely class that does not generate probability estimates.	Univ Calif San Diego, Dept Comp Sci & Engn, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Zadrozny, B (corresponding author), Univ Calif San Diego, Dept Comp Sci & Engn, La Jolla, CA 92093 USA.		Zadrozny, Bianca/Y-1847-2019					Allwein E. L., 2000, J MACHINE LEARNING R, V1, P113, DOI DOI 10.1162/15324430152733133; BRADLEY R, 1952, BIOMETRICS, P324; Crammer K., 2000, P 13 ANN C COMP LEAR, P35; Dietterich T. G., 1995, Journal of Artificial Intelligence Research, V2, P263; ELKAN C, 1997, CS97557 U CAL; HASTIE T, 1998, ADV NEURAL INFORMATI, V10; Newman C. B. D., 1998, UCI REPOSITORY MACHI; Utschick W, 2001, NEURAL COMPUT, V13, P1065, DOI 10.1162/08997660151134334; ZADROZNY B., 2001, P 7 INT C KNOWL DISC, P204; Zadrozny Bianca, 2001, ICML	10	27	27	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1041	1048						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100130
C	Williams, CKI		Leen, TK; Dietterich, TG; Tresp, V		Williams, CKI			On a connection between kernel PCA and metric multidimensional scaling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					In this paper we show that the kernel PCA algorithm of Scholkopf et al (1998) can be interpreted as a form of metric multidimensional scaling (MDS) when the kernel function k(x, y) is isotropic, i.e. it depends only on \ \x - y \ \. This leads to a metric MDS algorithm where the desired configuration of points is found via the solution of an eigenproblem rather than through the iterative optimization of the stress objective function. The question of kernel choice is also discussed.	Univ Edinburgh, Div Informat, Edinburgh EH1 2QL, Midlothian, Scotland	University of Edinburgh	Williams, CKI (corresponding author), Univ Edinburgh, Div Informat, 5 Forrest Hill, Edinburgh EH1 2QL, Midlothian, Scotland.							BERG C, 1984, HARMONIC ANAL SEMIGR; Cox T.F., 1994, MULTIDIMENSIONAL SCA; CRITCHLEY F, 1978, COMPSTAT 1978; Kruskal JosephB., 1978, MULTIDIMENSIONAL SCA, DOI [10.4135/9781412985130, DOI 10.4135/9781412985130]; Mardia KV, 1979, MULTIVARIATE ANAL; SAMMON JW, 1969, IEEE T COMPUT, VC 18, P401, DOI 10.1109/T-C.1969.222678; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Wahba G., 1990, CBMS NSF REGIONAL C; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807; Yaglom A. M., 1987, SPRINGER SERIES STAT, VI	11	27	28	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						675	681						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800096
C	Lowe, D; Tipping, ME		Mozer, MC; Jordan, MI; Petsche, T		Lowe, D; Tipping, ME			NeuroScale: Novel topographic feature extraction using RBF networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Dimension-reducing feature extraction neural network techniques which also preserve neighbourhood relationships in data have traditionally been the exclusive domain of Kohonen self organising maps. Recently, we introduced a novel dimension-reducing feature extraction process, which is also topographic, based upon a Radial Basis Function architecture. It has been observed that the generalisation performance of the system is broadly insensitive to model order complexity and other smoothing factors such as the kernel widths, contrary to intuition derived from supervised neural network models. In this paper we provide an effective demonstration of this property and give a theoretical justification for the apparent 'self-regularising' behaviour of the 'NEUROSCALE' architecture.			Lowe, D (corresponding author), ASTON UNIV,NEURAL COMP RES GRP,ASTON TRIANGLE,BIRMINGHAM B4 7ET,W MIDLANDS,ENGLAND.								0	27	30	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						543	549						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00077
C	Anderson, B; Hy, TS; Kondor, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Anderson, Brandon; Hy, Truong-Son; Kondor, Risi			Cormorant: Covariant Molecular Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ACCURACY; DYNAMICS; CHARMM; MODEL	We propose Cormorant, a rotationally covariant neural network architecture for learning the behavior and properties of complex many-body physical systems. We apply these networks to molecular systems with two goals: learning atomic potential energy surfaces for use in Molecular Dynamics simulations, and learning ground state properties of molecules calculated by Density Functional Theory. Some of the key features of our network are that (a) each neuron explicitly corresponds to a subset of atoms; (b) the activation of each neuron is covariant to rotations, ensuring that overall the network is fully rotationally invariant. Furthermore, the non-linearity in our network is based upon tensor products and the Clebsch-Gordan decomposition, allowing the network to operate entirely in Fourier space. Cormorant significantly outperforms competing algorithms in learning molecular Potential Energy Surfaces from conformational geometries in the MD-17 dataset, and is competitive with other methods at learning geometric, energetic, electronic, and thermodynamic properties of molecules on the GDB-9 dataset.	[Anderson, Brandon; Hy, Truong-Son; Kondor, Risi] Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA; [Kondor, Risi] Univ Chicago, Dept Stat, Chicago, IL 60637 USA; [Anderson, Brandon; Kondor, Risi] Flatiron Inst, Ctr Computat Math, New York, NY USA; [Anderson, Brandon; Kondor, Risi] Atomwise, San Francisco, CA USA	University of Chicago; University of Chicago	Anderson, B (corresponding author), Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA.; Anderson, B (corresponding author), Atomwise, San Francisco, CA USA.	brandona@jfi.uchicago.edu; hytruongson@uchicago.edu; risi@uchicago.edu	Hy, Truong Son/AGK-6752-2022		DARPA "Physics of AI" grant [HR0011837139]; NSF [MRI 1828629]	DARPA "Physics of AI" grant; NSF(National Science Foundation (NSF))	This project was supported by DARPA "Physics of AI" grant number HR0011837139, and used computational resources acquired through NSF MRI 1828629.	[Anonymous], 2018, CORR; [Anonymous], 2018, CORR; Bartok AP, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.136403; Behler J, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.146401; Bendory T, 2018, IEEE T SIGNAL PROCES, V66, P1037, DOI 10.1109/TSP.2017.2775591; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Brooks BR, 2009, J COMPUT CHEM, V30, P1545, DOI 10.1002/jcc.21287; BROOKS BR, 1983, J COMPUT CHEM, V4, P187, DOI 10.1002/jcc.540040211; Bruna J, 2014, SPECTRAL NETWORKS LO; Chmiela S, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-06169-2; Chmiela Stefan, 2016, MACHINE LEARNING ACC, P1; Cohen T. S., 2016, CORR; Cohen T. S., 2018, CORR; Cohen Taco S., 2019, CORR; Esteves Carlos, 2017, CORR, P1; Gilmer Justin, 2017, CORR; Henaff M., 2015, CORR; Hirn M, 2017, MULTISCALE MODEL SIM, V15, P827, DOI 10.1137/16M1075454; HOHENBERG P, 1964, PHYS REV B, V136, pB864, DOI 10.1103/PhysRevB.7.1912; Jackson J.D., 1999, CLASSICAL ELECTRODYN; Kakarala R., 1992, THESIS; Kondor R., 2018, ADV NEURAL INF PROCE, P10117; Kondor R., 2018, INT C MACH LEARN ICM; Kondor Risi, 2013, PHYS REV B, V87; Masci Jonathan, 2015, CORR; Monti F., 2016, ARXIV161108402; Ramakrishnan Raghunathan, 2014, SCI DATA, V1; Rupp M., 2012, PHYS REV LETT, V108; Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890; Schutt K., 2017, 31 C NEURAL INFORM P, ppp991; Shapeev Alexander V, 2015, MOMENT TENSOR POTENT; Smith JS, 2017, CHEM SCI, V8, P3192, DOI 10.1039/c6sc05720a; Stone A. J., 1997, THEORY INTERMOLECULA; Weiler Maurice, 2018, CORR; Zhang LF, 2018, PHYS REV LETT, V120, DOI 10.1103/PhysRevLett.120.143001	37	26	26	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906022
C	Ginart, AA; Guan, MY; Valiant, G; Zou, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ginart, Antonio A.; Guan, Melody Y.; Valiant, Gregory; Zou, James			Making AI Forget You: Data Deletion in Machine Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				STABILITY; QUANTIZATION; MODELS; KERNEL; BOUNDS	Intense recent discussions have focused on how to provide individuals with control over when their data can and cannot be used - the EU's Right To Be Forgotten regulation is an example of this effort. In this paper we initiate a framework studying what to do when it is no longer permissible to deploy models derivative from specific user data. In particular, we formulate the problem of efficiently deleting individual data points from trained machine learning models. For many standard ML models, the only way to completely remove an individual's data is to retrain the whole model from scratch on the remaining data, which is often not computationally practical. We investigate algorithmic principles that enable efficient data deletion in ML. For the specific setting of k-means clustering, we propose two provably efficient deletion algorithms which achieve an average of over 100x improvement in deletion efficiency across 6 datasets, while producing clusters of comparable statistical quality to a canonical k-means++ baseline.	[Ginart, Antonio A.] Stanford Univ, Dept Elect Engn, Palo Alto, CA 94305 USA; [Guan, Melody Y.; Valiant, Gregory] Stanford Univ, Dept Comp Sci, Palo Alto, CA 94305 USA; [Zou, James] Stanford Univ, Dept Biomedial Data Sci, Palo Alto, CA 94305 USA	Stanford University; Stanford University; Stanford University	Ginart, AA (corresponding author), Stanford Univ, Dept Elect Engn, Palo Alto, CA 94305 USA.	tginart@stanford.edu; mguan@stanford.edu; valiant@stanford.edu; jamesz@stanford.edu			NSF [AF:1813049, CCF:1704417, CCF 1763191]; NIH [R21 MD012867-01, P30AG059307]; Office of Naval Research Young Investigator Award [N00014-18-1-2295]; Stanford's Institute for Human-Centered AI; Chan-Zuckerberg Initiative	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Office of Naval Research Young Investigator Award(Office of Naval Research); Stanford's Institute for Human-Centered AI; Chan-Zuckerberg Initiative	This research was partially supported by NSF Awards AF:1813049, CCF:1704417, and CCF 1763191, NIH R21 MD012867-01, NIH P30AG059307, an Office of Naval Research Young Investigator Award (N00014-18-1-2295), a seed grant from Stanford's Institute for Human-Centered AI, and the Chan-Zuckerberg Initiative. We would also like to thank I. Lemhadri, B. He, V. Bagaria, J. Thomas and anonymous reviewers for helpful discussion and feedback.	Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Abu-Mostafa Y., 2012, LEARNING FROM DATA, V4; Aloise D, 2009, MACH LEARN, V75, P245, DOI 10.1007/s10994-009-5103-0; ALTMAN NS, 1992, AM STAT, V46, P175, DOI 10.2307/2685209; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Atkeson CG, 1997, ARTIF INTELL REV, V11, P75, DOI 10.1023/A:1006511328852; Bachem O, 2017, PR MACH LEARN RES, V70; Bahmani B, 2012, PROC VLDB ENDOW, V5, P622, DOI 10.14778/2180912.2180915; Balcan M.F., 2013, P 26 INT C NEURAL IN, P1995; Belsley R. E. W. D. A., 1980, REGRESSION DIAGNOSTI; BERMAN O, 1993, IEEE T SOFTWARE ENG, V19, P1119, DOI 10.1109/32.256858; Birattari M, 1999, ADV NEUR IN, V11, P375; Blackard JA, 1999, COMPUT ELECTRON AGR, V24, P131, DOI 10.1016/S0168-1699(99)00046-0; Bogdanov D, 2018, IEEE ACM T COMPUT BI, V15, P1427, DOI 10.1109/TCBB.2018.2858818; Bonawitz K, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1175, DOI 10.1145/3133956.3133982; Bontempi G, 2001, FUZZY SET SYST, V121, P59, DOI 10.1016/S0165-0114(99)00172-4; Bottou Leon, 1999, ON LINE LEARNING NEU, P9, DOI DOI 10.1017/CBO9780511569920.003; Cao YZ, 2015, P IEEE S SECUR PRIV, P463, DOI 10.1109/SP.2015.35; Cauwenberghs G, 2001, ADV NEUR IN, V13, P409; Chaudhuri K, 2013, J MACH LEARN RES, V14, P2905; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; COOMANS D, 1982, ANAL CHIM ACTA, V136, P15, DOI 10.1016/S0003-2670(01)95359-0; Courbariaux M., 2014, ARXIV PREPRINT ARXIV; Cover T. M., 1991, Elements of information theory, DOI 10.1002/0471200611; Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073; DEVROYE LP, 1979, IEEE T INFORM THEORY, V25, P601, DOI 10.1109/TIT.1979.1056087; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Erkin Z, 2012, IEEE T INF FOREN SEC, V7, P1053, DOI 10.1109/TIFS.2012.2190726; Galinsky KJ, 2016, AM J HUM GENET, V99, P1130, DOI 10.1016/j.ajhg.2016.09.014; Gardner A, 2014, PROC CVPR IEEE, P137, DOI 10.1109/CVPR.2014.25; Gardner A, 2014, IEEE SYS MAN CYBERN, P164, DOI 10.1109/SMC.2014.6973901; Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1; Ghorbani A, 2019, PR MACH LEARN RES, V97; Golub G.H., 2013, MATRIX COMPUTATIONS, P357; Gray RM, 1998, IEEE T INFORM THEORY, V44, P2325, DOI 10.1109/18.720541; Guha S., 1998, SIGMOD Record, V27, P73, DOI 10.1145/276305.276312; Gysel P., 2018, IEEE T NEURAL NETWOR; Han XP, 2018, CELL, V172, P1091, DOI 10.1016/j.cell.2018.02.001; Hastie T., 2001, SPRINGER SERIES STAT; Higham N. J., 2002, ACCURACY STABILITY N; Hinneburg A, 2007, LECT NOTES COMPUT SC, V4723, P70; Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400; Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246; Jun Ye, 2012, 2012 Conference on Lasers and Electro-Optics (CLEO); Knoblauch A, 2008, SIAM J APPL MATH, V69, P197, DOI 10.1137/070700024; Knops ZF, 2006, MED IMAGE ANAL, V10, P432, DOI 10.1016/j.media.2005.03.009; Kutin S., 2002, TR200203 U CHIC COMP; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lin DD, 2016, PR MACH LEARN RES, V48; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Lowe D.G., 1999, P IEEE INT C COMP VI, V2, P1150, DOI DOI 10.1109/ICCV.1999.790410; Lucas, 2008, UPDATING QR FACTORIZ; Maindonald J. H., 1984, STAT COMPUTATION; Meidan Y, 2018, IEEE PERVAS COMPUT, V17, P12, DOI 10.1109/MPRV.2018.03367731; Muggeo V.M, 2008, R NEWS, V8, P20, DOI DOI 10.1159/000323281; Muggeo VMR, 2016, J STAT COMPUT SIM, V86, P3059, DOI 10.1080/00949655.2016.1149855; Muggeo VMR, 2003, STAT MED, V22, P3055, DOI 10.1002/sim.1545; Nadaraya E.A., 1964, THEOR PROBAB APPL+, V9, P141, DOI [DOI 10.1137/1109020, 10.1137/1109020]; Nikolaenko V, 2013, P IEEE S SECUR PRIV, P334, DOI 10.1109/SP.2013.30; Ohrimenko O, 2016, PROCEEDINGS OF THE 25TH USENIX SECURITY SYMPOSIUM, P619; Papernot N., 2017, ICLR; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Peleg D., 2000, SIAM MONOG DISCR MAT, DOI 10.1137/1.9780898719772; Qin JH, 2017, IEEE T CYBERNETICS, V47, P772, DOI 10.1109/TCYB.2016.2526683; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7; Schellekens V, 2018, IEEE SIGNAL PROC LET, V25, P1211, DOI 10.1109/LSP.2018.2847908; Schelter S., 2019, 1 INT WORKSH APPL AI; Schomm F, 2013, SIGMOD REC, V42, P15; Schrauwen B, 2007, LECT NOTES COMPUT SC, V4668, P471; Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635; SHANNON CE, 1949, BELL SYST TECH J, V28, P656, DOI 10.1002/j.1538-7305.1949.tb00928.x; Sudlow C, 2015, PLOS MED, V12, DOI 10.1371/journal.pmed.1001779; Truong HL, 2012, INT J COMPUT SCI ENG, V7, P280, DOI 10.1504/IJCSE.2012.049749; Tsai CH, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P343, DOI 10.1145/2623330.2623661; van der Walt S, 2011, COMPUT SCI ENG, V13, P22, DOI 10.1109/MCSE.2011.37; Vanhoucke V., IMPROVING SPEED NEUR; Veale M, 2018, PHILOS T R SOC A, V376, DOI 10.1098/rsta.2018.0083; Villaronga EF, 2018, COMPUT LAW SECUR REV, V34, P304, DOI 10.1016/j.clsr.2017.08.007; Vinh NX, 2010, J MACH LEARN RES, V11, P2837; Webb G.I., 2010, LAZY LEARNING, P571; Zeb S, 2017, J INEQUAL APPL, DOI 10.1186/s13660-017-1547-0; Zhang J., 2018, ARXIV181100155; Zhao WZ, 2009, LECT NOTES COMPUT SC, V5931, P674, DOI 10.1007/978-3-642-10665-1_71	89	26	26	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													14	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303050
C	Hahn, T; Pyeon, M; Kim, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hahn, Taeyoung; Pyeon, Myeongjang; Kim, Gunhee			Self-Routing Capsule Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MIXTURES	Capsule networks have recently gained a great deal of interest as a new architecture of neural networks that can be more robust to input perturbations than similar-sized CNNs. Capsule networks have two major distinctions from the conventional CNNs: (i) each layer consists of a set of capsules that specialize in disjoint regions of the feature space and (ii) the routing-by-agreement coordinates connections between adjacent capsule layers. Although the routing-by-agreement is capable of filtering out noisy predictions of capsules by dynamically adjusting their influences, its unsupervised clustering nature causes two weaknesses: (i) high computational complexity and (ii) cluster assumption that may not hold in the presence of heavy input noise. In this work, we propose a novel and surprisingly simple routing strategy called self-routing, where each capsule is routed independently by its subordinate routing network. Therefore, the agreement between capsules is not required anymore, but both poses and activations of upper-level capsules are obtained in a way similar to Mixture-of-Experts. Our experiments on CIFAR-10, SVHN, and SmallNORB show that the self-routing performs more robustly against white-box adversarial attacks and affine transformations, requiring less computation.	[Hahn, Taeyoung; Pyeon, Myeongjang; Kim, Gunhee] Seoul Natl Univ, Seoul, South Korea	Seoul National University (SNU)	Hahn, T (corresponding author), Seoul Natl Univ, Seoul, South Korea.	taeyounghahn@snu.ac.kr; mjpyeon@snu.ac.kr; gunhee@snu.ac.kr			Samsung Research Funding Center of Samsung Electronics [SRFC-IT1502-51]; ICT R&D program of MSIT/IITP [2019-0-01309]	Samsung Research Funding Center of Samsung Electronics(Samsung); ICT R&D program of MSIT/IITP(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of Korea)	This work was supported by Samsung Research Funding Center of Samsung Electronics (No. SRFC-IT1502-51) and the ICT R&D program of MSIT/IITP (No. 2019-0-01309, Development of AI technology for guidance of a mobile robot to its goal with uncertain maps in indoor/outdoor environments). Gunhee Kim is the corresponding author.	Alcorn M. A., 2018, ARXIV181111553; Aljundi R, 2017, PROC CVPR IEEE, P7120, DOI 10.1109/CVPR.2017.753; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; [Anonymous], 2014, ICLR; Bahadori M.T., 2018, ICLR WORKSH; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cohen TS, 2016, PR MACH LEARN RES, V48; Croce F., 2018, ARXIV181007481; Engstrom L., 2017, ARXIV171202779; Geirhos R, 2018, ADV NEUR IN, V31; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hendrycks D., 2019, ICLR, P1; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Hinton Geoffrey E., 2018, INT C LEARN REPR; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; Jaiswal A, 2018, P EUR C COMP VIS EC; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; Kirsch L., 2018, NEURIPS; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kurakin A., 2016, ARXIV PREPRINT ARXIV; LeCun Y, 2004, PROC CVPR IEEE, P97; Li HY, 2018, LECT NOTES COMPUT SC, V11215, P266, DOI 10.1007/978-3-030-01252-6_16; Liu W, 2018, ARXIV180507706; Madry Aleksander, 2017, ARXIV; Ramachandran P., 2019, ICLR; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rosenbaum Clemens, 2017, ARXIV171101239; Sabour Sara, 2017, PROC 31 INT C NEURAL; Saito S., 2018, ARXIV181200181; Su JW, 2019, IEEE T EVOLUT COMPUT, V23, P828, DOI 10.1109/TEVC.2019.2890858; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Veit A, 2016, ADV NEUR IN, V29; Verma Saurabh, 2018, ABS180508090 CORR; Wang D, 2018, IEEE W CONTR MODEL; Wang Y. N. T., 2011, NEURIPS; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2	42	26	26	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307065
C	Hu, ZT; Tan, BW; Salakhutdinov, R; Mitchell, T; Xing, EP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hu, Zhiting; Tan, Bowen; Salakhutdinov, Ruslan; Mitchell, Tom; Xing, Eric P.			Learning Data Manipulation for Augmentation and Weighting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Manipulating data, such as weighting data examples or augmenting with new instances, has been increasingly used to improve model training. Previous work has studied various rule- or learning-based approaches designed for specific types of data manipulation. In this work, we propose a new method that supports learning different manipulation schemes with the same gradient-based algorithm. Our approach builds upon a recent connection of supervised learning and reinforcement learning (RL), and adapts an off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training. Different parameterization of the "data reward" function instantiates different manipulation schemes. We showcase data augmentation that learns a text transformation network, and data weighting that dynamically adapts the data sample importance. Experiments show the resulting algorithms significantly improve the image and text classification performance in low data regime and class-imbalance problems.	[Hu, Zhiting; Tan, Bowen; Salakhutdinov, Ruslan; Mitchell, Tom; Xing, Eric P.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Hu, Zhiting; Xing, Eric P.] Petuum Inc, Pittsburgh, PA 15222 USA	Carnegie Mellon University	Hu, ZT (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.; Hu, ZT (corresponding author), Petuum Inc, Pittsburgh, PA 15222 USA.	zhitingh@cs.cmu.edu; btan2@cs.cmu.edu; rsalakhu@cs.cmu.edu; tom.mitchell@cs.cmu.edu; eric.xing@petuum.com						Abdolmaleki A., 2018, INT C LEARN REPR; Andreas Jacob, 2016, ARXIV160101705, P1545, DOI [DOI 10.18653/V1/N16-1181, 10.18653/v1/N16-1181]; Baluja S., 2017, ARXIV170309387; Bell T, 2002, CONTRIBUTIONS OF SURFACE ENGINEERING TO MODERN MANUFACTURING AND REMANUFACTURING, P3; Chang H. -S., 2017, ADV NEURAL INFORM PR, V30, P1002; Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020; Dayan P, 1997, NEURAL COMPUT, V9, P271, DOI 10.1162/neco.1997.9.2.271; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Fan Y., 2018, ARXIV PREPRINT ARXIV; Finn C., 2016, ABS161103852 CORR; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Ganchev K, 2010, J MACH LEARN RES, V11, P2001; Giridhara P. K. B., 2019, ICPRAM; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Hu Z., 2018, NIPS; Hu Z., 2018, ICLR; Hu ZT, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P2410, DOI 10.18653/v1/p16-1228; Hu ZT, 2017, PR MACH LEARN RES, V70; Jang E., 2016, ARXIV; Jiang L., 2018, ICML; Katharopoulos A, 2018, PR MACH LEARN RES, V80; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kobayashi Sosuke, 2018, NAACL; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kumar M., 2010, NIPS, P1189, DOI DOI 10.5555/2997189.2997322; Lemley J, 2017, IEEE ACCESS, V5, P5858, DOI 10.1109/ACCESS.2017.2696121; Levine Sergey, 2018, ARXIV180500909; Maas A., 2011, P 49 ANN M ASS COMPU, P142; Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Mirza M., 2014, ARXIV; Norouzi M, 2016, ADV NEUR IN, V29; Oh S, 2015, INT C EMERG TECHN SM; Park D. S., ARXIV190408779; Peng X, 2018, PROC CVPR IEEE, P2226, DOI 10.1109/CVPR.2018.00237; Ratner A. J., 2017, NEURIPS; Ren Mengye, 2018, ARXIV PREPRINT ARXIV; Roweis S., 1999, NEURAL COMPUTATION; Samdani R., 2012, ACL; Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Simard PY, 1998, LECT NOTES COMPUT SC, V1524, P239; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Tan B., 2018, ARXIV181109740; Tran T, 2017, ADV NEUR IN, V30; Wei J, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P6382; Wu Xing, 2018, ABS181206705 CORR; Xie Qizhe, 2019, ARXIV190412848, P2; Xie Ziang, 2017, ICLR; Zheng Z., 2018, NEURIPS	52	26	26	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907042
C	Mathieu, E; Le Lan, C; Maddison, CJ; Tomioka, R; Teh, YW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mathieu, Emile; Le Lan, Charline; Maddison, Chris J.; Tomioka, Ryota; Teh, Yee Whye			Continuous Hierarchical Representations with Poincare Variational Auto-Encoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The variational auto-encoder (VAE) is a popular method for learning a generative model and embeddings of the data. Many real datasets are hierarchically structured. However, traditional VAEs map data in a Euclidean latent space which cannot efficiently embed tree-like structures. Hyperbolic spaces with negative curvature can. We therefore endow VAES with a Poincare ball model of hyperbolic geometry as a latent space and rigorously derive the necessary methods to work with two main Gaussian generalisations on that space. We empirically show better generalisation to unseen data than the Euclidean counterpart, and can qualitatively and quantitatively better recover hierarchical structures.	[Mathieu, Emile; Le Lan, Charline; Maddison, Chris J.; Teh, Yee Whye] Univ Oxford, Dept Stat, Oxford, England; [Maddison, Chris J.; Teh, Yee Whye] DeepMind, London, England; [Tomioka, Ryota] Microsoft Res, Cambridge, England	University of Oxford; Microsoft	Mathieu, E (corresponding author), Univ Oxford, Dept Stat, Oxford, England.	emile.mathieu@stats.ox.ac.uk; charline.lelan@stats.ox.ac.uk; cmaddis@stats.ox.ac.uk; ryoto@microsoft.com; y.w.teh@stats.ox.ac.uk			European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC [617071]; EPSRC [EP/N509711/1]; Microsoft Research	European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC(European Research Council (ERC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Microsoft Research(Microsoft)	We are extremely grateful to Adam Foster, Phillipe Gagnon and Emmanuel Chevallier for their help. EM, YWT's research leading to these results received funding from the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617071 and they acknowledge Microsoft Research and EPSRC for funding EM's studentship, and EPSRC grant agreement no. EP/N509711/1 for funding CL's studentship.	Beltrami E., 1868, TEORIA FONDAMENTALE; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Blei DM, 2004, ADV NEUR IN, V16, P17; Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619; BOX GEP, 1958, ANN MATH STAT, V29, P610, DOI 10.1214/aoms/1177706645; Burda Yuri, 2015, ARXIV150900519; Chamberlain Benjamin Paul, 2017, NEURAL EMBEDDINGS GR; Chevallier E, 2015, LECT NOTES COMPUT SC, V9389, P753, DOI 10.1007/978-3-319-25040-3_80; Clauset A, 2008, NATURE, V453, P98, DOI 10.1038/nature06830; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; COLLINS AM, 1969, J VERB LEARN VERB BE, V8, P240, DOI 10.1016/S0022-5371(69)80069-1; Darwin C., 1859, ORIGINS SPECIES MEAN; Davidson TR, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P856; Duda R.O., 2000, PATTERN CLASSIFICATI; Falorsi L., 2018, ARXIV180704689; Figurnov Mikhail, 2018, ADV NEURAL INFORM PR, P441; Ganea Octavian, 2018, ADV NEURAL INFORM PR, P5345; Ghahramani Z., 2010, ADV NEURAL INFORM PR, P19; Ghahramani Z., 2005, P 22 INT C MACH LEAR, P297, DOI DOI 10.1145/1102351.1102389; GILKS WR, 1992, J R STAT SOC C-APPL, V41, P337; Goh KI, 2007, P NATL ACAD SCI USA, V104, P8685, DOI 10.1073/pnas.0701361104; Grattarola D, 2019, APPL SOFT COMPUT, V81, DOI 10.1016/j.asoc.2019.105511; Hauberg S, 2018, 2018 21ST INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P704; HOFBAUER W.K., 2016, BRYOPHYTE DIVERSITY, V38, P1, DOI DOI 10.11646/bde.38.1.1; Hsu E. P., 2008, BRIEF INTRO BROWNIAN; Huang Fu Jie, 2006, 2006 IEEE COMPUTER S, V1, P284, DOI 10.1109/CVPR.2006.164; Keil F., 1979, COGNITIVE SCI SERIES; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2016, P INT C LEARN REPR I; Kipf Thomas N., 2016, ARXIV161107308, V2, P1; Larsen J., 2001, PROBABILISTIC HIERAR; LeCun Y., 2010, MNIST HANDWRITTEN DI; Li H, 2019, INT RELIAB PHY SYM; Mardia K., 2009, DIRECTIONAL STAT WIL; Nagano Y., 2019, INT C MACH LEARN ICM; Nickel M, 2018, PR MACH LEARN RES, V80; Nickel M, 2017, ADV NEUR IN, V30; Nooy W.D., 2011, EXPLORATORY SOCIAL N; Ovinnikov I, 2018, NEURIPS WORKSH BAYES, P1; Paeng SH, 2011, J GEOM PHYS, V61, P940, DOI 10.1016/j.geomphys.2011.01.005; Pascucci V., 2011, TOPOLOGICAL METHODS; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Pennec X, 2006, J MATH IMAGING VIS, V25, P127, DOI 10.1007/s10851-006-6228-4; Petersen P., 2006, RIEMANNIAN GEOMETRY; Platt J.C., 2008, ADV NEURAL INFORM PR, P1473; Rey L. A. P., 2019, CORR; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Rossi RA, 2015, AAAI CONF ARTIF INTE, P4292; Roy Daniel M., 2006, P NIPS, P1185; Saha S, 2018, INT C PATT RECOG, P1000, DOI 10.1109/ICPR.2018.8546020; Said S, 2014, ENTROPY-SWITZ, V16, P4015, DOI 10.3390/e16074015; Sala F, 2018, PR MACH LEARN RES, V80; Salakhutdinov R. R., 2011, ICML UNSUPERVISED TR; SANDERSON MJ, 1994, AM J BOT; Sarkar R, 2012, LECT NOTES COMPUT SC, V7034, P355; Tifrea A., 2019, INT C LEARN REPR ICL; Ungar Abraham Albert, 2008, SYNTHESIS LECT MATH, V1, P2; VERDEBOUT T., 2017, MODERN DIRECTIONAL S, DOI [DOI 10.1201/9781315119472, 10.1201/9781315119472]	59	26	26	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904024
C	Nijkamp, E; Hill, M; Zhu, SC; Wu, YN		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Nijkamp, Erik; Hill, Mitch; Zhu, Song-Chun; Wu, Ying Nian			Learning Non-Convergent Non-Persistent Short-Run MCMC Toward Energy-Based Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				FRAME	This paper studies a curious phenomenon in learning energy-based model (EBM) using MCMC. In each learning iteration, we generate synthesized examples by running a non-convergent, non-mixing, and non-persistent short-run MCMC toward the current model, always starting from the same initial distribution such as uniform noise distribution, and always running a fixed number of MCMC steps. After generating synthesized examples, we then update the model parameters according to the maximum likelihood learning gradient, as if the synthesized examples are fair samples from the current model. We treat this non-convergent short-run MCMC as a learned generator model or a flow model. We provide arguments for treating the learned non-convergent short-run MCMC as a valid model. We show that the learned short-run MCMC is capable of generating realistic images. More interestingly, unlike traditional EBM or MCMC, the learned short-run MCMC is capable of reconstructing observed images and interpolating between images, like generator or flow models. The code can be found in the Appendix.	[Nijkamp, Erik; Hill, Mitch; Zhu, Song-Chun; Wu, Ying Nian] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90024 USA	University of California System; University of California Los Angeles	Nijkamp, E (corresponding author), Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90024 USA.	enijkamp@ucla.edu; mkhill@ucla.edu; sczhu@stat.ucla.edu; ywu@stat.ucla.edu			DARPA XAI [N66001-17-2-4029]; ARO [W911NF1810296]; ONR MURI project [N00014-16-1-2007]; XSEDE grant [ASC170063]	DARPA XAI; ARO; ONR MURI project; XSEDE grant	The work is supported by DARPA XAI project N66001-17-2-4029; ARO project W911NF1810296; and ONR MURI project N00014-16-1-2007; and XSEDE grant ASC170063. We thank Prof. Stu Geman, Prof. Xianfeng (David) Gu, Diederik P. Kingma, Guodong Zhang, and Will Grathwohl for helpful discussions.	Abbeel Pieter, 2004, P 21 INT C ICML 2004, V69; Amit D. J., 1989, MODELING BRAIN FUNCT; [Anonymous], 2017, 5 INT C LEARN REPR; Barratt Shane T., 2018, ABS180101973 CORR, P7; Behrmann J, 2019, PR MACH LEARN RES, V97; Bengio Yoshua, 2015, 3 INT C LEARN REPR I; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI 10.1002/0471200611; Dai Jifeng, BENGIO LECUN; Dai Zihang, 5 INT C LEARN REPR; DellaPietra S, 1997, IEEE T PATTERN ANAL, V19, P380, DOI 10.1109/34.588021; Dinh L., 5 INT C LEARN REPR I; Dinh Laurent, 2015, ICLR WORKSH; Du Yilun, 2019, ABS190308689 CORR; Gao RQ, 2018, PROC CVPR IEEE, P9155, DOI 10.1109/CVPR.2018.00954; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grathwohl W., 2019, P INT C LEARN REPR; Grenander U., 2007, PATTERN THEORY REPRE; Guyon I, 2017, ADV NEURAL INFORM PR; Han T, 2017, AAAI CONF ARTIF INTE, P1976; Heusel Martin, GANS TRAINED 2 TIME, P6626; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; Jin L., 2017, ADV NEURAL INFORM PR, P823; Kim Taesup, 2016, ABS160603439 CORR; Kingma D.P, P 3 INT C LEARNING R; Kingma Diederik P, 2018, ADV NEURAL INFORM PR; Kingma Diederik P., BENGIO LECUN; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Krotov D, 2019, P NATL ACAD SCI USA, V116, P7723, DOI 10.1073/pnas.1820458116; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Yann, 2006, PREDICTING STRUCTURE, P2; Lee H, 2009, P 26 ANN INT C MACH, V26, P609, DOI [10.1145/1553374.1553453, DOI 10.1145/1553374.1553453]; Li Chun-Liang, MMD GAN DEEPER UNDER, P2203; Lu Y, 2016, AAAI CONF ARTIF INTE, P1902; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Ngiam J., 2011, P 28 INT C MACHINE L, P1105; Nijkamp Erik, 2020, 34 AAAI C ART INT; Poucet B, 2005, SCIENCE, V308, P799, DOI 10.1126/science.1112555; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roth Kevin, STABILIZING TRAINING, P2018; Salakhutdinov Ruslan, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPR.2009.5206577; Salimans T, 2016, ADV NEUR IN, V29; Sonderby Casper Kaae, 5 INT C LEARN REPR I; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tieleman T., 2008, P 25 INT C MACHINE L, P1064, DOI DOI 10.1145/1390156.1390290; Wu YN, 2000, INT J COMPUT VISION, V38, P247, DOI 10.1023/A:1008199424771; Xie J., 2018, IEEE T PATTERN ANAL; Xie JW, 2018, AAAI CONF ARTIF INTE, P4292; Xie JW, 2016, PR MACH LEARN RES, V48; Zhao Junbo Jake, 5 INT C LEARN REPR I; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420; Zhu SC, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P847, DOI 10.1109/ICCV.1998.710816; Ziebart B. D., 2008, AAAI, V8, P1433	56	26	26	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305025
C	Qiao, XM; Yang, YK; Li, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Qiao, Ximing; Yang, Yukun; Li, Hai			Defending Neural Backdoors via Generative Distribution Modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Neural backdoor attack is emerging as a severe security threat to deep learning, while the capability of existing defense methods is limited, especially for complex backdoor triggers. In the work, we explore the space formed by the pixel values of all possible backdoor triggers. An original trigger used by an attacker to build the backdoored model represents only a point in the space. It then will be generalized into a distribution of valid triggers, all of which can influence the backdoored model. Thus, previous methods that model only one point of the trigger distribution is not sufficient. Getting the entire trigger distribution, e.g., via generative modeling, is a key of effective defense. However, existing generative modeling techniques for image generation are not applicable to the backdoor scenario as the trigger distribution is completely unknown. In this work, we propose max-entropy staircase approximator (MESA) for high-dimensional sampling-free generative modeling and use it to recover the trigger distribution. We also develop a defense technique to remove the triggers from the backdoored model. Our experiments on Cifar10/100 dataset demonstrate the effectiveness of MESA in modeling the trigger distribution and the robustness of the proposed defense method.	[Qiao, Ximing; Yang, Yukun; Li, Hai] Duke Univ, ECE Dept, Durham, NC 27708 USA	Duke University	Qiao, XM (corresponding author), Duke Univ, ECE Dept, Durham, NC 27708 USA.	ximing.qiao@duke.edu; yukun.yang@duke.edu; hai.li@duke.edu	Li, Hai/L-8558-2017	Li, Hai/0000-0003-3228-6544				Beirlant J., 1997, INT J MATH STAT SCI; Belghazi MI, 2018, PR MACH LEARN RES, V80; Brown Tom B, 2017, ARXIV171209665; Buitinck L., 2013, API DESIGN MACHINE L, DOI DOI 10.48550/ARXIV.1309.0238; Chen Bryant, 2018, ARXIV181103728; Chen Xinyun, 2017, ARXIV171205526; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Gu T, 2017, ARXIV PREPRINT ARXIV; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Kingma D.P, P 3 INT C LEARNING R; Kingma DP, 2018, ADV NEUR IN, V31; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Liao Cong, 2018, ARXIV180810307; Liu K, 2018, LECT NOTES COMPUT SC, V11050, P273, DOI 10.1007/978-3-030-00470-5_13; Liu YQ, 2018, 25TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2018), DOI 10.14722/ndss.2018.23291; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Tran B, 2018, ADV NEUR IN, V31; Turner Alexander, 2018, CLEAN LABEL BACKDOOR, P3; van den Oord A, 2016, ADV NEUR IN, V29; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang Bolun, 2019, P 40 IEEE S SEC PRIV	25	26	26	3	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905065
C	Zellers, R; Holtzman, A; Rashkin, H; Bisk, Y; Farhadi, A; Roesner, F; Choi, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zellers, Rowan; Holtzman, Ari; Rashkin, Hannah; Bisk, Yonatan; Farhadi, Ali; Roesner, Franziska; Choi, Yejin			Defending Against Neural Fake News	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like 'Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias - and sampling strategies that alleviate its effects - both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.	[Zellers, Rowan; Holtzman, Ari; Rashkin, Hannah; Bisk, Yonatan; Farhadi, Ali; Roesner, Franziska; Choi, Yejin] Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA; [Farhadi, Ali; Choi, Yejin] Allen Inst Artificial Intelligence, Seattle, WA USA	University of Washington; University of Washington Seattle	Zellers, R (corresponding author), Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA.				National Science Foundation [DGE-1256082]; NSF [IIS-1524371, 1637479, 165205, 1703166]; DARPA CwC program through ARO [W911NF-15-1-0543]; Sloan Research Foundation through a Sloan Fellowship; Allen Institute for Artificial Intelligence; NVIDIA Artificial Intelligence Lab; Samsung through a Samsung AI research grant; Google Cloud	National Science Foundation(National Science Foundation (NSF)); NSF(National Science Foundation (NSF)); DARPA CwC program through ARO; Sloan Research Foundation through a Sloan Fellowship; Allen Institute for Artificial Intelligence; NVIDIA Artificial Intelligence Lab; Samsung through a Samsung AI research grant(Samsung); Google Cloud(Google Incorporated)	We thank the anonymous reviewers, as well as Dan Weld, for their helpful feedback. Thanks also to Zak Stone and the Google Cloud TPU team for help with the computing infrastructure. This work was supported by the National Science Foundation through a Graduate Research Fellowship (DGE-1256082) and NSF grants (IIS-1524371, 1637479, 165205, 1703166), the DARPA CwC program through ARO (W911NF-15-1-0543), the Sloan Research Foundation through a Sloan Fellowship, the Allen Institute for Artificial Intelligence, the NVIDIA Artificial Intelligence Lab, Samsung through a Samsung AI research grant, and gifts by Google and Facebook. Computations on beaker.org were supported in part by credits from Google Cloud.	Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223; Bourassa N., 2017, PARTISANSHIP PROPAGA; Bradshaw Samantha, 2017, TECHNICAL REPORT; Caccia Massimo, 2018, ARXIV181102549; Chen DQ, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1870, DOI 10.18653/v1/P17-1171; Devlin J., 2019, P 2019 C N AM CHAPT, V1, P4171; Dicker Rachel, 2016, AVOID THESE FAKE NEW; Dwoskin E., 2018, WASH POST; Fan A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P889; Ghazvininejad Marjan, 2019, ARXIV190409324; Gu J., 2019, ARXIV190201370; Han, 2019, ARXIV190402817; Hashimoto Tatsunori B, 2019, ARXIV190402792; Hecht Brent, 2018, ITS TIME DO SOMETHIN; Holtzman Ari, 2019, P INT C LEARN REPR; Hosseini H, 2017, PROCEEDINGS OF THE 2017 WORKSHOP ON MULTIMEDIA PRIVACY AND SECURITY (MPS'17), P21, DOI 10.1145/3137616.3137618; Hu ZT, 2017, PR MACH LEARN RES, V70; Joulin A., 2017, P 15 C EUR CHAPT ASS, P427, DOI DOI 10.18653/V1/E17-2068; Kingma D.P, P 3 INT C LEARNING R; Lee C, 2016, INT C PATT RECOG, P745, DOI 10.1109/ICPR.2016.7899724; Melford Clare, 2019, TECHNICAL REPORT; Mosseri Adam, 2018, FACEBOOK NEWSROOM, V19; Ott M., 2011, 49 ANN M ASS COMPUTA, P309, DOI DOI 10.1145/2567948.2577293; Perez Veronica<prime>, 2018, P 27 INT C COMP LING, P3391; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Radford A., 2019, LANGUAGE MODELS ARE; Radford Alec, 2018, OPENAI BLOG; Ranzato M., 2016, ICLR; Rashkin Hannah, 2017, P 2017 C EMP METH NA, P2931, DOI DOI 10.18653/V1/D17-1317; Shazeer N, 2018, PR MACH LEARN RES, V80; Solaiman I., 2019, ARXIV190809203; Stern Mitchell, 2019, ARXIV190203249; Strobelt Hendrik, 2019, TECHNICAL REPORT; Swire B, 2017, J EXP PSYCHOL LEARN, V43, P1948, DOI 10.1037/xlm0000422; Vargo CJ, 2018, NEW MEDIA SOC, V20, P2028, DOI 10.1177/1461444817712086; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Vinyals Oriol, 2016, CORR; Wang A., 2018, P 2018 EMNLP WORKSH; Wang WY, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 2, P422, DOI 10.18653/v1/P17-2067; Wardle C., 2017, INFORM DISORDER INTE; Wardle C., 2017, 1 DRAFT NEWS, V16; Zellers R., 2018, P C EMPIRICAL METHOD, P93; Zellers Rowan, 2019, IEEE C COMP VIS PATT; Zellers Rowan, 2019, TECHNICAL REPORT; Zellers Rowan, 2019, P 57 ANN M ASS COMP	46	26	26	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900062
C	Zhang, MH; Jiang, SL; Cui, ZC; Garnett, R; Chen, YX		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Muhan; Jiang, Shali; Cui, Zhicheng; Garnett, Roman; Chen, Yixin			D-VAE: A Variational Autoencoder for Directed Acyclic Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BAYESIAN NETWORKS	Graph structured data are abundant in the real world. Among different graph types, directed acyclic graphs (DAGs) are of particular interest to machine learning researchers, as many machine learning models are realized as computations on DAGs, including neural networks and Bayesian networks. In this paper, we study deep generative models for DAGs, and propose a novel DAG variational autoencoder (D-VAE). To encode DAGs into the latent space, we leverage graph neural networks. We propose an asynchronous message passing scheme that allows encoding the computations on DAGs, rather than using existing simultaneous message passing schemes to encode local graph structures. We demonstrate the effectiveness of our proposed D-VAE through two tasks: neural architecture search and Bayesian network structure learning. Experiments show that our model not only generates novel and valid DAGs, but also produces a smooth latent space that facilitates searching for DAGs with better performance through Bayesian optimization.	[Zhang, Muhan; Jiang, Shali; Cui, Zhicheng; Garnett, Roman; Chen, Yixin] Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63110 USA	Washington University (WUSTL)	Zhang, MH (corresponding author), Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63110 USA.	muhan@wustl.edu; jiang.s@wustl.edu; z.cui@wustl.edu; garnett@wustl.edu; chen@cse.wustl.edu			National Science Foundation (NSF) [III-1526012, SCH-1622678]; National Institute of Health [1R21HS024581]; NSF [IIA-1355406, IIS-1845434, OAC-1940224]	National Science Foundation (NSF)(National Science Foundation (NSF)); National Institute of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF))	MZ, ZC and YC were supported by the National Science Foundation (NSF) under award numbers III-1526012 and SCH-1622678, and by the National Institute of Health under award number 1R21HS024581. SJ and RG were supported by the NSF under award numbers IIA-1355406, IIS-1845434, and OAC-1940224. The authors would like to thank Liran Wang for the helpful discussions.	Anderson Blake, 2009, FAST BAYESIAN NETWOR; [Anonymous], 2016, ARXIV PREPRINT ARXIV; Bowman S. R., 2016, ARXIV, P10; CAI H., 2018, P INT C LEARN REPR; Chickering D., 1995, P 5 C ART INT STAT, P112; Cho K., 2014, P 2014 C EMP METH NA, P1724; Chollet F., 2017, P IEEE C COMP VIS PA, P1251; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Dai H., 2018, P 6 INT C LEARN REPR; De Cao N., 2018, ARXIV180511973; Diederik P, 2013, P 2 INT C LEARNING R; Duvenaud David K, 2015, P NIPS; Elsken T., 2017, ARXIV171104528; Fusi N., 2018, ADV NEURAL INFORM PR, P3352, DOI 10.5555/3327144.3327254; Gao T., 2018, P 35 INT C MACH LEAR, P1685; Gao T, 2017, PR MACH LEARN RES, V70; Ginsbourger D, 2010, ADAPT LEARN OPTIM, V2, P131; Gomez-Bombarelli R, 2018, ACS CENTRAL SCI, V4, P268, DOI 10.1021/acscentsci.7b00572; Gunnemann, 2018, ARXIV180300816; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Hutter F., 2018, AUTOMATIC MACHINE LE; Jin WG, 2018, PR MACH LEARN RES, V80; Kai Sheng Tai, 2015, ARXIV150300075; Kandasamy K., 2018, ADV NEURAL INFORM PR; Kingma D.P, P 3 INT C LEARNING R; Kipf TN, 2016, P INT C LEARN REPR; Koller D., 2009, PROBABILISTIC GRAPHI; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Kusner MJ, 2017, PR MACH LEARN RES, V70; Kusner Matt J, 2016, ARXIV161104051; LAURITZEN SL, 1988, J ROY STAT SOC B MET, V50, P157; Leskovec J, 2018, ADV NEURAL INFORM PR, P6412; Li Y., 2018, ARXIV; Linzner D., 2018, ADV NEURAL INFORM PR, P7891; Liu H., 2018, ARXIV180609055; Liu H., 2017, ARXIV171100436; Liu Q., 2018, ARXIV180509076; Luo R, 2018, ADV NEURAL INFORM PR; Ma T., 2018, ARXIV180902630; Miikkulainen R, 2019, ARTIFICIAL INTELLIGENCE IN THE AGE OF NEURAL NETWORKS AND BRAIN COMPUTING, P293, DOI 10.1016/B978-0-12-815480-9.00015-3; Mueller J, 2017, PR MACH LEARN RES, V70; Niepert M, 2016, PR MACH LEARN RES, V48; Pham H, 2018, 35 INT C MACH LEARN; Real E., 2017, P MACH LEARN RES, V84, P2902; Rezende D.J., 2014, PROC INT CONFER ENCE; Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093; Scutari M, 2010, J STAT SOFTW, V35, P1, DOI 10.18637/jss.v035.i03; Silander T., 2018, AISTATS, P948; Simonovsky Martin, 2018, ARXIV180203480; Singh A. P., 2005, FINDING OPTIMAL BAYE; Snelson Edward, 2006, ADV NEURAL INFORM PR, V3; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; WEININGER D, 1988, J CHEM INF COMP SCI, V28, P31, DOI 10.1021/ci00057a005; White T., 2016, ARXIV PREPRINT ARXIV; Wu Z., 2019, 190100596 ARXIV; Xu K., 2018, INT C LEARN REPR; Yackley Benjamin, 2012, INT C MACH LEARN; You J., 2018, ICML, P5694; Yu Y., 2019, ARXIV190410098 ARXIV190410098; Yuan C., 2011, INT JOINT C ARTIFICI, P2186; Yuan CH, 2013, J ARTIF INTELL RES, V48, P23, DOI 10.1613/jair.4039; Zhang M., 2018, 32 AAAI C ART INT; Zhang M., 2018, ARXIV180209691CSSTAT; Zhang Muhan, 2019, ARXIV190412058; Zheng Xun, 2018, ADV NEURAL INFORM PR, P9472; Zoller MA, 2019, ARXIV190412054; Zoph B., 2016, ICLR; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	71	26	26	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301056
C	Dash, S; Gunluk, O; Wei, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dash, Sanjeeb; Gunluk, Oktay; Wei, Dennis			Boolean Decision Rules via Column Generation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				LOGICAL ANALYSIS; FRAMEWORK	This paper considers the learning of Boolean rules in either disjunctive normal form (DNF, OR-of-ANDs, equivalent to decision rule sets) or conjunctive normal form (CNF, AND-of-ORs) as an interpretable model for classification. An integer program is formulated to optimally trade classification accuracy for rule simplicity. Column generation (CG) is used to efficiently search over an exponential number of candidate clauses (conjunctions or disjunctions) without the need for heuristic rule mining This approach also bounds the gap between the selected rule set and the best possible rule set on the training data. To handle large datasets, we propose an approximate CG algorithm using randomization. Compared to three recently proposed alternatives, the CG algorithm dominates the accuracy-simplicity tradeoff in 8 out of 16 datasets. When maximized for accuracy, CG is competitive with rule learners designed for this purpose, sometimes finding significantly simpler solutions that are no less accurate.	[Dash, Sanjeeb; Gunluk, Oktay; Wei, Dennis] IBM Res, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Dash, S (corresponding author), IBM Res, Yorktown Hts, NY 10598 USA.	sanjeebd@us.ibm.com; gunluk@us.ibm.com; dwei@us.ibm.com		Gunluk, Oktay/0000-0002-9272-377X				Agrawal R., 1994, P 20 INT C VER LARG, P487; Angelino E, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P35, DOI 10.1145/3097983.3098047; Barnhart C, 1998, OPER RES, V46, P316, DOI 10.1287/opre.46.3.316; Bazaraa MS, 2010, LINEAR PROGRAMMING N, V4th; Bertsimas D, 2017, MACH LEARN, V106, P1039, DOI 10.1007/s10994-017-5633-9; Bi J., 2004, P 10 ACM SIGKDD INT, P521; Borgelt C., 2005, P 1 INT WORKSHOP OPE, P1, DOI DOI 10.1145/1133905.1133907; Boros E, 2000, IEEE T KNOWL DATA EN, V12, P292, DOI 10.1109/69.842268; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breiman L., 2017, CLASSIFICATION REGRE; Chen GQ, 2006, DECIS SUPPORT SYST, V42, P674, DOI 10.1016/j.dss.2005.03.005; Cheng H, 2007, PROC INT CONF DATA, P691; Clark P., 1989, Machine Learning, V3, P261, DOI 10.1007/BF00116835; Clark P., 1991, P 5 EUR WORK SESS LE, P151, DOI [DOI 10.1007/BFB0017011, 10.1007/bfb0017011]; Cohen W.W., 1995, P 12 INT C MACH LEAR, P115, DOI DOI 10.1016/B978-1-55860-377-6.50023-2; Cohen WW, 1999, SIXTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-99)/ELEVENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE (IAAI-99), P335; Conforti M, 2014, GRAD TEXTS MATH, V271, P1, DOI 10.1007/978-3-319-11008-0; Dash Sanjeeb, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P3360, DOI 10.1109/ICASSP.2014.6854223; Dembczynski K, 2010, DATA MIN KNOWL DISC, V21, P52, DOI 10.1007/s10618-010-0177-7; Demiriz A, 2002, MACH LEARN, V46, P225, DOI 10.1023/A:1012470815092; Domingos P, 1996, MACH LEARN, V24, P141; Dua D., 2017, UCI MACHINE LEARNING, DOI DOI 10.1002/JCC.23219; Feldman Vitaly, 2012, P C LEARN THEOR COLT; Fiirnkranz J., 2014, FDN RULE LEARNING; Frank E., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P144; Freitas A, 2014, ACM SIGKDD EXPLORATI, V15, P1, DOI DOI 10.1145/2594473.2594475; Friedman JH, 1999, STAT COMPUT, V9, P123, DOI 10.1023/A:1008894516817; Frieman JH, 2008, ANN APPL STAT, V2, P916, DOI 10.1214/07-AOAS148; Hammer PL, 2006, ANN OPER RES, V148, P203, DOI 10.1007/s10479-006-0075-y; Klivans AR, 2004, J COMPUT SYST SCI, V68, P303, DOI 10.1016/j.jcss.2003.07.007; Lakkaraju H, 2017, PR MACH LEARN RES, V54, P166; Letham B, 2015, ANN APPL STAT, V9, P1350, DOI 10.1214/15-AOAS848; Li WM, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P369, DOI 10.1109/ICDM.2001.989541; Li X., 2013, P 30 INT C MACH LEAR; Liu B., 1998, P 4 INT C KNOWL DISC, P80; Malioutov D., 2013, P INT C MACH LEARN, P765; Marchand M, 2003, J MACH LEARN RES, V3, P723, DOI 10.1162/jmlr.2003.3.4-5.723; Muselli M, 2002, IEEE T KNOWL DATA EN, V14, P1258, DOI 10.1109/TKDE.2002.1047766; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Quinlan J. R., 1993, C4 5 PROGRAMS MACHIN; Rivest R. L., 1987, Machine Learning, V2, P229, DOI 10.1023/A:1022607331053; SALZBERG S, 1991, MACH LEARN, V6, P251, DOI 10.1007/BF00114779; Su GL, 2016, IEEE INT WORKS MACH; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Vanderbeck F, 1996, OPER RES LETT, V19, P151, DOI 10.1016/0167-6377(96)00033-8; Wang F, 2015, JMLR WORKSH CONF PRO, V38, P1013; Wang JY, 2005, SIAM PROC S, P205; Wang T, 2017, J MACH LEARN RES, V18, P1; Wang Tong, 2015, ARXIV151102210; Witten IH, 2011, MOR KAUF D, P1; Yang HY, 2017, PR MACH LEARN RES, V70; Yin XX, 2003, SIAM PROC S, P331	53	26	26	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304065
C	Kosiorek, AR; Kim, H; Posner, I; Teh, YW		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kosiorek, Adam R.; Kim, Hyunjik; Posner, Ingmar; Teh, Yee Whye			Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present Sequential Attend, Infer, Repeat (SQAIR), an interpretable deep generative model for videos of moving objects. It can reliably discover and track objects throughout the sequence of frames, and can also generate future frames conditioning on the current frame, thereby simulating expected motion of objects. This is achieved by explicitly encoding object presence, locations and appearances in the latent variables of the model. SQAIR retains all strengths of its predecessor, Attend, Infer, Repeat (AIR, Eslami et al., 2016), including learning in an unsupervised manner, and addresses its shortcomings. We use a moving multi MNIST dataset to show limitations of AIR in detecting overlapping or partially occluded objects, and show how SQAIR overcomes them by leveraging temporal consistency of objects. Finally, we also apply SQAIR to real world pedestrian CCTV data, where it learns to reliably detect, track and generate walking pedestrians with no supervision.	[Kosiorek, Adam R.; Posner, Ingmar] Univ Oxford, Oxford Robot Inst, Appl Artificial Intelligence Lab, Oxford, England; [Kosiorek, Adam R.; Kim, Hyunjik; Teh, Yee Whye] Univ Oxford, Dept Stat, Oxford, England	University of Oxford; University of Oxford	Kosiorek, AR (corresponding author), Univ Oxford, Oxford Robot Inst, Appl Artificial Intelligence Lab, Oxford, England.; Kosiorek, AR (corresponding author), Univ Oxford, Dept Stat, Oxford, England.	adamk@robots.ox.ac.uk			European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant [617071]	European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant(European Research Council (ERC))	We would like to thank Ali Eslami for his help in implementing AIR, Alex Bewley and Martin Engelcke for discussions and valuable insights and anonymous reviewers for their constructive feedback. Additionally, we acknowledge that HK and YWT's research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617071.	[Anonymous], 2017, NIPS; Bewley A., 2016, ICIP; Burda Y., 2016, ICLR; Cho M., 2015, CORR; Chung J., 2015, NIPS; Clevert Djork-Arne, 2015, CORR; Courville A., 2016, CORR; Denton Emily, 2018, ICML; Eslami S. M. A., 2016, NIPS 2016, P3; Gael J. V, 2009, NIPS; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Greff Klaus, 2017, NIPS; Greff Klaus, 2016, NIPS; Ha D., 2018, CORR; Ilin A., 2017, NIPS; Jacobsen J. H., 2016, CVPR; Kavukcuoglu K, 2015, ADV NEURAL INF PROCE, P2017; Kemp C., 2008, P NATL ACAD SCI USA, V105, P31; Kingma D.P., 2015, INT C LEARN REPR, P1; Kingma DP, 2 INT C LEARN REPR I, P1; Kosiorek A., 2017, NIPS; Kwak S., 2015, ICCV; LeCun Y., 1989, NEURAL COMPUTATION, V1; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Levine S., 2017, CORR; Maddison Chris J, 2017, ADV NEURAL INFORM PR; Mnih A, 2018, ICML; Mnih A., 2016, ICML; Mnih A., 2014, ICML; Neiswanger W., 2012, CORR; Niebles J. C., 2018, NIPS; Ranzato M., 2014, CORR; Ristani E, 2016, ECCV; Santoro A, 2017, NIPS; Schulter S., 2017, CVPR; Shi W., 2016, CVPR; Srivastava Nitish, 2015, ICML; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tulyakov S., 2018, CVPR; Valmadre J, 2017, CVPR; van den Oord Aron, 2016, NIPS; Weber T., 2017, NIPS; Xiao F., 2016, CVPR; Zaheer Manzil, 2017, NIPS	45	26	26	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003019
C	Li, CY; Liang, XD; Hu, ZT; Xing, EP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Christy Y.; Liang, Xiaodan; Hu, Zhiting; Xing, Eric P.			Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Generating long and coherent reports to describe medical images poses challenges to bridging visual patterns with informative human linguistic descriptions. We propose a novel Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) which reconciles traditional retrieval-based approaches populated with human prior knowledge, with modern learning-based approaches to achieve structured, robust, and diverse report generation. HRGR-Agent employs a hierarchical decision-making procedure. For each sentence, a high-level retrieval policy module chooses to either retrieve a template sentence from an off-the-shelf template database, or invoke a low-level generation module to generate a new sentence. HRGR-Agent is updated via reinforcement learning, guided by sentence-level and word-level rewards. Experiments show that our approach achieves the state-of-the-art results on two medical report datasets, generating well-balanced structured sentences with robust coverage of heterogeneous medical report contents. In addition, our model achieves the highest detection precision of medical abnormality terminologies, and improved human evaluation performance.	[Li, Christy Y.] Duke Univ, Durham, NC 27706 USA; [Liang, Xiaodan; Hu, Zhiting] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Li, Christy Y.; Xing, Eric P.] Petuum Inc, Pittsburgh, PA USA	Duke University; Carnegie Mellon University	Liang, XD (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	yl558@duke.edu; xiaodan1@cs.cmu.edu; zhitingh@cs.cmu.edu; epxing@cs.cmu.edu						Abigail See C. D. M., 2017, ACL; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2018, JIEBA CHINESE STUTTE; Bahdanau Dzmitry, 2017, ICLR; Banerjee Satanjeev, 2005, P ACL WORKSH INTR EX, P65; Bosmans JML, 2011, RADIOLOGY, V259, P184, DOI 10.1148/radiol.10101045; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Cohen R, 2016, ARXIV PREPRINT ARXIV; Demner-Fushman Dina, 2015, J AM MED INFORM ASS; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Goergen S. K., 2013, J MED IMAG RADIAT ON; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hinton, 1993, ADV NEURAL INFORM PR, V5, P271; Hong Yi, 2013, J Digit Imaging, V26, P843, DOI 10.1007/s10278-013-9597-4; Hu ZT, 2017, PR MACH LEARN RES, V70; Hu Zhiting, 2018, ARXIV180900794; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jing BY, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2577; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Li L., 2017, INT C LEARN REPR; Liang X., 2017, ARXIV170800315; Lin C.-Y., 2013, ACL; Liu SQ, 2017, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2017.100; Liu Y, 2017, AAAI CONF ARTIF INTE, P1445; Lu J., 2018, CVPR; Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Paulus R., 2018, ICLR; Ranzato M, 2016, ICLR; Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Tan B., 2018, CONNECTING DOTS MLE; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wang X., 2018, ACL; Wang XS, 2017, PROC CVPR IEEE, P3462, DOI 10.1109/CVPR.2017.369; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wiseman Sam, 2017, P 2017 C EMP METH NA; Wu Y., 2016, ADV NEURAL INFORM PR, V1609, DOI 10.48550/arXiv.1609.08144; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yarats D., 2017, EMNLP; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Ziqiang Cao S. L., 2018, ACL	44	26	26	2	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301051
C	Mutny, M; Krause, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mutny, Mojmir; Krause, Andreas			Efficient High Dimensional Bayesian Optimization with Additivity and Quadrature Fourier Features	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We develop an efficient and provably no-regret Bayesian optimization (BO) algorithm for optimization of black-box functions in high dimensions. We assume a generalized additive model with possibly overlapping variable groups. When the groups do not overlap, we are able to provide the first provably no-regret polynomial time (in the number of evaluations of the acquisition function) algorithm for solving high dimensional BO. To make the optimization efficient and feasible, we introduce a novel deterministic Fourier Features approximation based on numerical integration with detailed analysis for the squared exponential kernel. The error of this approximation decreases exponentially with the number of features, and allows for a precise approximation of both posterior mean and variance. In addition, the kernel matrix inversion improves in its complexity from cubic to essentially linear in the number of data points measured in basic arithmetic operations.	[Mutny, Mojmir; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Mutny, M (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	mojmir.mutny@inf.ethz.ch; krausea@inf.ethz.ch		Krause, Andreas/0000-0001-7260-9673	SNSF through the NRP 75 Big Data program [407540_167212]	SNSF through the NRP 75 Big Data program	This research was supported by SNSF grant 407540_167212 through the NRP 75 Big Data program. The authors would like to thank Johannes Kirschner for valuable discussions. In addition, we thank SwissFEL team for provision of the preliminary data from the free electron laser. In particular we thank Nicole Hiller, Franziska Frei and Rasmus Ischebeck of Paul Scherrer Institute, Switzerland.	Abbasi-Yadkori Y., 2012, THESIS U ALBERTA; Ambikasaran S, 2016, IEEE T PATTERN ANAL, V38, P252, DOI 10.1109/TPAMI.2015.2448083; [Anonymous], 2010, INT C MACH LEARN; Avron H, 2016, J MACH LEARN RES, V17; Balog M, 2016, P 32 C UNC ART INT N, P32; Bo Chen, 2012, JOINT OPTIMIZATION V; Boyd J. P., 1987, Journal of Scientific Computing, V2, P99, DOI 10.1007/BF01061480; Boyd J.P., 2000, CHEBYSHEV FOURIER SP; Brochu E, 2010, ARXIV PREPRINT ARXIV; Dao Tri, 2017, Adv Neural Inf Process Syst, V30, P6109; Diethelm K, 2014, SIAM J NUMER ANAL, V52, P877, DOI 10.1137/130921246; Duvenaud D.K., 2011, ADV NEURAL INFORM PR, P226; Gardner JR, 2017, PR MACH LEARN RES, V54, P1311; Gopalan A., 2015, P 28 C LEARNING THEO, P861; Gopalan Aditya, 2017, INT C MACH LEARN; HASTIE T. J., 1990, MONOGRAPHS STAT APPL, V43; Hennig P, 2012, J MACH LEARN RES, V13, P1809; Hensman J., 2013, UNCERTAINTY ARTIFICI; Hensman J, 2018, J MACH LEARN RES, V18, P1; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Hernandez-Lobato Jose Miguel, 2017, INT C MACH LEARN; Hildebrand F.B., 1987, INTRO NUMERICAL ANAL, V2nd ed.; JONES D. R., 2001, ENCY OPTIMIZATION, P431, DOI DOI 10.1007/0-306-48332-7_93; Kandasamy K, 2015, PR MACH LEARN RES, V37, P295; Klein Aaron, 2017, INT C ART INT STAT; Laurent B, 2000, ANN STAT, V28, P1302; Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865; Lizotte D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P944; McIntire M, 2016, UAI; McWilliams B., 2013, ADV NEURAL INFORM PR, P440; Mutny M, 2018, J COMPUT MATH, V36, P404, DOI 10.4208/jcm.1708-m2017-0113; Nesterov Y., 2004, INTRO CONVEX OPTIMIZ; Ong C. S., 2011, ADV NEURAL INFORM PR, P2447; Pleiss Geoff, 2018, INT C MACH LEARN; Rahimi A., 2007, NIPS, V3, P5; Rahimi A, 2008, ANN ALLERTON CONF, P555, DOI 10.1109/ALLERTON.2008.4797607; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Ravikumar P, 2007, ADV NEURAL INF PROCE, V20, P1201; Rolland P, 2018, PR MACH LEARN RES, V84; Rudin W, 1976, INT SERIES PURE APPL; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27; Scott SL, 2015, APPL STOCH MODEL BUS, V31, P37, DOI 10.1002/asmb.2104; Snelson Edward, 2006, ADV NEURAL INFORM PR, V3; Snoek J, 2015, PR MACH LEARN RES, V37, P2171; Springenberg J.T., 2016, P ADV NEURAL INFORM, P4134; Sriperumbudur Bharath, 2015, ADV NEURAL INFORM PR, P1144; Stoer J., 1983, INTRO NUMERICAL ANAL; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Wang ZY, 2016, J ARTIF INTELL RES, V55, P361, DOI 10.1613/jair.4806; Williams CKI, 2001, ADV NEUR IN, V13, P682; Wilson A.G., 2015, ARXIV151101870; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775; Zi Wang, 2017, INT C MACH LEARN; Zi Wang, 2018, INT C ART INT STAT; Zi Wang, 2017, INT C ART INT STAT	59	26	26	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003055
C	Shetty, R; Fritz, M; Schiele, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shetty, Rakshith; Fritz, Mario; Schiele, Bernt			Adversarial Scene Editing: Automatic Object Removal from Weak Supervision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					While great progress has been made recently in automatic image manipulation, it has been limited to object centric images like faces or structured scene datasets. In this work, we take a step towards general scene-level image editing by developing an automatic interaction-free object removal model. Our model learns to find and remove objects from general scene images using image-level labels and unpaired data in a generative adversarial network (GAN) framework. We achieve this with two key contributions: a two-stage editor architecture consisting of a mask generator and image in-painter that co-operate to remove objects, and a novel GAN based prior for the mask generator that allows us to flexibly incorporate knowledge about object shapes. We experimentally show on two datasets that our method effectively removes a wide variety of objects using weak supervision only.	[Shetty, Rakshith; Schiele, Bernt] Max Planck Inst Informat, Saarland Informat Campus, Saarbrucken, Germany; [Fritz, Mario] CISPA Helmholtz Ctr iG, Saarland Informat Campus, Saarbrucken, Germany	Max Planck Society	Shetty, R (corresponding author), Max Planck Inst Informat, Saarland Informat Campus, Saarbrucken, Germany.	rakshith.shetty@mpi-inf.mpg.de; mario.fritz@cispa.saarland; bernt.schiele@mpi-inf.mpg.de			German Research Foundation [DFG CRC 1223]	German Research Foundation(German Research Foundation (DFG))	This research was supported in part by the German Research Foundation (DFG CRC 1223).	Arjovsky M, 2017, PR MACH LEARN RES, V70; Chen X, 2015, CORR, V1504, P325; Choi Y., 2017, ARXIV171109020; Everingham M., 2012, PASCAL VISUAL OBJECT; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K., 2017, P IEEE INT C COMP VI, P2961, DOI DOI 10.1109/ICCV.2017.322; Huang R., 2017, P IEEE C COMP VIS PA; Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133; Kalantidis Y., 2011, P 1 ACM INT C MULT R, P20; Khoreva A, 2017, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2017.181; Lample Guillaume, 2017, ARXIV170600409; Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6; Mao Xudong, 2017, ICCV, DOI [10.1109/ICCV.2017.304, DOI 10.1109/ICCV.2017.304]; Mirkamali S. S., 2015, SIGNAL IMAGE VIDEO P; Mirza Mehdi, 2014, ARXIV14111784, P2672; Orekondy T., 2017, ARXIV171201066; Reed S, 2016, PR MACH LEARN RES, V48; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xu T, 2017, ARXIV171110485; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhu Jun-Yan, 2017, ICCV	32	26	26	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002027
C	Sun, SY; Pang, JM; Shi, JP; Yi, S; Ouyang, WL		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sun, Shuyang; Pang, Jiangmiao; Shi, Jianping; Yi, Shuai; Ouyang, Wanli			Fish Net: A Versatile Backbone for Image, Region, and Pixel Level Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The basic principles in designing convolutional neural network (CNN) structures for predicting objects on different levels, e.g., image-level, region-level, and pixel-level, are diverging. Generally, network structures designed specifically for image classification are directly used as default backbone structure for other tasks including detection and segmentation, but there is seldom backbone structure designed under the consideration of unifying the advantages of networks designed for pixel-level or region-level predicting tasks, which may require very deep features with high resolution. Towards this goal, we design a fish-like network, called FishNet. In FishNet, the information of all resolutions is preserved and refined for the final task. Besides, we observe that existing works still cannot directly propagate the gradient information from deep layers to shallow layers. Our design can better handle this problem. Extensive experiments have been conducted to demonstrate the remarkable performance of the FishNet. In particular, on ImageNet- lk, the accuracy of FishNet is able to surpass the performance of DenseNet and ResNet with fewer parameters. FishNet was applied as one of the modules in the winning entry of the COCO Detection 2018 challenge. https://github.com/kevin-ssy/FishNet.	[Sun, Shuyang; Ouyang, Wanli] Univ Sydney, Sydney, NSW, Australia; [Shi, Jianping; Yi, Shuai] SenseTime Res, Sydney, NSW, Australia; [Pang, Jiangmiao] Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China	University of Sydney; Zhejiang University	Sun, SY (corresponding author), Univ Sydney, Sydney, NSW, Australia.	shuyang.sun@sydney.edu.au						Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], [No title captured]; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Chen Kai, 2018, MMDETECTION; Chu Xiao, 2016, ADV NEURAL INFORM PR, P316; Gao P., 2018, ARXIV180802632; Goyal Priya, 2017, ARXIV170602677; Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang G., 2017, ABS170309844 CORR; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jacobsen J.H., 2018, ICLR; Jie H., 2017, P IEEE C COMP VIS PA, P99; Kim E, 2018, PROC CVPR IEEE, P8669, DOI 10.1109/CVPR.2018.00904; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Larsson G., 2016, ARXIV PREPRINT ARXIV; Li HY, 2019, INT J COMPUT VISION, V127, P225, DOI 10.1007/s11263-018-1101-7; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sun SY, 2018, PROC CVPR IEEE, P1390, DOI 10.1109/CVPR.2018.00151; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144; Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75; Yu Fisher, 2017, CVPR; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zeng XY, 2016, LECT NOTES COMPUT SC, V9911, P354, DOI 10.1007/978-3-319-46478-7_22; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhou H., 2018, IEEE T CIRCUITS SYST	36	26	29	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300070
C	Volpi, R; Namkoong, H; Sener, O; Duchi, J; Murino, V; Savarese, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Volpi, Riccardo; Namkoong, Hongseok; Sener, Ozan; Duchi, John; Murino, Vittorio; Savarese, Silvio			Generalizing to Unseen Domains via Adversarial Data Augmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We are concerned with learning models that generalize well to different unseen domains. We consider a worst-case formulation over data distributions that are near the source domain in the feature space. Only using training data from a single source distribution, we propose an iterative procedure that augments the dataset with examples from a fictitious target domain that is "hard" under the current model. We show that our iterative scheme is an adaptive data augmentation method where we append adversarial examples at each iteration. For softmax losses, we show that our method is a data-dependent regularization scheme that behaves differently from classical regularizers that regularize towards zero (e.g., ridge or lasso). On digit recognition and semantic segmentation tasks, our method learns models improve performance across a range of a priori unknown target domains.	[Volpi, Riccardo; Murino, Vittorio] Ist Italiano Tecnol, Genoa, Italy; [Namkoong, Hongseok; Duchi, John; Savarese, Silvio] Stanford Univ, Stanford, CA 94305 USA; [Sener, Ozan] Intel Labs, Santa Clara, CA USA; [Murino, Vittorio] Univ Verona, Verona, Italy	Istituto Italiano di Tecnologia - IIT; Stanford University; Intel Corporation; University of Verona	Volpi, R (corresponding author), Ist Italiano Tecnol, Genoa, Italy.		Sener, Ozan/ABF-9436-2020					Ben-David Shai, 2007, NEURIPS, P7; Blanchet  Jose, 2016, ARXIV160401446MATHPR; Blitzer J., 2006, P 2006 C EMP METH NA, P120, DOI DOI 10.3115/1610075.1610094; Bonnans J., 2013, PERTURBATION ANAL OP; Boyd S, 2004, CONVEX OPTIMIZATION; DENKER JS, 1989, ADV NEURAL INFORMATI, V1, P323; Dosovitskiy Alexey, 2016, NEURIPS; Everingham M., PASCAL VISUAL OBJECT; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Goodfellow I. J., 2015, P ICLR; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heinze-Deml C., 2017, ARXIV171011469; Hendrycks  Dan, 2016, ABS161002136 CORR; Hoffman  Judy, 2016, ABS161202649 CORR; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kingma D.P, P 3 INT C LEARNING R; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee J., 2017, ARXIV170507815; Levenberg K., 1944, Q APPL MATH, V2, P164, DOI 10.1090/qam/10666; Li  Da, 2017, ABS171003077 CORR; Long J., 2014, ABS14114038 CORR; Mancini M., 2018, IEEE ROBOTICS AUTOMA, V3, P2093, DOI DOI 10.1109/LRA.2018.2809700; Marcu Daniel, 2011, ABS11096341 CORR; MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030; Morerio P., 2018, INT C LEARN REPR; Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609; Muandet Krikamol, 2013, ICML; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Ros G., 2016, IEEE C COMP VIS PATT; Saenko K., 2016, ECCV WORKSH; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Shankar Shiv, 2018, ICLR; Sinha A., 2018, ICLR; Srivastava N., 2014, J MACH LEARN RES, V15, P1929; Tobin  Joshua, 2017, ABS170306907 CORR; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Tzeng E., 2017, IEEE C COMP VIS PATT; Volpi  Riccardo, 2018, IEEE C COMP VIS PATT	40	26	26	1	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305036
C	Elenberg, ER; Dimakis, AG; Feldman, M; Karbasi, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Elenberg, Ethan R.; Dimakis, Alexandros G.; Feldman, Moran; Karbasi, Amin			StreamingWeak Submodularity: Interpreting Neural Networks on the Fly	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SUBSET-SELECTION; APPROXIMATIONS; BOUNDS	In many machine learning applications, it is important to explain the predictions of a black-box classifier. For example, why does a deep neural network assign an image to a particular class? We cast interpretability of black-box classifiers as a combinatorial maximization problem and propose an efficient streaming algorithm to solve it subject to cardinality constraints. By extending ideas from Badanidiyuru et al. [2014], we provide a constant factor approximation guarantee for our algorithm in the case of random stream order and a weakly submodular objective function. This is the first such theoretical guarantee for this general class of functions, and we also show that no such algorithm exists for a worst case stream order. Our algorithm obtains similar explanations of Inception V3 predictions 10 times faster than the state-of-the-art LIME framework of Ribeiro et al. [2016].	[Elenberg, Ethan R.; Dimakis, Alexandros G.] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA; [Feldman, Moran] Open Univ Israel, Dept Math & Comp Sci, Raanana, Israel; [Karbasi, Amin] Yale Univ, Dept Elect Engn, Dept Comp Sci, New Haven, CT 06520 USA	University of Texas System; University of Texas Austin; Open University Israel; Yale University	Elenberg, ER (corresponding author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.	elenberg@utexas.edu; dimakis@austin.utexas.edu; moranfe@openu.ac.il; amin.karbasi@yale.edu	Dimakis, Alexandros G/P-6034-2019; Gurfil, Pini/AAA-4072-2020; Dimakis, Alexandros G/A-5496-2011	Dimakis, Alexandros G/0000-0002-4244-7033; Dimakis, Alexandros G/0000-0002-4244-7033; Feldman, Moran/0000-0002-1535-2979	NSF [CCF 1344364, 1407278, 1422549, 1618689]; ARO [YIP W911NF-14-1-0258]; ISF [1357/16]; Google Faculty Research Award; DARPA Young Faculty Award [D16AP00046]	NSF(National Science Foundation (NSF)); ARO; ISF(Israel Science Foundation); Google Faculty Research Award(Google Incorporated); DARPA Young Faculty Award	This research has been supported by NSF Grants CCF 1344364, 1407278, 1422549, 1618689, ARO YIP W911NF-14-1-0258, ISF Grant 1357/16, Google Faculty Research Award, and DARPA Young Faculty Award (D16AP00046).	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Altschuler J, 2016, PR MACH LEARN RES, V48; Bach F., 2013, FDN TRENDS MACHINE L, V6; Badanidiyuru A, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P671, DOI 10.1145/2623330.2623637; Bahmani S, 2013, J MACH LEARN RES, V14, P807; Barbosa R, 2015, PR MACH LEARN RES, V37, P1236; Barbosa RD, 2016, ANN IEEE SYMP FOUND, P645, DOI 10.1109/FOCS.2016.74; Bian AA, 2017, PR MACH LEARN RES, V54, P111; Buchbinder N., 2015, P ACM SIAM S DISCRET, P1202; Buchbinder N., 2016, P 27 ANN ACM SIAM S, P392; Buchbinder Niv, 2016, ABS161103253 CORR; Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991; CHAN THH, 2017, SODA, P1204; Chekuri C, 2015, LECT NOTES COMPUT SC, V9134, P318, DOI 10.1007/978-3-662-47672-7_26; CONFORTI M, 1984, DISCRETE APPL MATH, V7, P251, DOI 10.1016/0166-218X(84)90003-9; Das A, 2011, P 28 INT C INT C MAC; Elenberg Ethan R., 2016, NIPS WORKSH LEARN HI; Elenberg Ethan R., 2016, ABS161200804 CORR; Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059; FISHER ML, 1978, MATH PROGRAM STUD, V8, P73, DOI 10.1007/BFb0121195; Hassidim A., 2017, COLT, P1069; Hoi SCH, 2006, P 23 INT C MACH LEAR, V148, P417, DOI [10.1145/1143844.1143897, DOI 10.1145/1143844.1143897]; Horel T., 2016, NIPS; Khanna R, 2017, PR MACH LEARN RES, V70; Khanna R, 2017, PR MACH LEARN RES, V54, P1560; Krause A, 2014, TRACTABILITY, P71; Krause Andreas, 2010, ICML, P567; Lichman M, 2013, UCI MACHINE LEARNING; Mirzasoleiman B, 2015, AAAI CONF ARTIF INTE, P1812; Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Pan Xinghao, 2014, ADV NEURAL INFORM PR, P118; Ribeiro Marco Tulio, 2016, P KDD, P97, DOI [10.18653/v1/n16-3020, DOI 10.1145/2939672.2939778]; Sundararajan M, 2017, PR MACH LEARN RES, V70; Sviridenko M., 2015, PROC 26 ACM SIAM S D, P1134; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Vondrak Jan, 2010, RIMS KOKYUROKU BES B, VB23, P253; Wei K, 2015, PR MACH LEARN RES, V37, P1954; Yang Z., 2016, INT C MACH LEARN, P2472	40	26	26	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404012
C	Papamakarios, G; Pavlakou, T; Murray, I		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Papamakarios, George; Pavlakou, Theo; Murray, Iain			Masked Autoregressive Flow for Density Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.	[Papamakarios, George; Pavlakou, Theo; Murray, Iain] Univ Edinburgh, Edinburgh, Midlothian, Scotland	University of Edinburgh	Papamakarios, G (corresponding author), Univ Edinburgh, Edinburgh, Midlothian, Scotland.	g.papamakarios@ed.ac.uk; theo.pavlakou@ed.ac.uk; i.murray@ed.ac.uk	Jeong, Yongwook/N-7413-2016		Centre for Doctoral Training in Data Science - EPSRC [EP/L016427/1]; University of Edinburgh; Microsoft Research	Centre for Doctoral Training in Data Science - EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); University of Edinburgh; Microsoft Research(Microsoft)	We thank Maria Gorinova for useful comments. George Papamakarios and Theo Pavlakou were supported by the Centre for Doctoral Training in Data Science, funded by EPSRC (grant EP/L016427/1) and the University of Edinburgh. George Papamakarios was also supported by Microsoft Research through its PhD Scholarship Programme.	[Anonymous], 2017, P 5 INT C LEARN REPR; [Anonymous], 2016, ARXIV160502688 THEAN; Balle J., 2016, P 4 INT C LEARN REPR; Chen SSB, 2001, ADV NEUR IN, V13, P423; Chu Z, 2017, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS (ICT 2017); Dinh L., 2014, ARXIV; Fan Y, 2013, STAT-US, V2, P34, DOI 10.1002/sta4.15; Germain M, 2015, PR MACH LEARN RES, V37, P881; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gu S. S., 2015, ADV NEURAL INFORM PR, P2629; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kingma D.P., 2015, INT C LEARN REPR, P1; Kingma D.P., 2014, P 2 INT C LEARN REPR, DOI DOI 10.1093/BIOINFORMATICS/BTAA169; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kulkarni TD, 2015, PROC CVPR IEEE, P4390, DOI 10.1109/CVPR.2015.7299068; Loaiza-Ganem G., 2017, P 5 INT C LEARN REPR; Luxburg U. V., 2016, ADV NEURAL INFORM PR, V29, P4790; Oord A. v. d., 2016, ARXIV160903499; Paige B., 2016, P 33 INT C MACH LEAR; Papamakarios G., 2015, PROB INT WORKSH NEUR, P28; Papamakarios G, 2016, ADV NEUR IN, V29; Rezende D., 2015, ICML, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Rippel O., 2013, ARXIV13025125; Salimans T., 2017, INT C LEARNING REPRE; Tang Y., 2012, P 29 INT C MACH LEAR, P505; Theis L, 2015, ADV NEURAL INFORM PR, P1927; Theis L., 2016, P 4 INT C LEARN REPR; Uria B, 2016, J MACH LEARN RES, V17; Uria B, 2015, INT CONF ACOUST SPEE, P4465, DOI 10.1109/ICASSP.2015.7178815; Uria B, 2014, PR MACH LEARN RES, V32; Uria Benigno, 2013, P 26 INT C NEURAL IN; van den Oord A, 2016, PR MACH LEARN RES, V48; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	36	26	26	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402038
C	Srivastava, A; Valkov, L; Russell, C; Gutmann, MU; Sutton, C		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Srivastava, Akash; Valkov, Lazar; Russell, Chris; Gutmann, Michael U.; Sutton, Charles			VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.	[Srivastava, Akash; Valkov, Lazar; Gutmann, Michael U.; Sutton, Charles] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland; [Russell, Chris] Alan Turing Inst, London, England; [Sutton, Charles] Univ Edinburgh, Alan Turing Inst, Edinburgh, Midlothian, Scotland	University of Edinburgh; University of Edinburgh	Srivastava, A (corresponding author), Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.	akash.srivastava@ed.ac.uk; L.Valkov@sms.ed.ac.uk; crussell@turing.ac.uk; Michael.Gutmann@ed.ac.uk; csutton@inf.ed.ac.uk	Jeong, Yongwook/N-7413-2016; Russell, Chris/AAU-6819-2021	Russell, Chris/0000-0002-1942-7296				[Anonymous], 2017, INT C LEARN REPR ICL; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Donahue J., 2017, INT C LEARN REPR ICL; Dumoulin V., 2017, INT C LEARN REPR ICL; Dutta R., 2016, LIKELIHOOD FREE INFE; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gutmann M. U., 2011, P C UNC ART INT UAI, P283; Gutmann MU, 2012, J MACH LEARN RES, V13, P307; Gutmann Michael U, 2014, ARXIV14074981; Kingma D.P., 2014, INT C LEARN REPR ICL, Patent No. [1312.6114, 13126114]; Kingma DP, 2 INT C LEARN REPR I, P1; Larsen Anders Boesen Lindbo, 2016, INT C MACH LEARN ICM; Makhzani A., 2015, 151105644 ARXIV; Mescheder Lars M., 2017, ABS170104722 ARXIV; Metz L., 2016, ARXIV161102163; Radford A., 2015, P COMP C; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Salimans T., 2016, ABS160603498 CORR; Sonderby C. K., 2016, ARXIV PREPRINT ARXIV; Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613; Tran D, 2017, ARXIV E PRINTS; Zhu J.-Y., 2017, ARXIV170310593	23	26	26	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403037
C	Alemi, AA; Chollet, F; Een, N; Irving, G; Szegedy, C; Urban, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Alemi, Alexander A.; Chollet, Francois; Een, Niklas; Irving, Geoffrey; Szegedy, Christian; Urban, Josef			DeepMath - Deep Sequence Models for Premise Selection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				FORMAL VERIFICATION; MIZAR	We study the effectiveness of neural sequence models for premise selection in automated theorem proving, one of the main bottlenecks in the formalization of mathematics. We propose a two stage approach for this task that yields good results for the premise selection task on the Mizar corpus while avoiding the hand-engineered features of existing state-of-the-art models. To our knowledge, this is the first time deep learning has been applied to theorem proving on a large scale.	[Alemi, Alexander A.; Chollet, Francois; Een, Niklas; Irving, Geoffrey; Szegedy, Christian] Google Inc, Mountain View, CA 94043 USA; [Urban, Josef] Czech Tech Univ, Prague, Czech Republic	Google Incorporated; Czech Technical University Prague	Alemi, AA (corresponding author), Google Inc, Mountain View, CA 94043 USA.	alemi@google.com; fchollet@google.com; een@google.com; geoffreyi@google.com; szegedy@google.com; josef.urban@gmail.com			ERC Consolidator grant [649043 AI4REASON]	ERC Consolidator grant(European Research Council (ERC))	Supported by ERC Consolidator grant nr. 649043 AI4REASON.	Abadi M, 2015, P 12 USENIX S OPERAT; [Anonymous], 2001, HDB AUTOMATED REASON; Bancerek G, 2002, J AUTOM REASONING, V29, P189, DOI 10.1023/A:1021966832558; Baudis Petr, 2016, ARXIV160306127; Chollet F., 2015, KERAS; Chung JY, 2015, PR MACH LEARN RES, V37, P2067; Dai Andrew M., 2015, ADV NEURAL INFORM PR; de Bruijn N., P S AUT DEM VERS FRA, P29; Glorot X., 2010, PROC MACH LEARN RES, P249; Gonthier Georges, 2007, Computer Mathematics. 8th Asian Symposium, ASCM 2007. Revised and Invited Papers; Gonthier G, 2013, LECT NOTES COMPUT SC, V7998, P163, DOI 10.1007/978-3-642-39634-2_14; Grabowski A, 2010, J FORMALIZ REASON, V3, P153; Hales T, 2017, FORUM MATH PI, V5, DOI 10.1017/fmp.2017.1; Harrison J, 1996, LECT NOTES COMPUT SC, V1166, P265, DOI 10.1007/BFb0031814; Harrison J., 2014, COMPUTATIONAL LOGIC, V9, P135, DOI [10.1016/B978-0-444-51624-4.50004-6, DOI 10.1016/B978-0-444-51624-4.50004-6]; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kaiser L., 2015, COMPUTER SCI; Kaliszyk C., 2013, EPIC SER, V14, P87, DOI [10.29007/5gzr, DOI 10.29007/5GZR]; Kaliszyk C, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3084; Kaliszyk C, 2015, J AUTOM REASONING, V55, P245, DOI 10.1007/s10817-015-9330-8; Kingma D.P, P 3 INT C LEARNING R; Klein G, 2010, COMMUN ACM, V53, P107, DOI 10.1145/1743546.1743574; Leroy X, 2009, COMMUN ACM, V52, P107, DOI 10.1145/1538788.1538814; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Mohamed O. A., 2008, THEOREM PROVING HIGH; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Schulz S, 2002, AI COMMUN, V15, P111; Schulz S., 2013, PAAR 2012, V21, P82; Sukhbaatar S, 2015, ADV NEUR IN, V28; Urban Josef, 2013, Automated Reasoning and Mathematics. Essays in Memory of William W. McCune, P240, DOI 10.1007/978-3-642-36675-8_13; Urban J, 2006, J AUTOM REASONING, V37, P21, DOI 10.1007/s10817-006-9032-3; Vinyals O, 2015, ICML DEEP LEARN WORK; Zaremba W, 2014, CORR	35	26	26	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702027
C	Farnia, F; Tse, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Farnia, Farzan; Tse, David			A Minimax Approach to Supervised Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MAXIMUM-ENTROPY; REGRESSION; SELECTION	Given a task of predicting Y from X, a loss function L, and a set of probability distributions F on (X, Y), what is the optimal decision rule minimizing the worst case expected loss over F? In this paper, we address this question by introducing a generalization of the maximum entropy principle. Applying this principle to sets of distributions with marginal on X constrained to be the empirical marginal, we provide a minimax interpretation of the maximum likelihood problem over generalized linear models as well as some popular regularization schemes. For quadratic and logarithmic loss functions we revisit well-known linear and logistic regression models. Moreover, for the 0-1 loss we derive a classifier which we call the minimax SVM. The minimax SVM minimizes the worst-case expected 0-1 loss over the proposed F by solving a tractable optimization problem. We perform several numerical experiments to show the power of the minimax SVM in outperforming the SVM.	[Farnia, Farzan; Tse, David] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA	Stanford University	Farnia, F (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.	farnia@stanford.edu; dntse@stanford.edu		Farnia, Farzan/0000-0002-6049-9232	Stanford University; Center for Science of Information (CSoI), an NSF Science and Technology Center [CCF-0939370]	Stanford University(Stanford University); Center for Science of Information (CSoI), an NSF Science and Technology Center	We are grateful to Stanford University providing a Stanford Graduate Fellowship, and the Center for Science of Information (CSoI), an NSF Science and Technology Center under grant agreement CCF-0939370, for the support during this research.	Altun Yasemin, 2006, LEARNING THEORY; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Berger AL, 1996, COMPUT LINGUIST, V22, P39; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Cover T. M., 1991, Elements of information theory, DOI 10.1002/0471200611; Dawid Philip, 1998, 139 U COLL LOND; Dudik M, 2007, J MACH LEARN RES, V8, P1217; Eban E, 2014, PR MACH LEARN RES, V32, P1233; Erkan Ayse, 2010, AISTATS, P1; Feldman V, 2012, SIAM J COMPUT, V41, P1558, DOI 10.1137/120865094; Friedman J., 2001, ELEMENTS STAT LEARNI, V1; Globerson A., 2004, P 20 C UNC ART INT, P193; Grunwald PD, 2004, ANN STAT, V32, P1367, DOI 10.1214/009053604000000553; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI 10.1080/00401706.1970.10488634; Huber PJ, 1981, ROBUST STAT; Jacob L., 2009, P 26 INT C MACH LEAR, P433, DOI DOI 10.1145/1553374.1553431; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; Lanckriet G. R. G., 2003, J MACHINE LEARNING R, V3, P555; McCullagh P., 1989, GEN LINEAR MODELS, DOI [10. 1007/978-1-4899-3242-6, DOI 10.1007/978-1-4899-3242-6]; Razaviyayn Meisam, 2015, ADV NEURAL INFORM PR, V28, P3258; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x	25	26	28	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701104
C	Li, X; Qin, T; Yang, J; Liu, TY		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Li, Xiang; Qin, Tao; Yang, Jian; Liu, Tie-Yan			LightRNN: Memory and Computation-Efficient Recurrent Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Recurrent neural networks (RNNs) have achieved state-of-the-art performances in many natural language processing tasks, such as language modeling and machine translation. However, when the vocabulary is large, the RNN model will become very big (e.g., possibly beyond the memory capacity of a GPU device) and its training will become very inefficient. In this work, we propose a novel technique to tackle this challenge. The key idea is to use 2-Component (2C) shared embedding for word representations. We allocate every word in the vocabulary into a table, each row of which is associated with a vector, and each column associated with another vector. Depending on its position in the table, a word is jointly represented by two components: a row vector and a column vector. Since the words in the same row share the row vector and the words in the same column share the column vector, we only need 2 root vertical bar V vertical bar vectors to represent a vocabulary of vertical bar V vertical bar unique words, which are far less than the vertical bar V vertical bar vectors required by existing approaches. Based on the 2-Component shared embedding, we design a new RNN algorithm and evaluate it using the language modeling task on several benchmark datasets. The results show that our algorithm significantly reduces the model size and speeds up the training process, without sacrifice of accuracy (it achieves similar, if not better, perplexity as compared to state-of-the-art language models). Remarkably, on the One-Billion-Word benchmark Dataset, our algorithm achieves comparable perplexity to previous language models, whilst reducing the model size by a factor of 40-100, and speeding up the training process by a factor of 2. We name our proposed algorithm LightRNN to reflect its very small model size and very high training speed.	[Li, Xiang; Yang, Jian] Nanjing Univ Sci & Technol, Nanjing, Jiangsu, Peoples R China; [Qin, Tao; Liu, Tie-Yan] Microsoft Res Asia, Beijing, Peoples R China	Nanjing University of Science & Technology; Microsoft; Microsoft Research Asia	Li, X (corresponding author), Nanjing Univ Sci & Technol, Nanjing, Jiangsu, Peoples R China.	implusdream@gmail.com; taoqin@microsoft.com; csjyang@njust.edu.cn; tie-yan.liu@microsoft.com			National Science Fund of China [91420201, 61472187, 61502235, 61233011, 61373063]; Key Project of Chinese Ministry of Education [313030]; 973 Program [2014CB349303]; Program for Changjiang Scholars and Innovative Research Team in University	National Science Fund of China(National Natural Science Foundation of China (NSFC)); Key Project of Chinese Ministry of Education(Ministry of Education, China); 973 Program(National Basic Research Program of China); Program for Changjiang Scholars and Innovative Research Team in University(Program for Changjiang Scholars & Innovative Research Team in University (PCSIRT))	The authors would like to thank the anonymous reviewers for their critical and constructive comments and suggestions. This work was partially supported by the National Science Fund of China under Grant Nos. 91420201, 61472187, 61502235, 61233011 and 61373063, the Key Project of Chinese Ministry of Education under Grant No. 313030, the 973 Program No. 2014CB349303, and Program for Changjiang Scholars and Innovative Research Team in University. We also would like to thank Professor Xiaolin Hu from Department of Computer Science and Technology, Tsinghua National Laboratory for Information Science and Technology (TNList) for giving a lot of wonderful advices.	Ahuja Ravindra K, 1988, TECHNICAL REPORT; Appleyard Jeremy, 2016, ARXIV160401946; Bengio Yoshua, 2003, P C ART INT STAT AIS; Botha Jan A, 2014, P ICML; Chelba Ciprian, 2013, ARXIV13123005; Chen W., 2015, ARXIV151204906; Chung J., 2014, ARXIV14123555; Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015; Goodman J, 2001, INT CONF ACOUST SPEE, P561, DOI 10.1109/ICASSP.2001.940893; Graves A, 2013, ARXIV13080850; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Ji S., 2015, ARXIV151106909; Kim Y., 2015, ARXIV150806615; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Mikolov T, 2011, INT CONF ACOUST SPEE, P5528; Mnih Andriy, 2009, ADV NEURAL INFORM PR, P1081; Morin F., 2005, PROC INT WORKSHOP AR, P246; Papadimitriou C. H., 1982, COMBINATORIAL OPTIMI; Pomikalek J, 2012, LREC 2012 - EIGHTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P502; Preis R, 1999, LECT NOTES COMPUT SC, V1563, P259; Sak H, 2014, INTERSPEECH, P338; Sundermeyer M, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P194; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Tang Duyu, 2015, P 2015 C EMP METH NA, P1422, DOI DOI 10.18653/V1/D15-1167; WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337; Weston J., 2014, ARXIV14103916; Yu D., 2014, TECHNICAL REPORT; Zaremba Wojciech, 2014, ABS14092329 CORR	28	26	27	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704037
C	Osband, I; Blundell, C; Pritzel, A; Van Roy, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Osband, Ian; Blundell, Charles; Pritzel, Alexander; Van Roy, Benjamin			Deep Exploration via Bootstrapped DQN	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Efficient exploration remains a major challenge for reinforcement learning (RL). Common dithering strategies for exploration, such as epsilon-greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements. However, most algorithms for statistically efficient RL are not computationally tractable in complex environments. Randomized value functions offer a promising approach to efficient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions. As a first step towards addressing such contexts we develop bootstrapped DQN. We demonstrate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy. In the Arcade Learning Environment bootstrapped DQN substantially improves learning speed and cumulative performance across most games.	[Osband, Ian; Van Roy, Benjamin] Stanford Univ, Stanford, CA 94305 USA; [Osband, Ian; Blundell, Charles; Pritzel, Alexander] Google DeepMind, London, England	Stanford University; Google Incorporated	Osband, I (corresponding author), Stanford Univ, Stanford, CA 94305 USA.; Osband, I (corresponding author), Google DeepMind, London, England.	iosband@google.com; cblundell@google.com; apritzel@google.com; bvr@stanford.edu						[Anonymous], 2012, ARXIV12074708; Blundell C., 2015, ICML; Dann  Christoph, 2015, ADV NEURAL INFORM PR, P2800; Efron B., 1994, INTRO BOOTSTRAP; Efron B., 1982, JACKKNIFE BOOTSTRAP; Gal Y., 2015, ARXIV150602142; Guez A., 2012, ADV NEURAL INFORM PR; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Kakade Sham M., 2003, THESIS; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Osband I., 2014, ADV NEURAL INFORM PR, V27, P1466; Osband I., 2015, ARXIV150700300; Osband I., 2014, GEN EXPLORATION VIA; Owen AB, 2012, ANN APPL STAT, V6, P895, DOI 10.1214/12-AOAS547; Schaul T, 2016, C TRACK P, P1; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Stadie B. C., 2015, ARXIV150700814; Strens, 2000, P 17 INT C MACH LEAR, P943; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; TESAURO G, 1995, COMMUN ACM, V38, P58, DOI 10.1145/203330.203343; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; van Hasselt H., 2015, ARXIV150906461; Van Roy, 2013, ADV NEURAL INFORM PR, P3003; Wang Z, 2015, FINANCIAL REPORTING; Wen Z., 2013, ADV NEURAL INFORM PR	27	26	26	1	10	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704026
C	van Erven, T; Koolen, WM		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		van Erven, Tim; Koolen, Wouter M.			MetaGrad: Multiple Learning Rates in Online Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				PREDICTION	In online convex optimization it is well known that certain subclasses of objective functions are much easier than arbitrary convex functions. We are interested in designing adaptive methods that can automatically get fast rates in as many such subclasses as possible, without any manual tuning. Previous adaptive methods are able to interpolate between strongly convex and general convex functions. We present a new method, MetaGrad, that adapts to a much broader class of functions, including exp-concave and strongly convex functions, but also various types of stochastic and non-stochastic functions without any curvature. For instance, MetaGrad can achieve logarithmic regret on the unregularized hinge loss, even though it has no curvature, if the data come from a favourable probability distribution. MetaGrad's main feature is that it simultaneously considers multiple learning rates. Unlike previous methods with provable regret guarantees, however, its learning rates are not monotonically decreasing over time and are not tuned based on a theoretically derived bound on the regret. Instead, they are weighted directly proportional to their empirical performance on the data using a tilted exponential weights master algorithm.	[van Erven, Tim] Leiden Univ, Leiden, Netherlands; [Koolen, Wouter M.] Ctr Wiskunde & Informat, Amsterdam, Netherlands	Leiden University; Leiden University - Excl LUMC	van Erven, T (corresponding author), Leiden Univ, Leiden, Netherlands.	tim@timvanerven.nl; wmkoolen@cwi.nl			Netherlands Organization for Scientific Research (NWO) [639.021.439]	Netherlands Organization for Scientific Research (NWO)(Netherlands Organization for Scientific Research (NWO))	We would like to thank Haipeng Luo and the anonymous reviewers (in particular Reviewer 6) for valuable comments. Koolen acknowledges support by the Netherlands Organization for Scientific Research (NWO, Veni grant 639.021.439).	Bartlett PL, 2006, PROBAB THEORY REL, V135, P311, DOI 10.1007/s00440-005-0462-3; Bartlett PL, 2007, P 21 ANN C ADV NEUR, P65; Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7; Chernov A., 2010, PREDICTION ADVICE UN, P117; Chernov A, 2010, THEOR COMPUT SCI, V411, P2647, DOI 10.1016/j.tcs.2010.04.003; Chiang Chao-Kai, 2012, P 25 ANN C LEARN THE; Crammer K, 2013, MACH LEARN, V91, P155, DOI 10.1007/s10994-013-5327-x; Do C. B., 2009, P 26 ANN INT C MACH, P257; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Gaillard P., 2014, JMLR WORKSHOP C P, P176; Hazan E., 2016, INTRO ONLINE OPTIMIZ; Hazan E, 2010, MACH LEARN, V80, P165, DOI 10.1007/s10994-010-5175-x; Ihara S., 1993, INFORM THEORY CONTIN; Koolen W. M., 2016, NIPS, V29; Koolen W. M., 2016, METAGRAD OPEN SOURCE; Koolen Wouter M, 2015, COLT, P1155; Koolen Wouter M, 2014, ADV NEURAL INFORM PR, V27; Luo H., 2016, NIPS, V29; Mikolov T., 2013, ARXIV; Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x; Orabona F., 2016, NIPS 29; Orabona F, 2014, ADV NEUR IN, V27; Orabona F, 2015, MACH LEARN, V99, P411, DOI 10.1007/s10994-014-5474-8; Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Srebro N., 2010, ADV NEURAL INFORM PR, P2199; Steinhardt J, 2014, PR MACH LEARN RES, V32, P1593; van Erven T, 2015, J MACH LEARN RES, V16, P1793; Xiao L, 2010, J MACH LEARN RES, V11, P2543; Zinkevich, 2003, P 20 INT C MACH LEAR, P928; Zinkevich M., 2004, THESIS	34	26	26	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702015
C	Huang, Y; Wang, W; Wang, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Huang, Yan; Wang, Wei; Wang, Liang			Bidirectional Recurrent Convolutional Networks for Multi-Frame Super-Resolution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				RESOLUTION	Super resolving a low-resolution video is usually handled by either single-image super-resolution (SR) or multi-frame SR. Single-Image SR deals with each video frame independently, and ignores intrinsic temporal dependency of video frames which actually plays a very important role in video super-resolution. Multi-Frame SR generally extracts motion information, e.g., optical flow, to model the temporal dependency, which often shows high computational cost. Considering that recurrent neural networks (RNNs) can model long-term contextual information of temporal sequences well, we propose a bidirectional recurrent convolutional network for efficient multi-frame SR. Different from vanilla RNNs, 1) the commonly-used recurrent full connections are replaced with weight-sharing convolutional connections and 2) conditional convolutional connections from previous input layers to the current hidden layer are added for enhancing visual-temporal dependency modelling. With the powerful temporal dependency modelling, our model can super resolve videos with complex motions and achieve state-of-the-art performance. Due to the cheap convolution operations, our model has a low computational complexity and runs orders of magnitude faster than other multi-frame methods.	[Huang, Yan; Wang, Wei; Wang, Liang] Chinese Acad Sci, Natl Lab Pattern Recognit, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China; [Wang, Liang] Chinese Acad Sci, Inst Automat, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China	Chinese Academy of Sciences; Chinese Academy of Sciences; Institute of Automation, CAS	Huang, Y (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China.	yhuang@nlpr.ia.ac.cn; wangwei@nlpr.ia.ac.cn; wangliang@nlpr.ia.ac.cn	Huang, Yan/HCH-6526-2022		National Natural Science Foundation of China [61420106015, 61175003, 61202328, 61572504]; National Basic Research Program of China [2012CB316300]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Basic Research Program of China(National Basic Research Program of China)	This work is jointly supported by National Natural Science Foundation of China (61420106015, 61175003, 61202328, 61572504) and National Basic Research Program of China (2012CB316300).	Baker S., 1999, TECHNICAL REPORT; Bascle B., 1996, COMPUTER VISION ECCV, P571; Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135; Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Eigen D, 2013, IEEE I CONF COMP VIS, P633, DOI 10.1109/ICCV.2013.84; Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075; Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271; IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L; Jain V, 2008, P ADV NEUR INF PROC, P769; Jia K, 2013, IEEE T PATTERN ANAL, V35, P367, DOI 10.1109/TPAMI.2012.95; Mitzel D, 2009, LECT NOTES COMPUT SC, V5748, P432, DOI 10.1007/978-3-642-03798-6_44; Nair V., 2010, ICML, P807; Protter M, 2009, IEEE T IMAGE PROCESS, V18, P36, DOI 10.1109/TIP.2008.2008067; Schultz RR, 1996, IEEE T IMAGE PROCESS, V5, P996, DOI 10.1109/83.503915; Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093; Shahar O., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3353, DOI 10.1109/CVPR.2011.5995360; Sutskever Ilya, 2007, AISTATS; Taylor G., 2006, P ADV NEUR INF PROC, P448; Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241; Xu L., 2014, INT C NEUR INF PROC, V27, P1790; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Zeyde Roman, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47	25	26	26	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101038
C	Williams, PM		Jordan, MI; Kearns, MJ; Solla, SA		Williams, PM			Modelling seasonality and trends in daily rainfall data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					This paper presents a new approach to the problem of modelling daily rainfall using neural networks. We first model the conditional distributions of rainfall amounts, in such a way that the model itself determines the order of the process, and the time-dependent shape and scale of the conditional distributions. After integrating over particular weather patterns, we are able to extract seasonal variations and long-term trends.	Univ Sussex, Sch Cognit & Comp Sci, Brighton BN1 9QH, E Sussex, England	University of Sussex	Williams, PM (corresponding author), Univ Sussex, Sch Cognit & Comp Sci, Brighton BN1 9QH, E Sussex, England.								0	26	26	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						985	991						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700139
C	Sill, J; AbuMostafa, YS		Mozer, MC; Jordan, MI; Petsche, T		Sill, J; AbuMostafa, YS			Monotonicity hints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				A hint is any piece of side information about the target function to be learned. We consider the monotonicity hint. which states that the function to be learned is monotonic in some or all of the input variables. The application of monotonicity hints is demonstrated on two real-world problems- a credit card application task, and a problem in medical diagnosis. A measure of the monotonicity error of a candidate function is defined and an objective function for the enforcement of monotonicity is derived from Bayesian principles. We report experimental results which show that using monotonicity hints leads to a statistically significant improvement in performance on both problems.			Sill, J (corresponding author), CALTECH,COMPUTAT & NEURAL SYST PROGRAM,PASADENA,CA 91125, USA.								0	26	26	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						634	640						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00090
C	Fukumizu, K		Touretzky, DS; Mozer, MC; Hasselmo, ME		Fukumizu, K			Active learning in multilayer perceptrons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						RICOH CO LTD,INFORMAT & COMMUN R&D CTR,YOKOHAMA,KANAGAWA 222,JAPAN	Ricoh Company, Ltd.									0	26	26	0	2	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						295	301						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00042
C	Carroll, M; Shah, R; Ho, MK; Griffiths, TL; Seshia, SA; Abbeel, P; Dragan, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Carroll, Micah; Shah, Rohin; Ho, Mark K.; Griffiths, Thomas L.; Seshia, Sanjit A.; Abbeel, Pieter; Dragan, Anca			On the Utility of Learning about Humans for Human-AI Coordination	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					While we would like agents that can coordinate with humans, current algorithms such as self-play and population-based training create agents that can coordinate with themselves. Agents that assume their partner to be optimal or similar to them can converge to coordination protocols that fail to understand and be understood by humans. To demonstrate this, we introduce a simple environment that requires challenging coordination, based on the popular game Overcooked, and learn a simple model that mimics human play. We evaluate the performance of agents trained via self-play and population-based training. These agents perform very well when paired with themselves, but when paired with our human model, they are significantly worse than agents designed to play with the human model. An experiment with a planning algorithm yields the same conclusion, though only when the human-aware planner is given the exact human model that it is playing with. A user study with real humans shows this pattern as well, though less strongly. Qualitatively, we find that the gains come from having the agent adapt to the human's gameplay. Given this result, we suggest several approaches for designing agents that learn about humans in order to better coordinate with them.	[Carroll, Micah; Shah, Rohin; Seshia, Sanjit A.; Abbeel, Pieter; Dragan, Anca] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Ho, Mark K.; Griffiths, Thomas L.] Princeton Univ, Princeton, NJ 08544 USA	University of California System; University of California Berkeley; Princeton University	Carroll, M (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	mdc@berkeley.edu; rohinmshah@berkeley.edu; mho@princeton.edu	Carroll, Micah/ABC-6599-2020	Carroll, Micah/0000-0002-0716-8071; Griffiths, Thomas/0000-0002-5138-7255; Dragan, Anca/0000-0001-6312-5466	Open Philanthropy Project; NSF CAREER; NSF VeHICaL project [CNS-1545126]; National Science Foundation Graduate Research Fellowship [DGE 1752814]	Open Philanthropy Project; NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF VeHICaL project; National Science Foundation Graduate Research Fellowship(National Science Foundation (NSF))	We thank the researchers at the Center for Human Compatible AI and the Interact lab for valuable feedback. This work was supported by the Open Philanthropy Project, NSF CAREER, the NSF VeHICaL project (CNS-1545126), and National Science Foundation Graduate Research Fellowship Grant No. DGE 1752814.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Alami R., 2006, AAAI SPRING S BOLDLY, P39; [Anonymous], 2016, GHOST TOWN GAMES; [Anonymous], 2017, MASTERING CHESS SHOG; Bain M., 1999, MACHINE INTELLIGENCE, V15, P103; Boutilier C, 1996, THEORETICAL ASPECTS OF RATIONALITY AND KNOWLEDGE, P195; Carter Shan, 2017, DISTILL, DOI [10.23915/distill.00009, DOI 10.23915/DISTILL.00009]; Choudhury Rohan, 2019, UTILITY MODEL LEARNI; Donahue J., 2017, ARXIV171109846; Dragan AD, 2013, ACMIEEE INT CONF HUM, P301, DOI 10.1109/HRI.2013.6483603; Engelbart D.C., 1962, AUGMENTING HUMAN INT; Finn C, 2017, PR MACH LEARN RES, V70; Foerster J. N., 2018, ARXIV181101458; Gielniak M. J., 2011, 2011 RO-MAN: The 20th IEEE International Symposium on Robot and Human Interactive Communication, P449, DOI 10.1109/ROMAN.2011.6005255; Gupta Jayesh K., 2017, Autonomous Agents and Multiagent Systems, AAMAS 2017: Workshops, Best Papers. Revised Selected Papers: LNAI 10642, P66, DOI 10.1007/978-3-319-71682-4_5; Hesse C., 2017, OPENAI BASELINES; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Ho M. K., 2016, P 38 ANN C COGN SCI, P1158; Hughes E., 2018, ADV NEURAL INFORM PR; Jaderberg M, 2019, SCIENCE, V364, P859, DOI 10.1126/science.aau6249; Javdani Shervin, 2015, ROB SCI SYST ONL P; Leibo JZ, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P464; Lerer Adam, 2019, AAAI ACM C ART INT E; Lowe R., 2017, P INT C NEUR INF PRO, P6379; Nagabandi A, 2018, IEEE INT CONF ROBOT, P7579; Nikolaidis S, 2013, ACMIEEE INT CONF HUM, P33, DOI 10.1109/HRI.2013.6483499; Ohn-Bar Eshed, 2018, CORR; OpenAI, 2019, TRAIN YOUR OPENAI 5; OpenAI, 2019, OPENAI 5 FIN; Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88; Schulman J., 2017, ABS170706347 CORR; Vinyals O., 2019, ALPHASTAR MASTERING; ZHOU F, 2016, ROBOTICS SCI SYSTEMS, V2, DOI DOI 10.1038/NPJCOMPUMATS.2016.22	34	25	24	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305020
C	Hasanzadeh, A; Hajiramezanali, E; Duffield, N; Narayanan, K; Zhou, MY; Qian, XN		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hasanzadeh, Arman; Hajiramezanali, Ehsan; Duffield, Nick; Narayanan, Krishna; Zhou, Mingyuan; Qian, Xiaoning			Semi-Implicit Graph Variational Auto-Encoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Semi-implicit graph variational auto-encoder (SIG-VAE) is proposed to expand the flexibility of variational graph auto-encoders (VGAE) to model graph data. SIG-VAE employs a hierarchical variational framework to enable neighboring node sharing for better generative modeling of graph dependency structure, together with a Bernoulli-Poisson link decoder. Not only does this hierarchical construction provide a more flexible generative graph model to better capture real-world graph properties, but also does SIG-VAE naturally lead to semi-implicit hierarchical variational inference that allows faithful modeling of implicit posteriors of given graph data, which may exhibit heavy tails, multiple modes, skewness, and rich dependency structures. SIG-VAE integrates a carefully designed generative model, well suited to model real-world sparse graphs, and a sophisticated variational inference network, which propagates the graph structural information and distribution uncertainty to capture complex posteriors. SIG-VAE clearly outperforms a simple combination of VGAE with variational inference, including semi-implicit variational inference (SIVI) or normalizing flow (NF), which does not propagate uncertainty in its inference network, and provides more interpretable latent representations than VGAE does. Extensive experiments with a variety of graph data show that SIG-VAE significantly outperforms state-of-the-art methods on several different graph analytic tasks.	[Hasanzadeh, Arman; Hajiramezanali, Ehsan; Duffield, Nick; Narayanan, Krishna; Qian, Xiaoning] Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX 77843 USA; [Zhou, Mingyuan] Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA	Texas A&M University System; Texas A&M University College Station; University of Texas System; University of Texas Austin	Hasanzadeh, A (corresponding author), Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX 77843 USA.	armanihm@tamu.edu; ehsanr@tamu.edu; duffieldng@tamu.edu; krn@tamu.edu; mingyuan.zhou@mccombs.utexas.edu; xqian@tamu.edu	Zhou, Mingyuan/AAE-8717-2021	Narayanan, Krishna/0000-0001-8742-5332	National Science Foundation [ENG-1839816, IIS-1848596, CCF-1553281, IIS-1812641, IIS-1812699]	National Science Foundation(National Science Foundation (NSF))	The presented materials are based upon the work supported by the National Science Foundation under Grants ENG-1839816, IIS-1848596, CCF-1553281, IIS-1812641 and IIS-1812699. We also thank Texas A&M High Performance Research Computing and Texas Advanced Computing Center for providing computational resources to perform experiments in this work.	Abadi M, 2015, TENSORFLOW LARGE SCA; Ahmed A., 2013, WWW, P37; Airoldi EM, 2008, J MACH LEARN RES, V9, P1981; [Anonymous], 2018, ICLR; [Anonymous], 2016, ARXIV160308861; Armandpour M, 2019, AAAI CONF ARTIF INTE, P3191; Belkin M, 2002, ADV NEUR IN, V14, P585; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Bishop C.M., 2000, 16 C UNCERTAINTY ART, P46; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Chen HC, 2018, AAAI CONF ARTIF INTE, P2127; Defferrard M, PYGSP GRAPH SIGNAL P; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; Gasulla D. G., 2017, INT C COMPL NETW THE, P229; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kipf T., 2018, ARXIV180400891; Kipf T.N., 2016, VARIATIONAL GRAPH AU; Kipf T. N., 2017, INT C LEARN REPR, DOI [DOI 10.1109/ICDM.2008.17, DOI 10.1109/ICDM.2019.00070]; Lu Q., 2003, P 20 INT C MACH LEAR, P496, DOI DOI 10.5555/3041838.3041901; Maaloe L, 2016, PR MACH LEARN RES, V48; Newman MEJ, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.036104; Papamakarios George, 2017, ADV NEURAL INFORM PR, P2338; Perozzi Bryan, 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732; Ranganath R, 2016, PR MACH LEARN RES, V48; Rezende Danilo Jimenez, 2015, ARXIV150505770; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Spring N, 2002, ACM SIGCOMM COMP COM, V32, P133, DOI 10.1145/964725.633039; Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093; Tang L, 2011, DATA MIN KNOWL DISC, V23, P447, DOI 10.1007/s10618-010-0210-x; von Mering C, 2002, NATURE, V417, P399, DOI 10.1038/nature750; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918; Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34; Xu KYL, 2018, PR MACH LEARN RES, V80; Xu Keyulu, 2019, INT C LEARN REPR; Yin MZ, 2018, PR MACH LEARN RES, V80; Zhang M., 2018, ARXIV180209691; Zhou MY, 2015, JMLR WORKSH CONF PRO, V38, P1135; Zhu Xiaojin., 2003, P ICLR, P912	42	25	26	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902035
C	Lee, HY; Yang, XD; Liu, MY; Wang, TC; Lu, YD; Yang, MH; Kautz, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lee, Hsin-Ying; Yang, Xiaodong; Liu, Ming-Yu; Wang, Ting-Chun; Lu, Yu-Ding; Yang, Ming-Hsuan; Kautz, Jan			Dancing to Music	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Dancing to music is an instinctive move by humans. Learning to model the music-to-dance generation process is, however, a challenging problem. It requires significant efforts to measure the correlation between music and dance as one needs to simultaneously consider multiple aspects, such as style and beat of both music and dance. Additionally, dance is inherently multimodal and various following movements of a pose at any moment are equally likely. In this paper, we propose a synthesis-by-analysis learning framework to generate dance from music. In the analysis phase, we decompose a dance into a series of basic dance units, through which the model learns how to move. In the synthesis phase, the model learns how to compose a dance by organizing multiple basic dancing movements seamlessly according to the input music. Experimental qualitative and quantitative results demonstrate that the proposed method can synthesize realistic, diverse, style-consistent, and beat-matching dances from music.	[Lee, Hsin-Ying; Lu, Yu-Ding; Yang, Ming-Hsuan] Univ Calif, Merced, CA 95343 USA; [Yang, Xiaodong; Liu, Ming-Yu; Wang, Ting-Chun; Kautz, Jan] NVIDIA, Santa Clara, CA USA	University of California System; University of California Merced; Nvidia Corporation	Lee, HY (corresponding author), Univ Calif, Merced, CA 95343 USA.		Yang, Ming-Hsuan/T-9533-2019; Wang, Ting-Chun/AAZ-2408-2020; Wang, Ting-Chun/AAD-4410-2021	Yang, Ming-Hsuan/0000-0003-4848-2304; Wang, Ting-Chun/0000-0002-1522-2381; 				Aberman K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322999; Arandjelovic R., 2018, EUR C COMP VIS; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Chao YW, 2017, PROC CVPR IEEE, P3643, DOI 10.1109/CVPR.2017.388; Davis A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201371; Davis Abe, 2014, ACM T GRAPHICS; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Ellis D., 2007, J NEW MUSIC RES; Ephrat A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201357; Fan R., 2012, IEEE T VISUALIZATION; Harwath D., 2018, P EUR C COMP VIS ECC, P649; Hensel M, 2017, ADV NEUR IN, V30; Howe J, 2016, ONATI INT SER LAW SO, P3; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Karras T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073658; Kingma D.P, P 3 INT C LEARNING R; Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3; Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79; Lee M., 2013, MULTIMEDIA TOOLS APP; Lu Y.-D., 2019, IEEE INT C IM PROC; Madison G., 2011, J EXPT PSYCHOL HUMAN; Mathieu Michael, 2016, ICLR; Ofli F., 2012, IEEE T MULTIMEDIA; Owens A., 2018, EUR C COMP VIS; Owens A, 2016, PROC CVPR IEEE, P2405, DOI 10.1109/CVPR.2016.264; Reed S, 2016, PR MACH LEARN RES, V48; Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12; Schellenberg G., 2000, MUSIC PERCEPTION; Senocak A, 2018, PROC CVPR IEEE, P4358, DOI 10.1109/CVPR.2018.00458; Shlizerman E, 2018, PROC CVPR IEEE, P7574, DOI 10.1109/CVPR.2018.00790; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640; Tang T., 2018, ACM MULTIMEDIA; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Walker M, 2017, PALG ST GLOB CIT, P3, DOI 10.1057/978-1-137-55786-5_1; Wang Ting-Chun, 2018, ARXIV180806601; Yan Xinchen, 2018, EUR C COMP VIS; Yang Xiaodong, 2014, J VISUAL COMMUNICATI; Zhang H., 2017, ICCV; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472; Zhu YR, 2018, 2018 INTERNATIONAL SYMPOSIUM ON EDUCATIONAL TECHNOLOGY (ISET), P3, DOI 10.1109/ISET.2018.00011	46	25	25	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303056
C	Nagarajan, V; Kolter, JZ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Nagarajan, Vaishnavh; Kolter, J. Zico			Uniform convergence may be unable to explain generalization in deep learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Aimed at explaining the surprisingly good generalization behavior of overparameterized deep networks, recent works have developed a variety of generalization bounds for deep learning, all based on the fundamental learning-theoretic technique of uniform convergence. While it is well-known that many of these existing bounds are numerically large, through numerous experiments, we bring to light a more concerning aspect of these bounds: in practice, these bounds can increase with the training dataset size. Guided by our observations, we then present examples of overparameterized linear classifiers and neural networks trained by gradient descent (GD) where uniform convergence provably cannot "explain generalization" - even if we take into account the implicit bias of GD to the fullest extent possible. More precisely, even if we consider only the set of classifiers output by GD, which have test errors less than some small epsilon in our settings, we show that applying (two-sided) uniform convergence on this set of classifiers will yield only a vacuous generalization guarantee larger than 1 - epsilon. Through these findings, we cast doubt on the power of uniform convergence-based generalization bounds to provide a complete picture of why overparameterized deep networks generalize well.	[Nagarajan, Vaishnavh; Kolter, J. Zico] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA; [Kolter, J. Zico] Bosch Ctr Artificial Intelligence, Pittsburgh, PA USA	Carnegie Mellon University	Nagarajan, V (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.	vaishnavh@cs.cmu.edu; zkolter@cs.cmu.edu	velmurugan, nagarajan/AAD-6724-2020	velmurugan, nagarajan/0000-0002-1031-0737	Bosch Center for AI	Bosch Center for AI	Vaishnavh Nagarajan is supported by a grant from the Bosch Center for AI.	Allen-Zhu Zeyuan, 2018, LEARNING GEN OVER PA; Arora S, 2018, PR MACH LEARN RES, V80; Bousquet Olivier, 2002, J MACHINE LEARNING R, V2; Brutzkus A., 2018, ICLR; Draxler F, 2018, PR MACH LEARN RES, V80; Dziugaite GK, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Feldman Vitaly, 2018, ADV NEURAL INFORM PR; Garipov Timur, 2018, ADV NEURAL INFOR MAT; Golowich Noah, 2018, COMPUTATIONAL LEARNI; Hardt M, 2016, PR MACH LEARN RES, V48; Harvey Nick, 2017, P 30 C LEARN THEOR; Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hoffer Elad, 2017, ADV NEURAL INFORM PR, P1729; Jastrzebski Stanislaw, 2018, ART NEUR NETW MAHC L; Kawaguchi K., 2017, ARXIV171005468V4STAT, DOI DOI 10.2196/jmir.5870; Keskar N.S., 2017, ICLR; Langford John, 2002, ADV NEURAL INFORM PR, V15; Langford John, 2001, ADV NEURAL INFORM PR, V14; McAllester D, 2003, LECT NOTES ARTIF INT, V2777, P203, DOI 10.1007/978-3-540-45167-9_16; Nagarajan V., 2019, ARXIV190513344, p2019a; Nagarajan V, 2017, ADV NEUR IN, V30; Neyshabur B., 2015, INT C LEARN REPR WOR; Neyshabur B., 2015, C LEARN THEOR; Neyshabur Behnam, 2019, INT C LEARN REPR ICL; Neyshabur Behnam, 2018, P 6 INT C LEARN REPR; ROGERS WH, 1978, ANN STAT, V6, P506, DOI 10.1214/aos/1176344196; Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635; Soudry D., 2018, INT C LEARN REPR ICL; Talwalkar A., 2012, ADAPTIVE COMPUTATION; Vapnik V. N., 1971, UNIFORM CONVERGENCE; Wainwright MJ, 2019, CA ST PR MA, P1, DOI 10.1017/9781108627771; Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414; Zhou Wenda, 2019, ICLR	39	25	25	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903027
C	Wang, HH; Ge, SW; Xing, EP; Lipton, ZC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Haohan; Ge, Songwei; Xing, Eric P.; Lipton, Zachary C.			Learning Robust Global Representations by Penalizing Local Predictive Power	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Despite their well-documented predictive power on i.i.d. data, convolutional neural networks have been demonstrated to rely more on high-frequency (textural) patterns that humans deem superficial than on low-frequency patterns that agree better with intuitions about what constitutes category membership. This paper proposes a method for training robust convolutional networks by penalizing the predictive power of the local representations learned by earlier layers. Intuitively, our networks are forced to discard predictive signals such as color and texture that can be gleaned from local receptive fields and to rely instead on the global structure of the image. Across a battery of synthetic and benchmark domain adaptation tasks, our method confers improved generalization. To evaluate cross-domain transfer, we introduce ImageNet-Sketch, a new dataset consisting of sketch-like images and matching the ImageNet classification validation set in categories and scale.	[Wang, Haohan; Ge, Songwei; Xing, Eric P.; Lipton, Zachary C.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Wang, HH (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.	haohanw@cs.cmu.edu; songweig@cs.cmu.edu; epxing@cs.cmu.edu; zlipton@cs.cmu.edu		Wang, Haohan/0000-0002-1826-4069	NIH [R01GM114311, P30DA035778]; NSF [IIS1617583]; Salesforce Research; Facebook Research; Amazon Al	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF)); Salesforce Research; Facebook Research(Facebook Inc); Amazon Al	Haohan Wang is supported by NIH R01GM114311, NIH P30DA035778, and NSF IIS1617583. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Institutes of Health or the National Science Foundation. Zachary Lipton thanks the Center for Machine Learning and Health, a joint venture of Carnegie Mellon University, UPMC, and the University of Pittsburgh for supporting our collaboration with Abridge AI to develop robust models for machine learning in healthcare. He is also grateful to Salesforce Research, Facebook Research, and Amazon Al for faculty awards supporting his lab's research on robust deep learning under distribution shift.	Achille A, 2018, IEEE T PATTERN ANAL, V40, P2897, DOI 10.1109/TPAMI.2017.2784440; Balaji Y, 2018, ADV NEUR IN, V31; Ben-David S., 2010, INT C ART INT STAT A; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Bousmalis Konstantinos, 2016, ADV NEURAL INFORM PR, P343; Bridle J. S., 1991, ADV NEURAL INFORM PR, P234; Carlucci Fabio M, 2018, ARXIV180801102; Carlucci Fabio M, 2019, CVPR; Csurka Gabriela, 2017, ARXIV170205374; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Ding ZM, 2018, IEEE T IMAGE PROCESS, V27, P304, DOI 10.1109/TIP.2017.2758199; Erfani S., 2016, P INT JOINT C ART IN, P1455; Ganin Y., 2016, JMLR, V17, P2096; Gebru T, 2017, IEEE I CONF COMP VIS, P1358, DOI 10.1109/ICCV.2017.151; Geirhos R., 2019, P INT C LEARNING REP, P1; Ghifary M, 2015, IEEE I CONF COMP VIS, P2551, DOI 10.1109/ICCV.2015.293; Gretton A, 2009, NEURAL INF PROCESS S, P131; Heckman James J, 1977, 172 NAT BUR EC RES; Hendrycks D., 2019, ICLR, P1; Hoffman J, 2018, PR MACH LEARN RES, V80; Hoffman J, 2017, ADV COMPUT VIS PATT, P173, DOI 10.1007/978-3-319-58347-1_9; Hu W., 2016, ARXIV161102041; Jo J., 2017, ARXIV; Johansson F. D., 2019, ARXIV190303448; Kumagai A., 2018, ARXIV180702927; Kumar A, 2018, ADV NEUR IN, V31; Lee K., 2018, ADV NEURAL INFORM PR, V31, P1019; Li D., 2017, ARXIV171003463; Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591; Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566; Li W., 2017, IEEE T PATTERN ANAL; Li YT, 2018, ADV NEUR IN, V31; Lipton Zachary, 2018, ARXIV180203916, P3122; Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136; Long MS, 2018, ADV NEUR IN, V31; Mancini M., 2018, ARXIV180605810; Manski C. F., 1977, ECONOMETRICA J ECONO; Mansour Yishay, 2009, COLT 2009 22 C LEARN; Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609; Muandet Krikamol, 2013, ICML; Niu L, 2015, IEEE I CONF COMP VIS, P4193, DOI 10.1109/ICCV.2015.477; QuickDraw, 2018, QUICK DRAW DAT; Recht B., 2019, DO IMAGENET CLASSIFI; Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954; Schoenauer-Sebag Alice, 2019, ARXIV190309239; Scholkopf Bernhard, 2012, ICML; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Storkey A, 2009, NEURAL INF PROCESS S, P3; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Volpi Riccardo, 2018, ARXIV180512018; Wang H., 2019, INT C LEARN REPR; Wang Haohan, 2016, ARXIV160905244; Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083; Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6; WU Y, 2019, INT C MACH LEARN; Xie SA, 2018, PR MACH LEARN RES, V80; Zhang L, 2013, PROCEEDINGS OF 2013 CHINA INTERNATIONAL CONFERENCE ON INSURANCE AND RISK MANAGEMENT, P819; Zhao A, 2018, ADV NEUR IN, V31; Zhao H, 2018, ADV NEUR IN, V31	60	25	25	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902017
C	Zheng, S; Huang, ZY; Kwok, JT		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zheng, Shuai; Huang, Ziyue; Kwok, James T.			Communication-Efficient Distributed Blockwise Momentum SGD with Error-Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Communication overhead is a major bottleneck hampering the scalability of distributed machine learning systems. Recently, there has been a surge of interest in using gradient compression to improve the communication efficiency of distributed neural network training. Using 1-bit quantization, signSGD with majority vote achieves a 32x reduction on communication cost. However, its convergence is based on unrealistic assumptions and can diverge in practice. In this paper, we propose a general distributed compressed SGD with Nesterov's momentum. We consider two-way compression, which compresses the gradients both to and from workers. Convergence analysis on nonconvex problems for general gradient compressors is provided. By partitioning the gradient into blocks, a blockwise compressor is introduced such that each gradient block is compressed and transmitted in 1-bit format with a scaling factor, leading to a nearly 32x reduction on communication. Experimental results show that the proposed method converges as fast as full-precision distributed momentum SGD and achieves the same testing accuracy. In particular, on distributed ResNet training with 7 workers on the ImageNet, the proposed algorithm achieves the same testing accuracy as momentum SGD using full-precision gradients, but with 46% less wall clock time.	[Zheng, Shuai; Huang, Ziyue; Kwok, James T.] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China; [Zheng, Shuai] Amazon Web Serv, Seattle, WA 98109 USA	Hong Kong University of Science & Technology; Amazon.com	Zheng, S (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.; Zheng, S (corresponding author), Amazon Web Serv, Seattle, WA 98109 USA.	shzheng@amazon.com; zhuangbq@cse.ust.hk; jamesk@cse.ust.hk						Aji A.F., 2017, P 2017 C EMP METH NA, DOI [10.18653/v1/D17-1045, DOI 10.18653/V1/D17-1045]; Alistarh D., 2017, ADV NEURAL INF PROCE, P1709; [Anonymous], 2019, P INT C MACH LEARN; [Anonymous], 2013, PREPRINT ARXIV 1308; Bernstein J., 2018, INT C MACH LEARN, P560; Bernstein J., 2019, P INT C LEARN REPR; Bottou U, 2004, ADV NEUR IN, V16, P217; Chen J., 2016, ABS160400981 CORR; Cordonnier Jean-Baptiste, 2018, TECHNICAL REPORT; Dean J., 2012, NIPS 12, V1, P1223; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Karimireddy SP, 2019, PR MACH LEARN RES, V97; Li Mu, 2014, ADV NEURAL INFORM PR, V27, P19; Lin Y., 2018, P INT C REPR LEARN; NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Seide F, 2014, INTERSPEECH, P1058; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Stich S. U., 2018, P 32 INT C NEUR INF, P4452; Strom N., 2015, P ANN C INT SPEECH C; Sutskever I., 2014, ARXIV; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Wangni J., 2018, ADV NEURAL INFORM PR, P1299; Wen W., 2017, P NIPS, P1509; Wu JX, 2018, PR MACH LEARN RES, V80; Xing Eric P., 2015, IEEE Transactions on Big Data, V1, P49, DOI 10.1109/TBDATA.2015.2472014; Zinkevich M., 2010, P ADV NEUR INF PROC, P2595	28	25	25	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903012
C	Bistritz, I; Leshem, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bistritz, Ilai; Leshem, Amir			Distributed Multi-Player Bandits - a Game of Thrones Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MULTIARMED BANDIT; MEDIUM ACCESS	We consider a multi-armed bandit game where N players compete for K arms for T turns. Each player has different expected rewards for the arms, and the instantaneous rewards are independent and identically distributed. Performance is measured using the expected sum of regrets, compared to the optimal assignment of arms to players. We assume that each player only knows her actions and the reward she received each turn. Players cannot observe the actions of other players, and no communication between players is possible. We present a distributed algorithm and prove that it achieves an expected sum of regrets of near-O (log(2) T). This is the first algorithm to achieve a poly-logarithmic regret in this fully distributed scenario. All other works have assumed that either all players have the same vector of expected rewards or that communication between players is possible.	[Bistritz, Ilai] Stanford Univ, Stanford, CA 94305 USA; [Leshem, Amir] Bar Ilan Univ, Ramat Gan, Israel	Stanford University; Bar Ilan University	Bistritz, I (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	bistritz@stanford.edu; Amir.Leshem@biu.ac.il						Anandkumar A, 2011, IEEE J SEL AREA COMM, V29, P731, DOI 10.1109/JSAC.2011.110406; Avner Orly, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P66, DOI 10.1007/978-3-662-44848-9_5; Avner O, 2016, IEEE INFOCOM SER; Bertsekas D. P., 1988, Annals of Operations Research, V14, P105, DOI 10.1007/BF02186476; Besson L., 2018, P MACHINE LEARNING R, V83, P56; Cesa-Bianchi Nicol`o, 2016, C LEARN THEOR, P605; Chung KM, 2012, LEIBNIZ INT PR INFOR, V14, P124, DOI 10.4230/LIPIcs.STACS.2012.124; Cohen J., 2017, P 31 INT C NEUR INF; Evirgen N., 2017, ARXIV171101628; Hillel Eshcar, 2013, ADV NEURAL INFORM PR, P854; Kalathil D, 2014, IEEE T INFORM THEORY, V60, P2331, DOI 10.1109/TIT.2014.2302471; Korda N, 2016, PR MACH LEARN RES, V48; Lai LF, 2008, CONF REC ASILOMAR C, P98, DOI 10.1109/ACSSC.2008.5074370; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Landgren P, 2016, IEEE DECIS CONTR P, P167, DOI 10.1109/CDC.2016.7798264; Liu HY, 2013, IEEE T INFORM THEORY, V59, P1902, DOI 10.1109/TIT.2012.2230215; Liu KQ, 2010, IEEE T SIGNAL PROCES, V58, P5667, DOI 10.1109/TSP.2010.2062509; Marden JR, 2014, SIAM J CONTROL OPTIM, V52, P2753, DOI 10.1137/110850694; Menon A., 2013, AM CONTR C ACC; Nayyar N., 2016, IEEE T CONTROL NETW, P1; Papadimitriou C.H., 1998, COMBINATORIAL OPTIMI, VUnabridged edition; Pradelski BSR, 2012, GAME ECON BEHAV, V75, P882, DOI 10.1016/j.geb.2012.02.017; Rosenski J, 2016, PR MACH LEARN RES, V48; Shahrampour S, 2017, INT CONF ACOUST SPEE, P2786, DOI 10.1109/ICASSP.2017.7952664; Szorenyi B., 2013, INT C MACHINE LEARNI, P19; Vakili S, 2013, IEEE J-STSP, V7, P759, DOI 10.1109/JSTSP.2013.2263494; Zavlanos MM, 2008, IEEE DECIS CONTR P, P1212, DOI 10.1109/CDC.2008.4739098	27	25	25	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001074
C	Crowley, EJ; Gray, G; Storkey, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Crowley, Elliot J.; Gray, Gavin; Storkey, Amos			Moonshine: Distilling with Cheap Convolutions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Many engineers wish to deploy modern neural networks in memory-limited settings; but the development of flexible methods for reducing memory use is in its infancy, and there is little knowledge of the resulting cost-benefit. We propose structural model distillation for memory reduction using a strategy that produces a student architecture that is a simple transformation of the teacher architecture: no redesign is needed, and the same hyperparameters can be used. Using attention transfer, we provide Pareto curves/tables for distillation of residual networks with four benchmark datasets, indicating the memory versus accuracy payoff. We show that substantial memory savings are possible with very little loss of accuracy, and confirm that distillation provides student network performance that is better than training that student architecture directly on data.	[Crowley, Elliot J.; Gray, Gavin; Storkey, Amos] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland	University of Edinburgh	Crowley, EJ (corresponding author), Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.	elliot.j.crowley@ed.ac.uk; g.d.b.gray@ed.ac.uk; a.storkey@ed.ac.uk			European Union [732204]; Swiss State Secretariat for Education, Research and Innovation (SERI) [16.0159]	European Union(European Commission); Swiss State Secretariat for Education, Research and Innovation (SERI)	This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No. 732204 (Bonseyes). This work is supported by the Swiss State Secretariat for Education, Research and Innovation (SERI) under contract number 16.0159. The opinions expressed and arguments employed herein do not necessarily reflect the official views of these funding bodies. The authors are grateful to Sam Albanie, Luke Darlow, Jack Turner, and the anonymous reviewers for their helpful suggestions.	Ba L. J., 2014, ADV NEURAL INFORM PR; Bucila C, 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Denil Misha, 2013, NIPS, DOI DOI 10.5555/2999792.2999852; Garipov Timur, 2016, ARXIV161103214; Han S., 2016, P 4 INT C LEARN REPR, P1; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G., 2015, ARXIV150302531; Howard A.G., 2017, MOBILENETS EFFICIENT; Iandola Forrest N., 2016, SQUEEZENET ALEXNET L; Ioannou Y, 2017, P IEEE C COMP VIS PA, P1231; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; Jie Jin, 2015, 2015 23rd International Conference on Geoinformatics. Proceedings, P1, DOI 10.1109/GEOINFORMATICS.2015.7378627; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; LI Z., 2017, ARXIV170603912; Moczulski M., 2016, INT C LEARN REPR ICL; Romera E., 2017, IEEE INT VEH S GOLD; Romero Adriana, 2015, ICLR; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949; Sifre Laurent, 2014, THESIS; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Urban G., 2017, INT C LEARN REPR, P1; Wang M., 2016, ARXIV160804337; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yang Z., 2015, P IEEE INT C COMP VI; Yu F., 2016, P ICLR 2016; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zagoruyko S., 2017, P INT C LEARN REPR, DOI DOI 10.1109/CVPR.2019.00271; Zhang HY, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3277958; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	36	25	29	3	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302087
C	Ellis, K; Ritchie, D; Solar-Lezama, A; Tenenbaum, JB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ellis, Kevin; Ritchie, Daniel; Solar-Lezama, Armando; Tenenbaum, Joshua B.			Learning to Infer Graphics Programs from Hand-Drawn Images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce a model that learns to convert simple hand drawings into graphics programs written in a subset of (LTEX)-T-A. The model combines techniques from deep learning and program synthesis. We learn a convolutional neural network that proposes plausible drawing primitives that explain an image. These drawing primitives are a specification (spec) of what the graphics program needs to draw. We learn a model that uses program synthesis techniques to recover a graphics program from that spec. These programs have constructs like variable bindings, iterative loops, or simple kinds of conditionals. With a graphics program in hand, we can correct errors made by the deep network and extrapolate drawings.	[Ellis, Kevin; Solar-Lezama, Armando; Tenenbaum, Joshua B.] MIT, Cambridge, MA 02139 USA; [Ritchie, Daniel] Brown Univ, Providence, RI 02912 USA	Massachusetts Institute of Technology (MIT); Brown University	Ellis, K (corresponding author), MIT, Cambridge, MA 02139 USA.	ellisk@mit.edu; daniel_ritchie@brown.edu; asolar@csail.mit.edu; jbt@mit.edu		Solar Lezama, Armando/0000-0001-7604-8252; Ritchie, Daniel/0000-0002-8253-0069; Ellis, Kevin/0000-0001-6586-0632	NSF GRFP; NSF [1753684]; MUSE program (DARPA) [FA8750-14-2-0242]; AFOSR [FA9550-16-1-0012]; Center for Brains, Minds and Machines (CBMM) - NSF STC award [CCF-1231216]	NSF GRFP(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF(National Science Foundation (NSF)); MUSE program (DARPA); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); Center for Brains, Minds and Machines (CBMM) - NSF STC award	We are grateful for advice from Will Grathwohl and Jiajun Wu on the neural architecture, and for funding from NSF GRFP, NSF Award #1753684, the MUSE program (DARPA grant FA8750-14-2-0242), and AFOSR award FA9550-16-1-0012. This material is based upon work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216.	[Anonymous], 2016, ARXIV PREPRINT ARXIV; [Anonymous], 2015, P 28 INT C NEUR INF; Balog M., 2016, DEEPCODER LEARNING W; Beltramelli Tony, 2017, ABS170507962 CORR; Deng Y., 2017, ICML; Doucet A., 2001, SEQUENTIAL MONTE CAR; Eslami S. M. A., 2016, NIPS 2016, P3; Forbus K, 2011, TOP COGN SCI, V3, P648, DOI 10.1111/j.1756-8765.2011.01149.x; Ganin Yaroslav, 2018, ICML; Goodfellow I., DEEP LEARNING; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Hempel B, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P379, DOI 10.1145/2984511.2984575; Huang H., 2017, IEEE T VISUALIZATION; Kalyan A., 2018, ICLR; Levin L, 1973, PROBLEMY PEREDACI IN, V9, P115; Lezama A. S., 2008, THESIS; Nishida G, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925951; Paige B., 2016, INT C MACH LEARN, P3040; Polozov O, 2015, ACM SIGPLAN NOTICES, V50, P107, DOI [10.1145/2858965.2814310, 10.1145/2814270.2814310]; Ritchie  Daniel, 2016, NIPS; Schmidhuber J, 2004, MACH LEARN, V54, P211, DOI 10.1023/B:MACH.0000015880.99707.b2; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wu Jiajun, 2017, CVPR; [No title captured]	24	25	26	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000055
C	Gabrie, M; Manoel, A; Luneau, C; Barbier, J; Macris, N; Krzakala, F; Zdeborova, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gabrie, Marylou; Manoel, Andre; Luneau, Clement; Barbier, Jean; Macris, Nicolas; Krzakala, Florent; Zdeborova, Lenka			Entropy and mutual information in models of deep neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We examine a class of stochastic deep learning models with a tractable method to compute information-theoretic quantities. Our contributions are three-fold: (i) We show how entropies and mutual informations can be derived from heuristic statistical physics methods, under the assumption that weight matrices are independent and orthogonally-invariant. (ii) We extend particular cases in which this result is known to be rigorously exact by providing a proof for two-layers networks with Gaussian random weights, using the recently introduced adaptive interpolation method. (iii) We propose an experiment framework with generative models of synthetic datasets, on which we train deep neural networks with a weight constraint designed so that the assumption in (i) is verified during learning. We study the behavior of entropies and mutual informations throughout learning and conclude that, in the proposed setting, the relationship between compression and generalization remains elusive.	[Gabrie, Marylou; Barbier, Jean; Krzakala, Florent] PSL Univ, Lab Phys Stat, Ecole Normale Super, Paris, France; [Manoel, Andre] Univ Paris Saclay, CEA, INRIA, Parietal Team, St Aubin, France; [Manoel, Andre] Owkin Inc, New York, NY USA; [Manoel, Andre; Zdeborova, Lenka] Univ Paris Saclay, CNRS, Inst Phys Theor, CEA, St Aubin, France; [Luneau, Clement; Barbier, Jean; Macris, Nicolas] Ecole Polytech Fed Lausanne, Lab Theorie Commun, Lausanne, Switzerland; [Barbier, Jean] Abdus Salaam Int Ctr Theoret Phys, Trieste, Italy; [Krzakala, Florent; Zdeborova, Lenka] Duke Univ, Dept Math, Durham, NC 27706 USA; [Krzakala, Florent] Sorbonne Univ, Paris, France; [Krzakala, Florent] LightOn Inc, Paris, France	UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite; CEA; Inria; UDICE-French Research Universities; Universite Paris Saclay; CEA; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Abdus Salam International Centre for Theoretical Physics (ICTP); Duke University; UDICE-French Research Universities; Sorbonne Universite	Gabrie, M (corresponding author), PSL Univ, Lab Phys Stat, Ecole Normale Super, Paris, France.	marylou.gabrie@ens.fr	Krzakala, Florent/Q-9652-2019; Barbier, Jean/ABD-8759-2020	Krzakala, Florent/0000-0003-2313-2578; Barbier, Jean/0000-0002-2652-6727	ERC under the European Union's FP7 Grant [307087-SPARCS]; European Union's Horizon 2020 Research and Innovation Program [714608-SMiLe]; French Agence Nationale de la Recherche [ANR-17-CE23-0023-01 PAIL]; "Chaire de recherche sur les modeles et sciences des donnees", Fondation CFM pour la Recherche-ENS; Labex DigiCosme; Swiss National Science Foundation [200021E-175541]	ERC under the European Union's FP7 Grant; European Union's Horizon 2020 Research and Innovation Program; French Agence Nationale de la Recherche(French National Research Agency (ANR)); "Chaire de recherche sur les modeles et sciences des donnees", Fondation CFM pour la Recherche-ENS; Labex DigiCosme; Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission)	The authors would like to thank Leon Bottou, Antoine Maillard, Marc Mezard, Leo Miolane, and Galen Reeves for insightful discussions. This work has been supported by the ERC under the European Union's FP7 Grant Agreement 307087-SPARCS and the European Union's Horizon 2020 Research and Innovation Program 714608-SMiLe, as well as by the French Agence Nationale de la Recherche under grant ANR-17-CE23-0023-01 PAIL. Additional funding is acknowledged by MG from "Chaire de recherche sur les modeles et sciences des donnees", Fondation CFM pour la Recherche-ENS; by AM from Labex DigiCosme; and by CL from the Swiss National Science Foundation under grant 200021E-175541. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.	Achille A., 2017, ICML 2017 WORKSH PRI; Achille A, 2018, IEEE T PATTERN ANAL, V40, P2897, DOI 10.1109/TPAMI.2017.2784440; Advani Madhu S., 2017, ARXIV171003667, Patent No. [1710.03667, 171003667]; Alemi Alex, 2017, ICLR; AMIT DJ, 1985, PHYS REV LETT, V55, P1530, DOI 10.1103/PhysRevLett.55.1530; Baldassi C, 2007, P NATL ACAD SCI USA, V104, P11079, DOI 10.1073/pnas.0700324104; Barbier J., 2017, ARXIV170803395; Barbier J., 2017, PROBABILITY THEORY R; Barbier J., 2018, IEEE INT S INF THEOR; Barbier J, 2017, ANN ALLERTON CONF, P1056; Belghazi MI, 2018, PR MACH LEARN RES, V80; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Donoho D, 2016, PROBAB THEORY REL, V166, P935, DOI 10.1007/s00440-015-0675-z; Fletcher A. K., 2017, ARXIV170606549; Ganguli, 2017, ADV NEURAL INFORM PR, P4785; Ganguli S., 2014, INT C LEARN REPR; GARDNER E, 1989, J PHYS A-MATH GEN, V22, P1983, DOI 10.1088/0305-4470/22/12/004; Giryes R, 2016, IEEE T SIGNAL PROCES, V64, P3444, DOI 10.1109/TSP.2016.2546221; Jaakkola TS, 2001, NEU INF PRO, P129; Kabashima Y, 2008, J PHYS C SER, V95, DOI DOI 10.1088/1742-6596/95/1/012001; Kolchinsky A., 2017, ARXIV170502436; Kolchinsky A, 2017, ENTROPY-SWITZ, V19, DOI 10.3390/e19070361; Kraskov A, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066138; Louart C., 2017, IEEE INT C AC SPEECH; Manoel A, 2017, IEEE INT SYMP INFO, P2098, DOI 10.1109/ISIT.2017.8006899; MEZARD M, 1989, J PHYS A-MATH GEN, V22, P2181, DOI 10.1088/0305-4470/22/12/018; Mezard M., 1987, SPIN GLASS THEORY IN; Mezard M., 2009, INFORM PHYS COMPUTAT; Moczulski Marcin, 2016, INT C LEARN REPR ICL; Raghu M, 2017, PR MACH LEARN RES, V70; Rangan S., 2011, Proceedings of the 2011 IEEE International Symposium on Information Theory - ISIT, P2168, DOI 10.1109/ISIT.2011.6033942; Rangan S, 2017, IEEE INT SYMP INFO, P1588, DOI 10.1109/ISIT.2017.8006797; Reeves G., 2017, 55 ANN ALL C COMM CO; Saxe A. M., 2018, INT C LEARN REPR ICL; Schoenholz S. S., 2017, INT C LEARN REPR ICL; Shwartz-Ziv Ravid, 2017, ARXIV170300810; Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW); Tishby Naftali, 1999, P ANN ALL C COMM CON; Tulino AM, 2013, IEEE T INFORM THEORY, V59, P4243, DOI 10.1109/TIT.2013.2250578; Yang ZC, 2015, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2015.173; Zhao SS, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3490	47	25	25	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301078
C	Lu, YY; Fan, YY; Lv, JC; Noble, WS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lu, Yang Young; Fan, Yingying; Lv, Jinchi; Noble, William Stafford			DeepPINK: reproducible feature selection in deep neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				FALSE DISCOVERY RATE; TESTS; MASS; FAT	Deep learning has become increasingly popular in both supervised and unsupervised machine learning thanks to its outstanding empirical performance. However, because of their intrinsic complexity, most deep learning methods are largely treated as black box tools with little interpretability. Even though recent attempts have been made to facilitate the interpretability of deep neural networks (DNNs), existing methods are susceptible to noise and lack of robustness. Therefore, scientists are justifiably cautious about the reproducibility of the discoveries, which is often related to the interpretability of the underlying statistical models. In this paper, we describe a method to increase the interpretability and reproducibility of DNNs by incorporating the idea of feature selection with controlled error rate. By designing a new DNN architecture and integrating it with the recently proposed knockoffs framework, we perform feature selection with a controlled error rate, while maintaining high power. This new method, DeepPINK (Deep feature selection using Paired-Input Nonlinear Knockoffs), is applied to both simulated and real data sets to demonstrate its empirical utility.(2)	[Lu, Yang Young; Noble, William Stafford] Univ Washington, Dept Genome Sci, Seattle, WA 98195 USA; [Fan, Yingying; Lv, Jinchi] Univ Southern Calif, Marshall Sch Business, Data Sci & Operat Dept, Los Angeles, CA 90089 USA; [Noble, William Stafford] Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle; University of Southern California; University of Washington; University of Washington Seattle	Lu, YY (corresponding author), Univ Washington, Dept Genome Sci, Seattle, WA 98195 USA.	ylu465@uw.edu; fanyingy@marshall.usc.edu; jinchilv@marshall.usc.edu; william-noble@uw.edu			NIH [1R01GM131407-01, R01GM121818]; Simons Foundation; Adobe Data Science Research Award	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Simons Foundation; Adobe Data Science Research Award	This work was supported by NIH awards 1R01GM131407-01 and R01GM121818, a grant from the Simons Foundation, and an Adobe Data Science Research Award.	Abramovich F, 2006, ANN STAT, V34, P584, DOI 10.1214/009053606000000074; Barber RF, 2015, ANN STAT, V43, P2055, DOI 10.1214/15-AOS1337; Barber Rina Foygel, 2016, ARXIV160203574; Benjamini Y, 2001, ANN STAT, V29, P1165; BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x; Binder A, 2016, LECT NOTES COMPUT SC, V9887, P63, DOI 10.1007/978-3-319-44781-0_8; Blankson H, 2000, J NUTR, V130, P2943, DOI 10.1093/jn/130.12.2943; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breiman L., 2017, CLASSIFICATION REGRE; Candes E, 2018, J R STAT SOC B, V80, P551, DOI 10.1111/rssb.12265; Chang Y-W, 2008, CAUSATION PREDICTION, P53; Chiu CM, 2014, BIOMED RES INT, V2014, DOI 10.1155/2014/906168; Efron B, 2007, J AM STAT ASSOC, V102, P93, DOI 10.1198/016214506000001211; Fan JQ, 2007, J AM STAT ASSOC, V102, P1282, DOI 10.1198/016214507000000969; Fan YY, 2016, ANN STAT, V44, P2098, DOI 10.1214/15-AOS1416; Fan YY, 2011, J ECONOMETRICS, V164, P331, DOI 10.1016/j.jeconom.2011.06.014; Fan  Yingying, 2017, ARXIV170900092; Fan  Yingying, 2017, ARXIV170503604; Ghorbani A, 2017, ARXIV171010547; Hall P, 2010, BERNOULLI, V16, P418, DOI 10.3150/09-BEJ220; Kingma D.P, P 3 INT C LEARNING R; Kuang YS, 2017, GIGASCIENCE, V6, DOI 10.1093/gigascience/gix058; Lin W, 2014, BIOMETRIKA, V101, P785, DOI 10.1093/biomet/asu031; Lipton Zachary C, 2016, INT C MACH LEARN WOR; Obermeyer Z, 2016, NEW ENGL J MED, V375, P1216, DOI 10.1056/NEJMp1606181; Pimpin L, 2016, AM J CLIN NUTR, V103, P389, DOI 10.3945/ajcn.115.118612; Rabot S, 2016, SCI REP-UK, V6, DOI 10.1038/srep32484; Reeds DN, 2013, AESTHET SURG J, V33, P400, DOI 10.1177/1090820X13478630; Rhee SY, 2006, P NATL ACAD SCI USA, V103, P17355, DOI 10.1073/pnas.0607274103; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Shrikumar Avanti, 2017, PMLR, P3145, DOI DOI 10.1145/3292500.3330701; Simonyan Karen, 2013, DEEP INSIDE CONVOLUT, P2; Storey JD, 2004, J R STAT SOC B, V66, P187, DOI 10.1111/j.1467-9868.2004.00439.x; Sundararajan M, 2017, PR MACH LEARN RES, V70; Turner R, 2016, IEEE INT WORKS MACH; Vanhala M, 2012, AM J EPIDEMIOL, V176, P253, DOI 10.1093/aje/kwr504; Wu WB, 2008, ANN STAT, V36, P364, DOI 10.1214/009053607000000730; Yang Qing, 2010, Yale Journal of Biology and Medicine, V83, P101; Yang YJ, 2012, NUTR RES PRACT, V6, P68, DOI 10.4162/nrp.2012.6.1.68; Yun Y, 2017, BMC MICROBIOL, V17, DOI 10.1186/s12866-017-1052-0; Zhang Y, 2011, J AM STAT ASSOC, V106, P846, DOI 10.1198/jasa.2011.ap10657; Zheng Z., 2017, ARXIV171002704	45	25	25	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003025
C	Tartaglione, E; Lepsoy, S; Fiandrotti, A; Francini, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tartaglione, Enzo; Lepsoy, Skjalg; Fiandrotti, Attilio; Francini, Gianluca			Learning Sparse Neural Networks via Sensitivity-Driven Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The ever-increasing number of parameters in deep neural networks poses challenges for memory-limited applications. Regularize-and-prune methods aim at meeting these challenges by sparsifying the network weights. In this context we quantify the output sensitivity to the parameters (i.e. their relevance to the network output) and introduce a regularization term that gradually lowers the absolute value of parameters with low sensitivity. Thus, a very large fraction of the parameters approach zero and are eventually set to zero by simple thresholding. Our method surpasses most of the recent techniques both in terms of sparsity and error rates. In some cases, the method reaches twice the sparsity obtained by other techniques at equal error rates.	[Tartaglione, Enzo; Fiandrotti, Attilio] Politecn Torino, Turin, Italy; [Lepsoy, Skjalg] Nuance Commun, Turin, Italy; [Francini, Gianluca] Telecom Italia, Turin, Italy	Polytechnic University of Turin; Telecom Italia	Tartaglione, E (corresponding author), Politecn Torino, Turin, Italy.	tartaglioneenzo@gmail.com	Tartaglione, Enzo/Q-6440-2018	Fiandrotti, Attilio/0000-0002-9991-6822; TARTAGLIONE, ENZO/0000-0003-4274-8298	TIM	TIM	The authors would like to thank the anonymous reviewers for their valuable comments and suggestions. This work was done at the Joint Open Lab Cognitive Computing and was supported by a fellowship from TIM.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Brutzkus Alon, 2017, ARXIV PREPRINT ARXIV; Culurciello E, 2003, IEEE J SOLID-ST CIRC, V38, P281, DOI 10.1109/JSSC.2002.807412; Engelbrecht AP, 1996, IEEE IJCNN, P1274, DOI 10.1109/ICNN.1996.549081; Glorot X, 2011, P 14 INT C ART INT S, V15, P315, DOI DOI 10.1177/1753193410395357; Groetsch CW., 1993, INVERSE PROBLEMS MAT, DOI 10.1007/978-3-322-99202-4; Guo YW, 2016, ADV NEUR IN, V29; HAN S., 2015, ARXIV151000149; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; Hu J., 2018, CVPR; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Louizos C., 2018, ICLR; Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681; Molchanov D., 2017, ARXIV170105369; Mrazova I, 2012, PROCEDIA COMPUT SCI, V12, P194, DOI 10.1016/j.procs.2012.09.053; Mrazova I, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2143, DOI 10.1109/IJCNN.2011.6033493; Ranzato M., 2008, PROC NEURAL INF PROC, P1185; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Theis Lucas, 2018, ARXIV180105787; Ullrich K., 2017, 5 INT C LEARN REPPR; Wen W., 2016, ADV NEURAL INFORM PR, P2074; Yuret Deniz, 2016, MACH LEARN SYST WORK; Zhu Michael, 2017, ARXIV171001878	25	25	27	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303084
C	Hausman, K; Chebotar, Y; Schaal, S; Sukhatme, G; Lim, JJ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hausman, Karol; Chebotar, Yevgen; Schaal, Stefan; Sukhatme, Gaurav; Lim, Joseph J.			Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Imitation learning has traditionally been applied to learn a single task from demonstrations thereof. The requirement of structured and isolated demonstrations limits the scalability of imitation learning approaches as they are difficult to apply to real-world scenarios, where robots have to be able to execute a multitude of tasks. In this paper, we propose a multi-modal imitation learning framework that is able to segment and imitate skills from unlabelled and unstructured demonstrations by learning skill segmentation and imitation learning jointly. The extensive simulation results indicate that our method can efficiently separate the demonstrations into individual skills and learn to imitate them using a single multi-modal policy. The video of our experiments is available at http://sites.google.com/view/nips17intentiongan.	[Hausman, Karol; Chebotar, Yevgen; Schaal, Stefan; Sukhatme, Gaurav; Lim, Joseph J.] Univ Southern Calif, Los Angeles, CA 90007 USA; [Chebotar, Yevgen; Schaal, Stefan] Max Planck Inst Intelligent Syst, Tubingen, Germany	University of Southern California; Max Planck Society	Hausman, K (corresponding author), Univ Southern Calif, Los Angeles, CA 90007 USA.	hausman@usc.edu; ychebota@usc.edu; sschaal@usc.edu; gaurav@usc.edu; limjj@usc.edu	Jeong, Yongwook/N-7413-2016		National Science Foundation [IIS-1205249, IIS-1017134, EECS-0926052]; Office of Naval Research; Okawa Foundation; Max-Planck-Society	National Science Foundation(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research); Okawa Foundation; Max-Planck-Society(Max Planck SocietyFoundation CELLEX)	This research was supported in part by National Science Foundation grants IIS-1205249, IIS-1017134, EECS-0926052, the Office of Naval Research, the Okawa Foundation, and the Max-Planck-Society. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the funding organizations.	[Anonymous], 2016, ARXIV161001945; Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024; Arjovsky M., 2017, ARXIV170107875; BabeVroman M., 2011, P 28 INT C MACHINE L, P897; Chebotar Y., 2016, ARXIV161000529; Chen X., 2016, INFOGAN INTERPRETABL; Denton E, 2015, DEEP GENERATIVE IMAG, DOI DOI 10.5555/; Dimitrakakis C., 2011, P EUR WORKSH REINF L, P273; Duan Y, 2017, ARXIV170307326; Finn C., 2016, ARXIV161103852; Finn Chelsea, 2016, P 33 INT C MACH LEAR, V48; Florensa Carlos, 2017, ARXIV170403012; Fox R., 2017, ARXIV170308294; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Ho J., 2016, ABS160603476 CORR; Kalakrishnan M, 2011, IEEE INT C INT ROBOT, P4639, DOI 10.1109/IROS.2011.6048825; Kim T, 2017, PR MACH LEARN RES, V70; Kroemer O, 2015, IEEE INT CONF ROBOT, P1503, DOI 10.1109/ICRA.2015.7139389; Levine S., 2011, C NEURAL INFORM PROC, V24, P19; Li Yunzhu, 2017, ABS170308840 CORR; Mathieu M., 2016, INT C LEARN REPR ICL; Mulling K, 2013, INT J ROBOT RES, V32, P263, DOI 10.1177/0278364912472380; Ng A. Y., 2004, P ICML; Niekum S., 2013, ROBOTICS SCI SYSTEMS, V9; Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88; Ross S., 2010, AISTATS, P3; Schaal S, 1999, TRENDS COGN SCI, V3, P233, DOI 10.1016/S1364-6613(99)01327-3; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Sonderby Casper Kaae, 2016, ABS161004490 CORR; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Vezhnevets A. S., 2017, ARXIV170301161; Zhu J.-Y., 2017, ARXIV170310593; Ziebart B. D., 2008, AAAI, V8, P1433	35	25	25	2	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401027
C	Hayes, J; Danezis, G		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hayes, Jamie; Danezis, George			Generating steganographic images via adversarial training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Adversarial training has proved to be competitive against supervised learning methods on computer vision tasks. However, studies have mainly been confined to generative tasks such as image synthesis. In this paper, we apply adversarial training techniques to the discriminative task of learning a steganographic algorithm. Steganography is a collection of techniques for concealing the existence of information by embedding it within a non-secret medium, such as cover texts or images. We show that adversarial training can produce robust steganographic techniques: our unsupervised training scheme produces a steganographic algorithm that competes with state-of-the-art steganographic techniques. We also show that supervised training of our adversarial model produces a robust steganalyzer, which performs the discriminative task of deciding if an image contains secret information. We define a game between three parties, Alice, Bob and Eve, in order to simultaneously train both a steganographic algorithm and a steganalyzer. Alice and Bob attempt to communicate a secret message contained within an image, while Eve eavesdrops on their conversation and attempts to determine if secret information is embedded within the image. We represent Alice, Bob and Eve by neural networks, and validate our scheme on two independent image datasets, showing our novel method of studying steganographic problems is surprisingly competitive against established steganographic techniques.	[Hayes, Jamie] UCL, London, England; [Danezis, George] UCL, Alan Turing Inst, London, England	University of London; University College London; University of London; University College London	Hayes, J (corresponding author), UCL, London, England.	j.hayes@cs.ucl.ac.uk; g.danezis@ucl.ac.uk	Jeong, Yongwook/N-7413-2016		UK Government Communications Headquarters (GCHQ), University College London; Google PhD Fellowship in Machine Learning	UK Government Communications Headquarters (GCHQ), University College London; Google PhD Fellowship in Machine Learning(Google Incorporated)	The authors would like to acknowledge financial support from the UK Government Communications Headquarters (GCHQ), as part of University College London's status as a recognised Academic Centre of Excellence in Cyber Security Research. Jamie Hayes is supported by a Google PhD Fellowship in Machine Learning. We thank the anonymous reviewers for their comments.	Abadi M., TENSORFLOW LARGE SCA; [Anonymous], 2013, P ICML; [Anonymous], 2016, ARXIV161006918; Borisenko Boris, 2016, ICLR; Chen Z., 2016, P 12 USENIX S OP SYS, P265, DOI 10.5555/ 3026877.3026899; Filler Tomas, 2009, IS T SPIE ELECT IMAG; Fridrich J., 2001, IEEE Multimedia, V8, P22, DOI 10.1109/93.959097; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hayden M., 2014, PRICE PRIVACY REEVAL; Holub V, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-1; Holub V, 2012, IEEE INT WORKS INFOR, P234, DOI 10.1109/WIFS.2012.6412655; Kingma D.P, P 3 INT C LEARNING R; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Lerch-Hostalot D, 2016, ENG APPL ARTIF INTEL, V50, P45, DOI 10.1016/j.engappai.2015.12.013; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Mielikainen J, 2006, IEEE SIGNAL PROC LET, V13, P285, DOI 10.1109/LSP.2006.870357; Nair V., 2010, ICML, P807; Pevny T, 2010, LECT NOTES COMPUT SC, V6387, P161, DOI 10.1007/978-3-642-16435-4_13	18	25	26	1	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402001
C	Rebuffi, SA; Bilen, H; Vedaldi, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Rebuffi, Sylvestre-Alvise; Bilen, Hakan; Vedaldi, Andrea			Learning multiple visual domains with residual adapters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					There is a growing interest in learning data representations that work well for many different types of problems and data. In this paper, we look in particular at the task of learning a single visual representation that can be successfully utilized in the analysis of very different types of images, from dog breeds to stop signs and digits. Inspired by recent work on learning networks that predict the parameters of another, we develop a tunable deep network architecture that, by means of adapter residual modules, can be steered on the fly to diverse visual domains. Our method achieves a high degree of parameter sharing while maintaining or even improving the accuracy of domain-specific representations. We also introduce the Visual Decathlon Challenge, a benchmark that evaluates the ability of representations to capture simultaneously ten very different visual domains and measures their ability to perform well uniformly.	[Rebuffi, Sylvestre-Alvise; Bilen, Hakan; Vedaldi, Andrea] Univ Oxford, Visual Geometry Grp, Oxford, England; [Bilen, Hakan] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland	University of Oxford; University of Edinburgh	Rebuffi, SA (corresponding author), Univ Oxford, Visual Geometry Grp, Oxford, England.	srebuffi@robots.ox.ac.uk; hbilen@robots.ox.ac.uk; vedaldi@robots.ox.ac.uk	Jeong, Yongwook/N-7413-2016; Bilen, Hakan/ACY-3128-2022; Bilen, Hakan/AAG-3202-2022		Mathworks/DTA [DFR02620]; ERC [677195-IDIU]	Mathworks/DTA; ERC(European Research Council (ERC)European Commission)	This work acknowledges the support of Mathworks/DTA DFR02620 and ERC 677195-IDIU.	[Anonymous], 2017, P CVPR; Argyriou A., 2007, NIPS, V19, P41, DOI DOI 10.1007/S10994-007-5040-8; Bertinetto Luca, 2016, NIPS; Bilen H., 2016, P NIPS; Bilen H., 2016, P CVPR; Bilen Hakan, 2017, ARXIV170107275; Brabandere B.D., 2016, ADV NEURAL INFORM PR, P667; Caruana R., 1997, MACHINE LEARNING, V28; Cimpoi M., 2014, P CVPR; Collobert R., 2008, AUNIFIED ARCHITECTUR, P160; Daume H, 2007, P 45 ANN M ASS COMP, V45, P256; Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109; French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2; Ganin Y., 2015, P ICML; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Huang JT, 2013, INT CONF ACOUST SPEE, P7304, DOI 10.1109/ICASSP.2013.6639081; Ioffe S., 2015, P 32 INT C MACH LEAR, P448; Kirkpatrick J., 2017, NATL ACAD SCI; Kokkinos I., 2017, P CVPR; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Li ZZ, 2016, LECT NOTES COMPUT SC, V9908, P614, DOI 10.1007/978-3-319-46493-0_37; Liu Xiaodong, 2015, P 2015 C N AM CHAPT, P912, DOI [DOI 10.3115/V1/N15-1092, 10.3115/v1/ n15-1092]; Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136; Maji S., 2013, TECHNICAL REPORT, P6; Mitchell T., 2010, NEVER ENDING LEARNIN; Munder S, 2006, IEEE T PATTERN ANAL, V28, P1863, DOI 10.1109/TPAMI.2006.217; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Nilsback M., 2008, ICCVGIP; Razavian A., 2014, CVPR DEEPVISION WORK; Rosenfeld A., 2017, ARXIV170504228; Russakovsky O., 2014, IMAGENET LARGE SCALE; Rusu A. A., 2016, PROGR NEURAL NETWORK; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131; Soomro K., 2012, CRCVTR1201; Stallkamp J, 2012, NEURAL NETWORKS, V32, P323, DOI 10.1016/j.neunet.2012.02.016; Terekhov AV, 2015, LECT NOTES ARTIF INT, V9222, P268, DOI 10.1007/978-3-319-22979-9_27; Thrun S, 1998, LEARNING TO LEARN, P181; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]; Zhang TZ, 2013, INT J COMPUT VISION, V101, P367, DOI 10.1007/s11263-012-0582-z; Zhang Z., 2014, P ECCV	42	25	25	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400049
C	Tian, T; Zhu, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Tian, Tian; Zhu, Jun			Max-Margin Majority Voting for Learning from Crowds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				POSTERIOR REGULARIZATION	Learning-from-crowds aims to design proper aggregation strategies to infer the unknown true labels from the noisy labels provided by ordinary web workers. This paper presents max-margin majority voting ((MV)-V-3) to improve the discriminative ability of majority voting and further presents a Bayesian generalization to incorporate the flexibility of generative methods on modeling noisy observations with worker confusion matrices. We formulate the joint learning as a regularized Bayesian inference problem, where the posterior regularization is derived by maximizing the margin between the aggregated score of a potential true label and that of any alternative label. Our Bayesian model naturally covers the Dawid-Skene estimator and (MV)-V-3. Empirical results demonstrate that our methods are competitive, often achieving better results than state-of-the-art estimators.	[Tian, Tian] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China; Tsinghua Univ, Ctr Bioinspired Comp Res, Tsinghua Natl Lab Informat Sci & Technol, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China	Tsinghua University; Tsinghua University	Tian, T (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.	tiant13@mails.tsinghua.edu.cn; dcszj@tsinghua.edu.cn			National Basic Research Program (973 Program) of China [2013CB329403, 2012CB316301]; National NSF of China [61322308, 61332007]; Tsinghua National Laboratory for Information Science and Technology Big Data Initiative; Tsinghua Initiative Scientific Research Program [20121088071, 20141080934]	National Basic Research Program (973 Program) of China(National Basic Research Program of China); National NSF of China(National Natural Science Foundation of China (NSFC)); Tsinghua National Laboratory for Information Science and Technology Big Data Initiative; Tsinghua Initiative Scientific Research Program	The work was supported by the National Basic Research Program (973 Program) of China (Nos. 2013CB329403, 2012CB316301), National NSF of China (Nos. 61322308, 61332007), Tsinghua National Laboratory for Information Science and Technology Big Data Initiative, and Tsinghua Initiative Scientific Research Program (Nos. 20121088071, 20141080934).	Carlson Andrew, 2010, AAAI; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chen C., 2014, NIPS; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Dudik M., 2007, JMLR, V8; Ganchev K, 2010, J MACH LEARN RES, V11, P2001; Jagabathula Srikanth, 2014, NIPS; Karger D. R., 2011, ADV NEURAL INFORM PR, P1953; Li H., 2014, ARXIV14114086; Liu Qiang, 2012, ADV NEURAL INFORM PR, V25, P692; MICHAEL JR, 1976, AM STAT, V30, P88, DOI 10.2307/2683801; Otto C., 2014, IEEE T PAMI; PeterWelinder Steve Branson, 2010, P NIPS, V23, P1; Polson NG, 2011, BAYESIAN ANAL, V6, P1, DOI 10.1214/11-BA601; Raykar VC, 2010, J MACH LEARN RES, V11, P1297; Shi T., 2014, ICML; Snow Rion, 2008, P 2008 C EMP METH NA, P254, DOI DOI 10.3115/1613715.1613751; Tian T., 2015, PAKDD; Whitehill J., 2009, ADV NEURAL INFORM PR, P2035; Xu L., 2005, AAAI; Zaidan Omar, 2011, P 49 ANN M ASS COMPU, P37; Zhang Y., 2014, NIPS; Zhou D., 2012, NIPS; Zhou Dengyong, 2014, ICML; Zhu J, 2014, J MACH LEARN RES, V15, P1799; Zhu J, 2014, J MACH LEARN RES, V15, P1073	27	25	25	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103014
C	Dosovitskiy, A; Springenberg, JT; Riedmiller, M; Brox, T		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Dosovitskiy, Alexey; Springenberg, Jost Tobias; Riedmiller, Martin; Brox, Thomas			Discriminative Unsupervised Feature Learning with Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).	[Dosovitskiy, Alexey; Springenberg, Jost Tobias; Riedmiller, Martin; Brox, Thomas] Univ Freiburg, Dept Comp Sci, D-79110 Freiburg, Germany	University of Freiburg	Dosovitskiy, A (corresponding author), Univ Freiburg, Dept Comp Sci, D-79110 Freiburg, Germany.	dosovits@cs.uni-freiburg.de; springj@cs.uni-freiburg.de; riedmiller@cs.uni-freiburg.de; brox@cs.uni-freiburg.de			ERC Starting Grant VideoLearn [279401]; BrainLinks-BrainTools Cluster of Excellence - German Research Foundation (DFG) [EXC 1086]	ERC Starting Grant VideoLearn; BrainLinks-BrainTools Cluster of Excellence - German Research Foundation (DFG)(German Research Foundation (DFG))	We acknowledge funding by the ERC Starting Grant VideoLearn (279401); the work was also partly supported by the BrainLinks-BrainTools Cluster of Excellence funded by the German Research Foundation (DFG, grant number EXC 1086).	Adams R. P, 2013, NIPS; AHMED A, 2008, EUR C COMP VIS, V5304, P69, DOI DOI 10.1007/978-3-540-88690-76; AMINI MR, 2002, ECAI, V77, P390; Bo L, 2012, ISER; Bo LF, 2013, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2013.91; Boureau Y., 2011, ICCV 11; Cho  K., 2013, ICML JMLR WORKSH C P; Coates A., 2011, AISTATS; Coates A., 2011, ADV NEURAL INFORM PR, P2528, DOI DOI 10.1016/J.PSYCHRES.2009.03.008; Collobert R, 2011, J MACH LEARN RES, V12, P2493; DRUCKER H, 1992, IEEE T NEURAL NETWOR, V3, P991, DOI 10.1109/72.165600; Fei-Fei  L., 2004, CVPR WGMBV; Fischer P., 2014, ARXIV14055769V1CSCV; Girshick R., 2014, CVPR, DOI 10.1109/CVPR.2014.81; Grandvalet Y., 2006, SEMISUPERVISED LEARN, P1, DOI [10.7551/mitpress/9780262033589.001.0001, DOI 10.7551/MITPRESS/9780262033589.001.0001]; He K., 2014, ECCV; Hinton G. E., 2012, ARXIVCS12070580V3; Hui Ka Y, 2013, ICML; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Kavukcuoglu K., 2010, NIPS; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lin M., 2014, ICLR; Rifai S., 2011, ADV NEURAL INF PROCE, V24; Simard P., 1992, NIPS; Sohn K., 2012, 29 INT C MACH LEARN, P1311; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Wager S., 2013, NIPS; Zeiler MD, 2014, ECCV; Zhang Ning, 2014, ICML; Zou W., 2012, ADV NEURAL INFORM PR, P3203	32	25	25	2	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102109
C	Tompson, J; Jain, A; LeCun, Y; Bregler, C		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Tompson, Jonathan; Jain, Arjun; LeCun, Yann; Bregler, Christoph			Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					This paper proposes a new hybrid architecture that consists of a deep Convolutional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.	[Tompson, Jonathan; Jain, Arjun; LeCun, Yann; Bregler, Christoph] NYU, New York, NY 10003 USA	New York University	Tompson, J (corresponding author), NYU, New York, NY 10003 USA.	tompson@cs.nyu.edu; ajain@cs.nyu.edu; yann@cs.nyu.edu; bregler@cs.nyu.edu			Office of Naval Research ONR [N000141210327]	Office of Naval Research ONR(Office of Naval Research)	The authors would like to thank Mykhaylo Andriluka for his support. This research was funded in part by the Office of Naval Research ONR Award N000141210327.	Andriluka M., 2009, CVPR; [Anonymous], 2014, CVPR; Bergtholdt M., 2010, IJCV; Bourdev L., 2009, ICCV; Bourlard H., 1995, EUROSPEECH; Bregler C., 2014, ICLR; Buehler P., 2009, CVPR; Collobert R., 2011, BIGLEARN NIPS WORKSH; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dantone M., CVPR 13; Eichner M., 2009, BMVC; Eigen D., 2014, 6 INT C LEARN REPR I; Felzenszwalb P., 2008, CVPR; Giusti A., 2013, CORR; Gkioxari G., CVPR 13; Grauman K., 2003, ICCV; Heitz G., 2008, CASCADED CLASSIFICAT; Johnson S., 2010, P BRIT MACH VIS C; Johnson S., CVPR 11; Lowe D. G., 1999, ICCV; Mathieu M, 2013, CORR; Mori G., 2002, ECCV; Morin F., 2005, P MACHINE LEARNING R, VR5, P246; Ning F., 2005, IEEE TIP; Pishchulin L., ICCV 13; Pishchulin L., CVPR 13; Ramanan D., 2005, CVPR; Ross S., 2011, CVPR; Sapp B., 2013, CVPR; Tompson J., 2014, TOG; Yang Yi, CVPR 11	31	25	25	3	20	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103024
C	Chang, YH; Ho, A; Kaelbling, LP		Thrun, S; Saul, K; Scholkopf, B		Chang, YH; Ho, A; Kaelbling, LP			All learning is local: Multi-agent learning in global reward games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					In large multiagent games, partial observability, coordination, and credit assignment persistently plague attempts to design good learning algorithms. We provide a simple and efficient algorithm that in part uses a linear system to model the world from a single agent's limited perspective, and takes advantage of Kalman filtering to allow an agent to construct a good training signal and learn an effective policy.	MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Chang, YH (corresponding author), MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA.							AUER P, 1995, P 36 ANN S FDN COMP; CHANG Y, 2003, AIM2003025 MIT AI LA; Choi S. P., 1999, IJCAI WORKSH NEUR SY; CLAUS C, 1998, P 15 AAAI; KALMAN RE, 1960, T AM SOC MECH ENG J; MCMAHAN H, 2003, P 20 ICML; NG AY, 1999, P 16 ICML; Sutton R. S., 1999, REINFORCEMENT LEARNI; SZITA I, 2002, J MACHINE LEARNING R; Wolpert DH, 1999, NASAARCIC9963	10	25	27	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						807	814						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500101
C	Dekel, O; Manning, CD; Singer, Y		Thrun, S; Saul, K; Scholkopf, B		Dekel, O; Manning, CD; Singer, Y			Log-linear models for label ranking	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Label ranking is the task of inferring a total order over a predefined set of labels for each given instance. We present a general framework for batch learning of label ranking functions from supervised data. We assume that each instance in the training data is associated with a list of preferences over the label-set, however we do not assume that this list is either complete or consistent. This enables us to accommodate a variety of ranking problems. In contrast to the general form of the supervision, our goal is to learn a ranking function that induces a total order over the entire set of labels. Special cases of our setting are multilabel categorization and hierarchical classification. We present a general boosting-based learning algorithm for the label ranking problem and prove a lower bound on the progress of each boosting iteration. The applicability of our approach is demonstrated with a set of experiments on a large-scale text corpus.	Hebrew Univ Jerusalem, Jerusalem, Israel	Hebrew University of Jerusalem	Dekel, O (corresponding author), Hebrew Univ Jerusalem, Jerusalem, Israel.	oferd@cs.huji.ac.il; manning@cs.stanford.edu; singer@cs.huji.ac-il	Manning, Christopher/AAM-9535-2020	Manning, Christopher/0000-0001-6155-649X				Collins M, 2002, MACH LEARN, V48, P253, DOI 10.1023/A:1013912006537; COLLINS M, 2002, 30 ANN M ACL; Crammer K, 2003, J MACH LEARN RES, V3, P1025, DOI 10.1162/153244303322533188; CRAMMER K, 2001, NIPS, V14; DEKEL O, 2003, COLT, V16; Elisseeff A, 2001, NIPS, V14; Freund Y., 1998, MACH LEARN P 15 INT; LEBANON G, 2001, NIPS, V14; LEBANON G, 2002, NIPS, V15; SCHAPIRE RE, 2000, MACHINE LEARNING, V32; SHASHUA A, 2000, NIPS, V15; TOUTANOVA K, 2002, P 6 C NAT LANG LEARN	12	25	25	1	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						497	504						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500063
C	Raina, R; Shen, YR; Ng, AY; McCallum, A		Thrun, S; Saul, K; Scholkopf, B		Raina, R; Shen, YR; Ng, AY; McCallum, A			Classification with hybrid generative/discriminative models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Although discriminatively trained classifiers are usually more accurate when labeled training data is abundant, previous work has shown that when training data is limited, generative classifiers can out-perform them. This paper describes a hybrid model in which a high-dimensional subset of the parameters are trained to maximize generative likelihood, and another, small, subset of parameters are discriminatively trained to maximize conditional likelihood. We give a sample complexity bound showing that in order to fit the discriminative parameters well, the number of training examples required depends only on the logarithm of the number of feature occurrences and feature set size. Experimental results show that hybrid models can provide lower test error and can produce better accuracy/coverage curves than either their purely generative or purely discriminative counterparts. We also discuss several advantages of hybrid models, and advocate further work in this area.	Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Raina, R (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.							Andrew McCallum, 1998, AAAI 98 WORKSH LEARN, P752; [Anonymous], [No title captured]; BENNETT PN, 2003, P SIGIR 03 26 ACM IN; DEVROYE LP, 1979, IEEE T INFORMATION T, V5; JAAKKOLA T, 1998, ADV NEURAL INFORMATI, V11; JEBARA T, 1998, ADV NEURAL INFORMATI, V11; JURAFSKY D, 2000, SPEECH LANGUAGE POCE; KEARNS M, 1997, COMPUTATIONAL LEARNI; LAFFERTY J, 1999, IJCAI 99 WORKSH MACH; LANG K, 1997, P 9 EUR C MACH LEARN; LEWIS DD, 1994, P SIGIR 94 17 ACM IN; Platt J., 1999, ADV LARGE MARGIN CLA; Raina R., 2003, CLASSIFICATION HYBRI; Vapnik V.N, 1998, STAT LEARNING THEORY; WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1; ZADROZNY B, 2001, ICML 01	16	25	26	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						545	552						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500069
C	Schwaighofer, A; Grigoras, M; Tresp, V; Hoffmann, C		Thrun, S; Saul, K; Scholkopf, B		Schwaighofer, A; Grigoras, M; Tresp, V; Hoffmann, C			GPPS: A Gaussian process positioning system for cellular networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					In this article, we present a novel approach to solving the localization problem in cellular networks. The goal is to estimate a mobile user's position, based on measurements of the signal strengths received from network base stations. Our solution works by building Gaussian process models for the distribution of signal strengths, as obtained in a series of calibration measurements. In the localization stage, the user's position can be estimated by maximizing the likelihood of received signal strengths with respect to the position. We investigate the accuracy of the proposed approach on data obtained within a large indoor cellular network.	Siemens Corp Technol Informat & Commun, D-81730 Munich, Germany	Siemens AG; Siemens Germany	Schwaighofer, A (corresponding author), Siemens Corp Technol Informat & Commun, D-81730 Munich, Germany.							Bahl P., 2000, MICROSOFT RES, P1; Castro P., 2001, P 3 INT C UB COMP UB; HAMPRECHT FA, 2002, EXPT DESIGN COMBINAT; HASHEMI H, 1993, P IEEE, V81, P943, DOI 10.1109/5.231342; LADD AM, 2002, P 8 ACM INT C MOB CO; Rasmussen C.E., 1996, THESIS U TORONTO; Roos T., 2002, International Journal of Wireless Information Networks, V9, P155, DOI 10.1023/A:1016003126882; SCHWAIGHOFER A, 2003, THESIS GRAZ U TECHNO; Stein M.L., 1999, INTERPOLATION SPATIA	9	25	26	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						579	586						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500073
C	Sprague, N; Ballard, D		Thrun, S; Saul, K; Scholkopf, B		Sprague, N; Ballard, D			Eye movements for reward maximization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Recent eye tracking studies in natural tasks suggest that there is a tight link between eye movements and goal directed motor actions. However, most existing models of human eye movements provide a bottom up account that relates visual attention to attributes of the visual scene. The purpose of this paper is to introduce a new model of human eye movements that directly ties eye movements to the ongoing demands of behavior. The basic idea is that eye movements serve to reduce uncertainty about environmental variables that are task relevant. A value is assigned to an eye movement by estimating the expected cost of the uncertainty that will result if the movement is not made. If there are several candidate eye movements, the one with the highest expected value is chosen. The model is illustrated using a humanoid graphic figure that navigates on a sidewalk in a virtual urban environment. Simulations show our protocol is superior to a simple round robin scheduling mechanism.	Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA	University of Rochester	Sprague, N (corresponding author), Univ Rochester, Dept Comp Sci, 601 Elmwood Ave, Rochester, NY 14627 USA.							BALLARD D, 2002, J VISION, V2, pA568; BROOKS RA, 1986, IEEE T ROBOTIC AUTOM, V2, P14, DOI 10.1109/JRA.1986.1087032; Cassandra AR, 1998, EXACT APPROXIMATE AL; Humphrys M., 1996, P 4 INT C SIM AD BEH; Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500; Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301; Kalman RE., 1960, J BASIC ENG-T ASME, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Karlsson J., 1997, LEARNING SOLVE MULTI; LAND MF, 1994, NATURE, V377; MALONEY L, IN PRESS J VISION; SEARA JF, 2003, P 3 IEEE RAS INT C H; SHINODA H, 2001, VISION RES, V41; SPRAGUE N, 2004, 829 U ROCH COMP SCI; Sprague N., 2003, P 18 INT JOINT C ART, P1445; Sutton R.S., 1988, REINFORCEMENT LEARNI; SUTTON RS, 1996, ADV NEURAL INFORMATI, V8; WAELTI P, 2001, NATURE, V412	17	25	25	0	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1467	1474						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500182
C	Kivinen, J; Smola, AJ; Williamson, RC		Dietterich, TG; Becker, S; Ghahramani, Z		Kivinen, J; Smola, AJ; Williamson, RC			Online learning with kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				MARGIN	We consider online learning in a Reproducing Kernel Hilbert Space. Our method is computationally efficient and leads to simple algorithms. In particular we derive update equations for classification, regression, and novelty detection. The inclusion of the v-trick allows us to give a robust parameterization. Moreover, unlike in batch learning where the v-trick only applies to the E-insensitive loss function we are able to derive general trimmed-mean types of estimators such as for Huber's robust loss.	Australian Natl Univ, Res Sch Informat Sci & Engn, Canberra, ACT 0200, Australia	Australian National University	Kivinen, J (corresponding author), Australian Natl Univ, Res Sch Informat Sci & Engn, GPO Box 4, Canberra, ACT 0200, Australia.							Bennett K.P., 1992, OPT MET SOFTW, V1, P23; Cauwenberghs G, 2001, ADV NEUR IN, V13, P409; Csato L, 2001, ADV NEUR IN, V13, P444; Friedman J., 1998, ADDITIVE LOGISTIC RE; Gentile C, 2001, ADV NEUR IN, V13, P500; Graepel T, 2001, ADV NEUR IN, V13, P210; GUO Y, 2001, UNPUB J MACHINE LEAR; Herbster M, 2001, LECT NOTES ARTIF INT, V2111, P444; HUBER PJ, 1972, ANN MATH STAT, V43, P1041, DOI 10.1214/aoms/1177692459; KIMELDORF G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3; KIVINEN J, 2001, UNPUB LARGE MARGIN C; LI Y, 1999, ADV NEURAL INFORMATI, V12, P498; Mason L, 2000, ADV NEUR IN, P221; Scholkopf B., 2001, NEURAL COMPUTATION, V13; Vapnik V, 1997, ADV NEUR IN, V9, P281	16	25	27	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						785	792						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100098
C	Smith, MA; Cottrell, GW; Anderson, KL		Leen, TK; Dietterich, TG; Tresp, V		Smith, MA; Cottrell, GW; Anderson, KL			The early word catches the weights	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				AGE-OF-ACQUISITION; NAMING LATENCY; FREQUENCY	The strong correlation between the frequency of words and their naming latency has been well documented. However, as early as 1973, the Age of Acquisition (AoA) of a word was alleged to be the actual variable of interest, but these studies seem to have been ignored in most of the literature. Recently, there has been a resurgence of interest in AoA. While some studies have shown that frequency has no effect when AoA is controlled for, more recent studies have found independent contributions of frequency and AoA. Connectionist models have repeatedly shown strong effects of frequency, but little attention has been paid to whether they can also show AoA effects. Indeed, several researchers have explicitly claimed that they cannot show AoA effects. In this work, we explore these claims using a simple feed forward neural network. We find a significant contribution of AoA to naming latency, as well as conditions under which frequency provides an independent contribution.	Univ Calif San Diego, Dept Comp Sci, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Smith, MA (corresponding author), Univ Calif San Diego, Dept Comp Sci, La Jolla, CA 92093 USA.			Cottrell, Garrison/0000-0001-7538-1715				BROWN GDA, 1987, MEM COGNITION, V15, P208, DOI 10.3758/BF03197718; CARROLL JB, 1973, Q J EXP PSYCHOL, V25, P85, DOI 10.1080/14640747308400325; Ellis AW, 1998, J EXP PSYCHOL LEARN, V24, P515, DOI 10.1037/0278-7393.24.2.515; ELLIS AW, IN PRESS JEP LMC; Gerhand S, 1999, COGNITION, V73, pB27, DOI 10.1016/S0010-0277(99)00052-9; Gerhand S, 1998, J EXP PSYCHOL LEARN, V24, P267, DOI 10.1037/0278-7393.24.2.267; JARED D, 1990, J MEM LANG, V29, P687, DOI 10.1016/0749-596X(90)90044-Z; MORRISON CM, 1995, J EXP PSYCHOL LEARN, V21, P116, DOI 10.1037/0278-7393.21.1.116; MORRISON CM, 1992, MEM COGNITION, V20, P705, DOI 10.3758/BF03202720; OLDFIELD RC, 1965, Q J EXP PSYCHOL, V17, P273, DOI 10.1080/17470216508416445	10	25	26	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						52	58						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800008
C	Smola, AJ; Ovari, ZL; Williamson, RC		Leen, TK; Dietterich, TG; Tresp, V		Smola, AJ; Ovari, ZL; Williamson, RC			Regularization with dot-product kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					In this paper we give necessary and sufficient conditions under which kernels of dot product type k(x, y) = k(x - y) satisfy Mercer's condition and thus may be used in Support Vector Machines (SVM), Regularization Networks (RN) or Gaussian Processes (GP). In particular, we show that if the kernel is analytic (i.e. can be expanded in a Taylor series), all expansion coefficients have to be nonnegative. We give an explicit functional form for the feature map by calculating its eigenfunctions and eigenvalues.	Australian Natl Univ, Dept Engn, Canberra, ACT 2600, Australia	Australian National University	Smola, AJ (corresponding author), Australian Natl Univ, Dept Engn, GPO Box 4, Canberra, ACT 2600, Australia.							Burges CJC, 1999, ADVANCES IN KERNEL METHODS, P89; Gradshteyn IS., 1981, TABLE INTEGRALS SERI; Muller C., 1997, APPL MATH SCI, V129; Oliver N, 2000, ADV NEUR IN, P51; OVARI Z, 2000, THESIS AUSTR NATL U; Schoenberg I., 1942, DUKE MATH J, V9, P96, DOI DOI 10.1215/S0012-7094-42-00908-6; Smola AJ, 1998, NEURAL NETWORKS, V11, P637, DOI 10.1016/S0893-6080(98)00032-X; Wahba G., 1990, CBMS NSF REGIONAL C, V59; WILLIAMS C, 1998, LEARNING INFERENCE G	10	25	25	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						308	314						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800044
C	Attias, H; Schreiner, CE		Jordan, MI; Kearns, MJ; Solla, SA		Attias, H; Schreiner, CE			Coding of naturalistic stimuli by auditory midbrain neurons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					It is known that humans can make finer discriminations between familiar sounds (e.g. syllables) than between unfamiliar ones (e.g. different noise segments). Here we show that a corresponding enhancement is present in early auditory processing stages. Based on previous work which demonstrated that natural sounds had robust statistical properties that could be quantified, we hypothesize that the auditory system exploits those properties to construct efficient neural codes. To test this hypothesis, we measure the information rate carried by auditory spike trains on narrow-band stimuli whose amplitude modulation has naturalistic characteristics, and compare it to the information rate on stimuli with non-naturalistic modulation. We find that naturalistic inputs significantly enhance the rate of transmitted information, indicating that auditiory neural responses are matched to characteristics of natural auditory scenes.	Univ Calif San Francisco, Sloan Ctr Theoret Neurobiol, San Francisco, CA 94143 USA	University of California System; University of California San Francisco	Attias, H (corresponding author), Univ Calif San Francisco, Sloan Ctr Theoret Neurobiol, San Francisco, CA 94143 USA.			Schreiner, Christoph/0000-0002-4571-4328					0	25	26	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						103	109						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700015
C	O'Reilly, R; Norman, KA; McClelland, JL		Jordan, MI; Kearns, MJ; Solla, SA		O'Reilly, R; Norman, KA; McClelland, JL			A hippocampal model of recognition memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					A rich body of data exists showing that recollection of specific information makes an important contribution to recognition memory, which is distinct from the contribution of familiarity, and is not adequately captured by existing unitary memory models. Furthermore, neuropsychological evidence indicates that recollection is subserved by the hippocampus. We present a model, based largely on known features of hippocampal anatomy and physiology, that accounts for the following key characteristics of recollection: 1) false recollection is rare (i.e., participants rarely claim to recollect having studied nonstudied items), and 2) increasing interference leads to less recollection but apparently does not compromise the quality of recollection (i.e., the extent to which recollected information veridically reflects events that occurred at study).	Univ Colorado, Dept Psychol, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	O'Reilly, R (corresponding author), Univ Colorado, Dept Psychol, Campus Box 345, Boulder, CO 80309 USA.								0	25	25	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						73	79						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700011
C	Smola, AJ; Scholkopf, B		Jordan, MI; Kearns, MJ; Solla, SA		Smola, AJ; Scholkopf, B			From regularization operators to Support Vector Kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We derive the correspondence between regularization operators used in Regularization Networks and Hilbert Schmidt Kernels appearing in Support Vector Machines. More specifically, we prove that the Green's Functions associated with regularization operators are suitable Support Vector Kernels with equivalent regularization properties. As a by-product we show that a large number of Radial Basis Functions namely conditionally positive definite functions may be used as Support Vector kernels.	GMD FIRST, D-12489 Berlin, Germany	Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss Birlinghoven	Smola, AJ (corresponding author), GMD FIRST, Rudower Chaussee 5, D-12489 Berlin, Germany.		Schölkopf, Bernhard/A-7570-2013	Schölkopf, Bernhard/0000-0002-8177-0925					0	25	26	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						343	349						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700049
C	Blair, HT		Touretzky, DS; Mozer, MC; Hasselmo, ME		Blair, HT			Simulation of a thalamocortical circuit for computing directional heading in the rat	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						YALE UNIV,DEPT PSYCHOL,NEW HAVEN,CT 06520	Yale University									0	25	26	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						152	158						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00022
C	HINTON, GE; WILLIAMS, CKI; REVOW, MD		MOODY, JE; HANSON, SJ; LIPPMANN, RP		HINTON, GE; WILLIAMS, CKI; REVOW, MD			ADAPTIVE ELASTIC MODELS FOR HAND-PRINTED CHARACTER-RECOGNITION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	25	26	1	1	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						512	519						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00063
C	Hu, XY; Rudin, C; Seltzer, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hu, Xiyang; Rudin, Cynthia; Seltzer, Margo			Optimal Sparse Decision Trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Decision tree algorithms have been among the most popular algorithms for interpretable (transparent) machine learning since the early 1980's. The problem that has plagued decision tree algorithms since their inception is their lack of optimality, or lack of guarantees of closeness to optimality: decision tree algorithms are often greedy or myopic, and sometimes produce unquestionably suboptimal models. Hardness of decision tree optimization is both a theoretical and practical obstacle, and even careful mathematical programming approaches have not been able to solve these problems efficiently. This work introduces the first practical algorithm for optimal decision trees for binary variables. The algorithm is a co-design of analytical bounds that reduce the search space and modern systems techniques, including data structures and a custom bit-vector library. Our experiments highlight advantages in scalability, speed, and proof of optimality.	[Hu, Xiyang] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Rudin, Cynthia] Duke Univ, Durham, NC 27706 USA; [Seltzer, Margo] Univ British Columbia, Vancouver, BC, Canada	Carnegie Mellon University; Duke University; University of British Columbia	Seltzer, M (corresponding author), Univ British Columbia, Vancouver, BC, Canada.	xiyanghu@cmu.edu; cynthia@cs.duke.edu; mseltzer@cs.ubc.ca	Hu, Xiyang/AAW-9058-2020	Hu, Xiyang/0000-0001-7269-3828				Angelino E., 2017, P ACM SIGKDD INT C K; Angelino E, 2018, J MACH LEARN RES, V18; Bennett K., 1992, P 4 MIDW ART INT COG; Bertsimas D, 2017, MACH LEARN, V106, P1039, DOI 10.1007/s10994-017-5633-9; Blanquero R., 2018, OPTIMAL RANDOMIZED C; Blue, 1996, TECHNICAL REPORT; FICO Google Imperial College London MIT University of Oxford UC Irvine and UC Berkeley., 2018, EXPL MACH LEARN CHAL; Flores A. W., 2016, FEDERAL PROBATION, V80; Jeff Larson L. K., 2016, PROPUBLICA; Klivans AR, 2006, J MACH LEARN RES, V7, P587; Larus-Stone N., 2018, P C SYST MACH LEARN; Letham B, 2015, ANN APPL STAT, V9, P1350, DOI 10.1214/15-AOAS848; McGough M., 2018, SACRAMENTO BEE; Menickelly M., 2018, ARXIV161203225; Narodytska N, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1362; Nijssen S, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P530; Olshen R., 1984, CLASSIFICATION REGRE; Quinlan J., 2014, C4 5 PROGRAMS MACHIN, DOI DOI 10.1007/BF00993309; Verwer S, 2019, AAAI CONF ARTIF INTE, P1625; Yang H., 2017, INT C MACH LEARN ICM; Zech JR, 2018, PLOS MED, V15, DOI 10.1371/journal.pmed.1002683	21	24	24	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307030
C	Huang, L; Wang, WM; Xia, YX; Chen, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Huang, Lun; Wang, Wenmin; Xia, Yaxian; Chen, Jie			Adaptively Aligned Image Captioning via Adaptive Attention Time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent neural models for image captioning usually employ an encoder-decoder framework with an attention mechanism. However, the attention mechanism in such a framework aligns one single (attended) image feature vector to one caption word, assuming one-to-one mapping from source image regions and target caption words, which is never possible. In this paper, we propose a novel attention model, namely Adaptive Attention Time (AAT), to align the source and the target adaptively for image captioning. AAT allows the framework to learn how many attention steps to take to output a caption word at each decoding step. With AAT, an image region can be mapped to an arbitrary number of caption words while a caption word can also attend to an arbitrary number of image regions. AAT is deterministic and differentiable, and doesn't introduce any noise to the parameter gradients. In this paper, we empirically show that AAT improves over state-of-the-art methods on the task of image captioning.	[Huang, Lun; Wang, Wenmin; Xia, Yaxian; Chen, Jie] Peking Univ, Sch Elect & Comp Engn, Beijing, Peoples R China; [Chen, Jie] Peng Cheng Lab, Shenzhen, Peoples R China; [Wang, Wenmin] Macau Univ Sci & Technol, Macau, Peoples R China	Peking University; Peng Cheng Laboratory; Macau University of Science & Technology	Wang, WM (corresponding author), Peking Univ, Sch Elect & Comp Engn, Beijing, Peoples R China.; Wang, WM (corresponding author), Macau Univ Sci & Technol, Macau, Peoples R China.	huanglun@pku.edu.cn; wangwm@ece.pku.edu.cn; xiayaxian@pku.edu.cn; chenj@pcl.ac.cn	cai, jie/HHS-0606-2022; Wang, Wenmin/W-3511-2019	Wang, Wenmin/0000-0003-2664-4413	National Natural Science Foundation of China (NSFC) [61872256, 61972217]; Science and Technology Development Fund of Macau (FDCT) [0016/2019/A1]; National Engineering Laboratory for Video Technology - Shenzhen Division	National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Science and Technology Development Fund of Macau (FDCT); National Engineering Laboratory for Video Technology - Shenzhen Division	This project was supported by National Engineering Laboratory for Video Technology - Shenzhen Division, National Natural Science Foundation of China (NSFC, 61872256, 61972217), and The Science and Technology Development Fund of Macau (FDCT, 0016/2019/A1). We would also like to thank the anonymous reviewers for their insightful comments.	Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; Banerjee Satanjeev, 2005, P ACL WORKSH INTR EX, P65; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Dehghani Mostafa, 2019, ICLR; Du Jiajun, 2018, ARXIV181203283; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754; Graves Alex, 2016, ARXIV160308983; Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473; Huang L, 2019, INT CONF ACOUST SPEE, P4110, DOI 10.1109/ICASSP.2019.8682392; Jiang WH, 2018, LECT NOTES COMPUT SC, V11206, P510, DOI 10.1007/978-3-030-01216-8_31; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kingma D.P, P 3 INT C LEARNING R; Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162; Lin Chin-Yew, 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.2307/3105454; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345; Mitchell Margaret, 2012, EACL; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131; SOCHER R, 2010, PROC CVPR IEEE, P966, DOI DOI 10.1109/CVPR.2010.5540112; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; VEDANTAM R, 2015, PROC CVPR IEEE, P4566, DOI DOI 10.1109/CVPR.2015.7299087; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang Xu, 2019, COMPUTER VISION PATT; Yang Z, 2016, INT CONF ACOUST SPEE, P3236, DOI 10.1109/ICASSP.2016.7472275; Yao BZ, 2010, P IEEE, V98, P1485, DOI 10.1109/JPROC.2010.2050411; Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42; Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524	35	24	24	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900052
C	Ignatiev, A; Narodytska, N; Marques-Silva, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ignatiev, Alexey; Narodytska, Nina; Marques-Silva, Joao			On Relating Explanations and Adversarial Examples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SUBSETS	The importance of explanations (XP's) of machine learning (ML) model predictions and of adversarial examples (AE's) cannot be overstated, with both arguably being essential for the practical success of ML in different settings. There has been recent work on understanding and assessing the relationship between XP's and AE's. However, such work has been mostly experimental and a sound theoretical relationship has been elusive. This paper demonstrates that explanations and adversarial examples are related by a generalized form of hitting set duality, which extends earlier work on hitting set duality observed in model-based diagnosis and knowledge compilation. Furthermore, the paper proposes algorithms, which enable computing adversarial examples from explanations and vice-versa.	[Ignatiev, Alexey] Monash Univ, Clayton, Vic, Australia; [Narodytska, Nina] VMWare Res, Palo Alto, CA USA; [Marques-Silva, Joao] ANITI, Toulouse, France	Monash University	Ignatiev, A (corresponding author), Monash Univ, Clayton, Vic, Australia.	alexey.ignatiev@monash.edu; nnarodytska@vmware.com; joao.marques-silva@univ-toulouse.fr		Ignatiev, Alexey/0000-0002-4535-2902				Alvarez-Melis David, 2018, ARXIV180608049; Angelino E, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P35, DOI 10.1145/3097983.3098047; [Anonymous], [No title captured]; [Anonymous], 2018, IBM ILOG CPLEX OPTIM; Bailey J, 2005, LECT NOTES COMPUT SC, V3350, P174; Bastani O., 2016, P 30 INT C NEUR INF, P2613; Birnbaum E, 2003, J EXP THEOR ARTIF IN, V15, P25, DOI 10.1080/0952813021000026795; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chakraborty Anirban, 2018, ARXIV PREPRINT ARXIV; Chalasani P., 2018, ABS181006583 CORR; Chandrasekaran K, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P614; Cheng C, 2017, ABS171003107 CORR; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; DARPA, 2016, DARPA EXPL ART INT X; Fischetti M, 2018, CONSTRAINTS, V23, P296, DOI 10.1007/s10601-018-9285-6; Frosst N, 2017, CEXAIIA; Furukawa Yasutaka, 2019, ARXIV PREPRINT ARXIV; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Goodman B, 2017, AI MAG, V38, P50, DOI 10.1609/aimag.v38i3.2741; Guidotti R., 2018, ABS180510820 CORR, V1805, P10820; Guidotti R, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3236009; Ignatiev A., 2019, J SATISF BOOLEAN MOD, V11, P53, DOI DOI 10.3233/SAT190116; Ignatiev A, 2019, AAAI CONF ARTIF INTE, P1511; Ignatiev A, 2018, LECT NOTES ARTIF INT, V10900, P627, DOI 10.1007/978-3-319-94205-6_41; Ignatiev A, 2018, LECT NOTES COMPUT SC, V10929, P428, DOI 10.1007/978-3-319-94144-8_26; Katz G, 2017, LECT NOTES COMPUT SC, V10426, P97, DOI 10.1007/978-3-319-63387-9_5; Khalil E. B., 2018, ABS181003538 CORR; Kurakin A, 2018, ICLR, P99, DOI DOI 10.1201/9781351251389-8; Lakkaraju H, 2019, AIES '19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, P131, DOI 10.1145/3306618.3314229; Leofante F., 2018, ABS180509938 CORR; Li CM, 2009, FRONT ARTIF INTEL AP, V185, P613, DOI 10.3233/978-1-58603-929-5-613; Liffiton MH, 2016, CONSTRAINTS, V21, P223, DOI 10.1007/s10601-015-9183-0; Liu NH, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1803, DOI 10.1145/3219819.3220027; Lundberg SM, 2017, ADV NEUR IN, V30; MARQUIS P, 2000, HDB DEF REAS UNC, P41, DOI DOI 10.1007/978-94-017-1737-3; Moosavi-Dezfooli S., 2018, ICLR; Moosavi-Dezfooli Seyed-Mohsen, 2016, ABS161008401 CORR; Narodytska N, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1362; Narodytska N, 2018, AAAI CONF ARTIF INTE, P6615; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Previti A, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1980; REITER R, 1987, ARTIF INTELL, V32, P57, DOI 10.1016/0004-3702(87)90062-2; Ribeiro MT, 2018, AAAI CONF ARTIF INTE, P1527; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Ros AS, 2018, AAAI CONF ARTIF INTE, P1660; Rudin C., 2018, P 32 C NEUR INF PROC; Russell JA, 2010, SPORTS REHABILITATION AND INJURY PREVENTION, P3; Rymon R., 1994, Annals of Mathematics and Artificial Intelligence, V11, P351, DOI 10.1007/BF01530750; Shih A, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5103; Sundararajan M, 2017, PR MACH LEARN RES, V70; Tao GH, 2018, ADV NEUR IN, V31; Tomsett R, 2018, 2018 21ST INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P838; Wang T., 2019, ARXIV E PRINTS; Xu K., 2018, ABS180801664 CORR; Xu K., 2019, ABS190402057 CORR	58	24	25	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907053
C	Li, XT; Liu, SF; De Mello, S; Wang, XL; Kautz, J; Yang, MH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Xueting; Liu, Sifei; De Mello, Shalini; Wang, Xiaolong; Kautz, Jan; Yang, Ming-Hsuan			Joint-task Self-supervised Learning for Temporal Correspondence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper proposes to learn reliable dense correspondence from videos in a self-supervised manner. Our learning process integrates two highly related tasks: tracking large image regions and establishing fine-grained pixel-level associations between consecutive video frames. We exploit the synergy between both tasks through a shared inter-frame affinity matrix, which simultaneously models transitions between video frames at both the region- and pixel-levels. While region-level localization helps reduce ambiguities in fine-grained matching by narrowing down search regions; fine-grained matching provides bottom-up features to facilitate region-level localization. Our method outperforms the state-of-the-art self-supervised methods on a variety of visual correspondence tasks, including video-object and part-segmentation propagation, keypoint tracking, and object tracking. Our self-supervised method even surpasses the fully-supervised affinity feature representation obtained from a ResNet-18 pre-trained on the ImageNet. The project website can be found at https://sites.google.com/view/uvc2019/.	[Li, Xueting; Yang, Ming-Hsuan] Univ Calif, Merced, CA 95343 USA; [Liu, Sifei; De Mello, Shalini; Kautz, Jan] NVIDIA, Santa Clara, CA USA; [Wang, Xiaolong] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	University of California System; University of California Merced; Nvidia Corporation; Carnegie Mellon University	Li, XT (corresponding author), Univ Calif, Merced, CA 95343 USA.		Liu, Sifei/AGE-1968-2022; Yang, Ming-Hsuan/T-9533-2019	Liu, Sifei/0000-0002-6011-3686; Yang, Ming-Hsuan/0000-0003-4848-2304				Andriluka M., 2008, CVPR; Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56; Brox T., 2010, IEEE T PATTERN ANAL, V33, P500; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565; Caron M, 2018, ECCV; Choi J., 2017, CVPR; COMANICIU D, 2000, CVPR; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Fouhey D. F., 2018, CVPR; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Hong S, 2018, I C VIRTUAL REALITY, P28, DOI 10.1109/ICVRV.2018.00013; Hung Wei-Chih, 2019, CVPR; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Ji SL, 2013, 2013 INTERNATIONAL CONFERENCE ON MATERIALS FOR RENEWABLE ENERGY AND ENVIRONMENT (ICMREE), VOLS 1-3, P8, DOI 10.1109/ICMREE.2013.6893603; Kalal Z., 2011, TPAMI; Kingma D.P, P 3 INT C LEARNING R; Kong Shu, 2019, ARXIV190401693; Lai Zihang, 2019, BMVC; Lee Junghyup, 2019, CVPR; Li Q., 2017, ARXIV170903612; Li Y., 2014, LECT NOTES COMPUTER, V8926; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Liu G., 2015, NANOTECHNOLOGY NANOM, V2, P1, DOI DOI 10.1186/S12943-015-0455-5; Liu PF, 2015, INT CONF SERVICE SCI, P9, DOI 10.1109/ICSS.2015.24; Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Okutomi M., 1993, IEEE TPAMI; Pont-Tuset J., 2017, ARXIV170400675; Qiang J. X. M., 2017, ARXIV170404057; Rocco I., 2018, CVPR; Rubinstein M., 2012, BMVC; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158; Thomee B., 2015, ARXIV150301817; Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531; Vondrick C., 2018, ECCV, P391; Wang N., 2019, CVPR; Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142; Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510; Wang XG, 2019, PROC CVPR IEEE, P8868, DOI [10.1109/CVPR.2019.00908, 10.1109/CVPR.2019.00267]; Wang XL, 2017, STUD COMPUT INTELL, V705, P7, DOI 10.1007/978-3-319-53153-3_2; Wang Y, 2013, EURASIP J AUDIO SPEE, DOI 10.1186/1687-4722-2013-2; Xing J., 2017, ARXIV170404057; Xu N., 2018, ARXIV180903327; Yang C., 2005, CVPR; Yang LJ, 2018, PROC CVPR IEEE, P6499, DOI 10.1109/CVPR.2018.00680; Zhang W, 2018, IEEE CONF COMPUT; Zhou K. G. L. L. Qixian, 2018, P ACM INT C MULT ACM; Zhou Qixian, 2018, ACM MM	58	24	24	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300029
C	Liu, L; Zhou, TY; Long, GD; Jiang, J; Zhang, CQ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Lu; Zhou, Tianyi; Long, Guodong; Jiang, Jing; Zhang, Chengqi			Learning to Propagate for Graph Meta-Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Meta-learning extracts the common knowledge from learning different tasks and uses it for unseen tasks. It can significantly improve tasks that suffer from insufficient training data, e.g., few-shot learning. In most meta-learning methods, tasks are implicitly related by sharing parameters or optimizer. In this paper, we show that a meta-learner that explicitly relates tasks on a graph describing the relations of their output dimensions (e.g., classes) can significantly improve few-shot learning. The graph's structure is usually free or cheap to obtain but has rarely been explored in previous works. We develop a novel meta-learner of this type for prototype based classification, in which a prototype is generated for each class, such that the nearest neighbor search among the prototypes produces an accurate classification. The meta-learner, called "Gated Propagation Network (GPN)", learns to propagate messages between prototypes of different classes on the graph, so that learning the prototype of each class benefits from the data of other related classes. In GPN, an attention mechanism aggregates messages from neighboring classes of each class, with a gate choosing between the aggregated message and the message from the class itself. We train GPN on a sequence of tasks from many-shot to few-shot generated by subgraph sampling. During training, it is able to reuse and update previously achieved prototypes from the memory in a life-long learning cycle. In experiments, under different training-test discrepancy and test task generation settings, GPN outperforms recent meta-learning methods on two benchmark datasets. The code of GPN and dataset generation is available at https://github.com/liulu112601/Gated-Propagation-Net.	[Liu, Lu; Long, Guodong; Jiang, Jing; Zhang, Chengqi] Univ Technol Sydney, Ctr Artificial Intelligence, Sydney, NSW, Australia; [Zhou, Tianyi] Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA	University of Technology Sydney; University of Washington; University of Washington Seattle	Liu, L (corresponding author), Univ Technol Sydney, Ctr Artificial Intelligence, Sydney, NSW, Australia.	lu.liu-10@student.uts.edu.au; tianyizh@uw.edu; guodong.long@uts.edu.au; jing.jiang@uts.edu.au; chengqi.zhang@uts.edu.au	zhang, chi/GRX-3610-2022; Jiang, Jing/AAV-3500-2021	Zhou, Tianyi/0000-0001-5348-0632; Zhang, Chengqi/0000-0001-5715-7154	Australian Government through the Australian Research Council (ARC) [LP160100630, LP150100671]; Australia Government Department of Health; Australia Research Alliance for Children and Youth (ARACY); Global Business College Australia (GBCA); NVIDIA Corporation; Google Cloud	Australian Government through the Australian Research Council (ARC)(Australian Research Council); Australia Government Department of Health; Australia Research Alliance for Children and Youth (ARACY); Global Business College Australia (GBCA); NVIDIA Corporation; Google Cloud(Google Incorporated)	This research was funded by the Australian Government through the Australian Research Council (ARC) under grants 1) LP160100630 partnership with Australia Government Department of Health and 2) LP150100671 partnership with Australia Research Alliance for Children and Youth (ARACY) and Global Business College Australia (GBCA). We also acknowledge the support of NVIDIA Corporation and Google Cloud with the donation of GPUs and computation credits.	Allen KR, 2019, PR MACH LEARN RES, V97; Bendor D, 2012, NAT NEUROSCI, V15, P1439, DOI 10.1038/nn.3203; Chen Wei-Yu, 2019, INT C LEARN REPR, P12; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Finn C, 2017, PR MACH LEARN RES, V70; Garcia V., 2018, ICLR; Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Henaff M, 2015, ARXIV150605163; Kaiser Lukasz, 2017, INT C LEARN REPR ICL; Kingma D.P, P 3 INT C LEARNING R; Kruskal J. B., 1956, P AM MATH SOC, V7, P48, DOI [DOI 10.1090/S0002-9939-1956-0078686-7, 10.2307/2033241]; Li Ke, 2017, ICLR; Liu L, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P876; Liu YH, 2019, INT J PSYCHIAT CLIN, V23, P164, DOI 10.1080/13651501.2019.1569238; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Oreshkin B. N., 2018, C NEUR INF PROC SYST; Pearl J., 1982, AAAI C ART INT AAAI; Pearl J., 1982, AAAI 82 P 2 AAAI C A, P133; Ren M., 2018, ICLR; Santoro A, 2016, PR MACH LEARN RES, V48; Snell J., 2017, ADV NEURAL INFORM PR, P4077; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Velickovic P., 2018, P INT C LEARN REPR; Vinyals Oriol, 2016, ARXIV160604080, P3630; Weiss Y, 2000, NEURAL COMPUT, V12, P1, DOI 10.1162/089976600300015880; Zhou DY, 2004, ADV NEUR IN, V16, P321; Zhu Xiaojin, 2002, TECHNICAL REPORT, P1	30	24	24	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301008
C	Mania, H; Tu, S; Recht, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mania, Horia; Tu, Stephen; Recht, Benjamin			Certainty Equivalence is Efficient for Linear Quadratic Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the performance of the certainty equivalent controller on Linear Quadratic (LQ) control problems with unknown transition dynamics. We show that for both the fully and partially observed settings, the sub-optimality gap between the cost incurred by playing the certainty equivalent controller on the true system and the cost incurred by using the optimal LQ controller enjoys a fast statistical rate, scaling as the square of the parameter error. To the best of our knowledge, our result is the first sub-optimality guarantee in the partially observed Linear Quadratic Gaussian (LQG) setting. Furthermore, in the fully observed Linear Quadratic Regulator (LQR), our result improves upon recent work by Dean et al. [11], who present an algorithm achieving a sub-optimality gap linear in the parameter error. A key part of our analysis relies on perturbation bounds for discrete Riccati equations. We provide two new perturbation bounds, one that expands on an existing result from Konstantinov et al. [25], and another based on a new elementary proof strategy.	[Mania, Horia; Tu, Stephen; Recht, Benjamin] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Mania, H (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	hmania@berkeley.edu; stephentu@berkeley.edu; brecht@berkeley.edu			NSF CISE Expeditions Award [CCF-1730628]; DHS Award [HSHQDC-16-3-00083]; Google PhD fellowship; ONR [N00014-17-1-2191, N00014-17-1-2401, N00014-18-1-2833]; DARPA [FA8750-18-C-0101, W911NF-16-1-0552]; Siemens Futuremakers Fellowship; Amazon AWS AI Research Award	NSF CISE Expeditions Award; DHS Award; Google PhD fellowship(Google Incorporated); ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Siemens Futuremakers Fellowship; Amazon AWS AI Research Award	We thank the anonymous reviewers for their valuable feedback. We also thank Elad Hazan and Martin Wainwright, who both independently asked whether or not it was possible to show a fast rate for LQR. As part of the RISE lab, TIM is generally supported in part by NSF CISE Expeditions Award CCF-1730628, DHS Award HSHQDC-16-3-00083, and gifts from Alibaba, Amazon Web Services, Ant Financial, CapitalOne, Ericsson, GE, Google, Huawei, Intel, IBM, Microsoft, Scotiabank, Splunk and VMware. ST is supported by a Google PhD fellowship. BR is generously supported in part by ONR awards N00014-17-1-2191, N00014-17-1-2401, and N00014-18-1-2833, the DARPA Assured Autonomy (FA8750-18-C-0101) and Lagrange (W911NF-16-1-0552) programs, a Siemens Futuremakers Fellowship, and an Amazon AWS AI Research Award.	Abbasi-Yadkori Y., 2019, 22 INT C ART INT STA, P3108; Abbasi-Yadkori Y., 2011, C LEARN THEOR; Abeille M, 2018, PR MACH LEARN RES, V80; Anderson B. D., 2007, OPTIMAL CONTROL LINE; ASTROM KJ, 1973, AUTOMATICA, V9, P185, DOI 10.1016/0005-1098(73)90073-3; Astrom KJ, 1995, ADAPTIVE CONTROL; Boczar R., 2018, 57 IEEE C DEC CONTR; Cohen A., 2019, ARXIV190206223; Cohen A, 2018, PR MACH LEARN RES, V80; Dean S., 2018, NEURAL INFORM PROCES; Dhruv Malik Ashwin, 2019, AISTATS; DOYLE JC, 1978, IEEE T AUTOMAT CONTR, V23, P756, DOI 10.1109/TAC.1978.1101812; Faradonbeh M. K. S., 2017, ARXIV171107230; Faradonbeh MKS, 2018, AUTOMATICA, V96, P342, DOI 10.1016/j.automatica.2018.07.008; Faradonbeh Mohamad Kazem Shirani, 2018, ARXIV181104258; Fiechter C.-N., 1997, C LEARN THEOR; Hardt M, 2018, J MACH LEARN RES, V19; Hazan E., 2018, NEURAL INFORM PROCES; Hazan E., 2017, NEURAL INFORM PROCES; Ibrahimi M., 2012, NEURAL INFORM PROCES; Iyengar GN, 2005, MATH OPER RES, V30, P257, DOI 10.1287/moor.1040.0129; Kailath T, 2000, PR H INF SY, pXIX; Konstantinov M, 2003, STUDIES COMPUTATIONA, V9; KONSTANTINOV MM, 1993, KYBERNETIKA, V29, P18; Mania H, 2019, ADV NEUR IN, V32; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nilim A, 2005, OPER RES, V53, P780, DOI 10.1287/opre.1050.0216; Ouyang Y., 2017, ALLERTON; Oymak S., 2018, ARXIV180605722; Sarkar T, 2019, PR MACH LEARN RES, V97; Simchowitz M., 2019, C LEARN THEOR; Simchowitz M., 2018, C LEARNING THEORY, P439; Sun JG, 1998, SIAM J MATRIX ANAL A, V19, P39, DOI 10.1137/S0895479895291303; Sun JG, 1998, LINEAR ALGEBRA APPL, V276, P595; Tsiamis A, 2019, IEEE DECIS CONTR P, P3648, DOI 10.1109/CDC40024.2019.9029499; Tu S., 2017, ARXIV170704791; Xu H, 2012, MATH OPER RES, V37, P288, DOI 10.1287/moor.1120.0540; Zhou K., 1995, ROBUST OPTIMAL CONTR	42	24	24	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901075
C	Sun, FY; Qu, M; Hoffmann, J; Huang, CW; Tang, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sun, Fan-Yun; Qu, Meng; Hoffmann, Jordan; Huang, Chin-Wei; Tang, Jian			vGraph: A Generative Model for Joint Community Detection and Node Representation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NETWORKS	This paper focuses on two fundamental tasks of graph analysis: community detection and node representation learning, which capture the global and local structures of graphs, respectively. In the current literature, these two tasks are usually independently studied while they are actually highly correlated. We propose a probabilistic generative model called vGraph to learn community membership and node representation collaboratively. Specifically, we assume that each node can be represented as a mixture of communities, and each community is defined as a multinomial distribution over nodes. Both the mixing coefficients and the community distribution are parameterized by the low-dimensional representations of the nodes and communities. We designed an effective variational inference algorithm which regularizes the community membership of neighboring nodes to be similar in the latent space. Experimental results on multiple real-world graphs show that vGraph is very effective in both community detection and node representation learning, outperforming many competitive baselines in both tasks. We show that the framework of vGraph is quite flexible and can be easily extended to detect hierarchical communities.	[Sun, Fan-Yun] Natl Taiwan Univ, Taipei, Taiwan; [Sun, Fan-Yun; Qu, Meng; Hoffmann, Jordan; Huang, Chin-Wei; Tang, Jian] Mila Quebec Inst Learning Algorithms, Montreal, PQ, Canada; [Hoffmann, Jordan] Harvard Univ, Cambridge, MA 02138 USA; [Huang, Chin-Wei] Element AI, Montreal, PQ, Canada; [Tang, Jian] HEC Montreal, Montreal, PQ, Canada; [Tang, Jian] CIFAR AI Res Chair, Montreal, PQ, Canada	National Taiwan University; Harvard University; Universite de Montreal; HEC Montreal	Sun, FY (corresponding author), Natl Taiwan Univ, Taipei, Taiwan.; Sun, FY (corresponding author), Mila Quebec Inst Learning Algorithms, Montreal, PQ, Canada.	b04902045@ntu.edu.tw			Natural Sciences and Engineering Research Council of Canada; Canada CIFAR AI Chair Program	Natural Sciences and Engineering Research Council of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)CGIAR); Canada CIFAR AI Chair Program	This project is supported by the Natural Sciences and Engineering Research Council of Canada, as well as the Canada CIFAR AI Chair Program.	Ahn YY, 2010, NATURE, V466, P761, DOI 10.1038/nature09182; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Cao QL, 2015, LISS 2013, P1135, DOI 10.1007/978-3-642-40660-7_170; Cavallari S, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P377, DOI 10.1145/3132847.3132925; Derenyi I, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.160202; Dong XW, 2012, IEEE T SIGNAL PROCES, V60, P5820, DOI 10.1109/TSP.2012.2212886; Epasto Alessandro, 2019, IS SINGLE EMBEDDING; Gao S., 2011, P 20 ACM INT C INFOR, P1169, DOI [10.1145/2063576.2063744, DOI 10.1145/2063576.2063744]; GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552; Gopalan PK, 2013, P NATL ACAD SCI USA, V110, P14534, DOI 10.1073/pnas.1221839110; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289; Jang E., 2016, ARXIV; Jia YT, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P784, DOI 10.1145/3308558.3313564; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krogan NJ, 2006, NATURE, V440, P637, DOI 10.1038/nature04670; Kuang J, 2012, INT CONF CLOUD COMPU, P1013, DOI 10.1109/CCIS.2012.6664534; Li Y, 2018, AAAI CONF ARTIF INTE, P3579; Liang B, 2015, INT CONF SOFTW ENG, P894, DOI 10.1109/ICSESS.2015.7339198; Maddison Chris J, 2016, ARXIV161100712; McAuley J, 2014, ACM T KNOWL DISCOV D, V8, P73, DOI 10.1145/2556612; Morin F., 2005, PROC INT WORKSHOP AR, P246; Newman MEJ, 2004, PHYS REV E, V70, DOI [10.1103/PhysRevE.70.056131, 10.1103/PhysRevE.69.026113]; Pei YL, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2083; Perozzi B, 2014, KDD, V20, P701, DOI DOI 10.1145/2623330.2623732; Rozemberczki B, 2018, GEMSEC GRAPH EMBEDDI; Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093; Tian F, 2014, AAAI CONF ARTIF INTE, P1293; Tsitsulin A, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P539, DOI 10.1145/3178876.3186120; Tu Cunchao, 2018, IEEE T KNOWLEDGE DAT; Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225, DOI 10.1145/2939672.2939753; Wang F, 2011, DATA MIN KNOWL DISC, V22, P493, DOI 10.1007/s10618-010-0181-y; Wang X., 2017, 31 AAAI C ART INT; Wang YL, 2016, PROC CVPR IEEE, P6005, DOI 10.1109/CVPR.2016.646; White S, 2005, SIAM PROC S, P274; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Yang J., 2013, P 6 ACM INT C WEB SE, P587, DOI DOI 10.1145/2433396.2433471; Yang J, 2013, IEEE DATA MINING, P1151, DOI 10.1109/ICDM.2013.167; Zhang HY, 2015, AAAI CONF ARTIF INTE, P396	39	24	25	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300047
C	Vogels, T; Karinireddy, SP; Jaggi, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Vogels, Thijs; Karinireddy, Sai Praneeth; Jaggi, Martin			PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study lossy gradient compression methods to alleviate the communication bottleneck in data-parallel distributed optimization. Despite the significant attention received, current compression schemes either do not scale well, or fail to achieve the target test accuracy. We propose a new low-rank gradient compressor based on power iteration that can i) compress gradients rapidly, ii) efficiently aggregate the compressed gradients using all-reduce, and iii) achieve test performance on par with SGD. The proposed algorithm is the only method evaluated that achieves consistent wall-clock speedups when benchmarked against regular SGD using highly optimized off-the-shelf tools for distributed communication. We demonstrate reduced training times for convolutional networks as well as LSTMs on common datasets. Our code is available at https://github.com/epfml/powersgd.	[Vogels, Thijs; Karinireddy, Sai Praneeth; Jaggi, Martin] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Vogels, T (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	thijs.vogels@epf1.ch; sai.karimrieddy@epf1.ch; martin.jaggi@epfl.ch			SNSF [200021_175796]; Google Focused Research Award	SNSF(Swiss National Science Foundation (SNSF)); Google Focused Research Award(Google Incorporated)	We thank Alp Yurtsever and Tao Lin for valuable discussions and the reviewers for their feedback. This project was supported by SNSF grant 200021_175796, as well as a Google Focused Research Award.	Alistarh D, 2017, ADV NEUR IN, V30; Arbenz P., 2016, LECT NOTES SOLVING L, V2; Arora S, 2018, PR MACH LEARN RES, V80; Awan A. A., 2018, EUR MPI US GROUP M E; Baevski A., 2019, INT C LEARN REPR ICL; Bernstein J., 2019, INT C LEARN REPR ICL; Bernstein J, 2018, PR MACH LEARN RES, V80; Caldas Sebastian, 2018, ABS181207210 ARXIV; Carlson D., 2015, INT C ART INT STAT A; Collins E., 2018, ABS181003372 ARXIV; Dean J., 2012, ADV NEURAL INFORM PR, P1223, DOI DOI 10.5555/2999134.2999271; Gunasekar S, 2018, PR MACH LEARN RES, V80; Karimireddy SP, 2019, PR MACH LEARN RES, V97; Konecn J., 2016, ARXIV161005492; Kyrola A., 2017, ABS170602677 ARXIV; landola F. N., 2015, ABS151100175 ARXIV; Li Y., 2018, C LEARN THEOR COLT; Lin YL, 2018, INT CONF SYST SCI EN; Martin C. H., 2018, ABS181001075 ARXIV; Mazumder R, 2010, J MACH LEARN RES, V11, P2287; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; Panda D. K. D., 2019, S PRINC PRACT PAR PR; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Seide F, 2014, INTERSPEECH, P1058; Shallue C. J., 2018, ABS181103600 ARXIV; Stewart G., 1975, TOPICS NUMERICAL ANA, VII, P169; STEWART GW, 1976, NUMER MATH, V25, P123, DOI 10.1007/BF01462265; Stich S. U., 2019, ABS190905350 ARXIV; Stich SU, 2018, ADV NEUR IN, V31; Wang HY, 2018, ADV NEUR IN, V31; Wangni J., 2018, ADV NEURAL INFORM PR, P1299; Wen W., 2017, ADV NEURAL INFORM PR, P1, DOI DOI 10.1109/ICC.2017.7997306; Yoshida Y, 2017, ARXIV PREPRINT ARXIV; Yu M., 2018, ADV NEURAL INFORM PR; Yurtsever A., 2017, INT C ART INT STAT A; Zhao J, 2019, SIGNSGD MAJORITY VOT	38	24	25	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905086
C	Chow, Y; Nachum, O; Duenez-Guzman, E; Ghavamzadeh, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chow, Yinlam; Nachum, Ofir; Duenez-Guzman, Edgar; Ghavamzadeh, Mohammad			A Lyapunov-based Approach to Safe Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In many real-world reinforcement learning (RL) problems, besides optimizing the main objective function, an agent must concurrently avoid violating a number of constraints. In particular, besides optimizing performance, it is crucial to guarantee the safety of an agent during training as well as deployment (e.g., a robot should avoid taking actions - exploratory or not - which irrevocably harm its hardware). To incorporate safety in RL, we derive algorithms under the framework of constrained Markov decision processes (CMDPs), an extension of the standard Markov decision processes (MDPs) augmented with constraints on expected cumulative costs. Our approach hinges on a novel Lyapunov method. We define and present a method for constructing Lyapunov functions, which provide an effective way to guarantee the global safety of a behavior policy during training via a set of local linear constraints. Leveraging these theoretical underpinnings, we show how to use the Lyapunov approach to systematically transform dynamic programming (DP) and RL algorithms into their safe counterparts. To illustrate their effectiveness, we evaluate these algorithms in several CMDP planning and decision-making tasks on a safety benchmark domain. Our results show that our proposed method significantly outperforms existing baselines in balancing constraint satisfaction and performance.	[Chow, Yinlam; Duenez-Guzman, Edgar] DeepMind, London, England; [Nachum, Ofir] Google Brain, Mountain View, CA USA; [Ghavamzadeh, Mohammad] Facebook AI Res, Menlo Pk, CA USA	Google Incorporated; Facebook Inc	Chow, Y (corresponding author), DeepMind, London, England.	yinlamchow@google.com; ofirnachum@google.com; duenez@google.com; mgh@fb.com						Abe N., 2010, P 16 ACM SIGKDD INT, P75, DOI DOI 10.1145/1835804.1835817; Achiam J., 2017, INT C MACHINE LEARNI; Altman E, 1998, MATH METHOD OPER RES, V48, P387, DOI 10.1007/s001860050035; Altman E, 1999, CONSTRAINED MARKOV D, V7; Amodei D., 2016, CONCRETE PROBLEMS AI; [Anonymous], 2015, ARXIV151106295; [Anonymous], 2016, MULTIOBJECTIVE DEEP; Berkenkamp Felix, 2017, ADV NEURAL INFORM PR, P908; Bertsekas D. P., 1995, DYNAMIC PROGRAMMING; Boyd S, 2004, CONVEX OPTIMIZATION; Chow Y., 2015, J DYNAMIC SYSTEMS ME, V137; Chow Y., 2015, ARXIV151201629; Dalal G., 2018, ARXIV180108757; El Chamie M, 2016, P AMER CONTR CONF, P6290, DOI 10.1109/ACC.2016.7526658; Fruit R., 2017, AISTATS; Geibel P, 2005, J ARTIF INTELL RES, V24, P81, DOI 10.1613/jair.1666; Glynn P.W., 2008, MARKOV PROCESSES REL, P195; Gu SX, 2016, PR MACH LEARN RES, V48; Junges S, 2016, LECT NOTES COMPUT SC, V9636, P130, DOI 10.1007/978-3-662-49674-9_8; Kakade S., 2002, P 19 INT C MACH LEAR; Khalil H., 1996, NONLINEAR SYSTEMS, Vsecond; Lee Jason D, 2017, ARXIV171007406; Leike J, 2017, ARXIV171109883; Luenberger D.G., 1984, LINEAR NONLINEAR PRO, V2; Mastronarde N, 2011, IEEE T SIGNAL PROCES, V59, P6262, DOI 10.1109/TSP.2011.2165211; Mnih V., 2013, PLAYING ATARI DEEP R, P1; Moldovan T. M., 2012, ARXIV12054810; Neely M. J., 2010, SYNTHESIS LECT COMMU, V3, P1, DOI DOI 10.2200/S00271ED1V01Y201006CNT007; Ono M, 2015, AUTON ROBOT, V39, P555, DOI 10.1007/s10514-015-9467-7; Perkins TJ, 2003, J MACH LEARN RES, V3, P803, DOI 10.1162/jmlr.2003.3.4-5.803; Pirotta M., 2013, INT C MACH LEARN, P307; Pirotta M., 2013, ADV NEURAL INFORM PR, V26, P1394; Regan K., 2009, P 25THCONFERENCE UNC, P444; Roijers DM, 2013, J ARTIF INTELL RES, V48, P67, DOI 10.1613/jair.3987; Schaul T, 2016, C TRACK P, P1; Scherrer B, 2013, J MACH LEARN RES, V14, P1181; Schmitt M, 2006, J MACH LEARN RES, V7, P55; Shani G, 2005, J MACH LEARN RES, V6, P1265; Szepesvari C, 1998, INT C MACH LEARN; Tamar A, 2012, INT C MACH LEARN; Van Hasselt H, 2010, ADV NEURAL INFORM PR, P2613	42	24	24	1	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002062
C	Deng, YT; Kim, Y; Chiu, J; Guo, DM; Rush, AM		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Deng, Yuntian; Kim, Yoon; Chiu, Justin; Guo, Demi; Rush, Alexander M.			Latent Alignment and Variational Attention	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Neural attention has become central to many state-of-the-art models in natural language processing and related domains. Attention networks are an easy-to-train and effective method for softly simulating alignment; however, the approach does not marginalize over latent alignments in a probabilistic sense. This property makes it difficult to compare attention to other alignment approaches, to compose it with probabilistic models, and to perform posterior inference conditioned on observed data. A related latent approach, hard attention, fixes these issues, but is generally harder to train and less accurate. This work considers variational attention networks, alternatives to soft and hard attention for learning latent variable alignment models, with tighter approximation bounds based on amortized variational inference. We further propose methods for reducing the variance of gradients to make these approaches computationally feasible. Experiments show that for machine translation and visual question answering, inefficient exact latent variable models outperform standard neural attention, but these gains go away when using hard attention based training On the other hand, variational attention retains most of the performance gain but with training speed comparable to neural attention.	[Deng, Yuntian; Kim, Yoon; Chiu, Justin; Guo, Demi; Rush, Alexander M.] Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA	Harvard University	Deng, YT (corresponding author), Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.	dengyuntian@seas.harvard.edu; yoonkim@seas.harvard.edu; justinchiu@g.harvard.edu; dguo@college.harvard.edu; srush@seas.harvard.edu		Rush, Alexander/0000-0002-9900-1606	Facebook Research Award (Low Resource NMT); Google AI PhD Fellowship; Bloomberg Research Award; NSF [CCF-1704834]; Amazon AWS Research award	Facebook Research Award (Low Resource NMT); Google AI PhD Fellowship(Google Incorporated); Bloomberg Research Award; NSF(National Science Foundation (NSF)); Amazon AWS Research award	We are grateful to Sam Wiseman and Rachit Singh for insightful comments and discussion, as well as Christian Puhrsch for help with translations. This project was supported by a Facebook Research Award (Low Resource NMT). YK is supported by a Google AI PhD Fellowship. YD is supported by a Bloomberg Research Award. AMR gratefully acknowledges the support of NSF CCF-1704834 and an Amazon AWS Research award.	Alvarez-Melis David, 2017, P 2017 C EMP METH NA, P412, DOI DOI 10.18653/V1/D17-1042; ANDERSON P, 2018, CVPR, V3, P6, DOI DOI 10.1109/CVPR.2018.00636; Ba J., 2015, ICLR; Ba J, 2015, ADV NEUR IN, V28; Bahdanau Dzmitry, 2017, ICLR; Bahuleyan H., 2017, ARXIV171208207; Biao Zhang, 2016, P EMNLP; Bojar O., 2017, P 2 C MACH TRANSL AS; Bornschein J, 2017, ADV NEUR IN, V30; Brown P. F., 1993, Computational Linguistics, V19, P263; Burda Yuri, 2015, P ICLR; Cettolo M., 2014, P IWSLT; Cho K., 2015, IEEE T MULTIMEDIA; Chorowski I. K., 2015, ADV NEURAL INFORM PR, V28, P577, DOI DOI 10.1016/0167-739X(94)90007-8; Chung J, 2015, ADV NEURAL INFORM PR, P2980; Cohn T., 2016, INCORPORATING STRUCT, P876; Deng YT, 2017, PR MACH LEARN RES, V70; Dyer Chris, 2013, HLT NAACL; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Edunov Sergey, 2018, P 2018 C N AM CHAPT, V1, P355, DOI DOI 10.18653/V1/N18-1033; Fraccaro Marco, 2016, P NIPS; Goyal A., 2017, ADV NEURAL INFORM PR; Grathwohl Will, 2018, ICLR; Gu J., 2016, INCORPORATING COPYIN; Gulcehre C., 2016, ARXIV160700036; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Huang Po-Sen, 2018, ICLR; Jang E., 2017, ICLR; Jang Eric, 2017, P 5 INT C LEARN REPR; Jankowiak Martin, 2018, ARXIV180601851; Kim Y., 2017, ABS170200887 CORR; Kim Y, 2018, PR MACH LEARN RES, V80; Kingma D.P, P 3 INT C LEARNING R; Koehn P, 2007, 45 ANN M ASS COMP LI, P177, DOI DOI 10.3115/1557769.1557821; Koehn Philipp, 2017, NMT ACL; Krishnan RG, 2018, PR MACH LEARN RES, V84; Krishnan RG, 2017, AAAI CONF ARTIF INTE, P2101; Lawson Dieterich, 2018, P ICASSP; Lei T., 2016, P EMNLP; Lei Yu, 2017, P ICLR; Liu Yang, 2017, P TACL; Luong M., 2015, ARXIV150804025; Ma Xuezhe, 2017, P ICLR; Maddison Chris J, 2017, ICLR; Martins AFT, 2016, PR MACH LEARN RES, V48; Mensch A, 2018, PR MACH LEARN RES, V80; Mnih Andriy, 2014, INT C MACH LEARN; Mnih Andriy, 2016, P ICML; Mnih Volodymyr, 2015, P NIPS; Niculae V., 2017, ADV NEURAL INFORM PR, P3338; Niculae V, 2018, PR MACH LEARN RES, V80; Novak Roman, 2016, ARXIV161006602; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Raffel C, 2017, PR MACH LEARN RES, V70; Ranganath R., 2014, ARTIFICIAL INTELLIGE, P814; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Riloff Ellen, 2018, P 2018 C EMP METH NA; Rocktaschel Tim, 2016, P ICLR; Rush Alexander M, 2015, P 2015 C EMP METH NA, P379, DOI DOI 10.18653/V1/D15-1044; Schulz Philip, 2018, P ACL; Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715; Serban  I., 2017, AAAI C ART INT; Shankar Shiv, 2018, P EMNLP; Shin Bonggun, 2017, P IJCNN; Srivastava A, 2017, INT C LEARN REPR ICL, P1; Su Jinyue, 2018, P AAAI; Sukhbaatar S., 2015, P 28 INT C NEURAL IN, V28, P2440; Tu Z., 2016, CORR; Tucker G, 2017, ADV NEUR IN, V30; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vogel S., 1996, COLING 1996, V2; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wiseman S, 2016, EMNLP; Wu Shijie, 2018, P EMNLP; Wu Y., 2016, ADV NEURAL INFORM PR, V1609, DOI 10.48550/arXiv.1609.08144; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Yu Lei, 2016, P EMNLP; Zhu Chen, 2017, INT C COMP VIS ICCV	82	24	26	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004029
C	Dieleman, S; van den Oord, A; Simonyan, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dieleman, Sander; van den Oord, Aaron; Simonyan, Karen			The challenge of realistic music generation: modelling raw audio at scale	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.	[Dieleman, Sander; van den Oord, Aaron; Simonyan, Karen] DeepMind, London, England		Dieleman, S (corresponding author), DeepMind, London, England.	sedielem@google.com; avdnoord@google.com; simonyan@google.com						[Anonymous], 2016, ARXIV160903499; Arjovsky M, 2016, PR MACH LEARN RES, V48; Bachman Philip, 2016, ADV NEURAL INFORM PR, P4826; Belghazi M.I., 2018, INT C LEARN REPR, V1071; Bengio Yoshua, 2013, ARXIV; Boulanger-Lewandowski N., 2012, ICML; Bowman S. R., 2016, ARXIV, P10; Briot J., 2017, DEEP LEARNING TECHNI; Chang Shiyu, 2017, P ADV NEUR INF PROC, P76; Chen Xi, 2016, ARXIV161102731; Chung J, 2015, ADV NEURAL INFORM PR, P2980; Cope D, 1996, EXPT MUSICAL INTELLI, V12; Denton E, 2015, DEEP GENERATIVE IMAG, DOI DOI 10.5555/; Donahue C, 2018, ARXIV PREPRINT ARXIV; Donahue J., 2017, ARXIV171109846; ElHihi S, 1996, ADV NEUR IN, V8, P493; Engel J., 2017, NEURAL AUDIO SYNTHES; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves Alex, 2018, ARXIV180402476; Gulrajani Ishaan, 2016, ABS161105013 ARXIV; Henaff Mikael, 2016, CORR; Herremans D, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3108242; Heusel M., 2017, P 31 INT C NEUR INF, P6629; Kalchbrenner N., 2018, CORR; Kingma D.P., 2015, 3 INT C LEARN REPR I, P1, DOI DOI 10.1007/S11390-017-1754-7; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kingma DP, 2 INT C LEARN REPR I, P1; Kolesnikov A, 2017, PR MACH LEARN RES, V70; Koutnik Jan, 2014, ARXIV14023511; Liu Peter J., 2018, GENERATING WIKIPEDIA, P2; Liu S, 2017, ARXIV170505994; Mehri Soroush, 2017, ICLR; Mikolov Tomas, 2014, P 3 INT C LEARN REPR; Nair V., 2010, ICML, P807; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Rezende D.J., 2014, PROC INT CONFER ENCE; Roberts A, 2018, PR MACH LEARN RES, V80; Salimans T., 2016, ADV NEUR IN, P2234; Salimans T., 2017, INT C LEARNING REPRE; Serban IV, 2017, AAAI CONF ARTIF INTE, P3295; Simon I, 2017, PERFORMANCE RNN GENE; Trinh T. H., 2018, ARXIV180300144; Van Den Oord A., 2016, ICML; van den Oord A, 2017, P 31 C NEUR INF PROC, P6309; van den Oord A., 2017, CORR; Yu F., 2016, P INT C LEARNING REP	47	24	24	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002053
C	Gunasekar, S; Lee, JD; Soudry, D; Srebro, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gunasekar, Suriya; Lee, Jason D.; Soudry, Daniel; Srebro, Nathan			Implicit Bias of Gradient Descent on Linear Convolutional Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We show that gradient descent on full width linear convolutional networks of depth L converges to a linear predictor related to the l(2/L) bridge penalty in the frequency domain. This is in contrast to fully connected linear networks, where regardless of depth, gradient descent converges to the l(2) maximum margin solution.	[Gunasekar, Suriya; Srebro, Nathan] TTI, Chicago, IL 60637 USA; [Lee, Jason D.] USC Los Angeles, Los Angeles, CA USA; [Soudry, Daniel] Technion, Haifa, Israel	University of Southern California	Gunasekar, S (corresponding author), TTI, Chicago, IL 60637 USA.	suriya@ttic.edu; jasonlee@marshall.usc.edu; daniel.soudry@gmail.com; nati@ttic.edu		Lee, Jason/0000-0003-0064-7800				Andrychowicz Marcin, 2016, ADV NEURAL INFORM PR; [Anonymous], 2018, ARXIV180301905; Bartlett P. L., 2003, J MACHINE LEARNING R; Chaudhari P., 2016, ARXIV161101838; Dinh Laurent, 2017, INT C MACH LEARN; Ge Dongdong, 2011, MATH PROGRAMMING; GUNASEKAR S., 2018, CHARACTERIZING IMPLI; Gunasekar Suriya, 2017, NIPS; Hochreiter S, 1997, NEURAL COMPUTATION; Hoffer Elad, 2017, ADV NEURAL INFORM PR; Ji Z., 2018, ARXIV180307300; Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359; Kakade S.M., 2009, ADV NEURAL INFORM PR; Kawaguchi K., 2016, ADV NEURAL INFORM PR; Keskar Nitish Shirish, 2016, INT C LEARN REPR; Le Smith Kindermans, 2018, ICLR; Lee JD, 2016, DESCENT ONLY CONVERG; Li Yuanzhi, 2017, ARXIV171209203; Muresan M, 2009, CONCRETE APPROACH CL, V14; Neyshabur B., 2017, GEOMETRY OPTIMIZATIO; Neyshabur B., 2015, INT C LEARN REPR; Nguyen Q., 2017, ARXIV170408045; Rockafellar R Tyrrell, 1979, P LONDON MATH SOC; Srebro N., 2017, ARXIV171010345; Telgarsky M., 2013, INT C MACHINE LEARNI, P307; Wilson A. C., 2017, ADV NEURAL INFORM PR; Zeng HQ, 2017, PROC INT CONF RECON	29	24	24	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004006
C	Heo, J; Lee, HB; Kim, S; Lee, J; Kim, KJ; Yang, E; Hwang, SJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Heo, Jay; Lee, Hae Beom; Kim, Saehoon; Lee, Juho; Kim, Kwang Joon; Yang, Eunho; Hwang, Sung Ju			Uncertainty-Aware Attention for Reliable Interpretation and Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Attention mechanism is effective in both focusing the deep learning models on relevant features and interpreting them. However, attentions may be unreliable since the networks that generate them are often trained in a weakly-supervised manner. To overcome this limitation, we introduce the notion of input-dependent uncertainty to the attention mechanism, such that it generates attention for each feature with varying degrees of noise based on the given input, to learn larger variance on instances it is uncertain about. We learn this Uncertainty-aware Attention (UA) mechanism using variational inference, and validate it on various risk prediction tasks from electronic health records on which our model significantly outperforms existing attention models. The analysis of the learned attentions shows that our model generates attentions that comply with clinicians' interpretation, and provide richer interpretation via learned variance. Further evaluation of both the accuracy of the uncertainty calibration and the prediction performance with "I don't know" decision show that UA yields networks with high reliability as well.	[Heo, Jay; Lee, Hae Beom; Yang, Eunho; Hwang, Sung Ju] Korea Adv Inst Sci & Technol, Daejeon, South Korea; [Heo, Jay; Lee, Hae Beom; Kim, Saehoon; Lee, Juho; Yang, Eunho; Hwang, Sung Ju] AItrics, Seoul, South Korea; [Kim, Kwang Joon] Yonsei Univ, Coll Med, Seoul, South Korea; [Heo, Jay] UNIST, Ulsan, South Korea; [Lee, Juho] Univ Oxford, Oxford, England	Korea Advanced Institute of Science & Technology (KAIST); Yonsei University; Yonsei University Health System; Ulsan National Institute of Science & Technology (UNIST); University of Oxford	Heo, J (corresponding author), Korea Adv Inst Sci & Technol, Daejeon, South Korea.; Heo, J (corresponding author), AItrics, Seoul, South Korea.; Heo, J (corresponding author), UNIST, Ulsan, South Korea.	jayheo@kaist.ac.kr; haebeom.lee@kaist.ac.kr; shkim@aitrics.com; juho.lee@stats.ox.ac.uk; preppie@yuhs.ac; eunhoy@kaist.ac.kr; sjhwang82@kaist.ac.kr	Yang, Eunho/K-8395-2016; Lee, Juho/AAA-2901-2022		Machine Learning and Statistical Inference Framework for Explainable Artificial Intelligence - Institution for Information & Communications & Technology Promotion (IITP) [2017-0-01779]; Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Education of South Korea [2015R1D1A1A01061019]; European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant [617071]	Machine Learning and Statistical Inference Framework for Explainable Artificial Intelligence - Institution for Information & Communications & Technology Promotion (IITP)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of Korea); Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Education of South Korea(Ministry of Education (MOE), Republic of KoreaNational Research Foundation of Korea); European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant(European Research Council (ERC))	This work was supported by a Machine Learning and Statistical Inference Framework for Explainable Artificial Intelligence (No.2017-0-01779) funded by Institution for Information & Communications & Technology Promotion (IITP) and Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (2015R1D1A1A01061019) of South Korea. Juho Lee is funded by the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617071.	Ayhan Murat Seckin, 2018, MIDL; Choi E, 2016, ADV NEUR IN, V29; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Futoma  J., 2017, ICML; Gal  Y., THEORETICALLY GROUND; Gal Y., 2017, NIPS; Gal Y, 2016, PR MACH LEARN RES, V48; Gal Yarin, 2015, ARXIV150602158; Guo Chuan, 2017, ICML, DOI DOI 10.5555/3305381.3305518; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Ivanovitch Silva D. J. S. L. A. C., 2012, IN CINC; Johnson A. E., MIMIC 3 FREELY ACCES; Kendall A, 2015, P BRIT MACH VIS C 20; Kendall Alex, 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295309; Kingma D.P, P 3 INT C LEARNING R; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y., 2010, MNIST HANDWRITTEN DI; Maddison Chris J, 2016, ARXIV161100712; Naeini MP, 2015, AAAI CONF ARTIF INTE, P2901; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sukhbaatar S, 2015, ADV NEUR IN, V28; Tanno  R., 2017, BAYESIAN IMAGE QUALI; van der Westhuizen  J., 2017, BAYESIAN LSTMS MED; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Zhu  L., 2017, DEEP CONFIDENT PREDI	29	24	24	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300084
C	Jeon, Y; Kim, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jeon, Yunho; Kim, Junmo			Constructing Fast Network through Deconstruction of Convolution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Convolutional neural networks have achieved great success in various vision tasks; however, they incur heavy resource costs. By using deeper and wider networks, network accuracy can be improved rapidly. However, in an environment with limited resources (e.g., mobile applications), heavy networks may not be usable. This study shows that naive convolution can be deconstructed into a shift operation and pointwise convolution. To cope with various convolutions, we propose a new shift operation called active shift layer (ASL) that formulates the amount of shift as a learnable function with shift parameters. This new layer can be optimized end-to-end through backpropagation and it can provide optimal shift values. Finally, we apply this layer to a light and fast network that surpasses existing state-of-the-art networks. Code is available at https://github.com/jyh2986/Active-Shift.	[Jeon, Yunho; Kim, Junmo] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Jeon, Y (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.	jyh2986@kaist.ac.kr; junmo.kim@kaist.ac.kr	Jeon, Yunho/HDO-2841-2022	Jeon, Yunho/0000-0001-8043-480X				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chollet F., 2017, PROC CVPR IEEE, P1251, DOI DOI 10.1109/CVPR.2017.195; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Han S., 2016, P 4 INT C LEARN REPR, P1; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Howard A.G., 2017, MOBILENETS EFFICIENT; Iandola Forrest N., 2016, SQUEEZENET ALEXNET L; Jeon YH, 2017, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2017.200; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Juefei-Xu F, 2017, PROC CVPR IEEE, P4284, DOI 10.1109/CVPR.2017.456; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lebedev V., 2015, 3 INT C LEARNING REP; Li H., 2017, P INT C LEARN REPR I, P1; Sandler Mark, 2018, ARXIV180104381, DOI DOI 10.1109/CVPR.2018.00474; Sun G., 2018, P IEEE CVF C COMP VI, P7132, DOI DOI 10.1109/CVPR.2018.00745; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Wu  Bichen, 2018, IEEE C COMP VIS PATT; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yu F., 2016, P ICLR 2016; Zhang Xiangyu, 2018, IEEE C COMP VIS PATT	26	24	25	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000045
C	Jiang, CH; Xu, H; Liang, XD; Lin, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jiang, Chenhan; Xu, Hang; Liang, Xiaodan; Lin, Liang			Hybrid Knowledge Routed Modules for Large-scale Object Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The dominant object detection approaches treat the recognition of each region separately and overlook crucial semantic correlations between objects in one scene. This paradigm leads to substantial performance drop when facing heavy long-tail problems, where very few samples are available for rare classes and plenty of confusing categories exists. We exploit diverse human commonsense knowledge for reasoning over large-scale object categories and reaching semantic coherency within one image. Particularly, we present Hybrid Knowledge Routed Modules (HKRM) that incorporates the reasoning routed by two kinds of knowledge forms: an explicit knowledge module for structured constraints that are summarized with linguistic knowledge (e.g. shared attributes, relationships) about concepts; and an implicit knowledge module that depicts some implicit constraints (e.g. common spatial layouts). By functioning over a region-to-region graph, both modules can be individualized and adapted to coordinate with visual patterns in each image, guided by specific knowledge forms. HKRM are light-weight, general-purpose and extensible by easily incorporating multiple knowledge to endow any detection networks the ability of global semantic reasoning. Experiments on large-scale object detection benchmarks show HKRM obtains around 34.5% improvement on VisualGenome (1000 categories) and 30.4% on ADE in terms of mAP. Codes and trained model can be found in https://github.com/chanyn/HKRM.	[Jiang, Chenhan; Lin, Liang] Sun Yat Sen Univ, Guangzhou, Guangdong, Peoples R China; [Xu, Hang] Huawei Noahs Ark Lab, Shenzhen, Peoples R China; [Liang, Xiaodan] Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou, Guangdong, Peoples R China	Sun Yat Sen University; Huawei Technologies; Sun Yat Sen University	Liang, XD (corresponding author), Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou, Guangdong, Peoples R China.	jchcyan@gmail.com; xbjxh@live.com; xdliang328@gmail.com; linliang@ieee.org			National Key Research and Development Program of China [2018YFC0830103]; National High Level Talents Special Support Plan (Ten Thousand Talents Program); National Natural Science Foundation of China (NSFC) [61622214, 61836012]	National Key Research and Development Program of China; National High Level Talents Special Support Plan (Ten Thousand Talents Program); National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	This work was supported in part by the National Key Research and Development Program of China under Grant No. 2018YFC0830103, in part by National High Level Talents Special Support Plan (Ten Thousand Talents Program), and in part by National Natural Science Foundation of China (NSFC) under Grant No. 61622214, and 61836012.	Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111; Almazan J, 2014, IEEE T PATTERN ANAL, V36, P2552, DOI 10.1109/TPAMI.2014.2339814; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; BIEDERMAN I, 1982, COGNITIVE PSYCHOL, V14, P143, DOI 10.1016/0010-0285(82)90007-X; Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644; Chen XL, 2018, PROC CVPR IEEE, P7239, DOI 10.1109/CVPR.2018.00756; Chen XL, 2017, IEEE I CONF COMP VIS, P4106, DOI 10.1109/ICCV.2017.440; Dai B, 2017, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR.2017.352; Dai Jifeng, 2016, ADV NEURAL INFORM PR, P379, DOI DOI 10.1016/J.JPOWSOUR.2007.02.075; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deng J, 2014, LECT NOTES COMPUT SC, V8689, P48, DOI 10.1007/978-3-319-10590-1_4; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Farhadi Ali, 2009, CVPR; Frome Andrea, 2013, NEURIPS; Galleguillos C., 2008, CVPR; Garcia V., 2018, ICLR 2018 FEW SHOT L, P13; Gould S., 2009, ADV NEURAL INFORM PR, V22; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hoffman Judy, 2014, NIPS; Hu R, 2018, PROC CVPR IEEE, P4233, DOI 10.1109/CVPR.2018.00445; Jayaraman D, 2014, ADV NEUR IN, V27; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Krishna Ranjay, 2016, ARXIV160207332; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789; Li Yujia, 2016, P INT C LEARN REPR I, P2; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Mao J., 2015, ICCV; Marino K, 2017, PROC CVPR IEEE, P20, DOI 10.1109/CVPR.2017.10; Mensink T., 2012, COMPUTER VISION ECCV; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Misra I, 2017, PROC CVPR IEEE, P1160, DOI 10.1109/CVPR.2017.129; Niepert M, 2016, PR MACH LEARN RES, V48; Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salakhutdinov R, 2011, PROC CVPR IEEE, P1481, DOI 10.1109/CVPR.2011.5995720; Springenberg J. T, 2015, ARXIV PREPRINT ARXIV; Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273; Torres AG, 2004, IEEE IND APPLIC SOC, P1; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang XH, 2018, ADV EXP MED BIOL, V1078, P3, DOI 10.1007/978-981-13-0950-2_1; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wu Q, 2016, PROC CVPR IEEE, P4622, DOI 10.1109/CVPR.2016.500; Yang J., 2017, FASTER PYTORCH IMPLE; Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321	56	24	25	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301053
C	Lipton, ZC; Chouldechova, A; McAuley, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lipton, Zachary C.; Chouldechova, Alexandra; McAuley, Julian			Does mitigating ML's impact disparity require treatment disparity?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Following precedent in employment discrimination law, two notions of disparity are widely-discussed in papers on fairness and ML. Algorithms exhibit treatment disparity if they formally treat members of protected subgroups differently; algorithms exhibit impact disparity when outcomes differ across subgroups (even unintentionally). Naturally, we can achieve impact parity through purposeful treatment disparity. One line of papers aims to reconcile the two parities proposing disparate learning processes (DLPs). Here, the sensitive feature is used during training but a group-blind classifier is produced. In this paper, we show that: (i) when sensitive and (nominally) nonsensitive features are correlated, DLPs will indirectly implement treatment disparity, undermining the policy desiderata they are designed to address; (ii) when group membership is partly revealed by other features, DLPs induce within-class discrimination; and (iii) in general, DLPs provide suboptimal trade-offs between accuracy and impact parity. Experimental results on several real-world datasets highlight the practical consequences of applying DLPs.	[Lipton, Zachary C.; Chouldechova, Alexandra] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [McAuley, Julian] Univ Calif San Diego, San Diego, CA 92103 USA	Carnegie Mellon University; University of California System; University of California San Diego	Lipton, ZC (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	zlipton@cmu.edu; achould@cmu.edu; jmcauley@cs.ucsd.edu						Adler Philip, 2016, ARXIV160207043; [Anonymous], ARXIV170306856; Bechavod Y., 2017, ARXIV PREPRINT ARXIV; Berk R., 2017, ARXIV PREPRINT ARXIV; Chouldechova A., 2017, BIG DATA; Corbett-Davies Sam, 2017, ARXIV170108230; Datta Anupam, 2017, ARXIV170708120; Dwork Cynthia, 2012, P 3 INN THEOR COMP S, P214; Dwork Cynthia, 2017, ARXIV170706613; Feldman M., 2015, KDD; Hardt M., 2016, NIPS; Hartocollis Anemona, 2017, AFFIRMATIVE ACTION B; Joseph M., 2016, P 3 WORKSH FAIRN ACC; Kamiran F., 2010, ICDM; Kamiran F, 2012, KNOWL INF SYST, V33, P1, DOI 10.1007/s10115-011-0463-8; Kamiran Faisal, 2009, COMPUTER CONTROL COM; Kamishima T., 2011, ICDM WORKSH; Kilbertus N., 2017, ARXIV170602744; Kim Pauline, 2017, AUDITING ALGORITHMS; Kim PT., 2017, WILLIAM MARY LAW REV, V58, P81; Kleinberg Jon, 2016, P 8 INN THEOR COMP S; Kohavi R., 1996, KDD; Lum K, 2017, ALGORITHM REMOVING S; Menon Aditya, 2018, FAIRNESS ACCOUNTABIL; Moro S., 2014, DECISION SUPPORT SYS; Pedreshi D., 2008, KDD; Ritov Yaacov, 2017, ARXIV170608519; Sh- pitser Ilya, 2017, ARXIV170510378; Yeh I., 2009, EXPERT SYSTEMS APPL; Zafar Muhammad Bilal, 2017, ARXIV170700010; Zafar Muhammad Bilal, 2017, AISTATS; Zemel Rich, 2013, ICML	32	24	24	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002065
C	Scaman, K; Bach, F; Bubeck, S; Lee, YT; Massoulie, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Scaman, Kevin; Bach, Francis; Bubeck, Sebastien; Lee, Yin Tat; Massoulie, Laurent			Optimal Algorithms for Non-Smooth Distributed Optimization in Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CONVERGENCE	In this work, we consider the distributed optimization of non-smooth convex functions using a network of computing units. We investigate this problem under two regularity assumptions: (1) the Lipschitz continuity of the global objective function, and (2) the Lipschitz continuity of local individual functions. Under the local regularity assumption, we provide the first optimal first-order decentralized algorithm called multi-step primal-dual (MSPD) and its corresponding optimal convergence rate. A notable aspect of this result is that, for non-smooth functions, while the dominant term of the error is in O (1/root t), the structure of the communication network only impacts a second-order term in O (1/root t), where t is time. In other words, the error due to limits in communication resources decreases at a fast rate even in the case of non-strongly-convex objective functions. Under the global regularity assumption, we provide a simple yet efficient algorithm called distributed randomized smoothing (DRS) based on a local smoothing of the objective function, and show that DRS is within a d(1/4) multiplicative factor of the optimal convergence rate, where d is the underlying dimension.	[Scaman, Kevin] Huawei Noahs Ark Lab, Paris, France; [Bach, Francis; Massoulie, Laurent] PSL Res Univ, Ecole Normale Super, INRIA, Paris, France; [Bubeck, Sebastien; Lee, Yin Tat] Microsoft Res, Redmond, WA USA; [Lee, Yin Tat] Univ Washington, Seattle, WA 98195 USA; [Massoulie, Laurent] MSR INRIA Joint Ctr, Paris, France	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Microsoft; University of Washington; University of Washington Seattle	Scaman, K (corresponding author), Huawei Noahs Ark Lab, Paris, France.							Arioli M, 2014, NUMER ALGORITHMS, V66, P591, DOI 10.1007/s11075-013-9750-7; Arjevani Y., 2015, ADV NEURAL INFORM PR, V28, P1756; Auzinger W., 2011, LECT NOTES; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Boyd S, 2006, IEEE T INFORM THEORY, V52, P2508, DOI 10.1109/TIT.2006.874516; Bubeck S., 2015, FDN TRENDS MACHINE L; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Duchi JC, 2012, SIAM J OPTIMIZ, V22, P674, DOI 10.1137/110831659; Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027; He N, 2015, COMPUT OPTIM APPL, V61, P275, DOI 10.1007/s10589-014-9723-3; Jaggi M., 2014, ADV NEURAL INF PROCE, V4, P3068; Jakovetic D, 2015, IEEE T AUTOMAT CONTR, V60, P922, DOI 10.1109/TAC.2014.2363299; Jakovetic D, 2014, IEEE T AUTOMAT CONTR, V59, P1131, DOI 10.1109/TAC.2014.2298712; Lan G., 2017, ARXIV170103961; Mokhtari Aryan, 2016, J MACHINE LEARNING R, V17, P2165; Nedic A, 2017, SIAM J OPTIMIZ, V27, P2597, DOI 10.1137/16M1084316; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Nesterov Y., 2018, APPL OPTIMIZATION; Scaman K, 2017, PR MACH LEARN RES, V70; Shamir Ohad, 2014, ADV NEURAL INFORM PR, V27, P163; Shi W, 2015, SIAM J OPTIMIZ, V25, P944, DOI 10.1137/14096668X; Shi W, 2014, IEEE T SIGNAL PROCES, V62, P1750, DOI 10.1109/TSP.2014.2304432; Simon Lacoste-Julien, 2012, 12122002 ARXIV; Wei E, 2012, IEEE DECIS CONTR P, P5445, DOI 10.1109/CDC.2012.6425904	25	24	23	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302073
C	Yeh, CK; Kim, JS; Yen, IEH; Ravikumar, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yeh, Chih-Kuan; Kim, Joon Sik; Yen, Ian E. H.; Ravikumar, Pradeep			Representer Point Selection for Explaining Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose to explain the predictions of a deep neural network, by pointing to the set of what we call representer points in the training set, for a given test point prediction. Specifically, we show that we can decompose the pre-activation prediction of a neural network into a linear combination of activations of training points, with the weights corresponding to what we call representer values, which thus capture the importance of that training point on the learned parameters of the network. But it provides a deeper understanding of the network than simply training point influence: with positive representer values corresponding to excitatory training points, and negative values corresponding to inhibitory points, which as we show provides considerably more insight. Our method is also much more scalable, allowing for real-time feedback in a manner not feasible with influence functions.	[Yeh, Chih-Kuan; Kim, Joon Sik; Yen, Ian E. H.; Ravikumar, Pradeep] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Yeh, CK (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	cjyeh@cs.cmu.edu; joonsikk@cs.cmu.edu; eyan@cs.cmu.edu; pradeepr@cs.cmu.edu			DARPA [FA87501720152]; Zest Finance	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Zest Finance	We acknowledge the support of DARPA via FA87501720152, and Zest Finance.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Anirudh R., 2017, ARXIV171105407; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Bien J, 2011, ANN APPL STAT, V5, P2403, DOI 10.1214/11-AOAS495; Bohn Bastian, 2017, ARXIV170910441; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Kim B., 2014, ADV NEURAL INFORM PR, P1952; Kim Been, 2016, NEURIPS, V1, P8; Koh PW, 2017, PR MACH LEARN RES, V70; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Maas A., 2011, P 49 ANN M ASS COMPU, P142; Ribeiro M.T., 2018, ANCHORS HIGH PRECISI; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27; Shrikumar Avanti, 2017, PMLR, P3145, DOI DOI 10.1145/3292500.3330701; Simonyan Karen, 2013, DEEP INSIDE CONVOLUT, P2; Smilkov D, 2017, ARXIV; Sundararajan M, 2017, PR MACH LEARN RES, V70; Unser M., 2018, ARXIV180209210; Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768	21	24	25	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003081
C	He, Z; Gao, SB; Xiao, L; Liu, DX; He, HG; Barber, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		He, Zhen; Gao, Shaobing; Xiao, Liang; Liu, Daxue; He, Hangen; Barber, David			Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Long Short-Term Memory (LSTM) is a popular approach to boosting the ability of Recurrent Neural Networks to store longer term temporal information. The capacity of an LSTM network can be increased by widening and adding layers. However, usually the former introduces additional parameters, while the latter increases the runtime. As an alternative we propose the Tensorized LSTM in which the hidden states are represented by tensors and updated via a cross-layer convolution. By increasing the tensor size, the network can be widened efficiently without additional parameters since the parameters are shared across different locations in the tensor; by delaying the output, the network can be deepened implicitly with little additional runtime since deep computations for each timestep are merged into temporal computations of the sequence. Experiments conducted on five challenging sequence learning tasks show the potential of the proposed model.	[He, Zhen; Barber, David] UCL, London, England; [He, Zhen; Xiao, Liang; Liu, Daxue; He, Hangen] Natl Univ Def Technol, Changsha, Hunan, Peoples R China; [Gao, Shaobing] Sichuan Univ, Chengdu, Sichuan, Peoples R China; [Barber, David] Alan Turing Inst, London, England	University of London; University College London; National University of Defense Technology - China; Sichuan University	He, Z (corresponding author), UCL, London, England.; He, Z (corresponding author), Natl Univ Def Technol, Changsha, Hunan, Peoples R China.; Gao, SB (corresponding author), Sichuan Univ, Chengdu, Sichuan, Peoples R China.	hezhen.cs@gmail.com; gaoshaobing@scu.edu.cn	Jeong, Yongwook/N-7413-2016; Xiao, Liang/A-5160-2016	Xiao, Liang/0000-0001-6959-4343	NSFC [91220301]; Alan Turing Institute under the EPSRC [EP/N510129/1]; China Scholarship Council	NSFC(National Natural Science Foundation of China (NSFC)); Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); China Scholarship Council(China Scholarship Council)	This work is supported by the NSFC grant 91220301, the Alan Turing Institute under the EPSRC grant EP/N510129/1, and the China Scholarship Council.	[Anonymous], ARXIV150400941; Appleyard Jeremy, 2016, ARXIV160401946; Arjovsky M, 2016, PR MACH LEARN RES, V48; Bengio, 2016, ARXIV160901704; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Bertinetto Luca, 2016, NIPS; Chang SY, 2017, ADV NEUR IN, V30; Chen J., 2016, NIPS; Chung JY, 2015, PR MACH LEARN RES, V37, P2067; Collobert R., 2011, NIPS; Cooijmans T., 2017, ICLR; De Brabandere B, 2016, ADV NEUR IN, V29; Denil Misha, 2013, NIPS, DOI DOI 10.5555/2999792.2999852; Diamos Greg, 2016, ICML; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Garipov Timur, 2016, NIPS WORKSHOP; Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015; Graves A., 2013, P 2013 IEEE INT C AC, P6645, DOI [10.1109/ICASSP.2013.6638947, DOI 10.1109/ICASSP.2013.6638947]; Graves A, 2013, ARXIV13080850; Graves Alex, 2016, ARXIV160308983; Ha David, 2017, ICLR; Hinton, 2016, ARXIV PREPRINT ARXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; HUTTER M, 2012, HUMAN KNOWLEDGE COMP; Irsoy O., 2015, ICLR; Jozefowicz R, 2015, PR MACH LEARN RES, V37, P2342; Kaiser Lukasz, 2016, NIPS; Kaiser Lukasz, 2016, ICLR; Kalchbrenner Nal, 2016, ICLR; Kingma D.P, P 3 INT C LEARNING R; Krause Ben, 2017, ICLR WORKSH; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lei Tao, 2017, ARXIV170902755; Leifert G, 2016, J MACH LEARN RES, V17; Merity Stephen, 2017, P ICLR 2017; Mujika Asier, 2017, NIPS; Novikov A., 2015, ADV NEURAL INFORM PR, V28, P442, DOI DOI 10.5555/2969239.2969289; Oord A.V.D., 2016, SSW; Romera-Paredes B., 2016, ECCV; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131; Stollenga M. F., 2015, ADV NEURAL INFORM PR, P2998; Sutskever Ilya, 2011, P 28 INT C MACH LEAR; Taylor Graham, 2009, P 26 ANN INT C MACH, DOI DOI 10.1145/1553374.1553505; van den Hengel, 2016, ARXIV160601609; van den Oord A, 2016, PR MACH LEARN RES, V48; Viorica P .atr., 2016, ICLR WORKSH; Wisdom S, 2016, ADV NEUR IN, V29; Wu Y., 2016, NIPS; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Zhang SZ, 2016, ADV NEUR IN, V29	54	24	24	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400001
C	Ilievski, I; Feng, JS		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ilievski, Ilija; Feng, Jiashi			Multimodal Learning and Reasoning for Visual Question Answering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NEURAL-NETWORKS	Reasoning about entities and their relationships from multimodal data is a key goal of Artificial General Intelligence. The visual question answering (VQA) problem is an excellent way to test such reasoning capabilities of an AI model and its multimodal representation learning. However, the current VQA models are oversimplified deep neural networks, comprised of a long short-term memory (LSTM) unit for question comprehension and a convolutional neural network (CNN) for learning single image representation. We argue that the single visual representation contains a limited and general information about the image contents and thus limits the model reasoning capabilities. In this work we introduce a modular neural network model that learns a multimodal and multifaceted representation of the image and the question. The proposed model learns to use the multimodal representation to reason about the image entities and achieves a new state-of-the-art performance on both VQA benchmark datasets, VQA v1.0 and v2.0, by a wide margin.	[Ilievski, Ilija] Natl Univ Singapore, Integrat Sci & Engn, Singapore, Singapore; [Feng, Jiashi] Natl Univ Singapore, Elect & Comp Engn, Singapore, Singapore	National University of Singapore; National University of Singapore	Ilievski, I (corresponding author), Natl Univ Singapore, Integrat Sci & Engn, Singapore, Singapore.	ilija.ilievski@u.nus.edu; elefjia@nus.edu.sg	Feng, Jiashi/AGX-6209-2022; Jeong, Yongwook/N-7413-2016; Ilievski, Ilija/AAA-2733-2022	Ilievski, Ilija/0000-0001-5089-8814	National University of Singapore [R-263-000-C08-133]; Ministry of Education of Singapore AcRF Tier One grant [R-263-000-C21-112]; NUS IDS grant [R-263-000-C67-646]	National University of Singapore(National University of Singapore); Ministry of Education of Singapore AcRF Tier One grant(Ministry of Education, Singapore); NUS IDS grant	The work of Jiashi Feng was partially supported by National University of Singapore startup grant R-263-000-C08-133, Ministry of Education of Singapore AcRF Tier One grant R-263-000-C21-112 and NUS IDS grant R-263-000-C67-646.	Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Andreas Jacob, 2016, ARXIV160101705, P1545, DOI [DOI 10.18653/V1/N16-1181, 10.18653/v1/N16-1181]; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Charikar M, 2002, LECT NOTES COMPUT SC, V2380, P693; Chen Kan, 2015, ARXIV151105960; Fukui Akira, 2016, ARXIV160601847; Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93; Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470; Ilievski I., 2016, ARXIV160401485; Johnson J, 2017, IEEE I CONF COMP VIS, P3008, DOI 10.1109/ICCV.2017.325; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Kim Jin-Hwa, 2016, ADV NEURAL INFORM PR, P1; Kingma D.P, P 3 INT C LEARNING R; Kiros R., 2015, ADV NEURAL INFORM PR, V28, P3294; Klein D, 2003, 41ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P423, DOI 10.3115/1075096.1075150; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Levi G, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P503, DOI 10.1145/2823327.2823333; Levi Gil, 2015, EMOTION RECOGNITION; Lim W., 2017, P INT C LEARN REPR; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu Dhruv Batra Jiasen, 2015, DEEPER LSTM NORMALIZ; Lu JS, 2016, ADV NEUR IN, V29; Malinowski M., 2014, ADV NEURAL INFORM PR, V27, P1682; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Pinheiro P. O., 2016, LEARNING REFINE OBJE; Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5; Rothe R, 2018, INT J COMPUT VISION, V126, P144, DOI 10.1007/s11263-016-0940-3; Rothe Rasmus, 2016, DEEP EXPECTATION REA; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003; Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499; SPELKE ES, 1992, PSYCHOL REV, V99, P605, DOI 10.1037/0033-295X.99.4.605; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tenenbaum JB, 1997, ADV NEUR IN, V9, P662; Xiong CM, 2016, PR MACH LEARN RES, V48; Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009; Zhou Bolei, 2017, PLACES 10 MILLION IM; Zhu YK, 2016, PROC CVPR IEEE, P4995, DOI 10.1109/CVPR.2016.540	49	24	26	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400053
C	Mishchuk, A; Mishkin, D; Radenovic, F; Matas, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Mishchuk, Anastasiya; Mishkin, Dmytro; Radenovic, Filip; Matas, Jiri			Working hard to know your neighbor's margins: Local descriptor learning loss	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				GEOMETRY; SCALE	We introduce a loss for metric learning, which is inspired by the Lowe's matching criterion for SIFT. We show that the proposed loss, that maximizes the distance between the closest positive and closest negative example in the batch, is better than complex regularization methods; it works well for both shallow and deep convolution network architectures. Applying the novel loss to the L2Net CNN architecture results in a compact descriptor named HardNet. It has the same dimensionality as SIFT (128) and shows state-of-art performance in wide baseline stereo, patch verification and instance retrieval benchmarks.	[Mishchuk, Anastasiya] Szkocka Res Grp, Lvov, Ukraine; [Mishkin, Dmytro; Radenovic, Filip; Matas, Jiri] CTU, Visual Recognit Grp, Prague, Czech Republic	Czech Technical University Prague	Mishchuk, A (corresponding author), Szkocka Res Grp, Lvov, Ukraine.	anastasiya.mishchuk@gmail.com; mishkdmy@cmp.felk.cvut.cz; filip.radenovic@cmp.felk.cvut.cz; matas@cmp.felk.cvut.cz	Mishkin, Dmytro/GSI-5311-2022; , Matas/AAW-3282-2020; Jeong, Yongwook/N-7413-2016	Mishkin, Dmytro/0000-0001-8205-6718; 	Czech Science Foundation Project GACR [P103/12/G084]; Austrian Ministry for Transport, Innovation and Technology; Federal Ministry of Science, Research and Economy; Province of Upper Austria in the frame of the COMET center; CTU student grant [SGS17/185/OHK3/3T/13]; MSMT ERC-CZ grant [LL1303]; Szkocka Research Group Grant	Czech Science Foundation Project GACR; Austrian Ministry for Transport, Innovation and Technology; Federal Ministry of Science, Research and Economy; Province of Upper Austria in the frame of the COMET center; CTU student grant; MSMT ERC-CZ grant; Szkocka Research Group Grant	The authors were supported by the Czech Science Foundation Project GACR P103/12/G084, the Austrian Ministry for Transport, Innovation and Technology, the Federal Ministry of Science, Research and Economy, and the Province of Upper Austria in the frame of the COMET center, the CTU student grant SGS17/185/OHK3/3T/13, and the MSMT LL1303 ERC-CZ grant. Anastasiya Mishchuk was supported by the Szkocka Research Group Grant.	Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018; Balntas V., 2016, PROCEDINGS BRIT MACH; Balntas Vassileios, 2017, C COMP VIS PATT REC; Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3; Bursuc Andrei, 2015, ACM INT C MULT RETR; Choy Christopher, 2016, ADV NEURAL INFORM PR, V6; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; Dong JM, 2015, PROC CVPR IEEE, P5097, DOI 10.1109/CVPR.2015.7299145; Fernando B, 2015, COMPUT VIS IMAGE UND, V139, P21, DOI 10.1016/j.cviu.2015.05.016; Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948; Hauagge DC, 2012, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2012.6247677; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Jegou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609; Jegou H, 2010, IEEE T PATTERN ANAL, V32, P2, DOI 10.1109/TPAMI.2008.285; Kendall A., 2015, INT C COMP VIS ICCV; Kumar BGV, 2016, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2016.581; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Matas J., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P384; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Mikulik A, 2013, INT J COMPUT VISION, V103, P163, DOI 10.1007/s11263-012-0600-1; Mishkin D., 2015, ARXIV150406603; Mishkin D, 2015, COMPUT VIS IMAGE UND, V141, P81, DOI 10.1016/j.cviu.2015.08.005; Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331; Nair V., 2010, ICML, P807; Perd'och M, 2009, PROC CVPR IEEE, P9, DOI 10.1109/CVPRW.2009.5206529; Radenovic F, 2016, LECT NOTES COMPUT SC, V9905, P3, DOI 10.1007/978-3-319-46448-0_1; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Schonberger J. L., 2017, C COMP VIS PATT REC; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Schonberger JL, 2015, PROC CVPR IEEE, P5126, DOI 10.1109/CVPR.2015.7299148; Simo-Serra E, 2015, IEEE I CONF COMP VIS, P118, DOI 10.1109/ICCV.2015.22; Simonyan K, 2012, LECT NOTES COMPUT SC, V7572, P243, DOI 10.1007/978-3-642-33718-5_18; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Sivic J., 2008, COMPUTER VISION PATT, P1, DOI 10.1109/CVPR.2008.4587635; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Tian B. F. Y., 2017, C COMP VIS PATT REC; Tolias G, 2014, PATTERN RECOGN, V47, P3466, DOI 10.1016/j.patcog.2014.04.007; Wilson DR, 2003, NEURAL NETWORKS, V16, P1429, DOI 10.1016/S0893-6080(03)00138-2; Yang GH, 2007, IEEE T PATTERN ANAL, V29, P1973, DOI [10.1109/TPAMI.2007.1116, 10.1109/TPAMl.2007.1116.]; Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28; Zagoruyko S., 2015, C COMP VIS PATT REC; Zitnick CL, 2011, IEEE I CONF COMP VIS, P359, DOI 10.1109/ICCV.2011.6126263	45	24	24	3	19	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404087
C	Wen, Z; Kveton, B; Valko, M; Vaswani, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wen, Zheng; Kveton, Branislav; Valko, Michal; Vaswani, Sharan			Online Influence Maximization under Independent Cascade Model with Semi-Bandit Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We study the online influence maximization problem in social networks under the independent cascade model. Specifically, we aim to learn the set of "best influencers" in a social network online while repeatedly interacting with it. We address the challenges of (i) combinatorial action space, since the number of feasible influencer sets grows exponentially with the maximum number of influencers, and (ii) limited feedback, since only the influenced portion of the network is observed. Under a stochastic semi-bandit feedback, we propose and analyze IMLinUCB, a computationally efficient UCB-based algorithm. Our bounds on the cumulative regret are polynomial in all quantities of interest, achieve near-optimal dependence on the number of interactions and reflect the topology of the network and the activation probabilities of its edges, thereby giving insights on the problem complexity. To the best of our knowledge, these are the first such results. Our experiments show that in several representative graph topologies, the regret of IMLinUCB scales as suggested by our upper bounds. IMLinUCB permits linear generalization and thus is both statistically and computationally suitable for large-scale problems. Our experiments also show that IMLinUCB with linear generalization can lead to low regret in real-world online influence maximization.	[Wen, Zheng; Kveton, Branislav] Adobe Res, San Jose, CA 95110 USA; [Valko, Michal] INRIA Lille Nord Europe, SequeL Team, Lille, France; [Vaswani, Sharan] Univ British Columbia, Vancouver, BC, Canada	Adobe Systems Inc.; University of British Columbia	Wen, Z (corresponding author), Adobe Res, San Jose, CA 95110 USA.	zwen@adobe.com; kveton@adobe.com; michal.valko@inria.fr; sharanv@cs.ubc.ca	Jeong, Yongwook/N-7413-2016		French Ministry of Higher Education and Research; Nord-Pas-de-Calais Regional Council; French National Research Agency project ExTra-Learn [ANR-14-CE24-0010-01]; French National Research Agency project BoB [ANR-16-CE23-0003]	French Ministry of Higher Education and Research; Nord-Pas-de-Calais Regional Council(Region Hauts-de-France); French National Research Agency project ExTra-Learn(French National Research Agency (ANR)); French National Research Agency project BoB(French National Research Agency (ANR))	The research presented was supported by French Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council and French National Research Agency projects ExTra-Learn (n.ANR-14-CE24-0010-01) and BoB (n.ANR-16-CE23-0003). We would also like to thank Dr. Wei Chen and Mr. Qinshi Wang for pointing out a mistake in an earlier version of this paper.	Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Bao Yixin, 2016, INT S QUAL SERV APR; Barbieri N, 2013, KNOWL INF SYST, V37, P555, DOI 10.1007/s10115-013-0646-6; Bnaya Z., HUMAN, V2, P84; Carpentier Alexandra, 2016, INT C ART INT STAT; Chen W., 2013, ICML 2013, P151; Chen W, 2016, J MACH LEARN RES, V17; Chen Wei, 2010, KNOWLEDGE DISCOVERY; Dani V., 2008, 21 ANN C LEARN THEOR; Easley D., 2010, NETWORKS CROWDS MARK; Fang M, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1106, DOI 10.1145/2623330.2623672; Farajtabar Mehrdad, 2016, NEURAL INFORM PROCES; Goyal A., 2010, P WSDM 10, P241, DOI [10.1145/1718487.1718518, DOI 10.1145/1718487.1718518]; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Kempe D., 2003, ACM SIGKDD INT C KNO, P137, DOI DOI 10.1145/956750.956769; Kveton B., 2015, NIPS, V28, P1450; Kveton B, 2015, JMLR WORKSH CONF PRO, V38, P535; Kveton B, 2015, PR MACH LEARN RES, V37, P767; Lagree Paul, 2017, INT C DAT MIN; Lei Siyu, 2015, KNOWLEDGE DISCOVERY; Li Yanhua, 2013, ACM INT C WEB SEARCH; Netrapalli Praneeth, 2012, Performance Evaluation Review, V40, P211, DOI 10.1145/2318857.2254783; Rodriguez M Gomez, 2012, INT C MACH LEARN; Saito K, 2008, LECT NOTES ARTIF INT, V5179, P67, DOI 10.1007/978-3-540-85567-5_9; Singla Adish, 2015, INT JOINT C ART INT; Tang Youze, 2014, INFLUENCE MAXIMIZATI; Valko M., 2016, BANDITS GRAPHS STRUC; Vaswani S, 2017, PR MACH LEARN RES, V70; Vaswani Sharan, 2015, NIPS WORKSH NETW SOC, V2015; Vaswani Sharan, 2016, TECHNICAL REPORT; Wang Qinshi, 2017, NEURAL INFORM PROCES; Wen Z, 2015, PR MACH LEARN RES, V37, P1113; Zong Shi, 2016, UNCERTAINTY ARTIFICI	33	24	24	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403009
C	Pu, YC; Gan, Z; Henao, R; Yuan, X; Li, CY; Stevens, A; Carin, L		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Pu, Yunchen; Gan, Zhe; Henao, Ricardo; Yuan, Xin; Li, Chunyuan; Stevens, Andrew; Carin, Lawrence			Variational Autoencoder for Deep Learning of Images, Labels and Captions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					A novel variational autoencoder is developed to model images, as well as associated labels or captions. The Deep Generative Deconvolutional Network (DGDN) is used as a decoder of the latent image features, and a deep Convolutional Neural Network (CNN) is used as an image encoder; the CNN is used to approximate a distribution for the latent DGDN features/code. The latent code is also linked to generative models for labels (Bayesian support vector machine) or captions (recurrent neural network). When predicting a label/caption for a new image at test, averaging is performed across the distribution of latent codes; this is computationally efficient as a consequence of the learned CNN-based encoder. Since the framework is capable of modeling the image in the presence/absence of associated labels/captions, a new semi-supervised setting is manifested for CNN learning with images; the framework even allows unsupervised CNN learning, based on images alone.	[Pu, Yunchen; Gan, Zhe; Henao, Ricardo; Li, Chunyuan; Stevens, Andrew; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA; [Yuan, Xin] Nokia Bell Labs, Murray Hill, NJ USA	Duke University; Nokia Corporation; Nokia Bell Labs	Pu, YC (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.	yp42@duke.edu; zg27@duke.edu; r.henao@duke.edu; xyuan@bell-labs.com; cl319@duke.edu; ajs104@duke.edu; lcarin@duke.edu	Li, Chunyuan/AAG-1303-2020		ARO; DARPA; DOE; NGA; ONR; NSF	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NGA; ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	This research was supported in part by ARO, DARPA, DOE, NGA, ONR and NSF. The Titan X used in this work was donated by the NVIDIA Corporation.	[Anonymous], 2015, NIPS; [Anonymous], 2014, ICLR; Bastien F., 2012, NIPS WORKSH; Cho Kyunghyun, 2014, EMNLP; Dosovitskiy A., 2015, CVPR; Fang H., 2015, C COMP VIS PATT REC; Griffin G., 2007, 120 CAL I TECHN; Henao R, 2014, NIPS; Hochreiter S, 1997, NEURAL COMPUTATION; Hodosh M., 2013, J ARTIFICIAL INTELLI; Kingma D.P., 2015, INT C LEARN REPR, P1; Kingma D. P., 2014, NIPS; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Krizhevsky Alex, 2009, LEARNING MULTIPLE LA, P6; Kulkarni T. D., 2015, NIPS; Lavie A., 2005, ACL WORKSH; LeCun Y., 1989, NEURAL COMPUTATION; Li C., 2015, NIPS; Li F., 2007, COMPUTER VISION IMAG; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Mnih A., 2014, ICML; Papineni K., 2002, T ASS COMPUTATIONAL; Polson N. G., 2011, BAYES ANAL; Pu  Y., 2015, ICLR WORKSH; Pu Y., 2016, AISTATS; Simonyan Karen, 2015, INT C LEARN REPR; Sutskever I., 2014, NEURIPS; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Vedantam R., 2015, CVPR; Vinyals O., 2015, CVPR; Wu Q., 2016, P CVPR; Xu K., 2015, ICML; Young P., 2014, T ACL, V2	34	24	24	1	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704052
C	Titsias, MK; Lazaro-Gredilla, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Titsias, Michalis K.; Lazaro-Gredilla, Miguel			Local Expectation Gradients for Black Box Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We introduce local expectation gradients which is a general purpose stochastic variational inference algorithm for constructing stochastic gradients by sampling from the variational distribution. This algorithm divides the problem of estimating the stochastic gradients over multiple variational parameters into smaller sub-tasks so that each sub-task explores intelligently the most relevant part of the variational distribution. This is achieved by performing an exact expectation over the single random variable that most correlates with the variational parameter of interest resulting in a Rao-Blackwellized estimate that has low variance. Our method works efficiently for both continuous and discrete random variables. Furthermore, the proposed algorithm has interesting similarities with Gibbs sampling but at the same time, unlike Gibbs sampling, can be trivially parallelized.	[Titsias, Michalis K.] Athens Univ Econ & Business, Athens, Greece; [Lazaro-Gredilla, Miguel] Vicarious, San Francisco, CA USA	Athens University of Economics & Business	Titsias, MK (corresponding author), Athens Univ Econ & Business, Athens, Greece.	mtitsias@aueb.gr; miguel@vicarious.com						Bishop CM, 2006, PATTERN RECOGNITION; Bornschein Jrg, 2014, REWEIGHTED WAKE SLEE, P1; GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kucukelbir A, 2015, ADV NEUR IN, V28; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Paisley J., 2012, ARXIV12066430; Peters J., 2006, P IEEE INT C INT ROB; Ranganath R., 2014, ARTIFICIAL INTELLIGE, P814; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Salimans Tim, 2014, USING CONTROL VARIAT; Titsias Michalis K., 2014, 31 INT C MACH LEARN; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	21	24	24	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100046
C	Ott, L; Pang, L; Ramos, F; Chawla, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Ott, Lionel; Pang, Linsey; Ramos, Fabio; Chawla, Sanjay			On Integrated Clustering and Outlier Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We model the joint clustering and outlier detection problem using an extension of the facility location formulation. The advantages of combining clustering and outlier selection include: (i) the resulting clusters tend to be compact and semantically coherent (ii) the clusters are more robust against data perturbations and (iii) the outliers are contextualised by the clusters and more interpretable. We provide a practical subgradient-based algorithm for the problem and also study the theoretical properties of algorithm in terms of approximation and convergence. Extensive evaluation on synthetic and real data sets attest to both the quality and scalability of our proposed method.	[Ott, Lionel; Pang, Linsey; Ramos, Fabio; Chawla, Sanjay] Univ Sydney, Sydney, NSW, Australia	University of Sydney	Ott, L (corresponding author), Univ Sydney, Sydney, NSW, Australia.	lott4241@uni.sydney.edu.au; qlinsey@it.usyd.edu.au; fabio.ramos@sydney.edu.au; sanjay.chawla@sydney.edu.au						Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Bertsimas D., 2005, OPTIMIZATION INTEGER; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Breunig M., 2000, INT C MAN DAT; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chandola V, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541882; Charikar M, 2001, SIAM PROC S, P642; Chawla S., 2013, SIAM INT C DATA MINI; Chen K., 2008, P ACM SIAM S DISCR A; Croux C., 1996, P COMP STAT; Frey B., 2007, MULTI DATABASE RETRI, Vvol. 315, ppp, DOI [DOI 10.1126/SCIENCE.1136800, 10.1126/science.1136800]; Huber P., 2008, ROBUST STAT; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Pelillo M., 2009, P ADV NEUR INF PROC; Rosenberg A., 2007, P 2007 JOINT C EMP M, P410; Rousseeuw PJ, 1999, TECHNOMETRICS, V41, P212, DOI 10.2307/1270566; Wright J., 2009, P ADV NEUR INF PROC	17	24	25	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100085
C	Singh, S; Kearns, M; Litman, D; Walker, M		Solla, SA; Leen, TK; Muller, KR		Singh, S; Kearns, M; Litman, D; Walker, M			Reinforcement learning for spoken dialogue systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Recently, a number of authors have proposed treating dialogue systems as Markov decision processes (MDPs). However, the practical application of MDP algorithms to dialogue systems faces a number of severe technical challenges. We have built a general software tool (RLDS, for Reinforcement Learning for Dialogue Systems) based on the MDP framework, and have applied it to dialogue corpora gathered from two dialogue systems built at AT&T Labs. Our experiments demonstrate that RLDS holds promise as a tool for "browsing" and understanding correlations in complex, temporally dependent dialogue corpora.	AT&T Labs, Murray Hill, NJ 07974 USA	AT&T	Singh, S (corresponding author), AT&T Labs, Murray Hill, NJ 07974 USA.	baveja@research.att.com; mkearns@research.att.com; diane@research.att.com; walker@research.att.com						Biermann Alan W., 1996, INT S SPOK DIAL, P97; Gorin AL, 1996, THIRD IEEE WORKSHOP ON INTERACTIVE VOICE TECHNOLOGY FOR TELECOMMUNICATIONS APPLICATIONS - IVTTA-96, PROCEEDINGS, P57, DOI 10.1109/IVTTA.1996.552741; Levin E., 1997, P IEEE WORKSH AUT SP; LITMAN DJ, 1999, P 7 INT C US MOD; SINGH S, UNPUB; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; WALKER MA, 1998, P 36 ANN M ASS COMP, P1345	7	24	24	0	4	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						956	962						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700135
C	Vasconcelos, N; Lippman, A		Solla, SA; Leen, TK; Muller, KR		Vasconcelos, N; Lippman, A			Learning from user feedback in image retrieval systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We formulate the problem of retrieving images from visual databases as a problem of Bayesian inference. This leads to natural and effective solutions for two of the most challenging issues in the design of a retrieval system: providing support for region-based queries without requiring prior image segmentation, and accounting for user-feedback during a retrieval session. We present a new learning algorithm that relies on belief propagation to account for both positive and negative examples of the user's interests.	MIT, Media Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Vasconcelos, N (corresponding author), MIT, Media Lab, 20 Ames St,E15-354, Cambridge, MA 02139 USA.	nuno@media.mit.edu; lip@media.mit.edu		Vasconcelos, Nuno/0000-0002-9024-4302				Belongie S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P675, DOI 10.1109/ICCV.1998.710790; COX I, 1996, INT C PATT REC VIENN; Gelman A, 2013, BAYESIAN DATA ANAL, P16; Pentland A, 1996, INT J COMPUT VISION, V18, P233, DOI 10.1007/BF00123143	5	24	24	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						977	983						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700138
C	Hyvarinen, A; Hoyer, P; Oja, E		Kearns, MS; Solla, SA; Cohn, DA		Hyvarinen, A; Hoyer, P; Oja, E			Sparse code shrinkage: Denoising by nonlinear maximum likelihood estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				INDEPENDENT COMPONENT ANALYSIS	Sparse coding is a method for finding a representation of data in which each of the components of the representation is only rarely significantly active. Such a representation is closely related to redundancy reduction and independent component analysis, and has some neurophysiological plausibility. In this paper, we show how sparse coding can be used for denoising. Using maximum likelihood estimation of nongaussian variables corrupted by gaussian noise, we show how to apply a shrinkage nonlinearity on the components of sparse coding so as to reduce noise. Furthermore, we show how to choose the optimal sparse coding basis for denoising. Our method is closely related to the method of wavelet shrinkage, but has the important benefit over wavelet methods that both the features and the shrinkage parameters are estimated directly from the data.	Helsinki Univ Technol, Lab Comp & Informat Sci, FIN-02015 Helsinki, Finland	Aalto University	Hyvarinen, A (corresponding author), Helsinki Univ Technol, Lab Comp & Informat Sci, POB 5400, FIN-02015 Helsinki, Finland.							BARLOW HB, 1994, LARGE SCALE NEURONAL; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; DONOHO DL, 1995, J ROY STAT SOC B MET, V57, P301; HUBER PJ, 1985, ANN STAT, V13, P435, DOI 10.1214/aos/1176349519; Hyvarinen A, 1997, NEURAL COMPUT, V9, P1483, DOI 10.1162/neco.1997.9.7.1483; HYVARINEN A, 1998, A51 HELS U TECHN LAB; HYVARINEN A, 1997, P IEEE INT C AC SPEE, P3917; HYVARINEN A, 1998, IN PRESS APPL SPARSE; Lewicki MS, 1998, ADV NEUR IN, V10, P815; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0	10	24	24	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						473	479						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700067
C	Bishop, CM; Lawrence, N; Jaakkola, T; Jordan, MI		Jordan, MI; Kearns, MJ; Solla, SA		Bishop, CM; Lawrence, N; Jaakkola, T; Jordan, MI			Approximating posterior distributions in belief networks using mixtures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Exact inference in densely connected Bayesian networks is computationally intractable, and so there is considerable interest in developing effective approximation schemes. One approach which has been adopted is to bound the log likelihood using a mean-field approximating distribution. While this leads to a tractable algorithm, the mean field distribution is assumed to be factorial and hence unimodal. In this paper we demonstrate the feasibility of using a richer class of approximating distributions based on mixtures of mean field distributions. We derive an efficient algorithm for updating the mixture parameters and apply it to the problem of learning in sigmoid belief networks. Our results demonstrate a systematic improvement over simple mean field theory as the number of mixture components is increased.	Aston Univ, Dept Comp Sci & Appl Math, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England	Aston University	Bishop, CM (corresponding author), Aston Univ, Dept Comp Sci & Appl Math, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England.		Jordan, Michael I/C-5253-2013	Lawrence, Neil/0000-0001-9258-1030					0	24	24	1	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						416	422						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700059
C	Harrison, RR; Koch, C		Jordan, MI; Kearns, MJ; Solla, SA		Harrison, RR; Koch, C			An analog VLSI model of the fly elementary motion detector	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Flies are capable of rapidly detecting and integrating visual motion information in behaviorly-relevant ways. The first stage of visual motion processing in flies is a retinotopic array of functional units known as elementary motion detectors (EMDs). Several decades ago, Reichardt and colleagues developed a correlation-based model of motion detection that described the behavior of these neural circuits. We have implemented a variant of this model in a 2.0-mu m analog CMOS VLSI process. The result is a low-power, continuous-time analog circuit with integrated photoreceptors that responds to motion in real time. The responses of the circuit to drifting sinusoidal gratings qualitatively resemble the temporal frequency response, spatial frequency response, and direction selectivity of motion-sensitive neurons observed in insects. In addition to its possible engineering applications, the circuit could potentially be used as a building block for constructing hardware models of higher-level insect motion integration.	CALTECH, Computat & Neural Syst Program, Pasadena, CA 91125 USA	California Institute of Technology	Harrison, RR (corresponding author), CALTECH, Computat & Neural Syst Program, 139-74, Pasadena, CA 91125 USA.			Koch, Christof/0000-0001-6482-8067					0	24	26	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						880	886						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700124
C	Smyth, P; Wolpert, D		Jordan, MI; Kearns, MJ; Solla, SA		Smyth, P; Wolpert, D			Stacked density estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					In this paper, the technique of stacking, previously only used for supervised learning, is applied to unsupervised learning. Specifically, it is used for non-parametric multivariate density estimation, to combine finite mixture model and kernel density estimators. Experimental results on both simulated data and real world data sets clearly demonstrate that stacked density estimation outperforms other strategies such as choosing the single best model based on cross-validation, combining with uniform weights, and even the single best model chosen by "cheating" by looking at the data used for independent testing.	Univ Calif Irvine, Irvine, CA 92697 USA	University of California System; University of California Irvine	Smyth, P (corresponding author), Univ Calif Irvine, Irvine, CA 92697 USA.			Smyth, Padhraic/0000-0001-9971-8378					0	24	24	2	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						668	674						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700095
C	Xu, L		Touretzky, DS; Mozer, MC; Hasselmo, ME		Xu, L			A unified learning scheme: Bayesian-Kullback Ying-Yang machine	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						CHINESE UNIV HONG KONG,DEPT COMP SCI,HONG KONG,HONG KONG	Chinese University of Hong Kong			Xu, Lei/D-6781-2013	Xu, Lei/0000-0002-2752-1573					0	24	25	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						444	450						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00063
C	BRIDLE, JS; HEADING, AJR; MACKAY, DJC		MOODY, JE; HANSON, SJ; LIPPMANN, RP		BRIDLE, JS; HEADING, AJR; MACKAY, DJC			UNSUPERVISED CLASSIFIERS, MUTUAL INFORMATION AND PHANTOM TARGETS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	24	24	0	1	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1096	1101						6	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00135
C	WATROUS, RL; KUHN, GM		MOODY, JE; HANSON, SJ; LIPPMANN, RP		WATROUS, RL; KUHN, GM			INDUCTION OF FINITE-STATE AUTOMATA USING 2ND-ORDER RECURRENT NETWORKS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	24	24	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						309	316						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00038
C	Ahmed, K; Torresani, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ahmed, Karim; Torresani, Lorenzo			STAR-CAPS: Capsule Networks with Straight-Through Attentive Routing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Capsule networks have been shown to be powerful models for image classification, thanks to their ability to represent and capture viewpoint variations of an object. However, the high computational complexity of capsule networks that stems from the recurrent dynamic routing poses a major drawback making their use for large-scale image classification challenging. In this work, we propose STAR-CAPS a capsule-based network that exploits a straight-through attentive routing to address the drawbacks of capsule networks. By utilizing attention modules augmented by differentiable binary routers, the proposed mechanism estimates the routing coefficients between capsules without recurrence, as opposed to prior related work. Subsequently, the routers utilize straight-through estimators to make binary decisions to either connect or disconnect the route between capsules, allowing stable and faster performance. The experiments conducted on several image classification datasets, including MNIST, SmallNorb, CIFAR-10, CIFAR-100 and ImageNet show that STAR-CAPS outperforms the baseline capsule networks.	[Ahmed, Karim; Torresani, Lorenzo] Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA	Dartmouth College	Ahmed, K (corresponding author), Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA.	karim@cs.dartmouth.edu; LT@dartmouth.edu			NSF [CNS-120552]	NSF(National Science Foundation (NSF))	This work was funded in part by NSF award CNS-120552. We gratefully acknowledge NVIDIA and Facebook for the donation of GPUs used for portions of this work.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bengio Yoshua, 2013, ARXIV13083432; Burges, 1998, MNIST DATABASE HANDW; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Gumbel E. J., 1954, STAT THEORY EXTREME; Guo YH, 2019, PROC CVPR IEEE, P4800, DOI 10.1109/CVPR.2019.00494; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hinton Geoffrey E., 2018, INT C LEARN REPR; Jang E., 2017, ICLR; Kaiser L., 2017, ARXIV PREPRINT ARXIV; Kingma D.P, P 3 INT C LEARNING R; Krizhesvsky Alex, 2009, TECHNICAL REPORT; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 2004, PROC CVPR IEEE, P97; Li Hongyang, 2018, ECCV; Li Jian, 2019, ARXIV190403100; Lin Min, 2014, INT C LEARN REPR 201; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; Maddison Chris J, 2016, ARXIV161100712; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Sabour Sara, 2017, PROC 31 INT C NEURAL; Sifre Laurent, 2014, THESIS; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Veit A, 2018, LECT NOTES COMPUT SC, V11205, P3, DOI 10.1007/978-3-030-01246-5_1; Wu Felix, 2019, ICLR; Zhang N., 2018, EMNLP, DOI [10.18653/v1/D18-1120, DOI 10.18653/V1/D18-1120]; Zhang X, 2018, INT C LEARN REPR	30	23	23	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900066
C	Batty, E; Whiteway, MR; Saxena, S; Biderman, D; Abe, T; Musall, S; Gillis, W; Markowitz, JE; Churchland, A; Cunningham, J; Datta, SR; Linderman, SW; Paninski, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Batty, Eleanor; Whiteway, Matthew R.; Saxena, Shreya; Biderman, Dan; Abe, Taiga; Musall, Simon; Gillis, Winthrop; Markowitz, Jeffrey E.; Churchland, Anne; Cunningham, John; Datta, Sandeep Robert; Linderman, Scott W.; Paninski, Liam			BehaveNet: nonlinear embedding and Bayesian neural decoding of behavioral videos	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A fundamental goal of systems neuroscience is to understand the relationship between neural activity and behavior. Behavior has traditionally been characterized by low-dimensional, task-related variables such as movement speed or response times. More recently, there has been a growing interest in automated analysis of high-dimensional video data collected during experiments. Here we introduce a probabilistic framework for the analysis of behavioral video and neural activity. This framework provides tools for compression, segmentation, generation, and decoding of behavioral videos. Compression is performed using a convolutional autoencoder (CAE), which yields a low-dimensional continuous representation of behavior. We then use an autoregressive hidden Markov model (ARHMM) to segment the CAE representation into discrete "behavioral syllables." The resulting generative model can be used to simulate behavioral video data. Finally, based on this generative model, we develop a novel Bayesian decoding approach that takes in neural activity and outputs probabilistic estimates of the full-resolution behavioral video. We demonstrate this framework on two different experimental paradigms using distinct behavioral and neural recording technologies.	[Batty, Eleanor; Whiteway, Matthew R.; Saxena, Shreya; Biderman, Dan; Abe, Taiga; Cunningham, John; Paninski, Liam] Columbia Univ, New York, NY 10027 USA; [Musall, Simon; Churchland, Anne] Cold Spring Harbor, Long Isl City, NY USA; [Gillis, Winthrop; Markowitz, Jeffrey E.; Datta, Sandeep Robert] Harvard Med Sch, Boston, MA 02115 USA; [Linderman, Scott W.] Stanford Univ, Stanford, CA 94305 USA	Columbia University; Harvard University; Harvard Medical School; Stanford University	Batty, E (corresponding author), Columbia Univ, New York, NY 10027 USA.	erb2180@columbia.edu; m.whiteway@columbia.edu; ss5513@columbia.edu; db3236@columbia.edu; ta2507@columbia.edu; simon.musall@gmail.com; win.gillis@gmail.com; jeffrey_markowitz@hms.harvard.edu; churchland@cshl.edu; jpc2181@columbia.edu; srdatta@hms.harvard.edu; swl1@stanford.edu; liam@stat.columbia.edu	Musall, Simon/AAE-5335-2019; Markowitz, Jeffrey/GQP-3708-2022	Musall, Simon/0000-0002-9461-1042; Gillis, Winthrop/0000-0002-1659-8639; Linderman, Scott/0000-0002-3878-9073; Whiteway, Matthew/0000-0003-3756-1349; Datta, Sandeep/0000-0002-8068-3862	Simons Foundation; Gatsby Charitable Foundation; NSF NeuroNex Award [DBI-1707398]; NIH [5U19NS107613, 5U19NS104649, 1U19NS113201]	Simons Foundation; Gatsby Charitable Foundation; NSF NeuroNex Award; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	We thank N. Steinmetz, M. Carandini, and K. Harris for generously making their data publicly available. This work was supported by the Simons Foundation and the Gatsby Charitable Foundation, by NSF NeuroNex Award DBI-1707398, and by NIH awards 5U19NS107613, 5U19NS104649, and 1U19NS113201.	Ainsworth Samuel, 2018, CORR; Akbari H, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-37359-z; Anumanchipalli GK, 2019, NATURE, V568, P493, DOI 10.1038/s41586-019-1119-1; Berman GJ, 2018, BMC BIOL, V16, DOI 10.1186/s12915-018-0494-7; Berman GJ, 2014, J R SOC INTERFACE, V11, DOI 10.1098/rsif.2014.0672; Buchanan E. K., 2017, WORKSH WORMS NEUR IN; Burkhart Michael C, 2016, ARXIV160806622; Calhoun AJ, 2017, CURR OPIN NEUROBIOL, V46, P90, DOI 10.1016/j.conb.2017.08.006; Churchland Anne K, 2019, SINGLE TRIAL NEURAL, DOI [10.14224/1.38599, DOI 10.14224/1.38599]; Costa AC, 2019, P NATL ACAD SCI USA, V116, P1501, DOI 10.1073/pnas.1813476116; Gao YJ, 2016, ADV NEUR IN, V29; Glaser J. I., 2017, MACHINE LEARNING NEU; Golub Matthew D., 2019, COMPUTATIONAL SYSTEM; Gomez-Marin A, 2014, NAT NEUROSCI, V17, P1455, DOI 10.1038/nn.3812; Graving Jacob M., 2019, BIORXIV; Gunel Semih, 2019, BIORXIV; Jun JJ, 2017, NATURE, V551, P232, DOI 10.1038/nature24636; Kingma D.P, P 3 INT C LEARNING R; Krakauer JW, 2017, NEURON, V93, P480, DOI 10.1016/j.neuron.2016.12.041; Linderman S., 2019, BIORXIV, DOI 10.1101/621540; Linderman Scott W., 2017, P 20 INT C ART INT S; Markowitz JE, 2018, CELL, V174, P44, DOI 10.1016/j.cell.2018.04.019; Mathis A., 2018, TECHNICAL REPORT; Musall S, 2018, BIORXIV, DOI [10.1101/308288, DOI 10.1101/308288]; Nassar J, 2019, INT C LEARN REPR ICL; Orger MB, 2017, ANNU REV NEUROSCI, V40, P125, DOI 10.1146/annurev-neuro-071714-033857; Pandarinath C, 2018, NAT METHODS, V15, P805, DOI 10.1038/s41592-018-0109-9; Paninski L, 2018, CURR OPIN NEUROBIOL, V50, P232, DOI 10.1016/j.conb.2018.04.007; Parthasarathy N, 2017, ADV NEURAL INF PROCE, V30, P6434; Pereira TD, 2019, NAT METHODS, V16, P117, DOI 10.1038/s41592-018-0234-5; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Sani Omid G, 2019, BIORXIV; Saxena S., 2019, BIORXIV; Saxena S, 2019, CURR OPIN NEUROBIOL, V55, P103, DOI 10.1016/j.conb.2019.02.002; Serruya MD, 2002, NATURE, V416, P141, DOI 10.1038/416141a; Sharma Anuj, 2018, ADV NEURAL INFORM PR; Stephens GJ, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000028; Stringer C, 2019, SCIENCE, V364, P255, DOI 10.1126/science.aav7893; Sussillo D, 2012, J NEURAL ENG, V9, DOI 10.1088/1741-2560/9/2/026027; Wei Ziqiang, 2019, NATURE COMMUNICATION, V10; Wiltschko AB, 2015, NEURON, V88, P1121, DOI 10.1016/j.neuron.2015.11.031; Wulsin D., 2013, ICML, P356	45	23	23	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907037
C	Cohen, TS; Geiger, M; Weiler, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cohen, Taco S.; Geiger, Mario; Weiler, Maurice			A General Theory of Equivariant CNNs on Homogeneous Spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present a general theory of Group equivariant Convolutional Neural Networks (G-CNNs) on homogeneous spaces such as Euclidean space and the sphere. Feature maps in these networks represent fields on a homogeneous base space, and layers are equivariant maps between spaces of fields. The theory enables a systematic classification of all existing G-CNNs in terms of their symmetry group, base space, and field type. We also consider a fundamental question: what is the most general kind of equivariant linear map between feature spaces (fields) of given types? Following Mackey, we show that such maps correspond one-to-one with convolutions using equivariant kernels, and characterize the space of such kernels.	[Cohen, Taco S.] Qualcomm Technol Netherlands BV, Qualcomm AI Res, Nijmegen, Netherlands; [Geiger, Mario] Ecole Polytech Fed Lausanne, PCSL Res Grp, Lausanne, Switzerland; [Weiler, Maurice] Univ Amsterdam, QUVA Lab, Amsterdam, Netherlands	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; University of Amsterdam	Cohen, TS (corresponding author), Qualcomm Technol Netherlands BV, Qualcomm AI Res, Nijmegen, Netherlands.	tacos@qti.qualcomm.com; mario.geiger@epfl.ch; m.weiler@uva.nl						Anderson B., 2019, ADV NEURAL INFORM PR, V2019, P14510; [Anonymous], 2017, IEEE SIGNAL PROCESSI; Bekkers Erik J, 2018, MED IMAGE COMPUTING; Cohen TS, 2019, PR MACH LEARN RES, V97; Cohen TS, 2016, PR MACH LEARN RES, V48; Cohen Taco S., 2017, 5 INT C LEARN REPR I, P2; Cohen Taco S, 2017, ICML WORKSH PRINC AP; Cohen Taco S, 2018, ARXIV180310743CSLG; Cohen Taco S, 2018, INT C LEANR REPR ICL; Dieleman S, 2016, PR MACH LEARN RES, V48; Esteves C, 2018, LECT NOTES COMPUT SC, V11217, P54, DOI 10.1007/978-3-030-01261-8_4; EWorrall Daniel, 2019, INT C MACH LEARN ICM; Folland GB., 2015, COURSE ABSTRACT HARM; Gens R., 2014, NIPS; Ghosh Rohan, 2019, ARXIV190603861; Hamilton Mark, 2017, MATH GAUGE THEORY AP; Hinton Geoffrey E., 2018, INT C LEARN REPR; Hoogeboom E., 2018, INT C LEARN REPR; Jiang C.M., 2019, PROC INT C LEARN REP, P1; Keriven Nicolas, 2019, NEURAL INFORM PROCES; Koenderink J, 2008, J MATH IMAGING VIS, V31, P171, DOI 10.1007/s10851-008-0076-3; KOENDERINK JJ, 1990, PSYCHOL RES-PSYCH FO, V52, P122, DOI 10.1007/BF00877519; Kondor R, 2018, PR MACH LEARN RES, V80; Kondor Risi, 2018, ARXIV180102144CSLG; Kondor Risi, 2018, ARXIV180301588CSLG; Kondor Risi, 2018, C NEUR INF PROC SYST; LeCun Y., 1989, ADV NEURAL INFORM PR, V2; Mackey G.W., 1968, INDUCED REPRESENTATI; MACKEY GW, 1952, ANN MATH, V55, P101, DOI 10.2307/1969423; MACKEY GW, 1953, ANN MATH, V58, P193, DOI 10.2307/1969786; Mackey GW, 1951, AM J MATH, V73, P576, DOI 10.2307/2372309; Mallat S, 2016, PHILOS T R SOC A, V374, DOI 10.1098/rsta.2015.0203; Marcos D, 2017, IEEE I CONF COMP VIS, P5058, DOI 10.1109/ICCV.2017.540; Maron H., 2019, P INT C LEARN REPR, P1; Maron H, 2019, PR MACH LEARN RES, V97; Marsh Adam, 2016, GAUGE THEORIES FIBER; Olah C., 2014, GROUPS GROUP CONVOLU; Oyallon E, 2015, PROC CVPR IEEE, P2865, DOI 10.1109/CVPR.2015.7298904; Perraudin Nathanael, 2018, ASTRONOMY COMPUTING, V27; Petitot J, 2003, J PHYSIOL-PARIS, V97, P265, DOI 10.1016/j.jphysparis.2003.10.010; Ravanbakhsh S, 2017, PR MACH LEARN RES, V70; Sabour Sara, 2017, PROC 31 INT C NEURAL; Segol Nimrod, 2019, ARXIV191002421CSSTAT; Sharpe R. W., 1997, DIFFERENTIAL GEOMETR, V166; Sifre L, 2013, PROC CVPR IEEE, P1233, DOI 10.1109/CVPR.2013.163; Sosnovik Ivan., 2019, SCALE EQUIVARIANT ST; Thomas Nathaniel, 2018, ARXIV180208219; Tolli F., 2009, J MATH SCI, V156, P11; Weiler M, 2018, ADV NEUR IN, V31; Weiler Maurice, 2018, INT C COMP VIS PATT; Weiler Maurice, 2019, ADV NEURAL INFORM PR, V32; Winkels M., 2018, ARXIV PREPRINT ARXIV; Winkens Jim, 2018, INT C MED IM DEEP LE; Worrall D, 2018, LECT NOTES COMPUT SC, V11209, P585, DOI 10.1007/978-3-030-01228-1_35; Worrall DE, 2017, PROC CVPR IEEE, P7168, DOI 10.1109/CVPR.2017.758; Zhou Y., 2017, ARXIV171106396	57	23	23	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900070
C	Mott, A; Zoran, D; Chrzanowski, M; Wierstra, D; Rezende, DJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mott, Alex; Zoran, Daniel; Chrzanowski, Mike; Wierstra, Daan; Rezende, Danilo J.			Towards Interpretable Reinforcement Learning Using Attention Augmented Agents	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Inspired by recent work in attention models for image captioning and question answering, we present a soft attention model for the reinforcement learning domain. This model uses a soft, top-down attention mechanism to create a bottleneck in the agent, forcing it to focus on task-relevant information by sequentially querying its view of the environment. The output of the attention mechanism allows direct observation of the information used by the agent to select its actions, enabling easier interpretation of this model than of traditional models. We analyze different strategies that the agents learn and show that a handful of strategies arise repeatedly across different games. We also show that the model learns to query separately about space and content ("where" vs. "what"). We demonstrate that an agent using this mechanism can achieve performance competitive with state-of-the-art models on ATARI tasks while still being interpretable.	[Mott, Alex; Zoran, Daniel; Chrzanowski, Mike; Wierstra, Daan; Rezende, Danilo J.] DeepMind, London, England		Mott, A (corresponding author), DeepMind, London, England.	alexmott@google.com; danielzoran@google.com; chrzanowski@google.com; wierstra@google.com; danilor@google.com						Ablavatski A, 2017, IEEE WINT CONF APPL, P971, DOI 10.1109/WACV.2017.113; Ba J., 2014, ARXIV; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Chung Minki, 2018, ARXIV180410844; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Espeholt L, 2018, PR MACH LEARN RES, V80; Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476; Greydanus Sam, 2017, CORR; Hermann K. M., 2015, ADV NEURAL INFORM PR, V28, P1693; Hudson Drew A., 2018, ABS180303067; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jaderberg Max, 2017, ABS171109846 CORR; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Kosiorek AR, 2017, ADV NEUR IN, V30; Kosiorek Adam R, 2018, ARXIV180601794; Li XL, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2208; Malinowski M, 2018, LECT NOTES COMPUT SC, V11210, P3, DOI 10.1007/978-3-030-01231-1_1; Manchin Anthony, 2019, ABS190403367 ARXIV; Mnih V, 2014, ADV NEUR IN, V27; Parmar Niki, 2018, ARXIV180205751, P2; Shan Mo, 2017, ARXIV170702069; Shen SS, 2016, INTERSPEECH, P2716, DOI 10.21437/Interspeech.2016-1359; Sorokin I., 2015, CORR; Sukhbaatar S, 2015, ADV NEUR IN, V28; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wang Z., 2015, ARXIV150600327; Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang Z, 2016, P 2016 C N AM CHAPTE, P1480; Zahavy T., 2016, CORR; Zhang B.-T, 2017, WORKSH 31 AAAI C ART; Zhang Han, 2018, ARXIV180508318; Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557	35	23	23	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904004
C	Nayman, N; Noy, A; Ridnik, T; Friedman, I; Jin, R; Zelnik-Manor, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Nayman, Niv; Noy, Asaf; Ridnik, Tal; Friedman, Itamar; Jin, Rong; Zelnik-Manor, Lihi			XNAS: Neural Architecture Search with Expert Advice	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GRADIENT DESCENT; ALGORITHMS; MIXTURES	This paper introduces a novel optimization method for differential neural architecture search, based on the theory of prediction with expert advice. Its optimization criterion is well fitted for an architecture-selection, i.e., it minimizes the regret incurred by a sub-optimal selection of operations. Unlike previous search relaxations, that require hard pruning of architectures, our method is designed to dynamically wipe out inferior architectures and enhance superior ones. It achieves an optimal worst-case regret bound and suggests the use of multiple learning-rates, based on the amount of information carried by the backward gradients. Experiments show that our algorithm achieves a strong performance over several image classification datasets. Specifically, it obtains an error rate of 1.6% for CIFAR-10, 23.9% for ImageNet under mobile settings, and achieves state-of-the-art results on three additional datasets.	[Nayman, Niv; Noy, Asaf; Ridnik, Tal; Friedman, Itamar; Jin, Rong; Zelnik-Manor, Lihi] Alibaba Grp, Machine Intelligence Technol, Hangzhou, Peoples R China	Alibaba Group	Nayman, N (corresponding author), Alibaba Grp, Machine Intelligence Technol, Hangzhou, Peoples R China.	niv.nayman@alibaba-inc.com; asaf.noy@alibaba-inc.com; tal.ridnik@alibaba-inc.com; itamar.friedman@alibaba-inc.com; jinrong.jr@alibaba-inc.com; lihi.zelnik@alibaba-inc.com						Aljundi R, 2017, PROC CVPR IEEE, P7120, DOI 10.1109/CVPR.2017.753; Cai H., 2018, ARXIV PREPRINT ARXIV; Casale Francesco Paolo, 2019, PROBABILISTIC NEURAL; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chen K, 1999, NEURAL NETWORKS, V12, P1229, DOI 10.1016/S0893-6080(99)00043-X; Chen X, 2019, IEEE I CONF COMP VIS, P1294, DOI 10.1109/ICCV.2019.00138; Cubuk Ekin D., 2018, ARXIV180509501; Darlow Luke Nicholas, 2018, CORR; Eigen D., 2013, LEARNING FACTORED RE; Garmash Ekaterina, 2016, P COLING 2016 26 INT, P1409; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Haussler D., 1995, Computational Learning Theory. Second European Conference, EuroCOLT '95. Proceedings, P69; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; HOEFFDING W, 1953, ANN MATH STAT, V24, P127, DOI 10.1214/aoms/1177729092; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Hundt Andrew, 2019, ABS190309900 CORR; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; Jund P, 2016, ARXIV161105799; Kingma D.P, P 3 INT C LEARNING R; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Larsson G., 2016, ARXIV PREPRINT ARXIV; Li Liam, 2019, UAI; Liu C., 2018, P EUR C COMP VIS ECC, P19, DOI DOI 10.1007/978-3-030-01246-5_2; Liu Hanxiao, 2018, ARXIV180609055; Loshchilov I., 2016, ARXIV; Luo RQ, 2018, ADV NEUR IN, V31; Mullapudi RT, 2018, PROC CVPR IEEE, P8080, DOI 10.1109/CVPR.2018.00843; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Pham H, 2018, PR MACH LEARN RES, V80; Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6; Rasmussen CE, 2002, ADV NEUR IN, V14, P881; Real Esteban, 2018, INT C MACH LEARN ICM; Reddi Sashank J, 2019, ARXIV190409237, DOI DOI 10.48550/ARXIV.1904.09237; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Smith LN, 2017, IEEE WINT CONF APPL, P464, DOI 10.1109/WACV.2017.58; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; van Erven T., 2014, COLT, P949; Wang Xin, 2018, ARXIV180601531; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu B., 2018, FBNET HARDWARE AWARE; Xiao H., 2017, FASHION MNIST NOVEL; Xie SN, 2019, IEEE I CONF COMP VIS, P1284, DOI 10.1109/ICCV.2019.00137; Xie Sirui, 2018, ARXIV181209926, P2; Xie Sirui, 2019, ICLR, V1, P13; Yang BS, 2019, AAAI CONF ARTIF INTE, P387; Yao Bangpeng, 2009, ADV NEURAL INFORM PR, P2178; Yu Kaicheng, 2019, INT C LEARN REPR, P2; Zelnik-Manor L, 2019, ARXIV190404123; Zhang X., 2018, ARXIV181101567; Zoph B., 2016, ARXIV161101578; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	56	23	23	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302002
C	Osawa, K; Swaroop, S; Jain, A; Eschenhagen, R; Turner, RE; Yokota, R; Khan, ME		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Osawa, Kazuki; Swaroop, Siddharth; Jain, Anirudh; Eschenhagen, Runa; Turner, Richard E.; Yokota, Rio; Khan, Mohammad Emtiyaz			Practical Deep Learning with Bayesian Principles	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Bayesian methods promise to fix many shortcomings of deep learning, but they are impractical and rarely match the performance of standard methods, let alone improve them. In this paper, we demonstrate practical training of deep networks with natural-gradient variational inference. By applying techniques such as batch normalisation, data augmentation, and distributed training, we achieve similar performance in about the same number of epochs as the Adam optimiser, even on large datasets such as ImageNet. Importantly, the benefits of Bayesian principles are preserved: predictive probabilities are well-calibrated, uncertainties on out-of-distribution data are improved, and continual-learning performance is boosted. This work enables practical deep learning while preserving benefits of Bayesian principles. A PyTorch implementation(1) is available as a plug-and-play optimiser.	[Osawa, Kazuki; Yokota, Rio] Tokyo Inst Technol, Tokyo, Japan; [Swaroop, Siddharth; Turner, Richard E.] Univ Cambridge, Cambridge, England; [Jain, Anirudh] Indian Inst Technol ISM, Dhanbad, Bihar, India; [Eschenhagen, Runa] Univ Osnabruck, Osnabruck, Germany; [Jain, Anirudh; Eschenhagen, Runa; Khan, Mohammad Emtiyaz] RIKEN, Ctr AI Project, Tokyo, Japan	Tokyo Institute of Technology; University of Cambridge; Indian Institute of Technology System (IIT System); Indian Institute of Technology (Indian School of Mines) Dhanbad; University Osnabruck; RIKEN	Khan, ME (corresponding author), RIKEN, Ctr AI Project, Tokyo, Japan.	emtiyaz.khan@riken.jp		Yokota, Rio/0000-0001-7573-7873	Tokyo Institute of Technology (TSUBAME3.0) through the HPCI System Research Project [hp190122]; JSPS KAKENHI [JP19J13477]	Tokyo Institute of Technology (TSUBAME3.0) through the HPCI System Research Project; JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	We would like to thank Hikaru Nakata (Tokyo Institute of Technology) and Ikuro Sato (Denso IT Laboratory, Inc.) for their help on the PyTorch implementation. We are also thankful for the RAIDEN computing system and its support team at the RIKEN Center for Al Project which we used extensively for our experiments. This research used computational resources of the HPCI system provided by Tokyo Institute of Technology (TSUBAME3.0) through the HPCI System Research Project (Project ID:hp190122). K. O. is a Research Fellow of JSPS and is supported by JSPS KAKENHI Grant Number JP19J13477.	Barber D., 1998, Neural Networks and Machine Learning. Proceedings, P215; Bishop CM, 2006, PATTERN RECOGNITION; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Bottou L., 2016, ARXIV160604838; Bradshaw John, 2017, ARXIV70702476; DEGROOT MH, 1983, J ROY STAT SOC D-STA, V32, P12; DeVries Terrance, 2018, ARXIV180204865; Gal Y, 2016, PR MACH LEARN RES, V48; Gimpel Kevin, 2017, INT C LEARN REPR; Glorot X., 2010, PROC MACH LEARN RES, P249; Goodfellow Ian, 2015, ARXIV E PRINTS; Goyal Priya, 2017, ARXIV170602677; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Guo Chuan, 2017, ICML, DOI DOI 10.5555/3305381.3305518; Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Hoeting JA, 1999, STAT SCI, V14, P382, DOI 10.1214/ss/1009212519; Khan M, 2012, THESIS; Khan ME, 2018, PROCEEDINGS OF 2018 INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY AND ITS APPLICATIONS (ISITA2018), P31, DOI 10.23919/ISITA.2018.8664326; Khan ME, 2017, PR MACH LEARN RES, V54, P878; Kingma D.P, P 3 INT C LEARNING R; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387; Lee Kimin, 2018, INT C LEARN REPR; Liang Shiyu, 2018, INT C LEARN REPR; Lopez-Paz D, 2017, ADV NEUR IN, V30; Loshchilov Ilya, 2019, INT C LEARN REPR; MacKay D. J. C., 1991, THESIS; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; Maddox WJ, 2019, ADV NEUR IN, V32; Mikolov T., 2013, ARXIV; Naeini MP, 2015, AAAI CONF ARTIF INTE, P2901; Neal R. M., 2012, BAYESIAN LEARNING NE; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Nguyen C. V., 2017, ARXIV171010628; Osawa K., 2018, ABS181112019 CORR; Peterson C., 1987, Complex Systems, V1, P995; Riquelme Carlos, 2018, ARXIV180209127; Ritter Hippolyt, 2018, ICLR; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Rusu A. A., 2016, ARXIV160604671; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251; Schwarz J, 2018, PR MACH LEARN RES, V80; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Swaroop S., 2019, ARXIV190502099; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Vovk V. G., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory, P371; Vuppalapati C, 2018, INT CONF MACH LEARN, P161; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7; Zhang G., 2018, ARXIV171202390	56	23	23	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304031
C	Rao, D; Visin, F; Rusu, AA; Teh, YW; Pascanu, R; Hadsell, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Rao, Dushyant; Visin, Francesco; Rusu, Andrei A.; Teh, Yee Whye; Pascanu, Razvan; Hadsell, Raia			Continual Unsupervised Representation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Continual learning aims to improve the ability of modern learning systems to deal with non-stationary distributions, typically by attempting to learn a series of tasks sequentially. Prior art in the field has largely considered supervised or reinforcement learning tasks, and often assumes full knowledge of task labels and boundaries. In this work, we propose an approach (CURL) to tackle a more general problem that we will refer to as unsupervised continual learning. The focus is on learning representations without any knowledge about task identity, and we explore scenarios when there are abrupt changes between tasks, smooth transitions from one task to another, or even when the data is shuffled. The proposed approach performs task inference directly within the model, is able to dynamically expand to capture new concepts over its lifetime, and incorporates additional rehearsal-based techniques to deal with catastrophic forgetting. We demonstrate the efficacy of CURL in an unsupervised learning setting with MNIST and Omniglot, where the lack of labels ensures no information is leaked about the task. Further, we demonstrate strong performance compared to prior art in an i.i.d setting, or when adapting the technique to supervised tasks such as incremental class learning.	[Rao, Dushyant; Visin, Francesco; Rusu, Andrei A.; Teh, Yee Whye; Pascanu, Razvan; Hadsell, Raia] DeepMind, London, England		Rao, D (corresponding author), DeepMind, London, England.	dushyantr@google.com; visin@google.com						ACHILLE A, 2018, ADV NEURAL INFORM PR, P9873, DOI DOI 10.1364/OPTICA.5.000803; Aljundi R, 2018, P EUR C COMP VIS ECC, P139; Aljundi Rahaf, 2019, P CVPR 2019; Burda Yuri, 2015, ICLR; Chaudhry Arslan, 2018, ARXIV181200420, V2, P6; Draelos TJ, 2017, IEEE IJCNN, P526, DOI 10.1109/IJCNN.2017.7965898; Farquhar Sebastian, 2018, ARXIV180509733; Fernando Chrisantha, 2017, PATHNET EVOLUTION CH; Golkar Siavash, 2019, ARXIV190304476; Goodfellow I.J., 2013, COMPUTER SCI; He X., 2018, OVERCOMING CATASTROP; Hsu Yen-Chang, 2018, ARXIV181012488; Jiang ZX, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1965; Joo Weonyoung, 2019, ARXIV190102739; Kingma DP, 2 INT C LEARN REPR I, P1; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Lake B., 2011, P ANN M COGN SCI SOC; LeCun Y., 2010, MNIST HANDWRITTEN DI, V2, P18; Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081; Lopez-Paz D., 2017, ADV NEURAL INF PROCE, P6467; Milan K., 2016, ADV NEURAL INFORM PR, V29, P3702; Nalisnick Eric, 2016, NIPS WORKSH BAYES DE, V2; Nalisnick Eric, 2017, INT C LEARN REPR ICL; Nguyen C. V., 2017, ARXIV171010628; Ostapenko Oleksiy, 2018, LEARNING REMEMBER DY; Parisi G. I., 2019, NEURAL NETWORKS; Rebuffi S. -A., 2017, P IEEE C COMPUTER V, P2001; Rezende D.J., 2014, PROC INT CONFER ENCE; Robins A., 1995, Connection Science, V7, P123, DOI 10.1080/09540099550039318; Rusu A. A., 2016, PROGR NEURAL NETWORK; Schwarz J., 2018, ARXIV180506370; Serr`a Joan, 2018, INT C MACH LEARN, P4555; Shin H., 2017, ADV NEURAL INFORM PR, P2990; Smith James, 2019, ARXIV190402021; Teh Yee Whye, 2010, ENCY MACHINE LEARNIN, DOI DOI 10.1007/978-0-387-30164-8_219; van de Ven Gido M, 2018, ARXIV180910635; van de Ven Gido M., 2019, ARXIV190407734, P2; Yoon J., 2017, ARXIV170801547; Zenke F, 2017, PR MACH LEARN RES, V70; Zeno C., 2018, ARXIV180310123; Zhou G., 2012, 15 INT C ARTIFICIAL, P1453	42	23	23	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307064
C	Schwab, P; Karlen, W		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Schwab, Patrick; Karlen, Walter			CXPlain: Causal Explanations for Model Interpretation under Uncertainty	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Feature importance estimates that inform users about the degree to which given inputs influence the output of a predictive model are crucial for understanding, validating, and interpreting machine-learning models. However, providing fast and accurate estimates of feature importance for high-dimensional data, and quantifying the uncertainty of such estimates remain open challenges. Here, we frame the task of providing explanations for the decisions of machine-learning models as a causal learning task, and train causal explanation (CXPlain) models that learn to estimate to what degree certain inputs cause outputs in another machine-learning model. CXPlain can, once trained, be used to explain the target model in little time, and enables the quantification of the uncertainty associated with its feature importance estimates via bootstrap ensembling. We present experiments that demonstrate that CXPlain is significantly more accurate and faster than existing model-agnostic methods for estimating feature importance. In addition, we confirm that the uncertainty estimates provided by CXPlain ensembles are strongly correlated with their ability to accurately estimate feature importance on held-out data.	[Schwab, Patrick; Karlen, Walter] Swiss Fed Inst Technol, Inst Robot & Intelligent Syst, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Schwab, P (corresponding author), Swiss Fed Inst Technol, Inst Robot & Intelligent Syst, Zurich, Switzerland.	patrick.schwab@hest.ethz.ch			Swiss National Science Foundation (SNSF) [167302, 75]	Swiss National Science Foundation (SNSF)(Swiss National Science Foundation (SNSF))	This work was partially funded by the Swiss National Science Foundation (SNSF) project No. 167302 within the National Research Program (NRP) 75 "Big Data". We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPUs used for this research. Patrick Schwab is an affiliated PhD fellow at the Max Planck ETH Center for Learning Systems. We additionally thank the anonymous reviewers whose comments helped improve this manuscript.	Abadi M, 2015, P 12 USENIX S OPERAT; Adebayo Julius, 2018, ADV NEURAL INFORM PR, V1, P7; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Andrews R, 1995, KNOWL-BASED SYST, V8, P373, DOI 10.1016/0950-7051(96)81920-4; Baehrens D, 2010, J MACH LEARN RES, V11, P1803; Bastani Osbert, 2017, PANCREATIC CANC; Bengio Yoshua, 2016, DEEP LEARNING; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Chattopadhyay A., 2019, ARXIV190202302; Che Zhengping, 2016, AMIA Annu Symp Proc, V2016, P371; Chen JB, 2018, PR MACH LEARN RES, V80; Choi E, 2016, ADV NEUR IN, V29; Dabkowski P, 2017, ADV NEUR IN, V30; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Efron Bradley, 1982, JACKKNIFE BOOTSTRAP, V38, DOI [10.1137/1.9781611970319, DOI 10.1137/1.9781611970319]; FEN H, 2019, ARXIV190412991; Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371; Gal Y, 2016, PR MACH LEARN RES, V48; Ghorbani A, 2019, AAAI CONF ARTIF INTE, P3681; GRANGER CWJ, 1969, ECONOMETRICA, V37, P424, DOI 10.2307/1912791; Guo WB, 2018, ADV NEUR IN, V31; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hollander M., 2014, NONPARAMETRIC STAT M; Janzing D, 2013, ANN STAT, V41, P2324, DOI 10.1214/13-AOS1145; Kaiser Lukasz, 2017, ARXIV170605137; Khosravi P, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2716; Kim B, 2017, ARXIV PREPRINT ARXIV; Kingma D.P, P 3 INT C LEARNING R; Klambauer Gnter, 2017, SELF NORMALIZING NEU; Koh PW, 2017, PR MACH LEARN RES, V70; Kullback S, 1997, INFORM THEORY STAT; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387; LeCun Y., 2010, MNIST HANDWRITTEN DI; Li Jiwei, 2016, ARXIV161208220; Lipton Zachary C, 2016, INT C MACH LEARN WOR; Lundberg SM, 2017, ADV NEUR IN, V30; Montavon G, 2017, PATTERN RECOGN, V65, P211, DOI 10.1016/j.patcog.2016.11.008; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Schwab P, 2017, COMPUT CARDIOL CONF, V44, DOI 10.22489/CinC.2017.363-223; Schwab P, 2019, AAAI CONF ARTIF INTE, P4846; Schwab Patrick, 2015, AAAI C ART INT DIG E; Shrikumar A, 2017, PR MACH LEARN RES, V70; SILVER NC, 1987, J APPL PSYCHOL, V72, P146, DOI 10.1037/0021-9010.72.1.146; Smilkov D, 2017, ARXIV; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; STONE R, 1993, J ROY STAT SOC B MET, V55, P455; Strumbelj E, 2009, DATA KNOWL ENG, V68, P886, DOI 10.1016/j.datak.2009.01.004; Sundararajan M, 2017, PR MACH LEARN RES, V70; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tsang Michael, 2017, INT C LEARN REPR; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zintgraf Luisa M., 2017, P ICLR	56	23	23	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901081
C	Tagasovska, N; Lopez-Paz, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tagasovska, Natasa; Lopez-Paz, David			Single-Model Uncertainties for Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We provide single-model estimates of aleatoric and epistemic uncertainty for deep neural networks. To estimate aleatoric uncertainty, we propose Simultaneous Quantile Regression (SQR), a loss function to learn all the conditional quantiles of a given target variable. These quantiles can be used to compute well-calibrated prediction intervals. To estimate epistemic uncertainty, we propose Orthonormal Certificates (OCs), a collection of diverse non-constant functions that map all training samples to zero. These certificates map out-of-distribution examples to non-zero values, signaling epistemic uncertainty. Our uncertainty estimators are computationally attractive, as they do not require ensembling or retraining deep models, and achieve competitive performance.	[Tagasovska, Natasa] HEC Lausanne, Dept Informat Syst, Lausanne, Switzerland; [Lopez-Paz, David] Facebook AI Res, Paris, France	Facebook Inc	Tagasovska, N (corresponding author), HEC Lausanne, Dept Informat Syst, Lausanne, Switzerland.	natasa.tagasovska@unil.ch; dlp@fb.com						Abadie A., 1998, INSTRUMENTAL VARIABL; Alvarez-Melis David, 2017, EMNLP; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Asuncion A, 2007, UCI MACHINE LEARNING; Begoli E., 2019, NATURE MACHINE INTEL; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Bouchacourt D., 2016, NEURIPS; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655; Burda Y., 2018, EXPLORATION RANDOM N; Cannon A. J., 2018, STOCHASTIC ENV RES R; Chandola V, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541882; Chen T., 2018, CONFIDENCE SCORING U; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cortes C., 2016, ALT; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Dabney W., 2018, IMPLICIT QUANTILE NE; Daniusis P., 2012, INFERRING DETERMINIS; de Haan L., 2007, EXTREME VALUE THEORY; Depeweg Stefan, 2018, ICML; Der Kiureghian A., 2009, STRUCTURAL SAFETY; Efron B., 1994, INTRO BOOTSTRAP; Ferguson T. S., 1967, MATH STAT DECISION T; Fox M., 1964, ANN MATH STAT; Gal Y., 2016, THESIS, V1, P3; Gal Y, 2016, PR MACH LEARN RES, V48; Geifman Yonatan, 2017, NIPS; Gneiting Tilmann, 2007, J ROYAL STAT SOC B; Goodfellow I., 2016, ARXIV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; Hafner Danijar, 2018, RELIABLE UNCERTAINTY; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hernandez-Lobato D., 2016, JMLR; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1699; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Hodge VJ, 2004, ARTIF INTELL REV, V22, P85, DOI 10.1023/B:AIRE.0000045502.10941.a9; Houlsby N, 2011, BAYESIAN ACTIVE LEAR; Hoyer P. O., 2009, NEURIPS; Janzing D., 2010, IEEE T INFORM THEORY; Kendall Alex, 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295309; Khan M. E., 2018, ICML; Kingma D.P, P 3 INT C LEARNING R; Koenker Roger, 1978, ECONOMETRICA J ECONO; Koenker RW., 2005, QUANTILE REGRESSION, DOI [DOI 10.1017/CBO9780511754098, 10.1017/CBO9780511754098]; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387; Liang Shiyu, 2018, INT C LEARN REPR; Lopez-Paz D., 2016, THESIS; Meinshausen N., 2006, JMLR; Mirza M., 2014, ARXIV; Mohamed S., 2016, LEARNING IMPLICIT GE; Mooij J. M., 2016, JMLR; Nix D. A., 1994, ICNN; Ostrovski G., 2018, AUTOREGRESSIVE QUANT; Pearce T., 2018, ICML; Pimentel MAF, 2014, SIGNAL PROCESSING; Qu Z., 2015, J ECONOMETRICS; Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4; Rodrigues F., 2018, EXPECTATION DEEP JOI; Ruff L, 2018, PR MACH LEARN RES, V80; Scholkopf B, 2000, ADV NEUR IN, V12, P582; Scholkopf B., 2001, NEURAL COMPUTATION; Settles B., 2010, COMPUTER SCI TECHNIC; Shafaei A., 2018, DOES YOUR MODEL KNOW; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Smith H. J., 2011, MIS Q; Stegle O., 2010, NEURIPS; Steinwart I., 2011, BERNOULLI; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; TAKEUCHI I, 2006, JMLR; Taylor J. W., 2000, J FORECASTING; Teye M, 2018, PR MACH LEARN RES, V80; Wang D., 2014, IJCNN; Wen R., 2017, TIME SERIES WORKSHOP; Wen Y. K, 2003, FD2 MAE; White H., 1992, Computing Science and Statistics. Statistics of Many Parameters: Curves, Images, Spatial Models. Proceedings of the 22nd Symposium on the Interface, P190; Wu W, 2014, COMPUT INTEL NEUROSC, V2014, DOI 10.1155/2014/161203; Zhang K., 2009, P TWENTYFIFTH C UNCE, P647; Zhang W., 2018, IEEE T SMART GRID; Zheng Songfeng, 2010, MEX INT C ART INT; Zhu L., 2017, ICDMW; [No title captured]	85	23	23	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306042
C	Villegas, R; Pathak, A; Kannan, H; Erhan, D; Le, QV; Lee, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Villegas, Ruben; Pathak, Arkanath; Kannan, Harini; Erhan, Dumitru; Le, Quoc V.; Lee, Honglak			High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Predicting future video frames is extremely challenging, as there are many factors of variation that make up the dynamics of how frames change through time. Previously proposed solutions require complex inductive biases inside network architectures with highly specialized computation, including segmentation masks, optical flow, and foreground and background separation. In this work, we question if such handcrafted architectures are necessary and instead propose a different approach: finding minimal inductive bias for video prediction while maximizing network capacity. We investigate this question by performing the first large-scale empirical study and demonstrate state-of-the-art performance by learning large models on three different datasets: one for modeling object interactions, one for modeling human motion, and one for modeling car driving(1).	[Villegas, Ruben] Univ Michigan, Ann Arbor, MI 48109 USA; [Kannan, Harini; Erhan, Dumitru; Le, Quoc V.; Lee, Honglak] Google Res, Ann Arbor, MI USA; [Pathak, Arkanath] Google, Mountain View, CA 94043 USA; [Villegas, Ruben] Adobe Res, San Jose, CA 95110 USA	University of Michigan System; University of Michigan; Google Incorporated; Google Incorporated; Adobe Systems Inc.	Villegas, R (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.; Villegas, R (corresponding author), Adobe Res, San Jose, CA 95110 USA.							Babaeizadeh M., 2018, ICLR; Brock Andrew, 2019, ICLR; Byeon Wonmin, 2018, ECCV; Denton Emily, 2018, ICML; Denton Emily L, 2017, NEURIPS; Devlin Jacob, 2019, P 2019 C N AM CHAPT, Patent No. [ArXiv181004805Cs, 181004805]; Dosovitskiy A., 2016, CVPR; Ebert Frederik, 2018, VISUAL FORESIGHT MOD; Finn Chelsea, 2016, NEURIPS; Geiger A., 2013, IJRR; Goodfellow Ian, 2014, 27 INT C NEURAL INFO; Hafner D., 2019, ICML; Heusel M., 2017, NEURIPS; Huang Y., 2018, GPIPE EFFICIENT TRAI; Ionescu C., 2014, PAMI; Kaiser Lukasz, 2019, ABS190300374 CORR; Kumar Manoj, 2018, ICML; Lee Alex X., 2018, STOCHASTIC ADVERSARI; Liang X., 2017, ICCV; Lotter W., 2017, ICLR; Michalski V., 2014, NEURIPS; Oh J., 2015, NEURIPS; Radford A., 2019, LANGUAGE MODELS ARE; Ranzato MarcAurelio, 2014, ARXIV14126604; Real E., 2018, REGULARIZED EVOLUTIO; Simonyan Karen, 2015, INT C LEARN REPR; Srivastava Nitish, 2015, ICML; Sutton Richard, 2019, BITTER LESSON; Tulyakov S., 2018, CVPR; Unterthiner Thomas, 2018, ABS181201717 CORR; Villegas Ruben, 2017, ICML; Villegas Ruben, 2017, ICLR; Vondrick Carl, 2016, NEURIPS; Wichers Nevan, 2018, ICML; Yan Xinchen, 2018, ECCV; Zoph B., 2018, CVPR	36	23	24	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300008
C	Daskalakis, C; Panageas, I		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Daskalakis, Constantinos; Panageas, Ioannis			The Limit Points of (Optimistic) Gradient Descent in Min-Max Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Motivated by applications in Optimization, Game Theory, and the training of Generative Adversarial Networks, the convergence properties of first order methods inmin-max problems have received extensive study. It has been recognized that they may cycle, and there is no good understanding of their limit points when they do not. When they converge, do they converge to local min-max solutions? We characterize the limit points of two basic first order methods, namely Gradient Descent/Ascent (GDA) and Optimistic Gradient Descent Ascent (OGDA). We show that both dynamics avoid unstable critical points for almost all initializations. Moreover, for small step sizes and under mild assumptions, the set of OGDA-stable critical points is a superset of GDA-stable critical points, which is a superset of local min-max solutions (strict in some cases). The connecting thread is that the behavior of these dynamics can be studied from a dynamical systems perspective.	[Daskalakis, Constantinos] MIT, CSAIL, Cambridge, MA 02138 USA; [Panageas, Ioannis] SUTD, ISTD, Singapore 487371, Singapore	Massachusetts Institute of Technology (MIT); Singapore University of Technology & Design	Daskalakis, C (corresponding author), MIT, CSAIL, Cambridge, MA 02138 USA.	costis@csail.mit.edu; ioannis@sutd.edu.sg			NSF [CCF-1617730, IIS-1741137]; Simons Investigator Award; Google Faculty Research Award; MIT-IBM Watson AI Lab research grant; SRG ISTD [2018 136]	NSF(National Science Foundation (NSF)); Simons Investigator Award; Google Faculty Research Award(Google Incorporated); MIT-IBM Watson AI Lab research grant(International Business Machines (IBM)); SRG ISTD	Constantinos Daskalakis was supported by NSF awards CCF-1617730 and IIS-1741137, a Simons Investigator Award, a Google Faculty Research Award, and an MIT-IBM Watson AI Lab research grant. Ioannis Panageas was supported by SRG ISTD 2018 136. This work was done when Ioannis was a postdoctoral fellow at MIT.	Adler I, 2013, INT J GAME THEORY, V42, P165, DOI 10.1007/s00182-012-0328-8; Arjovsky M, 2017, PR MACH LEARN RES, V70; Blackwell David, 1956, PAC J MATH, V6, P1, DOI [DOI 10.2140/PJM.1956.6.1, 10.2140/pjm.1956.6.1]; Brown G., 1951, ACT ANAL PROD ALLOCA, P374; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING; Daskalakis C., 2018, ICLR; Galor O., 2007, DISCRETE DYNAMICAL S; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Jin Chi, 2016, ADV NEURAL INFORM PR, P4116; Lee J., 2017, CORR; Lessard Laurent, 2016, SIAM J OPTIMIZATION; Mai Tung, 2018, EC COMPUTATION EC; Mehta Ruta, 2015, INNOVATIONS THEORETI; Mertikopoulos P, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2703; Nagarajan V, 2017, ADV NEUR IN, V30; PALAIOPANOS  G., 2017, ADV NEURAL INFORM PR, V30, P5872, DOI DOI 10.5555/3295222.3295337; Rakhlin Alexander, 2013, ONLINE LEARNING PRED; Ratliff Lillian J., 2013, ALLERTON	21	23	23	5	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003076
C	Guo, YW; Zhang, C; Zhang, CS; Chen, YR		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Guo, Yiwen; Zhang, Chao; Zhang, Changshui; Chen, Yurong			Sparse DNNs with Improved Adversarial Robustness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep neural networks (DNNs) are computationally/memory-intensive and vulnerable to adversarial attacks, making them prohibitive in some real-world applications. By converting dense models into sparse ones, pruning appears to be a promising solution to reducing the computation/memory cost. This paper studies classification models, especially DNN-based ones, to demonstrate that there exists intrinsic relationships between their sparsity and adversarial robustness. Our analyses reveal, both theoretically and empirically, that nonlinear DNN-based classifiers behave differently under l(2) attacks from some linear ones. We further demonstrate that an appropriately higher model sparsity implies better robustness of nonlinear DNNs, whereas over-sparsified models can be more difficult to resist adversarial examples.	[Guo, Yiwen; Chen, Yurong] Intel Labs China, Beijing, Peoples R China; [Guo, Yiwen; Zhang, Changshui] Tsinghua Univ, State Key Lab Intelligent Technol & Syst, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Inst Artificial Intelligence,Tsinghua Univ THUAI, Beijing, Peoples R China; [Zhang, Chao] Peking Univ, Ctr Data Sci, Acad Adv Interdisciplinary Studies, Beijing, Peoples R China	Intel Corporation; Tsinghua University; Peking University	Guo, YW (corresponding author), Intel Labs China, Beijing, Peoples R China.; Guo, YW (corresponding author), Tsinghua Univ, State Key Lab Intelligent Technol & Syst, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Inst Artificial Intelligence,Tsinghua Univ THUAI, Beijing, Peoples R China.	yiwen.guo@intel.com; pkuzc@pku.edu.cn; zcs@mail.tsinghua.edu.cn; yurong.chen@intel.com			NSFC [61876095, 61751308, 61473167]; Beijing Natural Science Foundation [L172037]	NSFC(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation)	We would like to thank anonymous reviewers for their constructive suggestions. Changshui Zhang is supported by NSFC (Grant No. 61876095, No. 61751308 and No. 61473167) and Beijing Natural Science Foundation (Grant No. L172037).	BARTLETT P., 2017, SPECTRALLY NORMALIZE; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Cao Y., 2018, ICLR WORKSH SUBM VAN, P1; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Cisse M, 2017, PR MACH LEARN RES, V70; d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269; Denil Misha, 2013, NIPS, DOI DOI 10.5555/2999792.2999852; Dhillon G. S., 2018, ICLR; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Galloway A., 2018, ARXIV171100449; Gao J., 2017, ARXIV PREPRINT ARXIV; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Gopalakrishnan S., 2018, ARXIV PREPRINT ARXIV; Guo YW, 2016, ADV NEUR IN, V29; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hein M., 2017, ADV NEURAL INFORM PR, V30, P2266; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Lu JJ, 2017, IEEE I CONF COMP VIS, P446, DOI 10.1109/ICCV.2017.56; Madry A., 2018, P ICLR VANC BC CAN; Marzi Z., 2018, ARXIV180104695; Molchanov D, 2017, PR MACH LEARN RES, V70; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Neklyudov Kirill, 2017, ADV NEURAL INFORM PR, V30; Park Jongsoo, 2017, INT C LEARN REPR; Szegedy Christian, 2014, P 2 INT C LEARNING R; Tramonti F, 2019, PSYCHOL HEALTH MED, V24, P27, DOI 10.1080/13548506.2018.1510131; Ullrich K., 2017, 5 INT C LEARN REPPR; Weng Thui-Wei, 2018, INT C LEARN REPR; Xie Cihang, 2018, P INT C LEARN REPR I; Ye S., 2018, INT C LEARN REPR WOR	32	23	24	0	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300023
C	Havasi, M; Hernandez-Lobato, JM; Murillo-Fuentes, JJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Havasi, Marton; Hernandez-Lobato, Jose Miguel; Jose Murillo-Fuentes, Juan			Inference in Deep Gaussian Processes using Stochastic Gradient Hamiltonian Monte Carlo	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian Processes that combine well calibrated uncertainty estimates with the high flexibility of multilayer models. One of the biggest challenges with these models is that exact inference is intractable. The current state-of-the-art inference method, Variational Inference (VI), employs a Gaussian approximation to the posterior distribution. This can be a potentially poor unimodal approximation of the generally multimodal posterior. In this work, we provide evidence for the non-Gaussian nature of the posterior and we apply the Stochastic Gradient Hamiltonian Monte Carlo method to generate samples. To efficiently optimize the hyperparameters, we introduce the Moving Window MCEM algorithm. This results in significantly better predictions at a lower computational cost than its VI counterpart. Thus our method establishes a new state-of-the-art for inference in DGPs.	[Havasi, Marton; Hernandez-Lobato, Jose Miguel] Univ Cambridge, Dept Engn, Cambridge, England; [Hernandez-Lobato, Jose Miguel] Microsoft Res, Cambridge, England; [Hernandez-Lobato, Jose Miguel] Alan Turing Inst, London, England; [Jose Murillo-Fuentes, Juan] Univ Seville, Dept Signal Theory & Commun, Seville, Spain	University of Cambridge; Microsoft; University of Sevilla	Havasi, M (corresponding author), Univ Cambridge, Dept Engn, Cambridge, England.	mh740@cam.ac.uk; jmh233@cam.ac.uk; murillo@us.es	Hernandez-Lobato, Jose Miguel/F-2056-2016	Hernandez-Lobato, Jose Miguel/0000-0001-7610-949X	Intel; EPSRC; Spanish government [TEC2016-78434-C3-R]; European Union (MINECO/FEDER, UE)	Intel(Intel Corporation); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Spanish government(Spanish GovernmentEuropean Commission); European Union (MINECO/FEDER, UE)	We want to thank Adri a Gariga-Alonso, John Bronskill, Robert Peharz and Siddharth Swaroop for their helpful comments and thank Intel and EPSRC for their generous support.; Juan Jose Murillo-Fuentes acknowledges funding from the Spanish government (TEC2016-78434-C3-R) and the European Union (MINECO/FEDER, UE).	Abadi M, 2015, P 12 USENIX S OPERAT; Brooks S, 2011, CH CRC HANDB MOD STA, pXIX; Bui TD, 2016, PR MACH LEARN RES, V48; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Cheng C.A., 2017, P 31 INT C NEUR INF, P5190; Cheng C.-A., 2016, ADV NEURAL INFORM PR, P4410; Cramer D., 1998, FUNDAMENTAL STAT SOC, P10001; Cutajar Kurt, 2016, ARXIV161004386; Damianou A., 2015, DEEP GAUSSIAN PROCES; Damianou Andreas, 2013, ARTIF INTELL, P207, DOI DOI 10.1002/NME.1296; Dunlop M. M., 2017, ARXIV171111280; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Hachmann J, 2011, J PHYS CHEM LETT, V2, P2241, DOI 10.1021/jz200866s; Hernandez-Lobato D., 2011, ADV NEURAL INFORM PR, P280; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1861; Hoffman MD, 2017, PR MACH LEARN RES, V70; Minka T.P., 2001, P 17 C UNC ART INT, P362; Neal RM, 1993, CRGTR931 U TOR DEP C; Neath, 2013, ADV MODERN STAT THEO, P43, DOI DOI 10.1214/12-IMSCOLL1003; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Salimbeni Hugh, 2017, ADV NEURAL INFORM PR, V30, P4588; Snelson E., 2006, ADV NEURAL INFORM PR, V18, P1259; Springenberg J.T., 2016, P ADV NEURAL INFORM, P4134; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Turner R. E., 2011, BAYESIAN TIME SERIES, P109; WEI GCG, 1990, J AM STAT ASSOC, V85, P699, DOI 10.2307/2290005; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Williams CKI, 1996, ADV NEUR IN, V8, P514	29	23	23	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002009
C	Herzig, R; Raboh, M; Chechik, G; Berant, J; Globerson, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Herzig, Roei; Raboh, Moshiko; Chechik, Gal; Berant, Jonathan; Globerson, Amir			Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Machine understanding of complex images is a key goal of artificial intelligence. One challenge underlying this task is that visual scenes contain multiple interrelated objects, and that global context plays an important role in interpreting the scene. A natural modeling framework for capturing such effects is structured prediction, which optimizes over complex labels, while modeling within-label interactions. However, it is unclear what principles should guide the design of a structured prediction model that utilizes the power of deep learning components. Here we propose a design principle for such architectures that follows from a natural requirement of permutation invariance. We prove a necessary and sufficient characterization for architectures that follow this invariance, and discuss its implication on model design. Finally, we show that the resulting model achieves new state-of-the-art results on the Visual Genome scene-graph labeling benchmark, outperforming all recent approaches.	[Herzig, Roei; Raboh, Moshiko; Globerson, Amir] Tel Aviv Univ, Tel Aviv, Israel; [Chechik, Gal] Bar Ilan Univ, NVIDIA Res, Ramat Gan, Israel; [Berant, Jonathan] Tel Aviv Univ, AI2, Tel Aviv, Israel	Tel Aviv University; Bar Ilan University; Tel Aviv University	Herzig, R (corresponding author), Tel Aviv Univ, Tel Aviv, Israel.	roeiherzig@mail.tau.ac.il; mosheraboh@mail.tau.ac.il; gal.chechik@biu.ac.il; joberant@cs.tau.ac.il; gamir@post.tau.ac.il		Globerson, Amir/0000-0003-2557-1742; Herzig, Roei/0000-0002-5451-6059	ISF Centers of Excellence grant; Yandex Initiative in Machine Learning	ISF Centers of Excellence grant; Yandex Initiative in Machine Learning	This work was supported by the ISF Centers of Excellence grant, and by the Yandex Initiative in Machine Learning. Work by GC was performed while at Google Brain Research.	Belanger D, 2017, PR MACH LEARN RES, V70; Bello I., 2016, ARXIV161109940; Bui Hung Hai, 2013, UAI, P132; Chen D., 2014, P 2014 C EMPIRICAL M, P740, DOI DOI 10.3115/V1/D14-1082; Chen Liang Chieh, 2014, P 2 INT C LEARN REPR; Chen LC, 2015, PR MACH LEARN RES, V37, P1785; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Gilmer J, 2017, PR MACH LEARN RES, V70; Gygli M, 2017, PR MACH LEARN RES, V70; Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133; Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990; Khalil Elias, 2017, ADV NEURAL INFORM PR, P1; Kingma D.P, P 3 INT C LEARNING R; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Liao Wentong, 2016, ARXIV160905834; Lin G., 2015, ADV NEURAL INFORM PR, P361; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; Meshi O., 2010, P 27 INT C MACH LEAR, P783; Newell A., 2017, ADV NEURAL INFORM PR, V30, P1172; Newell A, 2017, ADV NEUR IN, V30; Pei WZ, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P313; Plummer BA, 2017, IEEE I CONF COMP VIS, P1946, DOI 10.1109/ICCV.2017.213; Santoro A, 2017, ARXIV PREPRINT ARXIV; Schwing A.G., 2015, ARXIV E PRINTS; Taskar B, 2004, ADV NEUR IN, V16, P25; Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330; Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41; Zaheer Manzil, 2017, P ADV NEUR INF PROC, P3394; Zellers R., 2017, ARXIV171106640; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179	33	23	23	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001073
C	Kandasamy, K; Neiswanger, W; Schneider, J; Poczos, B; Xing, EP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kandasamy, Kirthevasan; Neiswanger, Willie; Schneider, Jeff; Poczos, Barnabas; Xing, Eric P.			Neural Architecture Search with Bayesian Optimisation and Optimal Transport	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Bayesian Optimisation (BO) refers to a class of methods for global optimisation of a function f which is only accessible via point evaluations. It is typically used in settings where f is expensive to evaluate. A common use case for BO in machine learning is model selection, where it is not possible to analytically model the generalisation performance of a statistical model, and we resort to noisy and expensive training and validation procedures to choose the best model. Conventional BO methods have focused on Euclidean and categorical domains, which, in the context of model selection, only permits tuning scalar hyper-parameters of machine learning algorithms. However, with the surge of interest in deep learning, there is an increasing demand to tune neural network architectures. In this work, we develop NASBOT, a Gaussian process based BO framework for neural architecture search. To accomplish this, we develop a distance metric in the space of neural network architectures which can be computed efficiently via an optimal transport program. This distance might be of independent interest to the deep learning community as it may find applications outside of BO. We demonstrate that NASBOT outperforms other alternatives for architecture search in several cross validation based model selection tasks on multi-layer perceptrons and convolutional neural networks.	[Kandasamy, Kirthevasan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; Petuum Inc, Pittsburgh, PA USA	Carnegie Mellon University	Kandasamy, K (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	kandasamy@cs.cmu.edu; willie@cs.cmu.edu; schneide@cs.cmu.edu; bapoczos@cs.cmu.edu; epxing@cs.cmu.edu			DOE grant [DESC0011114]; NSF [IIS1563887]; Darpa D3M program; Facebook fellowship; Siebel scholarship	DOE grant(United States Department of Energy (DOE)); NSF(National Science Foundation (NSF)); Darpa D3M program; Facebook fellowship(Facebook Inc); Siebel scholarship	We would like to thank Guru Guruganesh and Dougal Sutherland for the insightful discussions. This research is partly funded by DOE grant DESC0011114, NSF grant IIS1563887, and the Darpa D3M program. KK is supported by a Facebook fellowship and a Siebel scholarship.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, ARXIV170300548; [Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; Bergstra J, 2013, 30 INT C MACH LEARN; Brochu E, 2010, ARXIV10122599; Buza K, 2014, STUD CLASS DATA ANAL, P145, DOI 10.1007/978-3-319-01595-8_16; Coraddu A, 2016, P I MECH ENG M-J ENG, V230, P136, DOI 10.1177/1475090214540874; Cortes C, 2016, ARXIV160701097; Fernandes Kelwin, 2015, PORT C ART INT; Gao XB, 2010, PATTERN ANAL APPL, V13, P113, DOI 10.1007/s10044-008-0141-y; Ginsbourger  David, 2011, ERCIM; Graf F, 2011, LECT NOTES COMPUT SC, V6892, P607, DOI 10.1007/978-3-642-23629-7_74; Huang G., 2017, CVPR; Hutter F., 2011, LION; Jenatton  Rodolphe, 2017, INT C MACH LEARN; Jiang Nan, 2016, ARXIV161009512; Kandasamy K, 2017, ARXIV170306240; Kandasamy K., 2016, ADV NEURAL INFORM PR, V29, P1777; Kandasamy K., 2016, ADV NEURAL INFORM PR, P992; Kandasamy Kirthevasan, 2016, ARXIV160306288; Klein A., 2016, FAST BAYESIAN OPTIMI; Kondor R.I., 2002, P 19 INT C MACHINE L, P315; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Liu C., 2017, ARXIV171200559; Liu H., 2017, ARXIV171100436; Mendoza H, 2016, PROC WORKSHOP AUTOM, P58; Messmer BT, 1998, IEEE T PATTERN ANAL, V20, P493, DOI 10.1109/34.682179; Mockus J. B., 1991, J OPTIMIZATION THEOR; Negrinho R., 2017, ARXIV170408792; Peyre G., COMPUTATIONAL OPTIMA; Rana P. S., 2013, PHYSICOCHEMICAL PROP; Rasmussen C., 2006, ADAPTATIVE COMPUTATI; Real E., 2017, P MACH LEARN RES, V84, P2902; Smola AJ, 2003, LECT NOTES ARTIF INT, V2777, P144, DOI 10.1007/978-3-540-45167-9_12; Snelson Edward, 2006, ADV NEURAL INFORM PR, V3; Snoek J., 2012, P ADV NEUR INF PROC, V2, P2960, DOI DOI 10.48550/ARXIV.1206.2944; Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811; Sutherland D.R.J., 2015, THESIS; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Swersky K., P NIPS WORKSH BAYES; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Torres-Sospedra J, 2014, INT C INDOOR POSIT, P261, DOI 10.1109/IPIN.2014.7275492; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201; Wallis WD, 2001, PATTERN RECOGN LETT, V22, P701, DOI 10.1016/S0167-8655(01)00022-8; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775; Xie L., 2017, ARXIV170301513; Zhong Z., 2017, ARXIV170805552; Zoph B., 2016, ICLR; Zoph B., 2017, ARXIV170707012	54	23	23	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302006
C	Scott, TR; Ridgeway, K; Mozer, MC		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Scott, Tyler R.; Ridgeway, Karl; Mozer, Michael C.			Adapted Deep Embeddings: A Synthesis of Methods for k-Shot Inductive Transfer Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The focus in machine learning has branched beyond training classifiers on a single task to investigating how previously acquired knowledge in a source domain can be leveraged to facilitate learning in a related target domain, known as inductive transfer learning. Three active lines of research have independently explored transfer learning using neural networks. In weight transfer, a model trained on the source domain is used as an initialization point for a network to be trained on the target domain. In deep metric learning, the source domain is used to construct an embedding that captures class structure in both the source and target domains. In few-shot learning, the focus is on generalizing well in the target domain based on a limited number of labeled examples. We compare state-of-the-art methods from these three paradigms and also explore hybrid adapted-embedding methods that use limited target-domain data to fine tune embeddings constructed from sourcedomain data. We conduct a systematic comparison of methods in a variety of domains, varying the number of labeled instances available in the target domain (k), as well as the number of target-domain classes. We reach three principal conclusions: (1) Deep embeddings are far superior, compared to weight transfer, as a starting point for inter-domain transfer or model re-use (2) Our hybrid methods robustly outperform every few-shot learning and every deep metric learning method previously proposed, with a mean error reduction of 34% over state-of-the-art. (3) Among loss functions for discovering embeddings, the histogram loss (Ustinova & Lempitsky, 2016) is most robust. We hope our results will motivate a unification of research in weight transfer, deep metric learning, and few-shot learning.	[Scott, Tyler R.; Ridgeway, Karl; Mozer, Michael C.] Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Scott, TR (corresponding author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.	tysc7237@colorado.edu; karl.ridgeway@colorado.edu; mozer@colorado.edu			National Science Foundation [EHR-1631428, SES-1461535]	National Science Foundation(National Science Foundation (NSF))	We would like to thank Chenhao Tan for helpful discussions. This research was supported by the National Science Foundation awards EHR-1631428 and SES-1461535.	Amiriparian S., 2017, INTERSPEECH 2017; Bellet A., 2013, 13066709 ARXIV; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Cole R., 1994, ISOLET DATASET; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Edwards H., 2017, NEURAL STAT, P2; Fei-Fei L, 2018, TINY IMAGENET VISUAL; Finn C, 2017, PR MACH LEARN RES, V70; Kaiser Lukasz, 2017, INT C LEARN REPR ICL; Kingma D.P, P 3 INT C LEARNING R; Koch G., 2015, ICML DEEP LEARNING W; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; LeCun Y., 2010, MNIST HANDWRITTEN DI; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Long MS, 2015, PR MACH LEARN RES, V37, P97; Lu JW, 2017, IEEE SIGNAL PROC MAG, V34, P76, DOI 10.1109/MSP.2017.2732900; Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Ravi S., 2017, INT C LEARN REPR, P12; Ridgeway K., 2018, 180205312 ARXIV; Rozario A. M., 2018, INT J DIGITAL ACCOUN, V18, P1, DOI [10.4192/1577-8517-v18_1, DOI 10.4192/1577-8517-V18_1.PDF]; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Triantafillou E, 2017, ADV NEUR IN, V30; Ustinova E., 2016, ADV NEURAL INFORM PR, V29, P4170; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wang J, 2017, IEEE I CONF COMP VIS, P2612, DOI 10.1109/ICCV.2017.283; Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhang Z., 2017, 171009505 ARXIV; Zheng L, 2015, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2015.7298783	36	23	23	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300008
C	Zhang, LH; Edraki, M; Qi, GJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Liheng; Edraki, Marzieh; Qi, Guo-Jun			CapProNet: Deep Feature Learning via Orthogonal Projections onto Capsule Subspaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we formalize the idea behind capsule nets of using a capsule vector rather than a neuron activation to predict the label of samples. To this end, we propose to learn a group of capsule subspaces onto which an input feature vector is projected. Then the lengths of resultant capsules are used to score the probability of belonging to different classes. We train such a Capsule Projection Network (CapProNet) by learning an orthogonal projection matrix for each capsule subspace, and show that each capsule subspace is updated until it contains input feature vectors corresponding to the associated class. We will also show that the capsule projection can be viewed as normalizing the multiple columns of the weight matrix simultaneously to form an orthogonal basis, which makes it more effective in incorporating novel components of input features to update capsule representations. In other words, the capsule projection can be viewed as a multi-dimensional weight normalization in capsule subspaces, where the conventional weight normalization is simply a special case of the capsule projection onto 1D lines. Only a small negligible computing overhead is incurred to train the network in low-dimensional capsule subspaces or through an alternative hyper-power iteration to estimate the normalization matrix. Experiment results on image datasets show the presented model can greatly improve the performance of the state-of-the-art ResNet backbones by 10 20% and that of the Densenet by 5 7% respectively at the same level of computing and memory expenses. The CapProNet establishes the competitive state-of-the-art performance for the family of capsule nets by significantly reducing test errors on the benchmark datasets.	[Zhang, Liheng; Edraki, Marzieh; Qi, Guo-Jun] Univ Cent Florida, Lab MAchine Percept & LEarning, Orlando, FL 32816 USA; [Qi, Guo-Jun] Huawei Cloud, Seattle, WA 98104 USA	State University System of Florida; University of Central Florida; Huawei Technologies	Qi, GJ (corresponding author), Univ Cent Florida, Lab MAchine Percept & LEarning, Orlando, FL 32816 USA.; Qi, GJ (corresponding author), Huawei Cloud, Seattle, WA 98104 USA.	guojunq@gmail.com	Qi, Guo-Jun/AAH-8294-2019	Qi, Guo-Jun/0000-0003-3508-1851				Bahadori M. T., 2018, SPECTRAL CAPSULE NET; Ben-Israel A., 1966, SIAM J NUMER ANAL, V3, P410, DOI DOI 10.1137/0703035; Ben-Israel A., 2003, GEN INVERSES; BENISRAEL A, 1965, MATH COMPUT, V19, P452, DOI 10.2307/2003676; Goodfellow I. J., 2013, ARXIV13024389; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Hinton Geoffrey E, 2018, MATRIX CAPSULES ROUT; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; Petersen K. B., MATRIX COOKBOOK; Rawlinson David, 2018, ARXIV180406094; Sabour Sara, 2017, PROC 31 INT C NEURAL; Sermanet P, 2012, INT C PATT RECOG, P3288; Wang  Dilin, 2018, 6 INT C LEARN REPR; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87	18	23	24	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000033
C	Zheng, Y; Meng, ZP; Hao, JY; Zhang, ZZ; Yang, TP; Fan, CJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zheng, Yan; Meng, Zhaopeng; Hao, Jianye; Zhang, Zongzhang; Yang, Tianpei; Fan, Changjie			A Deep Bayesian Policy Reuse Approach Against Non-Stationary Agents	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In multiagent domains, coping with non-stationary agents that change behaviors from time to time is a challenging problem, where an agent is usually required to be able to quickly detect the other agent's policy during online interaction, and then adapt its own policy accordingly. This paper studies efficient policy detecting and reusing techniques when playing against non-stationary agents in Markov games. We propose a new deep BPR+ algorithm by extending the recent BPR+ algorithm with a neural network as the value-function approximator. To detect policy accurately, we propose the rectified belief model taking advantage of the opponent model to infer the other agent's policy from reward signals and its behaviors. Instead of directly storing individual policies as BPR+, we introduce distilled policy network that serves as the policy library in BPR+, using policy distillation to achieve efficient online policy learning and reuse. Deep BPR+ inherits all the advantages of BPR+ and empirically shows better performance in terms of detection accuracy, cumulative rewards and speed of convergence compared to existing algorithms in complex Markov games with raw visual inputs.	[Zheng, Yan; Meng, Zhaopeng; Hao, Jianye; Yang, Tianpei] Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China; [Zhang, Zongzhang] Soochow Univ, Sch Comp Sci & Technol, Suzhou, Peoples R China; [Fan, Changjie] NetEase Inc, NetEase Fuxi Lab, Hangzhou, Zhejiang, Peoples R China	Tianjin University; Soochow University - China	Hao, JY (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.	yanzheng@tju.edu.cn; mengzp@tju.edu.cn; jianye.hao@tju.edu.cn; zzzhang@suda.edu.cn; tpyang@tju.edu.cn; fanchangjie@netease.com	Yang, Tianpei/AAO-3113-2021	ZHENG, YAN/0000-0003-2741-058X	National Natural Science Foundation of China [61702362, 61876119, 61502323]; Special Program of Artificial Intelligence, Tianjin Research Program of Application Foundation and Advanced Technology [16JCQNJC00100]; Special Program of Artificial Intelligence of Tianjin Municipal Science and Technology Commission [569 17ZXRGGX00150]; Science and Technology Program of Tianjin, China [15PT-CYSY00030, 16ZXHLGX00170]; Natural Science Foundation of Jiangsu [BK20181432]; High School Natural Foundation of Jiangsu [16KJB520041]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Special Program of Artificial Intelligence, Tianjin Research Program of Application Foundation and Advanced Technology; Special Program of Artificial Intelligence of Tianjin Municipal Science and Technology Commission; Science and Technology Program of Tianjin, China; Natural Science Foundation of Jiangsu(Natural Science Foundation of Jiangsu Province); High School Natural Foundation of Jiangsu	The work is supported by the National Natural Science Foundation of China (Grant Nos.: 61702362, 61876119, 61502323), Special Program of Artificial Intelligence, Tianjin Research Program of Application Foundation and Advanced Technology (No.: 16JCQNJC00100), Special Program of Artificial Intelligence of Tianjin Municipal Science and Technology Commission (No.: 569 17ZXRGGX00150), Science and Technology Program of Tianjin, China (Grant Nos. 15PT-CYSY00030, 16ZXHLGX00170), Natural Science Foundation of Jiangsu (No.: BK20181432), and High School Natural Foundation of Jiangsu (No.: 16KJB520041)	Albrecht SV, 2018, ARTIF INTELL, V258, P66, DOI 10.1016/j.artint.2018.01.002; Chalkiadakis Georgios, 2003, P 2 INT JOINT C AUT, P709; Crandall J.W., 2012, P 11 INT C AUT AG MU, V1, P399; Foerster J, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P122; Gupta J. K., 2017, AD LEARN AG WORKSH; He H, 2016, PR MACH LEARN RES, V48; Hernandez-Leal Pablo, 2017, Autonomous Agents and Multiagent Systems, AAMAS 2017: Workshops, Best Papers. Revised Selected Papers: LNAI 10642, P239, DOI 10.1007/978-3-319-71682-4_15; Hernandez-Leal P, 2017, 3 MULT C REINF LEARN; Hernandez- Leal Pablo, 2017, ABS170709183 CORR; HernandezLeal Pablo, 2016, P AAMAS, P1315; Hinton G., 2015, STAT-US, V1050, P9; Hong ZW, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P1388; Junling Hu, 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P242; Lillicrap Timothy P, 2016, 4 INT C LEARN REPR S, P1; Littman M.L, 1994, P 11 INT C INT C MAC, P157; Lowe R., 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295385; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Palmer G, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P443; Rosman B, 2016, MACH LEARN, V104, P99, DOI 10.1007/s10994-016-5547-y; Rusu Andrei A., 2015, ABS151106295 CORR; Schaul T., 2016, INT C LEARN REPR ICL; Tampuu A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0172395; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; Wang ZY, 2016, PR MACH LEARN RES, V48; Zhao Xiangyu, 2018, ABS180100209 CORR	26	23	24	3	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300088
C	Alaa, AM; van der Schaar, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Alaa, Ahmed M.; van der Schaar, Mihaela			Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Predicated on the increasing abundance of electronic health records, we investigate the problem of inferring individualized treatment effects using observational data. Stemming from the potential outcomes model, we propose a novel multi-task learning framework in which factual and counterfactual outcomes are modeled as the outputs of a function in a vector-valued reproducing kernel Hilbert space (vvRKHS). We develop a nonparametric Bayesian method for learning the treatment effects using a multi-task Gaussian process (GP) with a linear coregionalization kernel as a prior over the vvRKHS. The Bayesian approach allows us to compute individualized measures of confidence in our estimates via pointwise credible intervals, which are crucial for realizing the full potential of precision medicine. The impact of selection bias is alleviated via a risk-based empirical Bayes method for adapting the multi-task GP prior, which jointly minimizes the empirical error in factual outcomes and the uncertainty in (unobserved) counterfactual outcomes. We conduct experiments on observational datasets for an interventional social program applied to premature infants, and a left ventricular assist device applied to cardiac patients wait-listed for a heart transplant. In both experiments, we show that our method significantly outperforms the state-of-the-art.	[Alaa, Ahmed M.] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90024 USA; [van der Schaar, Mihaela] Univ Oxford, Dept Engn Sci, Oxford, England	University of California System; University of California Los Angeles; University of Oxford	Alaa, AM (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90024 USA.	ahmedmalaa@ucla.edu; mihaela.vanderschaar@eng.ox.ac.uk	Jeong, Yongwook/N-7413-2016	van der schaar, Mihaela/0000-0003-3933-6049				Abadie A, 2016, ECONOMETRICA, V84, P781, DOI 10.3982/ECTA11293; Adams CP, 2010, HEALTH ECON, V19, P130, DOI 10.1002/hec.1454; Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; Athey S, 2016, P NATL ACAD SCI USA, V113, P7353, DOI 10.1073/pnas.1510489113; Bickel S, 2009, J MACH LEARN RES, V10, P2137; Bonilla E.V., 2007, ADV NEURAL INF PROCE, V20, P601; Cawley GC, 2007, J MACH LEARN RES, V8, P841; Chernozhukov V, 2016, DOUBLE DEBIASED MACH; Dudk M., 2011, ICML; Foster JC, 2011, STAT MED, V30, P2867, DOI 10.1002/sim.4322; Hartford Jason, 2016, ARXIV E PRINTS; Hill J. L., 2012, J COMPUTATIONAL GRAP; Johansson F. D., 2016, ICML; Kingma D.P, P 3 INT C LEARNING R; Lu M., 2017, ARXIV170105306; Porter KE, 2011, INT J BIOSTAT, V7, DOI 10.2202/1557-4679.1308; Rasmussen C.E., 2006, CITESEER; Sauerbrei W, 2014, STAT MED, V33, P5413, DOI 10.1002/sim.6265; Schlkopf B., 2001, INT C COMP LEARN THE; Shalit Uri, 2016, ARXIV160603976; Slaughter MS, 2009, NEW ENGL J MED, V361, P2241, DOI 10.1056/NEJMoa0909938; Sniekers S, 2015, ELECTRON J STAT, V9, P2475, DOI 10.1214/15-EJS1078; Swaminathan A, 2015, J MACH LEARN RES, V16, P1731; Wager S, 2015, ARXIV151004342; Xie Y, 2012, SOCIOL METHODOL, V42, P314, DOI 10.1177/0081175012452652; Xu Y., 2016, ARXIV160805182	26	23	23	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403048
C	Gucluturk, Y; Guclu, U; Seeliger, K; Bosch, S; van Lier, R; van Gerven, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Gucluturk, Yagmur; Guclu, Umut; Seeliger, Katja; Bosch, Sander; van Lier, Rob; van Gerven, Marcel			Reconstructing perceived faces from brain activations with deep adversarial neural decoding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NATURAL IMAGES; REPRESENTATIONS; MODELS; SHAPE; ATTRACTIVENESS; INFORMATION; PERCEPTION; RESPONSES; NETWORKS; STREAM	Here, we present a novel approach to solve the problem of reconstructing perceived stimuli from brain responses by combining probabilistic inference with deep learning. Our approach first inverts the linear transformation from latent features to brain responses with maximum a posteriori estimation and then inverts the nonlinear transformation from perceived stimuli to latent features with adversarial training of convolutional neural networks. We test our approach with a functional magnetic resonance imaging experiment and show that it can generate state-of-the-art reconstructions of perceived faces from brain activations.	[Gucluturk, Yagmur; Guclu, Umut; Seeliger, Katja; Bosch, Sander; van Lier, Rob; van Gerven, Marcel] Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, Nijmegen, Netherlands	Radboud University Nijmegen	Gucluturk, Y (corresponding author), Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, Nijmegen, Netherlands.	y.gucluturk@donders.ru.nl; u.guclu@donders.ru.nl	van Lier, Rob/E-1654-2012; Bosch, Sander Erik/C-2927-2013; Jeong, Yongwook/N-7413-2016; Güçlü, Umut/AAX-6105-2020; Seeliger, K./Q-8676-2018	van Lier, Rob/0000-0002-4705-5725; Bosch, Sander Erik/0000-0001-6845-0911; Güçlü, Umut/0000-0003-4753-159X; Seeliger, K./0000-0002-5343-0595	VIDI grant from the Netherlands Organization for Scientific Research [639.072.513]; GPU grant (GeForce Titan X) from the Nvidia Corporation	VIDI grant from the Netherlands Organization for Scientific Research; GPU grant (GeForce Titan X) from the Nvidia Corporation	This work has been partially supported by a VIDI grant (639.072.513) from the Netherlands Organization for Scientific Research and a GPU grant (GeForce Titan X) from the Nvidia Corporation.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Birkas B, 2014, PERS INDIV DIFFER, V69, P56, DOI 10.1016/j.paid.2014.05.012; Carrito MD, 2016, EVOL HUM BEHAV, V37, P125, DOI 10.1016/j.evolhumbehav.2015.09.006; Cichy RM, 2016, SCI REP-UK, V6, DOI 10.1038/srep27755; Cowen AS, 2014, NEUROIMAGE, V94, P12, DOI 10.1016/j.neuroimage.2014.03.018; Donderi DC, 2005, DISPLAYS, V26, P71, DOI 10.1016/j.displa.2005.02.002; Dosovitskiy A., 2016, ARXIV161101779; Du C., 2017, CORR; Eickenberg M, 2017, NEUROIMAGE, V152, P184, DOI 10.1016/j.neuroimage.2016.10.001; Friston K, 2007, STATISTICAL PARAMETRIC MAPPING: THE ANALYSIS OF FUNCTIONAL BRAIN IMAGES, P10, DOI 10.1016/B978-012372560-8/50002-4; Goesaert E, 2013, J NEUROSCI, V33, P8549, DOI 10.1523/JNEUROSCI.1829-12.2013; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guclu U., 2013, BELG DUTCH C MACH LE; Guclu U., 2016, ADV NEURAL INFORM PR; Guclu U, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00007; Guclu U, 2017, NEUROIMAGE, V145, P329, DOI 10.1016/j.neuroimage.2015.12.036; Guclu U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015; Gucluturk Yagmur, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P810, DOI 10.1007/978-3-319-46604-0_56; Gucluturk Y, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00112; Hahn AC, 2014, NEUROSCI BIOBEHAV R, V46, P591, DOI 10.1016/j.neubiorev.2014.08.015; Haxby JV, 2001, SCIENCE, V293, P2425, DOI 10.1126/science.1063736; Horikawa T, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms15037; Jenkinson M, 2012, NEUROIMAGE, V62, P782, DOI 10.1016/j.neuroimage.2011.09.015; Jia Y., 2014, P 22 ACM INT C MULT, P675; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kamitani Y, 2005, NAT NEUROSCI, V8, P679, DOI 10.1038/nn1444; Kay KN, 2008, NATURE, V452, P352, DOI 10.1038/nature06713; Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915; Kingma D.P, P 3 INT C LEARNING R; Kriegeskorte N, 2015, ANNU REV VIS SCI, V1, P417, DOI 10.1146/annurev-vision-082114-035447; Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lee H, 2016, J NEUROSCI, V36, P6069, DOI 10.1523/JNEUROSCI.4286-15.2016; Little AC, 2014, BRIT J PSYCHOL, V105, P364, DOI 10.1111/bjop.12043; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Ma DS, 2015, BEHAV RES METHODS, V47, P1122, DOI 10.3758/s13428-014-0532-5; Mitchell TM, 2008, SCIENCE, V320, P1191, DOI 10.1126/science.1152876; Miyawaki Y, 2008, NEURON, V60, P915, DOI 10.1016/j.neuron.2008.11.004; Mumford JA, 2012, NEUROIMAGE, V59, P2636, DOI 10.1016/j.neuroimage.2011.08.076; Naselaris T, 2011, NEUROIMAGE, V56, P400, DOI 10.1016/j.neuroimage.2010.07.073; Naselaris T, 2009, NEURON, V63, P902, DOI 10.1016/j.neuron.2009.09.006; Nishimoto S, 2011, CURR BIOL, V21, P1641, DOI 10.1016/j.cub.2011.08.031; Parkhi O. M., 2016, BRIT MACH VIS C; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; PERRETT DI, 1994, NATURE, V368, P239, DOI 10.1038/368239a0; Petersen K. B., 2012, MATRIX COOKBOOK; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Schoenmakers S, 2015, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00173; Schoenmakers S, 2013, NEUROIMAGE, V83, P951, DOI 10.1016/j.neuroimage.2013.07.043; Strohminger N, 2016, BEHAV RES METHODS, V48, P1197, DOI 10.3758/s13428-015-0641-9; Strom MA, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0041193; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Thaler L, 2013, VISION RES, V76, P31, DOI 10.1016/j.visres.2012.10.012; Thirion B, 2006, NEUROIMAGE, V33, P1104, DOI 10.1016/j.neuroimage.2006.06.062; Tokui S., 2015, ADV NEUR INF PROC SY; van Gerven M., 2012, 2012 2 INT WORKSH PA; van Gerven MAJ, 2017, J MATH PSYCHOL, V76, P172, DOI 10.1016/j.jmp.2016.06.009; van Gerven MAJ, 2010, NEURAL COMPUT, V22, P3127, DOI 10.1162/NECO_a_00047; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40	63	23	23	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404031
C	Kanai, S; Fujiwara, Y; Iwamura, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kanai, Sekitoshi; Fujiwara, Yasuhiro; Iwamura, Sotetsu			Preventing Gradient Explosions in Gated Recurrent Units	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				STABILITY	A gated recurrent unit (GRU) is a successful recurrent neural network architecture for time-series data. The GRU is typically trained using a gradient-based method, which is subject to the exploding gradient problem in which the gradient increases significantly. This problem is caused by an abrupt change in the dynamics of the GRU due to a small variation in the parameters. In this paper, we find a condition under which the dynamics of the GRU changes drastically and propose a learning method to address the exploding gradient problem. Our method constrains the dynamics of the GRU so that it does not drastically change. We evaluated our method in experiments on language modeling and polyphonic music modeling. Our experiments showed that our method can prevent the exploding gradient problem and improve modeling accuracy.	[Kanai, Sekitoshi; Fujiwara, Yasuhiro; Iwamura, Sotetsu] NTT Software Innovat Ctr, 3-9-11 Midori Cho, Musashino, Tokyo, Japan		Kanai, S (corresponding author), NTT Software Innovat Ctr, 3-9-11 Midori Cho, Musashino, Tokyo, Japan.	kanai.sekitoshi@lab.ntt.co.jp; fujiwara.yasuhiro@lab.ntt.co.jp; iwamura.sotetsu@lab.ntt.co.jp	Jeong, Yongwook/N-7413-2016					Amodei D, 2016, PR MACH LEARN RES, V48; Arjovsky M, 2016, PR MACH LEARN RES, V48; Baldi P, 1996, ADV NEUR IN, V8, P451; Barabanov NE, 2002, IEEE T NEURAL NETWOR, V13, P292, DOI 10.1109/72.991416; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bengio Y., 2014, ARXIV14061078; Boulanger-Lewandowski N, 2012, P 29 INT C MACH LEAR, P1159; Chilali M, 1996, IEEE T AUTOMAT CONTR, V41, P358, DOI 10.1109/9.486637; Chung J., 2014, ARXIV14123555; Collins Jasmine, 2017, P ICLR; DOYA K, 1992, 1992 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS 1-6, P2777, DOI 10.1109/ISCAS.1992.230622; Doyon Bernard, 1993, P NIPS, P549; Ganguli S., 2014, INT C LEARN REPR; Graves A., 2008, ADV NEURAL INFORM PR, P545, DOI DOI 10.1007/978-1-4471-4072-6; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Haschke R, 2005, NEUROCOMPUTING, V64, P25, DOI 10.1016/j.neucom.2004.11.030; Hermans M., 2013, P ADV NEUR INF PROC, V26, P190; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jaeger H., 2002, TUTORIAL TRAINING RE, V5; Jozefowicz R, 2015, PR MACH LEARN RES, V37, P2342; Kingma D.P, P 3 INT C LEARNING R; Krueger David, 2016, P ICLR; KUAN CM, 1994, NEURAL COMPUT, V6, P420, DOI 10.1162/neco.1994.6.3.420; Laurent Thomas, 2017, P ICLR; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Mikolov T., 2012, GOOGLE; Nakahara H, 1996, ADV NEUR IN, V8, P38; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Suykens JAK, 2000, IEEE T NEURAL NETWOR, V11, P222, DOI 10.1109/72.822525; Talathi S. S., 2015, ARXIV PREPRINT ARXIV; Tang ZY, 2017, INT CONF ACOUST SPEE, P2736, DOI 10.1109/ICASSP.2017.7952654; TOKER O, 1995, PROCEEDINGS OF THE 1995 AMERICAN CONTROL CONFERENCE, VOLS 1-6, P2525; Vorontsov Eugene, 2017, P ICML; Wiggins S., 1990, INTRO APPL NONLINEAR; Yu W, 2004, INFORM SCIENCES, V158, P131, DOI 10.1016/j.ins.2003.08.002; Zaremba Wojciech, 2014, ABS14092329 CORR	38	23	23	0	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400042
C	Lai, YA; Hsu, CC; Chen, WH; Yeh, MY; Lin, SD		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lai, Yi-An; Hsu, Chin-Chi; Chen, Wen-Hao; Yeh, Mi-Yen; Lin, Shou-De			PRUNE: Preserving Proximity and Global Ranking for Network Embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We investigate an unsupervised generative approach for network embedding. A multi-task Siamese neural network structure is formulated to connect embedding vectors and our objective to preserve the global node ranking and local proximity of nodes. We provide deeper analysis to connect the proposed proximity objective to link prediction and community detection in the network. We show our model can satisfy the following design properties: scalability, asymmetry, unity and simplicity. Experiment results not only verify the above design properties but also demonstrate the superior performance in learning-to-rank, classification, regression, and link prediction tasks.	[Lai, Yi-An; Chen, Wen-Hao; Lin, Shou-De] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei, Taiwan; [Hsu, Chin-Chi; Yeh, Mi-Yen] Acad Sinica, Inst Informat Sci, Taipei, Taiwan	National Taiwan University; Academia Sinica - Taiwan	Lai, YA (corresponding author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei, Taiwan.	b99202031@ntu.edu.tw; chinchi@iis.sinica.edu.tw; b02902023@ntu.edu.tw; miyen@iis.sinica.edu.tw; sdlin@csie.ntu.edu.tw	Yeh, Mi-Yen/AAQ-4801-2021	Yeh, Mi-Yen/0000-0001-7707-4487	Ministry of Science and Technology (MOST) of Taiwan, R.O.C. [105-2628-E-001-002-MY2, 106-2628-E-006-005-MY3, 104-2628-E-002 -015 -MY3, 106-2218-E-002 -014 -MY4]; Air Force Office of Scientific Research; Asian Office of Aerospace Research and Development (AOARD) [FA2386-17-1-4038]; Microsoft [FY16-RES-THEME-021]	Ministry of Science and Technology (MOST) of Taiwan, R.O.C.; Air Force Office of Scientific Research(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); Asian Office of Aerospace Research and Development (AOARD); Microsoft(Microsoft)	This study was supported in part by the Ministry of Science and Technology (MOST) of Taiwan, R.O.C., under Contracts 105-2628-E-001-002-MY2, 106-2628-E-006-005-MY3, 104-2628-E-002 -015 -MY3 & 106-2218-E-002 -014 -MY4, Air Force Office of Scientific Research, Asian Office of Aerospace Research and Development (AOARD) under award number No.FA2386-17-1-4038, and Microsoft under Contracts FY16-RES-THEME-021. All opinions, findings, conclusions, and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.	Ahmed Amr, WWW 13; Cao Shaosheng, CIKM 15; Cao Shaosheng, AAAI 16; Djork-Arn, ICLR 2016; Glorot Xavier, AISTATS 11; Grover A., KDD 16; Heidemann Julia, ICIS 10; Huang Xiao, WSDM 17; Kingma D.P, P 3 INT C LEARNING R; Kleinberg JM, 1999, J ACM, V46, P604, DOI 10.1145/324133.324140; Leicht EA, 2008, PHYS REV LETT, V100, DOI 10.1103/PhysRevLett.100.118703; Levy Omer, NIPS 14; Menon Aditya Krishna, ECML PKDD 11; Mikolov Tomas, NIPS 13; Ou Mingdong, KDD 16; Page L., 1999, PAGERANK CITATION RA; Pan Shirui, IJCAI 16; Perozzi B., KDD 14; Song Han Hee, IMC 09; Tang Jian, WWW 15; Tang Lei, KDD 09; Tu Cunchao, IJCAI 16; Wang Daixin, KDD 16; Wang Xiao, AAAI 17; Wang Yujing, AAAI 13; Wei Xiaokai, WWW 17; Wu Zhizheng, 2015, ICASSP 15; Yang Cheng, IJCAI 15; Yang Jaewon, WSDM 13; Zhang DK, 2016, IEEE DATA MINING, P609, DOI [10.1109/ICDM.2016.0072, 10.1109/ICDM.2016.139]; Zhou Chang, AAAI 17; Zhu Shenghuo, SIGIR 07	32	23	24	5	13	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405033
C	Chiang, KY; Hsieh, CJ; Dhillon, IS		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Chiang, Kai-Yang; Hsieh, Cho-Jui; Dhillon, Inderjit S.			Matrix Completion with Noisy Side Information	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We study the matrix completion problem with side information. Side information has been considered in several matrix completion applications, and has been empirically shown to be useful in many cases. Recently, researchers studied the effect of side information for matrix completion from a theoretical viewpoint, showing that sample complexity can be significantly reduced given completely clean features. However, since in reality most given features are noisy or only weakly informative, the development of a model to handle a general feature set, and investigation of how much noisy features can help matrix recovery, remains an important issue. In this paper, we propose a novel model that balances between features and observations simultaneously in order to leverage feature information yet be robust to feature noise. Moreover, we study the effect of general features in theory and show that by using our model, the sample complexity can be lower than matrix completion as long as features are sufficiently informative. This result provides a theoretical insight into the usefulness of general side information. Finally, we consider synthetic data and two applications - relationship prediction and semi-supervised clustering - and show that our model outperforms other methods for matrix completion that use features both in theory and practice.	[Chiang, Kai-Yang; Dhillon, Inderjit S.] Univ Texas Austin, Austin, TX 78712 USA; [Hsieh, Cho-Jui] Univ Calif Davis, Davis, CA 95616 USA	University of Texas System; University of Texas Austin; University of California System; University of California Davis	Chiang, KY (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	kychiang@cs.utexas.edu; chohsieh@ucdavis.edu; inderjit@cs.utexas.edu			NSF [CCF-1320746, CCF-1117055]	NSF(National Science Foundation (NSF))	We thank David Inouye and Hsiang-Fu Yu for helpful comments and discussions. This research was supported by NSF grants CCF-1320746 and CCF-1117055.	Abernethy J, 2009, J MACH LEARN RES, V10, P803; [Anonymous], 2010, NIPS; [Anonymous], 2006, P ECAI WORKSHOP RECO; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Chandrasekaran V., 2012, ANN STAT; Chen TQ, 2012, J MACH LEARN RES, V13, P3619; Chen Y., 2014, ICML; Chen YD, 2014, J MACH LEARN RES, V15, P2213; Chiang KY, 2014, J MACH LEARN RES, V15, P1177; Feige U, 2002, RANDOM STRUCT ALGOR, V20, P403, DOI 10.1002/rsa.10036; Grippo L, 1999, OPTIM METHOD SOFTW, V10, P587, DOI 10.1080/10556789908805730; Hsieh Cho-Jui, 2012, KDD; Hsieh Cho-Jui, 2014, ICML; Jain P., 2013, ABS13060626 CORR; Kakade Sham M., 2008, NIPS; Keshavan R., 2010, JMLR; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Laurent B, 2000, ANN STAT, V28, P1302; Leskovec J., 2010, WWW; Li Z., 2009, ICCV; Meir R., 2003, JMLR; Menon A. K., 2011, P 17 ACM SIGKDD INT, P141, DOI DOI 10.1145/2020408.2020436; Natarajan N, 2014, BIOINFORMATICS, V30, P60, DOI 10.1093/bioinformatics/btu269; Negahban S, 2012, J MACH LEARN RES, V13, P1665; Rudelson M, 2009, COMMUN PUR APPL MATH, V62, P1707, DOI 10.1002/cpa.20294; Shamir O, 2014, J MACH LEARN RES, V15, P3401; Shin D., 2015, P 24 ACM INT C INF K, P203; SREBRO N, 2005, COLT, V3559, P545; Xu M., 2013, NIPS; Yang E., 2013, NIPS; Yi J., 2013, ICML; Zhong K., 2015, INT C ALG LEARN THEO	38	23	23	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102088
C	Wu, HS; Srikant, R; Liu, X; Jiang, C		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Wu, Huasen; Srikant, R.; Liu, Xin; Jiang, Chong			Algorithms with Logarithmic or Sublinear Regret for Constrained Contextual Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We study contextual bandits with budget and time constraints, referred to as constrained contextual bandits. The time and budget constraints significantly complicate the exploration and exploitation tradeoff because they introduce complex coupling among contexts over time. To gain insight, we first study unit-cost systems with known context distribution. When the expected rewards are known, we develop an approximation of the oracle, referred to Adaptive-Linear-Programming ( ALP), which achieves near-optimality and only requires the ordering of expected rewards. With these highly desirable features, we then combine ALP with the upper-confidence-bound (UCB) method in the general case where the expected rewards are unknown a priori. We show that the proposed UCB-ALP algorithm achieves logarithmic regret except for certain boundary cases. Further, we design algorithms and obtain similar regret bounds for more general systems with unknown context distribution and heterogeneous costs. To the best of our knowledge, this is the first work that shows how to achieve logarithmic regret in constrained contextual bandits. Moreover, this work also sheds light on the study of computationally efficient algorithms for general constrained contextual bandits.	[Wu, Huasen; Liu, Xin] Univ Calif Davis, Davis, CA 95616 USA; [Srikant, R.; Jiang, Chong] Univ Illinois, Champaign, IL USA	University of California System; University of California Davis; University of Illinois System; University of Illinois Urbana-Champaign	Wu, HS (corresponding author), Univ Calif Davis, Davis, CA 95616 USA.	hswu@ucdavis.edu; rsrikant@illinois.edu; liu@cs.ucdavis.edu; jiang17@illinois.edu			NSF [CCF-1423542, CNS-1457060, CNS-1547461]; AFOSR MURI [FA 9550-10-1-0573]	NSF(National Science Foundation (NSF)); AFOSR MURI(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)MURI)	This research was supported in part by NSF Grants CCF-1423542, CNS-1457060, CNS-1547461, and AFOSR MURI Grant FA 9550-10-1-0573.	Agrawal S, 2014, P 15 ACM C EC COMPUT, P989, DOI [10.1145/2600057.2602844, DOI 10.1145/2600057.2602844.URL]; Agrawal S., 2015, ARXIV150603374; Agrawal S., 2015, ARXIV150706738; Ashwinkumar B., 2012, P 13 ACM C ELECT COM, P128; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Badanidiyuru A., 2014, C LEARN THEOR COLT; Badanidiyuru A, 2013, ANN IEEE SYMP FOUND, P207, DOI 10.1109/FOCS.2013.30; Combes Richard, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P231; Combes R, 2014, IEEE INFOCOM SER, P2760, DOI 10.1109/INFOCOM.2014.6848225; Dubhashi DP, 2009, CONCENTRATION OF MEASURE FOR THE ANALYSIS OF RANDOMIZED ALGORITHMS, P1, DOI 10.1017/CBO9780511581274; Garivier A., 2011, P 24 ANN C LEARNING, V24, P359; Golovin D., 2009, DEALING PARTIAL FEED; Jiang C, 2013, IEEE DECIS CONTR P, P5345, DOI 10.1109/CDC.2013.6760730; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lai TL, 2012, SEQUENTIAL ANAL, V31, P441, DOI 10.1080/07474946.2012.719433; Langford John, 2007, ADV NEURAL INFORM PR, V20, P817; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; LU T., 2010, PROC 30 INT C ARTIF, P485; Ortner P., 2007, ADV NEURAL INFORM PR, V19, P49; Slivkins A., 2013, ARXIV13060155; Tran- Thanh L., 2012, AAAI C ART INT; Veatch MH, 2013, MATH OPER RES, V38, P535, DOI 10.1287/moor.1120.0574; Xia Y., 2015, INT JOINT C ART INT; Zhou L., 2015, ARXIV	26	23	24	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103046
C	McMahan, HB; Streeter, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		McMahan, H. Brendan; Streeter, Matthew			Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We analyze new online gradient descent algorithms for distributed systems with large delays between gradient computations and the corresponding updates. Using insights from adaptive gradient methods, we develop algorithms that adapt not only to the sequence of gradients, but also to the precise update delays that occur. We first give an impractical algorithm that achieves a regret bound that precisely quantifies the impact of the delays. We then analyze AdaptiveRevision, an algorithm that is efficiently implementable and achieves comparable guarantees. The key algorithmic technique is appropriately and efficiently revising the learning rate used for previous gradient steps. Experimental results show when the delays grow large (1000 updates or more), our new algorithms perform significantly better than standard adaptive gradient methods.	[McMahan, H. Brendan; Streeter, Matthew] Google Inc, Seattle, WA 98103 USA; [Streeter, Matthew] Duolingo Inc, Pittsburgh, PA USA	Google Incorporated	McMahan, HB (corresponding author), Google Inc, Seattle, WA 98103 USA.	mcmahan@google.com; matt@duolingo.com						Auer Peter, 2002, J COMPUTER SYSTEM SC; Bottou L., 2008, ADV NEURAL INFORM PR; Chang Chih- Chung, LIBSVM DATA SETS; Dean J., 2012, ADV NEURAL INFORM PR, V25; Dekel Ofer, 2012, J MACH LEARN RES, V13; Duchi J., 2010, COLT; Duchi J. C., 2013, NIPS; Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027; Graepel T., 2010, ICML; Hsu D., 2011, SCALING MACHINE LEAR; Langford John, 2009, ADV NEURAL INFORM PR, V22; Ma J., 2009, P 26 ANN INT C MACH; McMahan H. B., 2013, KDD; McMahan H Brendan, 2010, COLT; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Niu F., 2011, NIPS 11; Richtarik Peter, 2012, ARXIV12120873MATHOC; Shalev-Shwartz Shai, 2012, FDN TRENDS MACHINE L; TAKAC M., 2013, P 30 INT C MACH LEAR; Zhang T., 2004, ICML 2004; Zinkevich M, 2003, ICML	21	23	23	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100021
C	Narayanan, S; Jurafsky, D		Dietterich, TG; Becker, S; Ghahramani, Z		Narayanan, S; Jurafsky, D			A Bayesian model predicts human parse preference and reading times in sentence processing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				SYNTACTIC AMBIGUITY RESOLUTION	Narayanan and Jurafsky (1998) proposed that human language comprehension can be modeled by treating human comprehenders as Bayesian reasoners, and modeling the comprehension process with Bayesian decision trees. In this paper we extend the Narayanan and Jurafsky model to make further predictions about reading time given the probability of difference parses or interpretations, and test the model against reading time data from a psycholinguistic experiment.	SRI Int, Berkeley, CA USA	SRI International	Narayanan, S (corresponding author), SRI Int, Berkeley, CA USA.							FRAZIER L, 1987, J MEM LANG, V26, P505, DOI 10.1016/0749-596X(87)90137-9; HALE J, 2001, P NAACL 2001; JENSEN F, 1995, BAYESIAN NETWORKS; Jurafsky D, 1996, COGNITIVE SCI, V20, P137, DOI 10.1207/s15516709cog2002_1; MACDONALD MC, 1994, PSYCHOL REV, V101, P676, DOI 10.1037/0033-295X.101.4.676; McRae K, 1998, J MEM LANG, V38, P283, DOI 10.1006/jmla.1997.2543; NARAYANAN S, 1998, COGSCI 98, P752; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Spivey-Knowlton M, 1996, THESIS U ROCHESTER; TRUESWELL JC, 1993, J EXP PSYCHOL LEARN, V19, P528, DOI 10.1037/0278-7393.19.3.528; Trueswell JC, 1996, J MEM LANG, V35, P566, DOI 10.1006/jmla.1996.0030	11	23	23	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						59	65						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100008
C	Bousquet, O; Elisseeff, A		Leen, TK; Dietterich, TG; Tresp, V		Bousquet, O; Elisseeff, A			Algorithmic stability and generalization performance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We present a novel way of obtaining PAC-style bounds on the generalization error of learning algorithms, explicitly using their stability properties. A stable learner is one for which the learned solution does not change much with small changes in the training set. The bounds we obtain do not depend on any measure of the complexity of the hypothesis space (e.g. VC dimension) but rather depend on how the learning algorithm searches this space, and can thus be applied even when the VC dimension is infinite. We demonstrate that regularization networks possess the required stability property and apply our method to obtain new bounds on their generalization performance.	Ecole Polytech, CMAP, F-91128 Palaiseau, France	Institut Polytechnique de Paris	Bousquet, O (corresponding author), Ecole Polytech, CMAP, F-91128 Palaiseau, France.	bousquet@cmapx.polytechnique.fr; andre@barnhilltechnologies.com						ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI DOI 10.2307/1990404; ATTEIA M, 1992, STUDIES COMPUTATIONA, V4; DEVROYE LP, 1979, IEEE T INFORM THEORY, V25, P202, DOI 10.1109/TIT.1979.1056032; ELISSEEFF A, 2000, STUDY ALGORITHMIC ST; EVGENIOU T, 1999, AIM1654 MIT ART INT; Freund Y., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P247, DOI 10.1145/279943.279993; MCDIARMID C, 1989, LOND MATH S, V141, P148; POGGIO T, 1990, SCIENCE, V247, P978, DOI 10.1126/science.247.4945.978; Shawe-Taylor J., 1996, Proceedings of the Ninth Annual Conference on Computational Learning Theory, P68, DOI 10.1145/238061.238070; Shawe-Taylor J, 1999, LECT NOTES ARTIF INT, V1572, P274	12	23	23	1	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						196	202						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800028
C	Lodhi, H; Shawe-Taylor, J; Cristianini, N		Leen, TK; Dietterich, TG; Tresp, V		Lodhi, H; Shawe-Taylor, J; Cristianini, N			Text classification using string kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We introduce a novel kernel for comparing two text documents. The kernel is an inner product in the feature space consisting of all subsequences of length k. A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences which are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. A preliminary experimental comparison of the performance of the kernel compared with a standard word feature space kernel [6] is made showing encouraging results.	Univ London, Royal Holloway & Bedford New Coll, Dept Comp Sci, Egham TW20 0EX, Surrey, England	University of London; Royal Holloway University London	Lodhi, H (corresponding author), Univ London, Royal Holloway & Bedford New Coll, Dept Comp Sci, Egham TW20 0EX, Surrey, England.			Shawe-Taylor, John/0000-0002-2030-0073				Aizerman M. A., 1964, AUTOMAT REM CONTR, V25, P821, DOI DOI 10.1234/12345678; BOSER B, 1992, P 5 ANN WORKSH COMP, V5, P144; Cristianini N., 2000, INTRO SUPPORT VECTOR; HAUSSLER D, 1999, UCSCCRL9910 COMP SCI; Joachims T., 1998, P EUR C MACH LEARN E; Joachims T., 1997, 23 U DORTM LS 8; LEWIS D, 1987, REUTERS 21578 COLLEC; SHAWETAYLOR J, 2000, ADV LARGE MARGIN CLA; SHAWETAYLOR J, 1998, EEE T INF THEOR; Vapnik V.N, 1998, STAT LEARNING THEORY; WATKINS C, 1999, CSDTR9811 U LOND ROY	11	23	23	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						563	569						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800080
C	Blake, A; North, B; Isard, M		Kearns, MS; Solla, SA; Cohn, DA		Blake, A; North, B; Isard, M			Learning multi-class dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				ALGORITHM	Standard techniques (eg. Yule-Walker) are available for learning Auto-Regressive process models of simple, directly observable, dynamical processes. When sensor noise means that dynamics are observed only approximately, learning can still been achieved via Expectation-Maximisation (EM) together with Kalman Filtering. However, this does not handle more complex dynamics, involving multiple classes of motion. For that problem, we show here how EM can be combined with the CONDENSATION algorithm, which is based on propagation of random sample-sets. Experiments have been performed with visually observed juggling, and plausible dynamical models are found to emerge from the learning process.	Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England	University of Oxford	Blake, A (corresponding author), Univ Oxford, Dept Engn Sci, S Parks Rd, Oxford OX1 3PJ, England.							Blake A, 1997, ADV NEUR IN, V9, P361; BREGLER C, 1997, P C COMP VIS PATT RE; Brockwell P., 1996, INTRO TIME SERIES FO; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Gelb A., 1974, APPL OPTIMAL ESTIMAT; Kitagawa Genshiro, 2021, J COMPUT GRAPH STAT, V5, P1, DOI [DOI 10.2307/1390750, 10.2307/1390750]; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Ljung L., 1987, SYSTEM IDENTIFICATIO; North B, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P384, DOI 10.1109/ICCV.1998.710747; Pardey J, 1996, MED ENG PHYS, V18, P2, DOI 10.1016/1350-4533(95)00024-0; Rabiner L., 1993, FUNDAMENTALS SPEECH; Shumway R. H., 1982, Journal of Time Series Analysis, V3, P253, DOI 10.1111/j.1467-9892.1982.tb00349.x	13	23	23	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						389	395						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700055
C	Kappen, HJ; Rodriguez, FB		Jordan, MI; Kearns, MJ; Solla, SA		Kappen, HJ; Rodriguez, FB			Boltzmann Machine learning using mean field theory and linear response correction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We present a new approximate learning algorithm for Boltzmann Machines, using a systematic expansion of the Gibbs free energy to second order in the weights. The linear response correction to the correlations is given by the Hessian of the Gibbs free energy. The computational complexity of the algorithm is cubic in the number of neurons. We compare the performance of the exact BM learning algorithm with first order (Weiss) mean field theory and second order (TAP) mean field theory. The learning task consists of a fully connected Ising spin glass model on 10 neurons. We conclude that 1) the method works well for paramagnetic problems 2) the TAP correction gives a significant improvement over the Weiss mean field theory, both for paramagnetic and spin glass problems and 3) that the inclusion of diagonal weights improves the Weiss approximation for paramagnetic problems, but not for spin glass problems.	Catholic Univ Nijmegen, Dept Biophys, NL-6525 EZ Nijmegen, Netherlands	Radboud University Nijmegen	Kappen, HJ (corresponding author), Catholic Univ Nijmegen, Dept Biophys, Geert Grooteplein 21, NL-6525 EZ Nijmegen, Netherlands.		Kappen, H.J./L-4425-2015; de Borja Rodriguez, Francisco/F-7812-2013	de Borja Rodriguez, Francisco/0000-0003-4053-099X					0	23	23	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						280	286						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700040
C	THRUN, SB; MOLLER, K		MOODY, JE; HANSON, SJ; LIPPMANN, RP		THRUN, SB; MOLLER, K			ACTIVE EXPLORATION IN DYNAMIC ENVIRONMENTS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	23	23	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						531	538						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00065
C	Balunovic, M; Baader, M; Singh, G; Gehr, T; Vechev, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Balunovic, Mislav; Baader, Maximilian; Singh, Gagandeep; Gehr, Timon; Vechev, Martin			Certifying Geometric Robustness of Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The use of neural networks in safety-critical computer vision systems calls for their robustness certification against natural geometric transformations (e.g., rotation, scaling). However, current certification methods target mostly norm-based pixel perturbations and cannot certify robustness against geometric transformations. In this work, we propose a new method to compute sound and asymptotically optimal linear relaxations for any composition of transformations. Our method is based on a novel combination of sampling and optimization. We implemented the method in a system called DEEPG and demonstrated that it certifies significantly more complex geometric transformations than existing methods on both defended and undefended networks while scaling to large architectures.	[Balunovic, Mislav; Baader, Maximilian; Singh, Gagandeep; Gehr, Timon; Vechev, Martin] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Balunovic, M (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	mislav.balunovic@inf.ethz.ch; mbaader@inf.ethz.ch; gsingh@inf.ethz.ch; timon.gehr@inf.ethz.ch; martin.vechev@inf.ethz.ch						Alaifari R., 2019, INT C LEARN REPR ICL; [Anonymous], INT C LEARN REPR ICL; Biggio B., 2013, EUR C MACH LEARN KNO; Bunel R., 2018, ADV NEURAL INFORM PR; Cohen Jeremy M, 2019, INT C MACH LEARN ICM; Cousot P., 1978, S PRINC PROGR LANG P; de Weerdt E., 2009, IEEE T NEURAL NETWOR; Dvijotham K., 2018, UNCERTAINTY ARTIFICI; Ehlers Ruediger, 2017, AUTOMATED TECHNOLOGY; Engstrom L., 2017, NIPS 2017 WORKSH MAC; Fawzi Alhussein, 2017, GEOMETRIC PERSPECTIV; Gehr T, 2018, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2018.00058; Goodfellow I., 2009, ADV NEURAL INFORM PR; Hansen P., 1995, HDB GLOBAL OPTIMIZAT; Hinton G. E., 2011, ARTIFICIAL NEURAL NE; Jaderberg Max, 2015, ADV NEURAL INFORM PR; Jana Suman, 2017, ARXIV PREPRINT ARXIV; Kanbak C., 2018, IEEE C COMP VIS PATT; Katz G., 2017, COMPUTER AIDED VERIF; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; LeCun Y, 2010, ATT LAB; Lecuyer M., 2019, IEEE S SEC PRIV SP; Li B., 2018, ARXIV180903113, P1; Madry Aleksander, 2018, 6 INT C LEARN REPR I; Mirman M., 2019, ARXIV190312519; Mirman M., 2018, INT C MACH LEARN ICM; Qin C., 2019, INT C LEARN REPR ICL; Raghunathan A., 2018, ADV NEURAL INFORM PR; Ruan W., 2018, INT JOINT C ART INT; Salman Hadi, 2019, ADV NEURAL INFORM PR; Singh G., 2017, PRINCIPLES PROGRAMMI; Singh G., 2019, S PRINC PROGR LANG P; Singh G., 2019, INT C LEARN REPR ICL; Singh G., 2018, ADV NEURAL INFORM PR; Szegedy Christian, 2014, PROC 2 INT C LEARN R; Wang S., 2018, USENIX SEC S; Wang S., 2018, ADV NEURAL INFORM PR; Weng T., 2018, INT C MACH LEARN ICM; Wong Eric, 2018, INT C MACH LEARN ICM; Xiao Chaowei, 2018, INT C LEARN REPR ICL; Xiao H., 2017, ARXIV170807747; Zhang H., 2018, ADV NEURAL INFORM PR	42	22	23	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907002
C	d'Autume, CD; Ruder, S; Kong, LP; Yogatama, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		d'Autume, Cyprien de Masson; Ruder, Sebastian; Kong, Lingpeng; Yogatama, Dani			Episodic Memory in Lifelong Language Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly (similar to 50-90%) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction.	[d'Autume, Cyprien de Masson; Ruder, Sebastian; Kong, Lingpeng; Yogatama, Dani] DeepMind, London, England		d'Autume, CD (corresponding author), DeepMind, London, England.	cyprien@google.com; ruder@google.com; lingpenk@google.com; dyogatama@google.com						Chaudhry A, 2019, P ICLR; Chaudhry A., 2018, P ECCV; Choi Eunsol, 2018, P 2018 C EMP METH NA, P2174, DOI [10.18653/v1/D18-1241, DOI 10.18653/V1/D18-1241]; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Howard J, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P328; Joshi Mandar S., 2017, P ACL; Kingma D.P, P 3 INT C LEARNING R; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Kitaev Nikita, 2018, P ACL; Lee K., 2018, P NAACL; Lopez-Paz D, 2017, ADV NEUR IN, V30; McCann B., 2018, ARXIV180608730; McGaugh JL, 2000, SCIENCE, V287, P248, DOI 10.1126/science.287.5451.248; Miconi T., 2018, P ICML; Peters Matthew E., 2018, P 2018 C N AM CHAPT, V1, P2227, DOI DOI 10.18653/V1/N18-1202; Rajpurkar P., 2016, CORR, P2383, DOI [10.18653/v1/D16-1264, DOI 10.18653/V1/D16-1264]; Ramalho T., 2019, P ICLR; Ruder Sebastian, 2017, ARXIV170605098; Schwarz J, 2018, PR MACH LEARN RES, V80; Sprechmann P., 2018, P ICLR; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Toneva M., 2019, P ICLR; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang H., 2019, P NAACL; Yogatama D., 2019, ARXIV190111373; Zenke Friedemann, 2017, P ICML, P2; Zhang Xiang, 2015, ADV NEURAL INFORM PR, P649, DOI DOI 10.5555/2969239.2969312	29	22	22	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904074
C	Dai, WZ; Xu, QL; Yu, Y; Zhou, ZH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dai, Wang-Zhou; Xu, Qiuling; Yu, Yang; Zhou, Zhi-Hua			Bridging Machine Learning and Logical Reasoning by Abductive Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Perception and reasoning are two representative abilities of intelligence that are integrated seamlessly during human problem-solving processes. In the area of artificial intelligence (AI), the two abilities are usually realised by machine learning and logic programming, respectively. However, the two categories of techniques were developed separately throughout most of the history of AI. In this paper, we present the abductive learning targeted at unifying the two AI paradigms in a mutually beneficial way, where the machine learning model learns to perceive primitive logic facts from data, while logical reasoning can exploit symbolic domain knowledge and correct the wrongly perceived facts for improving the machine learning models. Furthermore, we propose a novel approach to optimise the machine learning model and the logical reasoning model jointly. We demonstrate that by using abductive learning, machines can learn to recognise numbers and resolve unknown mathematical operations simultaneously from images of simple hand-written equations. Moreover, the learned models can be generalised to longer equations and adapted to different tasks, which is beyond the capability of state-of-the-art deep learning models.	[Dai, Wang-Zhou; Xu, Qiuling; Yu, Yang; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China; [Dai, Wang-Zhou; Xu, Qiuling] Imperial Coll London, London, England; [Dai, Wang-Zhou; Xu, Qiuling] Purdue Univ, W Lafayette, IN 47907 USA	Nanjing University; Imperial College London; Purdue University System; Purdue University; Purdue University West Lafayette Campus	Dai, WZ (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.; Dai, WZ (corresponding author), Imperial Coll London, London, England.; Dai, WZ (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.	daiwz@lamda.nju.edu.cn; xuql@lamda.nju.edu.cn; yuy@lamda.nju.edu.cn; zhouzh@lamda.nju.edu.cn			National Key R&D Program of China [2018YFB1004300]; NSFC [61751306]; Collaborative Innovation Centre of Novel Software Technology	National Key R&D Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Collaborative Innovation Centre of Novel Software Technology	This research was supported by the National Key R&D Program of China (2018YFB1004300), NSFC (61751306), and the Collaborative Innovation Centre of Novel Software Technology and Industrialisation. The authors would like to thank Yu-Xuan Huang and Le-Wen Cai for their help on experiments, and thank the reviewers for their insightful comments.	Bosnjak M, 2017, PR MACH LEARN RES, V70; BOWDITCH CP, 1910, NUMERATION CALENDAR; Carlsson M., 1997, Programming Languages: Implementations, Logics, and Programs. 9th International Symposium, PLILP'97, Including a Special Track on Declarative Programming Languages in Education. Proceedings, P191, DOI 10.1007/BFb0033845; Dai WZ, 2017, AAAI CONF ARTIF INTE, P4392; De Raedt L, 2015, MACH LEARN, V100, P5, DOI 10.1007/s10994-015-5494-z; Evans R, 2018, J ARTIF INTELL RES, V61, P1; Flach P. A., 2000, ABDUCTION INDUCTION; Fruhwirth T, 1998, J LOGIC PROGRAM, V37, P95, DOI 10.1016/S0743-1066(98)10005-5; Garcez A.S.d., 2012, NEURAL SYMBOLIC LEAR; Garcez ASD, 2007, TOPOI-INT REV PHILOS, V26, P37, DOI 10.1007/s11245-006-9005-5; Gaunt AL, 2017, PR MACH LEARN RES, V70; Getoor Lise, 2007, INTRO STAT RELATIONA; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Holldobler S., 2011, 2011 AAAI SPRINS S; Houston Stephen, 2001, DECIPHERMENT ANCIENT; JAFFAR J, 1994, J LOGIC PROGRAM, V20, P503, DOI 10.1016/0743-1066(94)90033-7; Jia Robin, 2017, P 2017 C EMP METH NA, P2021, DOI DOI 10.18653/V1/D17-1215; Kakas A. C., 1992, Journal of Logic and Computation, V2, P719, DOI 10.1093/logcom/2.6.719; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulkarni TD, 2015, PROC CVPR IEEE, P4390, DOI 10.1109/CVPR.2015.7299068; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Magnani L., 2009, ABDUCTIVE COGNITION; Manhaeve Robin, 2018, INT C NEURAL INFORM, P3753; Muggleton S., 1991, New Generation Computing, V8, P295, DOI 10.1007/BF03037089; Muggleton SH, 2015, MACH LEARN, V100, P49, DOI 10.1007/s10994-014-5471-y; NEWELL A, 1956, IRE T INFORM THEOR, V2, P61, DOI 10.1109/tit.1956.1056797; Peirce S. C., 1955, PHILOS WRITINGS PEIR; Ray O, 2009, J APPL LOGIC, V7, P329, DOI 10.1016/j.jal.2008.10.007; Russell S, 2015, COMMUN ACM, V58, P88, DOI 10.1145/2699411; Santoro A, 2017, ADV NEUR IN, V30; Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093; SIMON HA, 1971, AM PSYCHOL, V26, P145, DOI 10.1037/h0030806; Solso RobertL, 2013, COGNITIVE PSYCHOL; Tenenbaum JB, 2011, SCIENCE, V331, P1279, DOI 10.1126/science.1192788; Thoma M., 2017, ARXIV PREPRINT ARXIV; THOMPSON CA, 1994, PROCEEDINGS OF THE TWELFTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 AND 2, P664; TOWELL GG, 1994, ARTIF INTELL, V70, P119, DOI 10.1016/0004-3702(94)90105-8; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Yu Y, 2016, AAAI CONF ARTIF INTE, P2286; ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X; Zhou ZH, 2019, SCI CHINA INFORM SCI, V62, DOI 10.1007/s11432-018-9801-4	42	22	22	2	12	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302077
C	De Brouwer, E; Simm, J; Arany, A; Moreau, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		De Brouwer, Edward; Simm, Jaak; Arany, Adam; Moreau, Yves			GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				IMPUTATION; PREDICTION; VALUES	Modeling real-world multidimensional time series can be particularly challenging when these are sporadically observed (i.e., sampling is irregular both in time and across dimensions)-such as in the case of clinical patient data. To address these challenges, we propose (1) a continuous-time version of the Gated Recurrent Unit, building upon the recent Neural Ordinary Differential Equations (Chen et al., 2018), and (2) a Bayesian update network that processes the sporadic observations. We bring these two ideas together in our GRU-ODE-Bayes method. We then demonstrate that the proposed method encodes a continuity prior for the latent process and that it can exactly represent the Fokker-Planck dynamics of complex processes driven by a multidimensional stochastic differential equation. Additionally, empirical evaluation shows that our method outperforms the state of the art on both synthetic data and real-world data with applications in healthcare and climate forecast. What is more, the continuity prior is shown to be well suited for low number of samples settings.	[De Brouwer, Edward; Simm, Jaak; Arany, Adam; Moreau, Yves] Katholieke Univ Leuven, ESAT STADIUS, B-3001 Leuven, Belgium	KU Leuven	De Brouwer, E (corresponding author), Katholieke Univ Leuven, ESAT STADIUS, B-3001 Leuven, Belgium.	edward.debrouwer@esat.kuleuven.be; jaak.simm@esat.kuleuven.be; adam.arany@esat.kuleuven.be; moreau@esat.kuleuven.be		Moreau, Yves/0000-0002-4647-6560	FWO-SB grant; Research Council KU Leuven [C14/18/092]; Innovative Medicines Initiative: MELLODY; Flemish Government (ELIXIR Belgium, IWT) [FWO 06260]; Impulsfonds AI [VR 2019 2203 DOC.0318/1QUATER]; Research Foundation -Flanders (FWO); Flemish Government -department EWI; CELSA-HIDUCTION	FWO-SB grant; Research Council KU Leuven(KU Leuven); Innovative Medicines Initiative: MELLODY; Flemish Government (ELIXIR Belgium, IWT); Impulsfonds AI; Research Foundation -Flanders (FWO)(FWO); Flemish Government -department EWI; CELSA-HIDUCTION	Edward De Brouwer is funded by a FWO-SB grant. Yves Moreau is funded by (1) Research Council KU Leuven: C14/18/092 SymBioSys3; CELSA-HIDUCTION, (2) Innovative Medicines Initiative: MELLODY, (3) Flemish Government (ELIXIR Belgium, IWT, FWO 06260) and (4) Impulsfonds AI: VR 2019 2203 DOC.0318/1QUATER Kenniscentrum Data en Maatschappij. Computational resources and services used in this work were provided by the VSC (Flemish Supercomputer Center), funded by the Research Foundation -Flanders (FWO) and the Flemish Government -department EWI. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.	Baytas IM, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P65, DOI 10.1145/3097983.3097997; Bonilla EV., 2008, ADV NEURAL INF PROCE, V20, P153, DOI DOI 10.5555/2981562.2981582; Cao W., 2018, P 32 INT C NEUR INF, P6776; Chang Bo, 2019, ARXIV190209689; Che ZP, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-24271-9; Chen T. Q., 2018, ADV NEURAL INFORM PR, V2018; Cheng L., 2017, ARXIV170309112; Cho K., 2014, ABS14061078 CORR; Choi E, 2016, ADV NEUR IN, V29; Choi Edward, 2016, JMLR Workshop Conf Proc, V56, P301; Chung J., 2014, DEEP LEARN WORKSH C; De Brouwer E., 2018, ARXIV181110501; Du N, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1555, DOI 10.1145/2939672.2939875; Esteva A, 2019, NAT MED, V25, P24, DOI 10.1038/s41591-018-0316-z; Futoma J, 2017, ARXIV170604152; Garnelo M, 2018, ARXIV180701622; Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015; Ghassemi M, 2015, AAAI CONF ARTIF INTE, P446; Goldstein BA, 2017, J AM MED INFORM ASSN, V24, P198, DOI 10.1093/jamia/ocw042; Jensen AB, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5022; Johnson AEW, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.35; Junninen H, 2004, ATMOS ENVIRON, V38, P2895, DOI 10.1016/j.atmosenv.2004.02.026; Kingma DP, 2 INT C LEARN REPR I, P1; Krishnan RG, 2017, AAAI CONF ARTIF INTE, P2101; Lee C., 2017, HDB LARGE SCALE DIST, P11; Lipton Zachary C, 2016, MACHINE LEARNING HEA, P253; Mei H., 2017, ADV NEURAL INFORM PR, P6754; Menne M., LONG TERM DAILY CLIM; Mitchell TM, 1999, COMMUN ACM, V42, P30, DOI 10.1145/319382.319388; Moor M, 2019, ARXIV190201659; Nodelman U., 2002, P 18 C UNCERTAINTY A, P378; Prigogine I., 1982, BEING BECOMING; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rajkomar A, 2018, NPJ DIGIT MED, V1, DOI 10.1038/s41746-018-0029-1; Razavian N., 2015, ARXIV151107938; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Risken H., 1996, FOKKER PLANCK EQUATI, DOI 10.1007/978-3-642-61544-3_4; Rubanova Y., 2019, ARXIV190703907; Sarkka S., 2019, APPL STOCHASTIC DIFF, V10; SCARGLE JD, 1982, ASTROPHYS J, V263, P835, DOI 10.1086/160554; Schneider T, 2001, J CLIMATE, V14, P853, DOI 10.1175/1520-0442(2001)014<0853:AOICDE>2.0.CO;2; Simm J., 2017, 2017 IEEE 27 INT WOR, P1; Soleimani H, 2018, IEEE T PATTERN ANAL, V40, P1948, DOI 10.1109/TPAMI.2017.2742504; Sontag D., 2015, ARXIV151105121; Wang J., 2006, ADV NEURAL INFORM PR, P1441; Zhou GB, 2016, INT J AUTOM COMPUT, V13, P226, DOI 10.1007/s11633-016-1006-2	47	22	22	2	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307040
C	Eysenbach, B; Salakhutdinov, R; Levine, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Eysenbach, Benjamin; Salakhutdinov, Ruslan; Levine, Sergey			Search on the Replay Buffer: Bridging Planning and Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The history of learning for control has been an exciting back and forth between two broad classes of algorithms: planning and reinforcement learning. Planning algorithms effectively reason over long horizons, but assume access to a local policy and distance metric over collision-free paths. Reinforcement learning excels at learning policies and the relative values of states, but fails to plan over long horizons. Despite the successes of each method in various domains, tasks that require reasoning over long horizons with limited feedback and high-dimensional observations remain exceedingly challenging for both planning and reinforcement learning algorithms. Frustratingly, these sorts of tasks are potentially the most useful, as they are simple to design (a human only need to provide an example goal state) and avoid reward shaping, which can bias the agent towards find a suboptimal solution. We introduce a general-purpose control algorithm that combines the strengths of planning and reinforcement learning to effectively solve these tasks. Our aim is to decompose the task of reaching a distant goal state into a sequence of easier tasks, each of which corresponds to reaching a particular subgoal. Planning algorithms can automatically find these waypoints, but only if provided with suitable abstractions of the environment - namely, a graph consisting of nodes and edges. Our main insight is that this graph can be constructed via reinforcement learning, where a goal-conditioned value function provides edge weights, and nodes are taken to be previously seen observations in a replay buffer. Using graph search over our replay buffer, we can automatically generate this sequence of subgoals, even in image-based environments. Our algorithm, search on the replay buffer (SoRB), enables agents to solve sparse reward tasks over one hundred steps, and generalizes substantially better than standard RL algorithms.	[Eysenbach, Benjamin; Salakhutdinov, Ruslan] CMU, Pittsburgh, PA 15213 USA; [Eysenbach, Benjamin; Levine, Sergey] Google Brain, Mountain View, CA 94043 USA; [Levine, Sergey] Univ Calif Berkeley, Berkeley, CA USA	Carnegie Mellon University; Google Incorporated; University of California System; University of California Berkeley	Eysenbach, B (corresponding author), CMU, Pittsburgh, PA 15213 USA.; Eysenbach, B (corresponding author), Google Brain, Mountain View, CA 94043 USA.	beysenba@cs.cmu.edu			NSF [IIS1763562]; ONR [N000141812861]; AFRL CogDeCON; Apple	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); AFRL CogDeCON; Apple	We thank Vitchyr Pong, Xingyu Lin, and Shane Gu for helpful discussions on learning goal-conditioned value functions, Aleksandra Faust and Brian Okorn for feedback on connections to planning, and Nikolay Savinov for feedback on the SPTM baseline. RS is supported by NSF grant IIS1763562, ONR grant N000141812861, AFRL CogDeCON, and Apple. Any opinions, findings and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of NSF, AFRL, ONR, or Apple.	Agrawal P., 2016, P ADV NEURAL INFORM, P5074; Amos B., 2018, ADV NEURAL INFORM PR, P8289; Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; [Anonymous], 2018, P C ADV NEUR INF PRO; [Anonymous], 2018, ARXIV180506150; [Anonymous], 2005, IDENTIFYING USEFUL S, DOI [DOI 10.1145/1102351.1102454, 10.1145/1102351.1102454]; Bacon R-L., 2017, 31 AAAI C ART INT T; Bellemare MG, 2017, PR MACH LEARN RES, V70; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Chiang HTL, 2019, IEEE ROBOT AUTOM LET, V4, P2007, DOI 10.1109/LRA.2019.2899918; Drummond C, 2002, J ARTIF INTELL RES, V16, P59, DOI 10.1613/jair.904; Faust A, 2018, IEEE INT CONF ROBOT, P5113; Finn Chelsea, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2786, DOI 10.1109/ICRA.2017.7989324; Florensa C, 2019, ARXIV190100943; Fox R., 2017, ARXIV170308294; Frans Kevin, 2017, P INT C LEARN REPR; Gupta S., 2017, ARXIV170203920, P3; HADAR J, 1969, AM ECON REV, V59, P25; KAELBLING LP, 1993, IJCAI-93, VOLS 1 AND 2, P1094; Kaelbling LP., 1993, P 10 INT C MACHINE L, P951; Kavraki LE, 1996, IEEE T ROBOTIC AUTOM, V12, P566, DOI 10.1109/70.508439; Kulkarni T. D., 2016, PROC 30 INT C NEURAL, P3675; Kurutach T, 2018, ARXIV180210592; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402; Lau M., 2005, P ACM SIGGRAPH EUR S, P271; LaValle S. M., 2006, PLANNING ALGORITHMS; Lee L, 2018, PR MACH LEARN RES, V80; Lenz I., 2015, ROBOTICS SCI SYSTEMS; Levine S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966402; Levy A., 2019, P INT C LEARN REPR; Lillicrap TP, 2016, 4 INT C LEARN REPR; Lynch C, 2019, ARXIV190301973; Mirowski P., 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/j.neuroscience.2018.04.006; Mnih V., 2013, PLAYING ATARI DEEP R, P1; Nagabandi A, 2018, IEEE INT CONF ROBOT, P7579; Nair A, 2018, IEEE INT CONF ROBOT, P6292; Oh J., 2015, P ADV NEUR INF PROC, P2863; Pardo F., 2017, ARXIV171200378; Parr R, 1998, ADV NEUR IN, V10, P1043; Pong V., 2018, ARXIV180209081; Precup D., 2000, TEMPORAL ABSTRACTION; Racaniere S., 2017, ADV NEURAL INFORM PR, V30, P5690; Savinov, 2018, ARXIV181002274; Savinov N., 2018, ARXIV180300653; Savva M, 2017, CVPR, P1746; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Schulman J., 2017, ABS170706347 CORR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Schulman John, 2015, ARXIV150602438; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Srinivas A, 2018, PR MACH LEARN RES, V80; Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Tamar Aviv, 2016, ADV NEURAL INFORM PR, P2154, DOI DOI 10.5555/3171837.3171991; Thrun, 2005, PRINCIPLES ROBOT MOT; Vezhnevets AS, 2017, PR MACH LEARN RES, V70; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Watter Manuel, 2015, ADV NEURAL INFORM PR, V2, P2746; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu Y., 2018, ARXIV181004586; Zhang A., 2018, ARXIV180300512	65	22	23	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906086
C	Guo, Y; Zheng, Y; Tan, MK; Chen, Q; Chen, J; Zhao, PL; Huang, JZ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Guo, Yong; Zheng, Yin; Tan, Mingkui; Chen, Qi; Chen, Jian; Zhao, Peilin; Huang, Junzhou			NAT: Neural Architecture Transformer for Accurate and Compact Architectures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Designing effective architectures is one of the key factors behind the success of deep neural networks. Existing deep architectures are either manually designed or automatically searched by some Neural Architecture Search (NAS) methods. However, even a well-searched architecture may still contain many non-significant or redundant modules or operations (e.g., convolution or pooling), which may not only incur substantial memory consumption and computation cost but also deteriorate the performance. Thus, it is necessary to optimize the operations inside an architecture to improve the performance without introducing extra computation cost. Unfortunately, such a constrained optimization problem is NP-hard. To make the problem feasible, we cast the optimization problem into a Markov decision process (MDP) and seek to learn a Neural Architecture Transformer (NAT) to replace the redundant operations with the more computationally efficient ones (e.g., skip connection or directly removing the connection). Based on MDP, we learn NAT by exploiting reinforcement learning to obtain the optimization policies w.r.t. different architectures. To verify the effectiveness of the proposed strategies, we apply NAT on both hand-crafted architectures and NAS based architectures. Extensive experiments on two benchmark datasets, i.e., CIFAR-10 and ImageNet, demonstrate that the transformed architecture by NAT significantly outperforms both its original form and those architectures optimized by existing methods.	[Tan, Mingkui; Chen, Jian] South China Univ Technol, Weixin Grp, Tencent, Guangzhou, Peoples R China; Univ Texas Arlington, Tencent AI Lab, Arlington, TX 76019 USA	South China University of Technology; Tencent; University of Texas System; University of Texas Arlington	Tan, MK; Chen, J (corresponding author), South China Univ Technol, Weixin Grp, Tencent, Guangzhou, Peoples R China.	guo.yong@mail.scut.edu.cn; yinzheng@tencent.com; mingkuitan@scut.edu.cn; sechenqi@mail.scut.edu.cn; ellachen@scut.edu.cn; masonzhao@tencent.com; jzhuang@uta.edu			Guangdong Provincial Scientific and Technological Funds [2018B010107001]; National Natural Science Foundation of China (NSFC) [61602185]; key project of NSFC [61836003]; Fundamental Research Funds for the Central Universities [D2191240]; Program for Guangdong Introducing Innovative and Enterpreneurial Teams [2017ZT07X183]; Tencent AI Lab Rhino-Bird Focused Research Program [JR201902]; Guangdong Special Branch Plans Young Talent with Scientific and Technological Innovation [2016TQ03X445]; Guangzhou Science and Technology Planning Project [201904010197]; Microsoft Research Asia (MSRA Collaborative Research Program)	Guangdong Provincial Scientific and Technological Funds; National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); key project of NSFC(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Program for Guangdong Introducing Innovative and Enterpreneurial Teams; Tencent AI Lab Rhino-Bird Focused Research Program; Guangdong Special Branch Plans Young Talent with Scientific and Technological Innovation; Guangzhou Science and Technology Planning Project; Microsoft Research Asia (MSRA Collaborative Research Program)(Microsoft)	This work was partially supported by Guangdong Provincial Scientific and Technological Funds under Grants 2018B010107001, National Natural Science Foundation of China (NSFC) (No. 61602185), key project of NSFC (No. 61836003), Fundamental Research Funds for the Central Universities (No. D2191240), Program for Guangdong Introducing Innovative and Enterpreneurial Teams 2017ZT07X183, Tencent AI Lab Rhino-Bird Focused Research Program (No. JR201902), Guangdong Special Branch Plans Young Talent with Scientific and Technological Innovation (No. 2016TQ03X445), Guangzhou Science and Technology Planning Project (No. 201904010197), and Microsoft Research Asia (MSRA Collaborative Research Program). We last thank Tencent AI Lab.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Baker Bowen, 2017, ICLR; Cai Han, 2019, INT C LEARN REPR; Cao J., 2019, ADV NEURAL INFORM PR; Cao JZ, 2018, PR MACH LEARN RES, V80; Cao Steven, 2019, INT C LEARN REPR; Chen Tianqi, 2016, ICLR; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352; Guo Y, 2018, AAAI CONF ARTIF INTE, P3134; Guo Yong, 2018, ARXIV180907099; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang H, 2018, BIOCOMPUT-PAC SYM, P304; Jiang ZX, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1965; Jin H., 2018, ARXIV180610282; Jin W., 2019, INT C LEARN REPR; Kipf TN, 2016, P INT C LEARN REPR; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lauly Stanislas, 2017, J MACHINE LEARNING R, V18, P4046; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Li H., 2017, P INT C LEARN REPR I, P1; Li YJ, 2018, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2018.00054; Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8; Liu C., 2018, P EUR C COMP VIS ECC, P19, DOI DOI 10.1007/978-3-030-01246-5_2; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Luo RQ, 2018, ADV NEUR IN, V31; Nair V, 2010, P 27 INT C MACHINE L, P807; Nam CS, 2018, BRAIN-COMPUTER INTERFACES HANDBOOK: TECHNOLOGICAL AND THEORETICAL ADVANCES, P1; Pan JH, 2016, IEEE IJCNN, P2063, DOI 10.1109/IJCNN.2016.7727453; Pham H, 2018, PR MACH LEARN RES, V80; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; So DR, 2019, PR MACH LEARN RES, V97; Srivastava Rupesh Kumar, 2015, ADV NEURAL INFORM PR, P2377; Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907; Tan M., 2016, ARXIV161101773; Tan MK, 2014, J MACH LEARN RES, V15, P1371; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xie Sirui, 2019, ICLR, V1, P13; Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719; Zeng RH, 2019, IEEE T IMAGE PROCESS, V28, P5797, DOI 10.1109/TIP.2019.2922108; Zhang C, 2019, INT C LEARN REPR; Zhang X., 2018, ARXIV181101567; Zhang YB, 2019, I IEEE EMBS C NEUR E, P360, DOI 10.1109/NER.2019.8717177; Zheng Y., 2016, P 1 WORKSH DEEP LEAR, P2, DOI DOI 10.1145/2988450.2988453; Zheng Y, 2016, PR MACH LEARN RES, V48; Zheng Y, 2016, IEEE T PATTERN ANAL, V38, P1056, DOI 10.1109/TPAMI.2015.2476802; Zheng Y, 2014, PROC CVPR IEEE, P1370, DOI 10.1109/CVPR.2014.178; Zheng Y, 2015, INT J COMPUT VISION, V113, P67, DOI 10.1007/s11263-014-0765-x; Zhuang ZW, 2018, ADV NEUR IN, V31; Zoph B., 2017, P1; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	62	22	22	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300067
C	Kirsch, A; van Amersfoort, J; Gal, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kirsch, Andreas; van Amersfoort, Joost; Gal, Yarin			BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We develop BatchBALD, a tractable approximation to the mutual information between a batch of points and model parameters, which we use as an acquisition function to select multiple informative points jointly for the task of deep Bayesian active learning. BatchBALD is a greedy linear-time 1- 1/e-approximate algorithm amenable to dynamic programming and efficient caching. We compare BatchBALD to the commonly used approach for batch data acquisition and find that the current approach acquires similar and redundant points, sometimes performing worse than randomly acquiring data. We finish by showing that, using BatchBALD to consider dependencies within an acquisition batch, we achieve new state of the art performance on standard benchmarks, providing substantial data efficiency improvements in batch acquisition.	[Kirsch, Andreas; van Amersfoort, Joost; Gal, Yarin] Univ Oxford, Dept Comp Sci, OATML, Oxford, England	University of Oxford	Kirsch, A (corresponding author), Univ Oxford, Dept Comp Sci, OATML, Oxford, England.	andreas.kirsch@cs.ox.ac.uk; joost.van.amersfoort@cs.ox.ac.uk; yarin@cs.ox.ac.uk			UK EPSRC CDT in Autonomous Intelligent Machines and Systems [EP/L015897/1]; EPSRC [EP/N509711/1]; Google-DeepMind; Allan Turing Institute; Google	UK EPSRC CDT in Autonomous Intelligent Machines and Systems(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Google-DeepMind(Google Incorporated); Allan Turing Institute; Google(Google Incorporated)	The authors want to thank Binxin (Robin) Ru for helpful references to submodularity and the appropriate proofs. We would also like to thank the rest of OATML for their feedback at several stages of the project. AK is supported by the UK EPSRC CDT in Autonomous Intelligent Machines and Systems (grant reference EP/L015897/1). JvA is grateful for funding by the EPSRC (grant reference EP/N509711/1) and Google-DeepMind. Funding for computational resources was provided by the Allan Turing Institute and Google.	Adomavicius G., 2005, IEEE T KNOWLEDGE DAT; Alvi Ahsan S, 2019, ARXIV190110452; Azimi J, 2012, ARXIV12066458; Blundell Charles, 2015, INT C MACH LEARN, V37, P1613; Calinon S, 2007, IEEE T SYST MAN CY B, V37, P286, DOI 10.1109/TSMCB.2006.886952; Cohen G, 2017, IEEE IJCNN, P2921, DOI 10.1109/IJCNN.2017.7966217; Cohn DA, 1996, J ARTIF INTELL RES, V4, P129, DOI 10.1613/jair.295; Darlow Luke Nicholas, 2018, CORR; Frankle J., 2019, TRAINABLE NEURAL NET, P05; Freeman Linton C, 1965, ELEMENTARY APPL STAT; Gal Y, 2016, PR MACH LEARN RES, V48; Gonzalez J, 2016, JMLR WORKSH CONF PRO, V51, P648; Guo Yuhong, 2008, ADV NEURAL INFORM PR, V20, P593; Hernandez-Lobato D, 2016, PR MACH LEARN RES, V48; Hoi SCH, 2006, P 23 INT C MACH LEAR, V148, P417, DOI [10.1145/1143844.1143897, DOI 10.1145/1143844.1143897]; Houlsby N, 2011, STAT; Janz David, 2017, ARXIV170804465; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kendall A, 2015, P BRIT MACH VIS C 20; Kingma D.P, P 3 INT C LEARNING R; Kingma DP, 2014, ADV NEUR IN, P3581, DOI DOI 10.5555/2969033.2969226; Krause A, 2008, J MACH LEARN RES, V9, P235; Lipton Z.C., 2018, P C EMP METH NAT LAN; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Nguyen Viet Cuong, 2013, ADV NEURAL INFORM PR, V26, P1457; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546; Sener O, 2018, INT C LEARN REPR; Shen Y, 2018, INT C LEARN REPR; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Sinha Trevor Darrell Samarth, 2019, ARXIV190400370; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Tong Simon, 2001, ACTIVE LEARNING THEO, V1; Wang KZ, 2017, IEEE T CIRC SYST VID, V27, P2591, DOI 10.1109/TCSVT.2016.2589879; YEUNG RW, 1991, IEEE T INFORM THEORY, V37, P466, DOI 10.1109/18.79902	36	22	22	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307008
C	Kulkarni, T; Gupta, A; Ionescu, C; Borgeaud, S; Reynolds, M; Zisserman, A; Mnih, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kulkarni, Tejas; Gupta, Ankush; Ionescu, Catalin; Borgeaud, Sebastian; Reynolds, Malcolm; Zisserman, Andrew; Mnih, Volodymyr			Unsupervised Learning of Object Keypoints for Perception and Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The study of object representations in computer vision has primarily focused on developing representations that are useful for image classification, object detection, or semantic segmentation as downstream tasks. In this work we aim to learn object representations that are useful for control and reinforcement learning (RL). To this end, we introduce Transporter,a neural network architecture for discovering concise geometric object representations in terms of keypoints or image-space coordinates. Our method learns from raw video frames in a fully unsupervised manner, by transporting learnt image features between video frames using a keypoint bottleneck. The discovered keypoints track objects and object parts across long time-horizons more accurately than recent similar methods. Furthermore, consistent long-term tracking enables two notable results in control domains - (1) using the keypoint co-ordinates and corresponding image features as inputs enables highly sample-efficient reinforcement learning; (2) learning to explore by controlling keypoint locations drastically reduces the search space, enabling deep exploration (leading to states unreachable through random action exploration) without any extrinsic rewards.	[Kulkarni, Tejas; Gupta, Ankush; Ionescu, Catalin; Borgeaud, Sebastian; Reynolds, Malcolm; Zisserman, Andrew; Mnih, Volodymyr] DeepMind, London, England; [Zisserman, Andrew] Univ Oxford, Dept Engn Sci, VGG, Oxford, England	University of Oxford	Kulkarni, T (corresponding author), DeepMind, London, England.	tkulkarni@google.com; ankushgupta@google.com; cdi@google.com; sborgeaud@google.com; mareynolds@google.com; zisserman@google.com; vmnih@google.com						Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Buckman J., 2018, INT C LEARN REPR; Burgess Christopher P, 2019, ARXIV190111390; Chen X., 2016, ARXIV160603657, P2172; Ecoffet A, 2019, ARXIV PREPRINT ARXIV; Finn C, 2016, IEEE INT CONF ROBOT, P512, DOI 10.1109/ICRA.2016.7487173; Goel V, 2018, ADV NEUR IN, V31; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gordon A, 2019, IEEE I CONF COMP VIS, P8976, DOI 10.1109/ICCV.2019.00907; Greff K, 2019, PR MACH LEARN RES, V97; Grundmann M., 2010, P C COMP VIS PATT RE; Hessel M, 2018, AAAI CONF ARTIF INTE, P3215; Higgins I., 2017, P INT C LEARN REPR T; Horgan D., 2018, P INT C LEARN REPR I; Ionescu C., 2018, NEURIPS DEEP REINF L; Jakab T, 2018, ADV NEUR IN, V31; Ji Xu, 2018, ARXIV180706653; Kaiser L., 2019, ARXIV190300374; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulkarni TD, 2016, ADV NEUR IN, V29; Kulkarni TD, 2015, ADV NEUR IN, V28; Li S., 2018, P C COMP VIS PATT RE; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Pathak D., 2017, ICML; Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5; Plappert Matthias, 2017, ARXIV170601905; Riedmiller M, 2005, LECT NOTES ARTIF INT, V3720, P317, DOI 10.1007/11564096_32; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Shu Zhixin, 2018, P EUR C COMP VIS ECC; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Spelke Elizabeth S, 2007, DEV SCI, V1; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Suwajanakorn S, 2018, ADV NEUR IN, V31; Tassa Y., 2018, ARXIV180100690; Thewlis J., 2017, P INT C COMP VIS ICC; Whitney W. F., 2016, ARXIV160206822; Wiles O, 2018, LECT NOTES COMPUT SC, V11217, P690, DOI 10.1007/978-3-030-01261-8_41; Xue T., 2016, P ADV NEUR INF PROC; Zhang Y., 2018, P C COMP VIS PATT RE	40	22	23	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902036
C	Maron, H; Ben-Hamu, H; Serviansky, H; Lipman, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Maron, Haggai; Ben-Hamu, Heli; Serviansky, Hadar; Lipman, Yaron			Provably Powerful Graph Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recently, the Weisfeiler-Lehman (WL) graph isomorphism test was used to measure the expressive power of graph neural networks (GNN). It was shown that the popular message passing GNN cannot distinguish between graphs that are indistinguishable by the 1-WL test (Morris et al., 2018; Xu et al., 2019). Unfortunately, many simple instances of graphs are indistinguishable by the 1-WL test. In search for more expressive graph learning models we build upon the recent k-order invariant and equivariant graph neural networks (Maron et al., 2019a,b) and present two results: First, we show that such k-order networks can distinguish between non-isomorphic graphs as good as the k-WL tests, which are provably stronger than the 1-WL test for k > 2. This makes these models strictly stronger than message passing models. Unfortunately, the higher expressiveness of these models comes with a computational cost of processing high order tensors. Second, setting our goal at building a provably stronger, simple and scalable model we show that a reduced 2-order network containing just scaled identity operator, augmented with a single quadratic operation (matrix multiplication) has a provable 3-WL expressive power. Differently put, we suggest a simple model that interleaves applications of standard Multilayer-Perceptron (MLP) applied to the feature dimension and matrix multiplication. We validate this model by presenting state of the art results on popular graph classification and regression tasks. To the best of our knowledge, this is the first practical invariant/equivariant model with guaranteed 3-WL expressiveness, strictly stronger than message passing models.	[Maron, Haggai; Ben-Hamu, Heli; Serviansky, Hadar; Lipman, Yaron] Weizmann Inst Sci, Rehovot, Israel	Weizmann Institute of Science	Maron, H (corresponding author), Weizmann Inst Sci, Rehovot, Israel.				European Research Council (ERC Consolidator Grant, "LiftMatch" ) [771136]; Israel Science Foundation [1830/17]	European Research Council (ERC Consolidator Grant, "LiftMatch" )(European Research Council (ERC)); Israel Science Foundation(Israel Science Foundation)	This research was supported in part by the European Research Council (ERC Consolidator Grant, "LiftMatch" 771136) and the Israel Science Foundation (Grant No. 1830/17).	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Atwood J., 2016, ADV NEURAL INFORM PR, P1993, DOI DOI 10.5555/3157096.3157320; Babai L, 2016, ACM S THEORY COMPUT, P684, DOI 10.1145/2897518.2897542; Briand E., 2004, CONTRIB ALG GEOM, V45, P353; Bruna J, 2013, PROC INT C LEARN REP; CAI JY, 1992, COMBINATORICA, V12, P389, DOI 10.1007/BF01305232; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Douglas B. L., 2011, MATHEMATICS-BASEL; Duvenaud David K, 2015, P NIPS; Fey M., 2019, ICLR WORKSH REPR LEA; Gilmer J, 2017, PR MACH LEARN RES, V70; GORI M, 2005, IEEE IJCNN, P729, DOI DOI 10.1109/IJCNN.2005.1555942; Grohe M, 2015, J SYMBOLIC LOGIC, V80, P797, DOI 10.1017/jsl.2015.28; Grohe Martin, 2017, DESCRIPTIVE COMPLEXI, V47; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Hamilton WL, 2017, REPRESENTATION LEARN; Henaff M, 2015, ARXIV150605163; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; Ivanov S, 2018, PR MACH LEARN RES, V80; Keriven N., 2019, CORR; Kipf TN, 2016, P INT C LEARN REPR; Kondor R., 2018, ARXIV180102144; Kriege N.M., 2019, SURVEY GRAPH KERNELS; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kutzkov K., 2016, LEARNING CONVOLUTION; Lei T, 2017, PR MACH LEARN RES, V70; Levie R., 2017, CAYLEYNETS GRAPH CON, P1; Li YX, 2015, 23RD ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON ADVANCES IN GEOGRAPHIC INFORMATION SYSTEMS (ACM SIGSPATIAL GIS 2015), DOI 10.1145/2820783.2820837; Maron H., 2019, INT C LEARN REPR; Maron H, 2019, PR MACH LEARN RES, V97; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Monti L, 2018, GOODTECHS '18: PROCEEDINGS OF THE 4TH EAI INTERNATIONAL CONFERENCE ON SMART OBJECTS AND TECHNOLOGIES FOR SOCIAL GOOD (GOODTECHS), P1, DOI 10.1145/3284869.3284878; Morris C, 2017, IEEE DATA MINING, P327, DOI 10.1109/ICDM.2017.42; Morris Christopher, 2019, ARXIV190401543; Morris Christopher, 2018, ABS181002244 CORR; Rydh D, 2007, ANN I FOURIER, V57, P1741, DOI 10.5802/aif.2312; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Shervashidze N., 2009, P 12 INT C ART INT S, P488; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Simonovsky M., 2017, P 30 IEEE C COMP VIS; Velickovic P., 2017, ARXIV MACHINE LEARNI, P1; Verma S, 2017, ADV NEUR IN, V30; Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201; Wu ZQ, 2018, CHEM SCI, V9, P513, DOI 10.1039/c7sc02664a; Xu K, 2019, PROC INT CONF PARAL, DOI 10.1145/3337821.3337923; Yanardag Pinar, 2015, P 21 ACM SIGKDD INT; Ying R., 2018, HIERARCHICAL GRAPH R, P3; Zaheer M., 2017, ADV NEURAL INFORM PR, P3391; Zhang M., 2018, P AAAI C ART INT; Zhang M, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P575, DOI 10.1145/3097983.3097996	53	22	22	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302018
C	Pinsler, R; Gordon, J; Nalisnick, E; Hernandez-Lobato, JM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Pinsler, Robert; Gordon, Jonathan; Nalisnick, Eric; Hernandez-Lobato, Jose Miguel			Bayesian Batch Active Learning as Sparse Subset Approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Leveraging the wealth of unlabeled data produced in recent years provides great potential for improving supervised models. When the cost of acquiring labels is high, probabilistic active learning methods can be used to greedily select the most informative data points to be labeled. However, for many large-scale problems standard greedy procedures become computationally infeasible and suffer from negligible model change. In this paper, we introduce a novel Bayesian batch active learning approach that mitigates these issues. Our approach is motivated by approximating the complete data posterior of the model parameters. While naive batch construction methods result in correlated queries, our algorithm produces diverse batches that enable efficient active learning at scale. We derive interpretable closed-form solutions akin to existing active learning procedures for linear models, and generalize to arbitrary models using random projections. We demonstrate the benefits of our approach on several large-scale regression and classification tasks.	[Pinsler, Robert; Gordon, Jonathan; Nalisnick, Eric; Hernandez-Lobato, Jose Miguel] Univ Cambridge, Dept Engn, Cambridge, England	University of Cambridge	Pinsler, R (corresponding author), Univ Cambridge, Dept Engn, Cambridge, England.	rp586@cam.ac.uk; jg801@cam.ac.uk; etn22@cam.ac.uk; jmh233@cam.ac.uk		Pinsler, Robert/0000-0003-1454-188X	iCASE grant [1950384]; Nokia; Samsung Research, Samsung Electronics Co., Seoul, Republic of Korea	iCASE grant; Nokia(Nokia Corporation); Samsung Research, Samsung Electronics Co., Seoul, Republic of Korea(Samsung)	Robert Pinsler receives funding from iCASE grant #1950384 with support from Nokia. Jonathan Gordon, Eric Nalisnick and Jose Miguel Hernandez-Lobato were funded by Samsung Research, Samsung Electronics Co., Seoul, Republic of Korea. We thank Adria Garriga-Alonso, James Requeima, Marton Havasi, Carl Edward Rasmussen and Trevor Campbell for helpful feedback and discussions.	Azimi J, 2010, ADV NEURAL INFORM PR, V23, P109; Blundell C., 2015, ARXIV PREPRINT ARXIV; Campbell T, 2019, J MACH LEARN RES, V20; Chevalier Clement, 2013, Learning and Intelligent Optimization. 7th International Conference, LION 7. Revised Selected Papers: LNCS 7997, P59, DOI 10.1007/978-3-642-44973-4_7; Contal Emile, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8188, P225, DOI 10.1007/978-3-642-40988-2_15; Dasgupta S, 2005, ADV NEURAL INFORM PR, P337; Dereziliski Michal, 2018, ADV NEURAL INFORM PR, V31, P2510; Desautels T, 2014, J MACH LEARN RES, V15, P3873; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Elhamifar E, 2013, IEEE I CONF COMP VIS, P209, DOI 10.1109/ICCV.2013.33; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Gal Y., 2017, ARXIV170302910; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; Gonzalez J, 2016, JMLR WORKSH CONF PRO, V51, P648; Guo Y, 2010, ADV NEURAL INFORM PR, P802; Guo Yuhong, 2008, ADV NEURAL INFORM PR, V20, P593; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1861; Hoi SCH, 2006, P 23 INT C MACH LEAR, V148, P417, DOI [10.1145/1143844.1143897, DOI 10.1145/1143844.1143897]; Houlsby N, 2011, STAT; Huggins Jonathan, 2016, P ADV NEUR INF PROC, P4080; Johnson O, 2004, PROBAB THEORY REL, V129, P391, DOI 10.1007/s00440-004-0344-0; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kapoor A, 2007, IEEE I CONF COMP VIS, P134; Kingma D.P, P 3 INT C LEARNING R; Ma P, 2015, J MACH LEARN RES, V16, P861; MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590; OWEN DB, 1956, ANN MATH STAT, V27, P1075, DOI 10.1214/aoms/1177728074; Riquelme Carlos, 2018, ICLR; Sener O, 2018, INT C LEARN REPR; Settles B., 2012, SYNTH LECT ARTIF INT, V6, P1; Shah A, 2015, ADV NEURAL INFORM PR, V12, P3330; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wei K, 2015, PR MACH LEARN RES, V37, P1954; Wilson AG, 2016, JMLR WORKSH CONF PRO, V51, P370; Yang Y, 2015, INT J COMPUT VISION, V113, P113, DOI 10.1007/s11263-014-0781-x; Yu K, 2006, P 23 INT C MACH LEAR, ppp1081	38	22	22	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306037
C	Qiao, TT; Zhang, J; Xu, DQ; Tao, DC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Qiao, Tingting; Zhang, Jing; Xu, Duanqing; Tao, Dacheng			Learn, Imagine and Create: Text-to-Image Generation from Prior Knowledge	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Text-to-image generation, i.e: generating an image given a text description, is a very challenging task due to the significant semantic gap between the two domains. Humans, however, tackle this problem intelligently. We learn from diverse objects to form a solid prior about semantics, textures, colors, shapes, and layouts. Given a text description, we immediately imagine an overall visual impression using this prior and, based on this, we draw a picture by progressively adding more and more details. In this paper, and inspired by this process, we propose a novel text-to-image method called LeicaGAN to combine the above three phases in a unified framework. First, we formulate the multiple priors learning phase as a textual-visual co-embedding (TVE) comprising a text-image encoder for learning semantic, texture, and color priors and a text-mask encoder for learning shape and layout priors. Then, we formulate the imagination phase as multiple priors aggregation (MPA) by combining these complementary priors and adding noise for diversity. Lastly, we formulate the creation phase by using a cascaded attentive generator (CAG) to progressively draw a picture from coarse to fine. We leverage adversarial learning for LeicaGAN to enforce semantic consistency and visual realism. Thorough experiments on two public benchmark datasets demonstrate LeicaGAN's superiority over the baseline method. Code has been made available at https://github.com/qiaott/LeicaGAN.	[Qiao, Tingting; Xu, Duanqing] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China; [Qiao, Tingting; Zhang, Jing; Tao, Dacheng] Univ Sydney, Fac Engn, Sch Comp Sci, UBTECH Sydney Ctr, Darlington, NSW 2008, Australia	Zhejiang University; University of Sydney	Xu, DQ (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.	qiaott@zju.edu.cn; jing.zhang1@sydney.edu.au; xdq@zju.edu.cn; dacheng.tao@sydney.edu.au		ZHANG, JING/0000-0001-6595-7661	National Natural Science Foundation of China [61806062]; key provincial R&D project of Zhejiang Province [2019C03137]; Science and Technology Project of Cultural Relics Protection in Zhejiang Province [2019008, 2018007]; Chinsese National Double First-rate Project about digital protection of cultural relics in Grotto Temple and equipment upgrading of the Chinese National Cultural Heritage Administration scientific research institutes	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); key provincial R&D project of Zhejiang Province; Science and Technology Project of Cultural Relics Protection in Zhejiang Province; Chinsese National Double First-rate Project about digital protection of cultural relics in Grotto Temple and equipment upgrading of the Chinese National Cultural Heritage Administration scientific research institutes	This work was supported in part by the National Natural Science Foundation of China Project 61806062, the key provincial R&D project of Zhejiang Province 2019C03137, the the Science and Technology Project of Cultural Relics Protection in Zhejiang Province 2019008 and 2018007, Chinsese National Double First-rate Project about digital protection of cultural relics in Grotto Temple and equipment upgrading of the Chinese National Cultural Heritage Administration scientific research institutes.	Ba J. L, 2015, ARXIV151102793; Barratt S., 2018, ARXIV180101973; COX MV, 1986, BRIT J DEV PSYCHOL, V4, P341, DOI 10.1111/j.2044-835X.1986.tb01029.x; Eslami SM, 2016, NEURIPS, V1; Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hinz T., 2019, ICLR; Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133; Kataoka Y, 2016, 2016 IEEE/ACIS 15TH INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCE (ICIS), P933; Lu JS, 2017, ADV NEUR IN, V30; Miyato Takeru, 2018, ARXIV180205637; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; Odena A, 2017, PR MACH LEARN RES, V70; Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068; PHILLIPS WA, 1978, COGNITION, V6, P15, DOI 10.1016/0010-0277(78)90007-0; Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160; QucikDraw G., GOOGLE QUCIK DRAW WE; QucikDraw G., 2017, EXPLORING VISUALIZIN; Reed S. E., 2016, ADV NEURAL INFORM PR, P217; Reed S, 2016, PR MACH LEARN RES, V48; Reichert DP, 2011, LECT NOTES COMPUT SC, V6791, P18, DOI 10.1007/978-3-642-21735-7_3; Rezende DJ, 2016, PR MACH LEARN RES, V48; Salimans T, 2016, ADV NEUR IN, V29; Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093; Shane Barratt R. S., INCEPTION SCORE PYTO; Shen XB, 2017, IEEE T CYBERNETICS, V47, P4275, DOI 10.1109/TCYB.2016.2606441; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tang Y., 2014, ADV NEURAL INFORM PR, P1808; Wah C., 2011, TECH REP; Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326; Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541; Wang Ting-Chun, 2018, ARXIV180806601; Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143; Yao T, 2015, IEEE I CONF COMP VIS, P28, DOI 10.1109/ICCV.2015.12; Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704; Zhang G, 2018, LECT NOTES COMPUT SC, V11210, P422, DOI 10.1007/978-3-030-01231-1_26; Zhang H., 2017, ICCV; Zhang Han, 2018, ARXIV180508318; Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhuang Yueting, 2013, 27 AAAI C ART INT, P1070	46	22	23	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300080
C	Ravuri, S; Vinyals, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ravuri, Suman; Vinyals, Oriol			Classification Accuracy Score for Conditional Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep generative models (DGMs) of images are now sufficiently mature that they produce nearly photorealistic samples and obtain scores similar to the data distribution on heuristics such as Frechet Inception Distance (FID). These results, especially on large-scale datasets such as ImageNet, suggest that DGMs are learning the data distribution in a perceptually meaningful space and can be used in downstream tasks. To test this latter hypothesis, we use class-conditional generative models from a number of model classes-variational autoencoders, autoregressive models, and generative adversarial networks (GANs)-to infer the class labels of real data. We perform this inference by training an image classifier using only synthetic data and using the classifier to predict labels on real data. The performance on this task, which we call Classification Accuracy Score (CAS), reveals some surprising results not identified by traditional metrics and constitute our contributions. First, when using a state-of-the-art GAN (BigGAN-deep), Top-1 and Top-5 accuracy decrease by 27.9% and 41.6%, respectively, compared to the original data; and conditional generative models from other model classes, such as Vector-Quantized Variational Autoencoder-2 (VQ-VAE-2) and Hierarchical Autoregressive Models (HAMs), substantially outperform GANs on this benchmark. Second, CAS automatically surfaces particular classes for which generative models failed to capture the data distribution, and were previously unknown in the literature. Third, we find traditional GAN metrics such as Inception Score (IS) and FID neither predictive of CAS nor useful when evaluating non-GAN models. Furthermore, in order to facilitate better diagnoses of generative models, we open-source the proposed metric.	[Ravuri, Suman; Vinyals, Oriol] DeepMind, London N1C 4AG, England		Ravuri, S (corresponding author), DeepMind, London N1C 4AG, England.	ravuris@google.com; vinyals@google.com						Arora S., 2017, ARXIV170608224; Barratt S., 2018, ARXIV180101973; Bifikowski Mikolaj, 2018, ARXIV180101401; Brock Andrew, 2018, ARXIV180911096; Chen S.F., 1998, EVALUATION METRICS L; De Fauw Jeffrey, 2019, ARXIV190304933; Dinh L, 2016, ARXIV PREPRINT ARXIV; Esteban C, 2017, REAL VALUED MEDICAL; Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goyal Priya, 2017, ARXIV170602677; Gretton A, 2012, J MACH LEARN RES, V13, P723; He Kaiming, 2015, ARXIV07061234; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Hu ZT, 2017, PR MACH LEARN RES, V70; Jiang Y, 2017, IGGRAPH ASIA 2017 TECHNICAL BRIEFS (SA'17), DOI 10.1145/3145749.3149440; Karras T., 2017, PROGR GROWING GANS I; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Khrulkov Valentin, 2018, ARXIV180202664; Kingma D. P., 2018, P ADV NEUR INF PROC, P10215; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kynkaanniemi T, 2019, ADV NEUR IN, V32; Lesort Timoth6e, 2018, ARXIV180610840; Liu Ruishan, 2018, ARXIV181202271; Menick Jacob, 2018, ARXIV181201608; Miyato Takeru, 2018, ARXIV180205637; Nalisnick Eric, 2018, INT C LEARN REPR; Ostrovski G, 2018, PR MACH LEARN RES, V80; Razavi A, 2019, ADV NEUR IN, V32; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Sajjadi Mehdi SM, 2018, ADV NEURAL INFORM PR, P5234; Salimans T, 2016, ADV NEUR IN, V29; Santurkar Shibani, 2017, ARXIV171100970; Semeniuta Stanislau, 2019, ACCURATE EVALUATION; Shmelkov K, 2018, LECT NOTES COMPUT SC, V11206, P218, DOI 10.1007/978-3-030-01216-8_14; Smolensky P., 1986, PARALLEL DISTRIBUTED, V1, P194; Theis Lucas, 2016, ICLR; van den Oord Aaron, 2016, ARXIV160605328; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhou Sharon, 2019, ARXIV190401121	40	22	22	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903084
C	Wan, ZY; Chen, DD; Li, Y; Yan, XG; Zhang, JG; Yu, YZ; Liao, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wan, Ziyu; Chen, Dongdong; Li, Yan; Yan, Xingguang; Zhang, Junge; Yu, Yizhou; Liao, Jing			Transductive Zero-Shot Learning with Visual Structure Constraint	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					To recognize objects of the unseen classes, most existing Zero-Shot Learning(ZSL) methods first learn a compatible projection function between the common semantic space and the visual space based on the data of source seen classes, then directly apply it to the target unseen classes. However, in real scenarios, the data distribution between the source and target domain might not match well, thus causing the well-known domain shift problem. Based on the observation that visual features of test instances can be separated into different clusters, we propose a new visual structure constraint on class centers for transductive ZSL, to improve the generality of the projection function (i.e.alleviate the above domain shift problem). Specifically, three different strategies (symmetric Chamfer-distance, Bipartite matching distance, and Wasserstein distance) are adopted to align the projected unseen semantic centers and visual cluster centers of test instances. We also propose a new training strategy to handle the real cases where many unrelated images exist in the test dataset, which is not considered in previous methods. Experiments on many widely used datasets demonstrate that the proposed visual structure constraint can bring substantial performance gain consistently and achieve state-of-the-art results.	[Wan, Ziyu; Liao, Jing] City Univ Hong Kong, Hong Kong, Peoples R China; [Chen, Dongdong] Microsoft Cloud AI, Redmond, WA USA; [Li, Yan] Tencent, PCG, Shenzhen, Peoples R China; [Yan, Xingguang] Shenzhen Univ, Shenzhen, Peoples R China; [Zhang, Junge] CASIA, NLPR, Beijing, Peoples R China; [Yu, Yizhou] Deepwise AI Lab, Beijing, Peoples R China	City University of Hong Kong; Tencent; Shenzhen University; Chinese Academy of Sciences; Institute of Automation, CAS	Liao, J (corresponding author), City Univ Hong Kong, Hong Kong, Peoples R China.	ziyuwan2-c@my.cityu.edu.hk; cddlyf@gmail.com; jingliao@cityu.edu.hk		Chen, Dongdong/0000-0002-4642-4373; LIAO, Jing/0000-0001-7014-5377	Natural Science Foundation of China (NSFC) [61876181, 7200607, 21209119]	Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	We would like to thank the anonymous reviewers for their thoughtful comments and efforts towards improving our work. This work was supported by the Natural Science Foundation of China (NSFC) No.61876181, CityU start-up grant 7200607 and Hong Kong ECS grant 21209119.	Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911; Annadani Yashas, 2018, CVPR; [Anonymous], 2018, PAMI; Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483; Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Fan H., 2017, P CVPR HON HAW 21 26, V2, P6; Frome Andrea, 2013, NEURIPS; Fu Y., 2016, CVPR; Fu Y., 2014, ECCV; Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354; Fuchs P, 2010, INT J CANCER, V126, P2663, DOI 10.1002/ijc.24970; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hong S, 2017, IEEE WIREL POWER TRA; Hong SD, 2019, PHYSIOL MEAS, V40, DOI 10.1088/1361-6579/ab15a2; Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473; Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Li Y., 2018, ABS180602877 ARXIV; Li Y., 2017, IEEE C COMP VIS PATT; Liu SC, 2018, ADV NEUR IN, V31; Lu Y., 2016, IJCAI; Morgado P., 2017, CVPR; Norouzi Mohammad, 2014, ICLR; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Radovanovic M, 2010, J MACH LEARN RES, V11, P2487; Reed S. E., 2016, CVPR; Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152; Socher R., 2012, ADV NEURAL INFORM PR, V1, P656; Song J, 2018, PROC CVPR IEEE, P1024, DOI 10.1109/CVPR.2018.00113; Wang W., 2018, AAAI; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Ye H, 2016, WIRELESS OPTIC COMM; Ye M, 2017, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR.2017.542; Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321; Zhang Z., 2016, ECCV; Zhang Z., 2015, ICCV; Zhu X, 2002, LEARNING LABELED UNL; Zhu Y., 2019, NEURAL INFORM PROCES	40	22	24	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901058
C	Zhu, YZ; Xie, JW; Tang, ZQ; Peng, X; Elgammal, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhu, Yizhe; Xie, Jianwen; Tang, Zhiqiang; Peng, Xi; Elgammal, Ahmed			Semantic-Guided Multi-Attention Localization for Zero-Shot Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Zero-shot learning extends the conventional object classification to the unseen class recognition by introducing semantic representations of classes. Existing approaches predominantly focus on learning the proper mapping function for visual-semantic embedding, while neglecting the effect of learning discriminative visual features. In this paper, we study the significance of the discriminative region localization. We propose a semantic-guided multi-attention localization model, which automatically discovers the most discriminative parts of objects for zero-shot learning without any human annotations. Our model jointly learns cooperative global and local features from the whole object as well as the detected parts to categorize objects based on semantic descriptions. Moreover, with the joint supervision of embedding softmax loss and class-center triplet loss, the model is encouraged to learn features with high inter-class dispersion and intra-class compactness. Through comprehensive experiments on three widely used zero-shot learning benchmarks, we show the efficacy of the multi-attention localization and our proposed approach improves the state-of-the-art results by a considerable margin.	[Zhu, Yizhe; Tang, Zhiqiang; Elgammal, Ahmed] Rutgers State Univ, New Brunswick, NJ 08901 USA; [Xie, Jianwen] Hikvis Res Inst, Hangzhou, Peoples R China; [Peng, Xi] Univ Delaware, Newark, DE 19716 USA	Rutgers State University New Brunswick; University of Delaware	Zhu, YZ (corresponding author), Rutgers State Univ, New Brunswick, NJ 08901 USA.	yizhe.zhu@rutgers.edu; jianwen@ucla.edu; zhiqiang.tang@rutgers.edu; xipeng@udel.edu; elgammal@cs.rutgers.edu	Zhu, Yizhe/AGO-2500-2022		NSFUSA award [1409683]	NSFUSA award	This work is partially supported by NSFUSA award 1409683.	Akata Z., 2016, CVPR; Akata Z., 2015, CVPR; Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; [Anonymous], 2016, ECCV; [Anonymous], 2014, ICLR; Changpinyo Soravit, 2016, CVPR; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Elhoseiny M., 2017, CVPR; Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875; Fu J., 2017, CVPR; Goodfellow Ian, 2014, 27 INT C NEURAL INFO; Han T., 2017, AAAI; Jaderberg Max, 2015, NEURIPS; Ji Zhong, 2018, NEURIPS; Kodirov E., 2017, CVPR; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Li Y., 2018, ABS180602877 ARXIV; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Morgado P., 2017, CVPR; Nilsback M. E., 2008, ICVGIP; Peng Xingchao, 2019, ICCV; Reed S. E., 2016, CVPR; Romera-Paredes B., 2015, ICML; Selvaraju R. R., 2017, ICCV; Snell J, 2017, NEURIPS; Vinyals Oriol, 2016, NEURIPS; Wah CK, 2011, HOUS SOC SER, P1; Wan Z., 2019, NEURIPS; Wang D., 2015, ICCV; Wang F., 2017, ACMMM; Wei S. E., 2016, CVPR; Wen Y., 2016, ECCV; Xian Y., 2016, CVPR; Xian Y., 2018, CVPR; Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768; Xiao T., 2015, CVPR; Zhang Hanwang, 2016, P IEEE C COMP VIS PA; Zhang L., 2017, CVPR; Zhang Xiaofan, 2016, CVPR; Zheng H., 2017, ICCV; Zhou B., 2016, CVPR; Zhu Yi, 2018, CVPR; Zhu Yizhe, 2019, ICCV	43	22	23	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906058
C	Zou, DF; Hu, ZN; Wang, YW; Jiang, S; Sun, YZ; Gu, QQ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zou, Difan; Hu, Ziniu; Wang, Yewen; Jiang, Song; Sun, Yizhou; Gu, Quanquan			Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Graph convolutional networks (GCNs) have recently received wide attentions, due to their successful applications in different graph tasks and different domains. Training GCNs for a large graph, however, is still a challenge. Original full-batch GCN training requires calculating the representation of all the nodes in the graph per GCN layer, which brings in high computation and memory costs. To alleviate this issue, several sampling-based methods have been proposed to train GCNs on a subset of nodes. Among them, the node-wise neighbor-sampling method recursively samples a fixed number of neighbor nodes, and thus its computation cost suffers from exponential growing neighbor size; while the layer-wise importance-sampling method discards the neighbor-dependent constraints, and thus the nodes sampled across layer suffer from sparse connection problem. To deal with the above two problems, we propose a new effective sampling algorithm called LAyer-Dependent ImportancE Sampling (LADIES) 2. Based on the sampled nodes in the upper layer, LADIES selects their neighborhood nodes, constructs a bipartite subgraph and computes the importance probability accordingly. Then, it samples a fixed number of nodes by the calculated probability, and recursively conducts such procedure per layer to construct the whole computation graph. We prove theoretically and experimentally, that our proposed sampling algorithm outperforms the previous sampling methods in terms of both time and memory costs. Furthermore, LADIES is shown to have better generalization accuracy than original full-batch GCN, due to its stochastic nature.	[Zou, Difan; Hu, Ziniu; Wang, Yewen; Jiang, Song; Sun, Yizhou; Gu, Quanquan] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Zou, DF (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.	knowzou@cs.ucla.edu; bull@cs.ucla.edu; wyw10804@cs.ucla.edu; songjiang@cs.ucla.edu; yzsun@cs.ucla.edu; qgu@cs.ucla.edu			NSF BIGDATA [IIS-1855099]; NSF CAREER Award [IIS-1906169, 1741634]; Salesforce Deep Learning Research Award; NSF [III-1705169, 1937599]; Amazon Research Award	NSF BIGDATA; NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Salesforce Deep Learning Research Award; NSF(National Science Foundation (NSF)); Amazon Research Award	We would like to thank the anonymous reviewers for their helpful comments. D. Zou and Q. Gu were partially supported by the NSF BIGDATA IIS-1855099, NSF CAREER Award IIS-1906169 and Salesforce Deep Learning Research Award. Z. Hu, Y. Wang, S. Jiang and Y. Sun were partially supported by NSF III-1705169, NSF 1937599, NSF CAREER Award 1741634, and Amazon Research Award. We also thank AWS for providing cloud computing credits associated with the NSF BIGDATA award. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.	Bloem P., 2018, P 15 EUR SEM WEB C E, P593, DOI [10.1007/978-3-319-93417-4_38, DOI 10.1007/978-3-319-93417-4_38]; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Chen J., 2018, 6 INT C LEARN REPR I; Chen JF, 2018, PR MACH LEARN RES, V80; Chiang WL, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P257, DOI 10.1145/3292500.3330925; Dai HJ, 2016, PR MACH LEARN RES, V48; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Hamilton WL, 2017, P 31 INT C NEUR INF, P1025; Hamilton WL, 2017, REPRESENTATION LEARN; Hardt M, 2016, PR MACH LEARN RES, V48; Huang W., 2018, ADAPTIVE SAMPLING FA, P4563; Kipf T.N, 5 INT C LEARN REPR I; LeCun Y, 1995, HDB BRAIN THEORY NEU, V3361, P1995, DOI DOI 10.1007/S13398-014-0173-7.2; Martino L, 2017, STAT COMPUT, V27, P599, DOI 10.1007/s11222-016-9642-5; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890	17	22	22	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902083
C	Heidari, H; Ferrari, C; Gummadi, KP; Krause, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Heidari, Hoda; Ferrari, Claudio; Gummadi, Krishna P.; Krause, Andreas			Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated Decision Making	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				UTILITY	We draw attention to an important, yet largely overlooked aspect of evaluating fairness for automated decision making systems-namely risk and welfare considerations. Our proposed family of measures corresponds to the long-established formulations of cardinal social welfare in economics, and is justified by the Rawlsian conception of fairness behind a veil of ignorance. The convex formulation of our welfare-based measures of fairness allows us to integrate them as a constraint into any convex loss minimization pipeline. Our empirical analysis reveals interesting trade-offs between our proposal and (a) prediction accuracy, (b) group discrimination, and (c) Dwork et al.'s notion of individual fairness. Furthermore and perhaps most importantly, our work provides both heuristic justification and empirical evidence suggesting that a lower-bound on our measures often leads to bounded inequality in algorithmic outcomes; hence presenting the first computationally feasible mechanism for bounding individual-level inequality.	[Heidari, Hoda; Ferrari, Claudio; Krause, Andreas] Swiss Fed Inst Technol, Zurich, Switzerland; [Gummadi, Krishna P.] MPI SWS, Saarbrucken, Germany	Swiss Federal Institutes of Technology Domain; ETH Zurich	Heidari, H (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	hheidari@inf.ethz.ch; ferraric@ethz.ch; gummadi@mpi-sws.org; krausea@ethz.ch		Krause, Andreas/0000-0001-7260-9673	CTI grant [27248.1 PFES-ES]; European Research Council (ERC) Advanced Grant "Foundations for Fair Social Computing" [789373]	CTI grant; European Research Council (ERC) Advanced Grant "Foundations for Fair Social Computing"(European Research Council (ERC))	H. Heidari and A. Krause acknowledge support from CTI grant no. 27248.1 PFES-ES. Krishna P. Gummadi was supported in part by a European Research Council (ERC) Advanced Grant "Foundations for Fair Social Computing" (No. 789373).	Amiel Y, 2003, RES EC INEQ, V9, P35; Angwin J., 2016, PROPUBLICA, P254; ATKINSON AB, 1970, J ECON THEORY, V2, P244, DOI 10.1016/0022-0531(70)90039-6; Barry-Jester Anna, 2015, NEW SCI SENTENCING; Calders T, 2013, IEEE DATA MINING, P71, DOI 10.1109/ICDM.2013.114; Carlsson F, 2005, ECONOMICA, V72, P375, DOI 10.1111/j.0013-0427.2005.00421.x; Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095; Cowell FA, 2001, EUR ECON REV, V45, P941, DOI 10.1016/S0014-2921(01)00121-0; DAGUM C, 1990, J ECONOMETRICS, V43, P91, DOI 10.1016/0304-4076(90)90109-7; Dalton H, 1920, ECON J, V30, P348, DOI 10.2307/2223525; Debreu Gerard, 1959, TECHNICAL REPORT; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Freeman Samuel, 2016, STANFORD ENCY PHILOS; GORMAN WM, 1968, REV ECON STUD, V35, P367, DOI 10.2307/2296766; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Harsanyi JC, 1955, J POLIT ECON, V63, P309, DOI 10.1086/257678; Harsanyi JC, 1953, J POLIT ECON, V61, P434, DOI 10.1086/257416; KAHNEMAN D, 1979, ECONOMETRICA, V47, P263, DOI 10.2307/1914185; Kamiran F., 2009, COMPUTER CONTROL COM, P1, DOI 10.1109/IC4.2009.4909197; Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83; Kleinberg J., 2017, P 8 INNOVATIONS THEO; Larson Jeff, 2016, DATA ANAL WE ANAL CO; Levin S., 2016, GUARDIAN; Miller Claire Cain, 2015, NEW YORK TIMES; Moulin H., 2004, FAIR DIVISION COLLEC; Petrasic K., 2017, WHITE CASE; Pigou AC, 1912, WEALTH WELFARE; Rawls J., 2009, THEORY JUSTICE; ROBERTS KWS, 1980, REV ECON STUD, V47, P421, DOI 10.2307/2297002; Rudin C., 2013, WIRED MAGAZINE; SCHWARTZ J, 1980, SOCIOL METHODOL, P1; SEN A, 1977, ECONOMETRICA, V45, P1539, DOI 10.2307/1913949; Speicher Till, 2018, P INT C KNOWL DISC D; Sweeney L., 2013, DISCRIMINATION ONLIN, V11, P10, DOI [DOI 10.1145/2460276.2460278, 10.1145/2460276.2460278]; Valera I., 2017, P 20 INT C ART INT S; VARIAN HR, 1974, J ECON THEORY, V9, P63, DOI 10.1016/0022-0531(74)90075-1; Zafar M. B., 2017, ADV NEURAL INFORM PR, P228	39	22	22	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301027
C	Huang, CW; Tan, S; Lacoste, A; Courville, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Huang, Chin-Wei; Tan, Shawn; Lacoste, Alexandre; Courville, Aaron			Improving Explorability in Variational Inference with Annealed Variational Objectives	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Despite the advances in the representational capacity of approximate distributions for variational inference, the optimization process can still limit the density that is ultimately learned. We demonstrate the drawbacks of biasing the true posterior to be unimodal, and introduce Annealed Variational Objectives (AVO) into the training of hierarchical variational methods. Inspired by Annealed Importance Sampling, the proposed method facilitates learning by incorporating energy tempering into the optimization objective. In our experiments, we demonstrate our method's robustness to deterministic warm up, and the benefits of encouraging exploration in the latent space.	[Huang, Chin-Wei; Tan, Shawn; Courville, Aaron] Univ Montreal, MILA, Montreal, PQ, Canada; [Huang, Chin-Wei; Lacoste, Alexandre] Element AI, Montreal, PQ, Canada	Universite de Montreal	Huang, CW (corresponding author), Univ Montreal, MILA, Montreal, PQ, Canada.; Huang, CW (corresponding author), Element AI, Montreal, PQ, Canada.	chin-wei.huang@umontreal.ca; jing.shan.shawn.tan@umontreal.ca; allac@elementai.com; aaron.courville@umontreal.ca						Abrol F., 2014, STAT, V12, P1; Agakov F. V., 2004, NEURAL INFORM PROCES; [Anonymous], ARXIV171105717; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Burda Yuri, 2016, 4 INT C LEARN REPR I; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; Cremer C, 2018, PR MACH LEARN RES, V80; Dai Jifeng, 2016, ADV NEURAL INFORM PR, P379, DOI DOI 10.1016/J.JPOWSOUR.2007.02.075; Dieng A. B., 2017, ADV NEURAL INFORM PR, P2732; Djork-Arn, ICLR 2016; Edwards H., 2017, NEURAL STAT, P2; Gulrajani I., 2017, INT C LEARN REPR; Huang CW, 2018, PR MACH LEARN RES, V80; Huszar F., 2017, ARXIV170208235; Karl M., 2016, INT C LEARN REPR; Katahira K, 2008, J PHYS CONF SER, V95, DOI 10.1088/1742-6596/95/1/012015; Kim Y, 2018, PR MACH LEARN RES, V80; Kingma D.P, P 3 INT C LEARNING R; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Krishnan Rahul G, 2017, ARXIV171006085; Krueger D., 2017, ARXIV PREPRINT ARXIV; Larochelle H., 2011, INT C ART INT STAT; Maaloe L, 2016, PR MACH LEARN RES, V48; Mandt S., 2016, INT C ART INT STAT; Marino J, 2018, PR MACH LEARN RES, V80; Mescheder L, 2017, PR MACH LEARN RES, V70; Nair V, 2010, P 27 INT C MACHINE L, P807; Neal R. M., 2001, STAT COMPUTING, V11; Nowozin S., 2018, INT C LEARN REPR; Raiko T., 2007, J MACHINE LEARNING R; Rainforth Tom, 2018, ARXIV180204537; Ranganath R, 2016, PR MACH LEARN RES, V48; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Salimans T, 2016, ADV NEUR IN, V29; Salimans T, 2015, PR MACH LEARN RES, V37, P1218; Shi J., 2018, INT C LEARN REPR; Sonderby CK, 2016, ADV NEUR IN, V29; Tomczak J. M., 2017, BENELEARN; Tomczak Jakub M, 2016, ARXIV161109630; Turner R. E., 2011, BAYESIAN TIME SERIES, P109; van den Berg R, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P393	43	22	22	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004028
C	Lee, D; Liu, SF; Gu, JW; Liu, MY; Yang, MH; Kautz, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lee, Donghoon; Liu, Sifei; Gu, Jinwei; Liu, Ming-Yu; Yang, Ming-Hsuan; Kautz, Jan			Context-Aware Synthesis and Placement of Object Instances	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Learning to insert an object instance into an image in a semantically coherent manner is a challenging and interesting problem. Solving it requires (a) determining a location to place an object in the scene and (b) determining its appearance at the location. Such an object insertion model can potentially facilitate numerous image editing and scene parsing applications. In this paper, we propose an end-to-end trainable neural network for the task of inserting an object instance mask of a specified class into the semantic label map of an image. Our network consists of two generative modules where one determines where the inserted object mask should be (i.e., location and scale) and the other determines what the object mask shape (and pose) should look like. The two modules are connected together via a spatial transformation network and jointly trained. We devise a learning procedure that leverage both supervised and unsupervised data and show our model can insert an object at diverse locations with various appearances. We conduct extensive experimental validations with comparisons to strong baselines to verify the effectiveness of the proposed network. Code is available at https://github.com/NVlabs/Instance_Insertion.	[Lee, Donghoon] Seoul Natl Univ, Seoul, South Korea; [Lee, Donghoon; Yang, Ming-Hsuan] Google Cloud AI, Sunnyvale, CA USA; [Lee, Donghoon; Liu, Sifei; Gu, Jinwei; Liu, Ming-Yu; Kautz, Jan] NVIDIA, Santa Clara, CA 95051 USA; [Yang, Ming-Hsuan] Univ Calif Merced, Merced, CA USA	Seoul National University (SNU); Nvidia Corporation; University of California System; University of California Merced	Lee, D (corresponding author), Seoul Natl Univ, Seoul, South Korea.; Lee, D (corresponding author), Google Cloud AI, Sunnyvale, CA USA.; Lee, D (corresponding author), NVIDIA, Santa Clara, CA 95051 USA.	donghoon.lee@rllab.snu.ac.kr; sifeil@nvidia.com; jinweig@nvidia.com; mingyul@nvidia.com; mhyang@ucmerced.edu; jkautz@nvidia.com	Yang, Ming-Hsuan/T-9533-2019; Yang, Ming-Hsuan/AAE-7350-2019; Liu, Sifei/AGE-1968-2022	Yang, Ming-Hsuan/0000-0003-4848-2304; Liu, Sifei/0000-0002-6011-3686	NSF CAREER Grant [1149783]	NSF CAREER Grant(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work was conducted in NVIDIA. Ming-Hsuan Yang is supported in part by the NSF CAREER Grant #1149783 and gifts from NVIDIA.	Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Bar M, 1996, PERCEPTION, V25, P343, DOI 10.1068/p250343; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833; HUANG X, 2018, EUR C COMP VIS; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Kingma D. P., 2013, AUTO ENCODING VARIAT; Larsen ABL, 2016, PR MACH LEARN RES, V48; Li C.-L., 2018, ARXIV PREPRINT ARXIV; Li Yi, 2017, P IEEE C COMP VIS PA; Lin CH, 2018, PROC CVPR IEEE, P9455, DOI 10.1109/CVPR.2018.00985; Liu Ming-Yu, 2016, ADV NEURAL INFORM PR, P2; Liu Ming-Yu, 2017, NIPS; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Qi XJ, 2018, PROC CVPR IEEE, P283, DOI 10.1109/CVPR.2018.00037; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed  S., 2016, NEURAL INFORM PROCES; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Sun J, 2017, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2017.136; Taigman Yaniv, 2017, 5 INT C LEARN REPR I; Tan  F., 2018, IEEE WINT C APPL COM; Tobin Josh, 2017, IROS; Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang Ting-Chun, 2018, ARXIV180806601; Wang XL, 2017, PROC CVPR IEEE, P3366, DOI 10.1109/CVPR.2017.359; Zhang H., 2017, ICCV; Zhu Jun-Yan, 2017, ICCV	35	22	22	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005001
C	Morgado, P; Vasconcelos, N; Langlois, T; Wang, O		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Morgado, Pedro; Vasconcelos, Nuno; Langlois, Timothy; Wang, Oliver			Self-Supervised Generation of Spatial Audio for 360 degrees Video	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				LOCALIZATION; TRACKING	We introduce an approach to convert mono audio recorded by a 360 degrees video camera into spatial audio, a representation of the distribution of sound over the full viewing sphere. Spatial audio is an important component of immersive 360 degrees video viewing, but spatial audio microphones are still rare in current 360 degrees video production. Our system consists of end-to-end trainable neural networks that separate individual sound sources and localize them on the viewing sphere, conditioned on multi-modal analysis of audio and 360 degrees video frames. We introduce several datasets, including one filmed ourselves, and one collected in-the-wild from YouTube, consisting of 360 degrees videos uploaded with spatial audio. During training, ground-truth spatial audio serves as self-supervision and a mixed down mono track forms the input to our network. Using our approach, we show that it is possible to infer the spatial location of sound sources based only on 360 degrees video and a mono audio track.	[Morgado, Pedro; Vasconcelos, Nuno] Univ Calif San Diego, La Jolla, CA 92093 USA; [Langlois, Timothy; Wang, Oliver] Adobe Res, Seattle, WA USA	University of California System; University of California San Diego; Adobe Systems Inc.	Morgado, P (corresponding author), Univ Calif San Diego, La Jolla, CA 92093 USA.	pmaravil@eng.ucsd.edu	Morgado, Pedro/ABE-8309-2021	Morgado, Pedro/0000-0002-0955-6510	Portuguese Ministry of Sciences and Education [SFRH/BD/109135/2015]; NRI [IIS-1637941]	Portuguese Ministry of Sciences and Education; NRI	This work was partially funded by graduate fellowship SFRH/BD/109135/2015 from the Portuguese Ministry of Sciences and Education and NRI Grant IIS-1637941.	Afouras T, 2018, INTERSPEECH, P3244; Amari S., 1996, ADV NEURAL INFORM PR; Argentieri S, 2015, COMPUT SPEECH LANG, V34, P87, DOI 10.1016/j.csl.2015.03.003; Barzelay Z., 2007, IEEE C COMP VIS PATT; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dickins G., 1999, AUDIO ENG SOC CONVEN; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Ephrat Ariel, 2018, ACM SIGGRAPH; Gabbay A, 2018, INTERSPEECH, P1170; GERZON MA, 1973, J AUDIO ENG SOC, V21, P2; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; GRAY AH, 1976, IEEE T ACOUST SPEECH, V24, P380, DOI 10.1109/TASSP.1976.1162849; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132; Hornstein J., 2006, IEEE RSJ INT C INT R; IIZUKA S, 2016, ACM T GRAPHIC, V35, DOI DOI 10.1145/2897824.2925974; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Izadinia H, 2013, IEEE T MULTIMEDIA, V15, P378, DOI 10.1109/TMM.2012.2228476; Jozefowicz Rafal, 2016, ARXIV160202410; JUTTEN C, 1991, SIGNAL PROCESSING, V24; Kidron E, 2005, PROC CVPR IEEE, P88; Kidron E, 2007, IEEE T SIGNAL PROCES, V55, P1390, DOI 10.1109/TSP.2006.888095; Kim J, 2016, IEEE CONF COMPUT; Kingma D.P, P 3 INT C LEARNING R; Kronlachner M., 2014, THESIS; Kuleshov V., 2017, ICLR WORKSH TRACK; Levina E., 2001, IEEE INT C COMP VIS; Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950; Nakadai K., 2002, INT C SPOK LANG PROC; Nakamura K., 2011, IEEE RSJ INT C INT R; Oord A.V.D., 2016, SSW; Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39; Owens A, 2016, PROC CVPR IEEE, P2405, DOI 10.1109/CVPR.2016.264; Po-Sen Huang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P205, DOI 10.1109/ICASSP.2014.6853587; Santala O, 2011, J ACOUST SOC AM, V129, P1522, DOI 10.1121/1.3533727; Smith J.O., 2007, MATH DISCRETE FOURIE; Soler M., 2016, EUR C COMP VIS ECCV; Strobel N, 2001, IEEE SIGNAL PROC MAG, V18, P22, DOI 10.1109/79.911196; Valin JM, 2007, ROBOT AUTON SYST, V55, P216, DOI 10.1016/j.robot.2006.08.004; Wang DL, 2018, IEEE-ACM T AUDIO SPE, V26, P1702, DOI 10.1109/TASLP.2018.2842159; Wilson EB, 1927, J AM STAT ASSOC, V22, P209, DOI 10.2307/2276774; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25	46	22	22	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300034
C	Morton, J; Witherden, FD; Jameson, A; Kochenderfer, MJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Morton, Jeremy; Witherden, Freddie D.; Jameson, Antony; Kochenderfer, Mykel J.			Deep Dynamical Modeling and Control of Unsteady Fluid Flows	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				FEEDBACK-CONTROL; VORTEX; CYLINDER	The design of flow control systems remains a challenge due to the nonlinear nature of the equations that govern fluid flow. However, recent advances in computational fluid dynamics (CFD) have enabled the simulation of complex fluid flows with high accuracy, opening the possibility of using learning-based approaches to facilitate controller design. We present a method for learning the forced and unforced dynamics of airflow over a cylinder directly from CFD data. The proposed approach, grounded in Koopman theory, is shown to produce stable dynamical models that can predict the time evolution of the cylinder system over extended time horizons. Finally, by performing model predictive control with the learned dynamical models, we are able to find a straightforward, interpretable control law for suppressing vortex shedding in the wake of the cylinder.	[Morton, Jeremy; Witherden, Freddie D.; Kochenderfer, Mykel J.] Stanford Univ, Dept Aeronaut & Astronaut, Stanford, CA 94305 USA; [Jameson, Antony] Texas A&M Univ, Dept Aerosp Engn, College Stn, TX 77843 USA	Stanford University; Texas A&M University System; Texas A&M University College Station	Morton, J (corresponding author), Stanford Univ, Dept Aeronaut & Astronaut, Stanford, CA 94305 USA.	jmorton2@stanford.edu; fdw@stanford.edu; antony.jameson@tamu.edu; mykel@stanford.edu			National Science Foundation Graduate Research Fellowship Program [DGE-114747]; Air Force Office of Scientific Research [FA9550-14-1-0186]	National Science Foundation Graduate Research Fellowship Program(National Science Foundation (NSF)); Air Force Office of Scientific Research(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	The authors would like to thank the reviewers for their insightful feedback. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE-114747. The authors would like to thank the Air Force Office of Scientific Research for their support via grant FA9550-14-1-0186.	Abadi M, 2015, P 12 USENIX S OPERAT; BERGER E, 1967, PHYS FLUIDS, V10, pS191, DOI 10.1063/1.1762444; Castonguay P, 2012, J SCI COMPUT, V51, P224, DOI 10.1007/s10915-011-9505-3; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Diamond S, 2016, J MACH LEARN RES, V17; Gunzburger MD, 1996, J APPL MECH-T ASME, V63, P828, DOI 10.1115/1.2823369; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hessel M., 2017, ABS171002298 CORR; Huynh H. T., 2007, AIAA PAPER 2007 4079, P4079, DOI DOI 10.2514/6.2007-4079; Illingworth SJ, 2016, THEOR COMP FLUID DYN, V30, P429, DOI 10.1007/s00162-016-0389-6; Illingworth SJ, 2014, PHYS REV E, V90, DOI 10.1103/PhysRevE.90.043014; Jameson A, 2012, J SCI COMPUT, V50, P434, DOI 10.1007/s10915-011-9490-6; Kaiser E., 2017, ARXIV170701146; Karl M., 2017, PROC INT C LEARN REP, P1; Koopman BO, 1931, P NATL ACAD SCI USA, V17, P315, DOI 10.1073/pnas.17.5.315; Korda M., 2016, ARXIV161103537; Kutz JN, 2016, OTHER TITL APPL MATH, V149; Lesort T, 2018, NEURAL NETWORKS, V108, P379, DOI 10.1016/j.neunet.2018.07.006; Li P, 2017, CHAOS, V27, DOI 10.1063/1.4982794; Lusch B., 2018, ARXIV171209707; Mettler B., 1999, PROC ANN FORUM AM HE, V2, P1706; Mishra N., 2017, ARXIV170304070; Nagabandi A., 2017, ARXIV171105253; Nagabandi A., 2017, ARXIV170802596; PARK DS, 1994, PHYS FLUIDS, V6, P2390, DOI 10.1063/1.868188; Proctor JL, 2016, SIAM J APPL DYN SYST, V15, P142, DOI 10.1137/15M1013857; ROSHKO A, 1955, J AERONAUT SCI, V22, P124; ROUSSOPOULOS K, 1993, J FLUID MECH, V248, P267, DOI 10.1017/S0022112093000771; Rowley CW, 2017, ANNU REV FLUID MECH, V49, P387, DOI 10.1146/annurev-fluid-010816-060042; Rowley CW, 2009, J FLUID MECH, V641, P115, DOI 10.1017/S0022112009992059; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Takeishi N., 2017, ADV NEURAL INFORM PR; Venkatraman A, 2015, AAAI CONF ARTIF INTE, P3024; Vincent PE, 2011, J SCI COMPUT, V47, P50, DOI 10.1007/s10915-010-9420-z; Watter Manuel, 2015, ADV NEURAL INFORM PR, P2746; Williams DM, 2014, J SCI COMPUT, V59, P721, DOI 10.1007/s10915-013-9780-2; Williams M. O., 2015, J NONLINEAR SCI, V8, P1, DOI DOI 10.1007/S00332-015-9258-5; Williamson CHK, 1996, ANNU REV FLUID MECH, V28, P477, DOI 10.1146/annurev.fl.28.010196.002401; Witherden FD, 2014, COMPUT PHYS COMMUN, V185, P3028, DOI 10.1016/j.cpc.2014.07.011; Yeung E, 2017, ARXIV170806850	41	22	22	3	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003078
C	Zhang, LJ; Lu, SY; Zhou, ZH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Lijun; Lu, Shiyin; Zhou, Zhi-Hua			Adaptive Online Learning in Dynamic Environments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we study online convex optimization in dynamic environments, and aim to bound the dynamic regret with respect to any sequence of comparators. Existing work have shown that online gradient descent enjoys an O(root T(1 + P-T)) dynamic regret, where T is the number of iterations and P-T is the path-length of the comparator sequence. However, this result is unsatisfactory, as there exists a large gap from the Omega(root T(1 + P-T)) lower bound established in our paper. To address this limitation, we develop a novel online method, namely adaptive learning for dynamic environment (Ader), which achieves an optimal O(root T(1 + P-T)) dynamic regret. The basic idea is to maintain a set of experts, each attaining an optimal dynamic regret for a specific path-length, and combines them with an expert-tracking algorithm. Furthermore, we propose an improved Ader based on the surrogate loss, and in this way the number of gradient evaluations per round is reduced from O(log T) to 1. Finally, we extend Ader to the setting that a sequence of dynamical models is available to characterize the comparators.	[Zhang, Lijun; Lu, Shiyin; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China	Nanjing University	Zhang, LJ (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.	zhanglj@lamda.nju.edu.cn; lusy@lamda.nju.edu.cn; zhouzh@lamda.nju.edu.cn			National Key R&D Program of China [2018YFB1004300]; NSFC [61603177]; JiangsuSF [BK20160658]; Microsoft Research Asia; YESS [2017QNRC001]	National Key R&D Program of China; NSFC(National Natural Science Foundation of China (NSFC)); JiangsuSF; Microsoft Research Asia(Microsoft); YESS	This work was partially supported by the National Key R&D Program of China (2018YFB1004300), NSFC (61603177), JiangsuSF (BK20160658), YESS (2017QNRC001), and Microsoft Research Asia.	Abernethy J., 2008, PROC 19 ANN C LEARNI, P415; Besbes O, 2015, OPER RES, V63, P1227, DOI 10.1287/opre.2015.1408; Boyd S, 2004, CONVEX OPTIMIZATION; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chiang C.-K., 2012, P 25 ANN C LEARN THE; Daniely A, 2015, PR MACH LEARN RES, V37, P1405; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Hall, 2013, P 30 INT C MACH LEAR, P579; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Hazan E., 2007, ELECT C COMPUTATIONA, V88; Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876; Jadbabaie A, 2015, JMLR WORKSH CONF PRO, V38, P398; Mokhtari A, 2016, IEEE DECIS CONTR P, P7195, DOI 10.1109/CDC.2016.7799379; Rakhlin A., 2013, C LEARN THEOR, V30, P993; SHALEV- SHWARTZ S., 2007, P 24 INT C MACH LEAR, P807, DOI [DOI 10.1145/1273496.1273598, 10.1145/1273496.1273598]; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Srebro N., 2010, ADV NEURAL INFORM PR, P2199; van Erven T, 2016, ADV NEUR IN, V29; Yang TB, 2016, PR MACH LEARN RES, V48; Zhang L., 2018, ARXIV E PRINTS; Zhang L., 2013, P 30 INT C MACH LEAR; Zhang L., 2018, P 35 INT C MACH LEAR; Zhang L., 2017, ADV NEURAL INFORM PR, P732; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	26	22	23	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301032
C	Havrylov, S; Titov, I		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Havrylov, Serhii; Titov, Ivan			Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SIMULATION	Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator (Jang et al., 2017)) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.	[Havrylov, Serhii; Titov, Ivan] Univ Edinburgh, Sch Informat, ILCC, Edinburgh, Midlothian, Scotland; [Titov, Ivan] Univ Amsterdam, ILLC, Amsterdam, Netherlands	University of Edinburgh; University of Amsterdam	Havrylov, S (corresponding author), Univ Edinburgh, Sch Informat, ILCC, Edinburgh, Midlothian, Scotland.	s.havrylov@inf.ed.ac.uk; ititov@inf.ed.ac.uk	Jeong, Yongwook/N-7413-2016		SAP ICN; ERC Starting Grant BroadSem [678254]; NWO Vidi Grant [639.022.518]	SAP ICN; ERC Starting Grant BroadSem; NWO Vidi Grant(Netherlands Organization for Scientific Research (NWO))	This project is supported by SAP ICN, ERC Starting Grant BroadSem (678254) and NWO Vidi Grant (639.022.518). We would like to thank Jelle Zuidema and anonymous reviewers for their helpful suggestions and comments.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2003, ADV NEURAL INFORM PR; Baronchelli A, 2006, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2006/06/P06014; Batali John, 1998, APPROACHES EVOLUTION, V405, P426; Bengio Yoshua, 2013, ARXIV13083432; Bhatnagar S., 2012, STOCHASTIC RECURSIVE, V434; Brighton H, 2002, ARTIF LIFE, V8, P25, DOI 10.1162/106454602753694756; Byron Donna, 2009, P 12 EUR WORKSH NAT; Chen X, 2015, CORR, V1504, P325; Chrupala G, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL) AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (IJCNLP), VOL 2, P112; Das Abhishek, 2017, P INT C COMP VIS IM; Foerster JN, 2016, ADV NEUR IN, V29; Golland D., 2010, P 2010 C EMP METH NA, P410; Gulcehre Caglar, 2017, ARXIV170108718; Henderson J, 2008, COMPUT LINGUIST, V34, P487, DOI 10.1162/coli.2008.07-028-R2-05-82; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jang E., 2017, INT C LEARN REPR; Jorge Emilio, 2016, NEURAL INFORM PROCES; Kingma D.P, P 3 INT C LEARNING R; Kirby S, 2002, ARTIF LIFE, V8, P185, DOI 10.1162/106454602320184248; Lazaridou Angeliki, 2017, P INT C LEARN REPR; Lemon O, 2002, TRAITEMENT AUTOMATIQ, V43, P131; Lewis David K., 1969, CONVENTION PHILOS ST; Maddison Chris J, 2017, ICLR; Melis Edward, 2016, ARXIV160909315; Miao Yishu, 2016, EMNLP; Mikolov Tomas, 2015, NEURAL INFORM PROCES; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Mordatch I., 2017, ARXIV170304908; Nolfi S, 2010, EVOLUTION OF COMMUNICATION AND LANGUAGE IN EMBODIED AGENTS, P1, DOI 10.1007/978-3-642-01250-1; Nowak MA, 1999, P NATL ACAD SCI USA, V96, P8028, DOI 10.1073/pnas.96.14.8028; Polyak BT, 1973, PSEUDOGRADIENT ADAPT; Schatzmann J, 2006, KNOWL ENG REV, V21, P97, DOI 10.1017/S0269888906000944; Steels Luc, 2005, WHAT TRIGGERS EMERGE; Sukhbaatar S., 2016, ADV NEURAL INF PROCE, V29, P2244, DOI DOI 10.5555/3157096.3157348; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wagner K, 2003, ADAPT BEHAV, V11, P37, DOI 10.1177/10597123030111003; Werning M., 2011, OXFORD HDB COMPOSITI; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	39	22	22	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402020
C	Lample, G; Zeghidour, N; Usunier, N; Bordes, A; Denoyer, L; Ranzato, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lample, Guillaume; Zeghidour, Neil; Usunier, Nicolas; Bordes, Antoine; Denoyer, Ludovic; Ranzato, Marc'Aurelio			Fader Networks: Manipulating Images by Sliding Attributes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper introduces a new encoder-decoder architecture that is trained to reconstruct images by disentangling the salient information of the image and the values of attributes directly in the latent space. As a result, after training, our model can generate different realistic versions of an input image by varying the attribute values. By using continuous attribute values, we can choose how much a specific attribute is perceivable in the generated image This property could allow for applications where users can modify an image using sliding knobs, like faders on a mixing console, to change the facial expression of a portrait, or to update the color of some objects. Compared to the state-of-the-art which mostly relies on training adversarial networks in pixel space by altering attribute values at train time, our approach results in much simpler training schemes and nicely scales to multiple attributes. We present evidence that our model can significantly change the perceived value of the attributes while preserving the naturalness of images.	[Lample, Guillaume; Zeghidour, Neil; Usunier, Nicolas; Bordes, Antoine; Ranzato, Marc'Aurelio] Facebook AI Res, Menlo Pk, CA 94025 USA; [Lample, Guillaume; Denoyer, Ludovic] Sorbonne Univ, UPMC Univ Paris 06, UMR 7606, LIP6, Paris, France; [Zeghidour, Neil] PSL Res Univ, INRIA, CNRS, EHESS,LSCP,ENS, Paris, France	Facebook Inc; UDICE-French Research Universities; Sorbonne Universite; Centre National de la Recherche Scientifique (CNRS); Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite	Lample, G (corresponding author), Facebook AI Res, Menlo Pk, CA 94025 USA.; Lample, G (corresponding author), Sorbonne Univ, UPMC Univ Paris 06, UMR 7606, LIP6, Paris, France.	gl@fb.com; neilz@fb.com; usunier@fb.com; abordes@fb.com; ludovic.denoyer@lip6.fr; ranzato@fb.com	Jeong, Yongwook/N-7413-2016					Antipov G., 2017, ARXIV170201983; Bowman S. R., 2016, ARXIV, P10; Chen X, 2016, ADV NEUR IN, V29; Edwards Harrison, 2016, INT C LEARN REPR ICL, P3; Ganin Y, 2016, J MACH LEARN RES, V17; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Higgins Irina, 2017, P ICLR 2017; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Isola Phillip, 2018, IMAGE TO IMAGE TRANS; Kingma D.P, P 3 INT C LEARNING R; Kulkarni TD, 2015, ADV NEUR IN, V28; Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43; Lim T., 2016, ARXIV160907093; Liu Ziwei, 2015, P INT C COMP VIS ICC; Louppe G., 2016, ARXIV161101046; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; Perarnau G., 2016, NIPS WORKSH; Reed S, 2015, ADV NEURAL INFORM PR, P1252; Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863; Taigman Y., 2016, ARXIV161102200; Upchurch P., 2016, ARXIV161105507; Wolf Lior, 2017, ARXIV170405693; Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47; Yang Jimei, 2015, NIPS; Zhu J.-Y., 2017, ARXIV170310593	27	22	22	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406005
C	Lu, XY; Van Roy, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lu, Xiuyuan; Van Roy, Benjamin			Ensemble Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Thompson sampling has emerged as an effective heuristic for a broad range of online decision problems. In its basic form, the algorithm requires computing and sampling from a posterior distribution over models, which is tractable only for simple special cases. This paper develops ensemble sampling, which aims to approximate Thompson sampling while maintaining tractability even in the face of complex models such as neural networks. Ensemble sampling dramatically expands on the range of applications for which Thompson sampling is viable. We establish a theoretical basis that supports the approach and present computational results that offer further insight.	[Lu, Xiuyuan; Van Roy, Benjamin] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Lu, XY (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	lxy@stanford.edu; bvr@stanford.edu			Boeing; Adobe	Boeing; Adobe	This work was generously supported by a research grant from Boeing and a Marketing Research Award from Adobe.	Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Gal Y, 2016, PR MACH LEARN RES, V48; Gomez-Uribe Carlos, 2016, ARXIV160505697V1; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Ma Y, 2012, ENSEMBLE MACHINE LEARNING: METHODS AND APPLICATIONS, P1, DOI [10.1007/978-1-4419-9326-7, 10.1007/978-1-4419-9326-7_1]; Papandreou G., 2010, NIPS, V23, P1858; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285	8	22	22	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403032
C	Moseley, B; Wang, JR		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Moseley, Benjamin; Wang, Joshua R.			Approximation Bounds for Hierarchical Clustering: Average Linkage, Bisecting K-means, and Local Search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Hierarchical clustering is a data analysis method that has been used for decades. Despite its widespread use, the method has an underdeveloped analytical foundation. Having a well understood foundation would both support the currently used methods and help guide future improvements. The goal of this paper is to give an analytic framework to better understand observations seen in practice. This paper considers the dual of a problem framework for hierarchical clustering introduced by Dasgupta [Das16]. The main result is that one of the most popular algorithms used in practice, average linkage agglomerative clustering, has a small constant approximation ratio for this objective. Furthermore, this paper establishes that using bisecting k-means divisive clustering has a very poor lower bound on its approximation ratio for the same objective. However, we show that there are divisive algorithms that perform well with respect to this objective by giving two constant approximation algorithms. This paper is some of the first work to establish guarantees on widely used hierarchical algorithms for a natural objective function. This objective and analysis give insight into what these popular algorithms are optimizing and when they will perform well.	[Moseley, Benjamin] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Wang, Joshua R.] Stanford Univ, Dept Comp Sci, 353 Serra Mall, Stanford, CA 94305 USA	Carnegie Mellon University; Stanford University	Moseley, B (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	moseleyb@andrew.cmu.edu; joshua.wang@cs.stanford.edu	Jeong, Yongwook/N-7413-2016		Google Research Award; Yahoo Research Award; NSF [CCF-1524062, CCF-1617724, CCF-1733873, CCF-1725661]	Google Research Award(Google Incorporated); Yahoo Research Award; NSF(National Science Foundation (NSF))	Benjamin Moseley was supported in part by a Google Research Award, a Yahoo Research Award and NSF Grants CCF-1617724, CCF-1733873 and CCF-1725661. This work was partially done while the author was working at Washington University in St. Louis.; Joshua R. Wang was supported in part by NSF Grant CCF-1524062.	Ackerman M., 2012, P 26 AAAI C ART INT; Ackerman Margareta, 2016, J MACHINE LEARNING R, V17, P1; Arora S, 2009, J ACM, V56, DOI 10.1145/1502793.1502794; Awasthi P, 2015, PROCEEDINGS OF THE 6TH INNOVATIONS IN THEORETICAL COMPUTER SCIENCE (ITCS'15), P191, DOI 10.1145/2688073.2688116; Ben-David S., 2008, P ADV NEURAL INFORM, V21, P121; Charikar M, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P841; Cohen-Addad Vincent, 2017, CORR; Dasgupta S, 2016, ACM S THEORY COMPUT, P118, DOI 10.1145/2897518.2897527; Ghahramani Z., 2005, P 22 INT C MACH LEAR, P297, DOI DOI 10.1145/1102351.1102389; Hastie T., 2009, ELEMENTS STAT LEARNI, P485, DOI [10.1007/978-0-387-84858-7_14, DOI 10.1007/978-0-387-84858-7_14]; Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011; Krishnamurthy Akshay, 2012, P 29 INT C MACH LEAR; Murtagh F, 2012, WIRES DATA MIN KNOWL, V2, P86, DOI 10.1002/widm.53; Roy A., 2016, ADV NEURAL INFORM PR, P2316; Zadeh R.B., 2009, P 25 C UNCERTAINTY A, P639	15	22	22	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403016
C	Nachum, O; Norouzi, M; Xu, K; Schuurmans, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Nachum, Ofir; Norouzi, Mohammad; Xu, Kelvin; Schuurmans, Dale			Bridging the Gap Between Value and Policy Based Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We establish a new connection between value and policy based reinforcement learning (RL) based on a relationship between softmax temporal value consistency and policy optimality under entropy regularization. Specifically, we show that softmax consistent action values correspond to optimal entropy regularized policy probabilities along any action sequence, regardless of provenance. From this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes a notion of soft consistency error along multi-step action sequences extracted from both on- and off-policy traces. We examine the behavior of PCL in different scenarios and show that PCL can be interpreted as generalizing both actor-critic and Q-learning algorithms. We subsequently deepen the relationship by showing how a single model can be used to represent both a policy and the corresponding softmax state values, eliminating the need for a separate critic. The experimental evaluation demonstrates that PCL significantly outperforms strong actor-critic and Q-learning baselines across several benchmarks.(2)	[Nachum, Ofir; Norouzi, Mohammad; Xu, Kelvin; Schuurmans, Dale] Google Brain, Mountain View, CA 94043 USA	Google Incorporated	Nachum, O (corresponding author), Google Brain, Mountain View, CA 94043 USA.	ofirnachum@google.com; mnorouzi@google.com; kelvinxx@google.com; daes@ualberta.ca	Jeong, Yongwook/N-7413-2016					Abbeel P., 2004, P 21 INT C MACHINE L, P1; [Anonymous], 2016, ICML; Antos A, 2008, MACH LEARN, V71, P89, DOI 10.1007/s10994-007-5038-2; Asadi K., 2016, ARXIV161205628; Azar M. G., 2011, AISTATS; Azar M. G., 2012, MACH LEARN J, V87; Azar M. G., 2012, JMLR, V13; Fox Roy, 2016, UAI; Gruslys A., 2017, ARXIV170404651; Gu S., 2017, ICLR; Gu S., 2016, ICRA; Haarnoja T., 2017, ARXIV170208165; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Huang D.-A., 2015, APPROXIMATE MAXENT I; Kakade S., 2001, NIPS; Kappen HJ, 2005, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2005/11/P11011; Kober J., 2013, IJRR; Levine S., 2016, JMLR, V17; Li L., 2010, CONTEXTUAL BANDIT AP; Lillicrap Timothy P, 2016, ICLR; Littman M., 1996, THESIS; Mnih V, 2015, NATURE, V518, P529; Munos R., 2016, NIPS; Nachum Ofir, 2017, ICLR; ODonoghue B., 2017, ICLR; Peng J, 1996, MACH LEARN, V22, P283, DOI 10.1007/BF00114731; Peters J., 2010, AAAI; Precup D., 2001, OFF POLICY TEMPORAL; Precup D., 2000, P 17 INT C MACH LEAR; Schaul T., 2016, ICLR; Schulman J., 2015, ICML; Schulman J., 2016, 4 INT C LEARN REPR; Schulman John, 2017, EQUIVALENCE POLICY; Silver D., 2014, ICML; Sutton, 2017, INTRO REINFORCEMENT; Sutton R. S., 1999, NIPS; Tesauro G., 1995, CACM; Theocharous G., 2015, IJCAI; Todorov E, 2006, NIPS; Todorov Emanuel, 2010, NIPS; Wang Z., 2016, ICLR; Wang Ziyu, 2017, ICLR; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Watkins CJCH., 1989, THESIS; Williams R. J., 1992, MACH LEARN J; Williams Ronald J, 1991, CONNECTION SCI; Ziebart B. D., 2008, AAAI; Ziebart B. D., 2010, THESIS	48	22	22	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402080
C	Seo, PH; Lehrmann, A; Han, B; Sigal, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Seo, Paul Hongsuck; Lehrmann, Andreas; Han, Bohyung; Sigal, Leonid			Visual Reference Resolution using Attention Memory for Visual Dialog	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Visual dialog is a task of answering a series of inter-dependent questions given an input image, and often requires to resolve visual references among the questions. This problem is different from visual question answering (VQA), which relies on spatial attention (a.k.a. visual grounding) estimated from an image and question pair. We propose a novel attention mechanism that exploits visual attentions in the past to resolve the current reference in the visual dialog scenario. The proposed model is equipped with an associative attention memory storing a sequence of previous (attention, key) pairs. From this memory, the model retrieves the previous attention, taking into account recency, which is most relevant for the current question, in order to resolve potentially ambiguous references. The model then merges the retrieved attention with a tentative one to obtain the final attention for the current question; specifically, we use dynamic parameter prediction to combine the two attentions conditioned on the question. Through extensive experiments on a new synthetic visual dialog dataset, we show that our model significantly outperforms the state-of-the-art (by approximate to 16 % points) in situations, where visual reference resolution plays an important role. Moreover, the proposed model achieves superior performance ( approximate to 2 % points improvement) in the Visual Dialog dataset [1], despite having significantly fewer parameters than the baselines.	[Seo, Paul Hongsuck; Han, Bohyung] POSTECH, Pohang, South Korea; [Lehrmann, Andreas; Sigal, Leonid] Disney Res, Los Angeles, CA USA	Pohang University of Science & Technology (POSTECH)	Seo, PH (corresponding author), POSTECH, Pohang, South Korea.	hsseo@postech.ac.kr; andreas.lehrmann@disneyresearch.com; bhhan@postech.ac.kr; lsigal@disneyresearch.com	Jeong, Yongwook/N-7413-2016		IITP grant - Korea government (MSIT) [2017-0-01778, 2017-0-01780, 2016-0-00563]	IITP grant - Korea government (MSIT)	This work was supported in part by the IITP grant funded by the Korea government (MSIT) [2017-0-01778, Development of Explainable Human-level Deep Machine Learning Inference Framework; 2017-0-01780, The Technology Development for Event Recognition/Relational Reasoning and Learning Knowledge based System for Video Understanding; 2016-0-00563, Research on Adaptive Machine Learning Technology Development for Intelligent Autonomous Digital Companion].	Andreas J., 2016, CVPR; [Anonymous], 2017, ARXIV170306585; [Anonymous], 2015, NIPS; [Anonymous], 2016, ICML; Antol S., 2015, ICCV; Ba J., 2017, P 3 INT C LEARN REPR; Clark K., 2016, ACL; Clark Kevin, 2015, ACL; Clark Kevin, 2016, EMNLP; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Das A., 2017, CVPR; de Vries Harm, 2017, CVPR; Fukui A., 2016, EMNLP; Goyal Y., 2017, CVPR; Han B., 2016, P CVPR; Huang D. A., 2017, CVPR; Kim J, 2017, ICLR; Kumar A., 2016, ICML; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Lu J., 2016, NIPS; Malinowski M., 2015, P INT C COMP VIS; Mansimov E., 2016, P INT C LEARN REPR; Miller A., 2016, EMNLP; Mun J., 2016, AAAI; Mun J., 2016, RXIV161201669; Noh Hyeonwoo, 2016, ARXIV160603647; Rohrbach Anna, 2016, ECCV; Seo P.H., 2016, PREPRINT ARXIV160602; Simonyan Karen, 2015, INT C LEARN REPR; Strub F., 2017, ARXIV170305423; Tapaswi M., 2016, CVPR; Vinyals O., 2015, CVPR; Weston J., 2015, ICLR; Xiong C., 2016, P INT C MACHINE LEAR; Xu H., 2016, P EUR C COMP VIS ECC; Xu K., 2015, ICML; Yang Z., 2016, CVPR; Zhang P., 2016, CVPR; Zhu L., 2015, ARXIV151104670	39	22	23	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403076
C	Zhou, YB; Porwal, U; Zhang, C; Ngo, H; Nguyen, X; Re, C; Govindaraju, V		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zhou, Yingbo; Porwal, Utkarsh; Zhang, Ce; Ngo, Hung; Nguyen, XuanLong; Re, Christopher; Govindaraju, Venu			Parallel Feature Selection inspired by Group Testing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				INFORMATION; DIVERGENCE; FRAMEWORK; RELEVANCE	This paper presents a parallel feature selection method for classification that scales up to very high dimensions and large data sizes. Our original method is inspired by group testing theory, under which the feature selection procedure consists of a collection of randomized tests to be performed in parallel. Each test corresponds to a subset of features, for which a scoring function may be applied to measure the relevance of the features in a classification task. We develop a general theory providing sufficient conditions under which true features are guaranteed to be correctly identified. Superior performance of our method is demonstrated on a challenging relation extraction task from a very large data set that have both redundant features and sample size in the order of millions. We present comprehensive comparisons with state-of-the-art feature selection methods on a range of data sets, for which our method exhibits competitive performance in terms of running time and accuracy. Moreover, it also yields substantial speedup when used as a pre-processing step for most other existing methods.	[Zhou, Yingbo; Porwal, Utkarsh; Ngo, Hung; Govindaraju, Venu] SUNY Buffalo, CSE Dept, Buffalo, NY 14260 USA; [Zhang, Ce] Univ Wisconsin, CS Dept, Madison, WI USA; [Nguyen, XuanLong] Univ Michigan, EECS Dept, Ann Arbor, MI 48109 USA; [Re, Christopher] Stanford Univ, CS Dept, Stanford, CA 94305 USA	State University of New York (SUNY) System; State University of New York (SUNY) Buffalo; University of Wisconsin System; University of Wisconsin Madison; University of Michigan System; University of Michigan; Stanford University	Zhou, YB (corresponding author), SUNY Buffalo, CSE Dept, Buffalo, NY 14260 USA.	yingbozh@buffalo.edu; utkarshp@buffalo.edu; czhang@cs.wisc.edu; hungngo@buffalo.edu; xuanlong@umich.edu; chrismre@cs.stanford.edu; govind@buffalo.edu	Porwal, Utkarsh/AAC-3620-2019	Porwal, Utkarsh/0000-0003-2768-0184; Zhang, Ce/0000-0002-8105-7505				ALI SM, 1966, J ROY STAT SOC B, V28, P131; Amaldi Edoardo, 1997, APPROXIMABILITY MINI; Bekkerman R., 2003, Journal of Machine Learning Research, V3, P1183, DOI 10.1162/153244303322753625; Brown G, 2012, J MACH LEARN RES, V13, P27; Csiszar I., 1967, STUD SCI MATH HUNG, V2, P299; Ding C, 2003, PROCEEDINGS OF THE 2003 IEEE BIOINFORMATICS CONFERENCE, P523, DOI 10.1109/CSB.2003.1227396; Du Ding-Zhu, 2000, SERIES APPL MATH, V12; Dubhashi DP, 2009, CONCENTRATION OF MEASURE FOR THE ANALYSIS OF RANDOMIZED ALGORITHMS, P1, DOI 10.1017/CBO9780511581274; Elisseeff A., 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616; Jakulin A., 2005, MACHINE LEARNING BAS; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; Kuncheva L.I., 2007, ARTIF INTELL, P421; LEWIS DD, 1992, SPEECH AND NATURAL LANGUAGE, P212; Lin DH, 2006, LECT NOTES COMPUT SC, V3951, P68; Meyer PE, 2006, LECT NOTES COMPUT SC, V3907, P91; Mintz Mike, 2009, P JOINT C 47 ANN M A, P1003, DOI DOI 10.3115/1690219.1690287; Ngo HQ, 2012, LEIBNIZ INT PR INFOR, V14, P230, DOI 10.4230/LIPIcs.STACS.2012.230; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Nguyen X, 2009, ANN STAT, V37, P876, DOI 10.1214/08-AOS595; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159; Stoppiglia H., 2003, Journal of Machine Learning Research, V3, P1399, DOI 10.1162/153244303322753733; Sun YJ, 2010, IEEE T PATTERN ANAL, V32, P1610, DOI 10.1109/TPAMI.2009.190; Tan M, 2010, P 27 INT C MACH LEAR, P1047; Yang HH, 2000, ADV NEUR IN, V12, P687; Yu L, 2004, J MACH LEARN RES, V5, P1205; Zhang C., 2012, P 50 ANN M ASS COMP, V1, P825	27	22	22	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100075
C	Crammer, K; Kandola, J; Singer, Y		Thrun, S; Saul, K; Scholkopf, B		Crammer, K; Kandola, J; Singer, Y			Online classification on a budget	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Online algorithms for classification often require vast amounts of memory and computation time when employed in conjunction with kernel functions. In this paper we describe and analyze a simple approach for an on-the-fly reduction of the number of past examples used for prediction. Experiments performed with real datasets show that using the proposed algorithmic approach with a single epoch is competitive with the support vector machine (SVM) although the latter, being a batch algorithm, accesses each training example multiple times.	Hebrew Univ Jerusalem, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Crammer, K (corresponding author), Hebrew Univ Jerusalem, IL-91904 Jerusalem, Israel.							Crammer K, 2003, J MACH LEARN RES, V3, P951, DOI 10.1162/jmlr.2003.3.4-5.951; GENTILE C, 2001, J MACHINE LEARNING R, V2, P213; Li Y, 2002, MACH LEARN, V46, P361, DOI 10.1023/A:1012435301888; MEZAR DM, 1987, J PHYS A, V20, P745; Novikoff, 1962, P S MATH THEOR AUT, P615; ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519; Vapnik V.N, 1998, STAT LEARNING THEORY	7	22	23	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						225	232						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500029
C	Parkes, DC; Singh, S		Thrun, S; Saul, K; Scholkopf, B		Parkes, DC; Singh, S			An MDP-based approach to online mechanism design	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Online mechanism design (MD) considers the problem of providing incentives to implement desired system-wide outcomes in systems with self-interested agents that arrive and depart dynamically. Agents can choose to misrepresent their arrival and departure times, in addition to information about their value for different outcomes. We consider the problem of maximizing the total long-term value of the system despite the self-interest of agents. The online MD problem induces a Markov Decision Process (MDP), which when solved can be used to implement optimal policies in a truth-revealing Bayesian-Nash equilibrium.	Harvard Univ, Div Engn & Appl Sci, Cambridge, MA 02138 USA	Harvard University	Parkes, DC (corresponding author), Harvard Univ, Div Engn & Appl Sci, Cambridge, MA 02138 USA.		Bi, Fan/R-9511-2017	Bi, Fan/0000-0003-1844-9685				AWERBUCH B, 2003, P ACM S THEOR COMP S; BLUM A, 2003, P 14 ANN ACM SIAM S; Friedman E., 2003, 4 ACM C EL COMM EC 0, P240, DOI 10.1145/779928.779978; LAVI R, 2000, P 2 ACM C EL COMM EC; Puterman M. L., 1994, MARKOV DECISION PROC, DOI DOI 10.1002/9780470316887; [No title captured]	6	22	22	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						791	798						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500099
C	Bach, FR; Jordan, MI		Dietterich, TG; Becker, S; Ghahramani, Z		Bach, FR; Jordan, MI			Thin junction trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We present an algorithm that induces a class of models with thin junction trees-models that are characterized by an upper bound on the size of the maximal cliques of their triangulated graph. By ensuring that the junction tree is thin, inference in our models remains tractable throughout the learning process. This allows both an efficient implementation of an iterative scaling parameter estimation algorithm and also ensures that inference can be performed efficiently with the final model. We illustrate the approach with applications in handwritten digit recognition and DNA splice site detection.	Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Bach, FR (corresponding author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.	fbach@cs.berkeley.edu; jordan@cs.berkeley.edu	Jordan, Michael I/C-5253-2013					Bodlaender HL, 1996, SIAM J COMPUT, V25, P1305, DOI 10.1137/S0097539793251219; CHOW CK, 1990, IEEE T INFORMATION T, V42, P393; Collobert R, 2001, J MACH LEARN RES, V1, P143, DOI 10.1162/15324430152733142; COWELL RG, 1999, PROBABILISTIC NETWOR; DARROCH JN, 1972, ANN MATH STAT, V43, P1470, DOI 10.1214/aoms/1177692379; DECOSTE D, 2002, MACH LEARN, V46, P1; DellaPietra S, 1997, IEEE T PATTERN ANAL, V19, P380, DOI 10.1109/34.588021; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; JIROUSEK R, 1995, COMPUT STAT DATA AN, V19, P177, DOI 10.1016/0167-9473(93)E0055-9; Kjaerulff U. B., 1990, R9009 AALB U DEP MAT; MAYRAZ G, 2001, ADV NIPS, P13; Meila M, 2001, J MACH LEARN RES, V1, P1, DOI 10.1162/153244301753344605; Srebro N., 2001, UAI; ZHU S, 1997, NEURAL COMPUTATION, V9	15	22	22	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						569	576						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100071
C	Lanckriet, GRG; El Ghaoui, L; Bhattacharyya, C; Jordan, MI		Dietterich, TG; Becker, S; Ghahramani, Z		Lanckriet, GRG; El Ghaoui, L; Bhattacharyya, C; Jordan, MI			Minimax probability machine	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					When constructing a classifier, the probability of correct classification of future data points should be maximized. In the current paper this desideratum is translated in a very direct way into an optimization problem, which is solved using methods from convex optimization. We also show how to exploit Mercer kernels in this setting to obtain nonlinear decision boundaries. A worst-case bound on the probability of misclassification of future data is obtained explicitly.	Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Lanckriet, GRG (corresponding author), Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA.		Jordan, Michael I/C-5253-2013					ANDERSON TW, 1962, ANN MATH STAT, V33, P420, DOI 10.1214/aoms/1177704568; Bertsimas D., 2000, HDB SEMIDEFINITE PRO, DOI 10.1007/978-1-4615-4381-7_16; Boyd S., 2001, CONVEX OPTIMIZATION; Breiman L., 1997, 460 U CAL STAT DEP; CHERNOFF H, 1972, FRONTIERS PATTERN RE, P55; ISII K, 1963, ANN I STAT MATH, V14, P185; Nesterov Y., 1994, INTERIOR POINT POLYN; RATSCH MM, 1999, NEURAL NETWORKS SIGN, V9, P41	8	22	22	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						801	807						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100100
C	Lebanon, G; Lafferty, J		Dietterich, TG; Becker, S; Ghahramani, Z		Lebanon, G; Lafferty, J			Boosting and maximum likelihood for exponential models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We derive an equivalence between AdaBoost and the dual of a convex optimization problem, showing that the only difference between minimizing the exponential loss used by AdaBoost and maximum likelihood for exponential models is that the latter requires the model to be normalized to form a conditional probability distribution over labels. In addition to establishing a simple and easily understood connection between the two methods, this framework enables us to derive new regularization procedures for boosting that directly correspond to penalized maximum likelihood. Experiments on UCI datasets support our theoretical analysis and give additional insight into the relationship between boosting and logistic regression.	Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Lebanon, G (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.							Chen S., 2000, IEEE T SPEECH AUDIO, V8; COLLINS M, IN PRESS MACHINE LEA; Della Pietra S., 2001, CMUCS01109; DELLAPIETRA S, 1997, IEEE T PATTERN ANAL, V19; DUFFY N, 2000, NEURAL INFORMATION P; Freund Y., 1996, INT C MACH LEARN; FRIEDMAN J, 2000, ANN STAT, V28; KIVINEN J, 1999, COMPUTATIONAL LEARNI; Lafferty J., 1999, COMPUTATIONAL LEARNI; Mason L, 1999, ADV LARGE MARGIN CLA; Ratsch Gunnar, 2001, MACHINE LEARNING	11	22	23	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						447	454						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100056
C	Jonsson, A; Barto, AG		Leen, TK; Dietterich, TG; Tresp, V		Jonsson, A; Barto, AG			Automated state abstraction for options using the U-Tree algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Learning a complex task can be significantly facilitated by defining a hierarchy of subtasks. An agent can learn to choose between various temporally abstract actions, each solving an assigned subtask, to accomplish the overall task. In this paper, we study hierarchical learning using the framework of options. We argue that to take full advantage of hierarchical structure, one should perform option-specific state abstraction, and that if this is to scale to larger tasks, state abstraction should be automated. We adapt McCallum's U-Tree algorithm to automatically build option-specific representations of the state feature space, and we illustrate the resulting algorithm using a simple hierarchical task. Results suggest that automated option-specific state abstraction is an attractive approach to making hierarchical learning systems more effective.	Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Jonsson, A (corresponding author), Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.	ajonsson@cs.umass.edu; barto@cs.umass.edu	Jonsson, Anders/B-2996-2016	Jonsson, Anders/0000-0002-5756-7847				Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639; Dietterich TG, 2000, ADV NEUR IN, V12, P994; DIGNEY B, 1996, ANIMALS ANIMATS, V4; McCallum AK., 1995, THESIS U ROCHESTER; Parr R, 1998, ADV NEUR IN, V10, P1043; Precup D, 1998, ADV NEUR IN, V10, P1050; SINGH SP, 1992, AAAI-92 PROCEEDINGS : TENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, P202; Sutton R. S., 1998, P 15 INT C MACH LEAR, P556; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; UTHER W, 1997, AAAI FALL S MOD DIR	10	22	24	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						1054	1060						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800148
C	Hinton, GE; Ghahramani, Z; Teh, YW		Solla, SA; Leen, TK; Muller, KR		Hinton, GE; Ghahramani, Z; Teh, YW			Learning to parse images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				LIKELIHOOD; ALGORITHM; NETWORKS	We describe a class of probabilistic models that we call credibility networks. Using parse trees as internal representations of images, credibility networks are able to perform segmentation and recognition simultaneously, removing the need for ad hoc segmentation heuristics. Promising results in the problem of segmenting handwritten digits were obtained.	Univ Coll London, Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Hinton, GE (corresponding author), Univ Coll London, Gatsby Computat Neurosci Unit, London WC1N 3AR, England.		Teh, Yee Whye/C-3400-2008					Dayan P., 1995, NEURAL COMPUT, V7, P1022, DOI [10.1162/neco.1995.7.5.889, DOI 10.1162/NECO.1995.7.5.889]; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; FREY BJ, 1995, ADV NEURAL INFORMATI, V8; Hinton GE, 1997, IEEE T NEURAL NETWOR, V8, P65, DOI 10.1109/72.554192; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; HINTON GE, 1997, USING MIXTURES FACTO; HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440; IRVING WW, 1995, IEEE T IMAGE PROCESS; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; LUETTGEN MR, 1995, IEEE T IMAGE PROCESS, V4, P194, DOI 10.1109/83.342185; Marr D., 1980, VISION COMPUTATIONAL; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Rumelhart D. E., 1988, PARALLEL DISTRIBUTED; SAUL LK, UNPUB ATTRACTOR DYNA; SIMARD P, 1992, ADV NEURAL INFORMATI, V5; TIPPING ME, 1997, NCRG97003 AST U DEP; ZEMEL RS, 1990, ADV NEURAL INFORMATI, V2	17	22	22	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						463	469						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700066
C	Grandvalet, Y; Canu, S		Kearns, MS; Solla, SA; Cohn, DA		Grandvalet, Y; Canu, S			Outcomes of the equivalence of adaptive ridge with least absolute shrinkage	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				NETWORKS	Adaptive Ridge is a special form of Ridge regression, balancing the quadratic penalization on each parameter of the model. It was shown to be equivalent to Lasso (least absolute shrinkage and selection operator), in the sense that both procedures produce the same estimate. Lasso can thus be viewed as a particular quadratic penalizer. From this observation, we derive a fixed point algorithm to compute the Lasso solution. The analogy provides also a new hyper-parameter for tuning effectively the model complexity. We finally present a series of possible extensions of lasso performing sparse regression in kernel smoothing, additive modeling and neural net training.	Univ Technol Compiegne, UMR CNRS 6599, F-60205 Compiegne, France	Centre National de la Recherche Scientifique (CNRS); Picardie Universites; Universite de Technologie de Compiegne	Grandvalet, Y (corresponding author), Univ Technol Compiegne, UMR CNRS 6599, BP 20-529, F-60205 Compiegne, France.			Canu, Stephane/0000-0002-7602-4557				Breiman L, 1996, ANN STAT, V24, P2350; Donoho DL, 1998, ANN STAT, V26, P879; GIROSI F, 1997, 1606 MIT AI LAB; GRANDVALET Y, 1998, PERSPECTIVES NEURAL, V1, P201; Hardle W, 1990, EC SOC MONOGRAPHS, V19; HASTIE T. J., 1990, MONOGRAPHS STAT APPL, V43; MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448; Neal R. M., 1996, LECT NOTES STAT; NOWLAN SJ, 1992, NEURAL COMPUT, V4, P473, DOI 10.1162/neco.1992.4.4.473; Tibshirani R. J., 1996, J ROYAL STAT SOC B, V58, P267	10	22	22	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						445	451						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700063
C	Stocker, A; Douglas, R		Kearns, MS; Solla, SA; Cohn, DA		Stocker, A; Douglas, R			Computation of smooth optical flow in a feedback connected analog network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				EARLY VISION	In 1986, Tanner and Mead [1] implemented an interesting constraint satisfaction circuit for global motion sensing in aVLSI. We report here a new and improved aVLSI implementation that provides smooth optical flow as well as global motion in a two dimensional visual held. The computation of optical now is an ill-posed problem, which expresses itself as the aperture problem. However, the optical flow can be estimated by the use of regularization methods, in which additional constraints are introduced in terms of a global energy functional that must be minimized. We show how the algorithmic constraints of Horn and Schunck [2] on computing smooth optical flow can be mapped onto the physical constraints of an equivalent electronic network.	Univ Zurich, Inst Neuroinformat, CH-8057 Zurich, Switzerland	University of Zurich	Stocker, A (corresponding author), Univ Zurich, Inst Neuroinformat, Winterthurerstr 190, CH-8057 Zurich, Switzerland.	alan@ini.phys.ethz.ch	Stocker, Alan A/A-6514-2008	Stocker, Alan A/0000-0002-2041-1515				HARRIS JG, 1990, INT J COMPUT VISION, V4, P211, DOI 10.1007/BF00054996; HORN BK, 1988, 1071 MIT AI LAB; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; HUTCHINSON J, 1988, COMPUTER, V21, P52, DOI 10.1109/2.31; LIU SC, 1997, ADV NEURAL INFORMATI, V10; POGGIO T, 1985, NATURE, V317, P314, DOI 10.1038/317314a0; POGGIO T, 1989, COMP NEUR S, P355; Tanner J., 1986, VLSI SIGNAL PROCESSI, VII; WYATT JL, 1995, VISION CHIPS IMPLEME, P72; YUILLE AL, 1989, BIOL CYBERN, V61, P115, DOI 10.1007/BF00204595	10	22	23	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						706	712						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700100
C	Yoon, H; Sompolinsky, H		Kearns, MS; Solla, SA; Cohn, DA		Yoon, H; Sompolinsky, H			The effect of correlations on the fisher information of population codes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				DISCHARGE; SYSTEMS; CORTEX	We study the effect of correlated noise On the accuracy of population coding using a model of a population of neurons that are broadly tuned to an angle in two-dimension. The fluctuations in the neuronal activity is modeled as a Gaussian noise with pairwise correlations which decays exponentially with the difference between the preferred orientations of the pair. By calculating the Fisher information of the system, we show that in the biologically relevant regime of parameters positive correlations decrease the estimation capability of the network relative to the uncorrelated population. Moreover strong positive correlations result in information capacity which saturates to a finite value as the number of cells in the population grows. In contrast, negative correlations substantially increase the information capacity of the neuronal population.	Hebrew Univ Jerusalem, Racah Inst Phys, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Yoon, H (corresponding author), Hebrew Univ Jerusalem, Racah Inst Phys, IL-91904 Jerusalem, Israel.		Sompolinsky, Haim/ABB-8398-2021					ABBOTT LF, 1998, IN PRESS NEURAL COMP; Lee D, 1998, J NEUROSCI, V18, P1161; PARADISO MA, 1988, BIOL CYBERN, V58, P35, DOI 10.1007/BF00363954; SEUNG HS, 1993, P NATL ACAD SCI USA, V90, P10749, DOI 10.1073/pnas.90.22.10749; SNIPPE HP, 1992, BIOL CYBERN, V66, P543, DOI 10.1007/BF00204120; SNIPPE HP, 1992, BIOL CYBERN, V67, P183, DOI 10.1007/BF00201025; YOON H, 1998, POPULATION CODING NE; ZOHARY E, 1994, NATURE, V370, P140, DOI 10.1038/370140a0; [No title captured]	9	22	22	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						167	173						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700024
C	Dayan, P; Long, T		Jordan, MI; Kearns, MJ; Solla, SA		Dayan, P; Long, T			Statistical models of conditioning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Conditioning experiments probe the ways that animals make predictions about rewards and punishments and use those predictions to control their behavior. One standard model of conditioning paradigms which involve many conditioned stimuli suggests that individual predictions should be added together. Various key results show that this model fails in some circumstances, and motivate an alternative model, in which there is attentional selection between different available stimuli. The new model is a form of mixture of experts, has a close relationship with some other existing psychological suggestions, and is statistically well-founded.	MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Dayan, P (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.								0	22	23	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						117	123						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700017
C	Neuneier, R		Jordan, MI; Kearns, MJ; Solla, SA		Neuneier, R			Enhancing Q-Learning for optimal asset allocation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					This paper enhances the Q-learning algorithm for optimal asset allocation proposed in (Neuneier, 1996 [6]). The new formulation simplifies the approach by using only one value-function for many assets and allows model-free policy-iteration. After testing the new algorithm on real data, the possibility of risk management within the framework of Markov decision problems is analyzed. The proposed methods allows the construction of a multi-period portfolio management system which takes into account transaction costs, the risk preferences of the investor, and several constraints on the allocation.	Siemens AG, Corp Technol, D-81730 Munich, Germany	Siemens AG; Siemens Germany	Neuneier, R (corresponding author), Siemens AG, Corp Technol, D-81730 Munich, Germany.								0	22	25	1	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						936	942						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700132
C	Weiss, Y		Mozer, MC; Jordan, MI; Petsche, T		Weiss, Y			Interpreting images by propagating Bayesian beliefs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				A central theme of computational vision research has been the realization that reliable estimation of local scene properties requires propagating measurements across the image. Many authors have therefore suggested solving vision problems using architectures of locally connected units updating their activity in parallel. Unforturaately, the convergence of traditional relaxation methods on such architectures has proven to be excruciatingly slow and in general they do not guarantee that the stable point will be a global minimum. In this paper we show that an architecture in which Bayesian Beliefs about image properties are propagated between neighboring units yields convergence times which are several orders of magnitude faster than traditional methods and avoids local minima. In particular our architecture is non-iterative in the sense of Marr [5]: at every time step, the local estimates at a given location are optimal given the information which has already been propagated to that location. We illustrate the algorithm's performance on real images and compare it to several existing methods.			Weiss, Y (corresponding author), MIT,DEPT BRAIN & COGNIT SCI,E10-120,CAMBRIDGE,MA 02139, USA.								0	22	22	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						908	914						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00128
C	Makeig, S; Jung, TP; Sejnowski, TJ		Touretzky, DS; Mozer, MC; Hasselmo, ME		Makeig, S; Jung, TP; Sejnowski, TJ			Using feedforward neural networks to monitor alertness from changes in EEG correlation and coherence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						USN,HLTH RES CTR,SAN DIEGO,CA 92186	United States Department of Defense; United States Navy; Naval Medical Research Center (NMRC); Naval Health Research Center (NHRC)			Sejnowski, Terrence/AAV-5558-2021						0	22	22	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						931	937						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00131
C	Neuneier, R		Touretzky, DS; Mozer, MC; Hasselmo, ME		Neuneier, R			Optimal asset allocation using adaptive dynamic programming	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SIEMENS AG,CORP RES & DEV,D-81730 MUNICH,GERMANY	Siemens AG; Siemens Germany									0	22	25	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						952	958						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00134
C	Senior, A; Robinson, T		Touretzky, DS; Mozer, MC; Hasselmo, ME		Senior, A; Robinson, T			Forward-backward retraining of recurrent neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV CAMBRIDGE,DEPT ENGN,CAMBRIDGE CB2 1TN,ENGLAND	University of Cambridge									0	22	24	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						743	749						7	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00105
C	Tenenbaum, JB		Touretzky, DS; Mozer, MC; Hasselmo, ME		Tenenbaum, JB			Learning the structure of similarity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	Advances in Neural Information Processing Systems		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						MIT, DEPT BRAIN & COGNIT SCI, CAMBRIDGE, MA 02139 USA	Massachusetts Institute of Technology (MIT)									0	22	22	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						3	9						7	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00001
C	Chang, JL; Zhang, XB; Guo, YW; Meng, GF; Xiang, SM; Pan, CH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chang, Jianlong; Zhang, Xinbang; Guo, Yiwen; Meng, Gaofeng; Xiang, Shiming; Pan, Chunhong			DATA: Differentiable ArchiTecture Approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Neural architecture search (NAS) is inherently subject to the gap of architectures during searching and validating. To bridge this gap, we develop Differentiable ArchiTecture Approximation (DATA) with an Ensemble Gumbel-Softmax (EGS) estimator to automatically approximate architectures during searching and validating in a differentiable manner. Technically, the EGS estimator consists of a group of Gumbel-Softmax estimators, which is capable of converting probability vectors to binary codes and passing gradients from binary codes to probability vectors. Benefiting from such modeling, in searching, architecture parameters and network weights in the NAS model can be jointly optimized with the standard back-propagation, yielding an end-to-end learning mechanism for searching deep models in a large enough search space. Conclusively, during validating, a high-performance architecture that approaches to the learned one during searching is readily built. Extensive experiments on a variety of popular datasets strongly evidence that our method is capable of discovering high-performance architectures for image classification, language modeling and semantic segmentation, while guaranteeing the requisite efficiency during searching.	[Chang, Jianlong; Zhang, Xinbang; Meng, Gaofeng; Xiang, Shiming; Pan, Chunhong] Chinese Acad Sci, Inst Automat, NLPR, Beijing, Peoples R China; [Chang, Jianlong; Zhang, Xinbang; Xiang, Shiming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China; [Chang, Jianlong] Samsung Res China, Beijing, Peoples R China; [Guo, Yiwen] Intel Labs, Beijing, Peoples R China; [Guo, Yiwen] Bytedance AI Lab, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Intel Corporation	Chang, JL (corresponding author), Chinese Acad Sci, Inst Automat, NLPR, Beijing, Peoples R China.; Chang, JL (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.; Chang, JL (corresponding author), Samsung Res China, Beijing, Peoples R China.	jianlong.chang@nlpr.ia.ac.cn; xinbang.zhang@nlpr.ia.ac.cn; guoyiwen.ai@bytedance.com; gfmeng@nlpr.ia.ac.cn; smxiang@nlpr.ia.ac.cn; chpan@nlpr.ia.ac.cn			Major Project for New Generation of AI Grant [2018AAA0100402]; National Natural Science Foundation of China [91646207, 61976208, 61773377, 61573352]	Major Project for New Generation of AI Grant; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This research was supported by Major Project for New Generation of AI Grant No. 2018AAA0100402, and the National Natural Science Foundation of China under Grants 91646207, 61976208, 61773377, and 61573352. We would like to thank Lele Yu, Jie Gu, Cheng Da, and Yukang Chen for their invaluable contributions in shaping the early stage of this work.	Adam, 2017, MOBILENETS EFFICIENT; Battaglia P.W., 2018, INT C LEARN REPR; Bello I., 2016, ARXIV PREPRINT ARXIV; Bello I, 2017, PR MACH LEARN RES, V70; Casale Francesco Paolo, 2019, PROBABILISTIC NEURAL; Caterini AL, 2018, SPRINGERBRIEF COMPUT, P11, DOI 10.1007/978-3-319-75304-1_2; Chen L.-C., 2017, RETHINKING ATROUS CO; Chen Y., 2019, DETNAS NEURAL ARCHIT, P6638; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dong XY, 2019, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2019.00186; Elsken T., 2018, ICLR WORKSH TRACK P; Elsken T, 2019, J MACH LEARN RES, V20; Elsken Thomas, 2018, EFFICIENT MULTI OBJE; Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720; Grave Edouard, 2016, ARXIV161204426; Gumbel E. J., 1954, STAT THEORY EXTREME; Guo Zichao, 2019, ARXIV190400420; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hsu C.-H, 2018, CORR; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Inan H, 2016, ARXIV161101462 CORR; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jang Eric, 2017, P 5 INT C LEARN REPR; Kamath Purushotham, 2018, CORR; Kandasamy Kirthevasan, 2018, ADV NEURAL INFORM PR, P2020, DOI DOI 10.5555/3326943.3327130; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; Li Liam, 2019, ABS190207638 CORR; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Liu Hanxiao, 2018, ARXIV180609055; Liu Hanxiao, 2017, INT C LEARN REPR ICL; Luo Renqian, 2018, NIPS; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; Maddison Chris J, 2016, ARXIV161100712; Melis G., 2017, ARXIV; Merity Stephen, 2017, ICLR; Miikkulainen R, 2017, ARXIV PREPRINT ARXIV; Pan C., 2019, IEEE T PATTERN ANAL; Pham H, 2018, PR MACH LEARN RES, V80; Real E., 2018, ARXIV180201548, DOI [DOI 10.1609/AAAI.V33I01.33014780, 10.1609/aaai.v33i01.33014780]; Real E, 2017, PR MACH LEARN RES, V70; Shin R., 2018, DIFFERENTIABLE NEURA; Stanley KO, 2019, NAT MACH INTELL, V1, P24, DOI 10.1038/s42256-018-0006-z; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tan M., 2018, CORR; Taylor Ann, 2003, TREEBANKS, P5, DOI DOI 10.1007/978-94-010-0201-1_1; Tran Du, 2017, ABS170805038 CORR; Xie S., 2018, INT C LEARNING REPRE; Yang Zhilin, 2017, ARXIV171103953; Ying C, 2019, PR MACH LEARN RES, V97; Yu Kaicheng, 2019, INT C LEARN REPR, P2; Zilly J.G., 2016, ARXIV PREPRINT ARXIV; Zoph B., 2016, ARXIV161101578; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	60	21	21	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300079
C	Dupont, E; Doucet, A; Teh, YW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dupont, Emilien; Doucet, Arnaud; Teh, Yee Whye			Augmented Neural ODEs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We show that Neural Ordinary Differential Equations (ODEs) learn representations that preserve the topology of the input space and prove that this implies the existence of functions Neural ODEs cannot represent. To address these limitations, we introduce Augmented Neural ODEs which, in addition to being more expressive models, are empirically more stable, generalize better and have a lower computational cost than Neural ODEs.	[Dupont, Emilien; Doucet, Arnaud; Teh, Yee Whye] Univ Oxford, Oxford, England	University of Oxford	Dupont, E (corresponding author), Univ Oxford, Oxford, England.	dupont@stats.ox.ac.uk; doucet@stats.ox.ac.uk; y.w.teh@stats.ox.ac.uk			Google DeepMind; UK Defence Science and Technology Laboratory (Dstl); Engineering and Physical Research Council (EPSRC) [EP/R013616/1]; European Research Council under the European Union [617071]	Google DeepMind(Google Incorporated); UK Defence Science and Technology Laboratory (Dstl); Engineering and Physical Research Council (EPSRC)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); European Research Council under the European Union(European Research Council (ERC))	We would like to thank Anthony Caterini, Daniel Paulin, Abraham Ng, Joost Van Amersfoort and Hyunjik Kim for helpful discussions and feedback. Emilien gratefully acknowledges his PhD funding from Google DeepMind. Arnaud Doucet acknowledges support of the UK Defence Science and Technology Laboratory (Dstl) and Engineering and Physical Research Council (EPSRC) under grant EP/R013616/1. This is part of the collaboration between US DOD, UK MOD and UK EPSRC under the Multidisciplinary University Research Initiative. Yee Whye Teh's research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617071.	[Anonymous], 2017, COMMUN MATH STAT; Ardizzone L., 2018, INT C LEARN REPR; Behrmann J., 2018, ARXIV181100995; Dinh L., 2016, ARXIV160508803CSSTAT; Grathwohl Will, 2018, ARXIV181001367; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kingma D. P., 2018, ADV NEURAL INFORM PR, P10215; Lin H, 2018, GEOECOL ESSAYS, P38; Lu Y., 2017, FINITE LAYER NEURAL; Papamakarios George, 2017, ADV NEURAL INFORM PR, P2338; Ruthotto L., 2018, ARXIV180404272; Tian Qi Chen, 2018, 32 C NEUR INF PROC S; Wang B., 2018, ARXIV181110745; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Younes L, 2010, APPL MATH SCI, V171, P177; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]	17	21	21	2	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303016
C	Haddadpour, F; Kamani, MM; Mahdavi, M; Cadambe, VR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Haddadpour, Farzin; Kamani, Mohammad Mahdi; Mahdavi, Mehrdad; Cadambe, Viveck R.			Local SGD with Periodic Averaging: Tighter Analysis and Adaptive Synchronization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Communication overhead is one of the key challenges that hinders the scalability of distributed optimization algorithms. In this paper, we study local distributed SGD, where data is partitioned among computation nodes, and the computation nodes perform local updates with periodically exchanging the model among the workers to perform averaging. While local SGD is empirically shown to provide promising results, a theoretical understanding of its performance remains open. We strengthen convergence analysis for local SGD, and show that local SGD can be far less expensive and applied far more generally than current theory suggests. Specifically, we show that for loss functions that satisfy the PolyakLojasiewicz condition, O((pT)(1/3)) rounds of communication suffice to achieve a linear speed up, that is, an error of O(1/pT), where T is the total number of model updates at each worker. This is in contrast with previous work which required higher number of communication rounds, as well as was limited to strongly convex loss functions, for a similar asymptotic performance. We also develop an adaptive synchronization scheme that provides a general condition for linear speed up. Finally, we validate the theory with experimental results, running over AWS EC2 clouds and an internal GPU cluster.	[Haddadpour, Farzin; Kamani, Mohammad Mahdi; Mahdavi, Mehrdad; Cadambe, Viveck R.] Penn State, State Coll, PA 16801 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University	Haddadpour, F (corresponding author), Penn State, State Coll, PA 16801 USA.	fxh18@psu.edu; mqk5591@psu.edu; mzm616@psu.edu; vxc12@psu.edu			NSF [CCF 1763657, CCF 1553248]	NSF(National Science Foundation (NSF))	This work was partially supported by the NSF CCF 1553248 and NSF CCF 1763657 grants.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Agarwal A, 2011, ADV NEURAL INFORM PR, P873; Aji A.F., 2017, P 2017 C EMP METH NA, DOI [10.18653/v1/D17-1045, DOI 10.18653/V1/D17-1045]; Alistarh D, 2017, ADV NEUR IN, V30; Bernstein J., 2018, ARXIV180204434, V80, P560; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Chen K, 2016, INT CONF ACOUST SPEE, P5880, DOI 10.1109/ICASSP.2016.7472805; De Sa Christopher, 2015, Adv Neural Inf Process Syst, V28, P2656; Dryden N, 2016, PROCEEDINGS OF 2016 2ND WORKSHOP ON MACHINE LEARNING IN HPC ENVIRONMENTS (MLHPC), P1, DOI [10.1109/MLHPC.2016.4, 10.1109/MLHPC.2016.004]; fiane Saadane So, 2017, ARXIV171007926; Friedlander Michael P, 2011, TECHNICAL REPORT; Golmant Noah, 2018, ARXIV181112941; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; Haddadpour F, 2019, PR MACH LEARN RES, V97; Haddadpour F, 2018, ANN ALLERTON CONF, P196, DOI 10.1109/ALLERTON.2018.8635933; Kamp M., 2018, P EUR C MACH LEARN K, P393; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Li JL, 2017, INT CONF MACH LEARN, P35; Lin T., 2018, P 8 INT C LEARN REPR, P1; Ma S., 1688, P INT C MACH LEARN S, V80; MCDONALD R, 2010, HUMAN LANGUAGE TECHN, V2010, P456; McMahan HB, 2017, PR MACH LEARN RES, V54, P1273; Na T, 2017, IEEE IJCNN, P3716, DOI 10.1109/IJCNN.2017.7966324; Paszke A., 2017, AUTOMATIC DIFFERENTI; Povey Daniel, 2014, ARXIV14107455; Recht B, 2011, ADV NEURAL INFORM PR, V2011, P693; Seide F, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2135, DOI 10.1145/2939672.2945397; Seide F, 2014, INTERSPEECH, P1058; Shamir O, 2014, ANN ALLERTON CONF, P850, DOI 10.1109/ALLERTON.2014.7028543; Stich S. U., 2019, PROC INT C LEARN REP, P1; Stich SU, 2018, ADV NEUR IN, V31; Strom N, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1488; Su H., 2015, ARXIV150701239; Sun X, 2017, PR MACH LEARN RES, V70; Wang J., 2018, ARXIV180807576; Wang Jianyu, 2018, ARXIV181008313; Wangni J., 2018, ADV NEURAL INFORM PR, P1299; Wen W., 2017, ADV NEURAL INFORM PR, P1, DOI DOI 10.1109/ICC.2017.7997306; Xiong FC, 2018, IEEE IMAGE PROC, P3219, DOI 10.1109/ICIP.2018.8451087; Yin D., 2017, ARXIV170605699V3; Yu H, 2019, PR MACH LEARN RES, V97; Yu H, 2019, AAAI CONF ARTIF INTE, P5693; Zhang Jian, 2016, ARXIV160607365; Zhang XC, 2020, IEEE T INTELL TRANSP, V21, P68, DOI 10.1109/TITS.2018.2888587; Zhang XR, 2018, PR MACH LEARN RES, V80; Zhang YC, 2012, IEEE DECIS CONTR P, P6792, DOI 10.1109/CDC.2012.6426691; Zhou Z, 2018, IMMUNOL INVEST, V47, P468, DOI 10.1080/08820139.2018.1458105; Zinkevich M., 2010, P ADV NEUR INF PROC, P2595	51	21	21	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902068
C	Heo, J; Joo, S; Moon, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Heo, Juyeon; Joo, Sunghwan; Moon, Taesup			Fooling Neural Network Interpretations via Adversarial Model Manipulation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We ask whether the neural network interpretation methods can be fooled via adversarial model manipulation, which is defined as a model fine-tuning step that aims to radically alter the explanations without hurting the accuracy of the original models, e.g., VGG19, ResNet50, and DenseNet121. By incorporating the interpretation results directly in the penalty term of the objective function for fine-tuning, we show that the state-of-the-art saliency map based interpreters, e.g., LRP, Grad-CAM, and SimpleGrad, can be easily fooled with our model manipulation. We propose two types of fooling, Passive and Active, and demonstrate such foolings generalize well to the entire validation set as well as transfer to other interpretation methods. Our results are validated by both visually showing the fooled explanations and reporting quantitative metrics that measure the deviations from the original explanations. We claim that the stability of neural network interpretation method with respect to our adversarial model manipulation is an important criterion to check for developing robust and reliable neural network interpretation method. The source code is available at https://github.com/rmrisforbidden/FoolingNeuralNetwork-Interpretations.	[Heo, Juyeon; Joo, Sunghwan; Moon, Taesup] Sungkyunkwan Univ, Dept Elect & Comp Engn, Suwon 16419, South Korea; [Moon, Taesup] Sungkyunkwan Univ, Dept Artificial Intelligence, Suwon 16419, South Korea	Sungkyunkwan University (SKKU); Sungkyunkwan University (SKKU)	Heo, J (corresponding author), Sungkyunkwan Univ, Dept Elect & Comp Engn, Suwon 16419, South Korea.	heojuyeon12@gmail.com; shjoo840@skku.edu; tsmoon@skku.edu			ICT R&D Program of MSIT/IITP of the Korean government [2016-0-00563, 2019-0-01396]; AI Graduate School Support Program of MSIT/IITP of the Korean government [2019-0-00421]; ITRC Support Program of MSIT/IITP of the Korean government [IITP-2019-2018-0-01798]; KIST Institutional Program [2E29330]	ICT R&D Program of MSIT/IITP of the Korean government(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); AI Graduate School Support Program of MSIT/IITP of the Korean government(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); ITRC Support Program of MSIT/IITP of the Korean government(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); KIST Institutional Program(Korea Institute of Science & Technology (KIST))	This work is supported in part by ICT R&D Program [No. 2016-0-00563, Research on adaptive machine learning technology development for intelligent autonomous digital companion][No. 2019-0-01396, Development of framework for analyzing, detecting, mitigating of bias in AI model and training data], AI Graduate School Support Program [No.2019-0-00421], and ITRC Support Program [IITP-2019-2018-0-01798] of MSIT/IITP of the Korean government, and by the KIST Institutional Program [No. 2E29330].	Adebayo Julius, 2018, ADV NEURAL INFORM PR, V1, P7; Adi Y, 2018, PROCEEDINGS OF THE 27TH USENIX SECURITY SYMPOSIUM, P1615; Alvares-Melis David, 2018, NEURIPS; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ann Arbor District Library, 1996, ELEPHANT PULLS FIRE; Asuncion A, 2007, UCI MACHINE LEARNING; Athalye A, 2018, PR MACH LEARN RES, V80; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Dombrowski Ann-Kathrin, 2019, NEURIPS; Ghorbani A., 2019, AAAI; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Gu T., 2017, ARXIV170806733; GUIDOTTI R, 2019, CSUR, V51, DOI DOI 10.1145/3236009; Gunning D., 2017, DARPA; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kim B, 2017, ARXIV PREPRINT ARXIV; Kindermans P.-J., 2019, LECT NOTES COMPUTER, P267, DOI [DOI 10.1007/978-3-030-28954-6_14, 10.1007/978-3-030-28954-6.14]; Kindermans Pieter-Jan, 2016, ABS161107270 ARXIV; Kurakin A., 2016, ARXIV PREPRINT ARXIV; Lapuschkin S, 2017, IEEE INT CONF COMP V, P1629, DOI 10.1109/ICCVW.2017.191; Lundberg SM, 2017, ADV NEUR IN, V30; Madry Aleksander, 2017, ARXIV; Murdoch W.J., 2019, ARXIV190104592; Petsiuk V., 2018, P BRIT MACH VIS C 20, P151; Ribeiro Marco Tulio, 2016, P KDD, P97, DOI [10.18653/v1/n16-3020, DOI 10.1145/2939672.2939778]; Roth Kevin, 2019, ICML; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Russell J, 2012, SPEARMANS RANK CORRE, P92; Samek W, 2017, IEEE T NEUR NET LEAR, V28, P2660, DOI 10.1109/TNNLS.2016.2599820; Samek Wojciech, 2018, CVPR TUTORIAL; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shafahi A, 2019, ADV NEUR IN, V32; Shrikumar Avanti, 2017, PMLR, P3145, DOI DOI 10.1145/3292500.3330701; Simonyan Karen, 2013, DEEP INSIDE CONVOLUT, P2; Smilkov D, 2017, ARXIV; Springenberg J. T, 2015, ARXIV PREPRINT ARXIV; Sundararajan M, 2017, PR MACH LEARN RES, V70; Yu B, 2013, BERNOULLI, V19, P1484, DOI 10.3150/13-BEJSP14; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang X., 2018, ARXIV181200891	42	21	23	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302087
C	Lample, G; Sablayrolles, A; Ranzato, M; Denoyer, L; Jegou, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lample, Guillaume; Sablayrolles, Alexandre; Ranzato, Marc'Aurelio; Denoyer, Ludovic; Jegou, Herve			Large Memory Layers with Product Keys	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes.(3)	[Lample, Guillaume; Sablayrolles, Alexandre; Ranzato, Marc'Aurelio; Denoyer, Ludovic; Jegou, Herve] Facebook AI Res, New York, NY 10011 USA; [Lample, Guillaume; Denoyer, Ludovic] UPMC Univ Paris 06, Sorbonne Univ, UMR 7606, LIP6, Paris, France	Facebook Inc; UDICE-French Research Universities; Sorbonne Universite	Lample, G (corresponding author), Facebook AI Res, New York, NY 10011 USA.; Lample, G (corresponding author), UPMC Univ Paris 06, Sorbonne Univ, UMR 7606, LIP6, Paris, France.	glample@fb.com; asablayrolles@fb.com; ranzato@fb.com; denoyer@fb.com; rvj@fb.com						Babenko Artem, 2014, IEEE T PATTERN ANAL; Baevski Alexei, 2019, INT C LEARN REPR; Bengio Yoshua, 2013, ARXIV; Chelba Ciprian, 2014, C INT SPEECH COMM AS; Chen Bo, 2019, DEVICE VISUAL INTELL; Cho Kyunghyun, 2014, ABS14067362 CORR; Courbariaux Matthieu, 2015, ADV NEURAL INFORM PR, V1, P2; Dai Zihang, 2019, C ASS COMP LING; Denoyer  Ludovic, 2014, ABS14100510 CORR; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Eigen D., 2014, WORKSH INT C LEARN R; Gerald Thomas, 2017, INT C NEUR INF PROC; Grave Edouard, 2017, ADV NEURAL INFORM PR; Grave Edouard, 2017, INT C REPR LEARN; Graves A., 2013, P 2013 IEEE INT C AC, P6645, DOI [10.1109/ICASSP.2013.6638947, DOI 10.1109/ICASSP.2013.6638947]; Graves A., 2014, ARXIV14105401; Gross Sam, 2017, C COMP VIS PATT REC; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Johnson Jeff, 2017, IEEE T BIG DATA; Joulin A, 2015, ADV NEUR IN, V28; Kavukcuoglu K, 2010, ARXIV10103467; Kingma Diederik, 2015, INT C REPR LEARN; Koehn Philipp, 2007, C ASS COMP LING; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lampiris G, 2020, ANN OPER RES, V294, P225, DOI 10.1007/s10479-019-03337-5; Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12; Makhzani A., 2015, ADV NEURAL INFORM PR, P2791; Makhzani Alireza, 2014, INT C REPR LEARN; Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376; Neyshabur Behnam, 2019, INT C REPR LEARN; Olshausen B. A., 1997, VISION RES; Paszke Adam, 2017, NEUR AUT WORKSH; Radford A., 2019, OPENAI BLOG, V1, P9, DOI DOI 10.18653/V1/P19-1195; Rae Jack, 2016, ADV NEURAL INFORM PR; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Sennrich Rico, 2015, C ASS COMP LING; Shazeer Noam, 2017, INT C REPR LEARN; Spigler Stefano, 2018, ABS181009665 CORR; Sukhbaatar S, 2015, ADV NEUR IN, V28; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vijayanarasimhan Sudheendra, 2015, WORKSH INT C LEARN R; Wang Alex, 2018, INT C REPR LEARN; Weston Jason, 2016, INT C REPR LEARN; Weston Jason, 2015, INT C REPR LEARN	48	21	21	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900017
C	Lederer, A; Umlauft, J; Hirche, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lederer, Armin; Umlauft, Jonas; Hirche, Sandra			Uniform Error Bounds for Gaussian Process Regression with Application to Safe Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BASIS FUNCTION INTERPOLATION; KERNEL; CONSISTENCY	Data-driven models are subject to model errors due to limited and noisy training data. Key to the application of such models in safety-critical domains is the quantification of their model error. Gaussian processes provide such a measure and uniform error bounds have been derived, which allow safe control based on these models. However, existing error bounds require restrictive assumptions. In this paper, we employ the Gaussian process distribution and continuity arguments to derive a novel uniform error bound under weaker assumptions. Furthermore, we demonstrate how this distribution can be used to derive probabilistic Lipschitz constants and analyze the asymptotic behavior of our bound. Finally, we derive safety conditions for the control of unknown dynamical systems based on Gaussian process models and evaluate them in simulations of a robotic manipulator.	[Lederer, Armin; Umlauft, Jonas; Hirche, Sandra] Tech Univ Munich, Munich, Germany	Technical University of Munich	Lederer, A (corresponding author), Tech Univ Munich, Munich, Germany.	armin.lederer@tum.de; jonas.umlauft@tum.de; hirche@tum.de	Lederer, Armin/AAX-5989-2021	Lederer, Armin/0000-0001-6263-5608	German Academic Scholarship Foundation	German Academic Scholarship Foundation	Armin Lederer gratefully acknowledges financial support from the German Academic Scholarship Foundation.	Adler R., 2007, RANDOM FIELDS GEOMET; Beatson R, 2010, J APPROX THEORY, V162, P512, DOI 10.1016/j.jat.2009.08.004; Beckers T, 2019, AUTOMATICA, V103, P390, DOI 10.1016/j.automatica.2019.01.023; Berkenkamp F., 2017, ADV NEURAL INFORM PR; Berkenkamp F, 2016, IEEE DECIS CONTR P, P4661, DOI 10.1109/CDC.2016.7798979; Chowdhury SR, 2017, PR MACH LEARN RES, V70; Cortes C, 2010, P 13 INT C ART INT S, P113; Dicker LH, 2017, ELECTRON J STAT, V11, P1022, DOI 10.1214/17-EJS1258; DUDLEY R.M, 1967, J FUNCT ANAL, V1, P290, DOI DOI 10.1016/0022-1236(67)90017-1; Fanger Y, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P3913, DOI 10.1109/IROS.2016.7759576; Ghosal S, 2006, ANN STAT, V34, P2413, DOI 10.1214/009053606000000795; Gonzalez J, 2016, JMLR WORKSH CONF PRO, V51, P648; Hubbert S, 2004, J APPROX THEORY, V129, P58, DOI 10.1016/j.jat.2004.04.006; Huval B, 2015, ARXIV150401716; Kanagawa M., 2018, GAUSSIAN PROCESSES K, P1; Khalil HK., 2002, NONLINEAR SYSTEM, V3rd; Laurent B, 2000, ANN STAT, V28, P1302; Lederer A., 2019, POSTERIOR VARIANCE A; Murray R. M., 1994, MATH INTRO ROBOTIC M; Narcowich FJ, 2006, CONSTR APPROX, V24, P175, DOI 10.1007/s00365-005-0624-7; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Ravn O., 2000, SYST CONTROL INF; Schaback R., 2002, MATH COMPUT, V68, P201; Shi L, 2013, APPL COMPUT HARMON A, V34, P252, DOI 10.1016/j.acha.2012.05.001; Srinivas N, 2012, IEEE T INFORM THEORY, V58, P3250, DOI 10.1109/TIT.2011.2182033; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Steinwart I, 2002, J MACH LEARN RES, V2, P67, DOI 10.1162/153244302760185252; Stuart AM, 2018, MATH COMPUT, V87, P721, DOI 10.1090/mcom/3244; TALAGRAND M, 1994, ANN PROBAB, V22, P28, DOI 10.1214/aop/1176988847; Umlauft Jonas, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P6428, DOI 10.1109/ICRA.2017.7989759; Umlauft J., 2018, P EUR CONTR C; Umlauft J., 2017, P5249; Umlauft J, 2018, IEEE CONTR SYST LETT, V2, P483, DOI 10.1109/LCSYS.2018.2841961; Umlauft J, 2017, P AMER CONTR CONF, P1499, DOI 10.23919/ACC.2017.7963165; van der Vaart A, 2011, J MACH LEARN RES, V12, P2095; Wang W, 2020, J AM STAT ASSOC, V115, P920, DOI 10.1080/01621459.2019.1598868; Wendland H, 2004, SCATTERED DATA APPRO, V17; WU ZM, 1993, IMA J NUMER ANAL, V13, P13, DOI 10.1093/imanum/13.1.13; Zhang T, 2005, NEURAL COMPUT, V17, P2077, DOI 10.1162/0899766054323008	42	21	21	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300060
C	Luan, ST; Zhao, MD; Chang, XW; Precup, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Luan, Sitao; Zhao, Mingde; Chang, Xiao-Wen; Precup, Doina			Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recently, neural network based approaches have achieved significant progress for solving large, complex, graph-structured problems. Nevertheless, the advantages of multi-scale information and deep architectures have not been sufficiently exploited. In this paper, we first analyze key factors constraining the expressive power of existing Graph Convolutional Networks (GCNs), including the activation function and shallow learning mechanisms. Then, we generalize spectral graph convolution and deep GCN in block Krylov subspace forms, upon which we devise two architectures, both scalable in depth however making use of multi-scale information differently. On several node classification tasks, the proposed architectures achieve state-of-the-art performance.	[Luan, Sitao; Zhao, Mingde; Chang, Xiao-Wen; Precup, Doina] McGill Univ, Montreal, PQ, Canada; [Luan, Sitao; Zhao, Mingde; Precup, Doina] Mila, Montreal, PQ, Canada; [Precup, Doina] DeepMind, London, England	McGill University	Luan, ST (corresponding author), McGill Univ, Montreal, PQ, Canada.; Luan, ST (corresponding author), Mila, Montreal, PQ, Canada.	sitao.luan@mail.mcgill.ca; mingde.zhao@mail.mcgill.ca; chang@cs.mcgill.ca; dprecup@cs.mcgill.ca						Atwood J., 2015, ABS151102136 ARXIV; Bronstein M. M., 2016, ABS61108097 ARXIV; Chen J., 2017, ARXIV171010568; Cheng JH, 2018, CHI 2018: EXTENDED ABSTRACTS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3170427.3188467; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P53, DOI 10.1016/j.acha.2006.04.004; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006; Defferrard M., 2016, ABS160609375 ARXIV; Duvenaud David K, 2015, P NIPS; Frommer A, 2017, ELECTRON T NUMER ANA, V47, P100; Frommer A, 2017, SIAM J MATRIX ANAL A, V38, P710, DOI 10.1137/16M1072565; Gilmer J, 2017, PR MACH LEARN RES, V70; Gutknecht MH, 2009, LINEAR ALGEBRA APPL, V430, P174, DOI 10.1016/j.laa.2008.07.008; Hamilton W. L., 2017, ABS170602216 ARXIV; Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Ke S., 2019, PROC ASS ADV ARTIFIC; Ke Sun, 2019, Pattern Recognition and Computer Vision. Second Chinese Conference, PRCV 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11857), P431, DOI 10.1007/978-3-030-31654-9_37; Kipf TN, 2016, P INT C LEARN REPR; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Li Qimai, 2018, ABS180107606 ARXIV; Li RY, 2018, AAAI CONF ARTIF INTE, P3546; Li Yujia, 2015, ARXIV151105493; Liao R, 2018, GRAPH PARTITION NEUR; Liao R., 2019, ABS190101484 ARXIV; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Musco C, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1605; Shuman D. I., 2012, ARXIV12110053; Si Zhang, 2018, Computational Data and Social Networks. 7th International Conference, CSoNet 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11280), P79, DOI 10.1007/978-3-030-04648-4_7; Tran LV, 2013, RANDOM STRUCT ALGOR, V42, P110, DOI 10.1002/rsa.20406; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Velickovic P., 2017, ABS171010903 ARXIV; Wu X.-M., 2012, NIPS, P3077; Wu Zonghan, 2019, ABS190100596 ARXIV; Yang Z, 2016, PR MACH LEARN RES, V48; Zhang T., 2015, ARXIV150806429	38	21	21	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902056
C	Ma, XD; Zhang, P; Zhang, S; Duan, N; Hou, YX; Song, DW; Zhou, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ma, Xindian; Zhang, Peng; Zhang, Shuai; Duan, Nan; Hou, Yuexian; Song, Dawei; Zhou, Ming			A Tensorized Transformer for Language Modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DECOMPOSITIONS	Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.	[Ma, Xindian; Zhang, Peng; Zhang, Shuai; Hou, Yuexian] Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China; [Duan, Nan; Zhou, Ming] Microsoft Res Asia, Beijing, Peoples R China; [Song, Dawei] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing, Peoples R China	Tianjin University; Microsoft; Microsoft Research Asia; Beijing Institute of Technology	Zhang, P (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.	xindianma@tju.edu.cn; pzhang@tju.edu.cn; szhang96@tju.edu.cn; nanduan@microsoft.com; yxhou@tju.edu.cn; dwsong@bit.edu.cn; mingzhou@microsoft.com			state key development program of China [2017YFE0111900, 2018YFC0831704]; Natural Science Foundation of China [61772363, U1636203]; European Unions Horizon 2020 research and innovation programme under the Marie SkodowskaCurie grant [721321]	state key development program of China; Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); European Unions Horizon 2020 research and innovation programme under the Marie SkodowskaCurie grant	This work is supported in part by the state key development program of China (grant No. 2017YFE0111900, 2018YFC0831704), Natural Science Foundation of China (grant No. 61772363, U1636203), and the European Unions Horizon 2020 research and innovation programme under the Marie SkodowskaCurie grant agreement No.721321.	Baevski A., 2018, PROC INT C LEARN REP; Bucilua Cristian, 2006, P 12 ACM SIGKDD INT, P535, DOI [10.1145/1150402.1150464, DOI 10.1145/1150402.1150464]; CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791; Chelba Ciprian, 2013, COMPUTER SCI; Chen Patrick, 2018, ADV NEURAL INFORM PR, P10988; Cohen TS, 2016, PR MACH LEARN RES, V48; Dai Z., 2019, ARXIV190102860; Dauphin YN, 2017, PR MACH LEARN RES, V70; De Lathauwer L, 2008, SIAM J MATRIX ANAL A, V30, P1033, DOI 10.1137/070690729; Dentinel Zarembaw, 2014, NEURIPS, P1269; Devlin J., 2019, P 2019 C N AM CHAPT, V1, P4171; Garipov Timur, 2016, ARXIV161103214; Huang XW, 2015, ACTA POLYM SIN, P1133; Inan H., 2016, ARXIV161101462; Jaderberg M., 2014, P BRIT MACH VIS C; Jozefowicz Rafal, 2016, ARXIV160202410; Khrulkov Valentin, 2019, ARXIV190110787; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Li, 2017, ARXIV171205689; Merity Stephen, 2018, ARXIV180308240; Mikolov Toma<prime>s., 2011, 12 ANN C INT SPEECH; Novikov A., 2015, ADV NEURAL INFORM PR, V28, P442, DOI DOI 10.5555/2969239.2969289; Oseledets IV, 2011, SIAM J SCI COMPUT, V33, P2295, DOI 10.1137/090752286; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Radford Alec, 2018, IMPROVING LANGUAGE S; Radford Alec, 2019, ARXIV190410509; Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949; SENNRICH R, 2016, ARXIV160602892; Shazeer N., 2017, ICLR; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; Variani Ehsan, 2018, ARXIV181108417; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Yang YC, 2017, PR MACH LEARN RES, V70; Yang Zhilin, 2017, ARXIV171103953; Ye JM, 2018, PROC CVPR IEEE, P9378, DOI 10.1109/CVPR.2018.00977; Zhang Lipeng, 2019, ARXIV190111167; Zhang P, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1303, DOI 10.1145/3269206.3271723; Zoph B., 2016, ICLR	39	21	21	4	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302025
C	Maaloe, L; Fraccaro, M; Lievin, V; Winther, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Maaloe, Lars; Fraccaro, Marco; Lievin, Valentin; Winther, Ole			BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.	[Maaloe, Lars] Corti, Copenhagen, Denmark; [Fraccaro, Marco] Unumed, Copenhagen, Denmark; [Lievin, Valentin; Winther, Ole] Tech Univ Denmark, Copenhagen, Denmark	Technical University of Denmark	Maaloe, L (corresponding author), Corti, Copenhagen, Denmark.	lm@corti.ai; mf@unumed.com; valv@dtu.dk; olwi@dtu.dk		Maaloe, Lars/0000-0003-2030-520X; Winther, Ole/0000-0002-1966-3205				[Anonymous], 2008, P 25 INT C MACH LEAR; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bian Z., 2018, ABS180204920 CORR; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Burda Y., 2015, P INT C ART INT STAT; Burda Yuri, 2015, ARXIV150900519; Chen X., 2017, REPRESENTATIONS; Dai Z., 2017, ADV NEURAL INFORM PR, P6510; Dieng A. B., 2018, ARXIV180704863; Dinh L, 2016, ARXIV PREPRINT ARXIV; Dinh Laurent, 2014, ARXIV14108516; Dosovitskiy Alexey, 2016, NEURIPS; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Gulrajani I., 2016, 161105013 ARXIV; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Ho Jonathan, 2019, ICML; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D.P., 2015, 3 INT C LEARN REPR I, P1, DOI DOI 10.1007/S11390-017-1754-7; Kingma D.P, P 3 INT C LEARNING R; Kingma Diederik P, 2018, ADV NEURAL INFORM PR; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kingma M., 2013, ARXIV13126114; Lake B.M., 2013, ADV NEURAL INFORM PR; Larochelle H., 2011, INT C ART INT STAT; Larsen ABL, 2016, PR MACH LEARN RES, V48; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Maaloe L., 2017, ARXIV170400637; Maaloe L, 2016, PR MACH LEARN RES, V48; Miyato T, 2015, ARXIV150700677; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Nalisnick Eric, 2018, INT C LEARN REPR; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Oord A.V.D., 2016, SSW; Paisley J., 2012, ARXIV12066430; Ranganath R, 2016, PR MACH LEARN RES, V48; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Rezende D. J., 2015, P INT C MACH LEARN; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Rezende Danilo Jimenez, 2018, ARXIV181000597; Rolfe Jason Tyler, 2017, P INT C LEARN REPR; Salimans T., 2017, 170105517 ARXIV; Salimans T., 2016, ADV NEUR IN, P2234; Salimans T., 2015, P INT C MACH LEARN; Salimans T, 2016, ADV NEUR IN, V29; Semeniuta Stanislau, 2017, P 2017 C EMP METH NA, P627, DOI DOI 10.18653/V1/D17-1066; Shah H., 2018, GENERATING SENTENCES; Sonderby CK, 2016, ADV NEUR IN, V29; Springenberg Jost Tobias, 2015, ARXIV151106390; Tomczak Jakub M, 2016, ARXIV161109630; Tran Dustin, 2016, INT C LEARN REPR, V5; van den Oord A., 2014, NIPS, P3518; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; Zhao S., 2017, ARXIV170208658; Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11	60	21	21	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306054
C	Nasiriany, S; Pong, VH; Lin, S; Levine, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Nasiriany, Soroush; Pong, Vitchyr H.; Lin, Steven; Levine, Sergey			Planning with Goal-Conditioned Policies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Planning methods can solve temporally extended sequential decision making problems by composing simple behaviors. However, planning requires suitable abstractions for the states and transitions, which typically need to be designed by hand. In contrast, model-free reinforcement learning (RL) can acquire behaviors from low-level inputs directly, but often struggles with temporally extended tasks. Can we utilize reinforcement learning to automatically form the abstractions needed for planning, thus obtaining the best of both approaches? We show that goal-conditioned policies learned with RL can be incorporated into planning, so that a planner can focus on which states to reach, rather than how those states are reached. However, with complex state observations such as images, not all inputs represent valid states. We therefore also propose using a latent variable model to compactly represent the set of valid states for the planner, so that the policies provide an abstraction of actions, and the latent variable model provides an abstraction of states. We compare our method with planning-based and model-free methods and find that our method significantly outperforms prior work when evaluated on image-based robot navigation and manipulation tasks that require non-greedy, multi-staged behavior.	[Nasiriany, Soroush; Pong, Vitchyr H.; Lin, Steven; Levine, Sergey] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Nasiriany, S (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	snasiriany@berkeley.edu; vitchyr@berkeley.edu; stevenlin598@berkeley.edu; svlevine@berkeley.edu			Office of Naval Research; National Science Foundation; Google; NVIDIA; Amazon; ARL DCIST CRA [W911NF-17-2-0181]	Office of Naval Research(Office of Naval Research); National Science Foundation(National Science Foundation (NSF)); Google(Google Incorporated); NVIDIA; Amazon; ARL DCIST CRA	This work was supported by the Office of Naval Research, the National Science Foundation, Google, NVIDIA, Amazon, and ARL DCIST CRA W911NF-17-2-0181.	Agarwal A, 2019, AAAI CONF ARTIF INTE, P3151; Andrychowicz M., 2017, ADV NEURAL INFORM PR; Babaeizadeh Mohammad, 2018, ICLR; Boots B, 2014, IEEE INT CONF ROBOT, P4021, DOI 10.1109/ICRA.2014.6907443; Byravan Arunkumar, IEEE INT C ROB AUT; Chiappa S., 2017, RECURRENT ENV SIMULA; Colas C, 2019, PR MACH LEARN RES, V97; Dai Bin, 2019, INT C LEARN REPR; Dayan P., 1993, ADV NEURAL INFORM PR; De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z; Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639; Ebert Frederik, 2018, ARXIV181200568; Ebert Frederik, 2017, ARXIV171005268; Finn C, 2016, IEEE INT CONF ROBOT, P512, DOI 10.1109/ICRA.2016.7487173; Florensa C, 2018, PR MACH LEARN RES, V80; Foster J, 2002, TRIQUARTERLY, P49; Fujimoto S, 2018, PR MACH LEARN RES, V80; Ghosh Dibya, 2019, INT C LEARN REPR ICL; Guez A, 2019, PR MACH LEARN RES, V97; Hafner D, 2019, PR MACH LEARN RES, V97; Heess N., 2015, NIPS; Kaelbling L. P., 1993, IJCAI; Kaelbling LP., 1993, P 10 INT C MACHINE L, P951; Kaiser L., 2019, ARXIV190300374; Kalchbrenner N, 2017, PR MACH LEARN RES, V70; Kingma D. P, 2014, ARXIV13126114; Kurutach T., 2018, ADV NEURAL INFORM PR; Lane Terran, 2001, P 2001 IJCAI WORKSH; Lee Alex X, 2018, ARXIV180401523; Lenz I, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Lu HM, 1998, P SOC PHOTO-OPT INS, V3307, P52, DOI 10.1117/12.304659; Makhzani Alireza, 2016, ICLR WORKSH; Mathieu Michael, 2016, ICLR; Moore Andrew W, 1999, P INT JOINT C ART IN; Nachum O., 2018, ADV NEURAL INFORM PR; Nagabandi A, 2018, IEEE INT CONF ROBOT, P7579; Nguyen D. H., 1990, IEEE Control Systems Magazine, V10, P18, DOI 10.1109/37.55119; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Oh J, 2017, ADV NEUR IN, V30; Pardo Fabio, 2018, CORR; Parikh N., 2014, FDN TRENDS OPTIM, V1, P127, DOI DOI 10.1561/2400000003; Plappert, 2018, ARXIV180209464, P1; Pong V., 2018, PROC 2 WORKSHOP LIFE; Pong V., 2018, INT C LEARN REPR ICL; Pong V.H., 2019, ARXIV190303698; Punjani A, 2015, IEEE INT CONF ROBOT, P3223, DOI 10.1109/ICRA.2015.7139643; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Saenko, 2019, INT C LEARN REPR, P1; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216; SUTTON R S, 2011, 10 INT C AUTONOMOUS, P761, DOI DOI 10.5555/2031678.2031726; Tamar Aviv, 2016, ADV NEURAL INFORM PR, P2154, DOI DOI 10.5555/3171837.3171991; Veeriah V., 2018, ARXIV180609605; Vezhnevets AS, 2017, PR MACH LEARN RES, V70; Warde-Farley D., 2018, ARXIV181111359; Watter Manuel, 2015, ADV NEURAL INFORM PR, P2746; Wiering M, 1997, ADAPT BEHAV, V6, P219, DOI 10.1177/105971239700600202; Zhang M, 2019, PR MACH LEARN RES, V97	62	21	21	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906049
C	Ni, J; Zhang, SH; Xie, HY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ni, Jian; Zhang, Shanghang; Xie, Haiyong			Dual Adversarial Semantics-Consistent Network for Generalized Zero-Shot Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Generalized zero-shot learning (GZSL) is a challenging class of vision and knowledge transfer problems in which both seen and unseen classes appear during testing. Existing GZSL approaches either suffer from semantic loss and discard discriminative information at the embedding stage, or cannot guarantee the visual-semantic interactions. To address these limitations, we propose a Dual Adversarial Semantics-Consistent Network (referred to as DASCN), which learns both primal and dual Generative Adversarial Networks (GANs) in a unified framework for GZSL. In DASCN, the primal GAN learns to synthesize inter-class discriminative and semantics-preserving visual features from both the semantic representations of seen/unseen classes and the ones reconstructed by the dual GAN. The dual GAN enforces the synthetic visual features to represent prior semantic knowledge well via semantics-consistent adversarial learning. To the best of our knowledge, this is the first work that employs a novel dual-GAN mechanism for GZSL. Extensive experiments show that our approach achieves significant improvements over the state-of-the-art approaches.	[Ni, Jian; Xie, Haiyong] Univ Sci & Technol China, Anhua 230026, Anhui, Peoples R China; [Zhang, Shanghang] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Xie, Haiyong] Capital Med Univ, Adv Innovat Ctr Human Brain Protect, Beijing 100054, Peoples R China; [Xie, Haiyong] Natl Engn Lab Publ Safety Risk Percept & Control, Beijing 100041, Peoples R China	Chinese Academy of Sciences; University of Science & Technology of China, CAS; Carnegie Mellon University; Capital Medical University	Ni, J (corresponding author), Univ Sci & Technol China, Anhua 230026, Anhui, Peoples R China.	nj1@mail.ustc.edu.cn; shanghaz@andrew.cmu.edu; haiyong.xie@ieee.org	Zhang, Lisa/AAW-9795-2021		National Key Research and Development Project [320 2017YFC0820503]; National Science and Technology Major Project for IND (investigational 321 new drug) [2018ZX09201014]; CETC Joint Advanced Research Foundation [6141B08080101, 6141B08010102]	National Key Research and Development Project; National Science and Technology Major Project for IND (investigational 321 new drug); CETC Joint Advanced Research Foundation	This research is supported in part by the National Key Research and Development Project (Grant No. 320 2017YFC0820503), the National Science and Technology Major Project for IND (investigational 321 new drug) (Project No. 2018ZX09201014), and the CETC Joint Advanced Research Foundation 322 (Grant No. 6141B08080101,6141B08010102).	Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911; Annadani Y, 2018, PROC CVPR IEEE, P7603, DOI 10.1109/CVPR.2018.00793; Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4; Chen L, 2018, PROC CVPR IEEE, P1043, DOI 10.1109/CVPR.2018.00115; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Felix R, 2018, LECT NOTES COMPUT SC, V11210, P21, DOI 10.1007/978-3-030-01231-1_2; Frome Andrea, 2013, NEURIPS; Gulrajani I, 2017, P NIPS 2017; Guo YC, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1767; Kim Taeksoo, 2017, P 34 INT C MACH LEAR, P1857, DOI [10.5555/3305381.3305573, DOI 10.5555/3305381.3305573]; Kingma D.P, P 3 INT C LEARNING R; Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Li HT, 2018, IEEE T APPL SUPERCON, V28, DOI 10.1109/TASC.2018.2874422; Liu SC, 2018, ADV NEUR IN, V31; Long Y, 2017, PROC CVPR IEEE, P6165, DOI 10.1109/CVPR.2017.653; Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Tang L, 2017, WINT SIMUL C PROC, P2021; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Verma VK, 2018, PROC CVPR IEEE, P4281, DOI 10.1109/CVPR.2018.00450; Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717; Welinder P., 2010, CNSTR2010001 CALTECH; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768; Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhu J, 2018, EARTH AND SPACE 2018: ENGINEERING FOR EXTREME ENVIRONMENTS, P1004; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	35	21	23	3	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306018
C	Requeima, J; Gordon, J; Bronskill, J; Nowozin, S; Turner, RE		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Requeima, James; Gordon, Jonathan; Bronskill, John; Nowozin, Sebastian; Turner, Richard E.			Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The goal of this paper is to design image classification systems that, after an initial multi-task training phase, can automatically adapt to new tasks encountered at test time. We introduce a conditional neural process based approach to the multi-task classification setting for this purpose, and establish connections to the meta-learning and few-shot learning literature. The resulting approach, called CNAPS, comprises a classifier whose parameters are modulated by an adaptation network that takes the current task's dataset as input. We demonstrate that CNAPS achieves state-of-theart results on the challenging META-DATASET benchmark indicating high-quality transfer-learning. We show that the approach is robust, avoiding both over-fitting in low-shot regimes and under-fitting in high-shot regimes. Timing experiments reveal that CNAPS is computationally efficient at test-time as it does not involve gradient based adaptation. Finally, we show that trained models are immediately deployable to continual learning and active learning where they can outperform existing approaches that do not leverage transfer learning.	[Requeima, James; Gordon, Jonathan; Bronskill, John; Turner, Richard E.] Univ Cambridge, Cambridge, England; [Requeima, James] Invenia Labs, Cambridge, England; [Nowozin, Sebastian] Google Res Berlin, Berlin, Germany; [Turner, Richard E.] Microsoft Res, Redmond, WA USA	University of Cambridge; Microsoft	Requeima, J (corresponding author), Univ Cambridge, Cambridge, England.; Requeima, J (corresponding author), Invenia Labs, Cambridge, England.	jrr41@cam.ac.uk; jg801@cam.ac.uk; jfb54@cam.ac.uk; nowozin@google.com; ret26@cam.ac.uk			Google; Amazon; Improbable; EPSRC [EP/M0269571, EP/L000776/1]	Google(Google Incorporated); Amazon; Improbable; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The authors would like to thank Ambrish Rawat for helpful discussions and David Duvenaud, Wessel Bruinsma, Will Tebbutt Adria Garriga Alonso, Eric Nalisnick, and Lyndon White for the insightful comments and feedback. Richard E. Turner is supported by Google, Amazon, Improbable and EPSRC grants EP/M0269571 and EP/L000776/1.	[Anonymous], 2017, ARXIV170603466; Bauer M., 2017, ARXIV170600326; Chaudhry A, 2018, LECT NOTES COMPUT SC, V11215, P556, DOI 10.1007/978-3-030-01252-6_33; Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461; Cohn DA, 1996, J ARTIF INTELL RES, V4, P129, DOI 10.1613/jair.295; Finn C, 2017, PR MACH LEARN RES, V70; Garnelo M, 2018, ARXIV180701622; Garnelo Marta, 2018, ARXIV180701613, P1704; Geisser S., 1993, PREDICTIVE INFERENCE; Geisser Seymour, 1983, TECHNICAL REPORT; Gordon Jonathan, 2019, INT C LEARN REPR; Ha David, 2017, ARXIV170403477; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Houben S, 2013, IEEE INT C INTELL TR, P7, DOI 10.1109/ITSC.2013.6728595; Iyer S, 2019, GLOB SPINE J, V9, P6, DOI 10.1177/2192568217705655; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lake Brenden, 2011, C COGN SCI SOC, P6; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; LeCun Y., 2010, MNIST HANDWRITTEN DI; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Maji Subhransu, 2013, ARXIV13065151; Nguyen C. V., 2017, ARXIV171010628; Nichol Alex, 2018, ARXIV180302999; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; Oreshkin Boris N, 2018, ADV NEURAL INFORM PR; Perez E, 2018, AAAI CONF ARTIF INTE, P3942; Qi C. R., 2017, IEEE P COMPUT VIS PA, V1, P4, DOI DOI 10.1109/CVPR.2017.16; Ravi S., 2017, INT C LEARN REPR, P12; Rebuffi SA, 2018, PROC CVPR IEEE, P8119, DOI 10.1109/CVPR.2018.00847; Ring MB, 1997, MACH LEARN, V28, P77, DOI 10.1023/A:1007331723572; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Rusu Andrei A, 2018, ARXIV180705960; Schmidhuber J, 1987, THESIS; Schroeder Brigit, 2018, FGVCX FUNGI CLASSIFI, P12; Settles B., 2012, SYNTH LECT ARTIF INT, V6, P1; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Swaroop S., 2019, ARXIV190502099; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Thrun S., 2012, LEARNING LEARN; Triantafillou Eleni, 2019, ARXIV190303096; Vartak Manasi, 2017, ADV NEURAL INFORM PR, P6904; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wah C., 2011, TECH REP; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zaheer Manzil, 2017, P ADV NEUR INF PROC, P3394; Zenke F, 2017, PR MACH LEARN RES, V70; Zintgraf Luisa M, 2018, ARXIV181003642	49	21	21	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308003
C	Shiv, VL; Quirk, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shiv, Vighnesh Leonardo; Quirk, Chris			Novel positional encodings to enable tree-based transformers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Neural models optimized for tree-based problems are of great value in tasks like SQL query extraction and program synthesis. On sequence-structured data, transformers have been shown to learn relationships across arbitrary pairs of positions more reliably than recurrent models. Motivated by this property, we propose a method to extend transformers to tree-structured data, enabling sequence-to-tree, tree-to-sequence, and tree-to-tree mappings. Our approach abstracts the transformer's sinusoidal positional encodings, allowing us to instead use a novel positional encoding scheme to represent node positions within trees. We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over both sequence-to-sequence transformers and state-of-the-art tree-based LSTMs on several datasets. In particular, our results include a 22% absolute increase in accuracy on a JavaScript to CoffeeScript translation dataset.	[Shiv, Vighnesh Leonardo; Quirk, Chris] Microsoft Res, Redmond, WA 98052 USA	Microsoft	Shiv, VL (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	vishiv@microsoft.com; chrisq@microsoft.com						Aharoni R, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 2, P132, DOI 10.18653/v1/P17-2021; Allamanis M., 2018, INT C LEARN REPR; Alvarez-Melis  D., 2017, INT C LEARN REPR ICL; Califf ME, 1999, SIXTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-99)/ELEVENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE (IAAI-99), P328; Chen X., 2018, P 32 C NEUR INF PROC; Dahl Deborah A, 1994, P WORKSH HUM LANG TE, P43, DOI DOI 10.3115/1075812.1075823; Dong Li, 2016, ASS COMPUTATIONAL LI; Eriguchi A, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 2, P72, DOI 10.18653/v1/P17-2012; Gulcehre Caglar, 2018, HYPERBOLIC ATTENTION; Hindle A., 2012, INT C SOFTW ENG; Kingma Diederik, 2015, INT C LEARN REPR, DOI 10.3115/1075096.1075150; Klein D, 2003, 41ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P423, DOI 10.3115/1075096.1075150; Kwiatkowski T., 2013, P 2013 C EMP METH NA, P1545; Li Y., 2016, INT C LEARN REPR; Liang Percy, 2011, P 49 ANN M ASS COMPU, P590; Miwa M, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1105; Peng N., 2017, T ASS COMPUT LINGUIS, V5, P101; Shaw Peter, 2018, P 2018 C N AM CHAPT, P464, DOI DOI 10.18653/V1/N18-2074; Shen Y., 2019, ICLR; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Tai Kai Sheng, 2015, ASS COMPUTATIONAL LI; Tang Lappoon R, 2001, P 12 EUR C MACH LEAR, P466, DOI [10.1007/3-540-44795, DOI 10.1007/3-540-44795, DOI 10.1007/3-540-44795-4_40]; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Vinyals Oriol, 2015, NIPS; Wu Y., 2016, ADV NEURAL INFORM PR, V1609, DOI 10.48550/arXiv.1609.08144	26	21	21	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903067
C	Tran, D; Dusenberry, MW; van der Wilk, M; Hafner, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tran, Dustin; Dusenberry, Michael W.; van der Wilk, Mark; Hafner, Danijar			Bayesian Layers: A Module for Neural Network Uncertainty	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GAUSSIAN-PROCESSES	We describe Bayesian Layers, a module designed for fast experimentation with neural network uncertainty. It extends neural network libraries with drop-in replacements for common layers. This enables composition via a unified abstraction over deterministic and stochastic functions and allows for scalability via the underlying system. These layers capture uncertainty over weights (Bayesian neural nets), pre-activation units (dropout), activations ("stochastic output layers"), or the function itself (Gaussian processes). They can also be reversible to propagate uncertainty from input to output. We include code examples for common architectures such as Bayesian LSTMs, deep GPs, and flow-based models. As demonstration, we fit a 5-billion parameter "Bayesian Transformer" on 512 TPUv2 cores for uncertainty in machine translation and a Bayesian dynamics model for model-based planning. Finally, we show how Bayesian Layers can be used within the Edward2 language for probabilistic programming with stochastic processes.(1)	[Tran, Dustin; Dusenberry, Michael W.; Hafner, Danijar] Google Brain, Mountain View, CA 94043 USA; [van der Wilk, Mark] Prowler io, Cambridge, England	Google Incorporated	Tran, D (corresponding author), Google Brain, Mountain View, CA 94043 USA.							Abadi M, 2015, P 12 USENIX S OPERAT; Al-Rfou R, 2016, THEANO PYTHON FRAMEW; Al-Shedivat M, 2017, J MACH LEARN RES, V18; [Anonymous], 2017, ADV NEURAL INFORM PR; [Anonymous], 2016, TENSORFLOW SLIM LIGH; Azizzadenesheli K, 2018, 2018 INFORMATION THEORY AND APPLICATIONS WORKSHOP (ITA); Bingham E, 2019, J MACH LEARN RES, V20; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Bui TD, 2016, PR MACH LEARN RES, V48; Carpenter B., 2017, J STAT SOFTW, DOI [10.18637/jss.v076.i01, DOI 10.18637/JSS.V076.I01]; Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3; Chollet F., 2015, KERAS; Collobert R., 2011, NIPS; Cutajar K, 2017, PR MACH LEARN RES, V70; Dai Z., 2018, MXFUSION MODULAR DEE; Damianou Andreas, 2013, ARTIF INTELL, P207, DOI DOI 10.1002/NME.1296; Dillon J.V., 2017, TENSORFLOW DISTRIBUT; Dinh L, 2017, 5 INT C LEARN REPR I; Dusenberry M. W., 2019, ARXIV PREPRINT ARXIV; Fortunato Meire, 2017, ARXIV170402798; Gal Y., 2016, P DAT EFF MACH LEARN, P1; Gal Y, 2016, PR MACH LEARN RES, V48; Gardner JR, 2018, ADV NEUR IN, V31; Germain M, 2015, PR MACH LEARN RES, V37, P881; Goodman N, 2012, ARXIV PREPRINT ARXIV; Guadarrama S., 2018, TF AGENTS LIB REINFO; Hafner D., 2019, RELIABLE UNCERTAINTY; Hafner D., 2018, ARXIV181104551; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Jacobsen J.H., 2018, ICLR; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; John S. T., 2018, P 35 INT C MACH LEAR, V80, P2362; Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246; Khan ME, 2018, PR MACH LEARN RES, V80; Kingma D.P, P 3 INT C LEARNING R; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Kiselyov O, 2009, LECT NOTES COMPUT SC, V5658, P360, DOI 10.1007/978-3-642-03034-5_17; Kumar Aviral, 2019, ARXIV190300802; Laumann F., 2018, ARXIV180605978; Louizos C, 2017, PR MACH LEARN RES, V70; Matthews AGD, 2017, J MACH LEARN RES, V18, P1; Narayanan Praveen, 2016, Functional and Logic Programming. 13th International Symposium, FLOPS 2016. Proceedings: LNCS 9613, P62, DOI 10.1007/978-3-319-29604-3_5; Neal R., 1995, SOFTWARE FLEXIBLE BA; Nguyen C. V., 2017, ARXIV171010628; Parmar Niki, 2018, PR MACH LEARN RES, P4055; Pawlowski N., 2017, ARXIV171101297; Ranganath R, 2016, PR MACH LEARN RES, V48; Rasmussen CE, 2010, J MACH LEARN RES, V11, P3011; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Salimbeni Hugh, 2017, ADV NEURAL INFORM PR, V30, P4588; Shazeer N, 2018, ADV NEUR IN, V31; Tran D., 2018, NEURAL INFORM PROCES; Tran Dustin, 2016, ARXIV161009787; van den Oord A, 2017, ADV NEUR IN, V30; Vanhatalo J, 2013, J MACH LEARN RES, V14, P1175; Vaswani A., 2018, MT RES TRACK; Wen Yeming, 2018, ARXIV180304386; Wu A., 2018, INT C LEARN REPR; Zhang G., 2017, ARXIV171202390	59	21	21	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906033
C	Wang, X; He, JM; Ma, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Xu; He, Jingming; Ma, Lin			Exploiting Local and Global Structure for Point Cloud Semantic Segmentation with Contextual Point Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we propose one novel model for point cloud semantic segmentation, which exploits both the local and global structures within the point cloud based on the contextual point representations. Specifically, we enrich each point representation by performing one novel gated fusion on the point itself and its contextual points. Afterwards, based on the enriched representation, we propose one novel graph pointnet module, relying on the graph attention block to dynamically compose and update each point representation within the local point cloud structure. Finally, we resort to the spatial-wise and channel-wise attention strategies to exploit the point cloud global structure and thereby yield the resulting semantic label for each point. Extensive results on the public point cloud databases, namely the S3DIS and ScanNet datasets, demonstrate the effectiveness of our proposed model, outperforming the state-of-the-art approaches.	[Wang, Xu; He, Jingming] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen, Peoples R China; [Ma, Lin] Tencent AI Lab, Shenzhen, Peoples R China	Shenzhen University; Tencent	Ma, L (corresponding author), Tencent AI Lab, Shenzhen, Peoples R China.	wangxu@szu.edu.cn; hejingming519@gmail.com; forest.linma@gmail.com			National Natural Science Foundation of China [61871270, 61672443]; Natural Science Foundation of SZU [827000144]; National Engineering Laboratory for Big Data System Computing Technology of China	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of SZU; National Engineering Laboratory for Big Data System Computing Technology of China	This work was supported in part by the National Natural Science Foundation of China (Grant 61871270 and Grant 61672443), in part by the Natural Science Foundation of SZU (grant no. 827000144) and in part by the National Engineering Laboratory for Big Data System Computing Technology of China.	ARMENI I, 2016, PROC CVPR IEEE, P1534, DOI DOI 10.1109/CVPR.2016.170; Charles R., 2017, ADV NEURAL INFORM PR; Dai A., 2017, CVPR, P5828; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Engelmann F, 2017, IEEE INT CONF COMP V, P716, DOI 10.1109/ICCVW.2017.90; Graham Benjamin, 2018, P IEEE C COMP VIS PA, P3577; Huang QG, 2018, PROC CVPR IEEE, P2626, DOI 10.1109/CVPR.2018.00278; Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99; Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479; Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959; Liu DYH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4146; Liu Y., 2019, IEEE C COMP VIS PATT, P8895; Pang G, 2016, INT C PATT RECOG, P585, DOI 10.1109/ICPR.2016.7899697; Qi Xiaojuan, 2017, P IEEE INT C COMP VI, P5199; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802; Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067; Velickovic P., 2018, P INT C LEARN REPR, DOI DOI 10.17863/CAM.48429; Wang Dominic Zeng, 2015, P ROB SCI SYST ROM I; Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608; Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272; Wang Yue, 2019, ACM T GRAPHICS TOG; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Xie SN, 2018, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR.2018.00484; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472	27	21	21	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304056
C	Wehenkel, A; Louppe, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wehenkel, Antoine; Louppe, Gilles			Unconstrained Monotonic Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Monotonic neural networks have recently been proposed as a way to define invertible transformations. These transformations can be combined into powerful autoregressive flows that have been shown to be universal approximators of continuous probability distributions. Architectures that ensure monotonicity typically enforce constraints on weights and activation functions, which enables invertibility but leads to a cap on the expressiveness of the resulting transformations. In this work, we propose the Unconstrained Monotonic Neural Network (UMNN) architecture based on the insight that a function is monotonic as long as its derivative is strictly positive. In particular, this latter condition can be enforced with a free-form neural network whose only constraint is the positiveness of its output. We evaluate our new invertible building block within a new autoregressive flow (UMNN-MAF) and demonstrate its effectiveness on density estimation experiments. We also illustrate the ability of UMNNs to improve variational inference.	[Wehenkel, Antoine; Louppe, Gilles] Univ Liege, Liege, Belgium	University of Liege	Wehenkel, A (corresponding author), Univ Liege, Liege, Belgium.		Louppe, Gilles/D-1923-2017	Louppe, Gilles/0000-0002-2082-3106	F.R.S.-FNRS (Belgium)	F.R.S.-FNRS (Belgium)(Fonds de la Recherche Scientifique - FNRS)	The authors would like to acknowledge Matthia Sabatelli, Nicolas Vecoven, Antonio Sutera and Louis Wehenkel for useful feedback on the manuscript. They would also like to thank the anonymous reviewers for many relevant remarks. Antoine Wehenkel is a research fellow of the F.R.S.-FNRS (Belgium) and acknowledges its financial support.	ARCHER NP, 1993, DECISION SCI, V24, P60, DOI 10.1111/j.1540-5915.1993.tb00462.x; Berg R. v. d., 2018, C UNC ART INT UAI; Chen R. T., 2018, ADV NEURAL INFORM PR, P6571; Daniels H, 2010, IEEE T NEURAL NETWOR, V21, P906, DOI 10.1109/TNN.2010.2044803; DeCao N., 2019, ARXIV190404676; Dinh Laurent., 2017, INT C LEARN REPR; Dinh Laurent., 2015, P 3 INT C LEARN REPR; Germain M, 2015, PR MACH LEARN RES, V37, P881; Gouk H., 2018, ARXIV180404368; Grathwohl Will, 2018, INT C MACH LEARN; Gupta  Maya, 2016, J MACHINE LEARNING R, V17, P3790; Ha D, 2017, 5 INT C LEARN REPR I; Huang CW, 2018, PR MACH LEARN RES, V80; Jaini P., 2019, ARXIV190502325; Kingma D.P., 2018, P 32 INT C NEUR INF, P10236; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Oliva Junier, 2018, INT C MACH LEARN, P3895; Oord A., 2018, P INT C MACH LEARN, P3915; Papamakarios G., 2019, 22 INT C ART INT STA; Papamakarios George, 2017, ADV NEURAL INFORM PR, P2338; Rezende D., 2015, ICML, P1530; Sill J, 1998, ADV NEUR IN, V10, P661; Tran D., 2017, 5 INT C LEARN REPR I; Welling M., 2013, P ICLR; You SY, 2017, J FOOD QUALITY, P1, DOI 10.1155/2017/9562981	26	21	21	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301052
C	Xu, HT; Luo, DX; Carin, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xu, Hongteng; Luo, Dixin; Carin, Lawrence			Scalable Gromov-Wasserstein Learning for Graph Partitioning and Matching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GLOBAL-NETWORK ALIGNMENT; ALGORITHM	We propose a scalable Gromov-Wasserstein learning (S-GWL) method and establish a novel and theoretically-supported paradigm for large-scale graph analysis. The proposed method is based on the fact that Gromov-Wasserstein discrepancy is a pseudometric on graphs. Given two graphs, the optimal transport associated with their Gromov-Wasserstein discrepancy provides the correspondence between their nodes and achieves graph matching. When one of the graphs has isolated but self-connected nodes (i.e., a disconnected graph), the optimal transport indicates the clustering structure of the other graph and achieves graph partitioning. Using this concept, we extend our method to multi-graph partitioning and matching by learning a Gromov-Wasserstein barycenter graph for multiple observed graphs; the barycenter graph plays the role of the disconnected graph, and since it is learned, so is the clustering. Our method combines a recursive K-partition mechanism with a regularized proximal gradient algorithm, whose time complexity is O(K (E +V) logy V) for graphs with V nodes and E edges. To our knowledge, our method is the first attempt to make Gromov-Wasserstein discrepancy applicable to large-scale graph analysis and unify graph partitioning and matching into the same framework. It outperforms state-of-the-art graph partitioning and matching methods, achieving a trade-off between accuracy and efficiency.	[Xu, Hongteng] Infinia ML Inc, Durham, NC 27703 USA; [Xu, Hongteng; Luo, Dixin; Carin, Lawrence] Duke Univ, Durham, NC 27706 USA	Duke University	Xu, HT (corresponding author), Infinia ML Inc, Durham, NC 27703 USA.; Xu, HT (corresponding author), Duke Univ, Durham, NC 27706 USA.	hongteng.xu@duke.edu; dixin.luo@duke.edu; lcarin@duke.edu	Xu, Hongteng/AAB-1636-2021		DARPA; DOE; NIH; ONR; NSF	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	This research was supported in part by DARPA, DOE, NIH, ONR and NSF. We thank Dr. Hongyuan Zha for helpful discussions.	Altschuler J., 2017, ADV NEURAL INFORM PR, P1964; Alvarez-Melis David, 2018, P 2018 C EMP METH NA, P1881, DOI DOI 10.18653/V1/D18-1214; Banerjee A, 2013, SCIENCE, V341, P363, DOI 10.1126/science.1236498; Barabasi A.-L., 2016, NETW SCI; Benamou JD, 2015, SIAM J SCI COMPUT, V37, pA1111, DOI 10.1137/141000439; Blondel VD, 2008, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2008/10/P10008; Brandes U, 2003, LECT NOTES COMPUT SC, V2832, P568; Bronstein AM, 2010, INT J COMPUT VISION, V89, P266, DOI 10.1007/s11263-009-0301-6; Bunne C., 2018, NEURIPS WORKSH REL R; Chindelevitch L, 2013, BIOINFORMATICS, V29, P2765, DOI 10.1093/bioinformatics/btt486; Chowdhury S., 2018, ARXIV180804337; Clauset A, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.066111; Cordella LP, 2004, IEEE T PATTERN ANAL, V26, P1367, DOI 10.1109/TPAMI.2004.75; Cour T., 2007, P ADV NEURAL INFORM, P313; Courty N., 2018, ARXIV180509114; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799; Gold S, 1996, IEEE T PATTERN ANAL, V18, P377, DOI 10.1109/34.491619; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Hashemifar S, 2014, BIOINFORMATICS, V30, pI438, DOI 10.1093/bioinformatics/btu450; Jun SH, 2017, PR MACH LEARN RES, V54, P1075; Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997; Kuchaiev O., 2010, J ROYAL SOC INTERFAC; Kuchaiev O, 2011, BIOINFORMATICS, V27, P1390, DOI 10.1093/bioinformatics/btr127; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Leskovec J, 2014, SNAP DATASETS STANFO; Liao CS, 2009, BIOINFORMATICS, V25, pI253, DOI 10.1093/bioinformatics/btp203; Liaoruo Wang, 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P784, DOI 10.1109/ICDM.2011.48; Malod-Dognin N, 2015, BIOINFORMATICS, V31, P2182, DOI 10.1093/bioinformatics/btv130; Memoli Facundo, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P256, DOI 10.1109/ICCVW.2009.5457690; Memoli F, 2011, FOUND COMPUT MATH, V11, P417, DOI 10.1007/s10208-011-9093-5; Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46; Neyshabur B, 2013, BIOINFORMATICS, V29, P1654, DOI 10.1093/bioinformatics/btt202; Pachauri D., 2013, ADV NEURAL INFORM PR, V26, P1860; Pares F, 2018, STUD COMPUT INTELL, V689, P229, DOI 10.1007/978-3-319-72150-7_19; Patro R, 2012, BIOINFORMATICS, V28, P3105, DOI 10.1093/bioinformatics/bts592; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Peyre G, 2016, PR MACH LEARN RES, V48; Raghavan UN, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.036106; Sapiro G., 2004, P 2004 EUR ACM SIGGR, V71, P33, DOI [10.1145/1057432.1057436, DOI 10.1145/1057432.1057436]; Sharan R, 2006, NAT BIOTECHNOL, V24, P427, DOI 10.1038/nbt1196; Singh R., 2008, P NATL ACAD SCI; SINKHORN R, 1967, PAC J MATH, V21, P343, DOI 10.2140/pjm.1967.21.343; Sturm KT, 2006, ACTA MATH-DJURSHOLM, V196, P65, DOI 10.1007/s11511-006-0002-8; Vayer T., 2018, ARXIV181102834; Vijayan V, 2015, BIOINFORMATICS, V31, P2409, DOI 10.1093/bioinformatics/btv161; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Xie Y., 2018, ARXIV180204307; Xu Hongteng, 2019, ARXIV190106003; Yan JC, 2015, IEEE I CONF COMP VIS, P199, DOI 10.1109/ICCV.2015.31; Yan JC, 2015, IEEE T IMAGE PROCESS, V24, P994, DOI 10.1109/TIP.2014.2387386; Yang Z, 2016, SCI REP-UK, V6, DOI 10.1038/srep30750; Yu T., 2018, PROC 31TH INT C NEUR, P861; Zhang JW, 2015, IEEE DATA MINING, P599, DOI 10.1109/ICDM.2015.114	54	21	21	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303008
C	Yeh, CK; Hsieh, CY; Suggala, AS; Inouye, DI; Ravikumar, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yeh, Chih-Kuan; Hsieh, Cheng-Yu; Suggala, Arun Sai; Inouye, David I.; Ravikumar, Pradeep			On the (In)fidelity and Sensitivity of Explanations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider objective evaluation measures of saliency explanations for complex black-box machine learning models. We propose simple robust variants of two notions that have been considered in recent literature: (in)fidelity, and sensitivity. We analyze optimal explanations with respect to both these measures, and while the optimal explanation for sensitivity is a vacuous constant explanation, the optimal explanation for infidelity is a novel combination of two popular explanation methods. By varying the perturbation distribution that defines infidelity, we obtain novel explanations by optimizing infidelity, which we show to out-perform existing explanations in both quantitative and qualitative measurements. Another salient question given these measures is how to modify any given explanationto have better values with respect to these measures. We propose a simple modification based on lowering sensitivity, and moreover show that when done appropriately, we could simultaneously improve both sensitivity as well as fidelity.	[Yeh, Chih-Kuan; Hsieh, Cheng-Yu; Suggala, Arun Sai; Ravikumar, Pradeep] Carnegie Mellon Univ, Dept Machine Learning, Pittsburgh, PA 15213 USA; [Inouye, David I.] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA	Carnegie Mellon University; Purdue University System; Purdue University; Purdue University West Lafayette Campus	Yeh, CK (corresponding author), Carnegie Mellon Univ, Dept Machine Learning, Pittsburgh, PA 15213 USA.	cjyeh@cs.cmu.edu; chyu.hsieh@gmail.com; asuggala@andrew.cmu.edu; dinouye@purdue.edu; pradeepr@cs.cmu.edu			DARPA [FA87501720152]; Accenture	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Accenture	We acknowledge the support of DARPA via FA87501720152, and Accenture.	Adebayo Julius, 2018, ADV NEURAL INFORM PR, V1, P7; Alvarez-Melis David, 2018, ARXIV180608049; Ancona Marco, 2018, ICLR, DOI DOI 10.1109/TNSE.2020.2996738; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Baehrens D, 2010, J MACH LEARN RES, V11, P1803; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Chang C.-H., 2019, INT C LEARN REPR; Chen JB, 2018, PR MACH LEARN RES, V80; Cohen J, 2019, PR MACH LEARN RES, V97; Dabkowski P, 2017, ADV NEUR IN, V30; Datta A, 2016, P IEEE S SECUR PRIV, P598, DOI 10.1109/SP.2016.42; EricWong Zico, 2018, INT C MACH LEARN, P5286; Ghorbani A., 2019, AAAI; Ghorbani Amirata, 2019, ARXIV190203129, P2; Goyal Y., 2019, ABS190407451 CORR; Khanna R., 2018, ARXIV181010118; Kim B, 2017, ARXIV PREPRINT ARXIV; Kim B., 2018, ARXIV180610758; Kindermans P., 2018, P INT C LEARNING REP, P1; Kindermans P-J, 2019, EXPLAINABLE INTERPRE, P267, DOI DOI 10.1007/978-3-030-28954-6_14; Koh PW, 2017, PR MACH LEARN RES, V70; Kulesza T., 2015, P 20 INT C INTELLIGE, P126, DOI 10.1145/2678025.2701399; Lee G.-H., 2019, INT C LEARN REPR; Liu XQ, 2018, LECT NOTES COMPUT SC, V11211, P381, DOI 10.1007/978-3-030-01234-2_23; Lundberg SM, 2017, ADV NEUR IN, V30; Madry Aleksander, 2017, ARXIV; Miller T., 2017, EXPLANATION ARTIFICI; Montavon G, 2018, DIGIT SIGNAL PROCESS, V73, P1, DOI 10.1016/j.dsp.2017.10.011; Petsiuk V., 2018, P BRIT MACH VIS C 20, P151; Plumb G, 2018, ADV NEUR IN, V31; Raghunathan Aditi, 2018, ARXIV180109344; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Ross A.S., 2017, ARXIV171109404; Samek W, 2017, IEEE T NEUR NET LEAR, V28, P2660, DOI 10.1109/TNNLS.2016.2599820; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shrikumar A, 2017, PR MACH LEARN RES, V70; Shrikumar Avanti, 2016, ARXIV160501713; Simonyan Karen, 2013, DEEP INSIDE CONVOLUT, P2; Sinha A, 2017, ARXIV171010571; Smilkov D, 2017, ARXIV; Springenberg J.T., 2014, ARXIV14126806; Strumbelj E, 2014, KNOWL INF SYST, V41, P647, DOI 10.1007/s10115-013-0679-x; Sundararajan M, 2017, PR MACH LEARN RES, V70; Yeh CK, 2018, ADV NEUR IN, V31; Ying Rex, 2019, Adv Neural Inf Process Syst, V32, P9240; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang X., 2018, ARXIV181200891; Zintgraf Luisa M., 2017, P ICLR	49	21	21	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902058
C	Duan, X; Huang, WB; Gan, C; Wang, JD; Zhu, WW; Huang, JZ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Duan, Xuguang; Huang, Wenbing; Gan, Chuang; Wang, Jingdong; Zhu, Wenwu; Huang, Junzhou			Weakly Supervised Dense Event Captioning in Videos	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Dense event captioning aims to detect and describe all events of interest contained in a video. Despite the advanced development in this area, existing methods tackle this task by making use of dense temporal annotations, which is dramatically source-consuming. This paper formulates a new problem: weakly supervised dense event captioning, which does not require temporal segment annotations for model training Our solution is based on the one-to-one correspondence assumption, each caption describes one temporal segment, and each temporal segment has one caption, which holds in current benchmark datasets and most real-world cases. We decompose the problem into a pair of dual problems: event captioning and sentence localization and present a cycle system to train our model. Extensive experimental results are provided to demonstrate the ability of our model on both dense event captioning and sentence localization in videos.	[Duan, Xuguang; Zhu, Wenwu] Tsinghua Univ, Beijing, Peoples R China; [Huang, Wenbing; Huang, Junzhou] Tencent AI Lab, Beijing, Peoples R China; [Gan, Chuang] MIT IBM Watson Lab, Beijing, Peoples R China; [Wang, Jingdong] Microsoft Res Asia, Beijing, Peoples R China	Tsinghua University; Tencent; Microsoft; Microsoft Research Asia	Duan, X (corresponding author), Tsinghua Univ, Beijing, Peoples R China.	duan_xg@outlook.com; hwenbing@126.com; ganchuang1990@gmail.com; jingdw@microsoft.com; wwzhu@tsinghua.edu.cn; joehhuang@tencent.com	Wang, Jingdong/E-9920-2017; Huang, Wenbing/AHB-1846-2022; Huang, Wenbing/AAI-7943-2021	Wang, Jingdong/0000-0002-4888-4445; Huang, Wenbing/0000-0002-2566-4159; Huang, Wenbing/0000-0002-2566-4159	National Program on Key Basic Research Project [2015CB352300]; National Natural Science Foundation of China Major Project [U1611461]	National Program on Key Basic Research Project(National Basic Research Program of China); National Natural Science Foundation of China Major Project(National Natural Science Foundation of China (NSFC))	This work was supported in part by National Program on Key Basic Research Project (No. 2015CB352300), and National Natural Science Foundation of China Major Project (No. U1611461).	Banerjee S., 2005, P ACL WORKSH INTR EX, P65; Bengio Y., 2014, ARXIV14061078; Bojanowski P, 2015, IEEE I CONF COMP VIS, P4462, DOI 10.1109/ICCV.2015.507; Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675; Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698; CHIDUME CE, 1987, P AM MATH SOC, V99, P283, DOI 10.2307/2046626; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47; Gao J., 2017, ARXIV170306189; Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563; Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kay W., 2017, ARXIV PREPRINT ARXIV; Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83; Li Yehao, 2018, ARXIV180408274; Lin Chin-Yew, 2004, P 42 ANN M ASS COMP, P605, DOI DOI 10.3115/1218955.1219032; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Naim I, 2014, AAAI CONF ARTIF INTE, P1558; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Shen ZQ, 2017, PROC CVPR IEEE, P5159, DOI 10.1109/CVPR.2017.548; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Song Y. C., 2016, INT JOINT C ART INT, P2025; VEDANTAM R, 2015, PROC CVPR IEEE, P4566, DOI DOI 10.1109/CVPR.2015.7299087; Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515; Venugopalan Subhashini, 2014, ARXIV14124729; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Wang Jingwen, 2018, ARXIV180400100; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Xu H, 2015, WORKSH CLOS LOOP VIS; Yao  T., MSR ASIA MSM ACTIVIT; YAO T, 2016, PROC CVPR IEEE, P982, DOI DOI 10.1109/CVPR.2016.112; Yao T, 2016, ARXIV161101646; Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496; Yuan Yitian, 2018, ARXIV180407014; Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911	35	21	22	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303009
C	Goel, V; Weng, J; Poupart, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Goel, Vik; Weng, Jameson; Poupart, Pascal			Unsupervised Video Object Segmentation for Deep Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a new technique for deep reinforcement learning that automatically detects moving objects and uses the relevant information for action selection. The detection of moving objects is done in an unsupervised way by exploiting structure from motion. Instead of directly learning a policy from raw images, the agent first learns to detect and segment moving objects by exploiting flow information in video sequences. The learned representation is then used to focus the policy of the agent on the moving objects. Over time, the agent identifies which objects are critical for decision making and gradually builds a policy based on relevant moving objects. This approach, which we call Motion-Oriented REinforcement Learning (MOREL), is demonstrated on a suite of Atari games where the ability to detect moving objects reduces the amount of interaction needed with the environment to obtain a good policy. Furthermore, the resulting policy is more interpretable than policies that directly map images to actions or values with a black box neural network. We can gain insight into the policy by inspecting the segmentation and motion of each object detected by the agent. This allows practitioners to confirm whether a policy is making decisions based on sensible information. Our code is available at https://github.com/vik-goel/MOREL.	[Goel, Vik] Univ Waterloo, Cheriton Sch Comp Sci, Waterloo AI Inst, Toronto, ON, Canada; Vector Inst, Toronto, ON, Canada	University of Waterloo	Goel, V (corresponding author), Univ Waterloo, Cheriton Sch Comp Sci, Waterloo AI Inst, Toronto, ON, Canada.	v5goel@uwaterloo.ca; jj2weng@uwaterloo.ca; ppoupart@uwaterloo.ca						Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Battaglia Peter W, 2016, ARXIV161200222; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; BERGEN JR, 1992, LECT NOTES COMPUT SC, V588, P237; Diuk C., 2008, ICML, DOI [10.1145/1390156.1390187, DOI 10.1145/1390156.1390187]; Dubey R., 2018, INVESTIGATING HUMAN; Finn C, 2016, IEEE INT CONF ROBOT, P512, DOI 10.1109/ICRA.2016.7487173; Friedman N., 1997, P C UNC ART INT AUG, P175, DOI DOI 10.1016/J.CVIU.2007.08.003; Grondman I, 2012, IEEE T SYST MAN CY C, V42, P1291, DOI 10.1109/TSMCC.2012.2218595; Ha David, 2018, NEURIPS, DOI [10.5281/zenodo.1207631, DOI 10.5281/ZENODO.1207631]; Higgins I, 2017, PR MACH LEARN RES, V70; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Iyer R., 2018, TRANSPARENCY EXPLANA; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jaderberg Max, 2017, ICLR; Kansky K, 2017, PR MACH LEARN RES, V70; Khan O. Z., 2009, ICAPS; Kingma D.P, P 3 INT C LEARNING R; Lange S., 2010, PROC INT JOINT C NEU, P1; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Y. Z., 2017, PROC 3 GLOBAL C ARTI, V50, P20; Mahmoudieh  Parsa, 2017, SELF SUPERVISION REI; Mirowski P., 2017, PROC INT C LEARN REP, P1; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Mnih V, 2016, PR MACH LEARN RES, V48; Moerland T. M., 2017, LEARNING MULTIMODAL; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Scholz J, 2014, PR MACH LEARN RES, V32, P1089; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Simonyan Karen, 2013, DEEP INSIDE CONVOLUT, P2; Sutton R. S., 1998, REINFORCEMENT LEARNI, V1; Van Hasselt Hado, 2016, P AAAI C ART INT, V30; Vijayanarasimhan Sudheendra, 2017, ARXIV170407804; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu YH, 2017, ADV NEUR IN, V30; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700	39	21	21	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000021
C	He, TY; Tan, X; Xia, YC; He, D; Qin, T; Chen, ZB; Liu, TY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		He, Tianyu; Tan, Xu; Xia, Yingce; He, Di; Qin, Tao; Chen, Zhibo; Liu, Tie-Yan			Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Neural Machine Translation (NMT) has achieved remarkable progress with the quick evolvement of model structures. In this paper, we propose the concept of layer-wise coordination for NMT, which explicitly coordinates the learning of hidden representations of the encoder and decoder together layer by layer, gradually from low level to high level. Specifically, we design a layer-wise attention and mixed attention mechanism, and further share the parameters of each layer between the encoder and decoder to regularize and coordinate the learning. Experiments show that combined with the state-of-the-art Transformer model, layer-wise coordination achieves improvements on three IWSLT and two WMT translation tasks. More specifically, our method achieves 34.43 and 29.01 BLEU score on WMT16 English-Romanian and WMT14 English-German tasks, outperforming the Transformer baseline.	[He, Tianyu; Chen, Zhibo] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei, Anhui, Peoples R China; [Tan, Xu; Xia, Yingce; Qin, Tao; Liu, Tie-Yan] Microsoft Res, Beijing, Peoples R China; [He, Di] Peking Univ, Sch EECS, MOE, Key Lab Machine Percept, Beijing, Peoples R China; [He, Tianyu] Microsoft Res Asia, Beijing, Peoples R China	Chinese Academy of Sciences; University of Science & Technology of China, CAS; Microsoft; Peking University; Microsoft; Microsoft Research Asia	He, TY (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei, Anhui, Peoples R China.	hetianyu@mail.ustc.edu.cn; xuta@microsoft.com; yingce.xia@microsoft.com; di_he@pku.edu.cn; taoqin@microsoft.com; chenzhibo@ustc.edu.cn; tie-yan.liu@microsoft.com		Qin, Tao/0000-0002-9095-0776	National Key Research and Development Program of China [2016YFC0801001]; National Program on Key Basic Research Projects (973 Program) [2015CB351803]; NSFC [61571413, 61632001, 61390514]	National Key Research and Development Program of China; National Program on Key Basic Research Projects (973 Program)(National Basic Research Program of China); NSFC(National Natural Science Foundation of China (NSFC))	This work was partially supported by the National Key Research and Development Program of China under Grant No. 2016YFC0801001, the National Program on Key Basic Research Projects (973 Program) under Grant 2015CB351803, NSFC under Grant 61571413, 61632001, 61390514. We thank all the anonymous reviewers for their valuable comments on our paper.	[Anonymous], CORR; [Anonymous], [No title captured]; Ba J., 2017, P 3 INT C LEARN REPR; Bahdanau D., 2015, ICLR 2015; Bahdanau D., 2017, 5 INT C LEARN REPR I; Cettolo M., 2014, P 11 IWSLT; Chen JZ, 2016, PROCEEDINGS OF 2016 12TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS), P551, DOI [10.1109/CIS.2016.133, 10.1109/CIS.2016.0134]; Cho K., 2014, P 2014 C EMP METH NA, P1724; Ding YZ, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1150, DOI 10.18653/v1/P17-1106; Gehring J, 2017, PR MACH LEARN RES, V70; Gu Jiatao, 2017, CORR; Hu B., 2014, P 27 INT C NEUR INF, P2042; Huang Po-Sen, 2017, ARXIV170605565; Koehn Philipp, 2017, NEURAL MACHINE TRANS, P117; Le Q. V., 2014, ADV NEURAL INFORM PR, V2014, P3104, DOI DOI 10.5555/2969033; Lin Z., 2017, ICLR; Luong M. -T., 2015, EMNLP; Pang L, 2016, AAAI CONF ARTIF INTE, P2793; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Ranzato M., 2015, CORR; Sennrich R., 2016, WMT, V2, P371, DOI DOI 10.18653/V1/W16-2323; Sennrich Rico, 2016, P 54 ANN M ASS COMP, V1; Shazeer Noam, 2017, CORR; Shen Y., 2018, P 2018 C N AM CHAPT, V1, P1294, DOI DOI 10.18653/V1/N18-1117; Simonyan K., 2016, CORR; Song K., 2018, COLING, P3064; T_ackstr_om O, 2016, P 2016 C EMP METH NA; Tu Z., 2017, T ASSOC COMPUT LING, V5, P87, DOI DOI 10.1162/TACL_A_00048; Tu ZP, 2017, AAAI CONF ARTIF INTE, P3097; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Vaswani Ashish, 2018, CORR; Wang Y., 2018, AAAI; Wu L., 2018, EMNLP; Xia Yingce, 2018, P 35 INT C MACH LEAR, P5379	37	21	21	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002049
C	Huang, SY; Qi, SY; Xiao, YX; Zhu, YX; Wu, YN; Zhu, SC		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Huang, Siyuan; Qi, Siyuan; Xiao, Yinxue; Zhu, Yixin; Wu, Ying Nian; Zhu, Song-Chun			Cooperative Holistic Scene Understanding: Unifying 3D Object, Layout, and Camera Pose Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Holistic 3D indoor scene understanding refers to jointly recovering the i) object bounding boxes, ii) room layout, and iii) camera pose, all in 3D. The existing methods either are ineffective or only tackle the problem partially. In this paper, we propose an end-to-end model that simultaneously solves all three tasks in real-time given only a single RGB image. The essence of the proposed method is to improve the prediction by i) parametrizing the targets (e.g., 3D boxes) instead of directly estimating the targets, and ii) cooperative training across different modules in contrast to training these modules individually. Specifically, we parametrize the 3D object bounding boxes by the predictions from several modules, i.e., 3D camera pose and object attributes. The proposed method provides two major advantages: i) The parametrization helps maintain the consistency between the 2D image and the 3D world, thus largely reducing the prediction variances in 3D coordinates. ii) Constraints can be imposed on the parametrization to train different modules simultaneously. We call these constraints "cooperative losses" as they enable the joint training and inference. We employ three cooperative losses for 3D bounding boxes, 2D projections, and physical constraints to estimate a geometrically consistent and physically plausible 3D scene. Experiments on the SUN RGB-D dataset shows that the proposed method significantly outperforms prior approaches on 3D object detection, 3D layout estimation, 3D camera pose estimation, and holistic scene understanding.	[Huang, Siyuan; Zhu, Yixin; Wu, Ying Nian; Zhu, Song-Chun] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90024 USA; [Qi, Siyuan; Xiao, Yinxue; Zhu, Song-Chun] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA	University of California System; University of California Los Angeles; University of California System; University of California Los Angeles	Huang, SY (corresponding author), Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90024 USA.	huangsiyuan@ucla.edu; syqi@cs.ucla.edu; yinxuex@ucla.edu; yixin.zhu@ucla.edu; ywu@stat.ucla.edu; sczhu@stat.ucla.edu			DARPA XAI grant [N66001-17-2-4029]; ONR MURI grant [N00014-16-1-2007]; ARO grant [W911NF-18-1-0296]; NVIDIA GPU donation grant	DARPA XAI grant; ONR MURI grant; ARO grant; NVIDIA GPU donation grant	The work reported herein was supported by DARPA XAI grant N66001-17-2-4029, ONR MURI grant N00014-16-1-2007, ARO grant W911NF-18-1-0296, and an NVIDIA GPU donation grant. We thank Prof. Hongjing Lu from the UCLA Psychology Department for useful discussions on the motivation of this work, and three anonymous reviewers for their constructive comments.	Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593; Chen L, 2018, PROC CVPR IEEE, P1043, DOI 10.1109/CVPR.2018.00115; Choi WG, 2013, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2013.12; Dahua Lin, 2013, IEEE INT C COMP VIS; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Gupta Abhinav, 2010, C NEUR INF PROC SYST; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hedau Varsha, 2009, IEEE C COMP VIS PATT; Izadinia Hamid, 2017, P IEEE C COMP VIS PA, P3; Jacobs RA, 2002, TRENDS COGN SCI, V6, P345, DOI 10.1016/S1364-6613(02)01948-4; Kehl W, 2017, PROC CVPR IEEE, P465, DOI 10.1109/CVPR.2017.57; Kingma D.P, P 3 INT C LEARNING R; Kubricht JR, 2017, TRENDS COGN SCI, V21, P749, DOI 10.1016/j.tics.2017.06.002; Kundu A, 2018, PROC CVPR IEEE, P3559, DOI 10.1109/CVPR.2018.00375; Lahoud J, 2017, IEEE I CONF COMP VIS, P4632, DOI 10.1109/ICCV.2017.495; LANDY MS, 1995, VISION RES, V35, P389, DOI 10.1016/0042-6989(94)00176-M; Lee CY, 2017, IEEE I CONF COMP VIS, P4875, DOI 10.1109/ICCV.2017.521; Mousavian A., 2017, PROC CVPR IEEE, P7074, DOI DOI 10.1109/CVPR.2017.597; Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2; Oliva Aude, 2005, P251, DOI 10.1016/B978-012375731-9/50045-8; Paszke A., 2017, AUTOMATIC DIFFERENTI; POTTER MC, 1976, J EXP PSYCHOL-HUM L, V2, P509, DOI 10.1037/0278-7393.2.5.509; POTTER MC, 1975, SCIENCE, V187, P965, DOI 10.1126/science.1145183; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rezende Danilo Jimenez, 2016, C NEUR INF PROC SYST; Ruiqi Guo, 2013, IEEE INT C COMP VIS; Schwing Alexander G, 2013, IEEE INT C COMP VIS; SCHYNS PG, 1994, PSYCHOL SCI, V5, P195, DOI 10.1111/j.1467-9280.1994.tb00500.x; Siyuan Huang, 2018, EUR C COMP VIS ECCV; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Song SR, 2014, LECT NOTES COMPUT SC, V8694, P634, DOI 10.1007/978-3-319-10599-4_41; Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0; Tulsiani Shubham, 2018, CVPR, DOI DOI 10.1109/CVPR.2018.00306; Tulsiani S, 2015, PROC CVPR IEEE, P1510, DOI 10.1109/CVPR.2015.7298758; Wu J, 2017, C NEUR INF PROC SYST; Wu JJ, 2016, LECT NOTES COMPUT SC, V9910, P365, DOI 10.1007/978-3-319-46466-4_22; Yan Xinchen, 2016, C NEUR INF PROC SYST; Yibiao Zhao, 2013, IEEE C COMP VIS PATT; Yibiao Zhao, 2011, C NEUR INF PROC SYST; Yinda Zhang, 2017, IEEE INT C COMP VIS; Yinda Zhang, 2017, IEEE C COMP VIS PATT; Yinda Zhang, 2014, EUR C COMP VIS ECCV; Zhuo Deng, 2017, IEEE C COMP VIS PATT; Zou CH, 2018, PROC CVPR IEEE, P2051, DOI 10.1109/CVPR.2018.00219; Zou Chuhang, 2017, ARXIV171009490	47	21	22	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300020
C	Jean, N; Xie, SM; Ermon, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jean, Neal; Xie, Sang Michael; Ermon, Stefano			Semi-supervised Deep Kernel Learning: Regression with Unlabeled Data by Minimizing Predictive Variance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				POSTERIOR REGULARIZATION	Large amounts of labeled data are typically required to train deep learning models. For many real-world problems, however, acquiring additional data can be expensive or even impossible. We present semi-supervised deep kernel learning (SSDKL), a semi-supervised regression model based on minimizing predictive variance in the posterior regularization framework. SSDKL combines the hierarchical representation learning of neural networks with the probabilistic modeling capabilities of Gaussian processes. By leveraging unlabeled data, we show improvements on a diverse set of real-world regression tasks over supervised deep kernel learning and semi-supervised methods such as VAT and mean teacher adapted for regression.	[Jean, Neal; Xie, Sang Michael; Ermon, Stefano] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Jean, N (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	nealjean@cs.stanford.edu; xie@cs.stanford.edu; ermon@cs.stanford.edu		Xie, Sang Michael/0000-0002-0820-2753	NSF [1651565, 1522054, 1733686]; ONR; Sony; FLI; Department of Defense (DoD) through the National Defense Science & Engineering Graduate Fellowship (NDSEG) Program	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); Sony; FLI; Department of Defense (DoD) through the National Defense Science & Engineering Graduate Fellowship (NDSEG) Program	This research was supported by NSF (#1651565, #1522054, #1733686), ONR, Sony, and FLI. NJ was supported by the Department of Defense (DoD) through the National Defense Science & Engineering Graduate Fellowship (NDSEG) Program. We are thankful to Volodymyr Kuleshov and Aditya Grover for helpful discussions.	Abadi M, 2015, P 12 USENIX S OPERAT; Al-Shedivat M., 2016, ARXIV161008936; Arnold Andrew, 2007, P 7 IEEE INT C DAT M; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Chapelle O, 2005, P 10 INT WORKSH ART, V2005, P57; Damianou Andreas C., 2013, ARTIF INTELL STAT; Eissman Stephan, 2018, P 34 C UNC ART INT; Ganchev K, 2010, J MACH LEARN RES, V11, P2001; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Grandvalet Yves, 2004, NIPS, P529; Jean N, 2016, SCIENCE, V353, P790, DOI 10.1126/science.aaf7894; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kuleshov V, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Laine Samuli, 2017, P INT C LEARN REPR I, P3; Lichman M, 2013, UCI MACHINE LEARNING; Maaloe L, 2016, PR MACH LEARN RES, V48; Miyato T, 2015, ARXIV150700677; Miyato T., 2017, ABS170403976 CORR; Odena A., 2018, REALISTIC EVALUATION; Oshri Barak, 2018, P 24 ACM SIGKDD C; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Ren Russell, 2018, P 27 INT JOINT C ART; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Shu R., 2018, ADV NEURAL INFORM PR, P4393; Shu Rui, 2018, P 6 INT C LEARN REPR, P2; Tarvainen Antti, 2017, CORR, Vabs/1703; Wilson AG, 2016, ADV NEUR IN, V29; Wilson AG, 2016, JMLR WORKSH CONF PRO, V51, P370; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775; Wilson Andrew Gordon, 2015, J MACHINE LEARNING R; Xie M, 2016, AAAI CONF ARTIF INTE, P3929; You J., 2017, DEEP GAUSSIAN PROCES; Yu Kai, 2006, INT C MACH LEARN ICM; Zhao CY, 2015, 2015 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P1342, DOI 10.1109/GlobalSIP.2015.7418417; ZHOU ZH, 2005, IJCAI, P908; Zhu J, 2014, J MACH LEARN RES, V15, P1799; Zhu X., 2009, ADV NEURAL INFORM PR, P1513; Zhu X., 2009, SYNTHESIS LECT ARTIF, V3, P1, DOI [10.2200/S00196ED1V01Y200906AIM006, DOI 10.2200/S00196ED1V01Y200906AIM006]; Zhu Xiaojin, 2002, TECHNICAL REPORT, P1	41	21	21	2	11	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305035
C	Khayatkhoei, M; Elgammal, A; Singh, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Khayatkhoei, Mahyar; Elgammal, Ahmed; Singh, Maneesh			Disconnected Manifold Learning for Generative Adversarial Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Natural images may lie on a union of disjoint manifolds rather than one globally connected manifold, and this can cause several difficulties for the training of common Generative Adversarial Networks (GANs). In this work, we first show that single generator GANs are unable to correctly model a distribution supported on a disconnected manifold, and investigate how sample quality, mode dropping and local convergence are affected by this. Next, we show how using a collection of generators can address this problem, providing new insights into the success of such multi-generator GANs. Finally, we explain the serious issues caused by considering a fixed prior over the collection of generators and propose a novel approach for learning the prior and inferring the necessary number of generators without any supervision. Our proposed modifications can be applied on top of any other GAN model to enable learning of distributions supported on disconnected manifolds. We conduct several experiments to illustrate the aforementioned shortcoming of GANs, its consequences in practice, and the effectiveness of our proposed modifications in alleviating these issues.	[Khayatkhoei, Mahyar; Elgammal, Ahmed] Rutgers State Univ, Dept Comp Sci, New Brunswick, NJ 08901 USA; [Singh, Maneesh] Verisk Analyt, Jersey City, NJ USA	Rutgers State University New Brunswick	Khayatkhoei, M (corresponding author), Rutgers State Univ, Dept Comp Sci, New Brunswick, NJ 08901 USA.	m.khayatkhoei@cs.rutgers.edu; elgammal@cs.rutgers.edu; maneesh.singh@verisk.com			Verisk Analytics; NSF-USA [1409683]	Verisk Analytics; NSF-USA(National Science Foundation (NSF))	This work was supported by Verisk Analytics and NSF-USA award number 1409683.	Arjovsky Mart<prime>in, 2017, P 5 INT C LEARN REPR; Arjovsky M, 2017, PR MACH LEARN RES, V70; Barratt S., 2018, ARXIV180101973; Che Tong, 2016, ARXIV161202136; Donahue J., 2016, ARXIV160509782; Dumoulin Vincent, 2016, ARXIV E PRINTS; Ghosh A., 2017, ARXIV170402906; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Gurumurthy S, 2017, IEEE C COMP VIS PATT, V1; Hensel M, 2017, ADV NEUR IN, V30; Kelley J. L., 1975, GEN TOPOLOGY; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lucic Mario, 2017, ARXIV171110337; Mescheder L, 2018, PR MACH LEARN RES, V80; Metz Luke, 2016, ARXIV161102163; Nagarajan V, 2017, ADV NEUR IN, V30; Quan Hoang, 2018, INT C LEARN REPR; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Salimans T, 2016, ADV NEUR IN, V29; Srivastava Akash, 2017, ADV NEURAL INFORM PR, P3310, DOI DOI 10.5555/3294996.3295090; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Yang MH, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P224; Yann L., 1998, MNIST DATABASE HANDW, P1; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7	27	21	21	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001086
C	Wu, LJ; Tian, F; Xia, Y; Fan, Y; Qin, T; Lai, JH; Liu, TY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wu, Lijun; Tian, Fei; Xia, Yingce; Fan, Yang; Qin, Tao; Lai, Jianhuang; Liu, Tie-Yan			Learning to Teach with Dynamic Loss Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Teaching is critical to human society: it is with teaching that prospective students are educated and human civilization can be inherited and advanced. A good teacher not only provides his/her students with qualified teaching materials (e.g., textbooks), but also sets up appropriate learning objectives (e.g., course projects and exams) considering different situations of a student. When it comes to artificial intelligence, treating machine learning models as students, the loss functions that are optimized act as perfect counterparts of the learning objective set by the teacher. In this work, we explore the possibility of imitating human teaching behaviors by dynamically and automatically outputting appropriate loss functions to train machine learning models. Different from typical learning settings in which the loss function of a machine learning model is predefined and fixed, in our framework, the loss function of a machine learning model (we call it student) is defined by another machine learning model (we call it teacher). The ultimate goal of teacher model is cultivating the student to have better performance measured on development dataset. Towards that end, similar to human teaching, the teacher, a parametric model, dynamically outputs different loss functions that will be used and optimized by its student model at different training stages. We develop an efficient learning method for the teacher model that makes gradient based optimization possible, exempt of the ineffective solutions such as policy optimization. We name our method as "learning to teach with dynamic loss functions" (L2T-DLF for short). Extensive experiments on real world tasks including image classification and neural machine translation demonstrate that our method significantly improves the quality of various student models.	[Wu, Lijun; Lai, Jianhuang] Sun Yat Sen Univ, Guangzhou, Guangdong, Peoples R China; [Tian, Fei; Xia, Yingce; Qin, Tao; Liu, Tie-Yan] Microsoft Res, Beijing, Peoples R China; [Fan, Yang] Univ Sci & Technol China, Hefei, Anhui, Peoples R China; [Fan, Yang] Microsoft Res Asia, Beijing, Peoples R China	Sun Yat Sen University; Microsoft; Chinese Academy of Sciences; University of Science & Technology of China, CAS; Microsoft; Microsoft Research Asia	Wu, LJ (corresponding author), Sun Yat Sen Univ, Guangzhou, Guangdong, Peoples R China.	wulijun3@mail2.sysu.edu.cn; fetia@microsoft.com; yingce.xia@microsoft.com; fyabc@mail.ustc.edu.cn; taoqin@microsoft.com; stsljh@mail.sysu.edu.cn; tie-yanliu@microsoft.com		Qin, Tao/0000-0002-9095-0776	NSFC [61573387]	NSFC(National Natural Science Foundation of China (NSFC))	This work was partially supported by the NSFC 61573387. We thank all the anonymous reviewers for their constructive feedbacks.	Abbeel P., 2017, ARXIV171100694; ANDERSON JR, 1985, SCIENCE, V228, P456, DOI 10.1126/science.228.4698.456; Andrychowicz M, 2016, ADV NEUR IN, V29; [Anonymous], 2013, P INT C MACH LEARN I; Bahdanau D., 2016, ARXIV160707086; Bahdanau Dzmitry, 2015, NEURAL MACHINE TRANS; BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077; Baydin A. G., 2014, ARXIV14047456; Bengio Y, 2000, NEURAL COMPUT, V12, P1889, DOI 10.1162/089976600300015187; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Cettolo M., 2014, P 11 INT WORKSH SPOK, V57; Chen Y., 2016, ARXIV161103824; De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z; Dollar P, 2017, ARXIV170802002; Edunov Sergey, 2017, ARXIV171104956; Fan Y., 2018, ARXIV PREPRINT ARXIV; Finn C, 2017, PR MACH LEARN RES, V70; Franceschi L, 2017, PR MACH LEARN RES, V70; Gehring J, 2017, PR MACH LEARN RES, V70; GOLDMAN SA, 1995, J COMPUT SYST SCI, V50, P20, DOI 10.1006/jcss.1995.1003; Gong C, 2016, AAAI CONF ARTIF INTE, P1610; Gong C, 2017, IEEE T NEUR NET LEAR, V28, P1452, DOI 10.1109/TNNLS.2016.2514360; Hazan E, 2016, PR MACH LEARN RES, V48; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Henderson Paul, 2016, ACCV; Ho M. K., 2016, ADV NEURAL INFORM PR, P3027; Huang G., 2017, CVPR; Huang Po-Sen, 2018, INT C LEARN REPR; Kingma D.P, P 3 INT C LEARNING R; Kumar M., 2010, NIPS, P1189, DOI DOI 10.5555/2997189.2997322; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Ke, 2016, ARXIV160601885; Li Y., 2002, PROC ICML, V2, P379; Lillicrap TP, 2016, 4 INT C LEARN REPR; Lin Z., 2017, 5 INT C LEARNING REP; Liu J, 2016, PR MACH LEARN RES, V48; Liu WY, 2016, PR MACH LEARN RES, V48; Liu Weiyang, 2017, P 34 INT C MACH LEAR, P1188; Luo R., 2018, ARXIV180807233; Maclaurin D, 2015, PR MACH LEARN RES, V37, P2113; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Ranzato MarcAurelio, 2015, ARXIV; Schmidhuber J., 1987, THESIS TU MUNICH MUC; Shafto P, 2014, COGNITIVE PSYCHOL, V71, P55, DOI 10.1016/j.cogpsych.2013.12.004; Shen SQ, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1683; Song Y, 2016, PR MACH LEARN RES, V48; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Taylor M., 2008, P 2008 INT C WEB SEA, P77, DOI DOI 10.1145/1341531.1341544; Thrun Sebastian, 2012, LEARNING TO LEARN; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270; Wu L., 2018, EMNLP; Wu Lijun, 2018, ACML; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]; Zhu XJ, 2015, AAAI CONF ARTIF INTE, P4083; Zoph Barret, 2017, P 5 INT C LEARNING R	57	21	22	1	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001004
C	Zheng, ZY; Oh, J; Singh, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zheng, Zeyu; Oh, Junhyuk; Singh, Satinder			On Learning Intrinsic Rewards for Policy Gradient Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In many sequential decision making tasks, it is challenging to design reward functions that help an RL agent efficiently learn behavior that is considered good by the agent designer. A number of different formulations of the reward-design problem have been proposed in the literature. In this paper we build on the Optimal Rewards Framework of Singh et al. [2010] that defines the optimal intrinsic reward function as one that when used by an RL agent achieves behavior that optimizes the task-specifying or extrinsic reward function. Previous work in this framework has shown how good intrinsic reward functions can be learned for lookahead search based planning agents. Whether it is possible to learn intrinsic reward functions for learning agents remains an open problem. In this paper we derive a novel algorithm for learning intrinsic rewards for policy-gradient based learning agents. We compare the performance of an augmented agent that uses our algorithm to provide additive intrinsic rewards to an A2C-based policy learner (for Atari games) and a PPO-based policy learner (for Mujoco domains) with a baseline agent that uses the same policy learners but with only extrinsic rewards. We also compare our method with using a constant "live bonus" and with using a count-based exploration bonus (i.e., pixel-SimHash). Our results show improved performance on most but not all of the domains.	[Zheng, Zeyu; Oh, Junhyuk; Singh, Satinder] Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Zheng, ZY (corresponding author), Univ Michigan, Comp Sci & Engn, Ann Arbor, MI 48109 USA.	zeyu@umich.edu; junhyuk@umich.edu; baveja@umich.edu			NSF [IIS-1526059]; Toyota Research Institute (TRI); DARPA's L2M program	NSF(National Science Foundation (NSF)); Toyota Research Institute (TRI); DARPA's L2M program	We thank Richard Lewis for conversations on optimal rewards. This work was supported by NSF grant IIS-1526059, by a grant from Toyota Research Institute (TRI), and by a grant from DARPA's L2M program. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the views of the sponsor.	Amodei D., 2016, CONCRETE PROBLEMS AI; Andrychowicz M, 2016, ADV NEUR IN, V29; Bellemare M., 2016, NEURIPS; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Duan Y., 2016, RL2 FAST REINFORCEME; Duan Y, 2016, INT C MACH LEARN, P1329; Finn C, 2017, PR MACH LEARN RES, V70; Guo X., 2016, IJCAI; Hesse C., 2017, OPENAI BASELINES; JADERBERG M, 2016, 4 INT C LEARN REPR I, P1; Kingma D.P, P 3 INT C LEARNING R; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Nichol Alex, 2018, ABS180302999 ARXIV; Ostrovski G, 2017, PR MACH LEARN RES, V70; Oudeyer Pierre-Yves, 2007, Front Neurorobot, V1, P6, DOI 10.3389/neuro.12.006.2007; Pathak D., 2017, INT C MACH LEARN ICM, V2017; Rajeswaran A., 2017, ADV NEURAL INFORM PR, P6553; Santoro A, 2016, PR MACH LEARN RES, V48; Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368; Schulman J., 2017, ABS170706347 CORR; Singh S, 2010, IEEE T AUTON MENT DE, V2, P70, DOI 10.1109/TAMD.2010.2051031; Sorg J., 2010, ADV NEURAL INFORM PR, V23, P2190; Stadie B. C., 2015, ARXIV150700814; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Tang H., 2017, NEURIPS; Vezhnevets AS, 2017, PR MACH LEARN RES, V70; Wang J.X., 2016, ARXIV161105763; Xu Zhongwen, 2018, ARXIV180509801	30	21	21	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304064
C	Duan, Y; Andrychowicz, M; Stadie, B; Ho, J; Schneider, J; Sutskeyer, I; Abbeel, P; Zaremba, W		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Duan, Yan; Andrychowicz, Marcin; Stadie, Bradly; Ho, Jonathan; Schneider, Jonas; Sutskeyer, Ilya; Abbeel, Pieter; Zaremba, Wojciech			One-Shot Imitation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NEURAL-NETWORKS	Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning. Specifically, we consider the setting where there is a very large (maybe infinite) set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained such that when it takes as input the first demonstration demonstration and a state sampled from the second demonstration, it should predict the action corresponding to the sampled state. At test time, a full demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. Our experiments show that the use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks.	[Duan, Yan; Stadie, Bradly; Ho, Jonathan; Abbeel, Pieter] Berkeley AI Res Lab, Berkeley, CA 94720 USA; [Duan, Yan; Andrychowicz, Marcin; Stadie, Bradly; Ho, Jonathan; Schneider, Jonas; Sutskeyer, Ilya; Abbeel, Pieter; Zaremba, Wojciech] OpenAI, San Francisco, CA USA		Duan, Y (corresponding author), Berkeley AI Res Lab, Berkeley, CA 94720 USA.	rockyduan@eecs.berkeley.edu; marcin@openai.com; bstadie@openai.com; jonathanho@eecs.berkeley.edu; jonas@openai.com; ilyasu@openai.com; pabbeel@eecs.berkeley.edu; woj@openai.com	Jeong, Yongwook/N-7413-2016		ONR through a PECASE award; Huawei Fellowship; NSF	ONR through a PECASE award; Huawei Fellowship(Huawei Technologies); NSF(National Science Foundation (NSF))	We would like to thank our colleagues at UC Berkeley and OpenAI for insightful discussions. This research was funded in part by ONR through a PECASE award. Yan Duan was also supported by a Huawei Fellowship. Jonathan Ho was also supported by an NSF Fellowship.	Abbeel P., 2004, INT C MACH LEARN ICM; Andrychowicz Marcin, 2016, NEURAL INFORM PROCES; [Anonymous], 2014, ABS14123474 CORR; [Anonymous], 2007, P 15 ACM INT C MULTI; Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024; Aytar Y, 2011, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2011.6126504; Ba J. L., 2016, NEURAL INFORM PROCES; Bahdanau Dzmitry, 2015, NEURAL MACHINE TRANS; Battaglia Peter W, 2016, ARXIV161200222; Bengio Samy, 1992, C OPT ART BIOL NEUR, P6; Bengio Y., 1990, LEARNING SYNAPTIC LE; Bertsekas DP, 1995, PROCEEDINGS OF THE 34TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-4, P560, DOI 10.1109/CDC.1995.478953; Calinon S., 2009, ROBOT PROGRAMMING DE; Chang Michael B, 2017, INT C LEARN REPR ICL; Cho K., 2014, P 2014 C EMP METH NA, P1724; Donahue J, 2014, PR MACH LEARN RES, V32; Duan L., 2012, ARXIV12064660; Duan Y., 2016, RL2 FAST REINFORCEME; Edwards H., 2017, INT C LEARN REPR; Finn C, 2017, ARXIV170303400; Finn Chelsea, 2016, P 33 INT C MACH LEAR, V48; Gupta Abhishek, 2017, INT C LEARN REPR ICL; Heess N., 2015, NIPS; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Hochreiter S., 2001, INT C ART NEUR NETW; Hoffman J., 2013, ARXIV13013224; Kingma D. P., 2014, ADAM METHOD STOCHAST; Koch G., 2015, ICML DEEP LEARN WORK; Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702; Levine, 2016, CAD RL REAL SINGLE I; Levine S., 2011, P ADV NEUR INF PROC, V24; Levine S, 2016, J MACH LEARN RES, V17; Li Ke, 2016, ARXIV160601885; Lillicrap TP, 2016, 4 INT C LEARN REPR; Long M., 2015, ABS150202791 CORR; Maharaj T., 2016, ARXIV160601305; Mansour Yishay, 2009, P COLT; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Naik D, 1992, INT JOINT C NEUR NET; Ng A.Y., 2003, NIPS, V16; Ng A. Y., 2000, INT C MACH LEARN; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003; Pomerleau D.A., 1989, ALVINN AUTONOMOUS LA; Ravi Sachin, 2017, ICLR; Rezende Danilo Jimenez, 2016, INT C MACH LEARN ICM; Ross S., 2011, AISTATS, P6; Rusu A. A., 2016, PROGR NEURAL NETWORK; Santoro Adam, 2016, INT C MACH LEARN ICM; Schaal S, 1999, TRENDS COGN SCI, V3, P233, DOI 10.1016/S1364-6613(99)01327-3; Schmidhuber J., 1987, THESIS TU MUNICH MUC; Schmidhuber Jurgen, 1992, LEARNING, V4; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Stadie Bradly C., 2017, INT C LEARN REPR ICL; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; TESAURO G, 1995, COMMUN ACM, V38, P58, DOI 10.1145/203330.203343; Thrun Sebastian, 1998, LEARNING LEARN, P181, DOI 10.1007/978-1-4615-5529-2; Tzeng E., 2015, ARXIV151107111; Vinyals Oriol, 2016, NEURAL INFORM PROCES; Wang J.X., 2016, ARXIV161105763; Xu K., 2015, ICML, V14, P77; Yu Fisher, 2016, P INT C LEARN REPR; Ziebart Brian D., 2008, AAAI C ART INT	66	21	21	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401013
C	Raghu, M; Gilmer, J; Yosinski, J; Sohl-Dickstein, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Raghu, Maithra; Gilmer, Justin; Yosinski, Jason; Sohl-Dickstein, Jascha			SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less.	[Raghu, Maithra; Gilmer, Justin; Sohl-Dickstein, Jascha] Google Brain, Mountain View, CA 94043 USA; [Raghu, Maithra] Cornell Univ, Ithaca, NY 14853 USA; [Yosinski, Jason] Uber AI Labs, San Francisco, CA USA	Google Incorporated; Cornell University	Raghu, M (corresponding author), Google Brain, Mountain View, CA 94043 USA.; Raghu, M (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	maithrar@gmail.com; gilmer@google.com; yosinski@uber.com; jaschasd@google.com						Alain Guillaume, 2016, ARXIV161001644; [Anonymous], DEEP LEARN WORKSH IN; [Anonymous], 2013, ARXIV PREPRINT ARXIV; Faruqui M., 2014, IMPROVING VECTOR SPA; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814; He K., 2015, CORR; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Horn R. A., 1986, MATRIX ANAL; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li Y., 2015, FE NIPS, P196; Li Y., 2016, INT C LEARN REPR ICL; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; Montavon G, 2011, J MACH LEARN RES, V12, P2563; Simonyan K., 2013, DEEP INSIDE CONVOLUT; Sussillo D, 2015, NAT NEUROSCI, V18, P1025, DOI 10.1038/nn.4042; Szegedy C, 2013, 2 INT C LEARNING REP; Wu Y., 2016, GOOGLES NEURAL MACHI; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhou B., 2014, CORR, V1412, P6856	21	21	21	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406015
C	He, K; Wang, Y; Hopcroft, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		He, Kun; Wang, Yan; Hopcroft, John			A Powerful Generative Model Using Random Weights for the Deep Image Representation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					To what extent is the success of deep visualization due to the training? Could we do deep visualization using untrained, random weight networks? To address this issue, we explore new and powerful generative models for three popular deep visualization tasks using untrained, random weight convolutional neural networks. First we invert representations in feature spaces and reconstruct images from white noise inputs. The reconstruction quality is statistically higher than that of the same method applied on well trained networks with the same architecture. Next we synthesize textures using scaled correlations of representations in multiple layers and our results are almost indistinguishable with the original natural texture and the synthesized textures based on the trained network. Third, by recasting the content of an image in the style of various artworks, we create artistic images with high perceptual quality, highly competitive to the prior work of Gatys et al. on pretrained networks. To our knowledge this is the first demonstration of image representations using untrained deep neural networks. Our work provides a new and fascinating tool to study the representation of deep network architecture and sheds light on new understandings on deep visualization. It may possibly lead to a way to compare network architectures without training.	[He, Kun; Wang, Yan] Huazhong Univ Sci & Technol, Dept Comp Sci & Technol, Wuhan 430074, Hubei, Peoples R China; [Hopcroft, John] Cornell Univ, Dept Comp Sci, Ithaca, NY 14850 USA	Huazhong University of Science & Technology; Cornell University	Wang, Y (corresponding author), Huazhong Univ Sci & Technol, Dept Comp Sci & Technol, Wuhan 430074, Hubei, Peoples R China.	brooklet60@hust.edu.cn; yanwang@hust.edu.cn; jeh@cs.cornell.edu	He, Kun/AAQ-1555-2020	He, Kun/0000-0001-7627-4604	US Army Research Office [W911NF-14-1-0477]; National Science Foundation of China [61472147]; National Science Foundation of Hubei Province [2015CFB566]	US Army Research Office; National Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Science Foundation of Hubei Province	This research work was supported by US Army Research Office(W911NF-14-1-0477) and National Science Foundation of China(61472147) and National Science Foundation of Hubei Province(2015CFB566).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; Dosovitskiy A, 2016, PROC CVPR IEEE, P4829, DOI 10.1109/CVPR.2016.522; Dosovitskiy Alexey, 2016, NEURIPS; Erhan D., 2009, ICML 2009 WORKSH LEA; Gardner J. R., 2015, ARXIV151106421; Gatys L. A., 2015, ADV NEURAL INFORM PR, V28, P262, DOI DOI 10.1016/0014-5793(76)80724-7; Gatys L. A., 2015, ARXIV150507376; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; Nguyen A., 2016, MULTIFACETED FEATURE; Nikulin Y., 2016, ARXIV160207188; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Saxe A. M., 2011, P ICML, P1089; Ulyanov D, 2016, PR MACH LEARN RES, V48; Wei D., 2015, THE CNN, V6, P6; Yosinski J., 2015, ICML DEEP LEARN WORK	23	21	21	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704092
C	Stoudenmire, EM; Schwab, DJ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Stoudenmire, E. M.; Schwab, David J.			Supervised Learning with Tensor Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				FEATURE-EXTRACTION; DECOMPOSITIONS; CLASSIFICATION	Tensor networks are approximations of high-order tensors which are efficient to work with and have been very successful for physics and mathematics applications. We demonstrate how algorithms for optimizing tensor networks can be adapted to supervised learning tasks by using matrix product states (tensor trains) to parameterize non-linear kernel learning models. For the MNIST data set we obtain less than 1% test set classification error. We discuss an interpretation of the additional structure imparted by the tensor network to the learned model.	[Stoudenmire, E. M.] Perimeter Inst Theoret Phys, Waterloo, ON N2L 2Y5, Canada; [Schwab, David J.] Northwestern Univ, Dept Phys, Evanston, IL 60208 USA	Perimeter Institute for Theoretical Physics; Northwestern University	Stoudenmire, EM (corresponding author), Perimeter Inst Theoret Phys, Waterloo, ON N2L 2Y5, Canada.		Stoudenmire, Edwin Miles/D-1180-2018	Stoudenmire, Edwin Miles/0000-0003-3389-9692				Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; Anandkumar A, 2014, J MACH LEARN RES, V15, P2239; Phan AH, 2010, IEICE NONLINEAR THEO, V1, P37, DOI 10.1587/nolta.1.37; Bengua JA, 2015, IEEE INT CONGR BIG, P669, DOI 10.1109/BigDataCongress.2015.105; Bridgeman J. C., 2016, ARXIV160303039; Cesa-Bianchi N., 2015, P C LEARN THEOR, P297; Christopher J. C., MNIST HANDWRITTEN DI; Cichocki A., 2014, ARXIV14073124; Evenbly G, 2011, J STAT PHYS, V145, P891, DOI 10.1007/s10955-011-0237-4; Holtz S, 2012, SIAM J SCI COMPUT, V34, pA683, DOI 10.1137/100818893; Muller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517; Nielsen MA., 2015, NEURAL NETWORKS DEEP; Novikov A, 2016, ARXIV160503795; Novikov Alexander, 2015, ARXIV150906569; Oseledets IV, 2011, SIAM J SCI COMPUT, V33, P2295, DOI 10.1137/090752286; OSTLUND S, 1995, PHYS REV LETT, V75, P3537, DOI 10.1103/PhysRevLett.75.3537; Schollwock U, 2011, ANN PHYS-NEW YORK, V326, P96, DOI 10.1016/j.aop.2010.09.012; Stoudenmire EM, 2013, PHYS REV B, V87, DOI 10.1103/PhysRevB.87.155137; Vapnik V., 2000, NATURE STAT LEARNING; Verstraete F, 2004, PHYS REV LETT, V93, DOI 10.1103/PhysRevLett.93.227205; VERSTRAETE F, 2004, CONDMAT0407066; Vidal G, 2007, PHYS REV LETT, V99, DOI 10.1103/PhysRevLett.99.220405; Waegeman W, 2012, IEEE T FUZZY SYST, V20, P1090, DOI 10.1109/TFUZZ.2012.2194151; WHITE SR, 1992, PHYS REV LETT, V69, P2863, DOI 10.1103/PhysRevLett.69.2863	25	21	21	0	10	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701068
C	Briol, FX; Oates, CJ; Girolami, M; Osborne, MA		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Briol, Francois-Xavier; Oates, Chris J.; Girolami, Mark; Osborne, Michael A.			Frank-Wolfe Bayesian Quadrature: Probabilistic Integration with Theoretical Guarantees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					There is renewed interest in formulating integration as a statistical inference problem, motivated by obtaining a full distribution over numerical error that can be propagated through subsequent computation. Current methods, such as Bayesian Quadrature, demonstrate impressive empirical performance but lack theoretical analysis. An important challenge is therefore to reconcile these probabilistic integrators with rigorous convergence guarantees. In this paper, we present the first probabilistic integrator that admits such theoretical treatment, called Frank-Wolfe Bayesian Quadrature (FWBQ). Under FWBQ, convergence to the true value of the integral is shown to be up to exponential and posterior contraction rates are proven to be up to super-exponential. In simulations, FWBQ is competitive with state-of-the-art methods and out-performs alternatives based on Frank-Wolfe optimisation. Our approach is applied to successfully quantify numerical error in the solution to a challenging Bayesian model choice problem in cellular biology.	[Briol, Francois-Xavier; Girolami, Mark] Univ Warwick, Dept Stat, Coventry, W Midlands, England; [Oates, Chris J.] Univ Technol Sydney, Sch Math & Phys Sci, Sydney, NSW, Australia; [Osborne, Michael A.] Univ Oxford, Dept Engn Sci, Oxford, England	University of Warwick; University of Technology Sydney; University of Oxford	Briol, FX (corresponding author), Univ Warwick, Dept Stat, Coventry, W Midlands, England.	f-x.briol@warwick.ac.uk; christopher.oates@uts.edu.au; m.girolami@warwick.ac.uk; mosb@robots.ox.ac.uk			EPSRC [EP/L016710/1, EP/D002060/1, EP/J016934/1]; EU grant [EU/259348]; Royal Society Wolfson Research Merit Award; EPSRC Established Career Fellowship	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EU grant(European Commission); Royal Society Wolfson Research Merit Award(Royal Society of London); EPSRC Established Career Fellowship(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The authors are grateful for discussions with Simon Lacoste-Julien, Simo Sarkka, Arno Solin, Dino Sejdinovic, Tom Gunter and Mathias Cronjager. FXB was supported by EPSRC [EP/L016710/1]. CJO was supported by EPSRC [EP/D002060/1]. MG was supported by EPSRC [EP/J016934/1], an EPSRC Established Career Fellowship, the EU grant [EU/259348] and a Royal Society Wolfson Research Merit Award.	Bach F., 2012, P 29 INT C INT C MAC, P1355; Bach F., 2015, ARXIV150206800; Chen Y., 2015, J MACHINE LEARNING R; Chen YH, 2010, ADV INTEL SOFT COMPU, V66, P109, DOI 10.1145/1866919.1866935; Conrad Patrick R, 2015, ARXIV150604592; Diaconis P., 1988, STAT DECISION THEORY, V1, P163, DOI DOI 10.1007/978-1-4613-8768-8_20; Dick J., 2010, DIGITAL NETS SEQUENC, DOI 10.1017/CBO9780511761188; DUNN JC, 1980, SIAM J CONTROL OPTIM, V18, P473, DOI 10.1137/0318035; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Garber D, 2015, PR MACH LEARN RES, V37, P541; Gunter T., 2014, ADV NEURAL INFORM PR; Hamrick J.B., 2013, NIPS 2013 WORKSH BAY; Hennig P, 2015, P ROY SOC A-MATH PHY, V471, DOI 10.1098/rspa.2015.0142; Hennig P, 2015, SIAM J OPTIMIZ, V25, P234, DOI 10.1137/140955501; Husz Ferenc, 2012, P 28 C UNCERTAINTY A, P377; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Lacoste-Julien S, 2015, JMLR WORKSH CONF PRO, V38, P544; O'Hagan A., 1984, J ROYAL STAT SOC D, V36, P247; Oates C.J., 2015, ARXIV14102392; Oates CJ, 2014, BIOINFORMATICS, V30, pI468, DOI 10.1093/bioinformatics/btu452; OHAGAN A, 1991, J STAT PLAN INFER, V29, P245, DOI 10.1016/0378-3758(91)90002-V; Osborne Michael A, 2012, ARTIF INTELL, P832; Owen A.B., 2015, NUMER MATH, P1; Rasmussen C., 2003, ADV NEURAL INFORM PR, P489; Sarkka S, 2015, ARXIV150405994; Schober M, 2014, ADV NEURAL INFORM PR, V27, P739; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517	27	21	21	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101009
C	Denton, E; Chintala, S; Szlam, A; Fergus, R		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Denton, Emily; Chintala, Soumith; Szlam, Arthur; Fergus, Rob			Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				FIELDS	In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach [11]. Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.	[Denton, Emily] NYU, Dept Comp Sci, Courant Inst, New York, NY 10003 USA; [Chintala, Soumith; Szlam, Arthur; Fergus, Rob] Facebook AI Res, New York, NY USA	New York University; Facebook Inc	Denton, E (corresponding author), NYU, Dept Comp Sci, Courant Inst, New York, NY 10003 USA.				NSERC Fellowship; FAIR Infrastructure team	NSERC Fellowship(Natural Sciences and Engineering Research Council of Canada (NSERC)); FAIR Infrastructure team	We would like to thank the anonymous reviewers for their insightful and constructive comments. We also thank Andrew Tulloch, Wojciech Zaremba and the FAIR Infrastructure team for useful discussions and support. Emily Denton was supported by an NSERC Fellowship.	[Anonymous], 2015, ARXIV150203167V3; Ba J., 2017, P 3 INT C LEARN REPR; Coates A., 2011, AISTATS; De Benet J. S., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P361, DOI 10.1145/258734.258882; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Denton E.L., DEEP GENERATIVE IMAG; Dosovitskiy A., 2014, ARXIV14115928; Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383; Eslami SMA, 2014, INT J COMPUT VISION, V107, P155, DOI 10.1007/s11263-013-0669-1; Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747; Gauthier Jon, 2014, CLASS PROJECT STANFO; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gregor K., 2015, CORR; Hinton, 2008, ADV NEURAL INFORM PR, P1121; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; Mirza M., 2014, ARXIV PREPRINT ARXIV; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983; Ranzato M., 2010, P 13 INT C ART INT S, P621; Ranzato M, 2013, IEEE T PATTERN ANAL, V35, P2206, DOI 10.1109/TPAMI.2013.29; Rezende D.J., 2014, PROC INT CONFER ENCE; Roth S, 2005, PROC CVPR IEEE, P860; Salakhutdinov R., 2009, P 12 INT C ART INT S, P448; SIMONCELLI EP, 1992, IEEE T INFORM THEORY, V38, P587, DOI 10.1109/18.119725; Sohl-Dickstein J., 2015, CORR; Theis L., 2015, GENERATIVE IMAGE MOD; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470; Zhang Y., 2015, CVPR WORKSH; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420; Zoran Daniel, 2011, ICCV	34	21	21	7	31	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101033
C	Gu, SX; Ghahramani, Z; Turner, RE		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Gu, Shixiang; Ghahramani, Zoubin; Turner, Richard E.			Neural Adaptive Sequential Monte Carlo	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Sequential Monte Carlo (SMC), or particle filtering, is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions. Like other importance sampling-based methods, performance is critically dependent on the proposal distribution: a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution. This paper presents a new method for automatically adapting the proposal using an approximation of the Kullback-Leibler divergence between the true posterior and the proposal distribution. The method is very flexible, applicable to any parameterized proposal distribution and it supports online and batch variants. We use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to Neural Adaptive Sequential Monte Carlo (NASMC). Experiments indicate that NASMC significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the Extended Kalman and Unscented Particle Filters. Experiments also indicate that improved inference translates into improved parameter learning when NASMC is used as a subroutine of Particle Marginal Metropolis Hastings. Finally we show that NASMC is able to train a latent variable recurrent neural network (LV-RNN) achieving results that compete with the state-of-the-art for polymorphic music modelling. NASMC can be seen as bridging the gap between adaptive SMC methods and the recent work in scalable, black-box variational inference.	[Gu, Shixiang; Ghahramani, Zoubin; Turner, Richard E.] Univ Cambridge, Dept Engn, Cambridge, England; [Gu, Shixiang] MPI Intelligent Syst, Tubingen, Germany	University of Cambridge; Max Planck Society	Gu, SX (corresponding author), Univ Cambridge, Dept Engn, Cambridge, England.	sg717@cam.ac.uk; zoubin@eng.cam.ac.uk; ret26@cam.ac.uk			Cambridge-Tubingen Fellowship; ALTA Institute; Jesus College, Cambridge; EPSRC [EP/G050821/1, EP/L000776/1]	Cambridge-Tubingen Fellowship; ALTA Institute; Jesus College, Cambridge; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	SG is generously supported by Cambridge-Tubingen Fellowship, the ALTA Institute, and Jesus College, Cambridge. RET thanks the EPSRC (grants EP/G050821/1 and EP/L000776/1). We thank Theano developers for their toolkit, the authors of [5] for releasing the source code, and Roger Frigola, Sumeet Singh, Fredrik Lindsten, and Thomas Schon for helpful suggestions on experiments.	Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; Bayer J, 2014, ARXIV14117610; Bengio Y, 2013, INT CONF ACOUST SPEE, P8624, DOI 10.1109/ICASSP.2013.6639349; Bishop C.M., 1994, MIXTURE DENSITY NETW; Bornschein J., 2015, ICLR; Boulanger-Lewandowski N., 2012, P 29 INT C MACH LEAR, P1159, DOI DOI 10.32604/CSSE.2021.014030; Cornebise J., 2009, THESIS, P6; Doucet A, 2001, STAT ENG IN, P3; Frigola R., 2014, ADV NEURAL INFORM PR, V27, P3680; Graves A., 2013, P 2013 IEEE INT C AC, P6645, DOI [10.1109/ICASSP.2013.6638947, DOI 10.1109/ICASSP.2013.6638947]; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kingma D.P, P 3 INT C LEARNING R; MacKay D. J., 2003, INFORM THEORY INFERE, V7; McHutchon A., 2014, THESIS; Minka T.P., 2001, P 17 C UNC ART INT, P362; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Poyiadjis G, 2011, BIOMETRIKA, V98, P65, DOI 10.1093/biomet/asq062; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Turner R. E., 2011, BAYESIAN TIME SERIES, P109; van der Merwe R, 2001, ADV NEUR IN, V13, P584	24	21	21	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102109
C	Park, CC; Kim, G		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Park, Cesc Chunseong; Kim, Gunhee			Expressing an Image Stream with a Sequence of Natural Sentences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We propose an approach for retrieving a sequence of natural sentences for an image stream. Since general users often take a series of pictures on their special moments, it would better take into consideration of the whole image stream to produce natural language descriptions. While almost all previous studies have dealt with the relation between a single image and a single natural sentence, our work extends both input and output dimension to a sequence of images and a sequence of sentences. To this end, we design a multimodal architecture called coherent recurrent convolutional network (CRCN), which consists of convolutional neural networks, bidirectional recurrent neural networks, and an entity-based local coherence model. Our approach directly learns from vast user-generated resource of blog posts as text-image parallel training data. We demonstrate that our approach outperforms other state-of-the-art candidate methods, using both quantitative measures (e.g. BLEU and top-K recall) and user studies via Amazon Mechanical Turk.	[Park, Cesc Chunseong; Kim, Gunhee] Seoul Natl Univ, Seoul, South Korea	Seoul National University (SNU)	Park, CC (corresponding author), Seoul Natl Univ, Seoul, South Korea.	park.chunseong@snu.ac.kr; gunheeg@snu.ac.kr			Hancom; National Research Foundation of Korea [2015R1C1A1A02036562]	Hancom; National Research Foundation of Korea(National Research Foundation of Korea)	This research is partially supported by Hancom and Basic Science Research Program through National Research Foundation of Korea (2015R1C1A1A02036562).	Barzilay R., 2008, ACL; Bird S., 2009, NATURAL LANGUAGE PRO; Chen X., 2015, P IEEE INT C COMP VI; Choi F., 2001, EMNLP; Donahue J., 2015, CVPR; Gong Y., 2014, ECCV; He K., 2015, IEEE ICCV; Karpathy A., 2015, CVPR; Kim Gunhee, 2015, CVPR; Kiros R., 2014, ICML; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Kulkarni G., 2011, P 24 CVPR CIT; Kuznetsova P., 2014, TACL; Lavie S. B. A., 2005, ACL; Le Q., 2014, ICML; Manning C. D., 2014, ACL; Mao J, 2015, 3 INT C LEARN REPR I; Mikolov T.A., 2012, STAT LANGUAGE MODELS; Ordonez V., 2011, NIPS; Papineni K., 2002, P ANN M ASS COMP LIN; Rohrbach M., 2013, ICCV; Schuster M., 1997, IEEE TSP; Simonyan Karen, 2015, INT C LEARN REPR; Socher R., 2013, TACL; Srivastava N., 2012, NIPS; Tieleman T., 2012, COURSERA; Vedantam R., 2014, ARXIV14115726; Vinyals O., 2015, CVPR; WERBOS PJ, 1988, NEURAL NETWORKS, V1, P339, DOI 10.1016/0893-6080(88)90007-X; Xu R., 2015, AAAI	31	21	21	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101036
C	Carpentier, A; Valko, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Carpentier, Alexandra; Valko, Michal			Extreme bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In many areas of medicine, security, and life sciences, we want to allocate limited resources to different sources in order to detect extreme values. In this paper, we study an efficient way to allocate these resources sequentially under limited feedback. While sequential design of experiments is well studied in bandit theory, the most commonly optimized property is the regret with respect to the maximum mean reward. However, in other problems such as network intrusion detection, we are interested in detecting the most extreme value output by the sources. Therefore, in our work we study extreme regret which measures the efficiency of an algorithm compared to the oracle policy selecting the source with the heaviest tail. We propose the EXTREMEHUNTER algorithm, provide its analysis, and evaluate it empirically on synthetic and real-world experiments.	[Carpentier, Alexandra] Univ Cambridge, CMS, Stat Lab, Cambridge, England; [Valko, Michal] INRIA Lille Nord Europe, SequeL Team, Lille, France	University of Cambridge	Carpentier, A (corresponding author), Univ Cambridge, CMS, Stat Lab, Cambridge, England.	a.carpentier@statslab.cam.ac.uk; michal.valko@inria.fr			Intel Corporation; French Ministry of Higher Education and Research; European Community's Seventh Framework Programme (FP7/2007-2013) [270327]	Intel Corporation(Intel Corporation); French Ministry of Higher Education and Research; European Community's Seventh Framework Programme (FP7/2007-2013)(European Commission)	We would like to thank John Mark Agosta and Jennifer Healey for the network traffic data. The research presented in this paper was supported by Intel Corporation, by French Ministry of Higher Education and Research, and by European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement no 270327 (CompLACS).	Abe N., 2006, P 12 ACM SIGKDD INT, P504, DOI [10.1145/1150402.1150459, DOI 10.1145/1150402.1150459]; Agosta John Mark, IEEE P INFOCOM, P225; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Carpentier Alexandra, 2014, STAT SINICA; Carpentier Alexandra, 2014, ELECT J STAT; Cicirello Vincent A., 2005, AAAI C ART INT; de Haan L., 2006, SPRING S OPERAT RES, DOI 10.1007/0-387-34471-3; Fisher RA, 1928, P CAMB PHILOS SOC, V24, P180, DOI 10.1017/S0305004100015681; Gnedenko B, 1943, ANN MATH, V44, P423, DOI 10.2307/1968974; HALL P, 1984, ANN STAT, V12, P1079, DOI 10.1214/aos/1176346723; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Liu KQ, 2012, IEEE INT SYMP INFO; Markou M, 2003, SIGNAL PROCESS, V83, P2481, DOI 10.1016/j.sigpro.2003.07.018; Neill DB, 2010, MACH LEARN, V79, P261, DOI 10.1007/s10994-009-5144-4; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Priebe C. Y., 2005, Computational & Mathematical Organization Theory, V11, P229, DOI 10.1007/s10588-005-5378-z; Steinwart I, 2005, J MACH LEARN RES, V6, P211; Streeter MJ, 2006, LECT NOTES COMPUT SC, V4204, P560; Streeter Matthew J., 2006, AAAI, P135; Turner Ryan, 2010, IEEE WORKSH MACH LEA	22	21	21	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101050
C	Kantchelian, A; Tschantz, MC; Huang, L; Bartlett, PL; Joseph, AD; Tygar, JD		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Kantchelian, Alex; Tschantz, Michael Carl; Huang, Ling; Bartlett, Peter L.; Joseph, Anthony D.; Tygar, J. D.			Large-Margin Convex Polytope Machine	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We present the Convex Polytope Machine (CPM), a novel non-linear learning algorithm for large-scale binary classification tasks. The CPM finds a large margin convex polytope separator which encloses one class. We develop a stochastic gradient descent based algorithm that is amenable to massive datasets, and augment it with a heuristic procedure to avoid sub-optimal local minima. Our experimental evaluations of the CPM on large-scale datasets from distinct domains (MNIST handwritten digit recognition, text topic, and web security) demonstrate that the CPM trains models faster, sometimes several orders of magnitude, than state-of-the-art similar approaches and kernel-SVM methods while achieving comparable or better classification performance. Our empirical results suggest that, unlike prior similar approaches, we do not need to control the number of sub-classifiers (sides of the polytope) to avoid overfitting.	[Kantchelian, Alex; Tschantz, Michael Carl; Bartlett, Peter L.; Joseph, Anthony D.; Tygar, J. D.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Huang, Ling] Datavisor, Mountain View, CA USA	University of California System; University of California Berkeley	Kantchelian, A (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	akant@cs.berkeley.edu; mct@cs.berkeley.edu; ling.huang@datavisor.com; bartlett@cs.berkeley.edu; adj@cs.berkeley.edu; tygar@cs.berkeley.edu						Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Beygelzimer A., 2005, AAAI, P720; Beygelzimer A., 2009, P 25 C UNC ART INT, P51; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Burges C., 1998, MNIST DATASET; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Dietterich T. G., 1995, Journal of Artificial Intelligence Research, V2, P263; Fischer P., 1995, Proceedings of the Eighth Annual Conference on Computational Learning Theory, P337, DOI 10.1145/225298.225339; Hinton G.E., 2012, NEURAL NETWORKS TRIC, P599, DOI 10.1007/978-3-642-35289-8_32; Loosli Gaelle, 2007, LARGE SCALE KERNEL M, V6; Ma J, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1245; Manwani Naresh, 2013, ARXIV11071564; Manwani Naresh, 2010, P 2 AS C MACH LEARN; SHALEV- SHWARTZ S., 2007, P 24 INT C MACH LEAR, P807, DOI [DOI 10.1145/1273496.1273598, 10.1145/1273496.1273598]; Takacs Gabor, 2010, ACTA TECHNICA JAURIN, V3; Wang Zhuang, 2011, P 17 ACM SIGKDD INT; Zhu ZA, 2009, IEEE DATA MINING, P677, DOI 10.1109/ICDM.2009.29	19	21	21	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102072
C	Ontrup, J; Ritter, H		Dietterich, TG; Becker, S; Ghahramani, Z		Ontrup, J; Ritter, H			Hyperbolic self-organizing maps for semantic navigation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				TEXT	We introduce a new type of Self-Organizing Map (SOM) to navigate in the Semantic Space of large text collections. We propose a "hyperbolic SOM" (HSOM) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. The exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relations. We describe experiments, showing that the HSOM can successfully be applied to text categorization tasks and yields results comparable to other state-of-the-art methods.	Univ Bielefeld, Fac Technol, Neuroinformat Grp, D-33501 Bielefeld, Germany	University of Bielefeld	Ontrup, J (corresponding author), Univ Bielefeld, Fac Technol, Neuroinformat Grp, D-33501 Bielefeld, Germany.	jontrup@techfak.uni-bielefeld.de; helge@techfak.uni-bielefeld.de						COXETER HSM, 1957, NONEUCLIDEAN GEOMETR; Fricke R, 1897, VORLESUNGEN THEORIE, V1; JAOCHIMS T, 1998, P ECML 98, P137; Kohonen T, 2000, IEEE T NEURAL NETWOR, V11, P574, DOI 10.1109/72.846729; Kohonen T., 2001, SPRINGER SERIES INFO; Lodhi H, 2001, ADV NEUR IN, V13, P563; Magnus W., 1974, NONEUCLIDEAN TESSELA; Misner C. W., 1973, GRAVITATION; Morgan F, 1993, RIEMANNIAN GEOMETRY; Munzner T, 1998, IEEE COMPUT GRAPH, V18, P18, DOI 10.1109/38.689657; ONTRUP J, 2001, P PKDD01; Ritter H, 1999, KOHONEN MAPS, P97, DOI 10.1016/B978-044450270-4/50007-3; SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0; Sebastiani F., 2000, Proceedings of the Ninth International Conference on Information and Knowledge Management. CIKM 2000, P78, DOI 10.1145/354756.354804; Thorpe J. A., 1979, ELEMENTARY TOPICS DI; Yiming Yang, 1999, Information Retrieval, V1, P69, DOI 10.1023/A:1009982220290; [No title captured]	17	21	21	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1417	1424						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100176
C	Becker, S; Burgess, N		Leen, TK; Dietterich, TG; Tresp, V		Becker, S; Burgess, N			Modelling spatial recall, mental imagery and neglect	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				POSTERIOR PARIETAL NEURONS; UNILATERAL NEGLECT; ENTORHINAL CORTEX; PLACE FIELDS; NETWORK; CELLS; HIPPOCAMPUS; MEMORY	We present a computational model of the neural mechanisms in the parietal and temporal lobes that support spatial navigation, recall of scenes and imagery of the products of recall. Long term representations are stored in the hippocampus, and are associated with local spatial and object-related features in the parahippocampal region. Viewer-centered representations are dynamically generated from long term memory in the parietal part of the model. The model thereby simulates recall and imagery of locations and objects in complex environments. After parietal damage, the model exhibits hemispatial neglect in mental imagery that rotates with the imagined perspective of the observer, as in the famous Milan Square experiment [1]. Our model makes novel predictions for the neural representations in the parahippocampal and parietal regions and for behavior in healthy volunteers and neuropsychological patients.	McMaster Univ, Dept Psychol, Hamilton, ON L8S 4K1, Canada	McMaster University	Becker, S (corresponding author), McMaster Univ, Dept Psychol, 1280 Main St West, Hamilton, ON L8S 4K1, Canada.		Burgess, Neil/B-2420-2009	Burgess, Neil/0000-0003-0646-6584; Becker, Suzanna/0000-0002-2645-070X				ABBOTT LF, 1995, INT J NEUR SYS, V6, P115; ANDERSEN RA, 1985, SCIENCE, V230, P456, DOI 10.1126/science.4048942; BISIACH E, 1978, CORTEX, V14, P129, DOI 10.1016/S0010-9452(78)80016-1; Bostock E, 1991, Hippocampus, V1, P193, DOI 10.1002/hipo.450010207; BURGESS N, UNPUB; BURGESS N, 1999, HIPPOCAMPAL PARIETAL; Deneve S, 1999, NAT NEUROSCI, V2, P740, DOI 10.1038/11205; FLETCHER PC, 1995, NEUROIMAGE, V2, P195, DOI 10.1006/nimg.1995.1025; Guariglia C, 1998, CORTEX, V34, P233, DOI 10.1016/S0010-9452(08)70750-0; GUARIGLIA C, 1993, NATURE, V364, P235, DOI 10.1038/364235a0; Hartley T, 2000, HIPPOCAMPUS, V10, P369, DOI 10.1002/1098-1063(2000)10:4<369::AID-HIPO3>3.0.CO;2-0; LEVER C, 1999, SOC NEUR ABS, V24; Maguire EA, 1998, SCIENCE, V280, P921, DOI 10.1126/science.280.5365.921; MILNER AD, 1999, HIPPOCAMPAL PARIETAL; MULLER RU, 1987, J NEUROSCI, V7, P1951; OKeefe J, 1996, NATURE, V381, P425, DOI 10.1038/381425a0; OKEEFE J, 1976, EXP NEUROL, V51, P78, DOI 10.1016/0014-4886(76)90055-8; Pouget A, 1997, J COGNITIVE NEUROSCI, V9, P222, DOI 10.1162/jocn.1997.9.2.222; QUIRK GJ, 1992, J NEUROSCI, V12, P1945; SALINAS E, 1995, J NEUROSCI, V15, P6461; Samsonovich A, 1997, J NEUROSCI, V17, P5900; Snyder LH, 1997, NATURE, V386, P167, DOI 10.1038/386167a0; Suzuki WA, 1997, J NEUROPHYSIOL, V78, P1062, DOI 10.1152/jn.1997.78.2.1062; SUZUKI WA, 1994, J NEUROSCI, V14, P1856, DOI 10.1523/JNEUROSCI.14-03-01856.1994; Taube JS, 1998, PROG NEUROBIOL, V55, P225, DOI 10.1016/S0301-0082(98)00004-5; ZIPSER D, 1988, NATURE, V331, P679, DOI 10.1038/331679a0; [No title captured]	27	21	21	0	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						96	102						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800014
C	Coughlan, JM; Yuille, AL		Leen, TK; Dietterich, TG; Tresp, V		Coughlan, JM; Yuille, AL			The Manhattan world assumption: Regularities in scene statistics which enable Bayesian inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Preliminary work by the authors made use of the so-called "Manhattan world" assumption about the scene statistics of city and indoor scenes. This assumption stated that such scenes were built on a cartesian grid which led to regularities in the image edge gradient statistics. In this paper we explore the general applicability of this assumption and show that, surprisingly, it holds in a large variety of less structured environments including rural scenes. This enables us, from a single image, to determine the orientation of the viewer relative to the scene structure and also to detect target objects which are not aligned with the grid. These inferences are performed using a Bayesian model with probability distributions (e.g. on the image gradient statistics) learnt from real data.	Smith Kettlewell Eye Res Inst, San Francisco, CA 94115 USA	The Smith-Kettlewell Eye Research Institute	Coughlan, JM (corresponding author), Smith Kettlewell Eye Res Inst, 2318 Fillmore St, San Francisco, CA 94115 USA.			Yuille, Alan L./0000-0001-5207-9249				BRILLAULTOMAHONY B, 1991, CVGIP-IMAG UNDERSTAN, V54, P289, DOI 10.1016/1049-9660(91)90069-2; COUGHLAN J, 1999, P INT C COMP VIS ICC; COUGHLAN J, 2000, UNPUB INT J COMPUTER; HUANG J, 1999, P COMP VIS PATT REC; KONISHI S, 1999, P INT C COMP VIS PAT; LUTTON E, 1994, IEEE T PATTERN ANAL, V16, P430, DOI 10.1109/34.277598; ZHU SC, 1998, P WORKSH DET CLASS D	7	21	21	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						845	851						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800119
C	Zhang, T		Leen, TK; Dietterich, TG; Tresp, V		Zhang, T			Regularized Winnow methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				PERCEPTRON ALGORITHM; NETWORKS	In theory, the Winnow multiplicative update has certain advantages over the Perceptron additive update when there are many irrelevant attributes. Recently, there has been much effort on enhancing the Perceptron algorithm by using regularization, leading to a class of linear classification methods called support vector machines. Similarly, it is also possible to apply the regularization idea to the Winnow algorithm, which gives methods we call regularized Winnows. We show that the resulting methods compare with the basic Winnows in a similar way that a support vector machine compares with the Perceptron. We investigate algorithmic issues and learning properties of the derived methods. Some experimental results will also be provided to illustrate different methods.	IBM Corp, Thomas J Watson Res Ctr, Dept Math Sci, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Zhang, T (corresponding author), IBM Corp, Thomas J Watson Res Ctr, Dept Math Sci, Yorktown Hts, NY 10598 USA.							ANLAUF JK, 1989, EUROPHYS LETT, V10, P687, DOI 10.1209/0295-5075/10/7/014; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; DAGAN I, 1997, P 2 C EMP METH NLP; GROVE A, NIPS 10; GROVE A, 2000, IN PRESS MACHINE LEA; Grove A. J., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P171, DOI 10.1145/267460.267493; Jaakkola T, 2000, ADV NEUR IN, V12, P470; JAAKKOLA TS, IN PRESS J COMPUTATI; KINZEL W, 1990, LECT NOTES PHYS, V368, P175; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914; OPPER M, 1988, PHYS REV A, V38, P3824, DOI 10.1103/PhysRevA.38.3824; Rosenblatt F., 1961, PRINCIPLES NEURODYNA, DOI 10.21236/AD0256582; Scholkopf B., 1999, ADV KERNEL METHODS S; Vapnik V.N, 1998, STAT LEARNING THEORY; ZHANG T, 1999, NIPS 99, P370	17	21	22	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						703	709						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800100
C	Dietterich, TG		Solla, SA; Leen, TK; Muller, KR		Dietterich, TG			State abstraction in MAXQ hierarchical reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Many researchers have explored methods for hierarchical reinforcement learning (RL) with temporal abstractions, in which abstract actions are defined that can perform many primitive actions before terminating. However, little is known about learning with state abstractions, in which aspects of the state space are ignored. In previous work, we developed the MAXQ method for hierarchical RL. In this paper, we define five conditions under which state abstraction can be combined with the MAXQ value function decomposition. We prove that the MAXQ-Q learning algorithm converges under these conditions and show experimentally that state abstraction is important for the successful application of MAXQ-Q learning.	Oregon State Univ, Dept Comp Sci, Corvallis, OR 97331 USA	Oregon State University	Dietterich, TG (corresponding author), Oregon State Univ, Dept Comp Sci, Corvallis, OR 97331 USA.							BOUTILIER C, 1995, P 14 INT JOINT C ART, P1104; DAYAN P, 1993, NIPS 5, P271; DIETTERICH TG, 1998, ICML 15; Hauskrecht M., 1998, HIERARCHICAL SOLUTIO; Kaelbling LP., 1993, P 10 INT C MACHINE L, P951; PARR R, 1998, NIPS 10; PRECUP D, 1998, NIPS10; SINGH S, 1998, CONVERGENCE RESULTS; SINGH SP, 1992, MACH LEARN, V8, P323, DOI 10.1007/BF00992700; SUTTON R, 1998, MDPS SEMIMDPS LEARNI; Tsitsiklis J, 1996, NEURO DYNAMIC PROGRA	12	21	22	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						994	1000						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700140
C	Singer, Y; Warmuth, MK		Kearns, MS; Solla, SA; Cohn, DA		Singer, Y; Warmuth, MK			Batch and on-line parameter estimation of Gaussian mixtures based on the joint entropy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				EM ALGORITHM	We describe a new iterative method for parameter estimation of Gaussian mixtures. The new method is based on a framework developed by Kivinen and Warmuth for supervised on-line learning. In contrast to gradient descent and EM, which estimate the mixture's covariance matrices, the proposed method estimates the inverses of the covariance matrices. Furthermore, the new parameter estimation procedure can be applied in both on-line and batch settings. We show experimentally that it is typically faster than EM, and usually requires about half as many iterations as EM.										Bauer Eric, 1997, P 13 C UNC ART INT P, P3; Bishop, 1995, NEURAL NETWORKS PATT; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Duda R.O., 1973, J ROYAL STAT SOC SER; HELMBOLD DP, 1995, ADV NEURAL INFORMATI, V7, P309; HELMBOLD DP, 1997, MACH LEARNING, V7; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; KIVINEN J, 1997, ADV NEURAL INFORMATI, V10; Redner R., 1984, SIAM REV, V26; THOMAS M, 1991, ELEMENTS INFORMATION; TITTERINGTON DM, 1985, STAT ANAL FINITE MIX; WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060	12	21	21	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						578	584						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700082
C	Vovk, V		Jordan, MI; Kearns, MJ; Solla, SA		Vovk, V			Competitive on-line linear regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We apply a general algorithm for merging prediction strategies (the Aggregating Algorithm) to the problem of linear regression with the square loss; our main assumption is that the response variable is bounded. It turns out that for this particular problem the Aggregating Algorithm resembles, but is slightly different from, the well-known ridge estimation procedure. From general results about the Aggregating Algorithm we deduce a guaranteed bound on the difference between our algorithm's performance and the best, in some sense, linear regression function's performance. We show that the AA attains the optimal constant in our bound, whereas the constant attained by the ridge regression procedure in general can be 4 times worse.	Univ London Royal Holloway & Bedford New Coll, Dept Comp Sci, Egham TW20 0EX, Surrey, England	University of London; Royal Holloway University London	Vovk, V (corresponding author), Univ London Royal Holloway & Bedford New Coll, Dept Comp Sci, Egham TW20 0EX, Surrey, England.	vovk@dcs.rhbnc.ac.uk							0	21	21	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						364	370						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700052
C	Baluja, S		Mozer, MC; Jordan, MI; Petsche, T		Baluja, S			Genetic algorithms and explicit search statistics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The genetic algorithm (GA) is a heuristic search procedure based on mechanisms abstracted from population genetics. In a previous paper [Baluja & Caruana, 1995], we showed that much simpler algorithms, such as hillclimbing and Population-Based Incremental Learning (PBIL), perform comparably to GAs on an optimization problem custom designed to benefit from the GA's operators. This paper extends these results in two directions. First, in a large-scale empirical comparison of problems that have been reported in GA literature, we show that on many problems, simpler algorithms can perform significantly better than GAs. Second, we describe when crossover is useful, and show how it can be incorporated into PBIL.			Baluja, S (corresponding author), CARNEGIE MELLON UNIV,JUSTSYST PITTSBURGH RES CTR,PITTSBURGH,PA 15213, USA.								0	21	21	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						319	325						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00045
C	Merz, CJ; Pazzani, MJ		Mozer, MC; Jordan, MI; Petsche, T		Merz, CJ; Pazzani, MJ			Combining neural network regression estimates with regularized linear weights	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				When combining a set of learned models to form an improved estimator, the issue of redundancy or multicollinearity in the set of models must be addressed. A progression of existing approaches and their limitations with respect to the redundancy is discussed. A new approach, PCR*, based on principal components regression is proposed to address these limitations. An evaluation of the new approach on a collection of domains reveals that: 1) PCR* was the most robust combination method as the redundancy of the learned models increased, 2) redundancy could be handled without eliminating any of the learned models, and 3) the principal components of the learned models provided a continuum of ''regularized'' weights from which PCR* could choose.			Merz, CJ (corresponding author), UNIV CALIF IRVINE,DEPT INFORMAT & COMP SCI,IRVINE,CA 92717, USA.			Pazzani, Michael/0000-0002-4240-7349					0	21	21	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						564	570						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00080
C	Moody, JE; Rognvaldsson, TS		Mozer, MC; Jordan, MI; Petsche, T		Moody, JE; Rognvaldsson, TS			Smoothing regularizers for projective basis function networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Smoothing regularizers for radial basis functions have been studied extensively, but no general smoothing regularizers for projective basis functions (PBFs), such as the widely-used sigmoidal PBFs, have heretofore been proposed. We derive new classes of algebraically-simple m(th)-order smoothing regularizers for networks of the form f(W, x) = Sigma(j=1)(N) u(j)g [x(T)v(j) + v(j)0] + u(0), with general projective basis functions g[.]. These regularizers are: [GRAPHICS] These regularizers bound the corresponding m(th)-order smoothing integral [GRAPHICS] where W denotes all the network weights {u(j), u(0), v(j), v(0)}, and Omega(x) is a weighting function on the D-dimensional input space. The global and local cases are distinguished by different choices of Omega(x). The simple algebraic forms R(W, m) enable the direct enforcement of smoothness without the need for costly Monte-Carlo integrations of S(W, m). The new regularizers are shown to yield better generalization errors than weight decay when the implicit assumptions in the latter are wrong. Unlike weight decay, the new regularizers distinguish between the roles of the input and output weights and capture the interactions between them.			Moody, JE (corresponding author), OREGON GRAD INST,DEPT COMP SCI,POB 91000,PORTLAND,OR 97291, USA.		Rögnvaldsson, Thorsteinn/N-3106-2013	Rögnvaldsson, Thorsteinn/0000-0001-5163-2997					0	21	21	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						585	591						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00083
C	Yaeger, L; Lyon, R; Webb, B		Mozer, MC; Jordan, MI; Petsche, T		Yaeger, L; Lyon, R; Webb, B			Effective training of a neural network character classifier for word recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We have combined an artificial neural network (ANN) character classifier with context-driven search over character segmentation, word segmentation, and word recognition hypotheses to provide robust recognition of hand-printed English text in new models of Apple Computer's Newton MessagePad. We present some innovations in the training and use of ANNs as character classifiers for word recognition, including normalized output error, frequency balancing, error emphasis, negative training, and stroke warping. A recurring theme of reducing a priori biases emerges and is discussed.			Yaeger, L (corresponding author), APPLE COMP INC,5540 BITTERSWEET RD,MORGANTOWN,IN 46160, USA.			Lyon, Richard/0000-0003-2348-811X					0	21	22	1	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						807	813						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00114
C	CONNOR, J; ATLAS, LE; MARTIN, DR		MOODY, JE; HANSON, SJ; LIPPMANN, RP		CONNOR, J; ATLAS, LE; MARTIN, DR			RECURRENT NETWORKS AND NARMA MODELING	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	21	21	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						301	308						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00037
C	MONTANA, D		MOODY, JE; HANSON, SJ; LIPPMANN, RP		MONTANA, D			A WEIGHTED PROBABILISTIC NEURAL NETWORK	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	21	21	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1110	1117						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00137
C	Chen, HG; Zhang, H; Si, S; Li, Y; Boning, DN; Hsieh, CJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Hongge; Zhang, Huan; Si, Si; Li, Yang; Boning, Duane; Hsieh, Cho-Jui			Robustness Verification of Tree-based Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the robustness verification problem for tree based models, including decision trees, random forests (RFs) and gradient boosted decision trees (GBDTs). Formal robustness verification of decision tree ensembles involves finding the exact minimal adversarial perturbation or a guaranteed lower bound of it. Existing approaches find the minimal adversarial perturbation by a mixed integer linear programming (MILP) problem, which takes exponential time so is impractical for large ensembles. Although this verification problem is NP-complete in general, we give a more precise complexity characterization. We show that there is a simple linear time algorithm for verifying a single tree, and for tree ensembles the verification problem can be cast as a max-clique problem on a multi-partite graph with bounded boxicity. For low dimensional problems when boxicity can be viewed as constant, this reformulation leads to a polynomial time algorithm. For general problems, by exploiting the boxicity of the graph, we develop an efficient multi-level verification algorithm that can give tight lower bounds on robustness of decision tree ensembles, while allowing iterative improvement and any-time termination. On RF/GBDT models trained on 10 datasets, our algorithm is hundreds of times faster than a previous approach that requires solving MILPs, and is able to give tight robustness verification bounds on large GBDTs with hundreds of deep trees.	[Chen, Hongge; Boning, Duane] MIT, Dept EECS, Cambridge, MA 02139 USA; [Zhang, Huan; Hsieh, Cho-Jui] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA; [Si, Si; Li, Yang; Hsieh, Cho-Jui] Google Res, Mountain View, CA USA	Massachusetts Institute of Technology (MIT); University of California System; University of California Los Angeles; Google Incorporated	Chen, HG (corresponding author), MIT, Dept EECS, Cambridge, MA 02139 USA.; Zhang, H (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA.	chenhg@mit.edu; huan@huan-zhang.com; sisidaisy@google.com; liyang@google.com; boning@mtl.mit.edu; chohsieh@cs.ucla.edu			SenseTime; NSF [IIS-1719097]; Intel faculty award	SenseTime; NSF(National Science Foundation (NSF)); Intel faculty award	Chen and Boning acknowledge the support of SenseTime. Hsieh acknowledges the support of NSF IIS-1719097 and Intel faculty award.	Athalye A, 2018, PR MACH LEARN RES, V80; Bastani O, 2018, ADV NEUR IN, V31; Brendel W., 2018, PROC 6 INT C LEARN R; BRON C, 1973, COMMUN ACM, V16, P575, DOI 10.1145/362342.362367; Bunel R, 2018, ADV NEUR IN, V31; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chandran LS, 2010, ALGORITHMICA, V56, P129, DOI 10.1007/s00453-008-9163-5; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chen Hongge, 2019, ICML; Chen P.-Y., 2017, P AISEC CCS 2017, P15, DOI DOI 10.1145/3128572.3140448; Chen PY, 2018, AAAI CONF ARTIF INTE, P10; Chen T., 2016, KDD16 P 22 ACM, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]; Cheng M., 2019, P ICLR; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Einziger Gil, 2019, AAAI; Eykholt Kevin, 2017, ARXIV PREPRINT ARXIV; Gehr T, 2018, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2018.00058; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Ilyas A, 2018, PR MACH LEARN RES, V80; Julian Kyle D, 2019, ARXIV190300762; Kantchelian A, 2016, PR MACH LEARN RES, V48; Katz G, 2017, LECT NOTES COMPUT SC, V10426, P97, DOI 10.1007/978-3-319-63387-9_5; Ke G., 2017, P ADV NEURAL INFORM, P3146; Kurakin A, 2016, INT C LEARN REPR SAN; Liu Yanpei, 2016, ARXIV161102770; Madry Aleksander, 2017, ARXIV; Mirghorbani M, 2013, OPTIM LETT, V7, P1155, DOI 10.1007/s11590-012-0536-y; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Phillips CA, 2019, ALGORITHMS, V12, DOI 10.3390/a12010023; Ribeiro MT, 2018, AAAI CONF ARTIF INTE, P1527; Roberts Fred S, 1969, RECENT PROGR COMBINA, P301; Salman H, 2019, ADV NEUR IN, V32; Sato Naoto, 2019, ARXIV190411753; Schneider Markus, 2002, CLIQUES K PARTITE GR; Singh G, 2019, P ACM PROGRAM LANG, V3, DOI 10.1145/3290354; Singh G, 2018, ADV NEUR IN, V31; T6rnblom John, 2019, ARXIV190504194; Tjeng V., 2017, INT C LEARN REPR; Uesato J, 2018, PR MACH LEARN RES, V80; Wang S., 2018, ARXIV181102625; Weng TW, 2018, PR MACH LEARN RES, V80; Wong E, 2018, ADV NEUR IN, V31; Wong E, 2018, PR MACH LEARN RES, V80; Xu K., 2019, ICLR, P1, DOI DOI 10.1109/VTCFALL.2019.8891597; Zhang H, 2018, CAN CON EL COMP EN; Zhang H, 2018, ADV NEUR IN, V31	47	20	21	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904003
C	de Witt, CAS; Foerster, JN; Farquhar, G; Torr, PHS; Bohmer, W; Whiteson, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		de Witt, Christian A. Schroeder; Foerster, Jakob N.; Farquhar, Gregory; Torr, Philip H. S.; Boehmer, Wendelin; Whiteson, Shimon			Multi-Agent Common Knowledge Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COORDINATION	Cooperative multi-agent reinforcement learning often requires decentralised policies, which severely limit the agents' ability to coordinate their behaviour. In this paper, we show that common knowledge between agents allows for complex decentralised coordination. Common knowledge arises naturally in a large number of decentralised cooperative multi-agent tasks, for example, when agents can reconstruct parts of each others' observations. Since agents can independently agree on their common knowledge, they can execute complex coordinated policies that condition on this knowledge in a fully decentralised fashion. We propose multiagent common knowledge reinforcement learning (MACKRL), a novel stochastic actor-critic algorithm that learns a hierarchical policy tree. Higher levels in the hierarchy coordinate groups of agents by conditioning on their common knowledge, or delegate to lower levels with smaller subgroups but potentially richer common knowledge. The entire policy tree can be executed in a fully decentralised fashion. As the lowest policy tree level consists of independent policies for each agent, MACKRL reduces to independently learnt decentralised policies as a special case. We demonstrate that our method can exploit common knowledge for superior performance on complex decentralised coordination tasks, including a stochastic matrix game and challenging problems in StarCraft II unit micromanagement.	[de Witt, Christian A. Schroeder; Foerster, Jakob N.; Farquhar, Gregory; Torr, Philip H. S.; Boehmer, Wendelin; Whiteson, Shimon] Univ Oxford, Oxford, England	University of Oxford	de Witt, CAS (corresponding author), Univ Oxford, Oxford, England.	cs@robots.ox.ac.uk			European Research Council (ERC) under the European Union [637713]; National Institutes of Health [R01GM114311]; EPSRC/MURI [EP/N019474/1]; project Free the Drones (FreeD) under the Innovation Fund Denmark; Microsoft; Oxford-Google DeepMind Graduate Scholarship; NVIDIA; JP Morgan Chase Faculty Research Award	European Research Council (ERC) under the European Union(European Research Council (ERC)); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); EPSRC/MURI(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); project Free the Drones (FreeD) under the Innovation Fund Denmark; Microsoft(Microsoft); Oxford-Google DeepMind Graduate Scholarship(Google Incorporated); NVIDIA; JP Morgan Chase Faculty Research Award	We would like to thank Chia-Man Hung, Tim Rudner, Jelena Luketina, and Tabish Rashid for valuable discussions. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement number 637713), the National Institutes of Health (grant agreement number R01GM114311), EPSRC/MURI grant EP/N019474/1 and the JP Morgan Chase Faculty Research Award. This work is linked to and partly funded by the project Free the Drones (FreeD) under the Innovation Fund Denmark and Microsoft. It was also supported by the Oxford-Google DeepMind Graduate Scholarship and a generous equipment grant from NVIDIA.	Amato C, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1273; [Anonymous], 2017, ARXIV170602275; [Anonymous], 2017, ABS170804782 CORR; Aumann Robert, 1974, J MATH ECON, V1, P67, DOI 10.1016/0304-4068(74)90037-8; Bavarian M, 2016, OPTIMALITY CORRELATE; Boutilier C, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P478; Brafman RI, 2003, J ARTIF INTELL RES, V19, P11, DOI 10.1613/jair.1154; Busoniu L, 2008, IEEE T SYST MAN CY C, V38, P156, DOI 10.1109/TSMCC.2007.913919; Cao YC, 2013, IEEE T IND INFORM, V9, P427, DOI 10.1109/TII.2012.2219061; Cigler L, 2013, J ARTIF INTELL RES, V47, P441, DOI 10.1613/jair.3904; Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639; Foerster J., 2017, P 34 INT C MACH LEAR; Foerster J., 2018, AAAI; Foerster J. N., 2016, P ADV NEUR INF PROC, P2137; Genter K, 2017, AUTON AGENT MULTI-AG, V31, P790, DOI 10.1007/s10458-016-9353-5; Guestrin C, 2002, ADV NEUR IN, V14, P1523; Guestrin C., 2002, AAAI IAAI; Gupta J. K., 2017, COOPERATIVE MULTIAGE; Halpern J. Y., 2000, ARXIVCS0006009; Heider F, 1944, AM J PSYCHOL, V57, P243, DOI 10.2307/1416950; Holenstein T, 2007, ACM S THEORY COMPUT, P411, DOI 10.1145/1250790.1250852; Jorge E., 2016, ARXIV161103218; Kok J. R., 2004, P 21 INT C MACH LEAR, P61; Konda V., 1999, NIPS; Korkmaz G, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P253; Kraemer L, 2016, NEUROCOMPUTING, V190, P82, DOI 10.1016/j.neucom.2016.01.031; Krasucki P., 1991, PROBABILISTIC KNOWLE, P1; Kumar S., 2017, ARXIV171208266; Liu M., 2017, ARXIV170707399; Makar F., 2001, Proceedings of the Fifth International Conference on Autonomous Agents, P246, DOI 10.1145/375735.376302; Nayyar A., 2013, DECENTRALIZED STOCHA; Oh J., 2008, AAMAS; OLIEHOEK FA, 2008, OPTIMAL APPROXIMATE, V32, P289; Omidshafiei S., 2017, ARXIV170306182; Osborne MJ, 1994, COURSE GAME THEORY; Peng P., 2017, ARXIV170310069; Pham H. X., 2018, ARXIV180307250CS; Rashid T., 2018, P 35 INT C MACH LEAR; Rasouli A., 2017, ARXIV170203555CS; RUBINSTEIN A, 1989, AM ECON REV, V79, P385; Samvelyan M., 2019, ABS190204043 CORR; Sukhbaatar S., 2016, ADV NEURAL INF PROCE, V29, P2244, DOI DOI 10.5555/3157096.3157348; Sunehag P., 2017, ARXIV170605296; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Synnaeve G., 2016, ARXIV161100625; Tan M., 1993, P 10 INT C MACHINE L, P330, DOI DOI 10.1016/B978-1-55860-307-3.50049-6; Thomas KA, 2014, J PERS SOC PSYCHOL, V107, P657, DOI 10.1037/a0037037; Tian Z., 2018, ARXIV181004444CS; Tobin J., 2017, ARXIV170306907CS; Tremblay J., 2018, ARXIV180406516CS; Usunier N., 2017, P INT C LEARN REPR I; Vezhnevets A. S., 2017, ARXIV170301161CS; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xu Z, 2018, IEEE INT CONF CON AU, P306, DOI 10.1109/ICCA.2018.8444355; Yang E., 2004, MULTIAGENT REINFORCE	57	20	20	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901054
C	Guo, X; Hu, AR; Xu, RY; Zhang, JZ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Guo, Xin; Hu, Anran; Xu, Renyuan; Zhang, Junzi			Learning Mean-Field Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				EQUILIBRIA	This paper presents a general mean-field game (GMFG) framework for simultaneous learning and decision-making in stochastic games with a large population. It first establishes the existence of a unique Nash Equilibrium to this GMFG, and explains that naively combining Q-learning with the fixed-point approach in classical MFGs yields unstable algorithms. It then proposes a Q-learning algorithm with Boltzmann policy (GMF-Q), with analysis of convergence property and computational complexity. The experiments on repeated Ad auction problems demonstrate that this GMF-Q algorithm is efficient and robust in terms of convergence and learning accuracy. Moreover, its performance is superior in convergence, stability, and learning ability, when compared with existing algorithms for multi-agent reinforcement learning.	[Guo, Xin; Hu, Anran; Xu, Renyuan] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Zhang, Junzi] Stanford Univ, Stanford, CA 94305 USA	University of California System; University of California Berkeley; Stanford University	Guo, X (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	xinguo@berkeley.edu; anran_hu@berkeley.edu; renyuanxu@berkeley.edu; junziz@stanford.edu						Acciaio B., 2018, 180205754 ARXIV; [Anonymous], 2018, REINFORCEMENT LEARNI, DOI 10.1016/S1364-6613(99)01331-5; Asadi K., 2017, ICML, P243; Bellemare MG, 2016, AAAI CONF ARTIF INTE, P1476; Benaim M, 2008, PERFORM EVALUATION, V65, P823, DOI 10.1016/j.peva.2008.03.005; Bolley F, 2008, LECT NOTES MATH, V1934, P371; Cai H, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P661, DOI 10.1145/3018661.3018702; Gao B., 2017, 170400805 ARXIV; Gibbs AL, 2002, INT STAT REV, V70, P419, DOI 10.2307/1403865; Gummadi R., 2012, 8 AD AUCT WORKSH; Haarnoja T., 2017, 170208165 ARXIV; Hamari J, 2016, J ASSOC INF SCI TECH, V67, P2047, DOI 10.1002/asi.23552; Hernandez-Leal P., 2018, 181005587 ARXIV; Hu J., 2003, J MACHINE LEARNING R, V4, P1039, DOI DOI 10.5555/945365.964288; Huang M., 2017, 170106661 ARXIV; Huang MY, 2006, COMMUN INF SYST, V6, P221; Iyer K, 2011, ACM SIGECOM EXCH, V10, P10; Jeong SH, 2015, ACM SIGCOMM COMP COM, V45, P99, DOI 10.1145/2785956.2790005; Jin J., 2018, 180209756 ARXIV; Kapoor S., 2018, 180709427 ARXIV; Kizilkale AC, 2013, IEEE T AUTOMAT CONTR, V58, P905, DOI 10.1109/TAC.2012.2228032; Lasry JM, 2007, JAP J MATH, V2, P229, DOI 10.1007/s11537-007-0657-8; Lehalle C-A., 2019, 190209606 ARXIV; Lopez JPM, 2015, J DYN GAMES, V2, P89, DOI 10.3934/jdg.2015.2.89; Mguni D, 2018, AAAI CONF ARTIF INTE, P4686; Minh V. M., 2016, INT C MACH LEARN; Papadimitriou CH, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P82; Perolat J., 2016, 160608718 ARXIV; Perolat J, 2018, PR MACH LEARN RES, V84; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Recht B., 2018, ARXIV180609460; Saldi N, 2018, SIAM J CONTROL OPTIM, V56, P4256, DOI 10.1137/17M1112583; Subramanian J, 2019, AAMAS '19: PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P251; Tan M., 1993, P 10 INT C MACHINE L, P330, DOI DOI 10.1016/B978-1-55860-307-3.50049-6; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wai H. T., 2018, P ADV NEUR INF PROC, P9672; Yang J., 2017, 171103156 ARXIV; Yang Y., 2018, 180205438 ARXIV; Yin HB, 2014, IEEE T AUTOMAT CONTR, V59, P629, DOI 10.1109/TAC.2013.2287733	41	20	20	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305002
C	Icarte, RT; Waldie, E; Klassen, TQ; Valenzano, R; Castro, MP; McIlraith, SA		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Icarte, Rodrigo Toro; Waldie, Ethan; Klassen, Toryn Q.; Valenzano, Richard; Castro, Margarita P.; McIlraith, Sheila A.			Learning Reward Machines for Partially Observable Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Reward Machines (RMs) provide a structured, automata-based representation of a reward function that enables a Reinforcement Learning (RL) agent to decompose an RL problem into structured subproblems that can be efficiently learned via off-policy learning. Here we show that RMs can be learned from experience, instead of being specified by the user, and that the resulting problem decomposition can be used to effectively solve partially observable RL problems. We pose the task of learning RMs as a discrete optimization problem where the objective is to find an RM that decomposes the problem into a set of subproblems such that the combination of their optimal memoryless policies is an optimal policy for the original problem. We show the effectiveness of this approach on three partially observable domains, where it significantly outperforms A3C, PPO, and ACER, and discuss its advantages, limitations, and broader potential.(1)	[Icarte, Rodrigo Toro; Klassen, Toryn Q.; McIlraith, Sheila A.] Univ Toronto, Vector Inst, Toronto, ON, Canada; [Waldie, Ethan; Castro, Margarita P.] Univ Toronto, Toronto, ON, Canada; [Valenzano, Richard] Element AI, Montreal, PQ, Canada	University of Toronto; University of Toronto	Icarte, RT (corresponding author), Univ Toronto, Vector Inst, Toronto, ON, Canada.	rntoro@cs.toronto.edu	Castro, Margarita P/AAW-4688-2021; Castro, Margarita/AAV-9469-2021; Icarte, Rodrigo Toro/AAJ-2469-2020	Castro, Margarita P/0000-0002-4689-6143; Icarte, Rodrigo Toro/0000-0002-7734-099X	Natural Sciences and Engineering Research Council of Canada (NSERC); Microsoft Research; CONICYT (Becas Chile)	Natural Sciences and Engineering Research Council of Canada (NSERC)(Natural Sciences and Engineering Research Council of Canada (NSERC)); Microsoft Research(Microsoft); CONICYT (Becas Chile)(Comision Nacional de Investigacion Cientifica y Tecnologica (CONICYT))	We gratefully acknowledge funding from the Natural Sciences and Engineering Research Council of Canada (NSERC) and Microsoft Research. The first author also gratefully acknowledges funding from CONICYT (Becas Chile). A preliminary version of this work was presented at RLDM [34].	Aarts E., 2003, LOCAL SEARCH COMBINA; ANGLUIN D, 1983, COMPUT SURV, V15, P237, DOI 10.1145/356914.356918; Camacho A, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P6065; CASSANDRA AR, 1994, PROCEEDINGS OF THE TWELFTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 AND 2, P1023; Conforti M, 2014, GRAD TEXTS MATH, V271, P1, DOI 10.1007/978-3-319-11008-0; Doshi-Velez F, 2015, IEEE T PATTERN ANAL, V37, P394, DOI 10.1109/TPAMI.2013.191; Geoffrion A. M., 1972, Journal of Optimization Theory and Applications, V10, P237, DOI 10.1007/BF00934810; Ghavamzadeh M, 2015, FOUND TRENDS MACH LE, V8, P360, DOI 10.1561/2200000049; Giantamidis G, 2016, LECT NOTES COMPUT SC, V9995, P291, DOI 10.1007/978-3-319-48989-6_18; Glover F., 1998, HDB COMBINATORIAL OP, P2093, DOI [10.1007/978-1-4613-0303-9_33, DOI 10.1007/978-1-4613-0303-9_33]; Hesse Christopher, 2017, OPENAI BASELINES; Hung C-C, 2018, ARXIV181006721; Icarte RT, 2018, PR MACH LEARN RES, V80; Illanes L., 2019, P 4 MULT C REINF LEA, P191; JADERBERG M, 2016, 4 INT C LEARN REPR I, P1; Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301; Kasenberg D., 2017, P 56 IEEE ANN C DEC, P4914; Khan Arbaaz, 2017, ARXIV170905706; Kingma D.P, P 3 INT C LEARNING R; Mahmud M., 2010, P 27 INT C MACH LEAR, P727; Meuleau N, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P427; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Oh J, 2016, PR MACH LEARN RES, V48; Peshkin K, 1999, MACHINE LEARNING, PROCEEDINGS, P307; Pisinger D, 2010, INT SER OPER RES MAN, V146, P399, DOI 10.1007/978-1-4419-1665-5_13; Poupart Pascal, 2008, PROC INT S ARTIFICIA, P1; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Singh SP., 1994, MACH LEARN PROC, V1994, P284, DOI [10.1016/c2009-0-27542-8, DOI 10.1016/C2009-0-27542-8]; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Toro Icarte R., 2019, P 4 MULT C REINF LEA, P22; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; Voss S., 2012, METAHEURISTICS ADV T; Wang Z., 2016, ARXIV161101224; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Xu Z., 2019, ARXIV190905912; ZENG Z, 1993, NEURAL COMPUT, V5, P976, DOI 10.1162/neco.1993.5.6.976; Zhang A., 2019, ARXIV190610437	41	20	20	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907021
C	Lucas, J; Tucker, G; Grosse, R; Norouzi, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lucas, James; Tucker, George; Grosse, Roger; Norouzi, Mohammad			Don't Blame the ELBO! A Linear VAE Perspective on Posterior Collapse	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Posterior collapse in Variational Autoencoders (VAEs) arises when the variational posterior distribution closely matches the prior for a subset of latent variables. This paper presents a simple and intuitive explanation for posterior collapse through the analysis of linear VAEs and their direct correspondence with Probabilistic PCA (pPCA). We explain how posterior collapse may occur in pPCA due to local maxima in the log marginal likelihood. Unexpectedly, we prove that the ELBO objective for the linear VAE does not introduce additional spurious local maxima relative to log marginal likelihood. We show further that training a linear VAE with exact variational inference recovers an identifiable global maximum corresponding to the principal component directions. Empirically, we find that our linear analysis is predictive even for high-capacity, non-linear VAEs and helps explain the relationship between the observation noise, local maxima, and posterior collapse in deep Gaussian VAEs.	[Lucas, James; Grosse, Roger] Univ Toronto, Toronto, ON, Canada; [Lucas, James; Tucker, George; Norouzi, Mohammad] Google Brain, Mountain View, CA 94043 USA	University of Toronto; Google Incorporated	Lucas, J (corresponding author), Univ Toronto, Toronto, ON, Canada.; Lucas, J (corresponding author), Google Brain, Mountain View, CA 94043 USA.				CIFAR Canadian AI Chairs program	CIFAR Canadian AI Chairs program	This work was guided by many conversations with and feedback from our colleagues. In particular, we thank Durk Kingma, Alex Alemi, and Guodong Zhang for invaluable feedback on early versions of this work. RG acknowledges support from the CIFAR Canadian AI Chairs program.	Abadi M, 2015, P 12 USENIX S OPERAT; AITCHISON J, 1980, BIOMETRIKA, V67, P261, DOI 10.2307/2335470; Alemi AA, 2017, ARXIV171100464; BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; Bartholomew D. J., 1987, LATENT VARIABLE MODE; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Burges, 1998, MNIST DATABASE HANDW; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chen T.Q., 2018, NEURIPS, P2610; Chen Xi, 2016, ARXIV161102731; Cremer C, 2018, PR MACH LEARN RES, V80; Dai B., 2017, ARXIV170605148; Dai Bin, 2019, INT C LEARN REPR; Dieng A. B., 2018, ARXIV180704863; Gomez-Bombarelli R., 2018, AM CHEM SOC CENTRAL; He Junxian, 2019, INT C LEARN REPR; Higgins I, 2016, BETA VAE LEARNING BA; Hjelm D., 2016, ADV NEURAL INFORM PR; Huang CW, 2018, ADV NEUR IN, V31; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kim Y, 2018, PR MACH LEARN RES, V80; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kunin D., 2019, ARXIV190108168; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Ma XN, 2020, P I CIVIL ENG-GEOTEC, V173, P217, DOI 10.1680/jgeen.19.00088; Maaloe L, 2019, ADV NEUR IN, V32; Papamakarios George, 2017, ARXIV170507057; Petersen K. B., MATRIX COOKBOOK; Razavi Ali, 2019, INT C LEARN REPR ICL; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Rezende Danilo Jimenez, 2018, ARXIV181000597; Rolinek M., 2018, ARXIV181206775; Rumelhart D. E., 1985, COGNITIVE SCI; Sonderby CK, 2016, ADV NEUR IN, V29; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; Tomczak J. M., 2017, ARXIV170507120; Yeung S., 2017, ICML WORK PRINC APPR	41	20	20	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901008
C	Voelker, AR; Kajic, I; Eliasmith, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Voelker, Aaron R.; Kajic, Ivana; Eliasmith, Chris			Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LONG	We propose a novel memory cell for recurrent neural networks that dynamically maintains information across long windows of time using relatively few resources. The Legendre Memory Unit (LMU) is mathematically derived to orthogonalize its continuous-time history - doing so by solving d coupled ordinary differential equations (ODEs), whose phase space linearly maps onto sliding windows of time via the Legendre polynomials up to degree d - 1. Backpropagation across LMUs outperforms equivalently-sized LSTMs on a chaotic time-series prediction task, improves memory capacity by two orders of magnitude, and significantly reduces training and inference times. LMUs can efficiently handle temporal dependencies spanning 100,000 time-steps, converge rapidly, and use few internal state-variables to learn complex functions spanning long windows of time - exceeding state-of-the-art performance among RNNs on permuted sequential MNIST. These results are due to the network's disposition to learn scale-invariant features independently of step size. Backpropagation through the ODE solver allows each layer to adapt its internal time-step, enabling the network to learn task-relevant time-scales. We demonstrate that LMU memory cells can be implemented using m recurrently-connected Poisson spiking neurons, O(m) time and memory, with error scaling as O(d/root m). We discuss implementations of LMUs on analog and digital neuromorphic hardware.	[Voelker, Aaron R.; Kajic, Ivana; Eliasmith, Chris] Ctr Theoret Neurosci, Waterloo, ON, Canada; Appl Brain Res Inc, Waterloo, ON, Canada	University of Waterloo	Voelker, AR (corresponding author), Ctr Theoret Neurosci, Waterloo, ON, Canada.	arvoelke@uwaterloo.ca; i2kajic@uwaterloo.ca; celiasmith@uwaterloo.ca			CFI; OIT infrastructure funding; Canada Research Chairs program; NSERC [261453]; ONR [N000141310419]; AFOSR [FA8655-13-1-3084]; OGS; NSERC CGS-D	CFI(Canada Foundation for Innovation); OIT infrastructure funding; Canada Research Chairs program(Canada Research Chairs); NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); OGS(Ontario Graduate Scholarship); NSERC CGS-D(Natural Sciences and Engineering Research Council of Canada (NSERC))	We thank the reviewers for improving our work by identifying areas in need of clarification and suggesting additional points of validation. This work was supported by CFI and OIT infrastructure funding, the Canada Research Chairs program, NSERC Discovery grant 261453, ONR grant N000141310419, AFOSR grant FA8655-13-1-3084, OGS, and NSERC CGS-D.	Abadi M, 2015, P 12 USENIX S OPERAT; [Anonymous], 2003, NEURAL ENG COMPUTATI; Arjovsky M, 2016, PR MACH LEARN RES, V48; Bai S., 2018, ARXIV PREPRINT ARXIV; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Blouw P., P 7 ANN NEUR INSP CO; Chandar S, 2019, AAAI CONF ARTIF INTE, P3280; Chang SY, 2017, ADV NEUR IN, V30; Chen T.Q., 2018, ADV NEURAL INFORM PR; Chorowski I. K., 2015, ADV NEURAL INFORM PR, V28, P577, DOI DOI 10.1016/0167-739X(94)90007-8; Cooijmans T., 2016, ARXIV160309025; Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359; de Jong Joost, 2019, INT C COGN MOD; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Glorot X., 2010, PROC MACH LEARN RES, P249; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Grigoryeva L, 2015, SCI REP-UK, V5, DOI 10.1038/srep12858; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Inubushi M, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-10257-6; Kingma D.P, P 3 INT C LEARNING R; Le Q.V., 2015, ABS150400941 CORR; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Legendre Adrien-Marie, 1782, MEMOIRES MATH PHYS P, P411; Li S, 2018, PROC CVPR IEEE, P5457, DOI 10.1109/CVPR.2018.00572; Luong M., 2015, ARXIV150804025; MACKEY MC, 1977, SCIENCE, V197, P287, DOI 10.1126/science.267326; Maharaj T., 2016, ARXIV160601305; Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432; Neil D, 2016, ADV NEUR IN, V29; Pade H., 1892, ANN SCI ECOLE NORM S, V9, P3, DOI DOI 10.24033/ASENS.378; Rasmussen Daniel, 2019, NEUROINFORMATICS, P1; Rodrigues Olinde, 1816, THESIS; Strogatz S.H., 2015, NONLINEAR DYNAMICS C, DOI DOI 10.1201/9780429492563; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Takens F., 1981, DYNAMICAL SYSTEMS TU, P366, DOI [DOI 10.1007/BFB0091924, 10.1007/bfb0091924, 10.1007/BFb0091924]; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Voelker AR, 2018, NEURAL COMPUT, V30, P569, DOI 10.1162/neco_a_01046; Voelker Aaron R., 2019, THESIS; Xu K, 2015, PR MACH LEARN RES, V37, P2048	39	20	20	1	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907025
C	Zhang, MR; Lucas, J; Hinton, G; Ba, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Michael R.; Lucas, James; Hinton, Geoffrey; Ba, Jimmy			Lookahead Optimizer: k steps forward, 1 step back	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes, such as AdaGrad and Adam, and (2) accelerated schemes, such as heavy-ball and Nesterov momentum. In this paper, we propose a new optimization algorithm, Lookahead, that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by looking ahead at the sequence of "fast weights" generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings on ImageNet, CIFAR-10/100, neural machine translation, and Penn Treebank.	[Zhang, Michael R.; Lucas, James; Hinton, Geoffrey; Ba, Jimmy] Univ Toronto, Vector Inst, Dept Comp Sci, Toronto, ON, Canada	University of Toronto	Zhang, MR (corresponding author), Univ Toronto, Vector Inst, Dept Comp Sci, Toronto, ON, Canada.	michael@cs.toronto.edu; jlucas@cs.toronto.edu; hinton@cs.toronto.edu; jba@cs.toronto.edu						Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448; ANDERSON DG, 1965, J ACM, V12, P547, DOI 10.1145/321296.321305; Brezinski C., 2013, EXTRAPOLATION METHOD, V2; Defazio Aaron, 2018, INEFFECTIVENESS VARI; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Garipov T., 2018, ARXIV180210026; Goh G., 2017, DISTILL, V2, pe6, DOI DOI 10.23915/DISTILL.00006; Goyal Priya, 2017, ARXIV170602677; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton Geoffrey E, 1987, USING FAST WEIGHTS D; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; HOGAN E, 2013, ADV NEURAL INFORM PR, P315; Izmailov P, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P876; Jean Sebastien, 2014, ARXIV14122007; Jia Xianyan, 2018, ARXIV180711205; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lessard L, 2016, SIAM J OPTIMIZ, V26, P57, DOI 10.1137/15M1009597; Li Ren-cang, 2005, TECHNICAL REPORT; Loshchilov I., 2017, ARXIV171105101; Lucas J., 2018, P 6 INT C LEARN REPR, P6; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Martens James, 2014, NEW INSIGHTS PERSPEC; Merity Stephen, 2017, ICLR; Montavon G., 2012, NEURAL NETWORKS TRIC, V7700; Nesterov Yu. E., 1983, Doklady Akademii Nauk SSSR, V269, P543; Nichol Alex, 2018, ARXIV180302999; O'Donoghue B, 2015, FOUND COMPUT MATH, V15, P715, DOI 10.1007/s10208-013-9150-3; Osawa Kazuki, 2018, 2 ORDER OPTIMIZATION; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Ruppert D., 1988, TECHNICAL REPORT; Schaul T., 2013, INT C MACHINE LEARNI, P343; Scieur D., 2018, ARXIV180509639; Scieur D., 2018, ARXIV180600370; Shazeer N, 2018, PR MACH LEARN RES, V80; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Winkler Stefan, 2018, ARXIV180604498; Wu Yuhuai, 2018, ARXIV180302021; You Y, 2018, PROC INT CONF PARAL, DOI 10.1145/3225058.3225069; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang GD, 2019, ADV NEUR IN, V32	47	20	21	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901025
C	Chen, LJ; Wang, HY; Zhao, JM; Koutris, P; Papailiopoulos, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Lingjiao; Wang, Hongyi; Zhao, Jinman; Koutris, Paraschos; Papailiopoulos, Dimitris			The Effect of Network Width on the Performance of Large-batch Training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Distributed implementations of mini-batch stochastic gradient descent (SGD) suffer from communication overheads, attributed to the high frequency of gradient updates inherent in small-batch training. Training with large batches can reduce these overheads; however it besets the convergence of the algorithm and the generalization performance. In this work, we take a first step towards analyzing how the structure (width and depth) of a neural network affects the performance of large-batch training. We present new theoretical results which suggest that-for a fixed number of parameters-wider networks are more amenable to fast large-batch training compared to deeper ones. We provide extensive experiments on residual and fully-connected neural networks which suggest that wider networks can be trained using larger batches without incurring a convergence slow-down, unlike their deeper variants.	[Chen, Lingjiao; Wang, Hongyi; Zhao, Jinman; Koutris, Paraschos] Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA; [Papailiopoulos, Dimitris] Univ Wisconsin, Dept Elect & Comp Engn, 1415 Johnson Dr, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison; University of Wisconsin System; University of Wisconsin Madison	Chen, LJ (corresponding author), Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.								0	20	20	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003082
C	Cui, XD; Zhang, W; Tuske, Z; Picheny, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Cui, Xiaodong; Zhang, Wei; Tuske, Zoltan; Picheny, Michael			Evolutionary Stochastic Gradient Descent for Optimization of Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose a population-based Evolutionary Stochastic Gradient Descent (ESGD) framework for optimizing deep neural networks. ESGD combines SGD and gradient-free evolutionary algorithms as complementary algorithms in one framework in which the optimization alternates between the SGD step and evolution step to improve the average fitness of the population. With a back-off strategy in the SGD step and an elitist strategy in the evolution step, it guarantees that the best fitness in the population will never degrade. In addition, individuals in the population optimized with various SGD-based optimizers using distinct hyperparameters in the SGD step are considered as competing species in a coevolution setting such that the complementarity of the optimizers is also taken into account. The effectiveness of ESGD is demonstrated across multiple applications including speech recognition, image recognition and language modeling, using networks with a variety of deep architectures.	[Cui, Xiaodong; Zhang, Wei; Tuske, Zoltan; Picheny, Michael] IBM Res AI, IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Cui, XD (corresponding author), IBM Res AI, IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.	cuix@us.ibm.com; weiz@us.ibm.com; Zoltan.Tuske@ibm.com; picheny@us.ibm.com						Arnold DV, 2002, IEEE T EVOLUT COMPUT, V6, P30, DOI 10.1023/A:1015059928466; Bottou L., 2016, ARXIV160604838; Chrabaszcz P, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1419; Dehak N, 2011, IEEE T AUDIO SPEECH, V19, P788, DOI 10.1109/TASL.2010.2064307; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Ebrahimi S., 2017, P 1 C ROBOT LEARNING; Gal Yarin, 2016, ADV NEURAL INFORM PR, P1019, DOI DOI 10.5555/3157096.3157211; Garcia-Pedrajas N, 2005, IEEE T EVOLUT COMPUT, V9, P271, DOI 10.1109/TEVC.2005.844158; Garcia-Pedrajas N, 2003, IEEE T NEURAL NETWOR, V14, P575, DOI 10.1109/TNN.2003.810618; Goldberg DE, 1989, GENETIC ALGORITHMS S; Gomez F, 2008, J MACH LEARN RES, V9, P937; Hansen N., 2016, ARXIV160400772; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; HERMANSKY H, 1990, J ACOUST SOC AM, V87, P1738, DOI 10.1121/1.399423; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Igel C, 2003, IEEE C EVOL COMPUTAT, P2588; Jaderberg Max, 2017, ABS171109846 CORR; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Laine Samuli, 2017, P INT C LEARN REPR I, P3; Lehman J., 2017, ARXIV171206568; Liang J., 2018, ARXIV180303745; Liu C., 2017, ARXIV171200559; Loshchilov I., 2017, P INT C LEARNING REP; Loshchilov I., 2016, ARXIV; Loshchilov I, 2017, EVOL COMPUT, V25, P143, DOI 10.1162/EVCO_a_00168; Ma X., 2017, ICLR 2017; Merity Stephen, 2018, INT C LEARN REPR ICL; Miikkulainen R, 2019, ARTIFICIAL INTELLIGENCE IN THE AGE OF NEURAL NETWORKS AND BRAIN COMPUTING, P293, DOI 10.1016/B978-0-12-815480-9.00015-3; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Morse G, 2016, GECCO'16: PROCEEDINGS OF THE 2016 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P477, DOI 10.1145/2908812.2908916; Nesterov Yu. E., 1983, Doklady Akademii Nauk SSSR, V269, P543; Real E., 2018, ARXIV180201548, DOI [DOI 10.1609/AAAI.V33I01.33014780, 10.1609/aaai.v33i01.33014780]; Real E, 2017, PR MACH LEARN RES, V70; Salimans T., 2017, ARXIV170303864; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Such F. P., 2017, ARXIV171206567; Suganuma M, 2017, PROCEEDINGS OF THE 2017 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'17), P497, DOI 10.1145/3071178.3071229; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Wan L., 2013, P INT C MACHINE LEAR, P1058; Yang ZY, 2008, INFORM SCIENCES, V178, P2985, DOI 10.1016/j.ins.2008.02.017; Yao X, 1999, P IEEE, V87, P1423, DOI 10.1109/5.784219; Zhang X., 2017, IEEE WIRELESS COMMUN; Zolna Konrad, 2018, INT C LEARN REPR	45	20	20	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000054
C	Dubey, A; Gupta, O; Raskar, R; Naik, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dubey, Abhimanyu; Gupta, Otkrist; Raskar, Ramesh; Naik, Nikhil			Maximum Entropy Fine-Grained Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Fine-Grained Visual Classification (FGVC) is an important computer vision problem that involves small diversity within the different classes, and often requires expert annotators to collect data. Utilizing this notion of small visual diversity, we revisit Maximum-Entropy learning in the context of fine-grained classification, and provide a training routine that maximizes the entropy of the output probability distribution for training convolutional neural networks on FGVC tasks. We provide a theoretical as well as empirical justification of our approach, and achieve stateof-the-art performance across a variety of classification tasks in FGVC, that can potentially be extended to any fine-tuning task. Our method is robust to different hyperparameter values, amount of training data and amount of training label noise and can hence be a valuable tool in many similar problems.	[Dubey, Abhimanyu; Gupta, Otkrist; Raskar, Ramesh; Naik, Nikhil] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Dubey, A (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	dubeya@mit.edu; otkrist@mit.edu; raskar@mit.edu; naik@mit.edu						Abe S., 2001, NONEXTENSIVE STAT ME, V560; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Branson S., 2014, PROC BRIT MACH VIS C; Chen YH, 2009, J MACH LEARN RES, V10, P747; Chintala, TENSORS DYNAMIC NEUR; Cimpoi M., 2015, IEEE C COMP VIS PATT, DOI DOI 10.1109/CVPR.2015.7299007; Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Figueiredo MAT, 2002, IEEE T PATTERN ANAL, V24, P381, DOI 10.1109/34.990138; Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41; Golub GH, 1999, SIAM J MATRIX ANAL A, V21, P185, DOI 10.1137/S0895479897326432; Grandvalet Yves, ENTROPY REGULARIZATI; Gull S.F., 1988, MAXIMUM ENTROPY BAYE, P53, DOI [DOI 10.1007/978-94-009-3049-0, 10.1007/978-94-009-3049-0_4, DOI 10.1007/978-94-009-3049-0_4]; Guo Chuan, 2017, ICML; Hardoon D., 2009, P INT C ART INT STAT, P480; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; Jegou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235; JONSSON D, 1982, J MULTIVARIATE ANAL, V12, P1, DOI 10.1016/0047-259X(82)90080-X; Khosla A., NOVEL DATASET FINE G; Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743; Krause J, 2016, LECT NOTES COMPUT SC, V9907, P301, DOI 10.1007/978-3-319-46487-9_19; Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77; Krizhevsky A., 2010, THE CIFAR 10 DATASET; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Lin Tsung-Yu, 2017, ARXIV170706772; Liu ML, 2016, LECT NOTES COMPUT SC, V10040, P337, DOI 10.1007/978-3-319-48674-1_30; Luo Y., 2016, ICASSP; Maji S., 2013, TECH REP; Mnih V, 2016, PR MACH LEARN RES, V48; Moghimi M., 2016, BRIT MACH VIS C BMVC; Neyshabur Behnam, 2017, ARXIV170709564; Pereyra Gabriel, 2017, ARXIV170106548; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rose K, 1998, P IEEE, V86, P2210, DOI 10.1109/5.726788; Simon M., 2017, ARXIV170500487; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Szummer M, 2002, ADV NEUR IN, V14, P945; Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658; Wah C., 2011, TECH REP; Wang YM, 2016, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2016.131; Zhang N, 2012, PROC CVPR IEEE, P3665, DOI 10.1109/CVPR.2012.6248364; Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, P1713, DOI 10.1109/TIP.2016.2531289; Zhu J, 2009, J MACH LEARN RES, V10, P2531	49	20	20	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300059
C	Feng, J; Yu, Y; Zhou, ZH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Feng, Ji; Yu, Yang; Zhou, Zhi-Hua			Multi-Layered Gradient Boosting Decision Trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Multi-layered distributed representation is believed to be the key ingredient of deep neural networks especially in cognitive tasks like computer vision. While non-differentiable models such as gradient boosting decision trees (GBDTs) are still the dominant methods for modeling discrete or tabular data, they are hard to incorporate with such representation learning ability. In this work, we propose the multi-layered GBDT forest (mGBDTs), with an explicit emphasis on exploring the ability to learn hierarchical distributed representations by stacking several layers of regression GBDTs as its building block. The model can be jointly trained by a variant of target propagation across layers, without the need to derive back-propagation nor differentiability. Experiments confirmed the effectiveness of the model in terms of performance and representation learning ability.	[Feng, Ji; Yu, Yang; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China; [Feng, Ji] Sinovat Ventures AI Inst, Beijing, Peoples R China	Nanjing University	Feng, J (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.; Feng, J (corresponding author), Sinovat Ventures AI Inst, Beijing, Peoples R China.	fengj@lamda.nju.edu.cn; yuy@lamda.nju.edu.cn; zhouzh@lamda.nju.edu.cn			NSFC [61751306]; National Key R&D Program of China [2018YFB1004300]; Collaborative Innovation Center of Novel Software Technology and Industrialization	NSFC(National Natural Science Foundation of China (NSFC)); National Key R&D Program of China; Collaborative Innovation Center of Novel Software Technology and Industrialization	This research was supported by NSFC (61751306), National Key R&D Program of China (2018YFB1004300) and Collaborative Innovation Center of Novel Software Technology and Industrialization.	Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; [Anonymous], 2017, ARXIV171109784; Bengio, 2014, ARXIV14077906; Bengio Y., 2013, P 26 INT C NEUR INF, P899; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bengio Yoshua, 2013, INT C MACHINE LEARNI, P552; Chen T., 2015, NIPS 2014 WORKSHOP H, P69, DOI DOI 10.5555/2996850.2996854; Chen T., 2016, KDD16 P 22 ACM, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]; CLARKE FH, 1976, PAC J MATH, V64, P97, DOI 10.2140/pjm.1976.64.97; Feng J., 2018, AAAI; Freund Y., 1999, Journal of Japanese Society for Artificial Intelligence, V14, P771; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Hinton G.E., 1986, PARALLEL DISTRIBUTED, V1, P77; Huang S., 2017, 5 INT C LEARN REPR I; Ke G., 2017, P ADV NEURAL INFORM, V30, P3146; King DB, 2015, ACS SYM SER, V1214, P1; Kontschieder P, 2015, IEEE I CONF COMP VIS, P1467, DOI 10.1109/ICCV.2015.172; Lee DH, 2015, LECT NOTES ARTIF INT, V9284, P498, DOI 10.1007/978-3-319-23528-8_31; Lichman M., 2013, UCI MACHINE LEARNING; Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276; Nokland, 2016, ADV NEURAL INFORM PR, V29, P1037; Rashmi KV, 2015, JMLR WORKSH CONF PRO, V38, P489; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Si S, 2017, PR MACH LEARN RES, V70; Tishby N., 2015, ARXIV150302406; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Werbos P., 1975, REGRESSION NEW TOOLS; Zhou Z., 2017, P IJCAI, P3553; Zhou Z.-H, 2012, ENSEMBLE METHODS FDN, DOI DOI 10.1201/B12207; Zhou Z.H., 2018, NATL SCI REV; [No title captured]; [No title captured]	33	20	20	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303054
C	Hoffer, E; Banner, R; Golan, I; Soudry, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hoffer, Elad; Banner, Ron; Golan, Itay; Soudry, Daniel			Norm matters: efficient and accurate normalization schemes in deep networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Over the past few years, Batch-Normalization has been commonly used in deep networks, allowing faster training and high performance for a wide variety of applications. However, the reasons behind its merits remained unanswered, with several shortcomings that hindered its use for certain tasks. In this work, we present a novel view on the purpose and function of normalization methods and weight-decay, as tools to decouple weights' norm from the underlying optimized objective. This property highlights the connection between practices such as normalization, weight decay and learning-rate adjustments. We suggest several alternatives to the widely used L-2 batch-norm, using normalization in L-1 and L-infinity spaces that can substantially improve numerical stability in low-precision implementations as well as provide computational and memory benefits. We demonstrate that such methods enable the first batch-norm alternative to work for half-precision implementations. Finally, we suggest a modification to weight-normalization, which improves its performance on large-scale tasks.(2)	[Hoffer, Elad; Golan, Itay; Soudry, Daniel] Technion Israel Inst Technol, Haifa, Israel; [Banner, Ron] Intel Artificial Intelligence Prod Grp AIPG, Santa Clara, CA USA	Technion Israel Institute of Technology	Hoffer, E (corresponding author), Technion Israel Inst Technol, Haifa, Israel.	elad.hoffer@gmail.com; ron.banner@intel.com; itaygolan@gmail.com; daniel.soudry@gmail.com		Golan, Itay/0000-0003-2574-5976	Israel Science Foundation [31/1031]; Taub foundation	Israel Science Foundation(Israel Science Foundation); Taub foundation	This research was supported by the Israel Science Foundation (grant No. 31/1031), and by the Taub foundation. A Titan Xp used for this research was donated by the NVIDIA Corporation.	[Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; [Anonymous], 2017, ARXIV170403971; Arpit D, 2016, PR MACH LEARN RES, V48; Ba J., 2017, P 3 INT C LEARN REPR; Ba L.J, 2016, P C WORKSH NEUR INF; Bahdanau Dzmitry, 2015, NEURAL MACHINE TRANS; Bos S., 1996, Artificial Neural Networks - ICANN 96. 1996 International Conference Proceedings, P551; Bos S, 1996, IEEE IJCNN, P241, DOI 10.1109/ICNN.1996.548898; Bulo S.R., 2017, ARXIV171202616; Cooijmans T., 2016, P 5 INT C LEARN REPR; Courbariaux M., 2014, ABS14127024 CORR; Das D., 2018, CORR; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Eidnes L, 2017, ARXIV170904054; Gitman I., 2017, CORR; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hertz J.A, 1992, NIPS 91 P 4 INT C NE, P950; Hoffer E., 2017, ADV NEURAL INF PROCE; Huang L., 2017, ARXIV171002338; Hubara I, 2016, ADV NEUR IN, V29; Ioffe Sergey, 2017, NIPS, P3; Klambauer Gnter, 2017, SELF NORMALIZING NEU; Koster U., 2017, PROC C WORKSHOP NEUR, P1740; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Miyato M. K.-Y. Y. Takeru, 2018, INT C LEARN REPR; Salimans T., 2016, ADV NEUR IN, P2234; Smith Samuel L, 2017, ARXIV171100489; Soudry D., 2014, PROC 27 INT C NEURAL, P963, DOI DOI 10.5555/2968826.2968934; Soudry D, 2018, INT C LEARN REPR; Ulyanov D., 2016, CORR; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Venkatesh G, 2017, INT CONF ACOUST SPEE, P2861, DOI 10.1109/ICASSP.2017.7952679; Wu S., 2018, ARXIV E PRINTS; Wu Y., 2018, ARXIV180308494; Zhang Chiyuan, 2016, ARXIV161103530	39	20	22	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302019
C	Linsley, D; Kim, J; Veerabadran, V; Windolf, C; Serre, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Linsley, Drew; Kim, Junkyung; Veerabadran, Vijay; Windolf, Charlie; Serre, Thomas			Learning long-range spatial dependencies with horizontal gated recurrent units	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SURROUND SUPPRESSION; CONTOUR SALIENCY; IMAGE ELEMENTS; BOUNDARIES; COMPUTATIONS; INTEGRATION; CONNECTIONS	Progress in deep learning has spawned great successes in many engineering applications. As a prime example, convolutional neural networks, a type of feedforward neural networks, are now approaching - and sometimes even surpassing - human accuracy on a variety of visual recognition tasks. Here, however, we show that these neural networks and their recent extensions struggle in recognition tasks where co-dependent visual features must be detected over long spatial ranges. We introduce a visual challenge, Pathfinder, and describe a novel recurrent neural network architecture called the horizontal gated recurrent unit (hGRU) to learn intrinsic horizontal connections - both within and across feature columns. We demonstrate that a single hGRU layer matches or outperforms all tested feedforward hierarchical baselines including state-of-the-art architectures with orders of magnitude more parameters.	[Linsley, Drew; Kim, Junkyung; Veerabadran, Vijay; Windolf, Charlie; Serre, Thomas] Brown Univ, Dept Cognit Linguist & Psychol Sci, Carney Inst Brain Sci, Providence, RI 02912 USA	Brown University	Linsley, D (corresponding author), Brown Univ, Dept Cognit Linguist & Psychol Sci, Carney Inst Brain Sci, Providence, RI 02912 USA.	drew_linsley@brown.edu; junkyung_kim@brown.edu; vijay_veerabadran@brown.edu; thomas_serre@brown.edu			NSF early career award [IIS-1252951]; DARPA young faculty award [YFA N66001-14-1-4037]; Carney Institute for Brain Science; Center for Computation and Visualization (CCV) at Brown University	NSF early career award(National Science Foundation (NSF)); DARPA young faculty award; Carney Institute for Brain Science; Center for Computation and Visualization (CCV) at Brown University(Canadian Institutes of Health Research (CIHR))	This research was supported by NSF early career award [grant number IIS-1252951] and DARPA young faculty award [grant number YFA N66001-14-1-4037]. Additional support was provided by the Carney Institute for Brain Science and the Center for Computation and Visualization (CCV) at Brown University.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; [Anonymous], 2015, ADV NEURAL INFORM PR; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314; Ben-Shahar O, 2004, NEURAL COMPUT, V16, P445, DOI 10.1162/089976604772744866; Bengio Y., 2014, ARXIV14061078; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cooijmans T., 2017, ICLR; FIELD DJ, 1993, VISION RES, V33, P173, DOI 10.1016/0042-6989(93)90156-Q; George D, 2017, SCIENCE, V358, DOI 10.1126/science.aag2612; Gilbert CD, 2013, NAT REV NEUROSCI, V14, P350, DOI 10.1038/nrn3476; Glorot X., 2010, PROC MACH LEARN RES, P249; Graves A., 2008, ADV NEURAL INFORM PR, P545, DOI DOI 10.1007/978-1-4471-4072-6; Graves A, 2007, LECT NOTES COMPUT SC, V4668, P549; GROSSBERG S, 1985, PERCEPT PSYCHOPHYS, V38, P141, DOI 10.3758/BF03198851; Hamaguchi R., 2017, ARXIV170900179, V22, P1; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Houtkamp R, 2010, J EXP PSYCHOL HUMAN, V36, P1443, DOI 10.1037/a0020248; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kim J, 2018, INTERFACE FOCUS, V8, DOI 10.1098/rsfs.2018.0011; Kingma D.P, P 3 INT C LEARNING R; Kokkinos I.., 2016, P 4 INT C LEARN REPR, P1; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; LESHER GW, 1993, VISION RES, V33, P2253, DOI 10.1016/0042-6989(93)90104-5; Li W, 2002, J NEUROPHYSIOL, V88, P2846, DOI 10.1152/jn.00289.2002; Li W, 2008, NEURON, V57, P442, DOI 10.1016/j.neuron.2007.12.011; Li W, 2006, NEURON, V50, P951, DOI 10.1016/j.neuron.2006.04.035; Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958; Liao Q., 2016, CORR; Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lotter W., 2017, ICLR, DOI [DOI 10.48550/ARXIV.1605.08104, 10.48550/arXiv.1605.08104]; Maninis Kevis-Kokitsi, 2017, IEEE T PATTERN ANAL, V1; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Mely DA, 2018, PSYCHOL REV, V125, P769, DOI 10.1037/rev0000109; Nayebi A., 2018, ADV NEURAL INFORM PR, P5290; Papandreou G, 2015, PROC CVPR IEEE, P390, DOI 10.1109/CVPR.2015.7298636; Peirce JW, 2007, J NEUROSCI METH, V162, P8, DOI 10.1016/j.jneumeth.2006.11.017; ROCKLAND KS, 1983, J COMP NEUROL, V216, P303, DOI 10.1002/cne.902160307; Roelfsema PR, 2011, ATTEN PERCEPT PSYCHO, V73, P2542, DOI 10.3758/s13414-011-0200-0; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rubin DB, 2015, NEURON, V85, P402, DOI 10.1016/j.neuron.2014.12.026; Series P, 2003, J PHYSIOL-PARIS, V97, P453, DOI 10.1016/j.jphysparis.2004.01.023; Shushruth S, 2013, J NEUROSCI, V33, P106, DOI 10.1523/JNEUROSCI.2518-12.2013; Shushruth S, 2012, J NEUROSCI, V32, P308, DOI 10.1523/JNEUROSCI.3789-11.2012; Spoerer CJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01551; Stettler DD, 2002, NEURON, V36, P739, DOI 10.1016/S0896-6273(02)01029-2; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tallec Corentin, 2018, ICLR; Tanaka H, 2009, J NEUROPHYSIOL, V101, P1444, DOI 10.1152/jn.90749.2008; Theis L, 2015, ADV NEURAL INFORM PR, P1927; van den Oord A, 2016, PR MACH LEARN RES, V48; Volokitin A, 2017, ADV NEUR IN, V30; Wang TY, 2017, PROC INT C TOOLS ART, P1272, DOI 10.1109/ICTAI.2017.00192; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wang YP, 2017, PROC CVPR IEEE, P1724, DOI 10.1109/CVPR.2017.187; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Yu F., 2016, P ICLR 2016; Zamir A. R., 2016, COMPUTER SCI COMPUTE; Zhaoping L, 2011, CURR OPIN NEUROBIOL, V21, P808, DOI 10.1016/j.conb.2011.07.005; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zucker SW, 2014, P IEEE, V102, P812, DOI 10.1109/JPROC.2014.2314723	70	20	20	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300015
C	Wu, CF; Liu, JL; Wang, XJ; Dong, X		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wu, Chenfei; Liu, Jinlai; Wang, Xiaojie; Dong, Xuan			Chain of Reasoning for Visual Question Answering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Reasoning plays an essential role in Visual Question Answering (VQA). Multi-step and dynamic reasoning is often necessary for answering complex questions. For example, a question "What is placed next to the bus on the right of the picture?"talks about a compound object"bus on the right," which is generated by the relation . <bus, on the righ of, picture>.Furthermore, a new relation including this compound object <sign, next to, bus on the right> is then required to infer the answer. However, previous methods support either one-step or static reasoning, without updating relations or generating compound objects. This paper proposes a novel reasoning model for addressing these problems. A chain of reasoning (CoR) is constructed for supporting multi-step and dynamic reasoning on changed relations and objects. In detail, iteratively, the relational reasoning operations form new relations between objects, and the object refining operations generate new compound objects from relations. We achieve new state-of-the-art results on four publicly available datasets. The visualization of the chain of reasoning illustrates the progress that the CoR generates new compound objects that lead to the answer of the question step by step.	[Wu, Chenfei; Liu, Jinlai; Wang, Xiaojie; Dong, Xuan] Beijing Univ Posts & Telecommun, Ctr Intelligence Sci & Technol, Beijing, Peoples R China	Beijing University of Posts & Telecommunications	Wu, CF (corresponding author), Beijing Univ Posts & Telecommun, Ctr Intelligence Sci & Technol, Beijing, Peoples R China.	wuchenfei@bupt.edu.cn; liujinlai@bupt.edu.cn; xjwang@bupt.edu.cn; dongxuan8811@bupt.edu.cn	Wu, Chenfei/AAJ-5232-2020		NSFC [61273365]; NSSFC [2016ZDA055]; 111 Project [B08004]; Engineering Research Center of Information Networks of MOE, China; Beijing Advanced Innovation Center for Imaging Technology	NSFC(National Natural Science Foundation of China (NSFC)); NSSFC; 111 Project(Ministry of Education, China - 111 Project); Engineering Research Center of Information Networks of MOE, China; Beijing Advanced Innovation Center for Imaging Technology	We would like to thank the anonymous reviewers for their valuable comments. This paper is supported by NSFC (No. 61273365), NSSFC (2016ZDA055), 111 Project (No. B08004), Beijing Advanced Innovation Center for Imaging Technology, Engineering Research Center of Information Networks of MOE, China.	ANDERSON P, 2018, CVPR, V3, P6, DOI DOI 10.1109/CVPR.2018.00636; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Andreas Jacob, 2016, ARXIV160101705, P1545, DOI [DOI 10.18653/V1/N16-1181, 10.18653/v1/N16-1181]; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285; Chen Kan, 2015, ARXIV151105960; Chen Zhu, 2017, ICCV; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Feigenbaum E. A., 1977, TECHNICAL REPORT; Fukui Akira, 2016, ARXIV160601847; Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93; Ilievski I, 2017, ADV NEUR IN, V30; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Kafle K, 2017, IEEE I CONF COMP VIS, P1983, DOI 10.1109/ICCV.2017.217; Kingma D.P, P 3 INT C LEARNING R; Kiros R., 2015, ADV NEURAL INFORM PR, V28, P3294; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Li R., 2016, ADV NEURAL INF PROCE, P4655; Lim W., 2017, P INT C LEARN REPR; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lu JS, 2016, ADV NEUR IN, V29; Malinowski M., 2014, ADV NEURAL INFORM PR, V27, P1682; Pan Lu, 2018, AAAI; Ren M., 2015, ADV NEURAL INFORM PR, V28, P2953; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Santoro A, 2017, ADV NEUR IN, V30; Schwartz I., 2017, NIPS; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Teney D, 2018, PROC CVPR IEEE, P4223, DOI 10.1109/CVPR.2018.00444; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28; Yan Zhang, 2018, ICLR; Yang Shi, 2018, ECCV; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Zhou Yu, 2017, ICCV	37	20	21	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300026
C	Yoon, J; Kim, T; Dia, O; Kim, S; Bengio, Y; Ahn, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yoon, Jaesik; Kim, Taesup; Dia, Ousmane; Kim, Sungwoong; Bengio, Yoshua; Ahn, Sungjin			Bayesian Model-Agnostic Meta-Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Due to the inherent model uncertainty, learning to infer Bayesian posterior from a few-shot dataset is an important step towards robust meta-learning. In this paper, we propose a novel Bayesian model-agnostic meta-learning method. The proposed method combines efficient gradient-based meta-learning with nonparametric variational inference in a principled probabilistic framework. Unlike previous methods, during fast adaptation, the method is capable of learning complex uncertainty structure beyond a simple Gaussian approximation, and during meta-update, a novel Bayesian mechanism prevents meta-level overfitting. Remaining a gradientbased method, it is also the first Bayesian model-agnostic meta-learning method applicable to various tasks including reinforcement learning. Experiment results show the accuracy and robustness of the proposed method in sinusoidal regression, image classification, active learning, and reinforcement learning.	[Kim, Taesup; Dia, Ousmane] Element AI, Montreal, PQ, Canada; [Kim, Taesup; Bengio, Yoshua] MILA Univ Montreal, Montreal, PQ, Canada; [Yoon, Jaesik] SAP, Walldorf, Germany; [Kim, Sungwoong] Kakao Brain, Seoul, South Korea; [Ahn, Sungjin] Rutgers State Univ, New Brunswick, NJ 08901 USA	SAP; Rutgers State University New Brunswick	Ahn, S (corresponding author), Rutgers State Univ, New Brunswick, NJ 08901 USA.	sungjin.ahn@rutgers.edu			SAP; Kakao Brain; NSERC; MILA; CIFAR; IBM; Google; Facebook; Microsoft	SAP; Kakao Brain; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); MILA; CIFAR(Canadian Institute for Advanced Research (CIFAR)); IBM(International Business Machines (IBM)); Google(Google Incorporated); Facebook(Facebook Inc); Microsoft(Microsoft)	JY thanks SAP and Kakao Brain for their support. TK thanks NSERC, MILA and Kakao Brain for their support. YB thanks CIFAR, NSERC, IBM, Google, Facebook and Microsoft for their support. SA, Element AI Fellow, thanks Nicolas Chapados, Pedro Oliveira Pinheiro, Alexandre Lacoste, Negar Rostamzadeh for helpful discussions and feedback.	Ahn S., 2012, ARXIV12066380; [Anonymous], 2009, PROC AUAI 2009; Ba J., 2017, P 3 INT C LEARN REPR; Balan Anoop Korattikara, 2015, CORR; Bauer  M., 2017, ARXIV170600326; Bengio Y., 1990, LEARNING SYNAPTIC LE; BIGGS JB, 1985, BRIT J EDUC PSYCHOL, V55, P185, DOI 10.1111/j.2044-8279.1985.tb02625.x; Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242; Duan Y., 2016, RL2 FAST REINFORCEME; Fei-Fei L, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1134, DOI 10.1109/ICCV.2003.1238476; Finn C, 2017, ARXIV170303400; Gal Y., 2016, BAYES DEEP LEARN WOR; Goodfellow I, 2016, DEEP LEARNING; Grant E., 2018, ARXIV180108930; Houthooft R., 2016, ADV NEURAL INFORM PR, P1109; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lacoste  A., 2017, ARXIV171205016; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Lawrence N.D., 2004, P 21 INT C MACH LEAR, P65, DOI DOI 10.1145/1015330.1015382; Le Cam L., 1986, ASYMPTOTIC METHODS S; Liu Q., 2016, NEURIPS; Liu Y., 2017, ARXIV170402399; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Mishra Nikhil, 2017, ARXIV170703141; Neal Radford M, 2011, HDB MARKOV CHAIN MON, V2, P2, DOI DOI 10.1201/B10905-6; Nichol A., 2018, ARXIV E PRINTS; Ravi S., 2017, P INT C LEARN REPR, P1; Santoro A, 2016, PR MACH LEARN RES, V48; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Smith LB, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02124; Snell J., 2017, ADV NEURAL INFORM PR, P4080; Tenenbaum J.B., 1999, THESIS; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Vinyals Oriol, 2016, ARXIV160604080, P3630; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Williams R.J., 1992, REINFORCEMENT LEARNI, V173, P5, DOI [10.1007/978-1-4615-3618-5, DOI 10.1007/978-1-4615-3618-5]	37	20	20	2	15	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001085
C	Liu, S; Bousquet, O; Chaudhuri, K		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Liu, Shuang; Bousquet, Olivier; Chaudhuri, Kamalika			Approximation and Convergence Properties of Generative Adversarial Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Generative adversarial networks (GAN) approximate a target data distribution by jointly optimizing an objective function through a "two-player game" between a generator and a discriminator. Despite their empirical success, however, two very basic questions on how well they can approximate the target distribution remain unanswered. First, it is not known how restricting the discriminator family affects the approximation quality. Second, while a number of different objective functions have been proposed, we do not understand when convergence to the global minima of the objective function leads to convergence to the target distribution under various notions of distributional convergence. In this paper, we address these questions in a broad and unified setting by defining a notion of adversarial divergences that includes a number of recently proposed objective functions. We show that if the objective function is an adversarial divergence with some additional conditions, then using a restricted discriminator family has a moment-matching effect. Additionally, we show that for objective functions that are strict adversarial divergences, convergence in the objective function implies weak convergence, thus generalizing previous results.	[Liu, Shuang; Chaudhuri, Kamalika] Univ Calif San Diego, San Diego, CA 92103 USA; [Bousquet, Olivier] Google Brain, Mountain View, CA USA	University of California System; University of California San Diego; Google Incorporated	Liu, S (corresponding author), Univ Calif San Diego, San Diego, CA 92103 USA.	shuangliu@ucsd.edu; obousquet@google.com; kamalika@cs.ucsd.edu	Jeong, Yongwook/N-7413-2016		NSF [IIS 1617157]	NSF(National Science Foundation (NSF))	We thank Iliya Tolstikhin, Sylvain Gelly, and Robert Williamson for helpful discussions. The work of KC and SL were partially supported by NSF under IIS 1617157.	Aliprantis CD., 1998, PRINCIPLES REAL ANAL; Arora S., 2017, ABS170300573 CORR; Chintala S., 2017, ABS170107875 CORR; Dales HG, 2016, CMS BOOKS MATH, P1, DOI 10.1007/978-3-319-32349-7; Dziugaite G. K., 2015, UAI; Genevay Aude, 2016, NIPS; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I., 2017, ABS170400028 CORR; Li Y., 2015, ICML; Nowozin Sebastian, 2016, P ADV NEURAL INFORM; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Sutherland D. J., 2017, ICLR; Villani C., 2009, GRUNDLEHREN MATH WIS; Wu Y., 2017, LECT NOTES INFORM TH	15	20	20	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405061
C	Wu, J; Poloczek, M; Wilson, AG; Frazier, PI		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wu, Jian; Poloczek, Matthias; Wilson, Andrew Gordon; Frazier, Peter, I			Bayesian Optimization with Gradients	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				GLOBAL OPTIMIZATION	Bayesian optimization has been successful at global optimization of expensiveto-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to find good solutions with fewer objective function evaluations. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge gradient (d-KG), which is one-step Bayes-optimal, asymptotically consistent, and provides greater one-step value of information than in the derivative-free setting. d-KG accommodates noisy and incomplete derivative information, comes in both sequential and batch forms, and can optionally reduce the computational cost of inference through automatically selected retention of a single directional derivative. We also compute the d-KG acquisition function and its gradient using a novel fast discretization-free technique. We show d-KG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, deep learning, kernel learning, and k-nearest neighbors.	[Wu, Jian; Wilson, Andrew Gordon; Frazier, Peter, I] Cornell Univ, Ithaca, NY 14853 USA; [Poloczek, Matthias] Univ Arizona, Tucson, AZ 85721 USA	Cornell University; University of Arizona	Wu, J (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.		Jeong, Yongwook/N-7413-2016		NSF [IIS-1563887, CAREER CMMI-1254298, CMMI-1536895, IIS-1247696]; AFOSR [FA9550-12-1-0200, FA9550-15-1-0038, FA9550-16-1-0046]	NSF(National Science Foundation (NSF)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	Wilson was partially supported by NSF IIS-1563887. Frazier, Poloczek, and Wu were partially supported by NSF CAREER CMMI-1254298, NSF CMMI-1536895, NSF IIS-1247696, AFOSR FA9550-12-1-0200, AFOSR FA9550-15-1-0038, and AFOSR FA9550-16-1-0046.	Ahmed M. O., 2016, NIPS BAYESOPT; [Anonymous], [No title captured]; Bingham D., 2015, OPTIMIZATION TEST PR; Brochu E, 2010, ARXIV PREPRINT ARXIV; Contal Emile, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8188, P225, DOI 10.1007/978-3-642-40988-2_15; Desautels T, 2014, J MACH LEARN RES, V15, P3873; Foreman-Mackey D, 2013, PUBL ASTRON SOC PAC, V125, P306, DOI 10.1086/670067; Forrester A., 2008, ENG DESIGN VIA SURRO, P168; Frazier P, 2009, INFORMS J COMPUT, V21, P599, DOI 10.1287/ijoc.1080.0314; Gardner JR, 2014, PR MACH LEARN RES, V32, P937; Gelbart MA, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P250; Gonzalez J, 2016, JMLR WORKSH CONF PRO, V51, P648; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Jameson A, 1999, J AIRCRAFT, V36, P36, DOI 10.2514/2.2412; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Kathuria T., 2016, ADV NEURAL INFORM PR, P4206; Koistinen OP, 2016, NANOSYST-PHYS CHEM M, V7, P925, DOI 10.17586/2220-8054-2016-7-6-925-935; LECUN Y, 1998, THE MNIST DATABASE O; LECUYER P, 1995, MANAGE SCI, V41, P738, DOI 10.1287/mnsc.41.4.738; Lizotte DJ, 2008, PRACTICAL BAYESIAN O; Maclaurin D, 2015, PR MACH LEARN RES, V37, P2113; Marmin  S., 2016, ARXIV160902700; Milgrom P, 2002, ECONOMETRICA, V70, P583, DOI 10.1111/1468-0262.00296; N. T.. L. Commission, 2016, NYC TRIP REC DAT; Osborne M, 2009, LEARNING INTELLIGENT, P1; Pedregosa F, 2016, PR MACH LEARN RES, V48; Picheny V, 2013, TECHNOMETRICS, V55, P2, DOI 10.1080/00401706.2012.707580; Plessix RE, 2006, GEOPHYS J INT, V167, P495, DOI 10.1111/j.1365-246X.2006.02978.x; Poloczek M, 2017, ADV NEURAL INFORM PR; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Scott W, 2011, SIAM J OPTIMIZ, V21, P996, DOI 10.1137/100801275; Shah A., 2015, ADV NEURAL INFORM PR, P3312; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Snyman JA, 2005, APPL OPTIM, V97, P1, DOI 10.1007/b105200; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Swersky K., 2013, ADV NEURAL INFORM PR, P2004, DOI DOI 10.1038/S41598-021-83582-6; Wang J., 2016, ARXIV160205149; Wilson A., 2013, INT C MACH LEARN, P1067; Wilson A.G., 2015, ARXIV151101870; Wilson AG, 2016, JMLR WORKSH CONF PRO, V51, P370; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775; Wu Jiajun, 2016, ADV NEURAL INFORM PR	44	20	20	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405034
C	Mankowitz, DJ; Mann, TA; Mannor, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Mankowitz, Daniel J.; Mann, Timothy A.; Mannor, Shie			Adaptive Skills Adaptive Partitions (ASAP)	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We introduce the Adaptive Skills, Adaptive Partitions (ASAP) framework that (1) learns skills (i.e., temporally extended actions or options) as well as (2) where to apply them. We believe that both (1) and (2) are necessary for a truly general skill learning framework, which is a key building block needed to scale up to lifelong learning agents. The ASAP framework can also solve related new tasks simply by adapting where it applies its existing learned skills. We prove that ASAP converges to a local optimum under natural conditions. Finally, our experimental results, which include a RoboCup domain, demonstrate the ability of ASAP to learn where to reuse skills as well as solve multiple tasks with considerably less experience than solving each task from scratch.	[Mankowitz, Daniel J.; Mann, Timothy A.; Mannor, Shie] Technion Israel Inst Technol, Haifa, Israel; [Mann, Timothy A.] Google Deepmind, London, England	Technion Israel Institute of Technology; Google Incorporated	Mankowitz, DJ (corresponding author), Technion Israel Inst Technol, Haifa, Israel.	danielm@tx.technion.ac.il; mann.timothy@acm.org; shie@ee.technion.ac.il		Mannor, Shie/0000-0003-4439-7647	European Research Council under European Union / ERC [306638]	European Research Council under European Union / ERC(European Research Council (ERC))	The research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Program (FP/2007-2013) / ERC Grant Agreement n. 306638.	Akiyama H, 2014, LECT NOTES ARTIF INT, V8371, P528, DOI 10.1007/978-3-662-44468-9_46; Ammar HB, 2015, PR MACH LEARN RES, V37, P2361; [Anonymous], 2009, ADV NEURAL INFORM PR; Bacon P.-L., 2015, NIPS DEEP REINF LEAR; Bai Aijun, 2012, AAMAS; da Silva B., 2012, PROC INT C MACH LEAR, P1443; Fu J., 2015, ARXIV PREPRINT ARXIV; Hauskrecht M., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P220; Mankowitz DJ, 2014, PR MACH LEARN RES, V32, P1350; Mann TA, 2014, PR MACH LEARN RES, V32; Mann Timothy Arthur, 2015, AAAI WORKSH; Masson Warwick, 2015, ARXIV150901644; Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003; Peters J, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P2219, DOI 10.1109/IROS.2006.282564; Precup D., 1998, Machine Learning: ECML-98. 10th European Conference on Machine Learning. Proceedings, P382, DOI 10.1007/BFb0026709; Precup D., 1997, ADV NEUR INF PROC SY, V10; Ruvolo P., 2013, P 30 INT C MACHINE L, P507; Silver D, 2012, P 29 INT C MACH LEAR, V2, P1063; Stone, 2015, DEEP RECURRENT Q LEA; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Thrun  S., 1995, LIFELONG ROBOT LEARN	22	20	20	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702097
C	Springenberg, JT; Klein, A; Falkner, S; Hutter, F		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Springenberg, Jost Tobias; Klein, Aaron; Falkner, Stefan; Hutter, Frank			Bayesian Optimization with Robust Bayesian Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Bayesian optimization is a prominent method for optimizing expensive-to-evaluate black-box functions that is widely applied to tuning the hyperparameters of machine learning algorithms. Despite its successes, the prototypical Bayesian optimization approach-using Gaussian process models-does not scale well to either many hyperparameters or many function evaluations. Attacking this lack of scalability and flexibility is thus one of the key challenges of the field. We present a general approach for using flexible parametric models (neural networks) for Bayesian optimization, staying as close to a truly Bayesian treatment as possible. We obtain scalability through stochastic gradient Hamiltonian Monte Carlo, whose robustness we improve via a scale adaptation. Experiments including multi-task Bayesian optimization with 21 tasks, parallel optimization of deep neural networks and deep reinforcement learning show the power and flexibility of this approach.	[Springenberg, Jost Tobias; Klein, Aaron; Falkner, Stefan; Hutter, Frank] Univ Freiburg, Dept Comp Sci, Freiburg, Germany	University of Freiburg	Springenberg, JT (corresponding author), Univ Freiburg, Dept Comp Sci, Freiburg, Germany.	springj@cs.uni-freiburg.de; kleinaa@cs.uni-freiburg.de; sfalkner@cs.uni-freiburg.de; fh@cs.uni-freiburg.de			European Commission [H2020-ICT-645403-ROBDREAM]; German Research Foundation (DFG) [SPP 1527, HU 1900/3-1, HU 1900/2-1]; BrainLinks-BrainTools Cluster of Excellence [EXC 1086]	European Commission(European CommissionEuropean Commission Joint Research Centre); German Research Foundation (DFG)(German Research Foundation (DFG)); BrainLinks-BrainTools Cluster of Excellence	This work has partly been supported by the European Commission under Grant no. H2020-ICT-645403-ROBDREAM, by the German Research Foundation (DFG), under Priority Programme Autonomous Learning (SPP 1527, grant HU 1900/3-1), under Emmy Noether grant HU 1900/2-1, and under the BrainLinks-BrainTools Cluster of Excellence (grant number EXC 1086).	[Anonymous], 2014, CORR; Bergstra J., 2011, P NIPS 11; Blundell C., 2015, P ICML 15; Brochu E., 2010, CORR; Chen Changyou, 2016, P AISTATS; Chen T., 2014, P ICML 14; Duane S., 1987, PHYS LETT B; Duchi J., 2011, J MACHINE LEARNING R; Eggensperger K., 2013, BAYESOPT 13; Feurer M, 2015, P AAAI 15; Gal Y., 2015, ARXIV150602142; Girolami M., 2011, J ROYAL STAT SOC B; Graves A., 2011, P ICML 11; Gu S., 2016, P ICML; He Kaiming, 2016, P CVPR 16; Hernandez-Lobato J., 2015, P ICML 15; Hutter F., 2011, LION 11; Jones D., 1998, JGO; Kingma D. P., 2015, P NIPS 15; Klein A., 2016, CORR; Korattikara A., 2015, P NIPS 15; Li Chunyuan, 2016, P AAAI 16; Lillicrap T.P., 2016, P 4 INT C LEARN REPR; Ma Y., 2015, P NIPS 15; Neal R., 1996, THESIS; Schaul T., 2013, P ICML 13; Snoek J., 2015, P ICML 15; Snoek J, 2012, P NIPS 12; Srinivas N, 2010, P ICML 10; Swersky K, 2013, P NIPS 13; Tieleman T., 2012, COURSERA NEURAL NETW; Vanschoren J., 2014, SIGKDD EXPLOR NEWSL	32	20	20	2	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700081
C	Hazan, E; Levy, KY; Shalev-Shwartz, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Hazan, Elad; Levy, Kfir Y.; Shalev-Shwartz, Shai			Beyond Convexity: Stochastic Quasi-Convex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				CONVERGENCE	Stochastic convex optimization is a basic and well studied primitive in machine learning. It is well known that convex and Lipschitz functions can be minimized efficiently using Stochastic Gradient Descent (SGD). The Normalized Gradient Descent (NGD) algorithm, is an adaptation of Gradient Descent, which updates according to the direction of the gradients, rather than the gradients themselves. In this paper we analyze a stochastic version of NGD and prove its convergence to a global minimum for a wider class of functions: we require the functions to be quasi-convex and locally-Lipschitz. Quasi-convexity broadens the concept of unimodality to multidimensions and allows for certain types of saddle points, which are a known hurdle for first-order optimization methods such as gradient descent. Locally-Lipschitz functions are only required to be Lipschitz in a small region around the optimum. This assumption circumvents gradient explosion, which is another known hurdle for gradient descent variants. Interestingly, unlike the vanilla SGD algorithm, the stochastic normalized gradient descent algorithm provably requires a minimal minibatch size.	[Hazan, Elad] Princeton Univ, Princeton, NJ 08544 USA; [Levy, Kfir Y.] Technion, Haifa, Israel; [Shalev-Shwartz, Shai] Hebrew Univ Jerusalem, Jerusalem, Israel	Princeton University; Hebrew University of Jerusalem	Hazan, E (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	ehazan@cs.princeton.edu; kfiryl@tx.technion.ac.il; shais@cs.huji.ac.il		Hazan, Elad/0000-0002-1566-3216; Levy, Kfir Yehuda/0000-0003-1236-2626	European Union [336078 - ERC-SUBLRN]; ISF [1673/14]; Intel's ICRI-CI	European Union(European Commission); ISF(Israel Science Foundation); Intel's ICRI-CI	The research leading to these results has received funding from the European Union's Seventh Framework Programme (FP7/2007-2013) under grant agreement no 336078 - ERC-SUBLRN. Shai S-Shwartz is supported by ISF no 1673/14 and by Intel's ICRI-CI.	Auer P, 1996, ADV NEUR IN, V8, P316; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Boyd S, 2004, CONVEX OPTIMIZATION; Doya, 1993, IEEE T NEURAL NETWOR, V1, P75; Goffin JL, 1996, SIAM J OPTIMIZ, V6, P638, DOI 10.1137/S1052623493258635; KALAI A. T., 2009, COLT; Ke Q, 2007, IEEE T PATTERN ANAL, V29, P1834, DOI 10.1109/TPAMI.2007.1083; Khabibullin Rustem F, 1977, ISSLED PRIKL MAT, V4, P15; Kiwiel KC, 2001, MATH PROGRAM, V90, P1, DOI 10.1007/PL00011414; Konnov IV, 2003, OPTIM METHOD SOFTW, V18, P53, DOI 10.1080/1055678031000111236; Martimort D., 2009, THEORY INCENTIVES PR; McCullagh P., 1989, GEN LINEAR MODELS, V2nd; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; POLYAK BT, 1967, DOKL AKAD NAUK SSSR+, V174, P33; Sikorski Jaroslaw, 1986, ANAL ALGORITHMS OPTI, P203; Sutskever I., 2011, P 28 INT C MACH LEAR, P1033, DOI DOI 10.1145/346152.346166; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; VARIAN HR, 1985, AM ECON REV, V75, P870; Wolfstetter E, 1999, TOPICS MICROECONOMIC; Zabotin Yaroslav Ivanovich, 1972, IZV VUZ MATEM, P27	22	20	20	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100086
C	Kuleshov, V; Liang, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kuleshov, Volodymyr; Liang, Percy			Calibrated Structured Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					In user-facing applications, displaying calibrated confidence measures-probabilities that correspond to true frequency-can be as important as obtaining high accuracy. We are interested in calibration for structured prediction problems such as speech recognition, optical character recognition, and medical diagnosis. Structured prediction presents new challenges for calibration: the output space is large, and users may issue many types of probability queries (e.g., marginals) on the structured output. We extend the notion of calibration so as to handle various subtleties pertaining to the structured setting, and then provide a simple recalibration method that trains a binary classifier to predict probabilities of interest. We explore a range of features appropriate for structured recalibration, and demonstrate their efficacy on three real-world datasets.	[Kuleshov, Volodymyr; Liang, Percy] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Kuleshov, V (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.				NSERC Canada Graduate Scholarship; Sloan Research Fellowship	NSERC Canada Graduate Scholarship(Natural Sciences and Engineering Research Council of Canada (NSERC)); Sloan Research Fellowship(Alfred P. Sloan Foundation)	This research is supported by an NSERC Canada Graduate Scholarship to the first author and a Sloan Research Fellowship to the second author.	Brier G. W., 1950, MON WEATHER REV, V78, P1, DOI [10.1175/1520-0493(1950)0782.0.co;2, DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2]; Buja A., 2005, LOSS FUNCTIONS BINAR; Coates Adam, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P561, DOI 10.1007/978-3-642-35289-8_30; DAWID AP, 1982, J AM STAT ASSOC, V77, P605, DOI 10.2307/2287720; Fischhoff B., 1982, JUDGEMENT UNCERTAINT; Foster D. P., 1998, ASYMPTOTIC CALIBRATI; Gneiting T, 2007, J R STAT SOC B, V69, P243, DOI 10.1111/j.1467-9868.2007.00587.x; HECKERMAN DE, 1992, METHOD INFORM MED, V31, P106; Jiang XQ, 2012, J AM MED INFORM ASSN, V19, P263, DOI 10.1136/amiajnl-2011-000291; Kassel R. H., 1995, THESIS; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Liang P., 2006, INT C COMP LING ASS; Menon Aditya Krishna, 2012, INT C MACH LEARN ICM; Mueller A., 2013, THESIS; Murphy A. H., 1973, Journal of Applied Meteorology, V12, P595, DOI 10.1175/1520-0450(1973)012<0595:ANVPOT>2.0.CO;2; Nguyen Khanh, 2015, P C EMP METH NAT LAN; Niculescu-Mizil Alexandru, 2005, P 22 INT C MACHINE L, P625, DOI 10.1145/1102351.1102430; Platt JC, 2000, ADV NEUR IN, P61; Seigel Mathew Stephen, 2013, THESIS; Stephenson DB, 2008, WEATHER FORECAST, V23, P752, DOI 10.1175/2007WAF2006116.1; Yu D, 2011, IEEE T AUDIO SPEECH, V19, P2461, DOI 10.1109/TASL.2011.2141988; Zhong Leon Wenliang, 2013, P 23 INT JOINT C ART, P1939	24	20	20	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100026
C	Zhou, MY; Cong, YL; Chen, B		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Zhou, Mingyuan; Cong, Yulai; Chen, Bo			The Poisson Gamma Belief Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					To infer a multilayer representation of high-dimensional count vectors, we propose the Poisson gamma belief network (PGBN) that factorizes each of its layers into the product of a connection weight matrix and the nonnegative real hidden units of the next layer. The PGBN's hidden layers are jointly trained with an upward-downward Gibbs sampler, each iteration of which upward samples Dirichlet distributed connection weight vectors starting from the first layer (bottom data layer), and then downward samples gamma distributed hidden units starting from the top hidden layer. The gamma-negative binomial process combined with a layer-wise training strategy allows the PGBN to infer the width of each layer given a fixed budget on the width of the first layer. The PGBN with a single hidden layer reduces to Poisson factor analysis. Example results on text analysis illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the PGBN, whose hidden units are imposed with correlated gamma priors, can add more layers to increase its performance gains over Poisson factor analysis, given the same limit on the width of the first layer.	[Zhou, Mingyuan] Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA; [Cong, Yulai; Chen, Bo] Xidian Univ, Natl Lab RSP, Xian, Shaanxi, Peoples R China	University of Texas System; University of Texas Austin; Xidian University	Zhou, MY (corresponding author), Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA.		Zhou, Mingyuan/AAE-8717-2021		Thousand Young Talent Program of China [61372132, NCET-13-0945]	Thousand Young Talent Program of China	M. Zhou thanks TACC for computational support. B. Chen thanks the support of the Thousand Young Talent Program of China, NSC-China (61372132), and NCET-13-0945.	Acharya A., 2015, AISTATS; Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024; Bengio Y., 2007, LARGE SCALE KERNEL M, V34, P1, DOI [DOI 10.1038/NATURE14539, 10.1016/j.asoc.2014.05.028, DOI 10.1016/J.ASOC.2014.05.028]; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Gan Z, 2015, PR MACH LEARN RES, V37, P1823; Goodfellow I.J., 2015, DEEP LEARNING; Griffiths T. L., 2004, PNAS; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Larochelle H., 2012, ADV NEURAL INFORM PR, P2708; Nair V, 2010, P 27 INT C MACHINE L, P807; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Ranganath R, 2015, JMLR WORKSH CONF PRO, V38, P762; Ranzato M.A., 2007, IEEE C COMP VIS PATT, P1; Salakhutdinov R., 2009, P 12 INT C ART INT S, P448; Salakhutdinov R, 2013, IEEE T PATTERN ANAL, V35, P1958, DOI 10.1109/TPAMI.2012.269; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251; Srivastava N., 2013, UAI; Teh Yee Whye, 2006, J AM STAT ASS; Welling M., 2004, ADV NEURAL INFORM PR, V17, P1481; Xing E. P., 2005, UAI; Zhou M., 2014, NIPS; Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211; Zhou Mingyuan, 2012, AISTATS, P1462; Zhou Mingyuan, 2015, J AM STAT ASSOC, P00	26	20	20	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100013
C	Hughes, NP; Tarassenko, L; Roberts, SJ		Thrun, S; Saul, K; Scholkopf, B		Hughes, NP; Tarassenko, L; Roberts, SJ			Markov models for automated ECG interval analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We examine the use of hidden Markov and hidden semi-Markov models for automatically segmenting an electrocardiogram waveform into its constituent waveform features. An undecimated wavelet transform is used to generate an overcomplete representation of the signal that is more appropriate for subsequent modelling. We show that the state durations implicit in a standard hidden Markov model are ill-suited to those of real ECG features, and we investigate the use of hidden semi-Markov models for improved state duration modelling.	Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England	University of Oxford	Hughes, NP (corresponding author), Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.	nph@robots.ox.ac.uk; lionel@robots.ox.ac.uk; sjrob@robots.ox.ac.uk						Figueiredo MAT, 2002, IEEE T PATTERN ANAL, V24, P381, DOI 10.1109/34.990138; Graja S, 2003, I S INTELL SIG PR, P105; Jane R, 1997, COMPUT CARDIOL, V24, P295, DOI 10.1109/CIC.1997.647889; Koski A, 1996, ARTIF INTELL MED, V8, P453, DOI 10.1016/S0933-3657(96)00352-1; Levinson S. E., 1986, Computer Speech and Language, V1, P29, DOI 10.1016/S0885-2308(86)80009-2; Mallat S., 1999, WAVELET TOUR SIGNAL, DOI 10.1016/B978-012466606-1/50008-8; MRUPHY KP, 2002, HIDDEN SEMIMARKOV MO; Pratt CM, 1996, AM HEART J, V131, P472, DOI 10.1016/S0002-8703(96)90525-6; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626	9	20	20	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						611	618						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500077
C	Murphy, K; Torralba, A; Freeman, WT		Thrun, S; Saul, K; Scholkopf, B		Murphy, K; Torralba, A; Freeman, WT			Using the forest to see the trees: A graphical model relating features, objects, and scenes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				SYSTEM	Standard approaches to object detection focus on local patches of the image, and try to classify them as background or not. We propose to use the scene context (image as a whole) as an extra source of (global) information, to help resolve local ambiguities. We present a conditional random field for jointly solving the tasks of object detection and scene classification.	MIT, AI Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Murphy, K (corresponding author), MIT, AI Lab, Cambridge, MA 02139 USA.	murphyk@ai.mit.edu; torralba@ai.mit.edu; wtf@ai.mit.edu						Agarwal S., 2002, P EUR C COMP VIS; Biedermann I., 1981, PERCEPTUAL ORG, P213, DOI [10.4324/9781315512372-8, 10.4324/9781315512372]; BOUTELL M, 2003, MULTILABEL SEMANTIC; DAVON D, 1977, COGNITIVE PSYCHOL, V9, P353; Duygulu P., 2002, P EUR C COMP VIS; Fergus R., 2003, P IEEE C COMP VIS PA; FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379; FINK M, 2003, ADV NEURAL INFO P SY; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; HARALICK RM, 1983, IEEE T PATTERN ANAL, V5, P417, DOI 10.1109/TPAMI.1983.4767411; KUMAR S, 2003, IEEE C COMP VIS PATT; Lafferty J., 2001, INT C MACH LEARN; LIENHART R, 2003, DAGM 25 PATT REC S; Papageorgiou C, 2000, INT J COMPUT VISION, V38, P15, DOI 10.1023/A:1008162616689; Platt J., 1999, ADV LARGE MARGIN CLA; Schapire R., 2001, MSRI WORKSH NONL EST; Schapire RE, 2000, MACH LEARN, V39, P135, DOI 10.1023/A:1007649029923; SINGHAL A, 2003, P IEEE C COMP VIS PA; STRAT TM, 1991, IEEE T PATTERN ANAL, V13, P1050, DOI 10.1109/34.99238; Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951; TORRALBA A, 2001, 028 MIT AI LAB; Viola Paul, 2001, PROC CVPR IEEE; [No title captured]	24	20	20	0	4	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1499	1506						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500186
C	Philipona, D		Thrun, S; Saul, K; Scholkopf, B		Philipona, D			Perception of the structure of the physical world using unknown multimodal sensors and effectors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Is there a way for an algorithm linked to an unknown body to infer by itself information about this body and the world it is in? Taking the case of space for example, is there a way for this algorithm to realize that its body is in a three dimensional world? Is it possible for this algorithm to discover how to move in a straight line? And more basically: do these questions make any sense at all given that the algorithm only has access to the very high-dimensional data consisting of its sensory inputs and motor outputs? We demonstrate in this article how these questions can be given a positive answer. We show that it is possible to make an algorithm that, by analyzing the law that links its motor outputs to its sensory inputs, discovers information about the structure of the world regardless of the devices constituting the body it is linked to. We present results from simulations demonstrating a way to issue motor orders resulting in "fundamental" movements of the body as regards the structure of the physical world.	Sony CSL, F-75005 Paris, France		Philipona, D (corresponding author), Sony CSL, 6 Rue Amyot, F-75005 Paris, France.			Nadal, Jean-Pierre/0000-0003-0022-0647				PHILIPONA D, 2003, NEURAL COMPUTATION, V15; [No title captured]; [No title captured]; [No title captured]	4	20	21	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						945	952						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500118
C	Wang, Z; Simoncelli, EP		Thrun, S; Saul, K; Scholkopf, B		Wang, Z; Simoncelli, EP			Local phase coherence and the perception of blur	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				NATURAL SCENES; STATISTICS; ENERGY; CELLS; HYPERACUITY; INFORMATION; RESPONSES; SIGNALS; VISION; IMAGES	Humans are able to detect blurring of visual images, but the mechanism by which they do so is not clear. A traditional view is that a blurred image looks "unnatural" because of the reduction in energy (either globally or locally) at high frequencies. In this paper, we propose that the disruption of local phase can provide an alternative explanation for blur perception. We show that precisely localized features such as step edges result in strong local phase coherence structures across scale and space in the complex wavelet transform domain, and blurring causes loss of such phase coherence. We propose a technique for coarse-to-fine phase prediction of wavelet coefficients, and observe that (1) such predictions are highly effective in natural images, (2) phase coherence increases with the strength of image features, and (3) blurring disrupts the phase coherence relationship in images. We thus lay the groundwork for a new theory of perceptual blur estimation, as well as a variety of algorithms for restoration and manipulation of photographic images.	NYU, Ctr Neural Sci, Howard Hughes Med Inst, New York, NY 10003 USA	Howard Hughes Medical Institute; New York University	Wang, Z (corresponding author), NYU, Ctr Neural Sci, Howard Hughes Med Inst, New York, NY 10003 USA.			Simoncelli, Eero/0000-0002-1206-527X				ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; BERGEN JR, 1988, NATURE, V333, P363, DOI 10.1038/333363a0; Daugman J, 2001, INT J COMPUT VISION, V45, P25, DOI 10.1023/A:1012365806338; Field DJ, 1997, VISION RES, V37, P3367, DOI 10.1016/S0042-6989(97)00181-8; FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379; FLEET DJ, 1990, INT J COMPUT VISION, V5, P77, DOI 10.1007/BF00056772; FLEET DJ, 1991, CVGIP-IMAG UNDERSTAN, V53, P198, DOI 10.1016/1049-9660(91)90027-M; GEISLER WS, 1984, J OPT SOC AM A, V1, P775, DOI 10.1364/JOSAA.1.000775; Graham N., 1989, VISUAL PATTERN ANAL; HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640; HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P427, DOI 10.1017/S095252380001124X; Kovesi P, 2000, PSYCHOL RES-PSYCH FO, V64, P136, DOI 10.1007/s004260000024; KRETZMER ER, 1952, AT&T TECH J, V31, P751, DOI 10.1002/j.1538-7305.1952.tb01404.x; MORRONE MC, 1988, PROC R SOC SER B-BIO, V235, P221, DOI 10.1098/rspb.1988.0073; MORRONE MC, 1987, PATTERN RECOGN LETT, V6, P303, DOI 10.1016/0167-8655(87)90013-4; OPPENHEIM AV, 1981, P IEEE, V69, P529, DOI 10.1109/PROC.1981.12022; Perona P., 1990, Proceedings. Third International Conference on Computer Vision (Cat. No.90CH2934-8), P52, DOI 10.1109/ICCV.1990.139492; POLLEN DA, 1981, SCIENCE, V212, P1409, DOI 10.1126/science.7233231; Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983; Ruderman D.L., 1994, NETWORK COMPUTATION, V5, P517, DOI DOI 10.1088/0954-898X/5/4/006; RUDERMAN DL, 1992, NEURAL COMPUT, V4, P682, DOI 10.1162/neco.1992.4.5.682; Schwartz O, 2001, NAT NEUROSCI, V4, P819, DOI 10.1038/90526; Simoncelli E, 1997, P 31 AS C SIGN SYST, P673; SIMONCELLI EP, 1992, IEEE T INFORM THEORY, V38, P587, DOI 10.1109/18.119725; TADMOR Y, 1994, VISION RES, V34, P541, DOI 10.1016/0042-6989(94)90167-8; Thomson MGA, 1999, NETWORK-COMP NEURAL, V10, P123, DOI 10.1088/0954-898X/10/2/302; Venkatesh S., 1989, P INT C IM PROC SING, P553; Webster MA, 2002, NAT NEUROSCI, V5, P839, DOI 10.1038/nn906; WESTHEIMER G, 1977, VISION RES, V17, P941, DOI 10.1016/0042-6989(77)90069-4; WIENER N, 1958, NONLINEAR PROBLEMS R	30	20	21	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1435	1442						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500178
C	Horn, D; Gottlieb, A		Dietterich, TG; Becker, S; Ghahramani, Z		Horn, D; Gottlieb, A			The method of quantum clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We propose a novel clustering method that is an extension of ideas inherent to scale-space clustering and support-vector clustering. Like the latter, it associates every data point with a vector in Hilbert space, and like the former it puts emphasis on their total sum, that is equal to the scale-space probability function. The novelty of our approach is the study of an operator in Hilbert space, represented by the Schrodinger equation of which the probability function is a solution. This Schrodinger equation contains a potential function that can be derived analytically from the probability function. We associate minima of the potential with cluster centers. The method has one variable parameter, the scale of its Gaussian kernel. We demonstrate its applicability on known data sets. By limiting the evaluation of the Schrodinger potential to the locations of data points, we can apply this method to problems in high dimensions.	Tel Aviv Univ, Raymond & Beverly Sackler Fac Exact Sci, Sch Phys & Astron, IL-69978 Tel Aviv, Israel	Tel Aviv University	Horn, D (corresponding author), Tel Aviv Univ, Raymond & Beverly Sackler Fac Exact Sci, Sch Phys & Astron, IL-69978 Tel Aviv, Israel.			Horn, David/0000-0003-2708-186X				Ben-Hur A, 2001, ADV NEUR IN, V13, P367; Blatt M, 1996, PHYS REV LETT, V76, P3251, DOI 10.1103/PhysRevLett.76.3251; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; GASIOROWICZ S, 1996, QUANTUM PHYSICS; Horn D, 2002, PHYS REV LETT, V88, DOI 10.1103/PhysRevLett.88.018702; Jain A. K., 1988, ALGORITHMS CLUSTERIN, V6; Newman C. B. D., 1998, UCI REPOSITORY MACHI; Press W.H., 1992, NUMERICAL RECIPES AR, V2nd; Ripley BD., 1996; Roberts SJ, 1997, PATTERN RECOGN, V30, P261, DOI 10.1016/S0031-3203(96)00079-9; YUILLE AL, 1986, IEEE T PATTERN ANAL, V8, P15, DOI [10.1109/34.41383, 10.1109/TPAMI.1986.4767748]	13	20	20	0	1	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						769	776						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100096
C	Herbrich, R; Graepel, T		Leen, TK; Dietterich, TG; Tresp, V		Herbrich, R; Graepel, T			A PAC-Bayesian margin bound for linear classifiers: Why SVMs work	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We present a bound on the generalisation error of linear classifiers in terms of a refined margin quantity on the training set. The result is obtained in a PAC-Bayesian framework and is based on geometrical arguments in the space of linear classifiers. The new bound constitutes an exponential improvement of the so far tightest margin bound by Shawe-Taylor et al. [8] and scales logarithmically in the inverse margin. Even in the case of less training examples than input dimensions sufficiently large margins lead to non-trivial bound values and - for maximum margins - to a vanishing complexity term. Furthermore, the classical margin is too coarse a measure for the essential quantity that controls the generalisation error: the volume ratio between the whole hypothesis space and the subset of consistent hypotheses. The practical relevance of the result lies in the fact that the well-known support vector machine is optimal w.r.t. the new bound only if the feature vectors are all of the same length. As a consequence we recommend to use SVMs on normalised feature vectors only - a recommendation that is well supported by our numerical experiments on two benchmark data sets.	Tech Univ Berlin, Dept Comp Sci, Stat Res Grp, Berlin, Germany	Technical University of Berlin	Herbrich, R (corresponding author), Tech Univ Berlin, Dept Comp Sci, Stat Res Grp, Berlin, Germany.							HERBRICH R, 2000, THESIS TU BERLIN; HERBRICH R, 1999, 9911 TR TU BERL; KEARNS MJ, 1994, J COMPUT SYST SCI, V48, P464, DOI 10.1016/S0022-0000(05)80062-5; McAllester D. A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P230, DOI 10.1145/279943.279989; Sauer N., 1972, J COMB THEORY A, V13, P145, DOI [10.1016/0097-3165(72)90019-2, DOI 10.1016/0097-3165(72)90019-2]; SCHAPIRE R, 1997, P 14 INT C MACH LEAR; SHAWETAYLOR J, 1997, NC2TR1997013 U LOND; *UCI, 1990, MACH LEARN REP; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Vapnik V., 1982, ESTIMATION DEPENDENC; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025	14	20	21	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						224	230						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800032
C	Chapelle, O; Vapnik, V; Weston, J		Solla, SA; Leen, TK; Muller, KR		Chapelle, O; Vapnik, V; Weston, J			Transductive inference for estimating values of functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We introduce an algorithm for estimating the values of a function at a set of test points x(l+1), ..., x(l+m) given a set of training points (x(1), y(1)),...,(x(l), y(l)) without estimating (as an intermediate step) the regression function. We demonstrate that this direct (transductive) way for estimating values of the regression (or classification in pattern recognition) can be more accurate than the traditional one based on two steps, first estimating the function and then calculating the values of this function at the points of interest.	AT&T Res Labs, Red Bank, NJ USA	AT&T	Chapelle, O (corresponding author), AT&T Res Labs, Red Bank, NJ USA.							HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI 10.1080/00401706.1970.10488634; Luntz A., 1969, TECHNICHESKAYA KIBER; RATSCH G, 1998, TR9821 U LOND ROYAL; Saunders C., 1998, P 15 INT C MACH LEAR, P515; Vapnik V., 1982, ESTIMATION DEPENDENC; VAPNIK V, 1977, METHOD PATTERN RECOG	6	20	20	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						421	427						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700060
C	Hollmen, J; Tresp, V		Kearns, MS; Solla, SA; Cohn, DA		Hollmen, J; Tresp, V			Call-based fraud detection in mobile communication networks using a hierarchical regime-switching model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Fraud causes substantial losses to telecommunication carriers. Detection systems which automatically detect illegal use of the network can be used to alleviate the problem. Previous approaches worked on features derived from the call patterns of individual users. In this paper we present a call-based detection system based on a hierarchical regime-switching model. The detection problem is formulated as an inference problem on the regime probabilities. Inference is implemented by applying the junction tree algorithm to the underlying graphical model. The dynamics are learned from data using the EM algorithm and subsequent discriminative training. The methods are assessed using fraud data from a real mobile communication network.	Aalto Univ, Lab Comp & Informat Sci, Helsinki 02015, Finland	Aalto University	Hollmen, J (corresponding author), Aalto Univ, Lab Comp & Informat Sci, POB 5400, Helsinki 02015, Finland.	Jaakko.Hollmen@hut.fi; Volker.Tresp@mchp.siemens.de						BARSON P, 1996, NEURAL NETWORK WORLD, V6; BENGIO Y, 1996, 1049 U MONTR; BURGE P, 1997, P ACTS MOB TEL SUMM; Hamilton J.D., 1994, TIME SERIES ANAL, DOI 10.2307/j.ctv14jx6sm; Jensen Finn V., 1996, INTRO BAYESIAN NETWO; JORDAN MI, 1997, P 1996 C NIPS 9, P501; KIM CJ, 1994, J ECONOMETRICS, V60, P1, DOI 10.1016/0304-4076(94)90036-1; PEQUENON KA, 1997, TELECOMMUNICATIONS, V31, P59; Provost F., 1997, J DATA MINING KNOWLE, V1, P1; Taniguchi M, 1998, INT CONF ACOUST SPEE, P1241, DOI 10.1109/ICASSP.1998.675496	10	20	20	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						889	895						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700125
C	Berry, MJ; Meister, M		Jordan, MI; Kearns, MJ; Solla, SA		Berry, MJ; Meister, M			Refractoriness and neural precision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					The relationship between a neuron's refractory period and the precision of its response to identical stimuli was investigated. We constructed a model of a spiking neuron that combines probabilistic firing with a refractory period. For realistic refractoriness, the model closely reproduced both the average firing rate and the response precision of a retinal ganglion cell. The model is based on a "free" firing rate, which exists in the absence of refractoriness. This function may be a better description of a spiking neuron's response than the peri-stimulus time histogram.	Harvard Univ, Dept Mol & Cellular Biol, Cambridge, MA 02138 USA	Harvard University	Berry, MJ (corresponding author), Harvard Univ, Dept Mol & Cellular Biol, Cambridge, MA 02138 USA.			Meister, Markus/0000-0003-2136-6506					0	20	21	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						110	116						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700016
C	Hansen, EA		Jordan, MI; Kearns, MJ; Solla, SA		Hansen, EA			An improved policy iteration algorithm for partially observable MDPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					A new policy iteration algorithm for partially observable Markov decision processes is presented that is simpler and more efficient than an earlier policy iteration algorithm of Sondik (1971,1978). The key simplification is representation of a policy as a finite-state controller. This representation makes policy evaluation straightforward. The paper's contribution is to show that the dynamic-programming update used in the policy improvement step can be interpreted as the transformation of a finite-state controller into an improved finite-state controller. The new algorithm consistently outperforms value iteration as an approach to solving infinite-horizon problems.	Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Hansen, EA (corresponding author), Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.								0	20	20	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						1015	1021						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700143
C	Hafliger, P; Mahowald, M; Watts, L		Mozer, MC; Jordan, MI; Petsche, T		Hafliger, P; Mahowald, M; Watts, L			A spike based learning neuron in analog VLSI	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Many popular learning rules are formulated in terms of continuous, analog inputs and outputs. Biological systems, however, use action potentials, which are digital-amplitude events that encode analog information in the inter-event interval. Action-potential representations are now being used to advantage in neuromorphic VLSI systems as well. We report on a simple learning rule, based on the Riccati equation described by Kohonen [1], modified for action-potential neuronal outputs. We demonstrate this learning rule in an analog VLSI chip that uses volatile capacitive storage for synaptic weights. We show that our time-dependent learning rule is sufficient to achieve approximate weight normalization and can detect temporal correlations in spike trains.			Hafliger, P (corresponding author), UNIV ZURICH,ETHZ,INST NEUROINFORMAT,GLORIASTR 32,CH-8006 ZURICH,SWITZERLAND.		Häfliger, Philipp/G-6032-2012	Häfliger, Philipp/0000-0001-5940-9976					0	20	20	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						692	698						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00098
C	Maass, W		Touretzky, DS; Mozer, MC; Hasselmo, ME		Maass, W			On the computational power of noisy spiking neurons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						GRAZ TECH UNIV,INST THEORET COMP SCI,A-8010 GRAZ,AUSTRIA	Graz University of Technology									0	20	21	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						211	217						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00030
C	MUNRO, PW		MOODY, JE; HANSON, SJ; LIPPMANN, RP		MUNRO, PW			REPEAT UNTIL BORED - A PATTERN SELECTION STRATEGY	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	20	20	0	1	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1001	1008						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00123
C	Yang, YY; Rashtchian, C; Zhang, HY; Salakhutdinov, R; Chaudhuri, K		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Yang, Yao-Yuan; Rashtchian, Cyrus; Zhang, Hongyang; Salakhutdinov, Ruslan; Chaudhuri, Kamalika			A Closer Look at Accuracy vs. Robustness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Current methods for training robust networks lead to a drop in test accuracy, which has led prior works to posit that a robustness-accuracy tradeoff may be inevitable in deep learning. We take a closer look at this phenomenon and first show that real image datasets are actually separated. With this property in mind, we then prove that robustness and accuracy should both be achievable for benchmark datasets through locally Lipschitz functions, and hence, there should be no inherent tradeoff between robustness and accuracy. Through extensive experiments with robustness methods, we argue that the gap between theory and practice arises from two limitations of current methods: either they fail to impose local Lipschitzness or they are insufficiently generalized. We explore combining dropout with robust training methods and obtain better generalization. We conclude that achieving robustness and accuracy in practice may require using methods that impose local Lipschitzness and augmenting them with deep learning generalization techniques.(1)	[Yang, Yao-Yuan; Rashtchian, Cyrus; Chaudhuri, Kamalika] Univ Calif San Diego, La Jolla, CA 92093 USA; [Zhang, Hongyang] Toyota Technol Inst Chicago, Chicago, IL USA; [Salakhutdinov, Ruslan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	University of California System; University of California San Diego; Toyota Technological Institute - Chicago; Carnegie Mellon University	Yang, YY (corresponding author), Univ Calif San Diego, La Jolla, CA 92093 USA.	yay005@eng.uscd.edu; crashtchian@eng.ucsd.edu; hongyanz@ttic.edu; rsalakhu@cs.cmu.edu; kamalika@cs.ucsd.edu			NSF [CIF 1719133, CNS 1804829, IIS 1617157, IIS1763562]; Defense Advanced Research Projects Agency [HR00112020003]; ONR [N000141812861]	NSF(National Science Foundation (NSF)); Defense Advanced Research Projects Agency(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); ONR(Office of Naval Research)	Kamalika Chaudhuri and Yao-Yuan Yang thank NSF under CIF 1719133, CNS 1804829 and IIS 1617157 for support. Hongyang Zhang was supported in part by the Defense Advanced Research Projects Agency under cooperative agreement HR00112020003. The views expressed in this work do not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. Approved for public release; distribution is unlimited. This work was also supported in part by NSF IIS1763562 and ONR Grant N000141812861.	Alayrac J., 2019, NEURIPS, V32, P12192; Anil Cem, 2019, INT C MACH LEARN; [Anonymous], 2016, CORR; Awasthi Pranjal, 2019, ADV NEURAL INFORM PR, P13737; Bhattacharjee Robi, 2020, ARXIV200306121; Bubeck S., 2018, ARXIV180510204; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Carmon Y, 2019, ADV NEUR IN, V32; Chen TL, 2020, PROC CVPR IEEE, P696, DOI 10.1109/CVPR42600.2020.00078; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cohen J, 2019, PR MACH LEARN RES, V97; Fawzi A., 2018, ADV NEURAL INFORM PR, P1186; Fernando Chrisantha, 2017, PATHNET EVOLUTION CH; Finlay C., 2019, ARXIV190511468; Garg S, 2019, IJCCI: PROCEEDINGS OF THE 11TH INTERNATIONAL JOINT CONFERENCE ON COMPUTATIONAL INTELLIGENCE, P131, DOI 10.5220/0007948201310138; Gilmer J., 2018, INT C LEARN REPR WOR; Goodfellow I. J., 2015, P ICLR; Gowal Sven, 2019, ARXIV191009338; Guo Minghao, 2019, ARXIV191110695; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hein M., 2017, ADV NEURAL INFORM PR, V30, P2266; Huster T., 2018, JOINT EUR C MACH LEA, P16; King DB, 2015, ACS SYM SER, V1214, P1; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kurakin Alexey, 2017, INT C LEARNING REPRE; Li B., 2018, ARXIV180903113, P1; Li Yiming, 2020, ARXIV200306974; Liu Yanpei, 2017, ICLR; Lowd D., 2005, P 11 ACM SIGKDD INT, P641, DOI DOI 10.1145/1081870.1081950; Madry Aleksander, 2018, ICLR; Netzer Y., 2011, P NIPS WORKSH DEEP L; Ororbia AG, 2017, NEURAL COMPUT, V29, P867, DOI 10.1162/NECO_a_00928; Papernot Nicolas, 2015, ARXIV151104508; Pinot Rafael, 2019, ARXIV190201148; Pydi Muni Sreenivas, 2019, ARXIV191202794; Qian Haifeng, 2018, ABS180207896 ARXIV; Qin C., 2019, ADV NEURAL INFORM PR; Raghunathan A., 2019, ARXIV190606032; Raghunathan A., 2020, P 37 INT C MACH LEAR; Richardson Eitan, 2020, ARXIV200208859; Ross A.S., 2017, ARXIV171109404; Salman Hadi, 2019, ADV NEURAL INFORM PR; Saxena S., 2016, ABS160602492 CORR; Schmidt L, 2018, ADV NEUR IN, V31; Sehwag Vikash, 2019, P 12 ACM WORKSH ART, P105, DOI [DOI 10.1145/3338501.3357372, 10.1145/3338501.3357372]; Shafahi Ali, 2019, ADVERSARIAL TRAINING; Shaham U, 2018, NEUROCOMPUTING, V307, P195, DOI 10.1016/j.neucom.2018.04.027; Singh G, 2018, ADV NEUR IN, V31; Sinha Aman, 2018, ICLR; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Su D, 2018, LECT NOTES COMPUT SC, V11216, P644, DOI 10.1007/978-3-030-01258-8_39; Turner A., 2019, INT C LEARN REPR; von Luxburg U, 2004, J MACH LEARN RES, V5, P669; Wang YZ, 2018, PR MACH LEARN RES, V80; Weng Tsui-Wei, 2018, ARXIV180110578; Wong Eric, 2020, ARXIV200211569; Xu H, 2009, J MACH LEARN RES, V10, P1485; Yang Yao-Yuan, 2020, AISTATS; Zhai R., 2019, ARXIV190600555; Zhang Han, 2019, INT C MACH LEARN; Zhang Jingfeng, 2020, ARXIV200211242; Zhang Jingfeng, 2019, ARXIV191108696; Zhang Xiao, 2020, ARXIV200300378; Zhou J, 2021, ALGEBR REPRESENT TH, V24, P1221, DOI 10.1007/s10468-020-09986-6; Zoph B, ARXIV161101578	68	19	19	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													14	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000019
C	Bietti, A; Mairal, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bietti, Alberto; Mairal, Julien			On the Inductive Bias of Neural Tangent Kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					State-of-the-art neural networks are heavily over-parameterized, making the optimization algorithm a crucial ingredient for learning predictive models with good generalization properties. A recent line of work has shown that in a certain overparameterized regime, the learning dynamics of gradient descent are governed by a certain kernel obtained at initialization, called the neural tangent kernel. We study the inductive bias of learning in such a regime by analyzing this kernel and the corresponding function space (RKHS). In particular, we study smoothness, approximation, and stability properties of functions with finite norm, including stability to image deformations in the case of convolutional networks, and compare to other known kernels for similar architectures.	[Bietti, Alberto; Mairal, Julien] Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France	Centre National de la Recherche Scientifique (CNRS); Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); Inria	Bietti, A (corresponding author), Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France.	alberto.bietti@inria.fr; julien.mairal@inria.fr	Mairal, Julien/AAL-5611-2021		ERC [714381]; ANR 31A MIAI@Grenoble Alpes; MSR-Inria joint centre	ERC(European Research Council (ERC)European Commission); ANR 31A MIAI@Grenoble Alpes; MSR-Inria joint centre	This work was supported by the ERC grant number 714381 (SOLARIS project), the ANR 31A MIAI@Grenoble Alpes, and by the MSR-Inria joint centre. The authors thank Francis Bach and L~na'fc Chizat for useful discussions.	Allen-Zhu Z., 2019, P INT C MACH LEARN I; Allen- Zhu Z., 2019, ADV NEURAL INFORM PR; Arora S., 2019, ADV NEURAL INFORM PR; Arora S., 2019, P INT C MACH LEARN I; Atkinson K, 2012, LECT NOTES MATH, V2044, P1, DOI 10.1007/978-3-642-25983-8; Bach F., 2017, J MACH LEARN RES, V18, p1 629 681; Bartlett P. L., 2019, ARXIV190611300; Basri R., 2019, ADV NEURAL INFORM PR; Belkin M., 2018, P INT C MACH LEARN I; Bietti A, 2019, J MACH LEARN RES, V20; Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; Cao Yuan, 2019, ADV NEURAL INFORM PR; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Chizat L., 2018, ADV NEURAL INFORM PR; Chizat Lenaic, 2019, ADV NEURAL INFORM PR; Cho Y., 2009, NIPS; Cucker F, 2002, B AM MATH SOC, V39, P1; Daniely A., 2016, ADV NEURAL INFORM PR; Du S. S., 2019, P INT C MACH LEARN I; Du S. S., 2019, P INT C LEARN REPR I; Efthimiou C., 2014, SPHERICAL HARMONICS; Fischer Simon, 2017, ARXIV170207254; Garriga-Alonso A., 2019, P INT C LEARN REPR I; Ghorbani B., 2019, ARXIV190412191; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Jacot A., 2018, ADV NEURAL INFORM PR; Lee J., 2019, ADV NEURAL INFORM PR; Lee J., 2018, P INT C LEARN REPR I; Liang Yingyu, 2018, ADV NEURAL INFORM PR; Loosli Gaelle, 2007, LARGE SCALE KERNEL M, V6; Mairal J., 2016, ADV NEURAL INFORM PR; Mairal J., 2014, ADV NEURAL INFORM PR; Mallat S, 2012, COMMUN PUR APPL MATH, V65, P1331, DOI 10.1002/cpa.21413; Mei S., 2019, C LEARN THEOR COLT; Mei S, 2018, P NATL ACAD SCI USA, V115, pE7665, DOI 10.1073/pnas.1806579115; Neyshabur B., 2015, P INT C LEARN REPR I; Novak R., 2019, P INT C LEARN REPR I; Rahimi A, 2007, NEURAL INFORM PROCES; Rakhlin T, 2019, ANN STAT; Rudi A., 2017, ADV NEURAL INFORM PR; SAITOH S, 1997, INTEGRAL TRANSFORMS, V369; Savarese P, 2019, C LEARN THEOR COLT; Scholkopf B., 2001, LEARNING KERNELS SUP; Smola A. J., 2001, ADV NEURAL INFORM PR; Soudry D., J MACHINE LEARNING R; Williams F., 2019, ADV NEURAL INFORM PR; Xie B., 2017, P INT C ART INT STAT; Yang G., 2019, ARXIV190204760; Zhang Chiyuan, 2019, ARXIV190201996; Zou DF, 2020, MACH LEARN, V109, P467, DOI 10.1007/s10994-019-05839-6	53	19	19	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904052
C	Chizat, L; Oyallon, E; Bach, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chizat, Lenaic; Oyallon, Edouard; Bach, Francis			On Lazy Training in Differentiable Programming	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In a series of recent theoretical works, it was shown that strongly over-parameterized neural networks trained with gradient-based methods could converge exponentially fast to zero training loss, with their parameters hardly varying. In this work, we show that this "lazy training" phenomenon is not specific to over-parameterized neural networks, and is due to a choice of scaling, often implicit, that makes the model behave as its linearization around the initialization, thus yielding a model equivalent to learning with positive-definite kernels. Through a theoretical analysis, we exhibit various situations where this phenomenon arises in non-convex optimization and we provide bounds on the distance between the lazy and linearized optimization paths. Our numerical experiments bring a critical note, as we observe that the performance of commonly used non-linear deep convolutional neural networks in computer vision degrades when trained in the lazy regime. This makes it unlikely that "lazy training" is behind the many successes of neural networks in difficult high dimensional tasks.	[Chizat, Lenaic] Univ Paris Sud, CNRS, Orsay, France; [Oyallon, Edouard] INRIA, Cent Supelec, Gif Sur Yvette, France; [Bach, Francis] PSL Res Univ, ENS, INRIA, Paris, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay; Inria; UDICE-French Research Universities; Universite Paris Saclay; Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Chizat, L (corresponding author), Univ Paris Sud, CNRS, Orsay, France.	lenaic.chizat@u-psud.fr; edouard.oyallon@centralesupelec.fr; francis.bach@inria.fr			Region Ile-de-France; European Research Council [SEQUOIA 724063]; GPU donation from NVIDIA	Region Ile-de-France(Region Ile-de-France); European Research Council(European Research Council (ERC)European Commission); GPU donation from NVIDIA	We acknowledge supports from grants from Region Ile-de-France and the European Research Council (grant SEQUOIA 724063). Edouard Oyallon was supported by a GPU donation from NVIDIA. We thank Alberto Bietti for interesting discussions and Brett Bernstein for noticing an error in a previous version of this paper.	Allen-Zhu Z, 2019, PR MACH LEARN RES, V97; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arora Sanjeev, 2019, ADV NEURAL INFORM PR; Bai HH, 2012, 2012 12TH INTERNATIONAL CONFERENCE ON HYBRID INTELLIGENT SYSTEMS (HIS), P75, DOI 10.1109/HIS.2012.6421312; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Boutaib Youness, 2015, ARXIV151007614; Carratino L., 2018, ADV NEURAL INFORM PR, P10212; Chen BB, 2018, INT CONF COMMUN SYST, P167; Chizat L., 2018, ADV NEURAL INFORM PR; Cho Y., 2009, NIPS, P342; Du S.S., 2019, ARXIV191003016; Du Simon S., 2019, INT C MACH LEARN ICM; Gautschi W., 1997, NUMERICAL ANAL; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Hanin B, 2018, ADV NEURAL INFORM PR, P571; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Jacot A., 2018, ADV NEURAL INFORM PR; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kushner Harold, 2003, STOCHASTIC APPROXIMA, V35; Lee J. M, 2000, INTRO SMOOTH MANIFOL, DOI [10.1007/978-0-387-21752-9, DOI 10.1007/978-0-387-21752-9]; Lee Jaehoon, 2018, INT C LEARN REPR; Matthews Alexander G. de G., 2018, P 7 INT C LEARN REPR, V3; Mei S, 2018, P NATL ACAD SCI USA, V115, pE7665, DOI 10.1073/pnas.1806579115; Oyallon E, 2015, PROC CVPR IEEE, P2865, DOI 10.1109/CVPR.2015.7298904; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Recht B, 2019, PR MACH LEARN RES, V97; Rotskoff Grant M., 2018, ADV NEURAL INFORM PR; Scieur D., 2017, ADV NEURAL INFORM PR, P1109; Sirignano J., 2019, STOCHASTIC PROCESSES; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zeng HQ, 2017, PROC INT CONF RECON; Zou DF, 2020, MACH LEARN, V109, P467, DOI 10.1007/s10994-019-05839-6	37	19	19	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302088
C	Dalca, AV; Rakic, M; Guttag, J; Sabuncu, MR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dalca, Adrian V.; Rakic, Marianne; Guttag, John; Sabuncu, Mert R.			Learning Conditional Deformable Templates with Convolutional Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DIFFEOMORPHIC IMAGE REGISTRATION; OPTICAL-FLOW	We develop a learning framework for building deformable templates, which play a fundamental role in many image analysis and computational anatomy tasks. Conventional methods for template creation and image alignment to the template have undergone decades of rich technical development. In these frameworks, templates are constructed using an iterative process of template estimation and alignment, which is often computationally very expensive. Due in part to this shortcoming, most methods compute a single template for the entire population of images, or a few templates for specific sub-groups of the data. In this work, we present a probabilistic model and efficient learning strategy that yields either universal or conditional templates, jointly with a neural network that provides efficient alignment of the images to these templates. We demonstrate the usefulness of this method on a variety of domains, with a special focus on neuroimaging. This is particularly useful for clinical applications where a pre-existing template does not exist, or creating a new one with traditional methods can be prohibitively expensive. Our code and atlases are available online as part of the VoxelMorph library at http://voxelmorph.csail.mit.edu.	[Dalca, Adrian V.] MIT, CSAIL, MGH, HMS, Cambridge, MA 02139 USA; [Rakic, Marianne] MIT, ETH, CSAIL, D ITET, Cambridge, MA 02139 USA; [Guttag, John] MIT, CSAIL, Cambridge, MA 02139 USA; [Sabuncu, Mert R.] Cornell, ECE & BME, Ithaca, NY 14853 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); Cornell University	Dalca, AV (corresponding author), MIT, CSAIL, MGH, HMS, Cambridge, MA 02139 USA.	adalca@mit.edu; mrakic@mit.edu; guttag@mit.edu; msabuncu@cornell.edu	Sabuncu, Mert Rory/ABE-2284-2021		NIH [R01LM012719, R01AG053949, 1R21AG050122]; NSF CAREER [1748377]; NSF NeuroNex Grant [1707312]; Wistron Corporation	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF NeuroNex Grant; Wistron Corporation	This research was funded by NIH grants R01LM012719, R01AG053949, and 1R21AG050122, NSF CAREER 1748377, NSF NeuroNex Grant 1707312, and Wistron Corporation.	Abdulla WH, 2003, TENCON IEEE REGION, P1576; Acuna C, 2012, FRONT SYST NEUROSCI, V6, DOI 10.3389/fnsys.2012.00062; Ahmadi A, 2016, IEEE IMAGE PROC, P1629, DOI 10.1109/ICIP.2016.7532634; Allassonniere S, 2007, J R STAT SOC B, V69, P3; [Anonymous], 2017, CVPR; [Anonymous], 2019, IEEE T MED IMAGING; Arjovsky M., 2017, ARXIV170107875; Ashburner J, 2007, NEUROIMAGE, V38, P95, DOI 10.1016/j.neuroimage.2007.07.007; Avants BB, 2008, MED IMAGE ANAL, V12, P26, DOI 10.1016/j.media.2007.06.004; Ba J., 2017, P 3 INT C LEARN REPR; BAJCSY R, 1989, COMPUT VISION GRAPH, V46, P1, DOI 10.1016/S0734-189X(89)80014-3; Balakrishnan G., 2018, IEEE T MED IMAGING; Balakrishnan G, 2018, PROC CVPR IEEE, P9252, DOI 10.1109/CVPR.2018.00964; Beg MF, 2005, INT J COMPUT VISION, V61, P139, DOI 10.1023/B:VISI.0000043755.93987.aa; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Cao Xiaohuan, 2017, Med Image Comput Comput Assist Interv, V10433, P300, DOI 10.1007/978-3-319-66182-7_35; Cao Y, 2005, IEEE T MED IMAGING, V24, P1216, DOI 10.1109/TMI.2005.853923; Ceritoglu C, 2009, NEUROIMAGE, V47, P618, DOI 10.1016/j.neuroimage.2009.04.057; Chen X, 2016, ADV NEUR IN, V29; Dagley A., 2015, NEUROIMAGE; Dalca AV, 2019, MED IMAGE ANAL, V57, P226, DOI 10.1016/j.media.2019.07.006; Dalca AV, 2018, PROC CVPR IEEE, P9290, DOI 10.1109/CVPR.2018.00968; Dalca AV, 2018, LECT NOTES COMPUT SC, V11070, P729, DOI 10.1007/978-3-030-00928-1_82; Dalca AV, 2016, LECT NOTES COMPUT SC, V9993, P60, DOI 10.1007/978-3-319-47118-1_8; Davis B, 2004, 2004 2ND IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING: MACRO TO NANO, VOLS 1 AND 2, P173; Davis BC, 2010, INT J COMPUT VISION, V90, P255, DOI 10.1007/s11263-010-0367-1; de Vos BD, 2017, LECT NOTES COMPUT SC, V10553, P204, DOI 10.1007/978-3-319-67558-9_24; Di Martino A, 2014, MOL PSYCHIATR, V19, P659, DOI 10.1038/mp.2013.78; Dosovitskiy A., 2015, FLOWNET LEARNING OPT; Felzenszwalb P.F., 2007, P 2007 IEEE C COMP V, P1, DOI [DOI 10.1109/CVPR.2007.383018, 10.1109/CVPR.2007.383018]; Fischl B, 2012, NEUROIMAGE, V62, P774, DOI 10.1016/j.neuroimage.2012.01.021; Glocker B, 2008, MED IMAGE ANAL, V12, P731, DOI 10.1016/j.media.2008.03.006; Gollub RL, 2013, NEUROINFORMATICS, V11, P367, DOI 10.1007/s12021-013-9184-3; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Habas PA, 2009, LECT NOTES COMPUT SC, V5761, P289, DOI 10.1007/978-3-642-04268-3_36; Hernandez M, 2009, INT J COMPUT VISION, V85, P291, DOI 10.1007/s11263-009-0219-z; Higgins Irina, 2017, ICLR 2017, P2; Holmes AJ, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.31; Horn B., 1980, DETERMINING OPTICAL; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Jain AK, 1996, IEEE T PATTERN ANAL, V18, P267, DOI 10.1109/34.485555; Jongejan Jonas, 2020, QUICK DRAW AI EXPT; Joshi S, 2004, NEUROIMAGE, V23, pS151, DOI 10.1016/j.neuroimage.2004.07.068; Joshi SC, 2000, IEEE T IMAGE PROCESS, V9, P1357, DOI 10.1109/83.855431; Kim J, 2013, PROC CVPR IEEE, P2307, DOI 10.1109/CVPR.2013.299; Kokkinos I, 2012, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2012.6247671; Krebs J., 2018, DEEP LEARNING MED IM; Krebs J., 2017, LNCS, P344, DOI DOI 10.1007/978-3-319-66182-7_40; Kuklisova-Murgasova M, 2011, NEUROIMAGE, V54, P2750, DOI 10.1016/j.neuroimage.2010.10.019; Ma J, 2008, NEUROIMAGE, V42, P252, DOI 10.1016/j.neuroimage.2008.03.056; Makhzani A., 2015, ICLR WORKSH, DOI DOI 10.3389/FPHAR.2020.565644; Marcus DS, 2007, J COGNITIVE NEUROSCI, V19, P1498, DOI 10.1162/jocn.2007.19.9.1498; Marek K, 2011, PROG NEUROBIOL, V95, P629, DOI 10.1016/j.pneurobio.2011.09.005; Miller MI, 2005, P NATL ACAD SCI USA, V102, P9685, DOI 10.1073/pnas.0503892102; Modat M., 2012, 2012 IEEE Workshop on Mathematical Methods in Biomedical Image Analysis (MMBIA), P145, DOI 10.1109/MMBIA.2012.6164745; Modat M, 2014, LECT NOTES COMPUT SC, V8675, P57, DOI 10.1007/978-3-319-10443-0_8; Mueller Susanne G, 2005, Alzheimers Dement, V1, P55, DOI 10.1016/j.jalz.2005.06.003; Oishi K, 2009, NEUROIMAGE, V46, P486, DOI 10.1016/j.neuroimage.2009.01.002; Roh M.-M., 2017, INT C MEDICAL IMAGE, P266, DOI DOI 10.1007/978-3-319-66182-7_31; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sabuncu MR, 2009, IEEE T MED IMAGING, V28, P1473, DOI 10.1109/TMI.2009.2017942; Sokooti H, 2017, LECT NOTES COMPUTER, V10433, P232, DOI 10.1007/978-3-319-66182-7_27; Stoll C., 2006, P SGP, P27; Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939; Thirion J P, 1998, Med Image Anal, V2, P243, DOI 10.1016/S1361-8415(98)80022-4; Tran D., 2016, P IEEE C COMP VIS PA, P17, DOI DOI 10.1109/CVPRW.2016.57; Vercauteren T, 2009, NEUROIMAGE, V45, pS61, DOI 10.1016/j.neuroimage.2008.10.040; Weber Ron A Shapira, 2019, NEURIPS NEURAL INFOR; Yang X, 2017, NEUROIMAGE, V158, P378, DOI 10.1016/j.neuroimage.2017.07.008; Yeo BTT, 2010, IEEE T MED IMAGING, V29, P1424, DOI 10.1109/TMI.2010.2049497; Yu JJ, 2016, LECT NOTES COMPUT SC, V9915, P3, DOI 10.1007/978-3-319-49409-8_1; Yurtman A, 2014, COMPUT METH PROG BIO, V117, P189, DOI 10.1016/j.cmpb.2014.07.003; Zhang MM, 2017, LECT NOTES COMPUT SC, V10265, P559, DOI 10.1007/978-3-319-59050-9_44	74	19	19	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300073
C	Helwegen, K; Widdicombe, J; Geiger, L; Liu, ZC; Cheng, KT; Nusselder, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Helwegen, Koen; Widdicombe, James; Geiger, Lukas; Liu, Zechun; Cheng, Kwang-Ting; Nusselder, Roeland			Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Optimization of Binarized Neural Networks (BNNs) currently relies on real-valued latent weights to accumulate small update steps. In this paper, we argue that these latent weights cannot be treated analogously to weights in real-valued networks. Instead their main role is to provide inertia during training. We interpret current methods in terms of inertia and provide novel insights into the optimization of BNNs. We subsequently introduce the first optimizer specifically designed for BNNs, Binary Optimizer (Bop), and demonstrate its performance on CIFAR-10 and ImageNet. Together, the redefinition of latent weights as inertia and the introduction of Bop enable a better understanding of BNN optimization and open up the way for further improvements in training methodologies for BNNs. Code is available at: https://github.com/plumerai/rethinking-bnn-optimization.	[Helwegen, Koen; Widdicombe, James; Geiger, Lukas; Nusselder, Roeland] Plumerai Res, Amsterdam, Netherlands; [Liu, Zechun; Cheng, Kwang-Ting] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China	Hong Kong University of Science & Technology	Helwegen, K (corresponding author), Plumerai Res, Amsterdam, Netherlands.	koen@plumerai.com; james@plumerai.com; lukas@plumerai.com; zliubq@connect.ust.hk; timcheng@ust.hk; roeland@plumerai.com		Cheng, Kwang-Ting Tim/0000-0002-3885-4912				Abadi M, 2015, P 12 USENIX S OPERAT; Alizadeh M., 2019, INT C LEARN REPR; Anderson A.G., 2017, ABS170507199 CORR; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bengio Yoshua, 2013, ARXIV13083432; Bethge, 2019, ARXIV190608637; Bulat Adrian, 2019, ARXIV190405868; Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123, DOI DOI 10.1109/TWC.2016.2633262; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Ding RZ, 2019, PROC CVPR IEEE, P11400, DOI 10.1109/CVPR.2019.01167; Goh G., 2017, DISTILL, V2, pe6, DOI DOI 10.23915/DISTILL.00006; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Graham Benjamin, 2014, ARXIV14096070; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; Hinton G, 2012, NEURAL NETWORKS MACH; Hinton G., 2015, ARXIV150302531; Hubara I, 2016, ADV NEUR IN, V29; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Li H, 2017, ADV NEUR IN, V30; Lin Xiaofan, 2017, NEURIPS, V1, P2; Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44; Loshchilov I., 2017, ARXIV171105101; Merolla P., 2016, ARXIV160601981; Mishra A., 2017, ARXIV171105852; Peters Jorn W. T., 2018, ARXIV180903368; Polino A., 2018, ARXIV180205668; Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Zhou S., 2016, ARXIV160606160; Zhu S., 2018, ARXIV180607550; Zhuang Bingbing, 2019, CVPR	36	19	19	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307054
C	Ingraham, J; Garg, VK; Barzilay, R; Jaakkola, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ingraham, John; Garg, Vikas K.; Barzilay, Regina; Jaakkola, Tommi			Generative models for graph-based protein design	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				FOLD	Engineered proteins offer the potential to solve many problems in biomedicine, energy, and materials science, but creating designs that succeed is difficult in practice. A significant aspect of this challenge is the complex coupling between protein sequence and 3D structure, with the task of finding a viable design often referred to as the inverse protein folding problem. In this work, we introduce a conditional generative model for protein sequences given 3D structures based on graph representations. Our approach efficiently captures the complex dependencies in proteins by focusing on those that are long-range in sequence but local in 3D space. This graph-based approach improves in both speed and reliability over conventional and other neural network-based approaches, and takes a step toward rapid and targeted biomolecular design with the aid of deep generative models.	[Ingraham, John; Garg, Vikas K.; Barzilay, Regina; Jaakkola, Tommi] MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Ingraham, J (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA.	ingraham@csail.mit.edu; vgarg@csail.mit.edu; regina@csail.mit.edu; tommi@csail.mit.edu			Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS) consortium	Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS) consortium	We thank members of the MIT MLPDS consortium, the MIT NLP group, and the reviewers for helpful feedback. This work was supported by the Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS) consortium.	Alford RF, 2017, J CHEM THEORY COMPUT, V13, P3031, DOI 10.1021/acs.jctc.7b00125; Alley E.C., 2019, NATURE METHODS, DOI [10.1101/589333, DOI 10.1101/589333]; AlQuraishi M, 2019, CELL SYST, V8, P292, DOI 10.1016/j.cels.2019.03.006; Anand N., 2018, P 32 INT C NEUR INF P 32 INT C NEUR INF, P7505; Anand Namrata, 2019, ICLR WORKSH DEEP GEN; Balakrishnan S, 2011, PROTEINS, V79, P1061, DOI 10.1002/prot.22934; Bale JB, 2016, SCIENCE, V353, P389, DOI 10.1126/science.aaf8818; Battaglia Peter W, 2018, ARXIV 180601261; Bepler T., 2019, INT C LEARN REPR; Biswas Surojit, 2018, BIORXIV; Boomsma W., 2017, NIPS; Chen Sheng, 2019, BIORXIV; Conchuir SO, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130433; Dauphin Y., 2018, ARXIV180504833; El-Gebali S, 2019, NUCLEIC ACIDS RES, V47, pD427, DOI 10.1093/nar/gky995; Gilmer J, 2017, PR MACH LEARN RES, V70; Heinzinger M., 2019, BIORXIV; Huang Cheng-Zhi Anna, 2018, ARXIV180904281, P2; Huang PS, 2016, NATURE, V537, P320, DOI 10.1038/nature19946; Huynh DQ, 2009, J MATH IMAGING VIS, V35, P155, DOI 10.1007/s10851-009-0161-2; Ingraham J., 2019, INT C LEARN REPR; KABSCH W, 1983, BIOPOLYMERS, V22, P2577, DOI 10.1002/bip.360221211; Keefe AD, 2001, NATURE, V410, P715, DOI 10.1038/35070613; Koga N, 2012, NATURE, V491, P222, DOI 10.1038/nature11600; Kuhlman B, 2003, SCIENCE, V302, P1364, DOI 10.1126/science.1089427; Leaver-Fay A, 2011, METHOD ENZYMOL, P545, DOI [10.1016/B978-0-12-381270-4.00019-6, 10.1016/S0076-6879(11)87019-9]; Li H, 1996, SCIENCE, V273, P666, DOI 10.1126/science.273.5275.666; Marks DS, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0028766; Morcos F, 2011, P NATL ACAD SCI USA, V108, pE1293, DOI 10.1073/pnas.1111471108; O'Connell J, 2018, PROTEINS, V86, P629, DOI 10.1002/prot.25489; Ollikainen N, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003313; Orengo CA, 1997, STRUCTURE, V5, P1093, DOI 10.1016/S0969-2126(97)00260-8; Riesselman A, 2019, BIORXIV; Riesselman AJ, 2018, NAT METHODS, V15, P816, DOI 10.1038/s41592-018-0138-4; Rives A., 2019, BIORXIV; Rocklin GJ, 2017, SCIENCE, V357, P168, DOI 10.1126/science.aan0693; Schneider M, 2009, PROTEINS, V77, P97, DOI 10.1002/prot.22421; Shaw Peter, 2018, P 2018 C N AM CHAPT, P464, DOI DOI 10.18653/V1/N18-2074; Siegel JB, 2010, SCIENCE, V329, P309, DOI 10.1126/science.1190239; Sinai S., 2017, ARXIV171203346; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Tubiana J, 2019, ELIFE, V8, DOI 10.7554/eLife.39397; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Velickovic P., 2017, STAT-US, V1050, P20; Wang JX, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-24760-x; Weiler M, 2018, ADV NEUR IN, V31; Yang KK, 2019, NAT METHODS, V16, P687, DOI 10.1038/s41592-019-0496-6; Zhou Jianfu, 2018, BIORXIV	49	19	19	2	11	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907047
C	Li, YZ; Wei, C; Ma, TY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Yuanzhi; Wei, Colin; Ma, Tengyu			Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Stochastic gradient descent with a large initial learning rate is widely used for training modern neural net architectures. Although a small initial learning rate allows for faster training and better test performance initially, the large learning rate achieves better generalization soon after the learning rate is annealed. Towards explaining this phenomenon, we devise a setting in which we can prove that a two layer network trained with large initial learning rate and annealing provably generalizes better than the same network trained with a small learning rate from the start. The key insight in our analysis is that the order of learning different types of patterns is crucial: because the small learning rate model first memorizes easy-to-generalize, hard-to-fit patterns, it generalizes worse on hard-to-generalize, easier-to-fit patterns than its large learning rate counterpart. This concept translates to a larger-scale setting: we demonstrate that one can add a small patch to CIFAR-10 images that is immediately memorizable by a model with small initial learning rate, but ignored by the model with large learning rate until after annealing. Our experiments show that this causes the small learning rate model's accuracy on unmodified images to suffer, as it relies too much on the patch early on.	[Li, Yuanzhi] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Wei, Colin; Ma, Tengyu] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Carnegie Mellon University; Stanford University	Li, YZ (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	yuanzhil@andrew.cmu.edu; colinwei@stanford.edu; tengyuma@stanford.edu	Li, Yuan/GXV-1310-2022					Allen-Zhu Zeyuan, 2018, ARXIV181104918; Allen- Zhu Zeyuan, 2019, CORR; Allen-Zhu Zeyuan, 2018, ARXIV181012065; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], ARXIV181103962; Arora S, 2019, ADV NEUR IN, V32; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Chen J., 2018, ARXIV180606763; Dai X., 2018, ARXIV181200542; Du Simon S., 2018, INT C MACH LEARN ICM; Du Simon S., 2018, ARXIV E PRINTS; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Ge Rong, 2019, ARXIV E PRINTS; Goyal Priya, 2017, ARXIV170602677; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hoffer Elad, 2017, ADV NEURAL INFORM PR, P1729; Hu Wenqing, 2017, ARXIV170507562; Jastrzqbski Stanislaw, 2018, ARXIV180705031; Keskar N. S., 2017, ARXIV171207628; Keskar Nitish Shirish, 2016, LARGE BATCH TRAINING, P4; Kingma D.P, P 3 INT C LEARNING R; Kleinberg Robert, 2018, CORR; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Li Yuanzhi, 2017, CORR; Loshchilov I., 2017, P INT C LEARNING REP; Luo Liangchen, 2019, INT C LEARN REPR; Mangalam Karttikeya, 2019, DO DEEP NEURAL NETWO; Nakkiran Preetum, 2019, ARXIV E PRINTS; Smith LN, 2017, IEEE WINT CONF APPL, P464, DOI 10.1109/WACV.2017.58; Smith Samuel L, 2017, ARXIV171100489; Smith Samuel L, 2017, ARXIV171006451; Soudry D, 2018, J MACH LEARN RES, V19; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Wan WF, 2018, IEEE IND ELEC, P3815, DOI 10.1109/IECON.2018.8591522; Wen Y., 2019, ARXIV190208234; Wilson AC, 2017, ADV NEUR IN, V30; Xing C., 2018, ARXIV180208770; You Yang, 2017, ARXIV170803888, V6, P12; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zeiler Matthew D, 2012, ARXIV12125701	41	19	19	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903032
C	Reisizadeh, A; Taheri, H; Mokhtari, A; Hassani, H; Pedarsani, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Reisizadeh, Amirhossein; Taheri, Hossein; Mokhtari, Aryan; Hassani, Hamed; Pedarsani, Ramtin			Robust and Communication-Efficient Collaborative Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NONCONVEX OPTIMIZATION; GRADIENT DESCENT; ALGORITHM; CONVERGENCE; PARALLEL	We consider a decentralized learning problem, where a set of computing nodes aim at solving a non-convex optimization problem collaboratively. It is well-known that decentralized optimization schemes face two major system bottlenecks: stragglers' delay and communication overhead. In this paper, we tackle these bottlenecks by proposing a novel decentralized and gradient-based optimization algorithm named as QuanTimed-DSGD. Our algorithm stands on two main ideas: (i) we impose a deadline on the local gradient computations of each node at each iteration of the algorithm, and (ii) the nodes exchange quantized versions of their local models. The first idea robustifies to straggling nodes and the second alleviates communication efficiency. The key technical contribution of our work is to prove that with nonvanishing noises for quantization and stochastic gradients, the proposed method exactly converges to the global optimal for convex loss functions, and finds a first-order stationary point in non-convex scenarios. Our numerical evaluations of the QuanTimed-DSGD on training benchmark datasets, MNIST and CIFAR-10, demonstrate speedups of up to 3. in run-time, compared to state-of-the-art decentralized optimization methods.	[Reisizadeh, Amirhossein; Taheri, Hossein; Pedarsani, Ramtin] Univ Calif Santa Barbara, ECE Dept, Santa Barbara, CA 93106 USA; [Mokhtari, Aryan] Univ Texas Austin, ECE Dept, Austin, TX 78712 USA; [Hassani, Hamed] Univ Penn, ECE Dept, Philadelphia, PA 19104 USA	University of California System; University of California Santa Barbara; University of Texas System; University of Texas Austin; University of Pennsylvania	Reisizadeh, A (corresponding author), Univ Calif Santa Barbara, ECE Dept, Santa Barbara, CA 93106 USA.	reisizadeh@ucsb.edu; hossein@ucsb.edu; mokhtari@austin.utexas.edu; hassani@seas.upenn.edu; ramtin@ece.ucsb.edu			National Science Foundation (NSF) [CCF-1909320]; UC Office of President [LFR-18-548175]; NSF [1755707, 1837253]	National Science Foundation (NSF)(National Science Foundation (NSF)National Research Foundation of Korea); UC Office of President; NSF(National Science Foundation (NSF))	The authors acknowledge supports from National Science Foundation (NSF) under grant CCF-1909320 and UC Office of President under Grant LFR-18-548175. The research of H. Hassani is supported by NSF grants 1755707 and 1837253.	Alistarh D, 2017, ADV NEUR IN, V30; Ananthanarayanan G., 2010, P 9 USENIX C OP SYST, V10, P24; [Anonymous], ARXIV180909258; Aysal TC, 2007, 2007 IEEE/SP 14TH WORKSHOP ON STATISTICAL SIGNAL PROCESSING, VOLS 1 AND 2, P640, DOI 10.1109/SSP.2007.4301337; Berahas A. S., 2019, ARXIV190308149; Bernstein J., 2018, ARXIV180204434, V80, P560; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Chang TH, 2015, IEEE T SIGNAL PROCES, V63, P482, DOI 10.1109/TSP.2014.2367458; Choi HL, 2010, AUTOMATICA, V46, P1266, DOI 10.1016/j.automatica.2010.05.004; Dean J., 2012, ADV NEURAL INFORM PR, P1223, DOI DOI 10.5555/2999134.2999271; Dean J, 2013, COMMUN ACM, V56, P74, DOI 10.1145/2408776.2408794; Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027; Dutta S, 2018, PR MACH LEARN RES, V84; El Chamie M, 2016, IEEE T AUTOMAT CONTR, V61, P3870, DOI 10.1109/TAC.2016.2530939; Ferdinand Nuwan, 2019, INT C LEARN REPR; Hong M., 2018, ARXIV180208941; Hong MY, 2017, PR MACH LEARN RES, V70; Jakovetic D, 2014, IEEE T AUTOMAT CONTR, V59, P1131, DOI 10.1109/TAC.2014.2298712; Jha DK, 2016, INT J CONTROL, V89, P984, DOI 10.1080/00207179.2015.1110754; Jiang Z, 2017, ADV NEUR IN, V30; Kashyap A, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1-6, PROCEEDINGS, P635, DOI 10.1109/ISIT.2006.261862; Koloskova A, 2019, PR MACH LEARN RES, V97; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lee CS, 2018, CONF REC ASILOMAR C, P1876, DOI 10.1109/ACSSC.2018.8645345; Lee CS, 2018, IEEE DECIS CONTR P, P5857, DOI 10.1109/CDC.2018.8618973; Lee K, 2018, IEEE T INFORM THEORY, V64, P1514, DOI 10.1109/TIT.2017.2736066; Lian X., 2017, ARXIV PREPRINT ARXIV; Lian XR, 2017, ADV NEUR IN, V30; Mokhtari A, 2016, IEEE T SIGNAL PROCES, V64, P5158, DOI 10.1109/TSP.2016.2548989; Mokhtari Aryan, 2016, J MACHINE LEARNING R, V17, P2165; Nedic A, 2008, IEEE DECIS CONTR P, P4177, DOI 10.1109/CDC.2008.4738860; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Peng Z., 2016, J OPERATIONS RES SOC, P1; Qu G., 2017, ARXIV170507176; Rabbat MG, 2005, IEEE J SEL AREA COMM, V23, P798, DOI 10.1109/JSAC.2005.843546; Recht B, 2011, ADV NEURAL INFORM PR, V2011, P693; Reis A., 2021, IEEE T ANTENN PROPAG, P1; Reisizadeh A, 2019, IEEE T INFORM THEORY, V65, P4227, DOI 10.1109/TIT.2019.2904055; Reisizadeh A, 2018, IEEE DECIS CONTR P, P5838, DOI 10.1109/CDC.2018.8619539; RIBEIRO A, 2010, COMMUNICATION, V58, P6369, DOI DOI 10.1109/TSP.2010.2057247; Romberg J., 2018, ARXIV181013245; Scaman K, 2018, ADV NEUR IN, V31; Scaman K, 2017, PR MACH LEARN RES, V70; Schizas ID, 2008, IEEE T SIGNAL PROCES, V56, P350, DOI 10.1109/TSP.2007.906734; Scutari G., 2018, ARXIV180901106; Scutari G, 2017, IEEE T SIGNAL PROCES, V65, P1929, DOI 10.1109/TSP.2016.2637317; Seide F, 2014, INTERSPEECH, P1058; Shenker S., 2013, 10 USENIX S NETWORKE, P185; Shi W, 2015, IEEE T SIGNAL PROCES, V63, P6013, DOI 10.1109/TSP.2015.2461520; Shi W, 2015, SIAM J OPTIMIZ, V25, P944, DOI 10.1137/14096668X; Shi W, 2014, IEEE T SIGNAL PROCES, V62, P1750, DOI 10.1109/TSP.2014.2304432; Sun H., 2018, ARXIV180402729; Tandon R., 2016, ARXIV161203301; Tsianos KI, 2012, IEEE DECIS CONTR P, P5453, DOI 10.1109/cdc.2012.6426375; Tsitsiklis J. N., 1987, Journal of Complexity, V3, P231, DOI 10.1016/0885-064X(87)90013-6; Uribe C. A., 2018, ARXIV180900710; Wu T., 2017, IEEE T SIGNAL INF PR, V4, P307; Yuan K, 2016, SIAM J OPTIMIZ, V26, P1835, DOI 10.1137/130943170; Zeng JS, 2018, IEEE T SIGNAL PROCES, V66, P2834, DOI 10.1109/TSP.2018.2818081; Zhang RW, 2017, NEURAL PLAST, V2017, DOI 10.1155/2017/6809745; Zhang X., 2018, ARXIV181204048	65	19	19	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900003
C	Shi, C; Blei, DM; Veitch, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shi, Claudia; Blei, David M.; Veitch, Victor			Adapting Neural Networks for the Estimation of Treatment Effects	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper addresses the use of neural networks for the estimation of treatment effects from observational data. Generally, estimation proceeds in two stages. First, we fit models for the expected outcome and the probability of treatment (propensity score) for each unit. Second, we plug these fitted models into a downstream estimator of the effect. Neural networks are a natural choice for the models in the first step. The question we address is: how can we adapt the design and training of the neural networks used in the first step in order to improve the quality of the final estimate of the treatment effect? We propose two adaptations based on insights from the statistical literature on the estimation of treatment effects. The first is a new architecture, the Dragonnet, that exploits the sufficiency of the propensity score for estimation adjustment. The second is a regularization procedure, targeted regularization, that induces a bias towards models that have non-parametrically optimal asymptotic properties 'out-of-the-box'. Studies on benchmark datasets for causal inference show these adaptations outperform existing methods. Code is available at github.com/claudiashi57/dragonnet.	[Shi, Claudia; Blei, David M.] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA; [Blei, David M.; Veitch, Victor] Columbia Univ, Dept Stat, New York, NY 10027 USA	Columbia University; Columbia University	Shi, C (corresponding author), Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.				ONR [N00014-15-1-2209, 133691-5102004]; NIH [5100481-5500001084]; NSF [CCF-1740833]; FA [8750-14-2-0009]; Alfred P. Sloan Foundation; John Simon Guggenheim Foundation; Facebook; Amazon; IBM; government of Canada through NSERC	ONR(Office of Naval Research); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF)); FA; Alfred P. Sloan Foundation(Alfred P. Sloan Foundation); John Simon Guggenheim Foundation; Facebook(Facebook Inc); Amazon; IBM(International Business Machines (IBM)); government of Canada through NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC))	We are thankful to Yixin Wang, Dhanya Sridhar, Jackson Loper, Roy Adams, and Shira Mitchell for helpful comments and discussions. This work was supported by ONR N00014-15-1-2209, ONR 133691-5102004, NIH 5100481-5500001084, NSF CCF-1740833, FA 8750-14-2-0009, the Alfred P. Sloan Foundation, the John Simon Guggenheim Foundation, Facebook, Amazon, IBM, and the government of Canada through NSERC. The GPUs used for this research were donated by the NVIDIA Corporation.	Alaa A., 2017, ARXIV170402801; Alaa A.M., 2017, ARXIV PREPRINT ARXIV; [Anonymous], 2014, COMPUT VIS PATTERN R; [Anonymous], 2018, ARXIV180909953; Bottou L., 2012, ARXIV12092355; Chernozhukov V., 2017, AM EC REV, V5; Chernozhukov Victor, 2017, ECONOMETRICS J; Cole S. R., 2008, AM J EPIDEMIOLOGY; Dorie V., 2016, NONPARAMETRICS CAUSA; Hill JC, 2011, SPINE, V36, P2168, DOI 10.1097/BRS.0b013e31820712bb; Johansson F., 2016, ARXIV160503661; Kang J. D. Y., 2007, STAT SCI, V4; Kennedy E. H., 2016, STAT CAUSAL INFERENC; Louizos C., 2017, NEURIPS; MacDorman M F, 1998, Mon Vital Stat Rep, V46, P1; Potter F. J., 1993, P AM STAT ASS; Robins J. M., 2000, J AM STAT ASS, V450; Robins J. M., 2000, ASA P SECT BAYES STA; Rose S, 2011, SPRINGER SER STAT, P3, DOI 10.1007/978-1-4419-9782-1; Rosenbaum P. R., 1983, BIOMETRIKA, V1; Saquib N., 2013, BMJ; Scharfstein D. O., 1999, J AM STAT ASS, V448; Schwab P., 2018, ARXIV PREPRINT ARXIV; Shalit Uri, 2016, ARXIV160603976; SHIMONI Y., 2018, ARXIV180205046; van der Laan M, 2016, INT J BIOSTATISTICS; Veitch V., 2019, ADV NEURAL INFORM PR; Veitch V., 2019, ARXIV E PRINTS; Yoon Jaehong, 2018, 6 INT C LEARN REPR I	29	19	19	3	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302050
C	Wu, ZX; Xiong, CM; Jiang, YG; Davis, LS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wu, Zuxuan; Xiong, Caiming; Jiang, Yu-Gang; Davis, Larry S.			LiteEval: A Coarse-to-Fine Framework for Resource Efficient Video Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper presents LiteEval, a simple yet effective coarse-to-fine framework for resource efficient video recognition, suitable for both online and offline scenarios. Exploiting decent yet computationally efficient features derived at a coarse scale with a lightweight CNN model, LiteEval dynamically decides on-the-fly whether to compute more powerful features for incoming video frames at a finer scale to obtain more details. This is achieved by a coarse LSTM and a fine LSTM operating cooperatively, as well as a conditional gating module to learn when to allocate more computation. Extensive experiments are conducted on two large-scale video benchmarks, FCVID and ActivityNet, and the results demonstrate LiteEval requires substantially less computation while offering excellent classification accuracy for both online and offline predictions.	[Wu, Zuxuan; Davis, Larry S.] Univ Maryland, College Pk, MD 20742 USA; [Xiong, Caiming] Salesforce Res, Palo Alto, CA USA; [Jiang, Yu-Gang] Fudan Univ, Shanghai, Peoples R China	University System of Maryland; University of Maryland College Park; Salesforce; Fudan University	Wu, ZX (corresponding author), Univ Maryland, College Pk, MD 20742 USA.				Office of Naval Research [N000141612713]	Office of Naval Research(Office of Naval Research)	ZW and LSD are supported by Facebook and the Office of Naval Research under Grant N000141612713.	[Anonymous], 2018, CVPR; Belongie S., 2018, ECCV; Bengio E., 2016, ICML WORKSH ABSTR RE; Choi HS, 2017, ASIA-PAC INT SYM ELE, P4, DOI 10.1109/APEMC.2017.7975409; Deng J., 2009, CVPR; Fan H., 2018, IJCAI; Fedderke MA, 2014, J COSMOL ASTROPART P, DOI 10.1088/1475-7516/2014/01/001; Feichtenhofer C., 2019, ICCV; Figurnov M., 2017, CVPR; Gao M., 2018, CVPR; Graves A., 2016, ADAPTIVE COMPUTATION; Hazan T., 2012, ICML; He K., 2015, ICCV; He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI 10.1109/TPAMI.2018.2844175; Heilbron F. C., 2015, CVPR; Howard A. G., 2017, CVPR; Hu JG, 2018, COMM COM INF SC, V600, P1, DOI 10.1007/978-981-10-7844-6_1; Huang G., 2018, P INT C LEARNING REP, P1, DOI 10.48550/arXiv.1703.09844; Iandola F.N., 2016, ARXIV PREPRINT ARXIV; Jiang Y.-G., 2018, IEEE TPAMI; Korbar B., 2019, ICCV; Li Hao, 2017, ICLR; Liu Lanlan, 2017, ARXIV170100299; McGill M., 2017, ICML; Najibi M., 2019, ICCV; Rastegari M., 2016, ECCV; Sandler M., 2018, CVPR; Singh B., 2018, NIPS; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Teerapittayanon S., 2016, ICPR; Teh Y. W., 2017, ICLR; Viola P., 2004, IJCV; Wang L., 2016, ECCV; Wang Xiaolong, 2018, P EUR C COMP VIS ECC; Wu CY, 2018, INT SYMP COMP CONS, P80, DOI 10.1109/IS3C.2018.00028; Wu PC, 2015, IEEE MTTS INT MICROW, P115, DOI 10.1109/IMWS-BIO.2015.7303805; Wu Z., 2019, CVPR; Wu Zhirong, 2018, ARXIV180501978; Xie S., 2017, CVPR; Yao T., 2012, ACM MULTIMEDIA; Yeung S., 2016, CVPR; Zhang B., 2016, CVPR; Zolfaghari Mohammadreza, 2018, ECCV	43	19	19	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307076
C	Yehudai, G; Shamir, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yehudai, Gilad; Shamir, Ohad			On the Power and Limitations of Random Features for Understanding Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recently, a spate of papers have provided positive theoretical results for training over-parameterized neural networks (where the network size is larger than what is needed to achieve low error). The key insight is that with sufficient over-parameterization, gradient-based methods will implicitly leave some components of the network relatively unchanged, so the optimization dynamics will behave as if those components are essentially fixed at their initial random values. In fact, fixing these explicitly leads to the well-known approach of learning with random features (e.g. [27, 29]). In other words, these techniques imply that we can successfully learn with neural networks, whenever we can successfully learn with random features. In this paper, we formalize the link between existing results and random features, and argue that despite the impressive positive results, random feature approaches are also inherently limited in what they can explain. In particular, we prove that random features cannot be used to learn even a single ReLU neuron (over standard Gaussian inputs in R-d and poly(d) weights), unless the network size (or magnitude of its weights) is exponentially large in d. Since a single neuron is known to be learnable with gradient-based methods, we conclude that we are still far from a satisfying general explanation for the empirical success of neural networks. For completeness we also provide a simple self-contained proof, using a random features technique, that one-hidden-layer neural networks can learn low-degree polynomials.	[Yehudai, Gilad; Shamir, Ohad] Weizmann Inst Sci, Rehovot, Israel	Weizmann Institute of Science	Yehudai, G (corresponding author), Weizmann Inst Sci, Rehovot, Israel.	gilad.yehudai@weizmann.ac.il; ohad.shamir@weizmann.ac.il			European Research Council (ERC) [754705]	European Research Council (ERC)(European Research Council (ERC)European Commission)	This research is supported in part by European Research Council (ERC) grant 754705. We thank Yuanzhi Li for some helpful comments on the previous version of this paper.	Allen-Zhu Z., 2019, ARXIV190201028; Allen-Zhu Zeyuan, 2018, ARXIV181104918; Andoni A, 2014, PR MACH LEARN RES, V32, P1908; [Anonymous], 2016, FDN TRENDS IN OPTIMI; [Anonymous], 2018, ARXIV180301206; [Anonymous], ARXIV181103962; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Brutzkus Alon, 2017, ARXIV PREPRINT ARXIV; CAO Y., 2019, ARXIV190201384; Daniely Amit, 2017, ARXIV PREPRINT ARXIV; Du Simon S, 2018, GRADIENT DESCENT FIN; Du SS., 2019, P 7 INT C LEARN REPR; Ghorbani B., 2019, ARXIV190412191; Globerson A., 2018, ARXIV181003037; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Jacot Arthur, 2018, ADV NEURAL INFORM PR, P8571; Ledoux M., 2001, MATH SURVEYS MONOG; Li Yuanzhi, 2017, ARXIV171209203; Livni R., 2014, NIPS, V1, P855; Mei S., 2016, ARXIV160706534; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rahimi A, 2008, ANN ALLERTON CONF, P555, DOI 10.1109/ALLERTON.2008.4797607; SAFRAN I., 2017, ARXIV171208968; Safran I, 2016, PR MACH LEARN RES, V48; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Soltanolkotabi M., 2017, ARXIV PREPRINT ARXIV; Soltanolkotabi M, 2019, IEEE T INFORM THEORY, V65, P742, DOI 10.1109/TIT.2018.2854560; Soudry D., 2016, ARXIV PREPRINT ARXIV; Soudry Daniel, 2017, ARXIV170205777; Sun Y., 2018, ARXIV181004374; Wang G., 2018, ARXIV180804685; Yue YF, 2018, INT POW ELEC APPLICA, P1668; Zhang Chiyuan, 2016, ARXIV161103530; Zhang YC, 2016, PR MACH LEARN RES, V48	41	19	19	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306058
C	Bogunovic, I; Scarlett, J; Jegelka, S; Cevher, V		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bogunovic, Ilija; Scarlett, Jonathan; Jegelka, Stefanie; Cevher, Volkan			Adversarially Robust Optimization with Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we consider the problem of Gaussian process (GP) optimization with an added robustness requirement: The returned point may be perturbed by an adversary, and we require the function value to remain as high as possible even after this perturbation. This problem is motivated by settings in which the underlying functions during optimization and implementation stages are different, or when one is interested in finding an entire region of good inputs rather than only a single point. We show that standard GP optimization algorithms do not exhibit the desired robustness properties, and provide a novel confidence-bound based algorithm STABLEOPT for this purpose. We rigorously establish the required number of samples for STABLEOPT to find a near-optimal point, and we complement this guarantee with an algorithm-independent lower bound. We experimentally demonstrate several potential applications of interest using real-world data sets, and we show that STABLEOPT consistently succeeds in finding a stable maximizer where several baseline methods fail.	[Bogunovic, Ilija; Cevher, Volkan] Ecole Polytech Fed Lausanne, LIONS, Lausanne, Switzerland; [Scarlett, Jonathan] Natl Univ Singapore, Singapore, Singapore; [Jegelka, Stefanie] MIT, CSAIL, Cambridge, MA 02139 USA	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; National University of Singapore; Massachusetts Institute of Technology (MIT)	Bogunovic, I (corresponding author), Ecole Polytech Fed Lausanne, LIONS, Lausanne, Switzerland.	ilija.bogunovic@epfl.ch; scarlett@comp.nus.edu.sg; stefje@mit.edu; volkan.cevher@epfl.ch	Scarlett, Jonathan/AGK-0892-2022		Swiss National Science Foundation (SNSF) [407540_167319]; European Research Council (ERC) under the European Union [725594]; DARPA DSO's Lagrange program [FA86501827838]; NUS startup grant	Swiss National Science Foundation (SNSF)(Swiss National Science Foundation (SNSF)); European Research Council (ERC) under the European Union(European Research Council (ERC)); DARPA DSO's Lagrange program; NUS startup grant	This work was partially supported by the Swiss National Science Foundation (SNSF) under grant number 407540_167319, by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement no725594 - time-data), by DARPA DSO's Lagrange program under grant FA86501827838, and by an NUS startup grant.	Auer Peter, 1998, TECHNICAL REPORT; Beland J.J., 2017, NIPS BAYESOPT 2017 W; Bertsimas D, 2010, INFORMS J COMPUT, V22, P44, DOI 10.1287/ijoc.1090.0319; Bertsimas D, 2010, OPER RES, V58, P161, DOI 10.1287/opre.1090.0715; Bogunovic I., 2016, ADV NEURAL INFORM PR, V29, P1507; Bogunovic I, 2018, PR MACH LEARN RES, V84; Bogunovic I, 2017, PR MACH LEARN RES, V70; Bogunovic I, 2016, JMLR WORKSH CONF PRO, V51, P314; Chowdhury SR, 2017, PR MACH LEARN RES, V70; Contal Emile, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8188, P225, DOI 10.1007/978-3-642-40988-2_15; Desautels T, 2014, J MACH LEARN RES, V15, P3873; Dinh L., 2017, INT C MACH LEARN ICM; Gonzalez J, 2016, JMLR WORKSH CONF PRO, V51, P648; Gotovos A., 2013, 23 INT JOINT C ART I, P1344; Hennig P, 2012, J MACH LEARN RES, V13, P1809; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Kandasamy K, 2015, PR MACH LEARN RES, V37, P295; Krause A, 2008, J MACH LEARN RES, V9, P2761; Lizotte D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P944; Martinez-Cantin R, 2018, PR MACH LEARN RES, V84; Nogueira l., 2016, IEEE RSJ INT C INT R; Ong C. S., 2011, ADV NEURAL INFORM PR, P2447; Rasmussen C., 2006, GAUSSIAN PROCESSES M, V1; Rolland Paul, 2018, INT C ART INT STAT, P298; Ru B., 2017, ARXIV171100673; Scarlett J., 2017, C LEARN THEOR COLT; Shekhar S., 2017, ARXIV171201447; Sinha Aman, 2018, INT C LEARN REPR ICL; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Staib M., 2018, ARXIV180205249; Sui YA, 2015, PR MACH LEARN RES, V37, P997; Vanchinathan HP, 2014, PROCEEDINGS OF THE 8TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'14), P225, DOI 10.1145/2645710.2645733; Wang Z, 2017, PR MACH LEARN RES, V70; Wang Z, 2017, PR MACH LEARN RES, V70; Wilder Bryan, 2017, C ART INT AAAI	37	19	19	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000028
C	Chen, RTQ; Li, XC; Grosse, R; Duvenaud, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Ricky T. Q.; Li, Xuechen; Grosse, Roger; Duvenaud, David			Isolating Sources of Disentanglement in VAEs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				INDEPENDENT COMPONENT ANALYSIS	We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate the beta-TCVAE (Total Correlation Variational Autoencoder) algorithm, a refinement and plug-in replacement of the beta-VAE for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classifier-free measure of disentanglement called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the model is trained using our framework.	[Chen, Ricky T. Q.; Li, Xuechen; Grosse, Roger; Duvenaud, David] Univ Toronto, Vector Inst, Toronto, ON, Canada	University of Toronto	Chen, RTQ (corresponding author), Univ Toronto, Vector Inst, Toronto, ON, Canada.	rtqichen@cs.toronto.edu; lxuchen@cs.toronto.edu; rgrosse@cs.toronto.edu; duvenaud@cs.toronto.edu	Chen, Ricky Tian Qi/AAS-3168-2021					Achille A., 2017, ARXIV170601350; Achille A, 2018, IEEE T PATTERN ANAL, V40, P2897, DOI 10.1109/TPAMI.2017.2784440; Alemi Alex, 2017, ICLR; [Anonymous], 2016, P WORKSH ADV APPR BA; Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487; Belghazi I., 2018, ARXIV180104062; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Burgess C., 2017, WORKSH LEARN DIS REP; Cheung B., 2014, P INT C LEARN REPR W; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; Desjardins G., 2012, ARXIV12105474; Gao Shuyang, 2018, ARXIV180205822; Glorot Xavier, 2011, P 28 INT C MACH LEAR, P513, DOI DOI 10.1177/1753193411430810; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grathwohl Will, 2016, ARXIV161204440; Higgins I., 2017, ICLR, P1; Hinton G, 1994, ADV NEURAL INFORM PR, V6, DOI DOI 10.1021/jp906511z; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Hyvarinen A, 1999, NEURAL NETWORKS, V12, P429, DOI 10.1016/S0893-6080(98)00140-3; Jutten C, 2003, P 4 INT S IND COMP A; Karaletsos Theofanis, 2016, INT C LEARN REPR; Kim H, 2018, PR MACH LEARN RES, V80; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kulkarni TD, 2015, ADV NEUR IN, V28; Kumar A., 2017, INT C LEARN REPR; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Makhzani Alireza, 2016, ICLR 2016 WORKSH INT; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reshef DN, 2011, SCIENCE, V334, P1518, DOI 10.1126/science.1205438; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Ridgeway K., 2016, ARXIV161205299; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863; Sonderby C.K., 2017, ICLR, P1; Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613; Tang Yichuan, 2013, INT C MACH LEARN, P163; Vedantam R., 2018, 6 INT C LEARN REPR I; Ver Steeg G, 2015, JMLR WORKSH CONF PRO, V38, P1004; WATANABE S, 1960, IBM J RES DEV, V4, P66, DOI 10.1147/rd.41.0066; Williams Christopher K. I., 2018, INT C LEARN REPR; Xu Jin, 2018, ARXIV180605953; Zhao SS, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3490	44	19	19	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302061
C	Li, JN; Wong, YK; Zhao, Q; Kankanhalli, MS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Junnan; Wong, Yongkang; Zhao, Qi; Kankanhalli, Mohan S.			Unsupervised Learning of View-invariant Action Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ACTION RECOGNITION; ENSEMBLE	The recent success in human action recognition with deep learning methods mostly adopt the supervised learning paradigm, which requires significant amount of manually labeled data to achieve good performance. However, label collection is an expensive and time-consuming process. In this work, we propose an unsupervised learning framework, which exploits unlabeled data to learn video representations. Different from previous works in video representation learning, our unsupervised learning task is to predict 3D motion in multiple target views using video representation from a source view. By learning to extrapolate cross-view motions, the representation can capture view-invariant motion dynamics which is discriminative for the action. In addition, we propose a view-adversarial training method to enhance learning of view-invariant features. We demonstrate the effectiveness of the learned representations for action recognition on multiple datasets.	[Li, Junnan] Natl Univ Singapore, Grad Sch Integrat Sci & Engn, Singapore, Singapore; [Wong, Yongkang; Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Singapore, Singapore; [Zhao, Qi] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN USA	National University of Singapore; National University of Singapore; University of Minnesota System; University of Minnesota Twin Cities	Li, JN (corresponding author), Natl Univ Singapore, Grad Sch Integrat Sci & Engn, Singapore, Singapore.	lijunnan@u.nus.edu; yongkang.wong@nus.edu.sg; qzhao@cs.umn.edu; mohan@comp.nus.edu.sg	Kankanhalli, Mohan/Q-9284-2019	Kankanhalli, Mohan/0000-0002-4846-2015	National Research Foundation, Prime Minister's Office, Singapore under its Strategic Capability Research Centres Funding Initiative	National Research Foundation, Prime Minister's Office, Singapore under its Strategic Capability Research Centres Funding Initiative(National Research Foundation, Singapore)	This research is supported by the National Research Foundation, Prime Minister's Office, Singapore under its Strategic Capability Research Centres Funding Initiative.	Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024; Bengio Y, 2014, PR MACH LEARN RES, V32, P226; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Cheng G., 2015, ARXIV150105964; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714; Fernando B, 2017, PROC CVPR IEEE, P5729, DOI 10.1109/CVPR.2017.607; Ganin Y., 2016, JMLR, V17, P2096; Ganin Yaroslav, 2015, ICML; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Haque A, 2016, LECT NOTES COMPUT SC, V9905, P160, DOI 10.1007/978-3-319-46448-0_10; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; HU JF, 2015, PROC CVPR IEEE, P5344; Isik L, 2018, J NEUROPHYSIOL, V119, P631, DOI 10.1152/jn.00642.2017; Jaimez M, 2015, IEEE INT CONF ROBOT, P98, DOI 10.1109/ICRA.2015.7138986; Kingma D.P, P 3 INT C LEARNING R; Komodakis Nikos, 2018, INT C LEARN REPR; Kong Y, 2017, IEEE T IMAGE PROCESS, V26, P3028, DOI 10.1109/TIP.2017.2696786; Le QV, 2013, INT CONF ACOUST SPEE, P8595, DOI 10.1109/ICASSP.2013.6639343; Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79; Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115; Li BL, 2012, PROC CVPR IEEE, P1362, DOI 10.1109/CVPR.2012.6247822; Li J, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1, DOI 10.1145/3123266.3123432; Li JN, 2017, IEEE I CONF COMP VIS, P2669, DOI 10.1109/ICCV.2017.289; Li RN, 2012, PROC CVPR IEEE, P2855, DOI 10.1109/CVPR.2012.6248011; Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273; Liu J., 2017, PROC CVPR IEEE, P1647, DOI DOI 10.1109/CVPR.2017.391; Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50; Lu CW, 2014, PROC CVPR IEEE, P772, DOI 10.1109/CVPR.2014.104; Luo ZL, 2017, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR.2017.751; Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32; Noroozi M, 2017, IEEE I CONF COMP VIS, P5899, DOI 10.1109/ICCV.2017.628; Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P465, DOI 10.1109/CVPRW.2013.76; Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228; Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98; Parameswaran V, 2006, INT J COMPUT VISION, V66, P83, DOI 10.1007/s11263-005-3671-4; Rahmani H, 2018, IEEE T PATTERN ANAL, V40, P667, DOI 10.1109/TPAMI.2017.2691768; Rahmani H, 2017, IEEE I CONF COMP VIS, P5833, DOI 10.1109/ICCV.2017.621; Rahmani H, 2016, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2016.167; Rahmani H, 2016, IEEE T PATTERN ANAL, V38, P2430, DOI 10.1109/TPAMI.2016.2533389; Ranzato MarcAurelio, 2014, ARXIV14126604; Salakhutdinov Ruslan, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPR.2009.5206577; Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465; SHAHROUDY A, 2016, CVPR, P1010, DOI DOI 10.1109/CVPR.2016.115; Shahroudy A, 2018, IEEE T PATTERN ANAL, V40, P1045, DOI 10.1109/TPAMI.2017.2691321; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Springenberg J.T., 2014, ARXIV14126806; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Viorica P .atr., 2016, ICLR WORKSH; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180; Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198; Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813; Wang PC, 2017, PROC CVPR IEEE, P416, DOI 10.1109/CVPR.2017.52; Wang XL, 2017, IEEE I CONF COMP VIS, P1338, DOI 10.1109/ICCV.2017.149; Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108; Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI 10.1109/ICCV.2017.233; Zhang Z, 2013, PROC CVPR IEEE, P2690, DOI 10.1109/CVPR.2013.347	64	19	19	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301026
C	Li, ZZ; Li, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Zhize; Li, Jian			A Simple Proximal Stochastic Gradient Method for Nonsmooth Nonconvex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We analyze stochastic gradient algorithms for optimizing nonconvex, nonsmooth finite-sum problems. In particular, the objective function is given by the summation of a differentiable (possibly nonconvex) component, together with a possibly non-differentiable but convex component. We propose a proximal stochastic gradient algorithm based on variance reduction, called ProxSVRG+. Our main contribution lies in the analysis of ProxSVRG+. It recovers several existing convergence results and improves/generalizes them (in terms of the number of stochastic gradient oracle calls and proximal oracle calls). In particular, ProxSVRG+ generalizes the best results given by the SCSG algorithm, recently proposed by [Lei et al., 2017] for the smooth nonconvex case. ProxSVRG+ is also more straightforward than SCSG and yields simpler analysis. Moreover, ProxSVRG+ outperforms the deterministic proximal gradient descent (ProxGD) for a wide range of minibatch sizes, which partially solves an open problem proposed in [Reddi et al., 2016]. Also, ProxSVRG+ uses much less proximal oracle calls than ProxSVRG [Reddi et al., 2016]. Moreover, for nonconvex functions satisfied Polyak-Lojasiewicz condition, we prove that ProxSVRG+ achieves a global linear convergence rate without restart unlike ProxSVRG. Thus, it can automatically switch to the faster linear convergence in some regions as long as the objective function satisfies the PL condition locally in these regions. Finally, we conduct several experiments and the experimental results are consistent with the theoretical results.	[Li, Zhize; Li, Jian] Tsinghua Univ, IIIS, Beijing, Peoples R China	Tsinghua University	Li, ZZ (corresponding author), Tsinghua Univ, IIIS, Beijing, Peoples R China.	zz-li14@mails.tsinghua.edu.cn; lijian83@mail.tsinghua.edu.cn			National Basic Research Program of China [2015CB358700]; National Natural Science Foundation of China [61772297, 61632016, 61761146003]; Microsoft Research Asia	National Basic Research Program of China(National Basic Research Program of China); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Microsoft Research Asia(Microsoft)	This research is supported in part by the National Basic Research Program of China Grant 2015CB358700, the National Natural Science Foundation of China Grant 61772297, 61632016, 61761146003, and a grant from Microsoft Research Asia. The authors would like to thank Rong Ge (Duke), Xiangliang Zhang (KAUST) and the anonymous reviewers for their useful suggestions.	ALLEN- ZHU Z., 2017, ARXIV170808694; Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448; Anitescu M, 2000, SIAM J OPTIMIZ, V10, P1116, DOI 10.1137/S1052623499359178; [Anonymous], ARXIV150702000; Aravkin A., 2016, ARXIV161001101; Csiba D., 2017, ARXIV170903014; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; FU H., 2018, ARXIV180206463; Ghadimi S, 2016, MATH PROGRAM, V155, P267, DOI 10.1007/s10107-014-0846-1; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Keskar N.S., 2016, ABS160904836; Kyrola A., 2017, ABS170602677 ARXIV; Lei LH, 2017, ADV NEUR IN, V30; Li  Qunwei, 2017, P 34 INT C MACH LEAR, P2111; Necoara  Ion, 2015, ARXIV150406298; Nesterov Y., 2018, APPL OPTIMIZATION; Reddi S. J., 2016, ADV NEURAL INFORM PR, P1145; Reddi SJ, 2016, PR MACH LEARN RES, V48; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zhi-Quan Luo, 1993, Annals of Operations Research, V46-47, P157; Zhong  Kai, 2017, ARXIV170603175; Zhou D., 2018, ADV NEURAL INFORM PR	27	19	19	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000010
C	Magliacane, S; van Ommen, T; Claassen, T; Bongers, S; Versteeg, P; Mooij, JM		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Magliacane, Sara; van Ommen, Thijs; Claassen, Tom; Bongers, Stephan; Versteeg, Philip; Mooij, Joris M.			Domain Adaptation by Using Causal Inference to Predict Invariant Conditional Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					An important goal common to domain adaptation and causal inference is to make accurate predictions when the distributions for the source (or training) domain(s) and target (or test) domain(s) differ. In many cases, these different distributions can be modeled as different contexts of a single underlying system, in which each distribution corresponds to a different perturbation of the system, or in causal terms, an intervention. We focus on a class of such causal domain adaptation problems, where data for one or more source domains are given, and the task is to predict the distribution of a certain target variable from measurements of other variables in one or more target domains. We propose an approach for solving these problems that exploits causal inference and does not rely on prior knowledge of the causal graph, the type of interventions or the intervention targets. We demonstrate our approach by evaluating a possible implementation on simulated and real world data.	[Magliacane, Sara] IBM Res, MIT IBM Watson AI Lab, Cambridge, MA 02142 USA; [Magliacane, Sara; van Ommen, Thijs; Bongers, Stephan; Versteeg, Philip; Mooij, Joris M.] Univ Amsterdam, Amsterdam, Netherlands; [Claassen, Tom] Radboud Univ Nijmegen, Nijmegen, Netherlands	International Business Machines (IBM); University of Amsterdam; Radboud University Nijmegen	Magliacane, S (corresponding author), IBM Res, MIT IBM Watson AI Lab, Cambridge, MA 02142 USA.	sara.magliacane@gmail.com; thijsvanommen@gmail.com; tomc@cs.ru.nl; srbongers@gmail.com; p.j.j.p.versteeg@uva.nl; j.m.mooij@uva.nl	Magliacane, Sara/ABD-8241-2020		NWO, the Netherlands Organization for Scientific Research (VIDI grant) [639.072.410]; Dutch programme COMMIT/ under the Data2Semantics project; NWO [612.001.202]; EU-FP7 grant [603016]; European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [639466]	NWO, the Netherlands Organization for Scientific Research (VIDI grant); Dutch programme COMMIT/ under the Data2Semantics project; NWO(Netherlands Organization for Scientific Research (NWO)); EU-FP7 grant; European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC))	We thank Patrick Forre for proofreading a draft of this work. We thank Renee van Amerongen and Lucas van Eijk for sharing their domain knowledge about the hematology-related measurements from the International Mouse Phenotyping Consortium (IMPC). SM, TC, SB, and PV were supported by NWO, the Netherlands Organization for Scientific Research (VIDI grant 639.072.410). SM was also supported by the Dutch programme COMMIT/ under the Data2Semantics project. TC was also supported by NWO grant 612.001.202 (MoCoCaDi), and EU-FP7 grant agreement n.603016 (MATRICS). TvO and JMM were supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement 639466).	Bareinboim E, 2016, P NATL ACAD SCI USA, V113, P7345, DOI 10.1073/pnas.1510507113; Bongers S., 2018, ARXIV161106221V2STAT; Eaton D, 2007, P MACHINE LEARNING R, V2, P107; Forre P, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P269; Gebser M., 2014, TECHNICAL REPORT; Gong MM, 2016, PR MACH LEARN RES, V48; Hyttinen A, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P395; Hyttinen A, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P340; Magliacane S., 2016, ADV NEURAL INFORM PR, P4466; Mansour Yishay, 2009, COLT 2009 22 C LEARN; Markowetz F., 2005, P 10 INT WORKSHOP AR, VR5, P214; Mooij J. M., 2013, P 29 ANN C UNC ART I, P431; Mooij J. M., 2018, PREPRINT; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pearl J., 2011, 2011 IEEE International Conference on Data Mining Workshops, P540, DOI 10.1109/ICDMW.2011.169; Pearl J., 2009, CAUSALITY MODELS REA; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Quinonero-Candela J, 2009, NEURAL INF PROCESS S, pXI; Rantanen  K., 2018, P MACHINE LEARNING R, V72, P344; Richardson T, 2003, SCAND J STAT, V30, P145, DOI 10.1111/1467-9469.00323; Rojas-Carulla M, 2018, J MACH LEARN RES, V19; Scholkopf  B., 2012, P 29 INT C MACH LEAR, P1255; Spirtes P., 2000, CAUSATION PREDICTION; Storkey A, 2009, NEURAL INF PROCESS S, P3; Sugiyama M., 2008, NIPS, P1433; Tian  J., 2001, P 17 C UNC ART INT U; Zhang K, 2015, AAAI CONF ARTIF INTE, P3150; Zhang L, 2013, PROCEEDINGS OF 2013 CHINA INTERNATIONAL CONFERENCE ON INSURANCE AND RISK MANAGEMENT, P819	28	19	19	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005043
C	Malik, OA; Becker, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Malik, Osman Asif; Becker, Stephen			Low-Rank Tucker Decomposition of Large Tensors Using TensorSketch	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose two randomized algorithms for low-rank Tucker decomposition of tensors. The algorithms, which incorporate sketching, only require a single pass of the input tensor and can handle tensors whose elements are streamed in any order. To the best of our knowledge, ours are the only algorithms which can do this. We test our algorithms on sparse synthetic data and compare them to multiple other methods. We also apply one of our algorithms to a real dense 38 GB tensor representing a video and use the resulting decomposition to correctly classify frames containing disturbances.	[Malik, Osman Asif; Becker, Stephen] Univ Colorado, Dept Appl Math, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Malik, OA (corresponding author), Univ Colorado, Dept Appl Math, Boulder, CO 80309 USA.	osman.malik@colorado.edu; stephen.becker@colorado.edu	Becker, Stephen R/R-7528-2016; Malik, Osman Asif/AAJ-7126-2020	Becker, Stephen R/0000-0002-1932-8159; Malik, Osman Asif/0000-0003-4477-481X	National Science Foundation [ACI-1532235, ACI-1532236, 1810314]; University of Colorado Boulder; Colorado State University	National Science Foundation(National Science Foundation (NSF)); University of Colorado Boulder; Colorado State University	This material is based upon work supported by the National Science Foundation under Grant No. 1810314.; This work utilized the RMACC Summit supercomputer, which is supported by the National Science Foundation (awards ACI-1532235 and ACI-1532236), the University of Colorado Boulder, and Colorado State University. The Summit supercomputer is a joint effort of the University of Colorado Boulder and Colorado State University.	[Anonymous], 2006, MULTILINEAR OPERATOR; Austin W, 2016, INT PARALL DISTRIB P, P912, DOI 10.1109/IPDPS.2016.67; Avron H, 2014, ADV NEUR IN, V27; BADER B. W, MATLAB TENSOR TOOLBO; Baskaran M, 2012, IEEE HIGH PERF EXTR; Battaglino C, 2018, SIAM J MATRIX ANAL A, V39, P876, DOI 10.1137/17M1112303; Caiafa CF, 2010, LINEAR ALGEBRA APPL, V433, P557, DOI 10.1016/j.laa.2010.03.020; Charikar M, 2004, THEOR COMPUT SCI, V312, P3, DOI 10.1016/S0304-3975(03)00400-6; Clarkson KL, 2017, J ACM, V63, DOI 10.1145/3019134; da Costa MN, 2016, EUR SIGNAL PR CONF, P215, DOI 10.1109/EUSIPCO.2016.7760241; Diao HA, 2018, PR MACH LEARN RES, V84; Drineas P, 2007, LINEAR ALGEBRA APPL, V420, P553, DOI 10.1016/j.laa.2006.08.023; Fanaee H, 2015, KNOWL-BASED SYST, V89, P332, DOI 10.1016/j.knosys.2015.07.013; Friedland S., 2011, ELECT J LINEAR ALGEB, V22; Gujral E., P 2018 SIAM INT C DA, P387; Jeon Inah, 2015, IEEE INT C DAT ENG I; Kaya O, 2016, PROC INT CONF PARAL, P103, DOI 10.1109/ICPP.2016.19; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kolda TG, 2008, IEEE DATA MINING, P363, DOI 10.1109/ICDM.2008.89; Li J., 2015, SOFT COMPUT, V12, P1, DOI DOI 10.14288/1.0076146; Li JJ, 2016, PROCEEDINGS OF 2016 6TH WORKSHOP ON IRREGULAR APPLICATIONS: ARCHITECTURE AND ALGORITHMS (IA3), P26, DOI [10.1109/IA3.2016.010, 10.1109/IA3.2016.10]; Liu BT, 2017, IEEE INT C CL COMP, P47, DOI 10.1109/CLUSTER.2017.75; Mahoney MW, 2008, SIAM J MATRIX ANAL A, V30, P957, DOI 10.1137/060665336; Pham N, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P239, DOI 10.1145/2487575.2487591; Oh J, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P761, DOI 10.1145/3018661.3018721; Oseledets IV, 2008, SIAM J MATRIX ANAL A, V30, P939, DOI 10.1137/060655894; Pagh R, 2013, ACM T COMPUT THEORY, V5, DOI 10.1145/2493252.2493254; Shi Y, 2016, INT C HIGH PERFORM, P193, DOI [10.1109/HiPC.2016.031, 10.1109/HiPC.2016.46]; Sun Jimeng, 2008, ACM T KNOWL DISCOV D, V2; Tsourakakis C. E., 2010, P 2010 SIAM INT C DA, P689, DOI DOI 10.1137/1.9781611972801.60; Wang Y., 2015, ADV NEURAL INFORM PR, P991; Zhou Guoxu, 2014, CORR	32	19	19	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004063
C	Mitzenmacher, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mitzenmacher, Michael			A Model for Learned Bloom Filters, and Optimizing by Sandwiching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recent work has suggested enhancing Bloom filters by using a pre-filter, based on applying machine learning to determine a function that models the data set the Bloom filter is meant to represent. Here we model such learned Bloom filters, with the following outcomes: (1) we clarify what guarantees can and cannot be associated with such a structure; (2) we show how to estimate what size the learning function must obtain in order to obtain improved performance; (3) we provide a simple method, sandwiching, for optimizing learned Bloom filters; and (4) we propose a design and analysis approach for a learned Bloomier filter, based on our modeling approach.	[Mitzenmacher, Michael] Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA	Harvard University	Mitzenmacher, M (corresponding author), Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.	michaelm@eecs.harvard.edu			NSF [CCF-1563710, CCF-1535795, CCF-1320231, CNS-1228598]	NSF(National Science Foundation (NSF))	The author thanks Suresh Venkatasubramanian for suggesting a closer look at [7], and thanks the authors of [7] for helpful discussions involving their work. This work was supported in part by NSF grants CCF-1563710, CCF-1535795, CCF-1320231, and CNS-1228598. Part of this work was done while visiting Microsoft Research New England.	Bender M., 2017, BLOOM FILTERS ADAPTI; Broder A., 2003, INTERNET MATH, V1, P485, DOI [DOI 10.1080/15427951.2004.10129096, 10.1080/15427951.2004.10129096]; Charles D, 2008, LECT NOTES COMPUT SC, V5193, P259, DOI 10.1007/978-3-540-87744-8_22; Chazelle B., 2004, SODA 04, P30; Chung K., 2013, THEOR COMPUT, V9, P897; Fan B, 2014, PROCEEDINGS OF THE 2014 CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT'14), P75, DOI 10.1145/2674005.2674994; Mitzenmacher M., 2018, MODEL LEARNED BLOOM; Mitzenmacher M., 2018, PROC 20 WORKSHOP ALG, P36; MITZENMACHER M., 2018, OPTIMIZING LEARNED B; Mitzenmacher M., 2017, PROBABILITY COMPUTIN; Naor M, 2015, LECT NOTES COMPUT SC, V9216, P565, DOI 10.1007/978-3-662-48000-7_28; Polyzotis N, 2017, CASE LEARNED INDEX S	12	19	20	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300043
C	Plotz, T; Roth, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ploetz, Tobias; Roth, Stefan			Neural Nearest Neighbors Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				IMAGE; SIMILARITY	Non-local methods exploiting the self-similarity of natural signals have been well studied, for example in image analysis and restoration. Existing approaches, however, rely on k-nearest neighbors (KNN) matching in a fixed feature space. The main hurdle in optimizing this feature space w. r. t. application performance is the non-differentiability of the KNN selection rule. To overcome this, we propose a continuous deterministic relaxation of KNN selection that maintains differentiability w. r. t. pairwise distances, but retains the original KNN as the limit of a temperature parameter approaching zero. To exploit our relaxation, we propose the neural nearest neighbors block (N block), a novel non-local processing layer that leverages the principle of self-similarity and can be used as building block in modern neural network architectures.(1 )We show its effectiveness for the set reasoning task of correspondence classification as well as for image restoration, including image denoising and single image super-resolution, where we outperform strong convolutional neural network (CNN) baselines and recent non-local models that rely on KNN selection in hand-chosen features spaces.	[Ploetz, Tobias; Roth, Stefan] Tech Univ Darmstadt, Dept Comp Sci, Darmstadt, Germany	Technical University of Darmstadt	Plotz, T (corresponding author), Tech Univ Darmstadt, Dept Comp Sci, Darmstadt, Germany.				European Research Council under the European Union [307942]	European Research Council under the European Union(European Research Council (ERC))	The research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP/2007-2013)/ERC Grant agreement No. 307942. We would like to thank reviewers for their fruitful comments.	Agustsson E, 2017, P IEEE C COMP VIS PA, P126; Alain G, 2014, J MACH LEARN RES, V15, P3563; [Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; Arjomand Bigdeli Siavash, NIPS 2017, P763; Bae W, 2017, IEEE COMPUT SOC CONF, P1141, DOI 10.1109/CVPRW.2017.152; Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Cruz C, 2018, IEEE SIGNAL PROC LET, V25, P1216, DOI 10.1109/LSP.2018.2850222; Cruz C, 2018, IEEE T IMAGE PROCESS, V27, P1376, DOI 10.1109/TIP.2017.2779265; Dabov K., 2006, P SPIE; Diamond S, 2017, ARXIV170106487CSCV; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Foi A, 2008, IEEE T IMAGE PROCESS, V17, P1737, DOI 10.1109/TIP.2008.2001399; Goldberger Jacob, NIPS 2005, P513; Graves A, 2014, NEURAL TURING MACHIN; Guo Shi, 2018, ARXIV180704686CSCV; Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156; Jain Viren, NIPS 2008, P769; Jang Eric, 2017, ICLR 2017; Jun Xu, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11212), P21, DOI 10.1007/978-3-030-01237-3_2; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kingma D.P., 2015, INT C LEARN REPR, P1; Ledig C, 2018, CVPR, P4681; Lefkimmiatis S, 2018, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR.2018.00338; Lefkimmiatis S, 2017, PROC CVPR IEEE, P5882, DOI 10.1109/CVPR.2017.623; Lim B., 2017, P IEEE C COMP VIS PA, P136; Liu Ding, NIPS 2018; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lotan O, 2016, PROC CVPR IEEE, P439, DOI 10.1109/CVPR.2016.54; Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888; Mao Xiaojiao, NIPS 2016, P2802; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Mnih A, 2016, PR MACH LEARN RES, V48; Plotz Tobias, 2017, P IEEE C COMP VIS PA, P1586; Ren WQ, 2014, INT C PATT RECOG, P4358, DOI 10.1109/ICPR.2014.746; Romano Y, 2017, SIAM J IMAGING SCI, V10, P1804, DOI 10.1137/16M1102884; Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6; Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486; Teh Y. W., 2017, ICLR; Timofte R., 2017, P IEEE C COMP VIS PA, P114; Vaswani Ashish, NIPS 2017, P6000; Vinyals O., NIPS 2016, P3630; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wang Z, 2003, CONF REC ASILOMAR C, P1398; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28; Yang D, 2018, IEEE SIGNAL PROC LET, V25, P55, DOI 10.1109/LSP.2017.2768660; YI KM, 2018, CVPR, P2666, DOI DOI 10.1109/CVPR.2018.00282; Yu Fisher, 2015, ICLR; Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891; Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zontak M, 2013, PROC CVPR IEEE, P1195, DOI 10.1109/CVPR.2013.158; Zontak M, 2011, PROC CVPR IEEE, P977, DOI 10.1109/CVPR.2011.5995401	54	19	19	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301011
C	Rotskoff, GM; Vanden-Eijnden, E		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Rotskoff, Grant M.; Vanden-Eijnden, Eric			Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The performance of neural networks on high-dimensional data distributions suggests that it may be possible to parameterize a representation of a given high-dimensional function with controllably small errors, potentially outperforming standard interpolation methods. We demonstrate, both theoretically and numerically, that this is indeed the case. We map the parameters of a neural network to a system of particles relaxing with an interaction potential determined by the loss function. We show that in the limit that the number of parameters n is large, the landscape of the mean-squared error becomes convex and the representation error in the function scales as O(n(-1)). In this limit, we prove a dynamical variant of the universal approximation theorem showing that the optimal representation can be attained by stochastic gradient descent, the algorithm ubiquitously used for parameter optimization in machine learning. In the asymptotic regime, we study the fluctuations around the optimal representation and show that they arise at a scale O(n(-1)). These fluctuations in the landscape identify the natural scale for the noise in stochastic gradient descent. Our results apply to both single and multi-layer neural networks, as well as standard kernel methods like radial basis functions.	[Rotskoff, Grant M.; Vanden-Eijnden, Eric] NYU, Courant Inst Math Sci, New York, NY 10003 USA	New York University	Rotskoff, GM (corresponding author), NYU, Courant Inst Math Sci, New York, NY 10003 USA.	rotskoff@cims.nyu.edu; eye2@cims.nyu.edu			James S. McDonnell Foundation; National Science Foundation (NSF) Materials Research Science and Engineering Center Program Award [DMR-1420073]; NSF [DMS-1522767]	James S. McDonnell Foundation; National Science Foundation (NSF) Materials Research Science and Engineering Center Program Award(National Science Foundation (NSF)); NSF(National Science Foundation (NSF))	We would like to thank Andrea Montanan and Matthieu Wyart for useful discussions regarding the fixed points of gradient flows in the Wasserstein metric. GMR was supported by the James S. McDonnell Foundation. EVE was supported by National Science Foundation (NSF) Materials Research Science and Engineering Center Program Award DMR-1420073; and by NSF Award DMS-1522767.	Auffinger A, 2013, ANN PROBAB, V41, P4214, DOI 10.1214/13-AOP862; Auffinger Antonio, 2012, COMMUNICATIONS PURE, V66, P165; Bach F., 2017, J MACH LEARN RES, V18, p1 629 681; Behler J, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.146401; Bengio Y., 2006, ADV NEURAL INFORM PR, P123; Berg  Jens, 2017, ARXIV171106464; Bottou L., 2016, ARXIV160604838; Bottou U, 2004, ADV NEUR IN, V16, P217; Choromanska A., 2014, ARXIV14120233; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Dean DS, 1996, J PHYS A-MATH GEN, V29, pL613, DOI 10.1088/0305-4470/29/24/001; Freeman C.D., 2016, ARXIV161101540; Hoffer E., 2017, ARXIV170508741; Hu Wenqing, 2017, ARXIV170507562; Jacot A, 2018, ARXIV180509545, P8571; Keskar N.S., 2016, ABS160904836; KHOO Y., 2018, ARXIV180210275; Mei S, 2018, P NATL ACAD SCI USA, V115, pE7665, DOI 10.1073/pnas.1806579115; Park J, 1991, NEURAL COMPUT, V3, P246, DOI 10.1162/neco.1991.3.2.246; Sagun L., 2014, ARXIV14126615; Schneider E, 2017, PHYS REV LETT, V119, DOI 10.1103/PhysRevLett.119.150601; Serfaty S, 2017, ARXIV171204095; Sirignano J., 2018, MEAN FIELD ANAL NEUR; Soudry D., 2016, ARXIV PREPRINT ARXIV; Venturi L., 2018, ARXIV PREPRINT ARXIV	28	19	18	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001067
C	Sanjabi, M; Ba, J; Razaviyayn, M; Lee, JD		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sanjabi, Maziar; Ba, Jimmy; Razaviyayn, Meisam; Lee, Jason D.			On the Convergence and Robustness of Training GANs with Regularized Optimal Transport	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Generative Adversarial Networks (GANs) are one of the most practical methods for learning data distributions. A popular GAN formulation is based on the use of Wasserstein distance as a metric between probability distributions. Unfortunately, minimizing the Wasserstein distance between the data distribution and the generative model distribution is a computationally challenging problem as its objective is non-convex, non-smooth, and even hard to compute. In this work, we show that obtaining gradient information of the smoothed Wasserstein GAN formulation, which is based on regularized Optimal Transport (OT), is computationally effortless and hence one can apply first order optimization methods to minimize this objective. Consequently, we establish theoretical convergence guarantee to stationarity for a proposed class of GAN optimization algorithms. Unlike the original non-smooth formulation, our algorithm only requires solving the discriminator to approximate optimality. We apply our method to learning MNIST digits as well as CIFAR-10 images. Our experiments show that our method is computationally efficient and generates images comparable to the state of the art algorithms given the same architecture and computational power.	[Sanjabi, Maziar; Razaviyayn, Meisam; Lee, Jason D.] Univ Southern Calif, Los Angeles, CA 90089 USA; [Ba, Jimmy] Univ Toronto, Toronto, ON, Canada	University of Southern California; University of Toronto	Sanjabi, M (corresponding author), Univ Southern Calif, Los Angeles, CA 90089 USA.	sanjabi@usc.edu; jimmy@cs.toronto.edu; razaviya@usc.edu; jasonlee@marshall.usc.edu			ARO [W911NF-11-1-0303]	ARO	MS and MI, acknowledge support from ARO W911NF-11-1-0303. The authors would like to thank the anonymous reviewers whose comments/suggestions helped improve the quality/clarity of the paper.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Allen-Zhu ZY, 2016, PR MACH LEARN RES, V48; Altschuler J., 2017, ADV NEURAL INFORM PR, P1961; [Anonymous], 2018, ARXIV180104406; Ba J., 2017, P 3 INT C LEARN REPR; Bellemare MG, 2017, ARXIV; Blondel M., 2017, ARXIV171006276; Borkar VS, 1997, SYST CONTROL LETT, V29, P291, DOI 10.1016/S0167-6911(97)90015-3; Bottou L., 2017, ARXIV170107875STATML; Bousquet O., 2017, TECH REP; Cicalese F., 2017, ARXIV170105243; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Daskalakis Constantinos, 2017, ARXIV171100141; Dziugaite G.K., 2015, ARXIV150503906; Farnia F., 2018, ADV NEURAL INFORM PR, P5254; Feizi S., 2017, ARXIV PREPRINT ARXIV; Genevay A., 2016, P NEUR INF PROC SYST, P3440; Genevay A, 2018, PR MACH LEARN RES, V84; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I., 2017, INT C NEURAL INF PRO; Heusel M., 2017, P 31 INT C NEUR INF, P6629; Knight PA, 2008, SIAM J MATRIX ANAL A, V30, P261, DOI 10.1137/060659624; Li J, 2017, ARXIV170609884; Lim Jae Hyun, 2017, ABS170502894 ARXIV; Mescheder Lars, 2017, ADV NEURAL INFORM PR, P1825; Metz L., 2016, ARXIV161102163; Nagarajan V, 2017, ADV NEUR IN, V30; Nair Vinod, 2014, THE CIFAR 10 DATASET; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; POSNER EC, 1975, IEEE T INFORM THEORY, V21, P388, DOI 10.1109/TIT.1975.1055416; Radford A., 2015, P COMP C; Radford A., 2018, ARXIV180305573; Salimans T., 2016, ADV NEUR IN, P2234; Seguy Vivien, 2017, ARXIV PREPRINT ARXIV; Thibault A., 2017, NIPS 17 WORKSH OPT T; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5	42	19	20	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001062
C	Thekumparampil, KK; Khetan, A; Lin, ZN; Oh, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Thekumparampil, Kiran Koshy; Khetan, Ashish; Lin, Zinan; Oh, Sewoong			Robustness of conditional GANs to noisy labels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study the problem of learning conditional generators from noisy labeled samples, where the labels are corrupted by random noise. A standard training of conditional GANs will not only produce samples with wrong labels, but also generate poor quality samples. We consider two scenarios, depending on whether the noise model is known or not. When the distribution of the noise is known, we introduce a novel architecture which we call Robust Conditional GAN (RCGAN). The main idea is to corrupt the label of the generated sample before feeding to the adversarial discriminator, forcing the generator to produce samples with clean labels. This approach of passing through a matching noisy channel is justified by accompanying multiplicative approximation bounds between the loss of the RCGAN and the distance between the clean real distribution and the generator distribution. This shows that the proposed approach is robust, when used with a carefully chosen discriminator architecture, known as projection discriminator. When the distribution of the noise is not known, we provide an extension of our architecture, which we call RCGAN-U, that learns the noise model simultaneously while training the generator. We show experimentally on MNIST and CIFAR-10 datasets that both the approaches consistently improve upon baseline approaches, and RCGAN-U closely matches the performance of RCGAN.	[Thekumparampil, Kiran Koshy; Khetan, Ashish; Oh, Sewoong] Univ Illinois, Urbana, IL 61801 USA; [Lin, Zinan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	University of Illinois System; University of Illinois Urbana-Champaign; Carnegie Mellon University	Thekumparampil, KK (corresponding author), Univ Illinois, Urbana, IL 61801 USA.	thekump2@illinois.edu; ashish.khetan09@gmail.com; zinanl@andrew.cmu.edu; swoh@illinois.edu	Lin, Zinan/T-2375-2019		NSF [CNS-1527754, CCF-1553452, CCF-1705007, RI-1815535, ACI-1445606]; Google Faculty Research Award; National Science Foundation [OCI1053575]	NSF(National Science Foundation (NSF)); Google Faculty Research Award(Google Incorporated); National Science Foundation(National Science Foundation (NSF))	This work is supported by NSF awards CNS-1527754, CCF-1553452, CCF-1705007, RI-1815535 and Google Faculty Research Award. This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number OCI1053575. Specifically, it used the Bridges system, which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). This work is partially supported by the generous research credits on AWS cloud computing resources from Amazon.	Achlioptas Panos, 2017, ARXIV170702392; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bai Yu, 2018, ARXIV180610586; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Biau G, 2018, ARXIV180307819; Biggio B., 2011, PROC ACML, P97; Bing L, 2003, THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P179, DOI 10.1109/icdm.2003.1250918; Binkowski Mikolaj, 2018, ICML; Bora A, 2017, PR MACH LEARN RES, V70; Bora Ashish, 2018, INT C LEARN REPR; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Denton Emily L, 2015, NEURIPS, V2, P4; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Jolicoeur-Martineau Alexia, 2019, P INT C LEARN REPR I; Karger D. R., 2011, ADV NEURAL INFORM PR, P1953; Karras T., 2017, PROGR GROWING GANS I; Khetan A., 2017, P 6 INT C LEARN REPR; Kim Taeksoo, 2017, P 34 INT C MACH LEAR, P1857, DOI [10.5555/3305381.3305573, DOI 10.5555/3305381.3305573]; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kocaoglu Murat, 2018, INT C LEARN REPR, P3; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Liang X., 2017, DUAL MOTION GAN FUTU; Lin Z., 2017, ARXIV171204086; Mirza Mehdi, 2014, ARXIV14111784, P2672; Miyato Takeru, 2018, ARXIV180205637; Mukherjee S., 2018, CORR; Natarajan Nagarajan, 2013, ADV NEURAL INFORM PR; Nguyen Anh, 2016, ARXIV161200005; Odena A., 2016, SEMISUPERVISED LEARN; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S, 2016, PR MACH LEARN RES, V48; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T, 2016, ADV NEUR IN, V29; Sanjabi M., 2018, P NEURIPS MONTR QC, P7091; Sen Rajat, 2018, ARXIV180609708; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Sriperumbudur B. K., 2009, ARXIV PREPRINT ARXIV; Stempfel G, 2007, LECT NOTES ARTIF INT, V4754, P328; Stempfel G, 2009, LECT NOTES COMPUT SC, V5768, P884, DOI 10.1007/978-3-642-04274-4_91; Sukhbaatar Sainbayar, 2014, ARXIV14062080, P2; Sutherland D.J., 2016, ARXIV161104488; van den Oord A, 2016, PR MACH LEARN RES, V48; Xu Zhi, 2018, ARXIV180209700; Yann L., 1998, MNIST DATABASE HANDW, P1; Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728; Zhang H., 2017, ICCV; Zhang Han, 2018, ARXIV180508318; Zhang P., 2017, ARXIV171102771; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36; Zhu Jun-Yan, 2017, ICCV	58	19	20	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004079
C	Wangni, JQ; Wang, JL; Liu, J; Zhang, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wangni, Jianqiao; Wang, Jialei; Liu, Ji; Zhang, Tong			Gradient Sparsification for Communication-Efficient Distributed Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DESCENT	Modern large-scale machine learning applications require stochastic optimization algorithms to be implemented on distributed computational architectures. A key bottleneck is the communication overhead for exchanging information such as stochastic gradients among different workers. In this paper, to reduce the communication cost, we propose a convex optimization formulation to minimize the coding length of stochastic gradients. The key idea is to randomly drop out coordinates of the stochastic gradient vectors and amplify the remaining coordinates appropriately to ensure the sparsified gradient to be unbiased. To solve the optimal sparsification efficiently, a simple and fast algorithm is proposed for an approximate solution, with a theoretical guarantee for sparseness. Experiments on l(2)-regularized logistic regression, support vector machines and convolutional neural networks validate our sparsification approaches.	[Wangni, Jianqiao] Univ Penn, Tencent AI Lab, Philadelphia, PA 19104 USA; [Wang, Jialei] Two Sigma Investments, New York, NY USA; [Liu, Ji] Univ Rochester, Tencent AI Lab, Rochester, NY 14627 USA; [Zhang, Tong] Tencent AI Lab, Bellevue, WA USA	University of Pennsylvania; University of Rochester	Wangni, JQ (corresponding author), Univ Penn, Tencent AI Lab, Philadelphia, PA 19104 USA.	wnjq@seas.upenn.edu; jialei.wang@twosigma.com; ji.liu.uwisc@gmail.com; tongzhang@tongzhang-ml.org	Zhang, Tong/HGC-1090-2022		NSF [CCF1718513]; IBM faculty award; NEC fellowship	NSF(National Science Foundation (NSF)); IBM faculty award(International Business Machines (IBM)); NEC fellowship	Ji Liu is in part supported by NSF CCF1718513, IBM faculty award, and NEC fellowship.	Ahmed A., 2014, P 11 USENIX C OP SYS, P583; Aji A.F., 2017, P 2017 C EMP METH NA, DOI [10.18653/v1/D17-1045, DOI 10.18653/V1/D17-1045]; Alistarh D, 2017, ADV NEUR IN, V30; Arjevani Y., 2015, ADV NEURAL INFORM PR, V28, P1756; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Chen Jiecao, 2016, ADV NEURAL INFORM PR, V29, P3727; Chen ST, 2016, JMLR WORKSH CONF PRO, V51, P1299; De Sa C., 2015, NIPS, P2674; De Sa C, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P561, DOI 10.1145/3079856.3080248; Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Jaggi M., 2014, ADV NEURAL INF PROCE, V4, P3068; Jin C, 2017, PR MACH LEARN RES, V70; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Kingma D., 2014, ADAM METHOD STOCHAST; LEE JD, 2017, J MACH LEARN RES, V18; Li JL, 2017, INT CONF MACH LEARN, P35; Li M, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P661, DOI 10.1145/2623330.2623612; Liang Y., 2014, ADV NEURAL INFORM PR, P3113; Lin Yujun, 2018, P INT C LEARNING REP; Liu J, 2015, J MACH LEARN RES, V16, P285; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Seide F, 2014, INTERSPEECH, P1058; Shamir O, 2014, PR MACH LEARN RES, V32, P1000; Suresh AT, 2017, PR MACH LEARN RES, V70; Tsitsiklis J. N., 1987, Journal of Complexity, V3, P231, DOI 10.1016/0885-064X(87)90013-6; Wang J., 2017, P MACHINE LEARNING R, P1882; Wen W., 2017, P NIPS, P1509; Xing Eric P., 2015, IEEE Transactions on Big Data, V1, P49, DOI 10.1109/TBDATA.2015.2472014; Zhang T., 2004, P 21 INT C MACH LEAR, P116, DOI 10.1145/1015330.1015332; Zhang Y., 2012, ADV NEURAL INFORM PR, P1502	36	19	19	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301030
C	Daniely, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Daniely, Amit			SGD Learns the Conjugate Kernel Class of the Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We show that the standard stochastic gradient decent (SGD) algorithm is guaranteed to learn, in polynomial time, a function that is competitive with the best function in the conjugate kernel space of the network, as defined in Daniely et al. [2016]. The result holds for log-depth networks from a rich family of architectures. To the best of our knowledge, it is the first polynomial-time guarantee for the standard neural network learning algorithm for networks of depth more that two. As corollaries, it follows that for neural networks of any depth between 2 and log (n), SGD is guaranteed to learn, in polynomial time, constant degree polynomials with polynomially bounded coefficients. Likewise, it follows that SGD on large enough networks can learn any continuous function (not in polynomial time), complementing classical expressivity results.	[Daniely, Amit] Hebrew Univ Jerusalem, Jerusalem, Israel; [Daniely, Amit] Google Res, Mountain View, CA 94041 USA	Hebrew University of Jerusalem; Google Incorporated	Daniely, A (corresponding author), Hebrew Univ Jerusalem, Jerusalem, Israel.; Daniely, A (corresponding author), Google Res, Mountain View, CA 94041 USA.	amit.daniely@mail.huji.ac.il	Jeong, Yongwook/N-7413-2016					Andoni A, 2014, PR MACH LEARN RES, V32, P1908; Anselmi F., 2015, ARXIV50801084; Arora S, 2014, PR MACH LEARN RES, V32; Arora Sanjeev, 2016, ARXIV61208795; Bach F., 2015, EQUIVALENCE KERNEL Q; Bshouty NH, 1998, INFORM PROCESS LETT, V65, P217, DOI 10.1016/S0020-0190(97)00204-4; Cho Y., 2009, NIPS, P342; Daniely A., 2014, STOC; Daniely A., 2016, NIPS; Daniely A., 2016, COLT; Daniely A., 2017, ARXIV170307872; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Hazan T., 2015, ARXIV150805133; Kamath Gautam, 2015, BOUNDS EXPECTATION M; Kar P., 2012, ARXIV12016530; KEARNS M, 1994, J ACM, V41, P67, DOI 10.1145/174644.174647; Kharitonov M., 1993, Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, P372, DOI 10.1145/167088.167197; Klivans A. R., 2006, FOCS; Klivans AR, 2007, MACH LEARN, V69, P97, DOI 10.1007/s10994-007-5010-1; Rivest R.L., 1989, NIPS, P494	21	19	19	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402046
C	Deng, ZJ; Zhang, H; Liang, XD; Yang, LN; Xu, SZ; Zhu, J; Xing, EP		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Deng, Zhijie; Zhang, Hao; Liang, Xiaodan; Yang, Luona; Xu, Shizhen; Zhu, Jun; Xing, Eric P.			Structured Generative Adversarial Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We study the problem of conditional generative modeling based on designated semantics or structures. Existing models that build conditional generators either require massive labeled instances as supervision or are unable to accurately control the semantics of generated samples. We propose structured generative adversarial networks (SGANs) for semi-supervised conditional generative modeling. SGAN assumes the data x is generated conditioned on two independent latent variables: y that encodes the designated semantics, and z that contains other factors of variation. To ensure disentangled semantics in y and z, SGAN builds two collaborative games in the hidden space to minimize the reconstruction error of y and z, respectively. Training SGAN also involves solving two adversarial games that have their equilibrium concentrating at the true joint data distributions p(x, z) and p(x, y), avoiding distributing the probability mass diffusely over data space that MLE-based methods may suffer. We assess SGAN by evaluating its trained networks, and its performance on downstream tasks. We show that SGAN delivers a highly controllable generator, and disentangled representations; it also establishes start-of-the-art results across multiple datasets when applied for semi-supervised image classification (1.27%, 5.73%, 17.26% error rates on MNIST, SVHN and CIFAR-10 using 50, 1000 and 4000 labels, respectively). Benefiting from the separate modeling of y and z, SGAN can generate images with high visual quality and strictly following the designated semantic, and can be extended to a wide spectrum of applications, such as style transfer.	[Deng, Zhijie; Xu, Shizhen; Zhu, Jun] Tsinghua Univ, Beijing, Peoples R China; [Zhang, Hao; Liang, Xiaodan; Yang, Luona; Xu, Shizhen] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Zhang, Hao; Xing, Eric P.] Petuum Inc, Pittsburgh, PA USA	Tsinghua University; Carnegie Mellon University	Zhu, J (corresponding author), Tsinghua Univ, Beijing, Peoples R China.	dzj17@mails.tsinghua.edu.cn; hao@cs.cmu.edu; xiaodan1@cs.cmu.edu; luonay1@cs.cmu.edu; xsz12@mails.tsinghua.edu.cn; dcszj@mail.tsinghua.edu.cn; epxing@cs.cmu.edu	Jeong, Yongwook/N-7413-2016		NSF China [61620106010, 61621136008, 61332007]; MIIT Grant of Int. Man. Comp. Stan [2016ZXFB00001]; Tsinghua Tiangong Institute for Intelligent Computing; NVIDIA NVAIL Program; AFRL/DARPA project [FA872105C0003];  [FA870215D0002]	NSF China(National Natural Science Foundation of China (NSFC)); MIIT Grant of Int. Man. Comp. Stan; Tsinghua Tiangong Institute for Intelligent Computing; NVIDIA NVAIL Program; AFRL/DARPA project; 	Zhijie Deng and Jun Zhu are supported by NSF China (Nos. 61620106010, 61621136008, 61332007), the MIIT Grant of Int. Man. Comp. Stan (No. 2016ZXFB00001), Tsinghua Tiangong Institute for Intelligent Computing and the NVIDIA NVAIL Program. Hao Zhang is supported by the AFRL/DARPA project FA872105C0003. Xiaodan Liang is supported by award FA870215D0002.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Bergstra James, 2010, THEANO CPU GPU MATH, P3; Donahue J., 2016, ARXIV160509782; Dumoulin Vincent, 2016, ARXIV E PRINTS; Fu Tzu-Chien, 2017, ARXIV170501314; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hu ZT, 2017, PR MACH LEARN RES, V70; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Laine Samuli, 2016, ARXIV161002242; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li CX, 2015, ADV NEUR IN, V28; Li Chunyuan, 2017, NIPS; Liang XD, 2017, IEEE I CONF COMP VIS, P3382, DOI 10.1109/ICCV.2017.364; Maaloe L, 2016, PR MACH LEARN RES, V48; Mirza M., 2014, ARXIV; Ng AY, READING DIGITS NATUR; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Reed S. E., 2016, ADV NEURAL INFORM PR, P217; Reed S, 2016, PR MACH LEARN RES, V48; Reed Scott, 2017, INT C LEARN REPR; Salakhutdinov Ruslan, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPR.2009.5206577; Salimans T, 2016, ADV NEUR IN, V29; Springenberg Jost Tobias, 2015, ARXIV151106390; Tran Luan, 2017, C COMP VIS PATT REC; Wang H., 2017, ARXIV170307255; Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20; Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47; Zhang H, 2015, ARXIV151206216	33	19	20	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403093
C	Park, S; Min, S; Choi, HS; Yoon, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Park, Seunghyun; Min, Seonwoo; Choi, Hyun-Soo; Yoon, Sungroh			Deep Recurrent Neural Network-Based Identification of Precursor microRNAs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MIRNA GENES; CLASSIFICATION; REAL	MicroRNAs (miRNAs) are small non-coding ribonucleic acids (RNAs) which play key roles in post-transcriptional gene regulation. Direct identification of mature miRNAs is infeasible due to their short lengths, and researchers instead aim at identifying precursor miRNAs (pre-miRNAs). Many of the known pre-miRNAs have distinctive stem-loop secondary structure, and structure-based filtering is usually the first step to predict the possibility of a given sequence being a pre-miRNA. To identify new pre-miRNAs that often have non-canonical structure, however, we need to consider additional features other than structure. To obtain such additional characteristics, existing computational methods rely on manual feature extraction, which inevitably limits the efficiency, robustness, and generalization of computational identification. To address the limitations of existing approaches, we propose a pre-miRNA identification method that incorporates (1) a deep recurrent neural network (RNN) for automated feature learning and classification, (2) multimodal architecture for seamless integration of prior knowledge (secondary structure), (3) an attention mechanism for improving long-term dependence modeling, and (4) an RNN-based class activation mapping for highlighting the learned representations that can contrast pre-miRNAs and non-pre-miRNAs. In our experiments with recent benchmarks, the proposed approach outperformed the compared state-of-the-art alternatives in terms of various performance metrics.	[Park, Seunghyun; Min, Seonwoo; Choi, Hyun-Soo; Yoon, Sungroh] Seoul Natl Univ, Elect & Comp Engn, Seoul 08826, South Korea; [Park, Seunghyun] Korea Univ, Sch Elect Engn, Seoul 02841, South Korea	Seoul National University (SNU); Korea University	Yoon, S (corresponding author), Seoul Natl Univ, Elect & Comp Engn, Seoul 08826, South Korea.	sryoon@snu.ac.kr	Jeong, Yongwook/N-7413-2016; Choi, Hyun-Soo/W-9114-2019	Choi, Hyun-Soo/0000-0002-3594-8948; Park, Seunghyun/0000-0002-8509-9163	Samsung Research Funding Center of the Samsung Electronics [SRFC-IT1601-05]; Institute for Information & communications Technology Promotion (IITP) - Korea government (MSIT) [2016-0-00087]; Future Flagship Program - Ministry of Trade, Industry & Energy (MOTIE, Korea) [10053249]; Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Science, ICT & Future Planning [2016M3A7B4911115]; Brain Korea 21 Plus Project	Samsung Research Funding Center of the Samsung Electronics(Samsung); Institute for Information & communications Technology Promotion (IITP) - Korea government (MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); Future Flagship Program - Ministry of Trade, Industry & Energy (MOTIE, Korea)(Ministry of Trade, Industry & Energy (MOTIE), Republic of Korea); Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Science, ICT & Future Planning(National Research Foundation of KoreaMinistry of Science, ICT & Future Planning, Republic of Korea); Brain Korea 21 Plus Project	This work was supported in part by the Samsung Research Funding Center of the Samsung Electronics [No. SRFC-IT1601-05], the Institute for Information & communications Technology Promotion (IITP) grant funded by the Korea government (MSIT) [No. 2016-0-00087], the Future Flagship Program funded by the Ministry of Trade, Industry & Energy (MOTIE, Korea) [No. 10053249], the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT & Future Planning [No. 2016M3A7B4911115], and Brain Korea 21 Plus Project in 2017.	Agarwal S, 2010, BMC BIOINFORMATICS, V11, DOI 10.1186/1471-2105-11-S1-S29; Baldi P., 2001, BIOINFORMATICS MACHI; Bartel DP, 2004, CELL, V116, P281, DOI 10.1016/S0092-8674(04)00045-5; Batuwita R, 2009, BIOINFORMATICS, V25, P989, DOI 10.1093/bioinformatics/btp107; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bottou L., 1991, PROC NEURO NIMES, V91, P12; Bu D., 2011, NUCLEIC ACIDS RES; Chen J., 2016, SCI REPORTS, V6; Chung J., 2014, ARXIV14123555; Friedlander MR, 2008, NAT BIOTECHNOL, V26, P407, DOI 10.1038/nbt1394; Griffiths-Jones S, 2006, NUCLEIC ACIDS RES, V34, pD140, DOI 10.1093/nar/gkj112; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hofacker IL, 2003, NUCLEIC ACIDS RES, V31, P3429, DOI 10.1093/nar/gkg599; Jiang P, 2007, NUCLEIC ACIDS RES, V35, pW339, DOI 10.1093/nar/gkm368; Kin T, 2007, NUCLEIC ACIDS RES, V35, pD145, DOI 10.1093/nar/gkl837; King G., 2001, POLIT ANAL, V9, P137, DOI [10.1093/oxfordjournals.pan.a004868, DOI 10.1093/OXFORDJOURNALS.PAN.A004868]; Kingma D.P, P 3 INT C LEARNING R; Kleftogiannis D, 2013, J BIOMED INFORM, V46, P563, DOI 10.1016/j.jbi.2013.02.002; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; LEE RC, 1993, CELL, V75, P843, DOI 10.1016/0092-8674(93)90529-Y; Lestrade L, 2006, NUCLEIC ACIDS RES, V34, pD158, DOI 10.1093/nar/gkj002; LILLIEFORS HW, 1967, J AM STAT ASSOC, V62, P399, DOI 10.2307/2283970; Lopes IDN, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/1471-2105-15-124; Lorenz R, 2011, ALGORITHM MOL BIOL, V6, DOI 10.1186/1748-7188-6-26; Lyngso RB, 2004, LECT NOTES COMPUT SC, V3142, P919; Mathelier A, 2010, BIOINFORMATICS, V26, P2226, DOI 10.1093/bioinformatics/btq329; Mendes ND, 2009, NUCLEIC ACIDS RES, V37, P2419, DOI 10.1093/nar/gkp145; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Rahman ME, 2012, GENOMICS, V99, P189, DOI 10.1016/j.ygeno.2012.02.001; Rocktaschel Tim, 2015, ARXIV150906664; Starega-Roslan J, 2015, NUCLEIC ACIDS RES, V43, DOI 10.1093/nar/gkv968; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Thomas J, 2017, INT CONF BIG DATA, P96, DOI 10.1109/BIGCOMP.2017.7881722; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tran VDT, 2015, RNA, V21, P775, DOI 10.1261/rna.043612.113; Vinyals Oriol, 2015, NIPS; Wei LY, 2014, IEEE ACM T COMPUT BI, V11, P192, DOI 10.1109/TCBB.2013.146; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Xue CH, 2005, BMC BIOINFORMATICS, V6, DOI 10.1186/1471-2105-6-310; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319	40	19	19	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402092
C	Canini, K; Cotter, A; Gupta, MR; Fard, MM; Pfeifer, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Canini, K.; Cotter, A.; Gupta, M. R.; Fard, M. Milani; Pfeifer, J.			Fast and Flexible Monotonic Functions with Ensembles of Lattices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				REGRESSION	For many machine learning problems, there are some inputs that are known to be positively (or negatively) related to the output, and in such cases training the model to respect that monotonic relationship can provide regularization, and makes the model more interpretable. However, flexible monotonic functions are computationally challenging to learn beyond a few features. We break through this barrier by learning ensembles of monotonic calibrated interpolated look-up tables (lattices). A key contribution is an automated algorithm for selecting feature subsets for the ensemble base models. We demonstrate that compared to random forests, these ensembles produce similar or better accuracy, while providing guaranteed monotonicity consistent with prior knowledge, smaller model size and faster evaluation.	[Canini, K.; Cotter, A.; Gupta, M. R.; Fard, M. Milani; Pfeifer, J.] Google Inc, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA	Google Incorporated	Canini, K (corresponding author), Google Inc, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.	canini@google.com; acotter@google.com; mayagupta@google.com; mmilanifard@google.com; janpf@google.com						Abu-Mostafa Yaser S, 1993, ADV NEURAL INFORM PR, P73; Amaratunga D, 2008, BIOINFORMATICS, V24, P2010, DOI 10.1093/bioinformatics/btn356; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Cotter A., 2016, C LEARN THEOR, P729; Daniels H, 2010, IEEE T NEURAL NETWOR, V21, P906, DOI 10.1109/TNN.2010.2044803; Dugas C., 2009, J MACHINE LEARNING R; Fernandez-Delgado Manuel, 2014, J MACHINE LEARNING R; Garcia E. K., 2009, ADV NEURAL INFORM PR; Garcia E, 2012, IEEE T IMAGE PROCESS, V21, P4128, DOI 10.1109/TIP.2012.2200902; Gupta M, 2016, J MACH LEARN RES, V17; Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601; Howard A., 2007, ADV NEURAL INFORM PR; Kotlowski W., 2009, P 26 ANN INT C MACH, P537; Newman C. B. D., 1998, UCI REPOSITORY MACHI; Qu YJ, 2011, IEEE T NEURAL NETWOR, V22, P2447, DOI 10.1109/TNN.2011.2167348; Sharma G., 2002, DIGITAL COLOR IMAGIN; Spouge J, 2003, J OPTIMIZ THEORY APP, V117, P585, DOI 10.1023/A:1023901806339; Weichselbaumer D, 2005, J ECON SURV, V19, P479, DOI 10.1111/j.0950-0804.2005.00256.x; Xu HH, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P2132; Ye Y., 2013, PATTERN RECOGNITION; Zhang L, 2014, INT RELIAB PHY SYM	21	19	19	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703013
C	Gao, HY; Mao, JH; Zhou, J; Huang, ZH; Wang, L; Xu, W		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Gao, Haoyuan; Mao, Junhua; Zhou, Jie; Huang, Zhiheng; Wang, Lei; Xu, Wei			Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					In this paper, we present the mQA model, which is able to answer questions about the content of an image. The answer can be a sentence, a phrase or a single word. Our model contains four components: a Long Short-Term Memory (LSTM) to extract the question representation, a Convolutional Neural Network (CNN) to extract the visual representation, an LSTM for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer. We construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate our mQA model. It contains over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations. The quality of the generated answers of our mQA model on this dataset is evaluated by human judges through a Turing Test. Specifically, we mix the answers provided by humans and our model. The human judges need to distinguish our model from the human. They will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the quality of the answer. We propose strategies to monitor the quality of this evaluation process. The experiments show that in 64.7% of cases, the human judges cannot distinguish our model from humans. The average score is 1.454 (1.918 for human). The details of this work, including the FM-IQA dataset, can be found on the project page: http://idl.baidu.com/FM-IQA.html.	[Gao, Haoyuan; Zhou, Jie; Huang, Zhiheng; Wang, Lei; Xu, Wei] Baidu Res, Sunnyvale, CA 94089 USA; [Mao, Junhua] Univ Calif Los Angeles, Los Angeles, CA 90024 USA	Baidu; University of California System; University of California Los Angeles	Gao, HY (corresponding author), Baidu Res, Sunnyvale, CA 94089 USA.	gaohaoyuan@baidu.com; mjhustc@ucla.edu; zhoujie01@baidu.com; huangzhiheng@baidu.com; wanglei22@baidu.com; wei.xu@baidu.com						[Anonymous], 2015, ARXIV150500468; Bigham Jeffrey P, 2010, P 23 NUAL ACM S US I, P333, DOI [10.1145/1866029.1866080?casa_token=eqdciLsaAKsAAAAA:v_iSvCJKVqaa-xY5ls_4fwveOme0IVWxS0hy40kPYpp, DOI 10.1145/1866029.1866080]; Chen LC., ARXIV 2015ABS1412706; Chen X., 2015, P IEEE INT C COMP VI; Cho K., 2014, P 2014 C EMP METH NA, P1724; Donahue J., 2015, CVPR; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Fang H., 2015, C COMP VIS PATT REC; Geman D, 2015, P NATL ACAD SCI USA, V112, P3618, DOI 10.1073/pnas.1422953112; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Grubinger M., 2006, INT WORKSHOP ONTOIMA, V2; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Kalchbrenner Nal, 2013, P 2013 C EMP METH NA, P1700, DOI DOI 10.1146/ANNUREV.NEURO.26.041002.131047; Karpathy A., 2015, CVPR; Kiros Ryan, 2015, ARXIV14112539; Klein B, 2015, P IEEE CVF C COMP VI, P4437; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lavie A., 2007, WORKSH STAT MACH TRA, P228; Lebret R., 2014, ARXIV14128419; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Lin T., 2014, ECCV; Malinowski M, 2015, ARXIV150501121; Malinowski M., 2014, ADV NEURAL INFORM PR, V27, P1682; Mao J., 2015, ARXIV150406692; Mao J, 2015, 3 INT C LEARN REPR I; Mao J., 2014, NIPS DEEPLEARNING WO; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Mikolov Tomas, 2014, P 3 INT C LEARN REPR; Nair V., 2010, ICML, P807; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Ren M., 2015, ARXIV150502074; Russakovsky O., 2014, IMAGENET LARGE SCALE; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Szegedy C., 2014, 2015 IEEE C COMP VIS, P1, DOI [DOI 10.1109/CVPR.2015.7298594, 10.48550/arxiv.1409.4842]; Turing A.M., 1950, MIND, V49, P433; Vedantam R., 2015, CVPR; Vinyals O., 2015, CVPR; WU ZB, 1994, 32ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P133; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Young P., 2014, ACL, P479; Zhu J., 2014, P 27 INT C NEUR INF, V1, P1125	44	19	19	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100009
C	McInerney, J; Ranganath, R; Blei, D		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		McInerney, James; Ranganath, Rajesh; Blei, David			The Population Posterior and Bayesian Modeling on Streams	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				INFERENCE	Many modern data analysis problems involve inferences from streaming data. However, streaming data is not easily amenable to the standard probabilistic modeling approaches, which require conditioning on finite data. We develop population variational Bayes, a new approach for using Bayesian modeling to analyze streams of data. It approximates a new type of distribution, the population posterior, which combines the notion of a population distribution of the data with Bayesian inference in a probabilistic model. We develop the population posterior for latent Dirichlet allocation and Dirichlet process mixtures. We study our method with several large-scale data sets.	[McInerney, James; Blei, David] Columbia Univ, New York, NY 10027 USA; [Ranganath, Rajesh] Princeton Univ, Princeton, NJ 08544 USA	Columbia University; Princeton University	McInerney, J (corresponding author), Columbia Univ, New York, NY 10027 USA.	james@cs.columbia.edu; rajeshr@cs.princeton.edu; david.blei@columbia.edu	Mcinerney, James/AAF-7234-2020		NSF [IIS-0745520, IIS-1247664, IIS-1009542]; ONR [N00014-11-1-0651]; DARPA [FA8750-14-2-0009, N66001-15-C-4032]; NDSEG; Facebook; Adobe; Amazon; Siebel Scholar Foundation; John Templeton Foundation	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NDSEG; Facebook(Facebook Inc); Adobe; Amazon; Siebel Scholar Foundation; John Templeton Foundation	We thank Allison Chaney, John Cunningham, Alp Kucukelbir, Stephan Mandt, Peter Orbanz, Theo Weber, FrankWood, and the anonymous reviewers for their comments. This work is supported by NSF IIS-0745520, IIS-1247664, IIS-1009542, ONR N00014-11-1-0651, DARPA FA8750-14-2-0009, N66001-15-C-4032, NDSEG, Facebook, Adobe, Amazon, and the Siebel Scholar and John Templeton Foundations.	Ahmed A., 2011, INT C ART INT STAT, P101; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Bernardo J. M., 1994, BAYESIAN THEORY; Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Blondel V. D., 2012, ARXIV12100137; Bottou Leon, 1999, ON LINE LEARNING NEU, P9, DOI DOI 10.1017/CBO9780511569920.003; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038; Efron B., 1994, INTRO BOOTSTRAP; ESCOBAR MD, 1995, J AM STAT ASSOC, V90, P577, DOI 10.2307/2291069; Ghahramani Z., 2000, NIPS 2000 WORKSH ONL, P101; Hoffman M. D., 2015, INT C ART INT STAT, P101; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Honkela A., 2003, 4 INT S IND COMP AN, P803; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D. P., 2013, AUTO ENCODING VARIAT; Ranganath R., 2014, P 17 INT C ART INT S, P805; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Saul LK, 1996, ADV NEUR IN, V8, P486; SETHURAMAN J, 1994, STAT SINICA, V4, P639; Tank A., 2015, INT C ART INT STAT; Theis L., 2015, INT C MACH LEARN; Titsias MK, 2014, PR MACH LEARN RES, V32, P1971; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wallach H., 2009, P 26 ANN INT C MACHI, V382, P1105, DOI DOI 10.1145/1553374.1553515; Yao LM, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P937	27	19	19	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101053
C	Wang, J; Zhou, JY; Liu, J; Wonka, P; Ye, JP		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wang, Jie; Zhou, Jiayu; Liu, Jun; Wonka, Peter; Ye, Jieping			A Safe Screening Rule for Sparse Logistic Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				CLASSIFICATION; SELECTION; MODEL	The l(1)-regularized logistic regression (or sparse logistic regression) is a widely used method for simultaneous classification and feature selection. Although many recent efforts have been devoted to its efficient implementation, its application to high dimensional data still poses significant challenges. In this paper, we present a fast and effective sparse logistic regression screening rule (Slores) to identify the "0" components in the solution vector, which may lead to a substantial reduction in the number of features to be entered to the optimization. An appealing feature of Slores is that the data set needs to be scanned only once to run the screening and its computational cost is negligible compared to that of solving the sparse logistic regression problem. Moreover, Slores is independent of solvers for sparse logistic regression, thus Slores can be integrated with any existing solver to improve the efficiency. We have evaluated Slores using high-dimensional data sets from different applications. Experiments demonstrate that Slores outperforms the existing state-of-the-art screening rules and the efficiency of solving sparse logistic regression can be improved by one magnitude.	[Wang, Jie; Zhou, Jiayu; Wonka, Peter; Ye, Jieping] Arizona State Univ, Tempe, AZ 85287 USA; [Liu, Jun] SAS Inst Inc, Cary, NC 27513 USA	Arizona State University; Arizona State University-Tempe; SAS Institute Inc	Wang, J (corresponding author), Arizona State Univ, Tempe, AZ 85287 USA.	jie.wang.ustc@asu.edu; jiayu.zhou@asu.edu; jun.liu@sas.com; peter.wonka@asu.edu; jieping.ye@asu.edu						Asgary MP, 2007, BIOINFORMATICS, V23, P3125, DOI 10.1093/bioinformatics/btm324; BOYD CR, 1987, J TRAUMA, V27, P370, DOI 10.1097/00005373-198704000-00005; Brzezinski J. R., 1999, Proceedings. Tenth International Workshop on Database and Expert Systems Applications. DEXA 99, P755, DOI 10.1109/DEXA.1999.795279; Chaudhuri Kamalika, 2008, NIPS; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; El Ghaoui L., ARXIV10094219V2; Friedman J., 2000, ANN STAT, V38; Genkin A, 2007, TECHNOMETRICS, V49, P291, DOI 10.1198/004017007000000245; Gould S, 2008, INT J COMPUT VISION, V80, P300, DOI 10.1007/s11263-008-0140-x; Koh KM, 2007, J MACH LEARN RES, V8, P1519; Krishnapuram B, 2005, IEEE T PATTERN ANAL, V27, P957, DOI 10.1109/TPAMI.2005.127; Lee S. I., 2006, AAAI 06; Liao JG, 2007, BIOINFORMATICS, V23, P1945, DOI 10.1093/bioinformatics/btm287; Liu J., 2009, SLEP SPARSE LEARNING; Martins S., 2007, IEEE ICIP, P309; Nesterov Y., 2018, APPL OPTIMIZATION; Palei SK, 2009, SAFETY SCI, V47, P88, DOI 10.1016/j.ssci.2008.01.002; Park MY, 2007, J ROY STAT SOC B, V69, P659, DOI 10.1111/j.1467-9868.2007.00607.x; Sartor MA, 2009, BIOINFORMATICS, V25, P211, DOI 10.1093/bioinformatics/btn592; Sun DQ, 2009, BIOL PSYCHIAT, V66, P1055, DOI 10.1016/j.biopsych.2009.07.019; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x; Ueda N, 2003, ADV NEURAL INFORM PR, P737; Wang J., 2013, ARXIV13074145V2; Wu TT, 2009, BIOINFORMATICS, V25, P714, DOI 10.1093/bioinformatics/btp041; Xiang Z. J., 2012, IEEE ICASSP; Zhu J, 2004, BIOSTATISTICS, V5, P427, DOI 10.1093/biostatistics/kxg046; Zhu J, 2002, ADV NEUR IN, V14, P1081	28	19	19	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100073
C	Littlewort, GC; Bartlett, MS; Fasel, IR; Chenu, J; Kanda, T; Ishiguro, H; Movellan, JR		Thrun, S; Saul, K; Scholkopf, B		Littlewort, GC; Bartlett, MS; Fasel, IR; Chenu, J; Kanda, T; Ishiguro, H; Movellan, JR			Towards social robots: Automatic evaluation of human-robot interaction by face detection and expression classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				OBJECT RECOGNITION	Computer animated agents and robots bring a social dimension to human computer interaction and force us to think in new ways about how computers could be used in daily life. Face to face communication is a real-time process operating at a time scale of less than a second. In this paper we present progress on a perceptual primitive to automatically detect frontal faces in the video stream and code them with respect to 7 dimensions in real time: neutral, anger, disgust, fear, joy, sadness, surprise. The face finder employs a cascade of feature detectors trained with boosting techniques [13, 2]. The expression recognizer employs a combination of AdaBoost and SVM's. The generalization performance to new subjects for a 7-way forced choice was over 90% correct on two publicly available datasets. The outputs of the classifier change smoothly as a function of time, providing a potentially valuable representation to code facial expression dynamics in a fully automatic and unobtrusive manner. The system was deployed and evaluated for measuring spontaneous facial expressions in the field in an application for automatic assessment of human-robot interaction.	Univ Calif San Diego, Inst Neural Computat, San Diego, CA 92103 USA	University of California System; University of California San Diego	Littlewort, GC (corresponding author), Univ Calif San Diego, Inst Neural Computat, San Diego, CA 92103 USA.		Kanda, Takayuki/I-5843-2016	Kanda, Takayuki/0000-0002-9546-5825				Ekman P., 1976, HIL0984 UCSF; Fasel I., 2002, P INT C ART NEUR NET; Freund Y, 1996, P 13 INT C MACH LEAR, P148, DOI DOI 10.5555/3091696.3091715; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Ishiguro H, 2001, IND ROBOT, V28, P498, DOI 10.1108/01439910110410051; Kanade T., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P46, DOI 10.1109/AFGR.2000.840611; LADES M, 1993, IEEE T COMPUT, V42, P300, DOI 10.1109/12.210173; Lyons M. J., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P202, DOI 10.1109/AFGR.2000.840635; PADGETT C, 1997, ADV NEURAL INFORMATI, V9; Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647; Schneiderman H, 1998, PROC CVPR IEEE, P45, DOI 10.1109/CVPR.1998.698586; SUNG KK, 1994, AIM1521; VIOLA P, 2001, 200101 CAMBR RES LAB	13	19	21	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1563	1570						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500194
C	Chechik, G; Globerson, A; Tishby, N; Anderson, MJ; Young, ED; Nelken, I		Dietterich, TG; Becker, S; Ghahramani, Z		Chechik, G; Globerson, A; Tishby, N; Anderson, MJ; Young, ED; Nelken, I			Group redundancy measures reveal redundancy reduction in the auditory pathway	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				VISUAL INFORMATION	The way groups of auditory neurons interact to code acoustic information is investigated using an information theoretic approach. We develop measures of redundancy among groups of neurons, and apply them to the study of collaborative coding efficiency in two processing stations in the auditory pathway: the inferior colliculus (IC) and the primary auditory cortex (AI). Under two schemes for the coding of the acoustic content, acoustic segments coding and stimulus identity coding, we show differences both in information content and group redundancies between IC and AI neurons. These results provide for the first time a direct evidence for redundancy reduction along the ascending auditory pathway, as has been hypothesized for theoretical considerations [Barlow 1959,2001]. The redundancy effects under the single-spikes coding scheme are significant only for groups larger than ten cells, and cannot be revealed with the redundancy measures that use only pairs of cells. The results suggest that the auditory system transforms low level representations that contain redundancies due to the statistical structure of natural stimuli, into a representation in which cortical neurons extract rare and independent component of complex acoustic signals, that are useful for auditory scene analysis.	Hebrew Univ Jerusalem, Sch Comp Sci & Engn, IL-91905 Jerusalem, Israel	Hebrew University of Jerusalem	Chechik, G (corresponding author), Hebrew Univ Jerusalem, Sch Comp Sci & Engn, IL-91905 Jerusalem, Israel.	ggal@cs.huji.ac.il	Nelken, Israel/B-7753-2011	Nelken, Israel/0000-0002-6645-107X; Young, Eric/0000-0003-0334-408X				[Anonymous], 1998, NEURAL NETWORKS BRAI; Barlow H, 2001, NETWORK-COMP NEURAL, V12, P241, DOI 10.1088/0954-898X/12/3/301; Barlow H. B., 1959, MECH THOUGHT PROCESS, P535; BARYOSEF O, 2001, IN PRESS J NEUROSCIE; Brenner N, 2000, NEURAL COMPUT, V12, P1531, DOI 10.1162/089976600300015259; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; Dan Y, 1998, NAT NEUROSCI, V1, P501, DOI 10.1038/2217; GAT I, 1999, ADV NEURAL INFORMATI, V11; GAWNE TJ, 1993, J NEUROSCI, V13, P2758; GOCHIN PM, 1994, J NEUROPHYSIOL, V71, P2325, DOI 10.1152/jn.1994.71.6.2325; Meister M, 1996, P NATL ACAD SCI USA, V93, P609, DOI 10.1073/pnas.93.2.609; Nadal JP, 1998, NETWORK-COMP NEURAL, V9, P207, DOI 10.1088/0954-898X/9/2/004; NELKEN I, 1997, CENTRAL AUDITORY PRO; Nirenberg S, 2001, NATURE, V411, P698, DOI 10.1038/35079612; OPTICAN LM, 1991, BIOL CYBERN, V65, P305, DOI 10.1007/BF00216963; Samengo I, 2001, NETWORK-COMP NEURAL, V12, P21, DOI 10.1088/0954-898X/12/1/302; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; Studeny M, 1998, NATO ADV SCI I D-BEH, V89, P261; VANVREESWIJK C, 2001, COMPUTATIONAL NEUROS; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Warland DK, 1997, J NEUROPHYSIOL, V78, P2336, DOI 10.1152/jn.1997.78.5.2336	22	19	19	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						173	180						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100022
C	Watanabe, S		Leen, TK; Dietterich, TG; Tresp, V		Watanabe, S			Algebraic information geometry for learning machines with singularities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				MODEL	Algebraic geometry is essential to learning theory. In hierarchical learning machines such as layered neural networks and gaussian mixtures, the asymptotic normality does not hold, since Fisher information matrices are singular. In this paper, the rigorous asymptotic form of the stochastic complexity is clarified based on resolution of singularities and two different problems are studied. (1) If the prior is positive, then the stochastic complexity is far smaller than BIC, resulting in the smaller generalization error than regular statistical models, even when the true distribution is not contained in the parametric model. (2) If Jeffreys' prior, which is coordinate free and equal to zero at singularities, is employed then the stochastic complexity has the same form as BIC. It is useful for model selection, but not for generalization.	Tokyo Inst Technol, Precis & Intelligence Lab, Midori Ku, Yokohama, Kanagawa 2268503, Japan	Tokyo Institute of Technology	Watanabe, S (corresponding author), Tokyo Inst Technol, Precis & Intelligence Lab, Midori Ku, 4259 Nagatsuta, Yokohama, Kanagawa 2268503, Japan.		Watanabe, Sumio/C-3880-2015; Watanabe, Sumio/M-7370-2019	Watanabe, Sumio/0000-0001-8341-5639; 				Akaike H., 1980, TRABAJOSDE ESTADISTI, P143, DOI DOI 10.1007/BF02888350; [Anonymous], 1985, DIFFERENTIAL GEOMETR; ATIYAH MF, 1970, COMMUN PUR APPL MATH, V23, P145, DOI 10.1002/cpa.3160230202; DACUNHACASTELLE D, 1997, PROBABILITY STAT, V1, P285; HIRONAKA H, 1964, ANN MATH, V79, P109, DOI 10.2307/1970486; KASHIWARA M, 1976, INVENT MATH, V38, P33, DOI 10.1007/BF01390168; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Watanabe S, 2000, ELECTRON COMM JPN 3, V83, P95, DOI 10.1002/(SICI)1520-6440(200006)83:6<95::AID-ECJC11>3.0.CO;2-B; Watanabe S, 2000, ADV NEUR IN, V12, P356; WATANABE S, 2001, IN PRESS NEURAL COMP	10	19	19	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						329	335						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800047
C	Maass, W		Solla, SA; Leen, TK; Muller, KR		Maass, W			Neural computation with winner-take-all as the only nonlinear operation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Everybody "knows" that neural networks need more than a single layer of nonlinear units to compute interesting functions. We show that this is false if one employs winner-take-all as nonlinear unit: Any boolean function can be computed by a single k-winner-take-all unit applied to weighted sums of the input variables. Any continuous function can be approximated arbitrarily well by a single soft winner-take-all unit applied to weighted sums of the input variables. Only positive weights are needed in these (linear) weighted sums. This may be of interest from the point of view of neurophysiology, since only 15% of the synapses in the cortex are inhibitory. In addition it is widely believed that there are special microcircuits in the cortex that compute winner-take-all. Our results support the view that winner-take-all is a very useful basic computational unit in Neural VLSI: it is wellknown that winner-take-all of n input variables can be computed very efficiently with 2n transistors (and a total wire length and area that is linear in n) in analog VLSI [Lazzaro et al., 1989] we show that winner-take-all is not just useful for special purpose computations, but may serve as the only nonlinear unit for neural circuits with universal computational power we show that any multi-layer perceptron needs quadratically in n many gates to compute winner-take-all for n input variables, hence winner-take-all provides a substantially more powerful computational unit than a perceptron (at about the same cost of implementation in analog VLSI).	Graz Tech Univ, Inst Theoret Comp Sci, A-8010 Graz, Austria	Graz University of Technology	Maass, W (corresponding author), Graz Tech Univ, Inst Theoret Comp Sci, A-8010 Graz, Austria.	maass@igi.tu-graz.ac.at						BROWN TX, 1991, THESIS CALTECH; Horiuchi TK, 1997, ADV NEUR IN, V9, P706; INDIVERI G, 1999, UNPUB MODELING SELEC; Lazzaro J., 1988, ADV NEURAL INFORMATI, V1, P703; MAASS W, 2000, IN PRESS NEURAL COMP; MEADOR JL, 1994, SILICON IMPLEMENTATI, P79; Minsky M., 1969, PERCEPTRONS; Siu K, 1995, DISCRETE NEURAL COMP; URAHAMA K, 1995, IEEE T NEURAL NETWOR, V6, P776, DOI 10.1109/72.377986; [No title captured]	10	19	19	0	1	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						293	299						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700042
C	Sollich, P		Kearns, MS; Solla, SA; Cohn, DA		Sollich, P			Learning curves for Gaussian processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					I consider the problem of calculating learning curves (i.e., average generalization performance) of Gaussian processes used for regression. A simple expression for the generalization error in terms of the eigenvalue decomposition of the covariance function is derived, and used as the starting point for several approximation schemes. I identify where these become exact, and compare with existing bounds on learning curves; the new approximations, which can be used for any input space dimension, generally get substantially closer to the truth.	Univ Edinburgh, Dept Phys, Edinburgh EH9 3JZ, Midlothian, Scotland	University of Edinburgh	Sollich, P (corresponding author), Univ Edinburgh, Dept Phys, Edinburgh EH9 3JZ, Midlothian, Scotland.		Sollich, Peter/H-2174-2011; Sollich, Peter/ABC-2993-2020	Sollich, Peter/0000-0003-0169-7893; 				MACKAY DJC, 10 NIPS; MICHELLI CA, 1981, APPROXIMATION THEORY, P329; Opper M., 1997, THEORETICAL ASPECTS; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; WILLIAMS CJ, UNPUB; WILLIAMS CKI, IN PRESS LEARNING IN; Wong E., 1971, STOCHASTIC PROCESSES	8	19	19	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						344	350						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700049
C	Wahba, G; Lin, XW; Gao, FY; Xiang, D; Klein, R; Klein, B		Kearns, MS; Solla, SA; Cohn, DA		Wahba, G; Lin, XW; Gao, FY; Xiang, D; Klein, R; Klein, B			The bias-variance tradeoff and the randomized GACV	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				SMOOTHING SPLINE ANOVA; EXPONENTIAL-FAMILIES	We propose a new in-sample cross validation based method (randomized GACV) for choosing smoothing or bandwidth parameters that govern the bias-variance or fit-complexity tradeoff in 'soft' classification. Soft classification refers to a learning procedure which estimates the probability that an example with a given attribute vector is in class 1 vs class 0. The target for optimizing the the tradeoff is the Kullback-Liebler distance between the estimated probability distribution and the 'true' probability distribution, representing knowledge of an infinite population. The method uses a randomized estimate of the trace of a Hessian and mimics cross validation at the cast of ii single relearning with perturbed outcome data.	Univ Wisconsin, Dept Stat, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison	Wahba, G (corresponding author), Univ Wisconsin, Dept Stat, 1210 W Dayton St, Madison, WI 53706 USA.	wahba@stat.wisc.edu; xiwu@stat.wisc.edu; fgao@stat.wisc.edu; sasdxx@unx.sas.com; kleinr@epi.ophth.wisc.edu; kleinb@epi.ophth.wisc.edu						Girard DA, 1998, ANN STAT, V26, P315; Gong JJ, 1998, MON WEATHER REV, V126, P210, DOI 10.1175/1520-0493(1998)126<0210:ATONWP>2.0.CO;2; GU C, 1992, STAT SINICA, V2, P255; KLEIN R, 1995, ARCH OPHTHALMOL, V113; LIU Y, 1993, UNBIASED ESTIMATE GE; UTANS J, 1993, P 1 INT C ART INT AP; Wahba G, 1995, ANN STAT, V23, P1865; Wahba G., 1990, CBMS NSF REGIONAL C, V59; Wahba G., 1995, HDB BRAIN THEORY NEU, P426; WANG KS, 1994, J APPL PHYCOL, V6, P415, DOI 10.1007/BF02182158; Wang YD, 1997, COMMUN STAT-SIMUL C, V26, P765, DOI 10.1080/03610919708813408; WONG W, 1992, 356 U CHIC DEP STAT; XIANG D, 1996, STAT SINICA, V6, P675; XIANG D, 1998, IN PRESS P 1997 ASA, P94	15	19	20	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						620	626						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700088
C	Blake, A; Isard, M		Mozer, MC; Jordan, MI; Petsche, T		Blake, A; Isard, M			The CONDENSATION algorithm - Conditional density propagation and applications to visual tracking	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The power of sampling methods in Bayesian reconstruction of noisy signals is well known. The extension of sampling to temporal problems is discussed. Efficacy of sampling over time is demonstrated with visual tracking.			Blake, A (corresponding author), UNIV OXFORD,DEPT ENGN SCI,PARKS RD,OXFORD OX1 3PJ,ENGLAND.								0	19	20	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						361	367						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00051
C	Ferree, TC; Marcotte, BA; Lockery, SR		Mozer, MC; Jordan, MI; Petsche, T		Ferree, TC; Marcotte, BA; Lockery, SR			Neural network models of chemotaxis in the nematode Caenorhabditis elegans	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We train recurrent networks to control chemotaxis in a computer model of the nematode C. elegans. The model presented is based closely on the body mechanics, behavioral analyses, neuroanatomy and neurophysiology of C. elegans, each imposing constraints relevant for information processing. Simulated worms moving autonomously in simulated chemical environments display a variety of chemotaxis strategies similar to those of biological warms.			Ferree, TC (corresponding author), UNIV OREGON,INST NEUROSCI,EUGENE,OR 97403, USA.								0	19	19	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						55	61						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00008
C	Horiuchi, TK; Morris, TG; Koch, C; DeWeerth, SP		Mozer, MC; Jordan, MI; Petsche, T		Horiuchi, TK; Morris, TG; Koch, C; DeWeerth, SP			Analog VLSI circuits for attention-based, visual tracking	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				A one-dimensional visual tracking chip has been implemented using neuromorphic, analog VLSI techniques to model selective visual attention in the control of saccadic and smooth pursuit eye movements. The chip incorporates focal-plane processing to compute image saliency and a winner-take-all circuit to select a feature for tracking. The target position and direction of motion are reported as the target moves across the array. We demonstrate its functionality in a closed-loop system which performs saccadic and smooth pursuit tracking movements using a one-dimensional mechanical eye.			Horiuchi, TK (corresponding author), CALTECH,PASADENA,CA 91125, USA.			Koch, Christof/0000-0001-6482-8067					0	19	20	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						706	712						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00100
C	Hyvarinen, A; Oja, E		Mozer, MC; Jordan, MI; Petsche, T		Hyvarinen, A; Oja, E			One-unit learning rules for independent component analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Neural one-unit learning rules for the problem of Independent Component Analysis (ICA) and blind source separation are introduced. In these new algorithms, every ICA neuron develops into a separator that finds one of the independent components. The learning rules use very simple constrained Hebbian/anti-Hebbian learning in which decorrelating feedback may be added. To speed up the convergence of these stochastic gradient descent rules, a novel computationally efficient fixed-point algorithm is introduced.			Hyvarinen, A (corresponding author), HELSINKI UNIV TECHNOL,LAB COMP & INFORMAT SCI,RAKENTAJANAUKIO 2 C,FIN-02150 ESPOO,FINLAND.								0	19	19	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						480	486						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00068
C	Stahlberger, A; Riedmiller, M		Mozer, MC; Jordan, MI; Petsche, T		Stahlberger, A; Riedmiller, M			Fast network pruning and feature extraction using the Unit-OBS algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The algorithm described in this article is based on the OBS algorithm by Hassibi, Stork and Wolff ([1] and [2]). The main disadvantage of OBS is its high complexity. OBS needs to calculate the inverse Hessian to delete only one weight (thus needing much time to prune a big net). A better algorithm should use this matrix to remove more than only one weight, because calculating the inverse Hessian takes the most time in the OBS algorithm. The algorithm, called Unit-OBS, described in this article is a method to overcome this disadvantage. This algorithm only needs to calculate the inverse Hessian once to remove one whole unit thus drastically reducing the time to prune big nets. A further advantage of Unit-OBS is that it can be used to do a feature extraction on the input data. This can be helpful on the understanding of unknown problems.			Stahlberger, A (corresponding author), UNIV KARLSRUHE,INST LOGIK KOMPLEXITAT & DEDUKT SYST,KAISERSTR 12,D-76128 KARLSRUHE,GERMANY.								0	19	21	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						655	661						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00093
C	Kempter, R; Gerstner, W; vanHemmen, JL; Wagner, H		Touretzky, DS; Mozer, MC; Hasselmo, ME		Kempter, R; Gerstner, W; vanHemmen, JL; Wagner, H			Temporal coding in the sub-millisecond range: Model of barn owl auditory pathway	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						TECH UNIV MUNICH,DEPT PHYS,INST THEORET PHYS,D-85748 GARCHING,GERMANY	Technical University of Munich			Wagner, Hermann/G-5454-2012	Wagner, Hermann/0000-0002-8191-7595					0	19	19	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						124	130						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00018
C	Rao, RPN; Zelinsky, GJ; Hayhoe, MM; Ballard, DH		Touretzky, DS; Mozer, MC; Hasselmo, ME		Rao, RPN; Zelinsky, GJ; Hayhoe, MM; Ballard, DH			Modeling saccadic targeting in visual search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV ROCHESTER,DEPT COMP SCI,ROCHESTER,NY 14627	University of Rochester									0	19	21	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						830	836						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00117
C	ZADOR, AM; CLAIBORNE, BJ; BROWN, TH		MOODY, JE; HANSON, SJ; LIPPMANN, RP		ZADOR, AM; CLAIBORNE, BJ; BROWN, TH			NONLINEAR PATTERN SEPARATION IN SINGLE HIPPOCAMPAL-NEURONS WITH ACTIVE DENDRITIC MEMBRANE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	19	20	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						51	58						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00007
C	Gitman, I; Lang, H; Zhang, PC; Xiao, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gitman, Igor; Lang, Hunter; Zhang, Pengchuan; Xiao, Lin			Understanding the Role of Momentum in Stochastic Gradient Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				OPTIMIZATION; ALGORITHMS	The use of momentum in stochastic gradient methods has become a widespread practice in machine learning. Different variants of momentum, including heavy-ball momentum, Nesterov's accelerated gradient (NAG), and quasi-hyperbolic momentum (QHM), have demonstrated success on various tasks. Despite these empirical successes, there is a lack of clear understanding of how the momentum parameters affect convergence and various performance measures of different algorithms. In this paper, we use the general formulation of QHM to give a unified analysis of several popular algorithms, covering their asymptotic convergence conditions, stability regions, and properties of their stationary distributions. In addition, by combining the results on convergence rates and stationary distributions, we obtain sometimes counter-intuitive practical guidelines for setting the learning rate and momentum parameters.	[Gitman, Igor; Lang, Hunter; Zhang, Pengchuan; Xiao, Lin] Microsoft Res AI, Redmond, WA 98052 USA		Gitman, I (corresponding author), Microsoft Res AI, Redmond, WA 98052 USA.	igor.gitman@microsoft.com; hunter.lang@microsoft.com; penzhan@microsoft.com; lin.xiao@microsoft.com	Zhang, Pengchuan/AAR-3769-2021					An WP, 2018, PROC CVPR IEEE, P8522, DOI 10.1109/CVPR.2018.00889; Bengio Y, 2013, INT CONF ACOUST SPEE, P8624, DOI 10.1109/ICASSP.2013.6639349; Cyrus S, 2018, P AMER CONTR CONF, P1376; Dieuleveut Aymeric, 2017, ARXIV170706386; Ermoliev Y. M., 1969, KIBERNETIKA, V2, P72; Freidlin M I, 1998, GRUND MATH WISS, V260, DOI 10.1007/978-1-4612-0611-82; Gadat S, 2018, ELECTRON J STAT, V12, P461, DOI 10.1214/18-EJS1395; Georg Ch. Pflug, 1983, CP83025 IIASA; Goh G., 2017, DISTILL, V2, pe6, DOI DOI 10.23915/DISTILL.00006; Gupal A. M., 1972, Cybernetics, V8, P138, DOI 10.1007/BF01069146; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; KANIOVSKII YM, 1983, USSR COMP MATH MATH+, V23, P8, DOI 10.1016/S0041-5553(83)80003-2; Kidambi R, 2018, 2018 INFORMATION THEORY AND APPLICATIONS WORKSHOP (ITA); Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kushner HJ., 2003, STOCHASTIC APPROXIMA; Lang Hunter, 2019, STAT ADAPTIVE STOCHA; Lessard L, 2016, SIAM J OPTIMIZ, V26, P57, DOI 10.1137/15M1009597; Ma Jerry, 2019, INT C LEARN REPR; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Nesterov Y, 1983, SOVIET MATH DOKL, V27; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Polyak, 1987, INTRO OPTIMIZATION O, V1; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; POLYAK BT, 1977, ENG CYBERN, V15, P6; Recht B., 2010, CS726 LYAPUNOV ANAL; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; RUSZCZYNSKI A, 1983, IEEE T AUTOMAT CONTR, V28, P1097, DOI 10.1109/TAC.1983.1103184; Ruszczynski Andrzej, 1984, P 9 IFAC WORLD C BUD, P1023; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Tange O, 2011, THE USENIX MAGAZINE, P42, DOI DOI 10.5281/ZEN0D0.16303; Van Scoy B, 2018, IEEE CONTR SYST LETT, V2, P49, DOI 10.1109/LCSYS.2017.2722406; WASAN M., 1969, STOCHASTIC APPROXIMA; Williams D., 1991, PROBABILITY MARTINGA; Yaida S., 2018, ARXIV181000004; Yang T., 2016, ARXIV160403257; [No title captured]	40	18	18	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901028
C	Gupta, H; Srikant, R; Ying, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gupta, Harsh; Srikant, R.; Ying, Lei			Finite-Time Performance Bounds and Adaptive Learning Rate Selection for Two Time-Scale Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study two time-scale linear stochastic approximation algorithms, which can be used to model well-known reinforcement learning algorithms such as GTD, GTD2, and TDC. We present finite-time performance bounds for the case where the learning rate is fixed. The key idea in obtaining these bounds is to use a Lyapunov function motivated by singular perturbation theory for linear differential equations. We use the bound to design an adaptive learning rate scheme which significantly improves the convergence rate over the known optimal polynomial decay rule in our experiments, and can be used to potentially improve the performance of any other schedule where the learning rate is changed at pre-determined time instants.	[Gupta, Harsh; Srikant, R.] Univ Illinois, ECE & CSL, Champaign, IL 61820 USA; [Ying, Lei] Univ Michigan, EECS, Ann Arbor, MI 48109 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Michigan System; University of Michigan	Gupta, H (corresponding author), Univ Illinois, ECE & CSL, Champaign, IL 61820 USA.	hgupta10@illinois.edu; rsrikant@illinois.edu; leiying@umich.edu		Ying, Lei/0000-0001-7955-9445	ONR [N00014-19-1-2566]; NSF [CPS ECCS 1739189, NeTS 1718203, CMMI 1562276, ECCS 16-09370, CNS 1618768, ECCS 1609202, IIS 1715385, ECCS 1739344, CNS 1824393, CNS 1813392]; NSF/USDA Grant [AG 2018-67007-28379]	ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); NSF/USDA Grant	Research supported by ONR Grant N00014-19-1-2566, NSF Grants CPS ECCS 1739189, NeTS 1718203, CMMI 1562276, ECCS 16-09370, and NSF/USDA Grant AG 2018-67007-28379. Lei Ying's work supported by NSF grants CNS 1618768, ECCS 1609202, IIS 1715385, ECCS 1739344, CNS 1824393 and CNS 1813392.	[Anonymous], 2018, REINFORCEMENT LEARNI, DOI 10.1016/S1364-6613(99)01331-5; Benveniste Albert, 2012, ADAPTIVE ALGORITHMS, V22; Bertsekas D., 2011, DYNAMIC PROGRAMMING, VII; Bhandari Jalaj, 2018, C LEARN THEOR, P1691; Bhatnagar S., 2012, STOCHASTIC RECURSIVE, V434; Bhatnagar Shalabh, 2009, ADV NEURAL INFORM PR, V22, P1204; Borkar V. S., 2009, STOCHASTIC APPROXIMA, V48; Dalal G., 2017, ARXIV170305376; Dalal Gal, 2017, ARXIV170401161; Gupta Harsh, 2019, REAL WORLD SEQ DEC M; Khalil H. K., 2002, NONLINEAR SYSTEMS, V3; Kokotovic Petar, 1999, SINGULAR PERTURBATIO, V25; Konda VR, 2004, ANN APPL PROBAB, V14, P796, DOI 10.1214/105051604000000116; Konidaris G., 2011, 25 AAAI C ART INT; Kushner Harold, 2003, STOCHASTIC APPROXIMA, V35; Lakshminarayanan C., 2018, PROC INT C ARTIF INT, P1347; Liu B, 2018, J ARTIF INTELL RES, V63, P461, DOI 10.1613/jair.1.11251; Srikant R., 2019, C LEARN THEOR COLT; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton R. S., 2009, P INT C MACH LEARN I, P125, DOI DOI 10.1145/1553374.1553501; Sutton R. S., 2008, ADV NEURAL INFORM PR, V21, P1609; Sutton R. S., 2016, J MACHINE LEARNING R, V17, P2603; Szepesv?ri, 2010, SYNTHESIS LECT ARTIF, V4, DOI [10.2200/S00268ED1V01Y201005AIM009, DOI 10.2200/S00268ED1V01Y201005AIM009]; Tsitsiklis John N, 1997, IEEE T AUTOMATIC CON, V42	26	18	18	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304068
C	Huang, LX; Jiang, SHC; Vishnoi, NK		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Huang, Lingxiao; Jiang, Shaofeng H. -C.; Vishnoi, Nisheeth K.			Coresets for Clustering with Fairness Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In a recent work, [20] studied the following "fair" variants of classical clustering problems such as k-means and k-median: given a set of n data points in R-d and a binary type associated to each data point, the goal is to cluster the points while ensuring that the proportion of each type in each cluster is roughly the same as its underlying proportion. Subsequent work has focused on either extending this setting to when each data point has multiple, non-disjoint sensitive types such as race and gender [7], or to address the problem that the clustering algorithms in the above work do not scale well [42, 8, 6]. The main contribution of this paper is an approach to clustering with fairness constraints that involve multiple, non-disjoint types, that is also scalable. Our approach is based on novel constructions of coresets: for the k-median objective, we construct an epsilon-coreset of size O(Gamma k(2)epsilon(-d)) where is the number of distinct collections of groups that a point may belong to, and for the k-means objective, we show how to construct an epsilon-coreset of size O(Gamma k(3)epsilon(-d-1)). The former result is the first known coreset construction for the fair clustering problem with the k-median objective, and the latter result removes the dependence on the size of the full dataset as in [42] and generalizes it to multiple, non-disjoint types. Plugging our coresets into existing algorithms for fair clustering such as [6] results in the fastest algorithms for several cases. Empirically, we assess our approach over the Adult, Bank, Diabetes and Athlete dataset, and show that the coreset sizes are much smaller than the full dataset; applying coresets indeed accelerates the running time of computing the fair clustering objective while ensuring that the resulting objective difference is small. We also achieve a speed-up to recent fair clustering algorithms [6, 7] by incorporating our coreset construction.	[Huang, Lingxiao; Vishnoi, Nisheeth K.] Yale Univ, New Haven, CT 06520 USA; [Jiang, Shaofeng H. -C.] Weizmann Inst Sci, Rehovot, Israel	Yale University; Weizmann Institute of Science	Huang, LX (corresponding author), Yale Univ, New Haven, CT 06520 USA.			Jiang, Shaofeng/0000-0001-7972-827X	NSF [CCF-1908347]; SNSF [200021_182527]; ONR Award [N00014-18-1-2364]; Minerva Foundation	NSF(National Science Foundation (NSF)); SNSF(Swiss National Science Foundation (SNSF)); ONR Award; Minerva Foundation	This research was supported in part by NSF CCF-1908347, SNSF 200021_182527, ONR Award N00014-18-1-2364 and a Minerva Foundation grant.	Agarwal PK, 2004, J ACM, V51, P606, DOI 10.1145/1008731.1008736; Agarwal PK, 2002, ALGORITHMICA, V33, P201, DOI 10.1007/s00453-001-0110-y; Ahmadian Sara, 2019, 36 INT C MACH LEARN; Anagnostopoulos Aris, 2019, 36 INT C MACH LEARN; Backurs A, 2019, PR MACH LEARN RES, V97; Bera SK, 2019, ABS190102393 CORR; Bercea Ioana O, 2018, ARXIV181110319; Braverman Vladimir, 2016, ABS161200889 CORR; Burke R, 2011, AI MAG, V32, P13, DOI 10.1609/aimag.v32i3.2361; Celis L. E., 2019, FAIRNESS ACCOUNTABIL; Celis LE, 2019, PR MACH LEARN RES, V97; Celis LE, 2018, PR MACH LEARN RES, V80; Celis LE, 2019, FAT*'19: PROCEEDINGS OF THE 2019 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P319, DOI 10.1145/3287560.3287586; Celis LE, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P144; Celis L Elisa, 2018, ICALP, V28; Chen K, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1177, DOI 10.1145/1109557.1109687; Chen Xingyu, 2019, 36 INT C MACH LEARN; Chierichetti F, 2017, ADV NEUR IN, V30; Das J, 2014, 2014 INTERNATIONAL CONFERENCE ON CONTEMPORARY COMPUTING AND INFORMATICS (IC3I), P230, DOI 10.1109/IC3I.2014.7019655; Datta Amit, 2015, Proceedings on Privacy Enhancing Technologies, V1, P92, DOI 10.1515/popets-2015-0007; Feldman D, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1434; Feldman D, 2011, ACM S THEORY COMPUT, P569; Fichtenberger Hendrik, 2013, ESA; Glassman E.L., 2014, P 1 ACM C LEARNING S, P171; Har-Peled S, 2004, DISCRETE COMPUT GEOM, V31, P545, DOI 10.1007/s00454-004-2822-7; Har-Peled S, 2007, DISCRETE COMPUT GEOM, V37, P3, DOI 10.1007/s00454-006-1271-x; Huang LX, 2018, ANN IEEE SYMP FOUND, P814, DOI 10.1109/FOCS.2018.00082; Huang Lingxiao, 2019, ABS190608484 CORR; IBM, 2015, IBM CPLEX OPT 2019 C; Jiang Shengyi, 2008, J ELECT, V36, P157; Kleindessner Matthaus, 2019, 36 INT C MACH LEARN; Langberg M, 2010, PROC APPL MATH, V135, P598; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Miller Claire Cain, 2015, NY TIMES; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Pham MC, 2011, J UNIVERS COMPUT SCI, V17, P583; Rosner Clemens, 2018, 45 INT C AUT LANG PR; Schmidt M., 2018, ABS181210854 CORR; Tan P-N, 2013, INTRO DATA MINING, P487; Vishnoi Nisheeth K, 2017, FAIRNESS ACCOUNTABIL; Yang K, 2017, SSDBM 2017: 29TH INTERNATIONAL CONFERENCE ON SCIENTIFIC AND STATISTICAL DATABASE MANAGEMENT, DOI 10.1145/3085504.3085526	41	18	18	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307059
C	Jiang, YD; Gu, SX; Murphy, K; Finn, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jiang, Yiding; Gu, Shixiang; Murphy, Kevin; Finn, Chelsea			Language as an Abstraction for Hierarchical Deep Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Solving complex, temporally-extended tasks is a long-standing problem in reinforcement learning (RL). We hypothesize that one critical element of solving such problems is the notion of compositionality. With the ability to learn concepts and sub-skills that can be composed to solve longer tasks, i.e. hierarchical RL, we can acquire temporally-extended behaviors. However, acquiring effective yet general abstractions for hierarchical RL is remarkably challenging. In this paper, we propose to use language as the abstraction, as it provides unique compositional structure, enabling fast learning and combinatorial generalization, while retaining tremendous flexibility, making it suitable for a variety of problems. Our approach learns an instruction-following low-level policy and a high-level policy that can reuse abstractions across tasks, in essence, permitting agents to reason using structured language. To study compositional task learning, we introduce an open-source object interaction environment built using the MuJoCo physics engine and the CLEVR engine. We find that, using our approach, agents can learn to solve to diverse, temporally-extended tasks such as object sorting and multi-object rearrangement, including from raw pixel observations. Our analysis reveals that the compositional nature of language is critical for learning diverse sub-skills and systematically generalizing to new sub-skills in comparison to non-compositional abstractions that use the same supervision.(2)	[Jiang, Yiding; Gu, Shixiang; Murphy, Kevin; Finn, Chelsea] Google Res, Mountain View, CA 94043 USA; [Jiang, Yiding] Goolge AI, Residency Program, Mountain View, CA 94024 USA	Google Incorporated	Jiang, YD (corresponding author), Google Res, Mountain View, CA 94043 USA.; Jiang, YD (corresponding author), Goolge AI, Residency Program, Mountain View, CA 94024 USA.	ydjiang@google.com; shanegu@google.com; kpmurphy@google.com; chelseaf@google.com						Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Andreas Jacob, 2017, LEARNING LATENT LANG; Andreas Jacob, 2016, MODULAR MULTITASK RE; Andrychowicz M., 2017, ADV NEURAL INFORM PR; Bahdanau Dzmitry, 2018, LEARNING UNDERSTAND; Battaglia Peter W, 2018, ARXIV180601261; Callison-Burch Chris, 2006, P EACL; Chentanez, 2005, ADV NEURAL INFORM PR, P1281, DOI DOI 10.21236/ADA440280; Chevalier-Boisvert M., 2019, ICLR; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Co-Reyes John D, 2019, INT C LEARN REPR; Daniel C., 2012, ARTIF INTELL, P273; Das A., 2018, P SER P MACHINE LEAR; Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639; Florensa C., 2017, PROC INT C MACH LEAR, P1; Frans K., 2018, ICLR; Fried D, 2018, ADV NEUR IN, V31; Fu J., 2019, ARXIV190207742; Gleitman L., 2005, CAMBRIDGE HDB THINKI, P633; Grice H.P., 1975, SYNTAX SEMANTICS VOL, P41, DOI DOI 10.1163/9789004368811_003; Haarnoja T, 2018, PR MACH LEARN RES, V80; Harb Jean, 2017, ARXIV170904571; He J., 2015, ARXIV PREPRINT ARXIV; Heess N., 2016, LEARNING TRANSFER MO; Hinton, 1993, ADV NEURAL INFORM PR, V5, P271; Jang E., 2016, ARXIV; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Kaelbling LP., 1993, P 10 INT C MACHINE L, P951; Kaplan R., 2017, BEATING ATARI NATURA; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Klein U., 2004, P 21 INT C MACH LEAR, P71, DOI DOI 10.1145/1015330.1015355; Konidaris G, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P895; Kulkarni T. D., 2016, PROC 30 INT C NEURAL, P3675; Levine S, 2016, J MACH LEARN RES, V17; Levy A., 2019, P INT C LEARN REPR; Luketina J, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P6309; Metz L., 2017, ARXIV170505035; Miyato Takeru, 2018, ARXIV180205637; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nachum O., 2019, P INT C LEARN REPR I; Nachum O., 2018, DATA EFFICIENT HIERA; Narasimhan K., 2015, ARXIV150608941; Parr R, 1998, ADV NEUR IN, V10, P1043; Peng XB, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073602; Perez E, 2018, AAAI CONF ARTIF INTE, P3942; Piantadosi ST, 2012, COGNITION, V123, P199, DOI 10.1016/j.cognition.2011.11.005; Pong V., 2018, PROC 2 WORKSHOP LIFE; Pong V.H., 2019, ARXIV190303698; Pong Vitchyr, 2018, TEMPORAL DIFFERENCE; Precup D., 2000, TEMPORAL ABSTRACTION; Reiter E, 2018, COMPUT LINGUIST, V44, P393, DOI [10.1162/COLI.a.00322, 10.1162/coli_a_00322]; Sahni Himanshu, 2019, CORR ABS 1901 11529; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Schulman John, 2015, ARXIV150602438; Shu T., 2017, ARXIV171207294; Sigaud O., 2018, ARXIV180304706; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Sulem Elior, 2018, P 2018 C EMP METH NA, P738, DOI DOI 10.18653/V1/D18-1081; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Tavakoli A, 2018, AAAI CONF ARTIF INTE, P4131; Tessler C, 2017, AAAI CONF ARTIF INTE, P1553; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; van den Oord A, 2017, ADV NEUR IN, V30; Van Hasselt H., 2015, ARXIV150906461; Vezhnevets AS, 2017, PR MACH LEARN RES, V70; Wu Yuhuai, 2019, ACTRCE AUGMENTING EX	70	18	18	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901009
C	Liu, YW; Liu, JM; Zhang, ZJ; Zhu, LH; Li, AS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Yiwei; Liu, Jiamou; Zhang, Zijian; Zhu, Liehuang; Li, Angsheng			REM: From Structural Entropy To Community Structure Deception	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COMPLEX NETWORKS; GRAPH	This paper focuses on the privacy risks of disclosing the community structure in an online social network. By exploiting the community affiliations of user accounts, an attacker may infer sensitive user attributes. This raises the problem of community structure deception (CSD), which asks for ways to minimally modify the network so that a given community structure maximally hides itself from community detection algorithms. We investigate CSD through an information-theoretic lens. To this end, we propose a community-based structural entropy to express the amount of information revealed by a community structure. This notion allows us to devise residual entropy minimization (REM) as an efficient procedure to solve CSD. Experimental results over 9 real-world networks and 6 community detection algorithms show that REM is very effective in obfuscating the community structure as compared to other benchmark methods.	[Liu, Yiwei; Zhang, Zijian; Zhu, Liehuang] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China; [Liu, Yiwei] Zhejiang Univ, Inst Cyberspace Res, Hangzhou 310027, Zhejiang, Peoples R China; [Liu, Jiamou; Zhang, Zijian] Univ Auckland, Sch Comp Sci, Auckland 1142, New Zealand; [Li, Angsheng] Beihang Univ, Sch Comp Sci, Beijing 100083, Peoples R China	Beijing Institute of Technology; Zhejiang University; University of Auckland; Beihang University	Liu, YW (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.; Liu, YW (corresponding author), Zhejiang Univ, Inst Cyberspace Res, Hangzhou 310027, Zhejiang, Peoples R China.	yiweiliu@bit.edu.cn; jiamou.liu@auckland.ac.nz; zhangzijian@bit.edu.cn; liehuangz@bit.edu.cn; angsheng@buaa.edu.cn			Provincial Key Research and Development Program of Zhejiang [2019C03133]; Major Scientific Research Project of Zhejiang Lab [2018FDOZX01]; National Natural Science Foundation of China [61932002]	Provincial Key Research and Development Program of Zhejiang; Major Scientific Research Project of Zhejiang Lab; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is supported by Provincial Key Research and Development Program of Zhejiang (Grant No. 2019C03133) and Major Scientific Research Project of Zhejiang Lab (Grant No. 2018FDOZX01). The co-authors Angsheng Li and Jiamou Liu are supported by the National Natural Science Foundation of China (No. 61932002). We also thank our anonymous reviewers for their constructive comments.	Anand K, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.045102; Backstrom L., 2007, PROC 16 INT C WORLD, P181, DOI DOI 10.1145/1242572.1242598; Blondel VD, 2008, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2008/10/P10008; Braunstein SL, 2006, ANN COMB, V10, P291, DOI 10.1007/s00026-006-0289-3; Campan A, 2009, LECT NOTES COMPUT SC, V5456, P33, DOI 10.1007/978-3-642-01718-6_4; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Das S, 2010, PROC INT CONF DATA, P904, DOI 10.1109/ICDE.2010.5447915; Dehmer M, 2008, APPL MATH COMPUT, V201, P82, DOI 10.1016/j.amc.2007.12.010; Denceud L, 2006, STUD CLASS DATA ANAL, P21; Fionda V, 2018, IEEE T KNOWL DATA EN, V30, P660, DOI 10.1109/TKDE.2017.2776133; Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002; Guimera R, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.025101; Hay M, 2008, PROC VLDB ENDOW, V1, P102, DOI 10.14778/1453856.1453873; KVALSETH TO, 1987, IEEE T SYST MAN CYB, V17, P517, DOI 10.1109/TSMC.1987.4309069; Li AS, 2016, SCI REP-UK, V6, DOI 10.1038/srep26810; Li AS, 2016, IEEE T INFORM THEORY, V62, P3290, DOI 10.1109/TIT.2016.2555904; Li AS, 2015, PHYSICA A, V436, P878, DOI 10.1016/j.physa.2015.05.039; Liu K., 2008, P 2008 ACM SIGMOD IN, P93, DOI DOI 10.1145/1376616.1376629; Liu L., 2009, P SIAM INT C DAT MIN; Nagaraja S, 2010, LECT NOTES COMPUT SC, V6205, P253, DOI 10.1007/978-3-642-14527-8_15; Narayanan A, 2009, P IEEE S SECUR PRIV, P173, DOI 10.1109/SP.2009.22; Newman MEJ, 2004, PHYS REV E, V70, DOI [10.1103/PhysRevE.70.056131, 10.1103/PhysRevE.69.026113]; Pei YL, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2083; RASHEVSKY N., 1955, BULL MATH BIOPHYS, V17, P229, DOI 10.1007/BF02477860; Remy C, 2018, STUD COMPUT INTELL, V689, P166, DOI 10.1007/978-3-319-72150-7_14; Rosvall M, 2008, P NATL ACAD SCI USA, V105, P1118, DOI 10.1073/pnas.0706851105; Vinh NX, 2010, J MACH LEARN RES, V11, P2837; Waniek M, 2018, NAT HUM BEHAV, V2, P139, DOI 10.1038/s41562-017-0290-3; Wondracek G, 2010, P IEEE S SECUR PRIV, P223, DOI 10.1109/SP.2010.21; Zheleva E, 2008, LECT NOTES COMPUT SC, V4890, P153; Zheleva Elena, 2012, SYNTHESIS LECT DATA, V3, P1; Zhou B, 2011, KNOWL INF SYST, V28, P47, DOI 10.1007/s10115-010-0311-2; Zou L, 2009, PROC VLDB ENDOW, V2, P946, DOI [10.14778/1687627.1687734, DOI 10.14778/1687627.1687734]	33	18	19	3	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904056
C	Lowe, S; O'Connor, P; Veeling, BS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lowe, Sindy; O'Connor, Peter; Veeling, Bastiaan S.			Putting An End to End-to-End: Gradient-Isolated Learning of Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a novel deep learning method for local self-supervised representation learning that does not require labels nor end-to-end backpropagation but exploits the natural order in data instead. Inspired by the observation that biological neural networks appear to learn without backpropagating a global error signal, we split a deep neural network into a stack of gradient-isolated modules. Each module is trained to maximally preserve the information of its inputs using the InfoNCE bound from Oord et al. [2018]. Despite this greedy training, we demonstrate that each module improves upon the output of its predecessor, and that the representations created by the top module yield highly competitive results on downstream classification tasks in the audio and visual domain. The proposal enables optimizing modules asynchronously, allowing large-scale distributed training of very deep neural networks on unlabelled datasets.	[Lowe, Sindy; O'Connor, Peter; Veeling, Bastiaan S.] Univ Amsterdam, AMLab, Amsterdam, Netherlands	University of Amsterdam	Lowe, S (corresponding author), Univ Amsterdam, AMLab, Amsterdam, Netherlands.	loewe.sindy@gmail.com; basveeling@gmail.com			Philips Research; NVIDIA GPU Grant	Philips Research; NVIDIA GPU Grant	We thank Jorn Peters, Marco Federici, Rudy Corona, Pascal Esser, Joop Pascha and the anonymous reviewers for their insightful comments. This research was supported by Philips Research and the NVIDIA GPU Grant.	[Anonymous], 2018, ICML P MACH LEARN RE; Balduzzi D., 2015, 29 AAAI C ART INT; Belilovsky E, 2019, PR MACH LEARN RES, V97; Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024; Bengio Y., 2015, STDP BIOLOGICALLY PL; Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; CRICK F, 1989, NATURE, V337, P129, DOI 10.1038/337129a0; Devon Hjelm R, 2019, P 7 INT C LEARN REPR; DeVries T., 2017, P 2017 COMPUTER VISI; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Elad A, 2018, EFFECTIVENESS LAYER; Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787; Ganchev T., 2005, PROC SPECOM, V1, P191; Gao Shuyang, 2018, ARXIV180205822; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Henaff Olivier J., 2019, DATA EFFICIENT IMAGE, P2; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Hu WH, 2017, PR MACH LEARN RES, V70; Hyvarinen A, 2017, PR MACH LEARN RES, V54, P460; Hyvarinen Aapo, 2018, NONLINEAR ICA USING; Ioffe S, 2015, ARXIV 1502 03167, V32, P448; Jacobsen J.H., 2018, ICLR; Jaderberg M, 2017, PR MACH LEARN RES, V70; Kingma D.P, P 3 INT C LEARNING R; Kipf Thomas N, 2016, NIPS WORKSHOP BAYESI; Kohan Adam A, 2018, ARXIV180803357; Krause Andreas, 2010, ADV NEURAL INFORM PR, V23, P5; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109; Lee DH, 2015, LECT NOTES ARTIF INT, V9284, P498, DOI 10.1007/978-3-319-23528-8_31; Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276; LINSKER R, 1988, COMPUTER, V21, P105, DOI 10.1109/2.36; Marblestone AH, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00094; McAllester David, 2018, ARXIV180207572; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Nickel M., 2011, INT C INT C MACH LEA, P809, DOI DOI 10.5555/3104482.3104584; Nickel M, 2016, P IEEE, V104, P11, DOI 10.1109/JPROC.2015.2483592; Nokland Arild, 2019, P 36 INT C MACH LEAR; Ororbia A. G., 2018, ARXIV180301834; Palmer SE, 2015, P NATL ACAD SCI USA, V112, P6908, DOI 10.1073/pnas.1506855112; Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964; Panayotov Vassil, 2014, KALDI PRETRAINED MOD; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Perozzi Bryan, 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732; Poole B., 2018, NEURIPS WORKSH BAYES; Povey D., IEEE 2011 WORKSH AUT; Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580; Salimans T., 2017, GRADIENT CHECKPOINTI; Scellier B, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00024; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; van den Oord Aaron, 2019, REPRESENTATION LEARN, P4; Velickovi P., 2018, ARXIV180910341, P1; Ver Steeg G, 2015, JMLR WORKSH CONF PRO, V38, P1004; Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938; Xiao Will, 2019, P 7 INT C LEARN REPR	60	18	18	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303007
C	Mena, G; Niles-Weed, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mena, Gonzalo; Niles-Weed, Jonathan			Statistical bounds for entropic optimal transport: sample complexity and the central limit theorem	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				EARTH MOVERS DISTANCE	We prove several fundamental statistical bounds for entropic OT with the squared Euclidean cost between subgaussian probability measures in arbitrary dimension. First, through a new sample complexity result we establish the rate of convergence of entropic OT for empirical measures. Our analysis improves exponentially on the bound of Genevay et al. (2019) and extends their work to unbounded measures. Second, we establish a central limit theorem for entropic OT, based on techniques developed by Del Barrio and Loubes (2019). Previously, such a result was only known for finite metric spaces. As an application of our results, we develop and analyze a new technique for estimating the entropy of a random variable corrupted by gaussian noise.	[Mena, Gonzalo] Harvard, Cambridge, MA 02138 USA; [Niles-Weed, Jonathan] NYU, New York, NY 10003 USA	New York University	Mena, G (corresponding author), Harvard, Cambridge, MA 02138 USA.							Altschuler J., 2017, ADV NEURAL INFORM PR, P1961; Alvarez-Melis David, 2018, P 2018 C EMP METH NA, P1881, DOI DOI 10.18653/V1/D18-1214; Bercu B., 2018, ARXIV181209150; Bigot J., 2017, ARXIV171108947; Courty Nicolas, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P274, DOI 10.1007/978-3-662-44848-9_18; Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921; CUTURI M., 2013, P INT C ADV NEURAL I, V26; del Barrio E, 2019, ANN PROBAB, V47, P926, DOI 10.1214/18-AOP1275; DUDLEY RM, 1969, ANN MATH STAT, V40, P40, DOI 10.1214/aoms/1177697802; Ge nevay A., 2017, ARXIV170600292; Genevay A., 2016, P NEUR INF PROC SYST, P3440; Genevay A, 2019, PR MACH LEARN RES, V89; Gine E., 2016, CAMBRIDGE SERIES STA, V40; Goldfeld Z., 2019, ARXIV190513576; Goldfeld Z, 2018, ARXIV181005728; Grave Edouard, 2018, ARXIV180511222; Klatt M., 2018, ARXIV181009880; Kolouri S, 2017, IEEE SIGNAL PROC MAG, V34, P43, DOI 10.1109/MSP.2017.2695801; Leonard C, 2014, LECT NOTES MATH, V2123, P207, DOI 10.1007/978-3-319-11970-0_8; Li PH, 2013, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2013.212; Montavon G., 2016, ADV NEURAL INF PROCE, P3718; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Rigollet P., 2018, COMPTES RENDUS MATH, V356; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Sandler R, 2011, IEEE T PATTERN ANAL, V33, P1590, DOI 10.1109/TPAMI.2011.18; Schiebinger G, 2019, CELL, V176, DOI 10.1016/j.cell.2019.02.026; Sommerfeld M, 2018, J R STAT SOC B, V80, P219, DOI 10.1111/rssb.12236; Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW); Vershynin R., 2018, HIGH DIMENSIONAL PRO, V47; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Weed J., 2018, TECHNICAL REPORT; Weed J., 2018, BERNOULLI	34	18	18	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304053
C	Wang, K; Hua, H; Wan, XJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Ke; Hua, Hang; Wan, Xiaojun			Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Unsupervised text attribute transfer automatically transforms a text to alter a specific attribute (e.g. sentiment) without using any parallel data, while simultaneously preserving its attribute-independent content. The dominant approaches are trying to model the content-independent attribute separately, e.g., learning different attributes' representations or using multiple attribute-specific decoders. However, it may lead to inflexibility from the perspective of controlling the degree of transfer or transferring over multiple aspects at the same time. To address the above problems, we propose a more flexible unsupervised text attribute transfer framework which replaces the process of modeling attribute with minimal editing of latent representations based on an attribute classifier. Specifically, we first propose a Transformer-based autoencoder to learn an entangled latent representation for a discrete text, then we transform the attribute transfer task to an optimization problem and propose the Fast-Gradient-Iterative-Modification algorithm to edit the latent representation until conforming to the target attribute. Extensive experimental results demonstrate that our model achieves very competitive performance on three public data sets. Furthermore, we also show that our model can not only control the degree of transfer freely but also allow transferring over multiple aspects at the same time.	[Wang, Ke] Peking Univ, Wangxuan Inst Comp Technol, Beijing, Peoples R China; Peking Univ, MOE Key Lab Computat Linguist, Beijing, Peoples R China	Peking University; Peking University	Wang, K (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing, Peoples R China.	wangke17@pku.edu.cn; huahang@pku.edu.cn; wanxiaojun@pku.edu.cn		Wang, Ke/0000-0003-2300-0743	National Natural Science Foundation of China [61772036]; Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology)	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology)	This work was supported by National Natural Science Foundation of China (61772036) and Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology). We appreciate the anonymous reviewers for their helpful comments. Xiaojun Wan is the corresponding author.	Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374; Nguyen A, 2016, ADV NEUR IN, V29; [Anonymous], 2017, ABS170702633 CORR; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Devlin Jacob, 2018, P NAACL HLT, DOI DOI 10.18653/V1/N19-1423; Erhan D, 2009, BERNOULLI, V1341, P1; Fu ZX, 2018, AAAI CONF ARTIF INTE, P663; Gan C, 2017, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2017.108; GATYS LA, 2016, PROC CVPR IEEE, P2414, DOI DOI 10.1109/CVPR.2016.265; Goodfellow I. J., 2015, ICLR; He RN, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P507, DOI 10.1145/2872427.2883037; Hu ZT, 2017, PR MACH LEARN RES, V70; Jhamtani Harsh, 2017, ABS170701161 CORR; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Joulin A., 2017, P 15 C EUR CHAPT ASS, P427, DOI DOI 10.18653/V1/E17-2068; Kalchbrenner Nal, 2013, P 2013 C EMP METH NA, P1700, DOI DOI 10.1146/ANNUREV.NEURO.26.041002.131047; Kingma D.P., 2015, INT C LEARN REPR, P1; Lample Guillaume, 2019, ICLR; LI J, 2018, NAACL HLT, P186, DOI DOI 10.1109/ICMEAE.2018.00042; Liao  J., 2017, INT J APPL CERAM TEC, V15; Lipton Zachary C, 2015, ABS151103683 CORR; Logeswaran L., 2018, ADV NEURAL INFORM PR, P5108; Melnyk Igor, 2017, ABS171109395 CORR; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Prabhumoye S, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P866; Shen Tianxiao, 2017, P NIPS, P6830; Singh A., 2018, ABS180404003 CORR; Stolcke A., 2002, INTERSPEECH; Sutskever I., 2018, IMPROVING LANGUAGE U; Sutskever I., 2014, P 27 INT C NEUR INF, V3104, P3112, DOI DOI 10.1021/acs.analchem.7b05329; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tikhonov Alexey, 2018, ABS180804365 CORR; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Wang Chenguang, 2019, ABS190409408 CORR; Wang K, 2019, ARTIF INTELL, V275, P540, DOI 10.1016/j.artint.2019.07.003; Wang K, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4446; Xu JJ, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P979; Xu Wei, 2012, P COLING 2012, P2899; Yang Z., 2018, NEURIPS, P7298; Zhang Zhirui, 2018, ABS180807894 CORR; Zhao Z., 2018, ICLR; Zhou Z., 2018, ICLR; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	44	18	18	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902064
C	Wang, SQ; Zeng, YJ; Liu, XW; Zhu, E; Yin, JP; Xu, CF; Kloft, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Siqi; Zeng, Yijie; Liu, Xinwang; Zhu, En; Yin, Jianping; Xu, Chuanfu; Kloft, Marius			Effective End-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NOVELTY DETECTION; ANOMALY DETECTION	Despite the wide success of deep neural networks (DNN), little progress has been made on end-to-end unsupervised outlier detection (UOD) from high dimensional data like raw images. In this paper, we propose a framework named E(3)Outlier, which can perform UOD in a both effective and end-to-end manner: First, instead of the commonly-used autoencoders in previous end-to-end UOD methods, E(3)Outlier for the first time leverages a discriminative DNN for better representation learning, by using surrogate supervision to create multiple pseudo classes from original unlabelled data. Next, unlike classic UOD that utilizes data characteristics like density or proximity, we exploit a novel property named inlier priority to enable end-to-end UOD by discriminative DNN. We demonstrate theoretically and empirically that the intrinsic class imbalance of inliers/outliers will make the network prioritize minimizing inliers' loss when inliers/outliers are indiscriminately fed into the network for training, which enables us to differentiate outliers directly from DNN's outputs. Finally, based on inlier priority, we propose the negative entropy based score as a simple and effective outlierness measure. Extensive evaluations show that E(3)Outlier significantly advances UOD performance by up to 30% AUROC against state-of-the-art counterparts, especially on relatively difficult benchmarks.	[Wang, Siqi; Liu, Xinwang; Zhu, En; Xu, Chuanfu] Natl Univ Def Technol, Changsha, Peoples R China; [Zeng, Yijie] Nanyang Technol Univ, Singapore, Singapore; [Yin, Jianping] Dongguan Univ Technol, Dongguan, Peoples R China; [Kloft, Marius] Tech Univ Kaiserslautern, Kaiserslautern, Germany	National University of Defense Technology - China; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Dongguan University of Technology; University of Kaiserslautern	Wang, SQ (corresponding author), Natl Univ Def Technol, Changsha, Peoples R China.	wangsiqi10c@nudt.edu.cn; yzeng004@e.ntu.edu.sg; xinwangliu@nudt.edu.cn; enzhu@nudt.edu.cn; jpyin@dgut.edu.cn; xuchuanfu@nudt.edu.cn; kloft@cs.uni-kl.de			National Key R&D Program of China [2018YFB1003203]; National Natural Science Foundation of China (NSFC) [61773392, 61672528]; German Research Foundation (DFG) [KL 2698/2-1]; German Federal Ministry of Education and Research (BMBF) [031L0023A, 01IS18051A, 031B0770E]	National Key R&D Program of China; National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); German Research Foundation (DFG)(German Research Foundation (DFG)); German Federal Ministry of Education and Research (BMBF)(Federal Ministry of Education & Research (BMBF))	This work is supported by National Key R&D Program of China 2018YFB1003203 and National Natural Science Foundation of China (NSFC) under Grant No. 61773392, 61672528. This work is also supported by the German Research Foundation (DFG) award KL 2698/2-1 and by the German Federal Ministry of Education and Research (BMBF) awards 031L0023A, 01IS18051A, and 031B0770E. Xinwang Liu, En Zhu and Jianping Yin are corresponding authors of this paper.	Aggarwal Charu C., 2016, OUTLIER ANAL; Ahmed M, 2016, FUTURE GENER COMP SY, V55, P278, DOI 10.1016/j.future.2015.01.001; Angiulli F., 2002, Principles of Data Mining and Knowledge Discovery. 6th European Conference, PKDD 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2431), P15; Aryal Sunil, 2014, Advances in Knowledge Discovery and Data Mining. 18th Pacific-Asia Conference, PAKDD 2014. Proceedings: LNCS 8444, P510, DOI 10.1007/978-3-319-06605-9_42; Bottou Leon, 1999, ON LINE LEARNING NEU, P9, DOI DOI 10.1017/CBO9780511569920.003; Breunig MM, 2000, SIGMOD REC, V29, P93, DOI 10.1145/335191.335388; Buczak Anna L, IEEE COMMUNICATIONS, V18, P1153; Chalapathy R., 2019, ARXIV PREPRINT ARXIV; Chalapathy R., 2018, ARXIV PREPRINT ARXIV; Chandola V., 2007, ACM COMPUTING SURVEY, V41; Chang JL, 2017, IEEE I CONF COMP VIS, P5880, DOI 10.1109/ICCV.2017.626; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI 10.1002/0471200611; Cruz Rodrigo Santa, 2018, IEEE T PATTERN ANAL; Davis J., 2006, P 23 INT C MACH LEAR, V148, P233, DOI [DOI 10.1145/1143844.1143874, 10.1145/1143844.1143874]; De Paola A, 2015, IEEE T CYBERNETICS, V45, P888, DOI 10.1109/TCYB.2014.2338611; Deecke Lucas, 2018, JOINT EUR C MACH LEA, P3; Dosovitskiy Alexey, 2016, NEURIPS; Emmott AF, 2013, P ACM SIGKDD WORKSH, P16, DOI [10.1145/2500853.2500858, DOI 10.1145/2500853.2500858]; Ester M., 1996, P 2 INT C KNOWL DISC, P226; Golan I, 2018, ADV NEUR IN, V31; GRUBBS FE, 1969, TECHNOMETRICS, V11, P1, DOI 10.2307/1266761; Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86; Hawkins Douglas M, IDENTIFICATION OUTLI, V11; He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He ZY, 2003, PATTERN RECOGN LETT, V24, P1641, DOI 10.1016/S0167-8655(03)00003-5; Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601; Hoffmann H, 2007, PATTERN RECOGN, V40, P863, DOI 10.1016/j.patcog.2006.07.009; Huang C, 2018, IEEE T NEUR NET LEAR, V29, P1503, DOI 10.1109/TNNLS.2017.2671845; Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Japkowicz N, 1995, INT JOINT CONF ARTIF, P518; Ji Xu, 2018, ARXIV COMPUTER VISIO; Jing L., 2019, ARXIV COMPUTER VISIO; Johnson JM, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0192-5; Khan SH, 2018, IEEE T NEUR NET LEAR, V29, P3573, DOI 10.1109/TNNLS.2017.2732482; Kim J, 2012, J MACH LEARN RES, V13, P2529; Kiran BR, 2018, J IMAGING, V4, DOI 10.3390/jimaging4020036; Komodakis Nikos, 2018, INT C LEARN REPR ICL; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Larsen ABL, 2016, PR MACH LEARN RES, V48; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liang Shiyu, 2018, INT C LEARN REPR; Liu FT, 2010, LECT NOTES ARTIF INT, V6322, P274, DOI 10.1007/978-3-642-15883-4_18; Liu FT, 2008, IEEE DATA MINING, P413, DOI 10.1109/ICDM.2008.17; Liu W, 2014, PROC CVPR IEEE, P3826, DOI 10.1109/CVPR.2014.483; Liu Yezheng, 2019, IEEE T KNOWLEDGE DAT; Markou M, 2003, SIGNAL PROCESS, V83, P2481, DOI 10.1016/j.sigpro.2003.07.018; Martinus David, 2002, ONE CLASS CLASSIFICA; Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; Peng X, 2017, AAAI CONF ARTIF INTE, P2478; Perera Pramuditha, 2018, ARXIV180105365; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Ramaswamy S, 2000, SIGMOD REC, V29, P427, DOI 10.1145/335191.335437; Ruff L, 2018, PR MACH LEARN RES, V80; Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707; Schlachter Patrick, 2019, ARXIV190201194; Schlegl T, 2019, MED IMAGE ANAL, V54, P30, DOI 10.1016/j.media.2019.01.010; Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12; Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27; Shyu Mei-ling, 2003, P IEEE FDN NEW DIR D; Song Q., 2018, INT C LEARN REPR; Su F, 2016, 2016 16TH INTERNATIONAL SYMPOSIUM ON COMMUNICATIONS AND INFORMATION TECHNOLOGIES (ISCIT), P280, DOI 10.1109/ISCIT.2016.7751636; Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49; Wang C, 2018, INT C PATT RECOG, P1121, DOI 10.1109/ICPR.2018.8545381; Wang SQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P636, DOI 10.1145/3240508.3240615; Williams G, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P709, DOI 10.1109/ICDM.2002.1184035; Xia Y, 2015, IEEE I CONF COMP VIS, P1511, DOI 10.1109/ICCV.2015.177; Xiao H., 2017, FASHION MNIST NOVEL; Xu D, 2017, COMPUT VIS IMAGE UND, V156, P117, DOI 10.1016/j.cviu.2016.10.010; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhai SF, 2016, PR MACH LEARN RES, V48; Zhao Y., 2018, SECURITY COMMUNICATI, V2018, P1, DOI DOI 10.1145/3173574.3174118; Zhao YR, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1933, DOI 10.1145/3123266.3123451; Zhou C, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P665, DOI 10.1145/3097983.3098052	81	18	18	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													14	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306002
C	Yoo, J; Cho, M; Kim, T; Kang, U		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yoo, Jaemin; Cho, Minyong; Kim, Taebum; Kang, U.			Knowledge Extraction with No Observable Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Knowledge distillation is to transfer the knowledge of a large neural network into a smaller one and has been shown to be effective especially when the amount of training data is limited or the size of the student model is very small. To transfer the knowledge, it is essential to observe the data that have been used to train the network since its knowledge is concentrated on a narrow manifold rather than the whole input space. However, the data are not accessible in many cases due to the privacy or confidentiality issues in medical, industrial, and military domains. To the best of our knowledge, there has been no approach that distills the knowledge of a neural network when no data are observable. In this work, we propose KEGNET (Knowledge Extraction with Generative Networks), a novel approach to extract the knowledge of a trained deep neural network and to generate artificial data points that replace the missing training data in knowledge distillation. Experiments show that KEGNET outperforms all baselines for data-free knowledge distillation. We provide the source code of our paper in https://github.com/snudatalab/KegNet.	[Yoo, Jaemin; Cho, Minyong; Kim, Taebum; Kang, U.] Seoul Natl Univ, Seoul, South Korea	Seoul National University (SNU)	Kang, U (corresponding author), Seoul Natl Univ, Seoul, South Korea.	jaeminyoo@snu.ac.kr; chominyong@gmail.com; k.taebum@snu.ac.kr; ukang@snu.ac.kr			ICT R&D program of MSIT/IITP [2017-0-01772]	ICT R&D program of MSIT/IITP(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of Korea)	This work was supported by the ICT R&D program of MSIT/IITP (No.2017-0-01772, Development of QA systems for Video Story Understanding to pass the Video Turing Test).	Balan Anoop Korattikara, 2015, ADV NEURAL INFORM PR, P3; Chen Tianqi, 2016, ICLR; Cheng J, 2018, FRONT INFORM TECH EL, V19, P64, DOI 10.1631/FITEE.1700789; Cheng Yu, 2017, ARXIV171009282; Djork-Arn, ICLR 2016; Fernandez-Delgado M, 2014, J MACH LEARN RES, V15, P3133; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G., 2015, ARXIV150302531; Kholiavchenko Maksym, 2018, ITERATIVE LOW RANK A; Kim Y., 2016, ICLR; Kim Yoon, 2016, ARXIV160607947, DOI [10.18653/v1/D16-1139, DOI 10.18653/V1/D16-1139]; Kimura Akisato, 2018, BMVC, P105; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kossaifi J., 2017, ARXIV170708308; Kossaifi J, 2019, J MACH LEARN RES, V20; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y., 1995, NEURAL NETW STAT MEC; Li Tianhong, 2018, KNOWLEDGE DISTILLATI; Lopes Raphael Gontijo, 2017, NIPS WORKSH; Nakajima S, 2013, J MACH LEARN RES, V14, P1; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Odena A, 2017, PR MACH LEARN RES, V70; Olson Matthew, 2018, NEURIPS; Papernot Nicolas, 2017, P INT C LEARN REPR, P2; Polino Antonio, 2018, ICLR; Rabanser S., 2017, INTRO TENSOR DECOMPO; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Xiao H., 2017, FASHION MNIST NOVEL	33	18	18	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302067
C	Zhang, WR; Li, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Wenrui; Li, Peng			Spike-Train Level Backpropagation for Training Deep Recurrent Spiking Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Spiking neural networks (SNNs) well support spatio-temporal learning and energy-efficient event-driven hardware neuromorphic processors. As an important class of SNNs, recurrent spiking neural networks (RSNNs) possess great computational power. However, the practical application of RSNNs is severely limited by challenges in training. Biologically-inspired unsupervised learning has limited capability in boosting the performance of RSNNs. On the other hand, existing backpropagation (BP) methods suffer from high complexity of unfolding in time, vanishing and exploding gradients, and approximate differentiation of discontinuous spiking activities when applied to RSNNs. To enable supervised training of RSNNs under a well-defined loss function, we present a novel Spike-Train level RSNNs Backpropagation (ST-RSBP) algorithm for training deep RSNNs. The proposed ST-RSBP directly computes the gradient of a rate-coded loss function defined at the output layer of the network w.r.t tunable parameters. The scalability of ST-RSBP is achieved by the proposed spike-train level computation during which temporal effects of the SNN is captured in both the forward and backward pass of BP. Our ST-RSBP algorithm can be broadly applied to RSNNs with a single recurrent layer or deep RSNNs with multiple feedforward and recurrent layers. Based upon challenging speech and image datasets including TI46 [25], N-TIDIGITS [3], Fashion-MNIST [40] and MNIST, ST-RSBP is able to train SNNs with an accuracy surpassing that of the current state-of-the-art SNN BP algorithms and conventional non-spiking deep learning models.	[Zhang, Wenrui; Li, Peng] Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA	University of California System; University of California Santa Barbara	Zhang, WR (corresponding author), Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.	wenruizhang@ucsb.edu; lip@ucsb.edu	Zhang, Wenrui/AAB-2428-2020		National Science Foundation (NSF) [1639995, 1948201]; Semiconductor Research Corporation (SRC) [2692.001]	National Science Foundation (NSF)(National Science Foundation (NSF)); Semiconductor Research Corporation (SRC)	This material is based upon work supported by the National Science Foundation (NSF) under Grants No.1639995 and No.1948201. This work is also supported by the Semiconductor Research Corporation (SRC) under Task 2692.001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of NSF, SRC, UC Santa Barbara, and their contractors.	Agarap A. F. M., 2018, DEEP LEARNING USING, P1; Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396; Anumula J, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00023; Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0; Collobert R., 2008, AUNIFIED ARCHITECTUR, P160; Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359; Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099; Diehl Peter U, 2015, IJCNN, DOI DOI 10.1109/IJCNN.2015.7280696; Esser S. K., 2015, ADV NEURAL INFORM PR, P1117; Gerstner W., 2002, SPIKING NEURON MODEL; Ghani A, 2008, LECT NOTES COMPUT SC, V5163, P513, DOI 10.1007/978-3-540-87536-9_53; Goodfellow I., 2016, DEEP LEARNING; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Huh D., 2018, ADV NEURAL INFORM PR, P1433; Hunsberger Eric, 2015, ARXIV151008829; Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105; Jin Y., 2018, P ADV NEUR INF PROC, P7005; Jin YYZ, 2016, IEEE IJCNN, P1158, DOI 10.1109/IJCNN.2016.7727328; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508; Leonard R.G., 1993, TIDIGITS SPEECH CORP; Liberman M., 1991, TI 46 WORD LDC93S9; Lyon R. F., 1982, Proceedings of ICASSP 82. IEEE International Conference on Acoustics, Speech and Signal Processing, P1282; Maass, 2018, ADV NEURAL INFORM PR, P787; Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955; Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642; Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1; Ororbia II A. G., 2018, ARXIV180511703; Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901; Schrauwen B, 2003, IEEE IJCNN, P2825; Shrestha S.B., 2018, ADV NEURAL INFORM PR, V31, P1412; Simard PY, 2003, PROC INT CONF DOC, P958; SRINIVASAN G, 2018, FRONTIERS NEUROSCIEN, V12; Szegedy Christian, 2013, ADV NEURAL INFORM PR, P3, DOI DOI 10.5555/2999792.2999897; WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337; Wijesinghe P, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00504; Wu Y., 2017, ARXIV170602609; Xiao H., 2017, ARXIV 170807747; Zhang Y, 2015, IEEE T NEUR NET LEAR, V26, P2635, DOI 10.1109/TNNLS.2015.2388544	41	18	18	2	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307078
C	Chen, BH; Deng, WH; Shen, HF		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Binghui; Deng, Weihong; Shen, Haifeng			Virtual Class Enhanced Discriminative Embedding Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recently, learning discriminative features to improve the recognition performances gradually becomes the primary goal of deep learning, and numerous remarkable works have emerged. In this paper, we propose a novel yet extremely simple method Virtual Softmax to enhance the discriminative property of learned features by injecting a dynamic virtual negative class into the original softmax. Injecting virtual class aims to enlarge inter-class margin and compress intra-class distribution by strengthening the decision boundary constraint. Although it seems weird to optimize with this additional virtual class, we show that our method derives from an intuitive and clear motivation, and it indeed encourages the features to be more compact and separable. This paper empirically and experimentally demonstrates the superiority of Virtual Softmax, improving the performances on a variety of object classification and face verification tasks.	[Chen, Binghui; Deng, Weihong] Beijing Univ Posts & Telecommun, Beijing, Peoples R China; [Shen, Haifeng] Didi Chuxing, AI Labs, Beijing 100193, Peoples R China	Beijing University of Posts & Telecommunications	Chen, BH (corresponding author), Beijing Univ Posts & Telecommun, Beijing, Peoples R China.	chenbinghui@bupt.edu.cn; whdeng@bupt.edu.cn; shenhaifeng@didiglobal.com	Deng, Wei/GWC-9207-2022	Deng, Weihong/0000-0001-5952-6996	National Natural Science Foundation of China [61573068, 61871052]; Beijing Nova Program [Z161100004916088]; DiDi GAIA Research Collaboration Initiative	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Nova Program(Beijing Municipal Science & Technology Commission); DiDi GAIA Research Collaboration Initiative	This work was partially supported by the National Natural Science Foundation of China under Grant Nos. 61573068 and 61871052, Beijing Nova Program under Grant No. Z161100004916088, and sponsored by DiDi GAIA Research Collaboration Initiative.	Branson S., 2014, PROC BRIT MACH VIS C; Burges, 1998, MNIST DATABASE HANDW; Chen B, 2018, ARXIV180600974; Chen BC, 2017, IEEE COMPUT SOC CONF, P1872, DOI 10.1109/CVPRW.2017.234; Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149; Chrabaszcz Patryk, 2017, ARXIV170708819; Deng WH, 2017, PATTERN RECOGN, V66, P63, DOI 10.1016/j.patcog.2016.11.023; Goodfellow I. J., 2013, ARXIV13024389; Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang GB, 2007, 07 UMASS TR; Jeon YH, 2017, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2017.200; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jie H., 2017, P IEEE C COMP VIS PA, P99; Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lee CY, 2016, JMLR WORKSH CONF PRO, V51, P464; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958; Liu W., 2017, P IEEE C COMPUTER VI, P212; Liu WY, 2016, PR MACH LEARN RES, V48; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Parkhi Omkar M., 2015, BRIT MACH VIS C; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sohn Kihyuk, 2016, NEURIPS, DOI DOI 10.5555/3157096.3157304; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Wang Feng, 2017, ARXIV170406369; Wang YM, 2016, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2016.131; Welinder P., 2010, CNSTR2010001 CALTECH; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032; Xie LX, 2016, PROC CVPR IEEE, P4753, DOI 10.1109/CVPR.2016.514; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54; Zhou F, 2016, PROC CVPR IEEE, P1124, DOI 10.1109/CVPR.2016.127	41	18	18	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301089
C	Chen, LQ; Dai, SY; Tao, CY; Shen, DH; Gan, Z; Zhang, HC; Zhang, YZ; Carin, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Liqun; Dai, Shuyang; Tao, Chenyang; Shen, Dinghan; Gan, Zhe; Zhang, Haichao; Zhang, Yizhe; Carin, Lawrence			Adversarial Text Generation via Feature-Mover's Distance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Generative adversarial networks (GANs) have achieved significant success in generating real-valued data. However, the discrete nature of text hinders the application of GAN to text-generation tasks. Instead of using the standard GAN objective, we propose to improve text-generation GAN via a novel approach inspired by optimal transport. Specifically, we consider matching the latent feature distributions of real and synthetic sentences using a novel metric, termed the featuremover's distance (FMD). This formulation leads to a highly discriminative critic and easy-to-optimize objective, overcoming the mode-collapsing and brittle-training problems in existing methods. Extensive experiments are conducted on a variety of tasks to evaluate the proposed model empirically, including unconditional text generation, style transfer from non-parallel text, and unsupervised cipher cracking. The proposed model yields superior performance, demonstrating wide applicability and effectiveness.	[Chen, Liqun; Dai, Shuyang; Tao, Chenyang; Shen, Dinghan; Carin, Lawrence] Duke Univ, Durham, NC 27706 USA; [Gan, Zhe] Microsoft Dynam 365 AI Res, Newark, DE USA; [Zhang, Yizhe] Microsoft Res, Redmond, WA USA; [Zhang, Haichao] Baidu Res, Sunnyvale, CA USA	Duke University; Microsoft; Baidu	Chen, LQ (corresponding author), Duke Univ, Durham, NC 27706 USA.	liqun.chen@duke.edu		Carin, Lawrence/0000-0001-6277-7948	DARPA; ONR; NSF; DOE; NIH	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); DOE(United States Department of Energy (DOE)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This research was supported in part by DARPA, DOE, NIH, ONR and NSF.	Afriat S., 1971, SIAM J APPL MATH; [Anonymous], 2017, ABS170207983 CORR; [Anonymous], 2017, ICCV; Arjovsky M., 2017, P 2017 INT C LEARN R, P1; Artetxe M, 2018, ICLR; Bahdanau Dzmitry, 2015, ICLR; Bengio Samy, 2015, NIPS; Bottou L., 2017, ICML; Bruen AA, 2011, CRYPTOGRAPHY INFORM, V68; Chen L., 2018, AISTATS; Cho Kyunghyun, 2014, EMNLP; Collobert R, 2011, JMLR; Conneau Alexis, 2017, ARXIV171004087; Cuturi M., 2013, NEURIPS; Denoyer L, 2018, PROCEEDINGS; Fang H., 2015, C COMP VIS PATT REC; Fedus W., 2018, INT C LEARN REPR; Francis W. N., 1979, DEP LINGUISTICS; Gan Zhe, 2017, NIPS; Gan Zhe, 2017, EMNLP; Genevay A., 2018, AISTATS; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A., 2012, JMLR; Gulrajani Ishaan, 2017, NIPS; Guo J., 2018, AAAI; Hochreiter S, 1997, NEURAL COMPUTATION; Hu Z., 2017, ICML; Huang S., 2018, ARXIV180104883; Huszar Ferenc, 2015, ABS151105101 CORR; Jang Eric, 2017, ICLR 2017; Kim Y., 2014, ARXIV14085882; Kucera H., 1979, STANDARD CORPUS PRES; Kusner Matt J, 2016, ARXIV161104051; Lamb A., 2016, ARXIV160203220; Li C., 2017, NIPS; Li J., 2018, NAACL; Li J., 2017, EMNLP; Lin K, 2017, NIPS; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Liu Y., 2017, ARXIV170207817; Mikolov T., 2010, ISCA; Nowozin Sebastian, 2016, P ADV NEURAL INFORM; Papineni K., 2002, P ANN M ASS COMP LIN; Prabhumoye S., 2018, ACL; Pu Y., 2017, NIPS; Pu Yunchen, 2018, ICML; Ranzato M., 2016, ICLR; Rubner Y., 1998, ICCV; Salimans Tim, 2018, ICLR; Salimans Tim, 2016, ADV NEURAL INFORM PR; Shen Dinghan, 2018, AAAI; Shen Shiqi, 2015, ARXIV PREPRINT ARXIV; Shen T., 2017, NIPS; Tao C., 2018, ICML; Teh Y. W., 2017, ICLR; Vinyals O., 2015, CVPR; Wang G., 2018, ACL; Welinder Peter., 2010, CNSTR2010001 CALTECH, V200; Wiseman S, 2016, EMNLP; Xie Y., 2018, ARXIV180204307; Yu L., 2017, AAAI; Zhang Y., 2017, ICML; Zhu Y., 2018, SIGIR	63	18	18	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304066
C	Drumond, M; Lin, T; Jaggi, M; Falsafi, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Drumond, Mario; Lin, Tao; Jaggi, Martin; Falsafi, Babak			Training DNNs with Hybrid Block Floating Point	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The wide adoption of DNNs has given birth to unrelenting computing requirements, forcing datacenter operators to adopt domain-specific accelerators to train them. These accelerators typically employ densely packed full-precision floating-point arithmetic to maximize performance per area. Ongoing research efforts seek to further increase that performance density by replacing floating-point with fixedpoint arithmetic. However, a significant roadblock for these attempts has been fixed point's narrow dynamic range, which is insufficient for DNN training convergence. We identify block floating point (BFP) as a promising alternative representation since it exhibits wide dynamic range and enables the majority of DNN operations to be performed with fixed-point logic. Unfortunately, BFP alone introduces several limitations that preclude its direct applicability. In this work, we introduce HBFP, a hybrid BFP-FP approach, which performs all dot products in BFP and other operations in floating point. HBFP delivers the best of both worlds: the high accuracy of floating point at the superior hardware density of fixed point. For a wide variety of models, we show that HBFP matches floating point's accuracy while enabling hardware implementations that deliver up to 8.5 x higher throughput.	[Drumond, Mario; Lin, Tao; Jaggi, Martin; Falsafi, Babak] Ecole Polytech Fed Lausanne, Ecocloud, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Drumond, M (corresponding author), Ecole Polytech Fed Lausanne, Ecocloud, Lausanne, Switzerland.	mario.drumond@epfl.ch; tao.lin@epfl.ch; martin.jaggi@epfl.ch; babak.falsafi@epfl.ch		Jaggi, Martin/0000-0003-1579-5558	ColTraIn project of the Microsoft-EPFL Joint Research Center; SNSF [200021_175796]	ColTraIn project of the Microsoft-EPFL Joint Research Center(Microsoft); SNSF(Swiss National Science Foundation (SNSF))	The authors thank the anonymous reviewers, Mark Sutherland, Siddharth Gupta, and Alexandros Daglis for their precious comments and feedback. We also thank Ryota Tomioka and Eric Chung for many inspiring conversations on low-precision DNN processing. This work has been partially funded by the ColTraIn project of the Microsoft-EPFL Joint Research Center and by SNSF grant 200021_175796.	Alistarh D., 2017, P 31 C NEUR INF PROC; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Chen YH, 2016, CONF PROC INT SYMP C, P367, DOI 10.1109/ISCA.2016.40; Courbariaux M., 2015, P 29 C NEUR INF PROC; Dally W., 2015, HIGH PERFORMANCE HAR; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Huang G., 2016, P 13 EUR C COMP VIS; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Hubara I, 2016, ADV NEUR IN, V29; Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kster Urs, 2017, ABS171102213 CORR; [李凡杰 Li Fanjie], 2016, [低温工程, Cryogenics], P1; Marsaglia G., 2003, J STAT SOFTW, V8, P1, DOI [10.18637/jss.v008.i18, DOI 10.18637/JSS.V008.I18]; Merity S., 2018, 6 INT C LEARN REPR I; Micikevicius Paulius, 2018, P 6 INT C LEARN REPR, DOI [10.48550/arXiv.1710.03740, DOI 10.1109/CAMAD.2018.8514963]; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Netzer Yuval, 2011, NEURIPS WORKSH, V2, P6; Rastegari M., 2016, P 13 EUR C COMP VIS; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sa C. D., 2017, P 44 INT S COMP ARCH; Song Z., 2018, P 32 AAAI C ART INT; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang H., 2017, P 34 INT C MACH LEAR; Zhou S., 2016, ARXIV160606160; Zhu Chenzhuo, 2017, ICLR	27	18	18	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300042
C	Huo, ZY; Gu, B; Huang, H		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Huo, Zhouyuan; Gu, Bin; Huang, Heng			Training Neural Networks Using Features Replay	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Training a neural network using backpropagation algorithm requires passing error gradients sequentially through the network. The backward locking prevents us from updating network layers in parallel and fully leveraging the computing resources. Recently, there are several works trying to decouple and parallelize the backpropagation algorithm. However, all of them suffer from severe accuracy loss or memory explosion when the neural network is deep. To address these challenging issues, we propose a novel parallel-objective formulation for the objective function of the neural network. After that, we introduce features replay algorithm and prove that it is guaranteed to converge to critical points for the non-convex problem under certain conditions. Finally, we apply our method to training deep convolutional neural networks, and the experimental results show that the proposed method achieves faster convergence, lower memory consumption, and better generalization error than compared methods.	[Huo, Zhouyuan; Gu, Bin; Huang, Heng] Univ Pittsburgh, Elect & Comp Engn, Pittsburgh, PA 15260 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Huang, H (corresponding author), Univ Pittsburgh, Elect & Comp Engn, Pittsburgh, PA 15260 USA.	zhouyuan.huo@pitt.edu; jsgubin@gmail.com; heng.huang@pitt.edu						Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Balduzzi D, 2015, AAAI CONF ARTIF INTE, P485; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Bottou L., 2016, ARXIV160604838; Carreira-Perpinan MA, 2014, JMLR WORKSH CONF PRO, V33, P10; Chenoweth JM, 2016, FLA MUS NAT HIST-RIP, P1; dos Santos CN, 2014, PR MACH LEARN RES, V32, P1818; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Eldan R., 2016, P 29 C LEARNING THEO, V49, P907; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton Geoffrey, COURSERA LECT SLIDES; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huo ZY, 2018, PR MACH LEARN RES, V80; Jaderberg M., 2016, ARXIV160805343; Johnson J., 2017, BENCHMARKS POPULAR C; Kalchbrenner N, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P655, DOI 10.3115/v1/p14-1062; Kim Y., 2014, P 2014 C EMPIRICAL M, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954]; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nair V, 2010, P 27 INT C MACHINE L, P807; Nokland, 2016, ADV NEURAL INFORM PR, V29, P1037; Paszke A., 2017, AUTOMATIC DIFFERENTI; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Taylor G, 2016, PR MACH LEARN RES, V48; Telgarsky M.., 2016, C LEARNING THEORY, P1517; Zhang Xiang, 2015, ADV NEURAL INFORM PR, P649, DOI DOI 10.5555/2969239.2969312	36	18	18	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001022
C	Liu, SC; Hu, Y; Zeng, YM; Tang, QK; Jin, BB; Han, YH; Li, XW		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Shice; Hu, Yu; Zeng, Yiming; Tang, Qiankun; Jin, Beibei; Han, Yinhe; Li, Xiaowei			See and Think: Disentangling Semantic Scene Completion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				OBJECT DETECTION	Semantic scene completion predicts volumetric occupancy and object category of a 3D scene, which helps intelligent agents to understand and interact with the surroundings. In this work, we propose a disentangled framework, sequentially carrying out 2D semantic segmentation, 2D-3D reprojection and 3D semantic scene completion. This three-stage framework has three advantages: (1) explicit semantic segmentation significantly boosts performance; (2) flexible fusion ways of sensor data bring good extensibility; (3) progress in any subtask will promote the holistic performance. Experimental results show that regardless of inputing a single depth or RGB-D, our framework can generate high-quality semantic scene completion, and outperforms state-of-the-art approaches on both synthetic and real datasets.	[Liu, Shice; Hu, Yu; Zeng, Yiming; Tang, Qiankun; Jin, Beibei; Han, Yinhe; Li, Xiaowei] Chinese Acad Sci, Inst Comp Technol, State Key Lab Comp Architecture, Beijing, Peoples R China; [Liu, Shice; Hu, Yu; Zeng, Yiming; Tang, Qiankun; Jin, Beibei; Han, Yinhe; Li, Xiaowei] Univ Chinese Acad Sci, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Computing Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Liu, SC (corresponding author), Chinese Acad Sci, Inst Comp Technol, State Key Lab Comp Architecture, Beijing, Peoples R China.; Liu, SC (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.	liushice@ict.ac.cn; huyu@ict.ac.cn; zengyiming@ict.ac.cn; tangqiankun@ict.ac.cn; jinbeibei@ict.ac.cn; yinhes@ict.ac.cn; lxw@ict.ac.cn		Li, Xiaowei/0000-0002-0874-814X	National Natural Science Foundation of China [61274030]; Innovation Project of Institute of Computing Technology, Chinese Academy of Sciences [20186090]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Innovation Project of Institute of Computing Technology, Chinese Academy of Sciences	We thank Shuran Song for sharing the SSCNet results and the SUNCG development kits. This work is supported in part by National Natural Science Foundation of China under grant No. 61274030, and in part by Innovation Project of Institute of Computing Technology, Chinese Academy of Sciences under grant No. 20186090. Yu Hu is the corresponding author.	[Anonymous], 2018, ARXIV180403550; [Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; [Anonymous], 2017, UNDERSTANDING CONVOL; Blaha M, 2016, PROC CVPR IEEE, P3176, DOI 10.1109/CVPR.2016.346; CAPRILE B, 1990, INT J COMPUT VISION, V4, P127, DOI 10.1007/BF00127813; Chang A. X., 2015, ARXIV PREPRINT ARXIV; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Nguyen DT, 2016, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2016.612; Firman M, 2016, PROC CVPR IEEE, P5431, DOI 10.1109/CVPR.2016.586; Geiger A, 2015, LECT NOTES COMPUT SC, V9358, P183, DOI 10.1007/978-3-319-24947-6_15; Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29; Goldstein E.B., 2008, BLACKWELL HDB SENSAT; Guedes A. B. S., 2018, ARXIV180204735; Guo R., 2015, ARXIV150402437; Gupta S, 2015, PROC CVPR IEEE, P4731, DOI 10.1109/CVPR.2015.7299105; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79; Hane C, 2013, PROC CVPR IEEE, P97, DOI 10.1109/CVPR.2013.20; Handa A, 2015, ABS151107041 CORR; Jiang H, 2013, PROC CVPR IEEE, P2171, DOI 10.1109/CVPR.2013.282; Jimenez Rezende D., 2016, ADV NEURAL INFORM PR, P4996; Kar A., 2017, ADV NEURAL INFORM PR; Kim BS, 2013, IEEE I CONF COMP VIS, P1425, DOI 10.1109/ICCV.2013.180; Lin DH, 2013, IEEE I CONF COMP VIS, P1417, DOI 10.1109/ICCV.2013.179; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Mattausch O, 2014, COMPUT GRAPH FORUM, V33, P11, DOI 10.1111/cgf.12286; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Ren XF, 2012, PROC CVPR IEEE, P2759, DOI 10.1109/CVPR.2012.6247999; SEKULER AB, 1992, J EXP PSYCHOL GEN, V121, P95, DOI 10.1037/0096-3445.121.1.95; Silberman N, 2014, LECT NOTES COMPUT SC, V8691, P488, DOI 10.1007/978-3-319-10578-9_32; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94; Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695; Tateno K, 2016, IEEE INT CONF ROBOT, P2295, DOI 10.1109/ICRA.2016.7487378; Wu JJ, 2017, ADV NEUR IN, V30; Wu JJ, 2016, LECT NOTES COMPUT SC, V9910, P365, DOI 10.1007/978-3-319-46466-4_22; Yu Fisher, 2016, MULTISCALE CONTEXT A; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zheng B, 2013, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2013.402	43	18	18	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300025
C	Sidford, A; Wang, MD; Wu, X; Yang, LF; Ye, YY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sidford, Aaron; Wang, Mengdi; Wu, Xian; Yang, Lin F.; Ye, Yinyu			Near-Optimal Time and Sample Complexities for Solving Markov Decision Processes with a Generative Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper we consider the problem of computing an epsilon-optimal policy of a discounted Markov Decision Process (DMDP) provided we can only access its transition function through a generative sampling model that given any state-action pair samples from the transition function in O(1) time. Given such a DMDP with states S, actions A, discount factor gamma is an element of (0, 1), and rewards in range [0, 1] we provide an algorithm which computes an epsilon-optimal policy with probability 1 - delta where both the time spent and number of sample taken are upper bounded by O[vertical bar S vertical bar vertical bar A vertical bar/(1 - gamma)(3) epsilon(2) log (vertical bar S vertical bar vertical bar A vertical bar/(1 - gamma)delta epsilon) log (1/(1 - gamma)epsilon)] For fixed values of epsilon, this improves upon the previous best known bounds by a factor of (1 - gamma)(-1) and matches the sample complexity lower bounds proved in [AMK13] up to logarithmic factors. We also extend our method to computing epsilon-optimal policies for finite-horizon MDP with a generative model and provide a nearly matching sample complexity lower bound.	[Sidford, Aaron; Wu, Xian; Ye, Yinyu] Stanford Univ, Stanford, CA 94305 USA; [Wang, Mengdi; Yang, Lin F.] Princeton Univ, Princeton, NJ 08544 USA	Stanford University; Princeton University	Sidford, A (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	sidford@stanford.edu; mengdiw@princeton.edu; xwu20@stanford.edu; lin.yang@princeton.edu; yyye@stanford.edu		Wang, Mengdi/0000-0002-2101-9507				Bellman RE, 1957, DYNAMIC PROGRAMMING; Bertsekas D. P., 2013, ABSTRACT DYNAMIC PRO; Dantzig G., 2016, LINEAR PROGRAMMING E; De Ghellinck G., 1960, CAHIERS CTR ETUDES R, V2, P161; DEPENOUX F, 1963, MANAGE SCI, V10, P98, DOI 10.1287/mnsc.10.1.98; Hansen TD, 2013, J ACM, V60, DOI 10.1145/2432622.2432623; Howard Ronald A., 1960, DYNAMIC PROGRAMMING; Kakade Sham M., 2003, THESIS; Kalathil D., 2014, ARXIV14120180; Lee YT, 2015, ANN IEEE SYMP FOUND, P230, DOI 10.1109/FOCS.2015.23; Lee YT, 2014, ANN IEEE SYMP FOUND, P424, DOI 10.1109/FOCS.2014.52; Littman M. L., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P394; Mansour Y, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P401; Munos Remi, 1999, VARIABLE RESOLUTION, P256; Scherrer Bruno, 2013, ADV NEURAL INFORM PR, V26, P386; Sidford A, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P770; Strehl AL, 2009, J MACH LEARN RES, V10, P2413; TSENG P, 1990, OPER RES LETT, V9, P287, DOI 10.1016/0167-6377(90)90022-W; Wang M., 2017, ARXIV170401869; Ye YY, 2011, MATH OPER RES, V36, P593, DOI 10.1287/moor.1110.0516; Ye YY, 2005, MATH OPER RES, V30, P733, DOI 10.1287/moor.1050.0149	25	18	19	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305022
C	Tang, C; Garreau, D; von Luxburg, U		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tang, Cheng; Garreau, Damien; von Luxburg, Ulrike			When do random forests fail?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Random forests are learning algorithms that build large collections of random trees and make predictions by averaging the individual tree predictions. In this paper, we consider various tree constructions and examine how the choice of parameters affects the generalization error of the resulting random forests as the sample size goes to infinity. We show that subsampling of data points during the tree construction phase is important: Forests can become inconsistent with either no subsampling or too severe subsampling. As a consequence, even highly randomized trees can lead to inconsistent forests if no subsampling is used, which implies that some of the commonly used setups for random forests can be inconsistent. As a second consequence we can show that trees that have good performance in nearest-neighbor search can be a poor choice for random forests.	[Tang, Cheng] George Washington Univ, Washington, DC 20037 USA; [Garreau, Damien] Max Planck Inst Intelligent Syst, Tubingen, Germany; [von Luxburg, Ulrike] Univ Tubingen, Max Planck Inst Intelligent Syst, Tubingen, Germany	George Washington University; Max Planck Society; Eberhard Karls University of Tubingen; Max Planck Society	Tang, C (corresponding author), George Washington Univ, Washington, DC 20037 USA.	tangch@gwu.edu; damien.garreau@tuebingen.mpg.de; luxburg@informatik.uni-tuebingen.de			German Research Foundation via the Research Unit 1735 "Structural Inference in Statistics: Adaptation and Efficiency"; Institutional Strategy of the University of Tubingen [DFG ZUK 63]	German Research Foundation via the Research Unit 1735 "Structural Inference in Statistics: Adaptation and Efficiency"; Institutional Strategy of the University of Tubingen	The authors thank Debarghya Ghoshdastidar for his careful proofreading of a previous version of this article. This research has been supported by the German Research Foundation via the Research Unit 1735 "Structural Inference in Statistics: Adaptation and Efficiency" and and the Institutional Strategy of the University of Tubingen (DFG ZUK 63).	Belgiu M, 2016, ISPRS J PHOTOGRAMM, V114, P24, DOI 10.1016/j.isprsjprs.2016.01.011; Biau G, 2016, TEST-SPAIN, V25, P197, DOI 10.1007/s11749-016-0481-7; Biau G, 2012, J MACH LEARN RES, V13, P1063; Biau G, 2008, J MACH LEARN RES, V9, P2015; Billingsley P., 2008, PROBABILITY MEASURE; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Breiman L., 2000, TECHNICAL REPORT; Criminisi A., 2013, DECISION FORESTCOM; Dasgupta S, 2015, ALGORITHMICA, V72, P237, DOI 10.1007/s00453-014-9885-5; Dasgupta S, 2008, ACM S THEORY COMPUT, P537; Denil M, 2014, PR MACH LEARN RES, V32; Diaz-Uriarte R, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-3; Genuer R., 2010, PATTERN RECOGN LETT, V31, P2225, DOI [10.1038/nature08649, DOI 10.1038/nature08649, DOI 10.1016/j.patrec.2010.03.014]; Genuer R., 2014, ANAL PURELY RANDOM F; Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1; Hastie T TR, 2009, ELEMENTS STAT LEARNI, V2nd; Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601; Kpotufe S, 2012, J COMPUT SYST SCI, V78, P1496, DOI 10.1016/j.jcss.2012.01.002; Lin Y, 2006, J AM STAT ASSOC, V101, P578, DOI 10.1198/016214505000001230; Menze BH, 2011, LECT NOTES ARTIF INT, V6912, P453, DOI 10.1007/978-3-642-23783-6_29; Olshen R., 1984, CLASSIFICATION REGRE; Rodriguez JJ, 2006, IEEE T PATTERN ANAL, V28, P1619, DOI 10.1109/TPAMI.2006.211; Scornet E, 2015, ANN STAT, V43, P1716, DOI 10.1214/15-AOS1321; Scornet E, 2016, J MULTIVARIATE ANAL, V146, P72, DOI 10.1016/j.jmva.2015.06.009; STONE CJ, 1980, ANN STAT, V8, P1348, DOI 10.1214/aos/1176345206; Tomita TM, 2015, RANDOMER FORESTS; YIANILOS PN, 1993, PROCEEDINGS OF THE FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P311	31	18	18	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303002
C	Tschannen, M; Agustsson, E; Lucic, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tschannen, Michael; Agustsson, Eirikur; Lucic, Mario			Deep Generative Models for Distribution-Preserving Lossy Compression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				QUANTIZATION	We propose and study the problem of distribution-preserving lossy compression. Motivated by recent advances in extreme image compression which allow to maintain artifact-free reconstructions even at very low bitrates, we propose to optimize the rate-distortion tradeoff under the constraint that the reconstructed samples follow the distribution of the training data. The resulting compression system recovers both ends of the spectrum: On one hand, at zero bitrate it learns a generative model of the data, and at high enough bitrates it achieves perfect reconstruction. Furthermore, for intermediate bitrates it smoothly interpolates between learning a generative model of the training data and perfectly reconstructing the training samples. We study several methods to approximately solve the proposed optimization problem, including a novel combination of Wasserstein GAN and Wasserstein Autoencoder, and present an extensive theoretical and empirical characterization of the proposed compression systems.	[Tschannen, Michael] Swiss Fed Inst Technol, Zurich, Switzerland; [Agustsson, Eirikur] Google AI Percept, Zurich, Switzerland; [Lucic, Mario] Google Brain, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Tschannen, M (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	michaelt@nari.ee.ethz.ch; eirikur@google.com; lucic@google.com						Agustsson E., 2018, ARXIV180402958; Agustsson E, 2017, ADV NEUR IN, V30; Arjovsky M, 2017, PR MACH LEARN RES, V70; Ball Johannes, 2018, INT C LEARN REPR ICL; Balle J., 2016, INT C LEARN REPR ICL; Bellard F, 2018, BPG IMAGE FORMAT; DELP EJ, 1991, IEEE T COMMUN, V39, P1549, DOI 10.1109/26.111432; Donahue Jeff, 2017, INT C LEARN REPR ICL; Dosovitskiy Alexey, 2016, NEURIPS; Dumoulin Vincent, 2017, ICLR; Galteri L, 2017, IEEE I CONF COMP VIS, P4836, DOI 10.1109/ICCV.2017.517; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graf S., 2007, FDN QUANTIZATION PRO; Gretton A, 2012, J MACH LEARN RES, V13, P723; Gulrajani I, 2017, P NIPS 2017; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hensel M, 2017, ADV NEUR IN, V30; Johnston N, 2017, ARXIV170310114; Kankanahalli S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2521; Kingma D.P, P 3 INT C LEARNING R; Larsen A. B. L., 2015, ARXIV PREPRINT ARXIV; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li MH, 2018, PROC CVPR IEEE, P6644, DOI 10.1109/CVPR.2018.00695; Li MY, 2010, IEEE SIGNAL PROC LET, V17, P1014, DOI 10.1109/LSP.2010.2087749; Liese F., 2007, STAT DECISION THEORY, P1; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lucic Mario, 2018, ADV NEURAL INFORM PR; Luschgy H, 2002, J FUNCT ANAL, V196, P486, DOI 10.1016/S0022-1236(02)00010-1; Mathieu Michael, 2016, ICLR; Mentzer F, 2018, PROC CVPR IEEE, P4394, DOI 10.1109/CVPR.2018.00462; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rippel O, 2017, PR MACH LEARN RES, V70; Rosca Mihaela, 2017, ARXIV170604987; Santurkar S, 2018, PICT COD SYMP, P258; Theis Lucas, 2017, INT C LEARN REPR; Toderici G, 2015, INT C LEARN REPR ICL; Todeschini G, 2017, INVENTIONS-BASEL, V2, DOI 10.3390/inventions2030014; Torfason R., 2018, INT C LEARN REPR ICL; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wu C. - Y., 2018, EUR C COMP VIS ECCV; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	45	18	18	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000043
C	Yu, RS; Liu, WY; Zhang, YS; Qu, Z; Zhao, DL; Zhang, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yu, Runsheng; Liu, Wenyu; Zhang, Yasen; Qu, Zhi; Zhao, Deli; Zhang, Bo			DeepExposure: Learning to Expose Photos with Asynchronously Reinforced Adversarial Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				IMAGE; FUSION	The accurate exposure is the key of capturing high-quality photos in computational photography, especially for mobile phones that are limited by sizes of camera modules. Inspired by luminosity masks usually applied by professional photographers, in this paper, we develop a novel algorithm for learning local exposures with deep reinforcement adversarial learning. To be specific, we segment an image into sub-images that can reflect variations of dynamic range exposures according to raw low-level features. Based on these sub-images, a local exposure for each sub-image is automatically learned by virtue of policy network sequentially while the reward of learning is globally designed for striking a balance of overall exposures. The aesthetic evaluation function is approximated by discriminator in generative adversarial networks. The reinforcement learning and the adversarial learning are trained collaboratively by asynchronous deterministic policy gradient and generative loss approximation. To further simply the algorithmic architecture, we also prove the feasibility of leveraging the discriminator as the value function. Further more, we employ each local exposure to retouch the raw input image respectively, thus delivering multiple retouched images under different exposures which are fused with exposure blending. The extensive experiments verify that our algorithms are superior to state-of-the-art methods in terms of quantitative accuracy and visual illustration.	[Yu, Runsheng; Liu, Wenyu; Zhang, Yasen; Qu, Zhi; Zhao, Deli; Zhang, Bo] Xiaomi AI Lab, Guangzhou, Guangdong, Peoples R China; [Yu, Runsheng] South China Normal Univ, Guangzhou, Guangdong, Peoples R China; [Liu, Wenyu] Peking Univ, Beijing, Peoples R China	South China Normal University; Peking University	Yu, RS (corresponding author), Xiaomi AI Lab, Guangzhou, Guangdong, Peoples R China.; Yu, RS (corresponding author), South China Normal Univ, Guangzhou, Guangdong, Peoples R China.	runshengyu@gmail.com; liuwenyu@pku.edu.cn; zhangyasen@xiaomi.com; quzhi@xiaomi.com; zhaodeli@xiaomi.com; zhangbo@xiaomi.com		Wenyu, Liu/0000-0002-3035-987X				AGHAGOLZADEH S, 1992, OPT ENG, V31, P614, DOI 10.1117/12.56095; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bychkovsky V, 2011, PROC CVPR IEEE, P97; Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218; Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660; Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Florensa C., 2017, PROC 1 C ROBOT LEARN, P482; Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031; Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Hu Yuanming, 2017, ARXIV170909602; Hui Fang, 2017, ARXIV170703491; Ignatov A., 2017, ARXIV170901118; Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Kim Taeksoo, 2017, P 34 INT C MACH LEAR, P1857, DOI [10.5555/3305381.3305573, DOI 10.5555/3305381.3305573]; Kingma D.P, P 3 INT C LEARNING R; Kuyper Tony, LUMINOSITY MASKS; Li J, 2013, OPTIK, V124, P5986, DOI 10.1016/j.ijleo.2013.04.115; Li YJ, 2018, LECT NOTES COMPUT SC, V11207, P468, DOI 10.1007/978-3-030-01219-9_28; Li ZG, 2017, IEEE T IMAGE PROCESS, V26, P1243, DOI 10.1109/TIP.2017.2651366; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Mertens T, 2009, COMPUT GRAPH FORUM, V28, P161, DOI 10.1111/j.1467-8659.2008.01171.x; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Park J, 2018, PROC CVPR IEEE, P5928, DOI 10.1109/CVPR.2018.00621; Pathak D, 2017, PR MACH LEARN RES, V70; Silver D, 2014, PR MACH LEARN RES, V32; Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899; TESAURO G, 1995, COMMUN ACM, V38, P58, DOI 10.1145/203330.203343; Uhlenbeck GE, 1930, PHYS REV, V36, P0823, DOI 10.1103/PhysRev.36.823; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu HK, 2018, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2018.00197; Yan Jianzhou, 2017, COMPUTER VISION PATT; Yang Huan, 2018, ARXIV180302269; Yang X, 2018, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2018.00193; Yuan L, 2012, LECT NOTES COMPUT SC, V7575, P771, DOI 10.1007/978-3-642-33765-9_55; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	41	18	19	3	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302018
C	Altschuler, J; Weed, J; Rigollet, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Altschuler, Jason; Weed, Jonathan; Rigollet, Philippe			Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				COMPLEXITY; DISTANCE	Computing optimal transport distances such as the earth mover's distance is a fundamental problem in machine learning, statistics, and computer vision. Despite the recent introduction of several algorithms with good empirical performance, it is unknown whether general optimal transport distances can be approximated in near-linear time. This paper demonstrates that this ambitious goal is in fact achieved by Cuturi's Sinkhorn Distances. This result relies on a new analysis of Sinkhorn iterations, which also directly suggests a new greedy coordinate descent algorithm GREENKHORN with the same theoretical guarantees. Numerical simulations illustrate that GREENKHORN significantly outperforms the classical S INKHORN algorithm in practice.	[Altschuler, Jason; Weed, Jonathan; Rigollet, Philippe] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Altschuler, J (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	jasonalt@mit.edu; jweed@mit.edu; rigollet@mit.edu	Jeong, Yongwook/N-7413-2016		NSF Graduate Research Fellowship [1122374]; NSF [DMS-1541099, DMS-1541100, DMS-1712596]; DARPA [W911NF-16-1-0551]; ONR [N00014-17-1-2147]; MIT NEC Corporation	NSF Graduate Research Fellowship(National Science Foundation (NSF)); NSF(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); ONR(Office of Naval Research); MIT NEC Corporation	JA and JW were generously supported by NSF Graduate Research Fellowship 1122374. PR is supported in part by grants NSF CAREER DMS-1541099, NSF DMS-1541100, NSF DMS-1712596, DARPA W911NF-16-1-0551, ONR N00014-17-1-2147 and a grant from the MIT NEC Corporation.	Agarwal PK, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P555, DOI 10.1145/2591796.2591844; Allen-Zhu Z, 2017, ARXIV170402315; Andoni A, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P574, DOI 10.1145/2591796.2591805; [Anonymous], 2015, FOUND TRENDS MACH LE, V8, P232, DOI 10.1561/2200000050; Benamou JD, 2015, SIAM J SCI COMPUT, V37, pA1111, DOI 10.1137/141000439; Bigot J, 2017, ANN I H POINCARE-PR, V53, P1, DOI 10.1214/15-AIHP706; Bonneel N, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024192; Bottou L., 2017, ARXIV170107875STATML; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Cohen M. B, 2017, ARXIV170402310; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Flamary R., 2016, ARXIV160808063; Genevay A., 2016, P NEUR INF PROC SYST, P3440; Gramfort A., 2015, FAST OPTIMAL TRANSPO, P261; Grauman K, 2004, PROC CVPR IEEE, P220; Gurvits Leonid, 1998, TECHNICAL REPORT; Indyk P., 2003, 3 INT WORKSH STAT CO; Jitkrittum W., 2016, ADV NEURAL INFORM PR, P181; Juditsky A, 2008, ANN STAT, V36, P2183, DOI 10.1214/07-AOS546; Kalantari B, 2008, MATH PROGRAM, V112, P371, DOI 10.1007/s10107-006-0021-4; KALANTARI B, 1993, OPER RES LETT, V14, P237, DOI 10.1016/0167-6377(93)90087-W; Kalantari B, 1996, LINEAR ALGEBRA APPL, V240, P87, DOI 10.1016/0024-3795(94)00188-X; Lee YT, 2014, ANN IEEE SYMP FOUND, P424, DOI 10.1109/FOCS.2014.52; Leonard C, 2014, DISCRETE CONT DYN-A, V34, P1533, DOI 10.3934/dcds.2014.34.1533; Mueller J. W., 2015, ADV NEURAL INFORM PR, P1702; Panaretos VM, 2016, ANN STAT, V44, P771, DOI 10.1214/15-AOS1387; Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199; RENEGAR J, 1988, MATH PROGRAM, V40, P59, DOI 10.1007/BF01580724; Rigollet P, 2011, ANN STAT, V39, P731, DOI 10.1214/10-AOS854; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Sandler R, 2011, IEEE T PATTERN ANAL, V33, P1590, DOI 10.1109/TPAMI.2011.18; Schrodinger E., 1931, ANGEW CHEM, V44, P636; Sharathkumar R., 2012, P 44 S THEOR COMP C, P385; Shirk Susan L., 2008, CHINA FRAGILE SUPERP, P1, DOI 10.1109/CVPR.2008.4587662; SINKHORN R, 1967, AM MATH MON, V74, P402, DOI 10.2307/2314570; Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963; Spielman D., 2003, C P ANN ACM S THEOR, DOI 10.1145/1007352.1007372; Szekely G.J., 2004, INTERSTAT, V5; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; WERMAN M, 1985, COMPUT VISION GRAPH, V32, P328, DOI 10.1016/0734-189X(85)90055-6	42	18	18	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402002
C	Barreto, A; Dabney, W; Munos, R; Hunt, JJ; Schaul, T; van Hasselt, H; Silver, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Barreto, Andre; Dabney, Will; Munos, Remi; Hunt, Jonathan J.; Schaul, Tom; van Hasselt, Hado; Silver, David			Successor Features for Transfer in Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks. We propose a transfer framework for the scenario where the reward function changes between tasks but the environment's dynamics remain the same. Our approach rests on two key ideas: successor features, a value function representation that decouples the dynamics of the environment from the rewards, and generalized policy improvement, a generalization of dynamic programming's policy improvement operation that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows the free exchange of information across tasks. The proposed method also provides performance guarantees for the transferred policy even before any learning has taken place. We derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice, significantly outperforming alternative methods in a sequence of navigation tasks and in the control of a simulated robotic arm.	[Barreto, Andre; Dabney, Will; Munos, Remi; Hunt, Jonathan J.; Schaul, Tom; van Hasselt, Hado; Silver, David] DeepMind, London, England		Barreto, A (corresponding author), DeepMind, London, England.	andrebarreto@google.com; wdabney@google.com; munos@google.com; jjhunt@google.com; schaul@google.com; hado@google.com; davidsilver@google.com	Jeong, Yongwook/N-7413-2016; Barreto, André M S/J-5063-2013					Barto AG, 2003, DISCRETE EVENT DYN S, V13, P343; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Bellman RE, 1957, DYNAMIC PROGRAMMING; Bernstein Daniel S., 1999, TECHNICAL REPORT; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; DAYAN P, 1993, NEURAL COMPUT, V5, P613, DOI 10.1162/neco.1993.5.4.613; Fernandez F, 2010, ROBOT AUTON SYST, V58, P866, DOI 10.1016/j.robot.2010.03.007; Hastie T., 2002, ELEMENTS STAT LEARNI; Kulkarni T. D., 2016, ARXIV160602396; Lazaric A, 2012, ADAPT LEARN OPTIM, V12, P143; Lillicrap TP, 2016, 4 INT C LEARN REPR; Mehta Neville, 2008, MACHINE LEARNING, V73; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Modayil J, 2014, ADAPT BEHAV, V22, P146, DOI 10.1177/1059712313511648; Natarajan S., 2005, P 22 INT C MACH LEAR, P601; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Singh S, 2001, ADV NEURAL INFORM PR, P1555; SUTTON R S, 2011, 10 INT C AUTONOMOUS, P761, DOI DOI 10.5555/2031678.2031726; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Szepesvari C., 2010, SYNTHESIS LECT ARTIF; Taylor ME, 2009, J MACH LEARN RES, V10, P1633; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Yao H., 2014, ADV NEURAL INFORM PR, P990; Zhang J., 2016, CORR	29	18	18	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404013
C	Cheng, CA; Boots, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Cheng, Ching-An; Boots, Byron			Variational Inference for Gaussian Process Models with Linear Complexity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Large-scale Gaussian process inference has long faced practical challenges due to time and space complexity that is superlinear in dataset size. While sparse variational Gaussian process models are capable of learning from large-scale data, standard strategies for sparsifying the model can prevent the approximation of complex functions. In this work, we propose a novel variational Gaussian process model that decouples the representation of mean and covariance functions in reproducing kernel Hilbert space. We show that this new parametrization generalizes previous models. Furthermore, it yields a variational inference problem that can be solved by stochastic gradient ascent with time and space complexity that is only linear in the number of mean function parameters, regardless of the choice of kernels, likelihoods, and inducing points. This strategy makes the adoption of large-scale expressive Gaussian process models possible. We run several experiments on regression tasks and show that this decoupled approach greatly outperforms previous sparse variational Gaussian process inference procedures.	[Cheng, Ching-An; Boots, Byron] Georgia Inst Technol, Inst Robot & Intelligent Machines, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Cheng, CA (corresponding author), Georgia Inst Technol, Inst Robot & Intelligent Machines, Atlanta, GA 30332 USA.	cacheng@gatech.edu; bboots@cc.gatech.edu	Jeong, Yongwook/N-7413-2016; Cheng, Ching-An/AAZ-1802-2020	Cheng, Ching-An/0000-0002-0610-2070	NSF NRI [1637758]	NSF NRI(National Science Foundation (NSF))	This work was supported in part by NSF NRI award 1637758. The authors additionally thank the reviewers and Hugh Salimbeni for productive discussion which improved the quality of the paper.	Bauer M., 2016, ADV NEURAL INFORM PR, V29, P1533, DOI DOI 10.5555/3157096.3157268; Cheng CA, 2016, IEEE T CYBERNETICS, V46, P3247, DOI 10.1109/TCYB.2015.2501842; Cheng Ching- An, 2016, ADV NEURAL INFORM PR, P4403; Csato L, 2001, ADV NEUR IN, V13, P444; Dai B., 2014, NIPS; de Alexander G., 2016, P 19 INT C ART INT S; Eldredge Nathaniel, 2016, ARXIV160703591; Germain P., 2016, ADV NEURAL INFORM PR, P1884; Gross L., 1967, P 5 BERK S MATH ST 1, VII, P31, DOI [10.1143/PTPS.111.43, DOI 10.1143/PTPS.111.43]; Hensman J., 2013, P C UNC ART INT UAI, P282; Hensman J., 2016, ARXIV161106740; Hensman J., 2015, INT C ART INT STAT; Kingma D.P, P 3 INT C LEARNING R; Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991; Lawrence N., 2003, ARTIFICIAL INTELLIGE, V9; Lazaro-Gredilla M., 2009, ADV NEURAL INFORM PR, V22, P1087; Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865; Meier F, 2014, ADV NEURAL INFORM PR, P972; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Sheth R., 2015, P INT C MACH LEARN, P1302; Snelson E., 2005, ADV NEURAL INFORM PR, P1257; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Walder C., 2008, ICML, P1112; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775	27	18	18	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405026
C	Kosiorek, AR; Bewley, A; Posner, I		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kosiorek, Adam R.; Bewley, Alex; Posner, Ingmar			Hierarchical Attentive Recurrent Tracking	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Class-agnostic object tracking is particularly difficult in cluttered environments as target specific discriminative models cannot be learned a priori. Inspired by how the human visual cortex employs spatial attention and separate "where" and "what" processing pathways to actively suppress irrelevant visual features, this work develops a hierarchical attentive recurrent model for single object tracking in videos. The first layer of attention discards the majority of background by selecting a region containing the object of interest, while the subsequent layers tune in on visual features particular to the tracked object. This framework is fully differentiable and can be trained in a purely data driven fashion by gradient methods. To improve training convergence, we augment the loss function with terms for auxiliary tasks relevant for tracking. Evaluation of the proposed model is performed on two datasets: pedestrian tracking on the KTH activity recognition dataset and the more difficult KITTI object tracking dataset.	[Kosiorek, Adam R.; Bewley, Alex; Posner, Ingmar] Univ Oxford, Dept Engn Sci, Oxford, England	University of Oxford	Kosiorek, AR (corresponding author), Univ Oxford, Dept Engn Sci, Oxford, England.	adamk@robots.ox.ac.uk; bewley@robots.ox.ac.uk; ingmar@robots.ox.ac.uk	Jeong, Yongwook/N-7413-2016		UK's Engineering and Physical Sciences Research Council (EPSRC) [EP/M019918/1]; Doctoral Training Award (DTA)	UK's Engineering and Physical Sciences Research Council (EPSRC)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Doctoral Training Award (DTA)	We would like to thank Oiwi Parker Jones and Martin Engelcke for discussions and valuable insights and Neil Dhir for his help with editing the paper. Additionally, we would like to acknowledge the support of the UK's Engineering and Physical Sciences Research Council (EPSRC) through the Programme Grant EP/M019918/1 and the Doctoral Training Award (DTA). The donation from Nvidia of the Titan Xp GPU used in this work is also gratefully acknowledged.	[Anonymous], 2015, P 28 INT C NEUR INF; [Anonymous], 2009, ICML; Cheung Brian, 2017, ICLR; Cheung Brian, 2016, GPU TECHN C; Dayan P, 2001, THEORETICAL NEUROSCI; Eslami S. M. A., 2016, NIPS 2016, P3; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Geoffrey Hinton, 2012, OVERVIEW MINIBATCH G; Gordon D., 2017, ARXIV170506368; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Gregor K., 2015, ICML; Held David, 2016, ECCV WORK; JADERBERG M, 2016, 4 INT C LEARN REPR I, P1; Jia X., 2016, NIPS; Kahou Samira Ebrahimi, 2017, CVPR WORK; Karl Maximilian, 2017, ICLR; Kastner S, 2000, ANNU REV NEUROSCI, V23, P315, DOI 10.1146/annurev.neuro.23.1.315; Kendall A, 2017, ARXIV170507115; Kristan Matej, 2016, ECCV WORK; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Krueger D., 2017, ICLR; Mnih V., 2014, NIPS; Ning G., 2016, ARXIV160705781; Schuldt C, 2004, ICPR; Stollenga Marijn, 2014, ARXIV PREPR ARXIV, P13; Valmadre J, 2017, CVPR; Vinyals O., 2015, NIPS; Yu Jiahui., 2016, ACM MM, DOI DOI 10.1145/2964284.2967274	28	18	18	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403012
C	Parra, G; Tobar, F		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Parra, Gabriel; Tobar, Felipe			Spectral Mixture Kernels for Multi-Output Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Early approaches to multiple-output Gaussian processes (MOGPs) relied on linear combinations of independent, latent, single-output Gaussian processes (GPs). This resulted in cross-covariance functions with limited parametric interpretation, thus conflicting with the ability of single-output GPs to understand lengthscales, frequencies and magnitudes to name a few. On the contrary, current approaches to MOGP are able to better interpret the relationship between different channels by directly modelling the cross-covariances as a spectral mixture kernel with a phase shift. We extend this rationale and propose a parametric family of complex-valued cross-spectral densities and then build on Cramer's Theorem (the multivariate version of Bochner's Theorem) to provide a principled approach to design multivariate covariance functions. The so-constructed kernels are able to model delays among channels in addition to phase differences and are thus more expressive than previous methods, while also providing full parametric interpretation of the relationship across channels. The proposed method is first validated on synthetic data and then compared to existing MOGP methods on two real-world examples.	[Parra, Gabriel] Univ Chile, Dept Math Engn, Santiago, Chile; [Tobar, Felipe] Univ Chile, Ctr Math Modeling, Santiago, Chile	Universidad de Chile; Universidad de Chile	Parra, G (corresponding author), Univ Chile, Dept Math Engn, Santiago, Chile.	gparra@dim.uchile.cl; ftobar@dim.uchile.cl	Jeong, Yongwook/N-7413-2016		Conicyt Basal-CMM	Conicyt Basal-CMM	We thank Cristobal Silva (Universidad de Chile) for useful recommendations about GPU implementation, Rasmus Bonnevie from the GPflow team for his assistance on the experimental MOGP module within GPflow, and the anonymous reviewers. This work was financially supported by Conicyt Basal-CMM.	Abadi M, 2015, P 12 USENIX S OPERAT; Alvarez M. A., 2010, P 13 INT C ART INT S, P25; Alvarez M.A., 2008, NEURAL INFORM PROCES, P57; Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; Bochner S., 1959, ANN MATH STUDIES; Boloix-Tortosa R, 2014, PR IEEE SEN ARRAY, P137, DOI 10.1109/SAM.2014.6882359; Cramer H, 1940, ANN MATH, V41, P215, DOI 10.2307/1968827; Duvenaud DK, 2014, THESIS U CAMBRIDGE C, DOI [10.17863/CAM.14087, DOI 10.17863/CAM.14087]; Genton M. G., 2015, I MATH STAT, V30; Gneiting T, 2010, J AM STAT ASSOC, V105, P1167, DOI 10.1198/jasa.2010.tm09420; Goovaerts P, 1997, GEOSTATISTICS NATURA, DOI DOI 10.2307/1270969; Hensman J., 2016, ARXIV161106740; Kay S.M, 1988, MODERN SPECTRAL ESTI; Matthews A. G. d. G., 2016, GPFLOW GAUSSIAN PROC; PRATT J. W., 2012, SPRINGER SERIES STAT; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Tobar F., 2016, P IEEE SAM, P2209; Tobar F., 2015, ADV NEURAL INFORM PR, P3501; Tobar F., 2015, NIPS 2015 TIM SER WO; Tobar F, 2015, INT CONF ACOUST SPEE, P2209, DOI 10.1109/ICASSP.2015.7178363; Ulrich K.R., 2015, ADV NEURAL INFORM PR, P1999; Wilson A., 2013, INT C MACH LEARN, P1067; Yaglom A.M., 1987, CORRELATION THEORY S	23	18	19	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406072
C	Dixit, M; Vasconcelos, N		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Dixit, Mandar; Vasconcelos, Nuno			Object based Scene Representations using Fisher Scores of Local Subspace Projections	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MODELS	Several works have shown that deep CNNs can be easily transferred across datasets, e.g. the transfer from object recognition on ImageNet to object detection on Pascal VOC. Less clear, however, is the ability of CNNs to transfer knowledge across tasks. A common example of such transfer is the problem of scene classification, that should leverage localized object detections to recognize holistic visual concepts. While this problems is currently addressed with Fisher vector representations, these are now shown ineffective for the high-dimensional and highly non-linear features extracted by modern CNNs. It is argued that this is mostly due to the reliance on a model, the Gaussian mixture of diagonal covariances, which has a very limited ability to capture the second order statistics of CNN features. This problem is addressed by the adoption of a better model, the mixture of factor analyzers (MFA), which approximates the non-linear data manifold by a collection of local sub-spaces. The Fisher score with respect to the MFA (MFA-FS) is derived and proposed as an image representation for holistic image classifiers. Extensive experiments show that the MFA-FS has state of the art performance for object-to-scene transfer and this transfer actually outperforms the training of a scene CNN from a large scene dataset. The two representations are also shown to be complementary, in the sense that their combination outperforms each of the representations by itself. When combined, they produce a state-of-the-art scene classifier.	[Dixit, Mandar; Vasconcelos, Nuno] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Dixit, M (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.	mdixit@ucsd.edu; nvasconcelos@ucsd.edu		Vasconcelos, Nuno/0000-0002-9024-4302				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Cimpoi M., 2015, INT J COMPUTER VISIO; Dixit M., 2015, IEEE C COMP VIS PATT; Gao Y., 2015, CORR; Ghahramani Z., 1997, TECHNICAL REPORT; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26; Jaakkola TS, 1999, ADV NEUR IN, V11, P487; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Y, 2015, PROC CVPR IEEE, P287, DOI 10.1109/CVPR.2015.7298625; Lin Aruni RoyChowdhury Tsung-Yu, 2015, INT C COMP VIS ICCV; Liu L., 2014, P ADV NEUR INF PROC, V27, P1143; Liu Lingqiao, 2016, CORR; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x; Tanaka Masayuki, 2013, IPSJ T COMPUTER VISI, V5, P50, DOI 10.2197/ipsjtcva.5.50; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Verbeek J, 2006, IEEE T PATTERN ANAL, V28, P1236, DOI 10.1109/TPAMI.2006.166; Vinyals Oriol, 2014, CORR; Wu R., 2015, IEEE INT C COMP VIS; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	26	18	18	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702090
C	Iwata, T; Yamada, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Iwata, Tomoharu; Yamada, Makoto			Multi-view Anomaly Detection via Robust Probabilistic Latent Variable Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose probabilistic latent variable models for multi-view anomaly detection, which is the task of finding instances that have inconsistent views given multi-view data. With the proposed model, all views of a non-anomalous instance are assumed to be generated from a single latent vector. On the other hand, an anomalous instance is assumed to have multiple latent vectors, and its different views are generated from different latent vectors. By inferring the number of latent vectors used for each instance with Dirichlet process priors, we obtain multiview anomaly scores. The proposed model can be seen as a robust extension of probabilistic canonical correlation analysis for noisy multi-view data. We present Bayesian inference procedures for the proposed model based on a stochastic EM algorithm. The effectiveness of the proposed model is demonstrated in terms of performance when detecting multi-view anomalies.	[Iwata, Tomoharu] NTT Commun Sci Labs, Kyoto, Japan; [Yamada, Makoto] Kyoto Univ, Kyoto, Japan	Nippon Telegraph & Telephone Corporation; Kyoto University	Iwata, T (corresponding author), NTT Commun Sci Labs, Kyoto, Japan.	iwata.tomoharu@lab.ntt.co.jp; makoto.m.yamada@ieee.org			KAKENHI [16K16114]	KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	MY was supported by KAKENHI 16K16114.	Aleskerov E., 1997, Proceedings of the IEEE/IAFE 1997 Computational Intelligence for Financial Engineering (CIFEr) (Cat. No.97TH8304), P220, DOI 10.1109/CIFER.1997.618940; Alvarez A. M., 2013, P ACM INT C INF KNOW; Antonie Maria-Luiza, 2001, P 2 INT WORKSH MULT, P94; Archambeau C., 2006, P 23 INT C MACH LEAR, P33, DOI DOI 10.1145/1143844.1143849; Bach F. R., 2005, 688 U CAL DEP STAT; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Christoudias C. M., 2008, P 24 C UNV ART INT U; Ek CH, 2008, LECT NOTES COMPUT SC, V5237, P62, DOI 10.1007/978-3-540-85853-9_6; Fanaee-T H, 2016, KNOWL-BASED SYST, V98, P130, DOI 10.1016/j.knosys.2016.01.027; Herlocker JL, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P230, DOI 10.1145/312624.312682; Kevin Duh, 2013, ACM T SPEECH LANG PR, V10, P1; Lawrence ND, 2004, ADV NEUR IN, V16, P329; Liu A. Y., 2012, 2012 IEEE CS Security and Privacy Workshops (SPW 2012), P117, DOI 10.1109/SPW.2012.18; Portnoy L., 2001, P ACM CSS WORKSH DAT, P5; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; SETHURAMAN J, 1994, STAT SINICA, V4, P639; Shekhar S., 2002, Intelligent Data Analysis, V6, P451; Song XY, 2007, IEEE T KNOWL DATA EN, V19, P631, DOI 10.1109/TKDE.2007.1009; Sun JM, 2005, FIFTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P418; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; Wang X, 2009, IEEE DATA MINING, P1034, DOI 10.1109/ICDM.2009.95; Wu G, 2011, IEEE INT C CL COMP, P503, DOI 10.1109/CLUSTER.2011.62	24	18	19	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703092
C	Lapin, M; Hein, M; Schiele, B		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Lapin, Maksim; Hein, Matthias; Schiele, Bernt			Top-k Multiclass SVM	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Class ambiguity is typical in image classification problems with a large number of classes. When classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss. We propose top-k multiclass SVM as a direct method to optimize for top-k performance. Our generalization of the well-known multiclass SVM is based on a tight convex upper bound of the top-k error. We propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest. Experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines.	[Lapin, Maksim; Schiele, Bernt] Max Planck Inst Informat, Saarbrucken, Germany; [Hein, Matthias] Saarland Univ, Saarbrucken, Germany	Max Planck Society; Saarland University	Lapin, M (corresponding author), Max Planck Inst Informat, Saarbrucken, Germany.							[Anonymous], 2007, INT C MACH LEARN; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Boyd S, 2004, CONVEX OPTIMIZATION; Bu S., 2013, ACM INT C MULT, P681; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Doersch Carl, 2013, NIPS; Gong Y., 2014, ECCV; Gupta MR, 2014, J MACH LEARN RES, V15, P1461; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Joachims Thorsten, 2005, ICML, DOI DOI 10.1145/1102351.1102399; Juneja M., 2013, CVPR; Kiwiel KC, 2008, J OPTIMIZ THEORY APP, V136, P445, DOI 10.1007/s10957-007-9317-7; Koskela M, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1169, DOI 10.1145/2647868.2655024; Lapin M., 2014, CVPR; Li N., 2014, P NIPS, P1502; Ogryczak W, 2003, INFORM PROCESS LETT, V85, P117, DOI 10.1016/S0020-0190(02)00370-8; Patriksson M, 2008, EUR J OPER RES, V185, P1, DOI 10.1016/j.ejor.2006.12.006; Patriksson M, 2015, EUR J OPER RES, V243, P703, DOI 10.1016/j.ejor.2015.01.029; Quattoni A., 2009, CVPR; Razavian A- S., 2014, CORR, V1403, P6382; Russakovsky O., 2014, IMAGENET LARGE SCALE; Sanchez Garcia J. A., 2013, Insecta Mundi, P1; Shalev-Shwartz S, 2016, MATH PROGRAM, V155, P105, DOI 10.1007/s10107-014-0839-0; Sun J, 2013, IEEE I CONF COMP VIS, P3400, DOI 10.1109/ICCV.2013.422; Swersky K., 2012, NIPS, P3050; Usunier Nicolas, 2009, ICML; Weston Jason, 2011, 22 INT JOINT C ART I; Xiao J., 2010, CVPR; Zhou B., 2014, NIPS	30	18	18	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101002
C	Rothenhausler, D; Heinze, C; Peters, J; Meinshausen, N		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Rothenhausler, Dominik; Heinze, Christina; Peters, Jonas; Meinshausen, Nicolai			BACKSHIFT: Learning causal cyclic graphs from unknown shift interventions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MODELS	We propose a simple method to learn linear causal cyclic models in the presence of latent variables. The method relies on equilibrium data of the model recorded under a specific kind of interventions ("shift interventions"). The location and strength of these interventions do not have to be known and can be estimated from the data. Our method, called BACKSHIFT, only uses second moments of the data and performs simple joint matrix diagonalization, applied to differences between covariance matrices. We give a sufficient and necessary condition for identifiability of the system, which is fulfilled almost surely under some quite general assumptions if and only if there are at least three distinct experimental settings, one of which can be pure observational data. We demonstrate the performance on some simulated data and applications in flow cytometry and financial time series.	[Rothenhausler, Dominik; Heinze, Christina; Meinshausen, Nicolai] Swiss Fed Inst Technol, Seminar Stat, Zurich, Switzerland; [Peters, Jonas] Max Planck Inst Intelligent Syst, Tubingen, Germany	Swiss Federal Institutes of Technology Domain; ETH Zurich; Max Planck Society	Rothenhausler, D (corresponding author), Swiss Fed Inst Technol, Seminar Stat, Zurich, Switzerland.	rothenhaeusler@stat.math.ethz.ch; heinze@stat.math.ethz.ch; jonas.peters@tuebingen.mpg.de; meinshausen@stat.math.ethz.ch		Peters, Jonas/0000-0002-1487-7511				Bollen K. A, 1989, STRUCTURAL EQUATIONS; Burkard R. E., 2013, HDB COMBINATORIAL OP, P2741, DOI DOI 10.1007/978-1-4419-7997-1_22; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; Eaton D, 2007, P MACHINE LEARNING R, V2, P107; Eberhardt F., 2010, J MACHINE LEARNING W, P185; Eberhardt F, 2007, PHILOS SCI, V74, P981, DOI 10.1086/525638; Hauser A, 2012, J MACH LEARN RES, V13, P2409; Hoyer P.O., 2009, ADV NEURAL INFORM PR, P689; Hyttinen A, 2012, J MACH LEARN RES, V13, P3387; Jackson AL, 2003, NAT BIOTECHNOL, V21, P635, DOI 10.1038/nbt831; Korb KB, 2004, LECT NOTES ARTIF INT, V3157, P322; Kulkarni MM, 2006, NAT METHODS, V3, P833, DOI 10.1038/nmeth935; Lacerda G, 2008, P 24 C UNC ART INT U; Maathuis MH, 2009, ANN STAT, V37, P3133, DOI 10.1214/09-AOS685; Meinshausen N, 2010, J R STAT SOC B, V72, P417, DOI 10.1111/j.1467-9868.2010.00740.x; Mooij J. M., 2013, P 29 ANN C UNC ART I, P431; Pearl J., 2009, CAUSALITY MODELS REA; Peters J., 2015, J ROYAL STAT SOC B; Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809; Scholkopf B., 2011, P ADV NEURAL INF PRO, P639; Shimizu S, 2011, J MACH LEARN RES, V12, P1225; Spirtes P., 2000, CAUSATION PREDICTION; Tian J., 2001, P 17 C UNC ART INT, P512; Ziehe A, 2004, J MACH LEARN RES, V5, P777	25	18	18	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100050
C	Hardt, M; Price, E		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Hardt, Moritz; Price, Eric			The Noisy Power Method: A Meta Algorithm with Applications	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We provide a new robust convergence analysis of the well-known power method for computing the dominant singular vectors of a matrix that we call the noisy power method. Our result characterizes the convergence behavior of the algorithm when a significant amount noise is introduced after each matrix-vector multiplication. The noisy power method can be seen as a meta-algorithm that has recently found a number of important applications in a broad range of machine learning problems including alternating minimization for matrix completion, streaming principal component analysis (PCA), and privacy-preserving spectral analysis. Our general analysis subsumes several existing ad-hoc convergence bounds and resolves a number of open problems in multiple applications: Streaming PCA. A recent work of Mitliagkas et al. (NIPS 2013) gives a space-efficient algorithm for PCA in a streaming model where samples are drawn from a gaussian spiked covariance model. We give a simpler and more general analysis that applies to arbitrary distributions confirming experimental evidence of Mitliagkas et al. Moreover, even in the spiked covariance model our result gives quantitative improvements in a natural parameter regime. It is also notably simpler and follows easily from our general convergence analysis of the noisy power method together with a matrix Chernoff bound. Private PCA. We provide the first nearly-linear time algorithm for the problem of differentially private principal component analysis that achieves nearly tight worst-case error bounds. Complementing our worst-case bounds, we show that the error dependence of our algorithm on the matrix dimension can be replaced by an essentially tight dependence on the coherence of the matrix. This result resolves the main problem left open by Hardt and Roth (STOC 2013). The coherence is always bounded by the matrix dimension but often substantially smaller thus leading to strong average-case improvements over the optimal worst-case bound.	[Hardt, Moritz; Price, Eric] IBM Res Almaden, San Jose, CA 95120 USA	International Business Machines (IBM)	Hardt, M (corresponding author), IBM Res Almaden, San Jose, CA 95120 USA.	mhardt@us.ibm.com; ecprice@cs.utexas.edu						Arora R, 2012, ANN ALLERTON CONF, P861, DOI 10.1109/Allerton.2012.6483308; BALSUBRAMANI A., 2013, ADV NEURAL INFORM PR, V26, P3174, DOI 10.1016/j.compbiomed.2021.104502; Blum A., 2005, P 24 ACM SIGMOD SIGA, P128, DOI DOI 10.1145/1065167.1065184; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chaudhuri Kamalika, 2012, P 26 NEUR INF PROC S; DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001; Hardt M, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P1255; Hardt Moritz, 2014, P 55 FDN COMP SCI FO; Hardt Moritz, 2013, P 45 S THEOR COMP ST; Higham N. J., 2002, ACCURACY STABILITY N; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Kapralov Michael, 2013, P 24 S DISCR ALG SOD; McSherry F, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P627; Mitliagkas Ioannis, 2013, ADV NEURAL INFORM PR, P2886; Rudelson M, 2009, COMMUN PUR APPL MATH, V62, P1707, DOI 10.1002/cpa.20294; Simoncini V, 2007, NUMER LINEAR ALGEBR, V14, P1, DOI 10.1002/nla.499	18	18	18	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100105
C	Neill, DB; Moore, AW		Thrun, S; Saul, K; Scholkopf, B		Neill, DB; Moore, AW			A fast multi-resolution method for detection of significant spatial disease clusters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Given an N x N grid of squares, where each square has a count and an underlying population, our goal is to find the square region with the highest density, and to calculate its significance by randomization. Any density measure D, dependent on the total count and total population of a region, can be used. For example, if each count represents the number of disease cases occurring in that square, we can use Kulldorff's spatial scan. statistic D-K to find the most significant spatial disease cluster. A naive approach to finding the maximum density region requires O(N-3) time, and is generally computationally infeasible. We present a novel algorithm which partitions the grid into overlapping regions, bounds the maximum score of subregions contained in each region, and prunes regions which cannot contain the maximum density region. For sufficiently dense regions, this method finds the maximum density region in optimal O(N-2) time, in practice resulting in significant (10-200x) speedups.	Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Neill, DB (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.	neill@cs.cmu.edu; awm@cs.cmu.edu						DENG K, 1995, P 12 INT JOINT C ART, P1233; Goil S., 1999, CPDCTR9906010 NW U; Kulldorff M, 1999, STAT IND TECHNOL, P303; Kulldorff M, 1997, COMMUN STAT-THEOR M, V26, P1481, DOI 10.1080/03610929708831995; KULLDORFF M, 1995, STAT MED, V14, P799, DOI 10.1002/sim.4780140809; OPENSHAW S, 1988, LANCET, V1, P272; Prabhakar S, 1998, PROC INT CONF DATA, P94, DOI 10.1109/ICDE.1998.655763; Preparata F.P., 1985, COMPUTATIONAL GEOMET, V1; Samet H., 1990, DESIGN ANAL SPATIAL, V85; Waller LA, 1994, WILEY S PRO, P3; Wang W, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P186	11	18	18	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						651	658						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500082
C	Collobert, R; Bengio, S; Bengio, Y		Dietterich, TG; Becker, S; Ghahramani, Z		Collobert, R; Bengio, S; Bengio, Y			A parallel mixture of SVMs for very large scale problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Support Vector Machines (SVMs) are currently the state-of-the-art models for many classification problems but they suffer from the complexity of their training algorithm which is at least quadratic with respect to the number of examples. Hence, it is hopeless to try to solve real-life problems having more than a few hundreds of thousands examples with SVMs. The present paper proposes a new mixture of SVMs that can be easily implemented in parallel and where each SVM is trained on a small subset of the whole dataset. Experiments on a large benchmark dataset (Forest) as well as a difficult speech database, yielded significant time improvement (time complexity appears empirically to locally grow linearly with the number of examples). In addition, and that is a surprise, a significant improvement in generalization was observed on Forest.	Univ Montreal, DIRO, Montreal, PQ, Canada	Universite de Montreal	Collobert, R (corresponding author), Univ Montreal, DIRO, CP 6128,Succ Ctr Ville, Montreal, PQ, Canada.	collober@iro.umontreal.ca; bengio@idiap.ch						Collobert R, 2001, J MACH LEARN RES, V1, P143, DOI 10.1162/15324430152733142; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; Kwok JTY, 1998, INT C PATT RECOG, P255, DOI 10.1109/ICPR.1998.711129; Michaelis D, 1995, 4 EUROPEAN C SPEECH; RIDA A, 1999, INT WORKSH AI STAT U; Tresp V, 2000, NEURAL COMPUT, V12, P2719, DOI 10.1162/089976600300014908; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	7	18	19	0	5	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						633	640						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100079
C	Rosales, R; Sclaroff, S		Dietterich, TG; Becker, S; Ghahramani, Z		Rosales, R; Sclaroff, S			Learning body pose via specialized maps	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					A nonlinear supervised learning model, the Specialized Mappings Architecture (SMA), is described and applied to the estimation of human body pose from monocular images. The SMA consists of several specialized forward mapping functions and an inverse mapping function. Each specialized function maps certain domains of the input space (image features) onto the output space (body pose parameters). The key algorithmic problems faced are those of learning the specialized domains and mapping functions in an optimal way, as well as performing inference given inputs and knowledge of the inverse function. Solutions to these problems employ the EM algorithm and alternating choices of conditional independence assumptions. Performance of the approach is evaluated with synthetic and real video sequences of human motion.	Boston Univ, Dept Comp Sci, Boston, MA 02215 USA	Boston University	Rosales, R (corresponding author), Boston Univ, Dept Comp Sci, 111 Cummington St, Boston, MA 02215 USA.							BRAND M, 1999, ICCV; Bregler Christoph, 1998, CVPR; Csiszar I., 1984, STAT DECISIONS, V1, P205; DEMPSTER AP, 1977, J ROYAL STAT SOC B, V39; Deutsher J., 2000, CVPR; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; HINTON G, 1998, LEARNING GRAPICAL MO; HOWE N, 2000, NIP 12; Isard M., 1996, ECCV; JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P210; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; Neal R. M., 1998, LEARNING GRAPHICAL M; ORMONEIT D, 2001, NIPS 13; PAVLOVIC V, 2001, NIPS 13; REGH JM, 1995, ICCV; ROSALES R, 2000, IEEE HUM MOT WORKSH; Song Y., 2000, CVPR	17	18	18	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1263	1270						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100157
C	Graepel, T; Herbrich, R; Williamson, RC		Leen, TK; Dietterich, TG; Tresp, V		Graepel, T; Herbrich, R; Williamson, RC			From margin to sparsity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We present an improvement of Novikoff's perceptron convergence theorem. Reinterpreting this mistake bound as a margin dependent sparsity guarantee allows us to give a PAC-style generalisation error bound for the classifier learned by the perceptron learning algorithm. The bound value crucially depends on the margin a support vector machine would achieve on the same data set using the same kernel. Ironically, the bound yields better guarantees than are currently available for the support vector solution itself.	Tech Univ Berlin, Dept Comp Sci, Berlin, Germany	Technical University of Berlin	Graepel, T (corresponding author), Tech Univ Berlin, Dept Comp Sci, Berlin, Germany.							Aizerman M. A., 1964, AUTOMAT REM CONTR, V25, P821, DOI DOI 10.1234/12345678; FREUND Y, 1999, LARGE MARGIN CLASSIF; FRIESS T, 1998, P 15 INT C MACH LEAR, P188; GRAEPEL T, 2000, IN PRESS P 13 ANN C, P298; HERBRICH R, 2001, ADV NEURAL INFORMATI, V13; HERBRICH R, 2000, THESIS TU BERLIN; Konig H, 1986, EIGENVALUE DISTRIBUT; LITTLESTONE N, 1986, RELATING DATA COMPRE; NOVIKOFF A, 1962, S MATH THEOR AUT POL, P24; ROSENBLATT M, 1962, PRINCIPLES NEURODYNA; Vapnik V.N., 1999, NATURE STAT LEARNING; Vapnik V.N, 1998, STAT LEARNING THEORY; WAHBA G, 1997, TRNO984 U WISC DEP S	15	18	18	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						210	216						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800030
C	Parra, L; Spence, C; Sajda, P		Leen, TK; Dietterich, TG; Tresp, V		Parra, L; Spence, C; Sajda, P			Higher-order statistical properties arising from the non-stationarity of natural signals	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We present evidence that several higher-order statistical properties of natural images and signals can be explained by a stochastic model which simply varies scale of an otherwise stationary Gaussian process. We discuss two interesting consequences. The first is that a variety of natural signals can be related through a common model of spherically invariant random processes, which have the attractive property that the Joint densities can be constructed from the one dimensional marginal. The second is that in some cases the non-stationarity assumption and only second order methods can be explicitly exploited to find a linear basis that is equivalent to independent components obtained with higher-order methods. This is demonstrated on spectro-temporal components of speech.	Sarnoff Corp, Adapt Signal & Image Proc, Princeton, NJ 08540 USA	Sarnoff Corporation	Parra, L (corresponding author), Sarnoff Corp, Adapt Signal & Image Proc, Princeton, NJ 08540 USA.							BEALE EML, 1959, ANN MATH STAT, V30, P1145, DOI 10.1214/aoms/1177706099; Bollerslev T., 1994, HDB ECONOMETRICS, V4, P2959; BREHM H, 1987, SIGNAL PROCESS, V12, P119, DOI 10.1016/0165-1684(87)90001-6; Brodatz P., 1999, TEXTURES PHOTOGRAPHI; deCharms RC, 1998, ADV NEUR IN, V10, P124; GOLDMAN J, 1976, IEEE T INFORM THEORY, V22, P52, DOI 10.1109/TIT.1976.1055514; Kendal M. G., 1969, ADV THEORY STAT; Parra L, 2000, IEEE T SPEECH AUDI P, V8, P320, DOI 10.1109/89.841214; PARRA L, 2001, INDEPENDENT COMPONEN; TANG A, UNPUB NEURAL COMPUTA; Wainwright MJ, 2000, ADV NEUR IN, V12, P855	11	18	18	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						786	792						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800111
C	Tong, S; Koller, D		Leen, TK; Dietterich, TG; Tresp, V		Tong, S; Koller, D			Active learning for parameter estimation in Bayesian networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Bayesian networks are graphical representations of probability distributions. In virtually all of the work on learning these networks, the assumption is that we are presented with a data set consisting of randomly generated instances from the underlying distribution. In many situations, however, we also have the option of active learning, where we have the possibility of guiding the sampling process by querying for certain types of samples. This paper addresses the problem of estimating the parameters of Bayesian networks in an active learning setting. We provide a theoretical framework for this problem, and an algorithm that chooses which active learning queries to generate based on the model learned so far. We present experimental results showing that our active learning algorithm can significantly reduce the need for training data in many situations.	Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Tong, S (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.							Atkinson A., 1992, OPTIMAL EXPT DESIGNS; Cohn D. A., 1996, J ARTIFICIAL INTELLI, V4; COVER TM, 1991, INFORMATION THEORY; DeGroot M. H., 1970, OPTIMAL STAT DECISIO; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Seung H. S., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P287, DOI 10.1145/130385.130417; Spiegelhalter D. J., 1988, J ROYAL STAT SOC B, V50	9	18	19	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						647	653						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800092
C	Lee, TS; Yu, SX		Solla, SA; Leen, TK; Muller, KR		Lee, TS; Yu, SX			An information-theoretic framework for understanding saccadic eye movements	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				VISUAL-CORTEX	In this paper, we propose that information maximization can provide a unified framework for understanding saccadic eye movements. In this framework, the mutual information among the cortical representations of the retinal image, the priors constructed from our long term visual experience, and a dynamic short-term internal representation constructed from recent saccades provides a map for guiding eye navigation. By directing the eyes to locations of maximum complexity in neuronal ensemble responses at each step, the automatic saccadic eye movement system greedily collects information about the external world, while modifying the neural representations in the process. This framework attempts to connect several psychological phenomena, such as pop-out and inhibition of return, to long term visual experience and short term working memory. It also provides an interesting perspective on contextual computation and formation of neural representation in the visual system.	Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Lee, TS (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.							Ballard DH, 1997, BEHAV BRAIN SCI, V20, P723, DOI 10.1017/S0140525X97001611; Barlow HB, 1989, NEURAL COMPUT, V1, P295, DOI 10.1162/neco.1989.1.3.295; DAUGMAN JG, 1989, IEEE T BIO-MED ENG, V36, P107, DOI 10.1109/10.16456; IRWIN DE, 1991, COGNITIVE PSYCHOL, V23, P420, DOI 10.1016/0010-0285(91)90015-G; KNIERIM J, J NEUROPHYSIOLOGY, V67, P961; Lee TS, 1996, IEEE T PATTERN ANAL, V18, P959, DOI 10.1109/34.541406; Lee TS, 1998, VISION RES, V38, P2429, DOI 10.1016/S0042-6989(97)00464-1; LEWICKI MS, 1998, ADV NEURAL INFORMATI, V10; Linsker R, 1989, NEURAL COMPUT, V1, P402, DOI 10.1162/neco.1989.1.3.402; McConkie G. W., 1976, THEORETICAL MODELS P, P137; MUMFORD D, 1992, BIOL CYBERN, V66, P241, DOI 10.1007/BF00198477; NORTON D, 1971, SCI AM, V224, P34; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580; SIMONCELLI EP, 1999, ADV NEURAL INFORMATI, V11; Yarbus A. L., 1967, EYE MOVEMENTS VISION, P171	16	18	18	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						834	840						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700118
C	Lewicki, MS; Sejnowski, TJ		Mozer, MC; Jordan, MI; Petsche, T		Lewicki, MS; Sejnowski, TJ			Bayesian unsupervised learning of higher order structure	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Multilayer architectures such as those used in Bayesian belief networks and Helmholtz machines provide a powerful framework for representing and learning higher order statistical relations among inputs. Because exact probability calculations with these models are often intractable, there is much interest in finding approximate algorithms. We present an algorithm that efficiently discovers higher order structure using EM and Gibbs sampling. The model can be interpreted as a stochastic recurrent network in which ambiguity in lower-level states is resolved through feedback from higher levels. We demonstrate the performance of the algorithm on benchmark problems.			Lewicki, MS (corresponding author), SALK INST BIOL STUDIES,HOWARD HUGHES MED INST,COMPUTAT NEUROBIOL LAB,10010 N TORREY PINES RD,LA JOLLA,CA 92037, USA.		Sejnowski, Terrence/AAV-5558-2021						0	18	33	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						529	535						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00075
C	Munro, PW; Parmanto, B		Mozer, MC; Jordan, MI; Petsche, T		Munro, PW; Parmanto, B			Competition among networks improves committee performance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The separation of generalization error into two types, bias and variance (Geman, Bienenstock, Doursat, 1992), leads to the notion of error reduction by averaging over a ''committee'' of classifiers (Perrone, 1993). Committee performance decreases with both the average error of the constituent classifiers and increases with the degree to which the misclassifications are correlated across the committee. Here, a method for reducing correlations is introduced, that uses a winner-take-all procedure similar to competitive learning to drive the individual networks to different minima in weight space with respect to the training set, such that correlations in generalization performance will be reduced, thereby reducing committee error.			Munro, PW (corresponding author), UNIV PITTSBURGH,DEPT INFORMAT SCI & TELECOMMUN,PITTSBURGH,PA 15260, USA.								0	18	19	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						592	598						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00084
C	Hofmann, R; Tresp, V		Touretzky, DS; Mozer, MC; Hasselmo, ME		Hofmann, R; Tresp, V			Discovering structure in continuous variables using Bayesian networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SIEMENS AG,CENT RES,D-81730 MUNICH,GERMANY	Siemens AG; Siemens Germany									0	18	19	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						500	506						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00071
C	Pedersen, MW; Hansen, LK; Larsen, J		Touretzky, DS; Mozer, MC; Hasselmo, ME		Pedersen, MW; Hansen, LK; Larsen, J			Pruning with generalization based weight saliencies: gamma OBD, gamma OBS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						TECH UNIV DENMARK,INST ELECT,DK-2800 LYNGBY,DENMARK	Technical University of Denmark			Hansen, Lars/E-3174-2013	Hansen, Lars Kai/0000-0003-0442-5877					0	18	18	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						521	527						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00074
C	Ahn, H; Cha, S; Lee, D; Moon, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ahn, Hongjoon; Cha, Sungmin; Lee, Donggyu; Moon, Taesup			Uncertainty-based Continual Learning with Adaptive Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We introduce a new neural network-based continual learning algorithm, dubbed as Uncertainty-regularized Continual Learning (UCL), which builds on traditional Bayesian online learning framework with variational inference. We focus on two significant drawbacks of the recently proposed regularization-based methods: a) considerable additional memory cost for determining the per-weight regularization strengths and b) the absence of gracefully forgetting scheme, which can prevent performance degradation in learning new tasks. In this paper, we show UCL can solve these two problems by introducing a fresh interpretation on the Kullback-Leibler (KL) divergence term of the variational lower bound for Gaussian mean-field approximation. Based on the interpretation, we propose the notion of node-wise uncertainty, which drastically reduces the number of additional parameters for implementing per-weight regularization. Moreover, we devise two additional regularization terms that enforce stability by freezing important parameters for past tasks and allow plasticity by controlling the actively learning parameters for a new task. Through extensive experiments, we show UCL convincingly outperforms most of recent state-of-the-art baselines not only on popular supervised learning benchmarks, but also on challenging lifelong reinforcement learning tasks.	[Ahn, Hongjoon; Moon, Taesup] Sungkyunkwan Univ, Dept Artificial Intelligence, Suwon 16419, South Korea; [Cha, Sungmin; Lee, Donggyu; Moon, Taesup] Sungkyunkwan Univ, Dept Elect & Comp Engn, Suwon 16419, South Korea	Sungkyunkwan University (SKKU); Sungkyunkwan University (SKKU)	Ahn, H (corresponding author), Sungkyunkwan Univ, Dept Artificial Intelligence, Suwon 16419, South Korea.	hong0805@skku.edu; csm9493@skku.edu; 1dk308@skku.edu; tsmoon@skku.edu	Ahn, Hongjoon/GXG-3566-2022		ICT R&D Program of MSIT/IITP of the Korean government [2016-0-00563]; AI Graduate School Support Program of MSIT/IITP of the Korean government [2019-0-00421]; ITRC Support Program of MSIT/IITP of the Korean government [IITP-2019-2018-0-01798]	ICT R&D Program of MSIT/IITP of the Korean government; AI Graduate School Support Program of MSIT/IITP of the Korean government; ITRC Support Program of MSIT/IITP of the Korean government	This work is supported in part by ICT R&D Program [No. 2016-0-00563, Research on adaptive machine learning technology development for intelligent autonomous digital companion], AI Graduate School Support Program [No.2019-0-00421], and ITRC Support Program [IITP-2019-2018-0-01798] of MSIT/IITP of the Korean government.	Blundell Charles, 2015, INT C MACH LEARN, V37, P1613; CARPENTER GA, 1987, APPL OPTICS, V26, P4919, DOI 10.1364/AO.26.004919; French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2; Gal Y, 2016, PR MACH LEARN RES, V48; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1861; Hinton G., 2015, ARXIV150302531; Hinton Geoffrey E., 1999, P COLT 93; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kemker R., 2018, ICLR; Kingma D. P, 2014, ARXIV13126114; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Lee Jung Kwon, 2017, ADV NEURAL INFORM PR; LEE SW, 2017, ADV NEURAL INFORM PR, P4655; Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081; Lopez-Paz David, 2017, P 31 INT C NEUR INF, P6467; Mallya A, 2018, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2018.00810; Mermillod M, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00504; Nguyen C.V., 2018, INT C LEARNING REPRE; Parisi G. I., 2018, CORR; Plappert M., 2018, P INT C LEARN REPR I; Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587; Rusu A. A., 2016, ARXIV160604671; Schulman J., 2017, CORR; Schwarz J, 2018, PR MACH LEARN RES, V80; Serra J, 2018, PR MACH LEARN RES, V80; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Yoon Jaehong, 2018, 6 INT C LEARN REPR I; Zenke F, 2017, PR MACH LEARN RES, V70	30	17	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304040
C	Alghunaim, SA; Yuan, K; Sayed, AH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Alghunaim, Sulaiman A.; Yuan, Kun; Sayed, Ali H.			A Linearly Convergent Proximal Gradient Algorithm for Decentralized Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DISTRIBUTED OPTIMIZATION; DIFFUSION	Decentralized optimization is a powerful paradigm that finds applications in engineering and learning design. This work studies decentralized composite optimization problems with non-smooth regularization terms. Most existing gradient-based proximal decentralized methods are known to converge to the optimal solution with sublinear rates, and it remains unclear whether this family of methods can achieve global linear convergence. To tackle this problem, this work assumes the non-smooth regularization term is common across all networked agents, which is the case for many machine learning problems. Under this condition, we design a proximal gradient decentralized algorithm whose fixed point coincides with the desired minimizer. We then provide a concise proof that establishes its linear convergence. In the absence of the non-smooth term, our analysis technique covers the well known EXTRA algorithm and provides useful bounds on the convergence rate and step-size.	[Alghunaim, Sulaiman A.; Yuan, Kun] Univ Calif Los Angeles, Elect & Comp Engn Dept, Los Angeles, CA 90095 USA; [Sayed, Ali H.] Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland	University of California System; University of California Los Angeles; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Alghunaim, SA (corresponding author), Univ Calif Los Angeles, Elect & Comp Engn Dept, Los Angeles, CA 90095 USA.	salghunaim@ucla.edu; kunyuan@ucla.edu; ali.sayed@epfl.ch	Sayed, Ali/D-6251-2012	Sayed, Ali/0000-0002-5125-5519	NSF [CCF-1524250]	NSF(National Science Foundation (NSF))	This work was supported in part by NSF grant CCF-1524250. We would like to thank the anonymous reviewers for their insightful comments.	Agarwal A, 2011, ADV NEURAL INFORM PR, P873; Aybat NS, 2018, IEEE T AUTOMAT CONTR, V63, P5, DOI 10.1109/TAC.2017.2713046; BAUDET GM, 1978, J ACM, V25, P226, DOI 10.1145/322063.322067; BERTSEKAS DP, 1983, MATH PROGRAM, V27, P107, DOI 10.1007/BF02591967; Bot RI, 2015, MATH PROGRAM, V150, P251, DOI 10.1007/s10107-014-0766-0; Boyd S., 2011, FDN TRENDS MACHINE L, V3, P1, DOI DOI 10.1561/2200000016; Chambolle A, 2016, MATH PROGRAM, V159, P253, DOI 10.1007/s10107-015-0957-3; Chang TH, 2015, IEEE T SIGNAL PROCES, V63, P482, DOI 10.1109/TSP.2014.2367458; Chazan D., 1969, LINEAR ALGEBRA APPL, V2, P199, DOI DOI 10.1016/0024-3795(69)90028-7; Chen AI, 2012, ANN ALLERTON CONF, P601, DOI 10.1109/Allerton.2012.6483273; Chen JS, 2013, IEEE J-STSP, V7, P205, DOI 10.1109/JSTSP.2013.2246763; Chen PJ, 2013, INVERSE PROBL, V29, DOI 10.1088/0266-5611/29/2/025011; Di Lorenzo P, 2016, IEEE T SIGNAL INF PR, V2, P120, DOI 10.1109/TSIPN.2016.2524588; Dominguez-Garcia AD, 2012, IEEE DECIS CONTR P, P3688, DOI 10.1109/CDC.2012.6426665; Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027; Iutzeler F, 2016, IEEE T AUTOMAT CONTR, V61, P892, DOI 10.1109/TAC.2015.2448011; Jakovetic D, 2015, IEEE T AUTOMAT CONTR, V60, P922, DOI 10.1109/TAC.2014.2363299; Latafat P, 2019, IEEE T AUTOMAT CONTR, V64, P4050, DOI 10.1109/TAC.2019.2906924; Lee CP, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1646, DOI 10.1145/3219819.3220075; Li M., 2014, 11 USENIX S OP SYST, V14, P583, DOI DOI 10.1145/2640087.2644155; Li Z., 2017, ARXIV171106785; Li Z, 2019, IEEE T SIGNAL PROCES, V67, P4494, DOI 10.1109/TSP.2019.2926022; Lian XR, 2017, ADV NEUR IN, V30; Liang XD, 2019, INT J LOGIST-RES APP, V22, P519, DOI 10.1080/13675567.2018.1438378; Ling Q, 2015, IEEE T SIGNAL PROCES, V63, P4051, DOI 10.1109/TSP.2015.2436358; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Nedic A, 2017, SIAM J OPTIMIZ, V27, P2597, DOI 10.1137/16M1084316; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Peng ZM, 2016, SIAM J SCI COMPUT, V38, pA2851, DOI 10.1137/15M1024950; Pillai SU, 2005, IEEE SIGNAL PROC MAG, V22, P62, DOI 10.1109/MSP.2005.1406483; Qu GN, 2018, IEEE T CONTROL NETW, V5, P1245, DOI 10.1109/TCNS.2017.2698261; Sayed AH, 2014, FOUND TRENDS MACH LE, V7, pI, DOI 10.1561/2200000051; Sayed AH, 2014, P IEEE, V102, P460, DOI 10.1109/JPROC.2014.2306253; Scaman K, 2017, PR MACH LEARN RES, V70; Shamir O, 2014, PR MACH LEARN RES, V32, P1000; Shi W, 2015, IEEE T SIGNAL PROCES, V63, P6013, DOI 10.1109/TSP.2015.2461520; Shi W, 2015, SIAM J OPTIMIZ, V25, P944, DOI 10.1137/14096668X; Shi W, 2014, IEEE T SIGNAL PROCES, V62, P1750, DOI 10.1109/TSP.2014.2304432; Smith V, 2018, J MACH LEARN RES, V18; TSITSIKLIS JN, 1986, IEEE T AUTOMAT CONTR, V31, P803, DOI 10.1109/TAC.1986.1104412; Wang Y, 2017, PROCEEDINGS 2017 4TH IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P712, DOI 10.1109/ACPR.2017.75; Xiao L, 2004, SYST CONTROL LETT, V53, P65, DOI 10.1016/j.sysconle.2004.02.022; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Xu JM, 2015, IEEE DECIS CONTR P, P2055, DOI 10.1109/CDC.2015.7402509; Yuan K, 2019, IEEE T SIGNAL PROCES, V67, P708, DOI 10.1109/TSP.2018.2875898; Yuan K, 2019, IEEE T SIGNAL PROCES, V67, P724, DOI 10.1109/TSP.2018.2875883; Yuan K, 2016, SIAM J OPTIMIZ, V26, P1835, DOI 10.1137/130943170; Zinkevich M., 2010, P ADV NEUR INF PROC, P2595	51	17	17	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302080
C	Bradshaw, J; Paige, B; Kusner, MJ; Segler, MHS; Hernandez-Lobato, JM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bradshaw, John; Paige, Brooks; Kusner, Matt J.; Segler, Marwin H. S.; Hernandez-Lobato, Jose Miguel			A Model to Search for Synthesizable Molecules	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NEURAL-NETWORKS; DRUG DISCOVERY; GENERATION; PREDICTION	Deep generative models are able to suggest new organic molecules by generating strings, trees, and graphs representing their structure. While such models allow one to generate molecules with desirable properties, they give no guarantees that the molecules can actually be synthesized in practice. We propose a new molecule generation model, mirroring a more realistic real-world process, where (a) reactants are selected, and (b) combined to form more complex molecules. More specifically, our generative model proposes a bag of initial reactants (selected from a pool of commercially-available molecules) and uses a reaction model to predict how they react together to generate new molecules. We first show that the model can generate diverse, valid and unique molecules due to the useful inductive biases of modeling reactions. Furthermore, our model allows chemists to interrogate not only the properties of the generated molecules but also the feasibility of the synthesis routes. We conclude by using our model to solve retrosynthesis problems, predicting a set of reactants that can produce a target product.	[Bradshaw, John] Univ Cambridge, MPI Intelligent Syst, Cambridge, England; [Paige, Brooks; Hernandez-Lobato, Jose Miguel] Univ Cambridge, Alan Turing Inst, Cambridge, England; [Kusner, Matt J.] UCL, Alan Turing Inst, London, England; [Segler, Marwin H. S.] Westfal Wilhelms Univ Munster, BenevolentAI, Munster, Germany	Max Planck Society; University of Cambridge; University of Cambridge; University of London; University College London; University of Munster	Bradshaw, J (corresponding author), Univ Cambridge, MPI Intelligent Syst, Cambridge, England.	jab255@cam.ac.uk; bpaige@turing.ac.uk; m.kusner@ucl.ac.uk; marwin.segler@benevolent.ai; jmh233@cam.ac.uk			Alan Turing Institute under the EPSRC [EP/N510129/1]; EPSRC studentship	Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC studentship(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1. JB also acknowledges support from an EPSRC studentship.	Alemi AA, 2018, PR MACH LEARN RES, V80; Assouel R., 2018, ARXIV181109766; Battaglia Peter W, 2018, ARXIV 180601261; Bickerton GR, 2012, NAT CHEM, V4, P90, DOI [10.1038/nchem.1243, 10.1038/NCHEM.1243]; Bowman Samuel R, 2016, P 20 SIGNLL C COMP N; Bradshaw J., 2019, INT C LEARN REPR; Brown N, 2019, J CHEM INF MODEL, V59, P1096, DOI 10.1021/acs.jcim.8b00839; Chevillard F, 2015, J CHEM INF MODEL, V55, P1824, DOI 10.1021/acs.jcim.5b00203; Coley CW, 2019, CHEM SCI, V10, P370, DOI 10.1039/c8sc04228d; Dai Hanjun, 2018, INT C LEARN REPR; De Cao Nicola, 2018, INT C MACH LEARN DEE; Duvenaud David K, 2015, P NIPS; Gardner JR, 2014, PR MACH LEARN RES, V32, P937; Gomez-Bombarelli R, 2018, ACS CENTRAL SCI, V4, P268, DOI 10.1021/acscentsci.7b00572; Guimaraes Gabriel Lima, 2017, ARXI170510843; Hartenfeller M, 2011, WIRES COMPUT MOL SCI, V1, P742, DOI 10.1002/wcms.49; Hu QY, 2011, METHODS MOL BIOL, V685, P253, DOI 10.1007/978-1-60761-931-4_13; Janz Dave, 2018, INT C LEARN REPR; Jin W., 2017, ADV NEURAL INFORM PR; Jin W., 2019, INT C LEARN REPR; Jin Wengong, 2018, INT C MACH LEARN; Johnson Daniel D, 2017, INT C LEARN REPR; Kadurin A, 2017, ONCOTARGET, V8, P10883, DOI 10.18632/oncotarget.14073; Kajino H, 2019, PR MACH LEARN RES, V97; Kayala MA, 2011, J CHEM INF MODEL, V51, P2209, DOI 10.1021/ci200207y; Kingma D., 2014, ADAM METHOD STOCHAST; Kusner M. J., 2017, INT C MACH LEARN; Li Y., 2018, ARXIV; Li Y., 2016, INT C LEARN REPR; Liu Q., 2018, ADV NEURAL INFORM PR; Lowe D. M., 2012, EXTRACTION CHEM STRU; Merkwirth C, 2005, J CHEM INF MODEL, V45, P1159, DOI 10.1021/ci049613b; MORGAN HL, 1965, J CHEM DOC, V5, P107, DOI 10.1021/c160017a018; Nicolaou CA, 2016, J CHEM INF MODEL, V56, P1253, DOI 10.1021/acs.jcim.6b00173; Polykovskiy D., 2018, ARXIV PREPRINT ARXIV; Preuer K, 2018, J CHEM INF MODEL, V58, P1736, DOI 10.1021/acs.jcim.8b00234; Pyzer-Knapp EO, 2015, ANNU REV MATER RES, V45, P195, DOI 10.1146/annurev-matsci-070214-020823; Rarey M, 2001, J COMPUT AID MOL DES, V15, P497, DOI 10.1023/A:1011144622059; rdkit, RDKIT OPEN SOURCE CH; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Samanta B, 2019, AAAI CONF ARTIF INTE, P1110; Schneider P, 2016, J MED CHEM, V59, P4077, DOI 10.1021/acs.jmedchem.5b01849; Schwaller P, 2019, ACS CENTRAL SCI, V5, P1572, DOI 10.1021/acscentsci.9b00576; Schwaller P, 2018, CHEM SCI, V9, P6091, DOI 10.1039/c8sc02339e; Segler MHS, 2018, NATURE, V555, P604, DOI 10.1038/nature25978; Segler MHS, 2018, ACS CENTRAL SCI, V4, P120, DOI 10.1021/acscentsci.7b00512; Segler MHS, 2017, CHEM-EUR J, V23, P5966, DOI 10.1002/chem.201605499; Shoichet BK, 2004, NATURE, V432, P862, DOI 10.1038/nature03197; Simonovsky M, 2018, LECT NOTES COMPUT SC, V11139, P412, DOI 10.1007/978-3-030-01418-6_41; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Tolstikhin Ilya, 2018, ICLR 2018; van Hilten Niek, 2019, J CHEM INFORM MODELI; Wei JN, 2016, ACS CENTRAL SCI, V2, P725, DOI 10.1021/acscentsci.6b00219; WEININGER D, 1988, J CHEM INF COMP SCI, V28, P31, DOI 10.1021/ci00057a005; You Jiaxuan, 2018, ADV NEURAL INFORM PR	55	17	17	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308001
C	Coenen, A; Reif, E; Yuan, A; Kim, B; Pearce, A; Viegas, F; Wattenberg, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Coenen, Andy; Reif, Emily; Yuan, Ann; Kim, Been; Pearce, Adam; Viegas, Fernanda; Wattenberg, Martin			Visualizing and Measuring the Geometry of BERT	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Transformer architectures show significant promise for natural language processing. Given that a single pretrained model can be fine-tuned to perform well on many different tasks, these networks appear to extract generally useful linguistic features. How do such networks represent this information internally? This paper describes qualitative and quantitative investigations of one particularly effective model, BERT. At a high level, linguistic features seem to be represented in separate semantic and syntactic subspaces. We find evidence of a fine-grained geometric representation of word senses. We also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings, as well as a mathematical argument to explain the geometry of these representations.	[Coenen, Andy; Reif, Emily; Yuan, Ann; Kim, Been; Pearce, Adam; Viegas, Fernanda; Wattenberg, Martin] Google Brain, Cambridge, MA 02142 USA	Google Incorporated	Coenen, A (corresponding author), Google Brain, Cambridge, MA 02142 USA.	andycoenen@google.com; ereif@google.com; annyuan@google.com; beenkim@google.com; adampearce@google.com; viegas@google.com; wattenberg@google.com		Wattenberg, Martin/0000-0003-0904-4862				Blevins T., 2018, ARXIV180504218; Carter Shan, 2019, DISTILL, V1, P2, DOI [10.23915/distill.00015, DOI 10.23915/DISTILL.00015]; Conneau A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2126; De Marneffe M.-C., 2006, P LREC, V6, P449; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; James Melville, 2020, Arxiv, DOI arXiv:1802.03426; Kim Been, 2017, 35 INT C MACH LEARN; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; LeCun Y., 1998, CONVOLUTIONAL NETWOR, V3361, P255, DOI DOI 10.1109/IJCNN.2004.1381049; Linzen T., 2016, T ACL, V4, P521, DOI DOI 10.1162/TACL_A_00115; Liu Nelson F, 2019, P 2019 C N AM ASS CO, V1, P1073, DOI DOI 10.18653/V1/N19-1112; Maehara H, 2013, DISCRETE MATH, V313, P2848, DOI 10.1016/j.disc.2013.08.029; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; McClosky David, 2015, PYSTANFORDDEPENDENCI; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Miller George A., 1993, P WORKSH HUM LANG TE, P303; Nickel M, 2017, ADV NEUR IN, V30; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Raganato A, 2017, 15TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2017), VOL 1: LONG PAPERS, P99; Schoenberg IJ, 1937, ANN MATH, V38, P787, DOI 10.2307/1968835; Tenney Ian, 2019, ARXIV190505950, DOI [10.18653/v1/P19-1452, DOI 10.18653/V1/P19-1452]; Tenney Ian, 2018, WHAT DO YOU LEARN CO; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vig Jesse, 2019, ARXIV190402679; Yury, 2012, MINIMIZING MAXIMUM D; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53	28	17	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900021
C	Du, YL; Han, L; Fang, M; Dai, TH; Liu, J; Tao, DC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Du, Yali; Han, Lei; Fang, Meng; Dai, Tianhong; Liu, Ji; Tao, Dacheng			LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A great challenge in cooperative decentralized multi-agent reinforcement learning (MARL) is generating diversified behaviors for each individual agent when receiving only a team reward. Prior studies have paid many efforts on reward shaping or designing a centralized critic that can discriminatively credit the agents. In this paper, we propose to merge the two directions and learn each agent an intrinsic reward function which diversely stimulates the agents at each time step. Specifically, the intrinsic reward for a specific agent will be involved in computing a distinct proxy critic for the agent to direct the updating of its individual policy. Meanwhile, the parameterized intrinsic reward function will be updated towards maximizing the expected accumulated team reward from the environment so that the objective is consistent with the original MARL problem. The proposed method is referred to as learning individual intrinsic reward (LIIR) in MARL. We compare LIIR with a number of state-of-the-art MARL methods on battle games in StarCraft II. The results demonstrate the effectiveness of LIIR, and we show LIIR can assign each individual agent an insightful intrinsic reward per time step.	[Du, Yali] UCL, London, England; [Han, Lei] Tencent Al Lab, Shenzhen, Guangdong, Peoples R China; [Fang, Meng] Tencent Robot X, Shenzhen, Guangdong, Peoples R China; [Dai, Tianhong] Imperial Coll London, London, England; [Liu, Ji] Kwai Inc, Seattle, WA USA; [Tao, Dacheng] Univ Sydney, UBTECH Sydney AI Ctr, Sydney, NSW, Australia	University of London; University College London; Imperial College London; University of Sydney	Du, YL (corresponding author), UCL, London, England.	yali.du@ucl.ac.uk; leihan.cs@gmail.com; mfang@tencent.com; tianhong.dai15@imperial.ac.uk; ji.liu.uwisc@gmail.com; dacheng.tao@sydney.edu.au		Dai, Tianhong/0000-0001-8904-1551				Andrychowicz M, 2016, ADV NEUR IN, V29; Dilokthanakul Nat, 2019, IEEE T NEURAL NETWOR; Foerster Jakob N, 2018, 32 AAAI C ART INT AA; Grzes M, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P565; Guo Xiaoxiao, 2016, P 25 INT JOINT C ART, P1519; Han L., 2019, INT C MACH LEARN, P2576; John Christopher, 1989, THESIS; Kempka M, 2016, IEEE CONF COMPU INTE; Kulkarni T. D., 2016, PROC 30 INT C NEURAL, P3675; Liu BY, 2014, IEEE T AUTON MENT DE, V6, P286, DOI 10.1109/TAMD.2014.2362682; Ma JY, 2018, IEEE INT CONF ROBOT, P7254; MANNION P, 2018, KNOWLEDGE ENG REV, V33; Mao HZ, 2016, PROCEEDINGS OF THE 15TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS (HOTNETS '16), P50, DOI 10.1145/3005745.3005750; Nichol Alex, 2018, ARXIV180302999; OpenAl, 2018, OP 5; Pathak D, 2017, PR MACH LEARN RES, V70; Peng P., 2017, ARXIV170310069; Rashid T., 2018, INTERNATIONAL CONFER, P4292, DOI [10.48550/arXiv.1803.11485, DOI 10.48550/ARXIV.1803.11485]; Sallab A. E.L., 2017, ELECT IMAG, P70; Samvelyan M, 2019, AAMAS '19: PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P2186; Santoro A, 2016, PR MACH LEARN RES, V48; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Singh S, 2010, IEEE T AUTON MENT DE, V2, P70, DOI 10.1109/TAMD.2010.2051031; Sorg J., 2010, ADV NEURAL INFORM PR, V23, P2190; Sukhbaatar S., 2016, ADV NEURAL INF PROCE, V29, P2244, DOI DOI 10.5555/3157096.3157348; Sun P., 2018, ARXIV180907193, V1809; Sunehag P, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P2085; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Tampuu A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0172395; Tan M., 1993, P 10 INT C MACHINE L, P330, DOI DOI 10.1016/B978-1-55860-307-3.50049-6; Vinyals Oriol, 2017, ARXIV170804782; Wiering MarcoA, 2000, MACHINE LEARNING P 1, P1151, DOI DOI 10.1038/IJ0.2010.228; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xu ZW, 2018, ADV NEUR IN, V31, DOI 10.1142/S0192415X1850074X; Zheng ZY, 2018, ADV NEUR IN, V31	37	17	17	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304041
C	Fan, JC; Ding, LJ; Chen, YD; Udell, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fan, Jicong; Ding, Lijun; Chen, Yudong; Udell, Madeleine			Factor Group-Sparse Regularization for Efficient Low-Rank Matrix Recovery	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				TRACE-NORM; COMPLETION; MINIMIZATION; IMAGE	This paper develops a new class of nonconvex regularizers for low-rank matrix recovery. Many regularizers are motivated as convex relaxations of the matrix rank function. Our new factor group-sparse regularizers are motivated as a relaxation of the number of nonzero columns in a factorization of the matrix. These nonconvex regularizers are sharper than the nuclear norm; indeed, we show they are related to Schatten-p norms with arbitrarily small 0 < p <= 1. Moreover, these factor group-sparse regularizers can be written in a factored form that enables efficient and effective nonconvex optimization; notably, the method does not use singular value decomposition. We provide generalization error bounds for low-rank matrix completion which show improved upper bounds for Schatten-p norm reglarization as p decreases. Compared to the max norm and the factored formulation of the nuclear norm, factor group-sparse regularizers are more efficient, accurate, and robust to the initial guess of rank. Experiments show promising performance of factor group-sparse regularization for low-rank matrix completion and robust principal component analysis.	[Fan, Jicong; Ding, Lijun; Chen, Yudong; Udell, Madeleine] Cornell Univ, Ithaca, NY 14850 USA	Cornell University	Fan, JC (corresponding author), Cornell Univ, Ithaca, NY 14850 USA.	jf577@cornell.edu; ld446@cornell.edu; yudong.chen@cornell.edu; udell@cornell.edu		Udell, Madeleine/0000-0002-3985-915X	DARPA Award [FA8750-17-2-0101]; NSF [CCF-1740822]	DARPA Award; NSF(National Science Foundation (NSF))	The authors gratefully acknowledge support from DARPA Award FA8750-17-2-0101 and NSF CCF-1740822.	Bolte J, 2014, MATH PROGRAM, V146, P459, DOI 10.1007/s10107-013-0701-9; Bouwmans T, 2018, P IEEE, V106, P1427, DOI 10.1109/JPROC.2018.2853589; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chen YD, 2014, PR MACH LEARN RES, V32; Fan JL, 2020, IEEE T SYST MAN CY-S, V50, P4033, DOI 10.1109/TSMC.2019.2946382; Fan Jicong, 2019, IEEE C COMP VIS PATT; Fan JW, 2018, INT J ADV MANUF TECH, V98, P1131, DOI 10.1007/s00170-018-2335-9; Foygel R., 2011, JMLR WORKSHOP C P, P315; Gao W., 2018, ARXIV180209592; Goldberg A., 2010, P NIPS, V23, P757; Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366; Gunasekar S., 2017, ADV NEURAL INFORM PR, P6151; Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75; Hu Y, 2013, IEEE T PATTERN ANAL, V35, P2117, DOI 10.1109/TPAMI.2012.271; Liu L, 2014, J COMPUT APPL MATH, V267, P218, DOI 10.1016/j.cam.2014.02.015; Liu Q., 2017, ARXIV170502502; Mohan K, 2012, J MACH LEARN RES, V13, P3441; Nie F., 2012, PROC 26 AAAI C ARTIF, P655; Ongie G, 2017, PR MACH LEARN RES, V70; Pimentel-Alarcon D, 2017, PR MACH LEARN RES, V54, P344; Rennie J. D., 2005, P 22 INT C MACHINE L, P713; Shamir O, 2014, J MACH LEARN RES, V15, P3401; Shang FH, 2018, IEEE T PATTERN ANAL, V40, P2066, DOI 10.1109/TPAMI.2017.2748590; Shang FH, 2016, JMLR WORKSH CONF PRO, V51, P620; Srebro N., 2010, PROC INT C NEURAL IN, P2056; Srebro N., 2005, P ADV NEURAL INFORM; Sun RY, 2016, IEEE T INFORM THEORY, V62, P6535, DOI 10.1109/TIT.2016.2598574; Tan MK, 2014, PR MACH LEARN RES, V32, P1539; Toh KC, 2010, PAC J OPTIM, V6, P615; Udell M, 2016, FOUND TRENDS MACH LE, V9, P2, DOI 10.1561/2200000055; [王延贵 Wang Yangui], 2015, [水利水电科技进展, Advances in Science and Technology of Water Resources], V35, P1; Wen ZW, 2012, MATH PROGRAM COMPUT, V4, P333, DOI 10.1007/s12532-012-0044-1; Wu XJ, 2014, INT CONF MEAS, P55, DOI 10.1109/ICMTMA.2014.20	39	17	17	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305014
C	Ghandeharioun, A; Shen, JH; Jaques, N; Ferguson, C; Jones, N; Lapedriza, A; Picard, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ghandeharioun, Asma; Shen, Judy Hanwen; Jaques, Natasha; Ferguson, Craig; Jones, Noah; Lapedriza, Agata; Picard, Rosalind			Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Building an open-domain conversational agent is a challenging problem. Current evaluation methods, mostly post-hoc judgments of static conversation, do not capture conversation quality in a realistic interactive context. In this paper, we investigate interactive human evaluation and provide evidence for its necessity; we then introduce a novel, model-agnostic, and dataset-agnostic method to approximate it. In particular, we propose a self-play scenario where the dialog system talks to itself and we calculate a combination of proxies such as sentiment and semantic coherence on the conversation trajectory. We show that this metric is capable of capturing the human-rated quality of a dialog model better than any automated metric known to-date, achieving a significant Pearson correlation (r > .7, p < .05). To investigate the strengths of this novel metric and interactive evaluation in comparison to state-of-the-art metrics and human evaluation of static conversations, we perform extended experiments with a set of models, including several that make novel improvements to recent hierarchical dialog generation architectures through sentiment and semantic knowledge distillation on the utterance level. Finally, we open-source the interactive evaluation platform we built and the dataset we collected to allow researchers to efficiently deploy and evaluate dialog models.	[Ghandeharioun, Asma; Shen, Judy Hanwen; Jaques, Natasha; Ferguson, Craig; Jones, Noah; Lapedriza, Agata; Picard, Rosalind] MIT, Dept Media Arts & Sci, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Ghandeharioun, A (corresponding author), MIT, Dept Media Arts & Sci, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	asma_gh@mit.edu; judyshen@mit.edu; jaquesn@mit.edu; fergusoc@mit.edu; ncjones@mit.edu; agata@mit.edu; picard@media.mit.edu		Jaques, Natasha/0000-0002-8413-9469; Lapedriza, Agata/0000-0002-5248-0443	Spanish Ministry of Science [RT12018-095232-B-C22]; MIT Media Lab Consortium	Spanish Ministry of Science(Ministry of Science and Innovation, Spain (MICINN)Spanish Government); MIT Media Lab Consortium	We thank Ardavan Saeedi, Max Kleiman-Weiner, Oliver Saunders Wilder, Kyle Kastner, Sebastian Zepf, Ryan Lowe, Abdul Saleh, and Kristy Johnson for helpful discussions, and many others for helping test-drive our bots. We thank the MIT Quest for Intelligence, and MIT Stephen A. Schwarzman College of Computing, Machine Learning Across Disciplines Challenge for providing computing resources, and MIT Media Lab Consortium and RT12018-095232-B-C22 grant from the Spanish Ministry of Science for supporting this research.	Bodie G. D., 2012, INT J LISTENING, V26, P1, DOI [10.1080/10904018.2012.639645, DOI 10.1080/10904018.2012.639645]; Conneau A, 2017, PROC 2017 C EMPIR ME, P670, DOI [10.18653/v1/d17-1070, DOI 10.18653/V1/D17-1070]; Danescu-Niculescu-Mizil Cristian, 2011, P 2 WORKSH COGN MOD, P76; Deb Kalyanmoy, 2014, SEARCH METHODOLOGIES, V4, P5, DOI DOI 10.1007/978-1-4614-6940-7_15; Dinan Emily, 2019, ARXIV190200098; Du L, 2016, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE 2016 CONFERENCE ON DIAGNOSTICS IN ELECTRICAL ENGINEERING (DIAGNOSTIKA), P119; Felbo Bjarke, 2017, 2017 C EMP METH NAT; FLEISS JL, 1969, PSYCHOL BULL, V72, P323, DOI 10.1037/h0028106; Ghosh S., 2016, ARXIV160206291; Gonzales AL, 2010, COMMUN RES, V37, P3, DOI 10.1177/0093650209351468; Hashimoto C, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P147, DOI 10.1145/3178876.3185992; Hashimoto TB, 2019, P 2019 C N AM CHAPT, V1, P1689, DOI DOI 10.18653/V1/N19-1169; Hay J, 2000, J PRAGMATICS, V32, P709, DOI 10.1016/S0378-2166(99)00069-7; Hinton G., 2015, ARXIV150302531; Huang  Chenyang, 2018, P 2018 C N AM CHAPT, V2, P49, DOI DOI 10.18653/V1/N18-2008; Huang K, 2017, J PERS SOC PSYCHOL, V113, P430, DOI 10.1037/pspi0000097; Ireland ME, 2011, PSYCHOL SCI, V22, P39, DOI 10.1177/0956797610392928; Jaques Natasha, 2019, ARXIV190700456; Li JW, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P994; Li Jiwei, 2017, P EMNLP; Liu C, 2016, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON MEDICINE AND BIOPHARMACEUTICALS, P1226; Lowe R, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1116, DOI 10.18653/v1/P17-1103; Mazare Pierre-Emmanuel, 2018, P 2018 C EMP METH NA, P2775, DOI DOI 10.18653/V1/D18-1298; Miller Alexander, 2017, P 2017 C EMP METH NA, P79, DOI DOI 10.18653/V1/D17-2014; Mitchell J., 2008, ACL, P236, DOI DOI 10.1039/9781847558633-00236; Papangelis Alexandros, 2019, ARXIV190705507; Park Yookoon, 2018, P 2018 C N AM CHAPT, V1, P1792, DOI DOI 10.18653/V1/N18-1162; Rashkin H, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P5370; Rashkin Hannah, 2018, ARXIV181100207; Saleh Abdelrhman, 2019, ARXIV190907547; Sedoc J, 2019, NAACL HLT 2019: THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES: PROCEEDINGS OF THE DEMONSTRATIONS SESSION, P60; Serban Iulian V, 2017, ARXIV170902349; Serban IV, 2017, AAAI CONF ARTIF INTE, P3288; Shah  Pararth, 2018, P 2018 C N AM CHAPT, V3, P41, DOI DOI 10.18653/V1/N18-3006; Shen XY, 2018, AAAI CONF ARTIF INTE, P5456; Venkatesh A., 2018, ARXIV PREPRINT ARXIV, V4, P60; Xing Chen, 2017, 31 AAAI C ART INT; Yan X., 2013, P 22 INT C WORLD WID, P1445, DOI DOI 10.1145/2488388.2488514; Zhao TC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P654, DOI 10.18653/v1/P17-1061; Zhou Hao, 2018, 32 AAAI C ART INT; Zhou XD, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1128	42	17	17	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905034
C	Grover, A; Song, JM; Agarwal, A; Tran, K; Kapoor, A; Horvitz, E; Ermon, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Grover, Aditya; Song, Jiaming; Agarwal, Alekh; Tran, Kenneth; Kapoor, Ashish; Horvitz, Eric; Ermon, Stefano			Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A learned generative model often produces biased statistics relative to the underlying data distribution. A standard technique to correct this bias is importance sampling, where samples from the model are weighted by the likelihood ratio under model and true distributions. When the likelihood ratio is unknown, it can be estimated by training a probabilistic classifier to distinguish samples from the two distributions. We show that this likelihood-free importance weighting method induces a new energy-based model and employ it to correct for the bias in existing models. We find that this technique consistently improves standard goodness-of-fit metrics for evaluating the sample quality of state-of-the-art deep generative models, suggesting reduced bias. Finally, we demonstrate its utility on representative applications in a) data augmentation for classification using generative adversarial networks, and b) model-based policy evaluation using off-policy data.	[Grover, Aditya; Song, Jiaming; Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA; [Agarwal, Alekh; Tran, Kenneth; Kapoor, Ashish; Horvitz, Eric] Microsoft Res, Redmond, WA USA	Stanford University; Microsoft	Grover, A (corresponding author), Stanford Univ, Stanford, CA 94305 USA.				NSF [1651565, 1522054, 1733686]; ONR; AFOSR [FA9550-19-1-0024]; FLI	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); FLI	This project was initiated when AG was an intern at Microsoft Research. We are thankful to Daniel Levy, Rui Shu, Yang Song, and members of the Reinforcement Learning, Deep Learning, and Adaptive Systems and Interaction groups at Microsoft Research for helpful discussions and comments on early drafts. This research was supported by NSF (#1651565, #1522054, #1733686), ONR, AFOSR (FA9550-19-1-0024), and FLI.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Antoniou Antreas, 2017, ARXIV171104340; Arora S., 2018, P INT C LEARN REPR; Azadi Samaneh, 2018, ARXIV181006758; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Brockman G., 2016, OPENAI GYM; Burda Yuri, 2015, ARXIV150900519; Byrd Jonathon, 2018, ARXIV181203372; Choi Hyunsun, 2018, WAIC WHY GENERATIVE; Dayan P., 2017, ARXIV PREPRINT ARXIV; Dhariwal Prafulla, 2017, GITHUB GITHUB REPOSI; Diesendruck Maurice, 2018, ARXIV180602512; Dinh Laurent, 2014, ARXIV14108516; Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038; Efron B., 1994, INTRO BOOTSTRAP; Farajtabar M, 2018, PR MACH LEARN RES, V80; Friedman S, 2001, IND REL RES, P1, DOI 10.1097/00054725-200102000-00001; Gal Y, 2016, PR MACH LEARN RES, V48; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grover A, 2018, AAAI CONF ARTIF INTE, P3069; Grover Aditya, 2018, INT C ART INT STAT; Gulrajani I., 2019, INT C LEARN REPR; Guo Chuan, 2017, ICML, DOI DOI 10.5555/3305381.3305518; Gutmann MU, 2012, J MACH LEARN RES, V13, P307; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; HORVITZ DG, 1952, J AM STAT ASSOC, V47, P663, DOI 10.2307/2280784; Im Daniel Jiwoong, 2018, ARXIV180301045; Kingma D. P., 2013, AUTO ENCODING VARIAT; Komorowski Matthieu, 2016, NEUR INF PROC SYST W; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387; Le, 2017, ARXIV PREPRINT ARXIV; Liu JS, 1998, J AM STAT ASSOC, V93, P1032, DOI 10.2307/2669847; LopezPaz D., 2016, ARXIV161006545; Mannor S, 2007, MANAGE SCI, V53, P308, DOI 10.1287/mnsc.1060.0614; Mohamed Shakir, 2016, ARXIV161003483; Naesseth C. A., 2017, ARXIV170511140; Nalisnick Eric, 2018, INT C LEARN REPR; Niculescu-Mizil Alexandru, 2005, P 22 INT C MACHINE L, P625, DOI 10.1145/1102351.1102430; Nowozin S, 2016, ADV NEUR IN, V29; Odena A., 2019, DISTILL, V4, pe18; Odena A, 2016, DISTILL, DOI [10.23915/distill.00003.-URL, 10.23915/distill.00003]; Precup D., 2000, P 17 INT C MACH LEAR; Raghu A, 2017, P MACH LEARN HEALTHC, P147; Ratner AJ, 2017, ADV NEUR IN, V30; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Rosca Mihaela, 2017, ARXIV170604987; ROSENBLATT M, 1956, ANN MATH STAT, V27, P832, DOI 10.1214/aoms/1177728190; Ross S., 2010, PROC 13 INT C ARTIF, V9, P661; Salimans T, 2016, ADV NEUR IN, V29; Salimans T, 2015, PR MACH LEARN RES, V37, P1218; Salimans Tim, 2017, ARXIV170105517; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Smola, 2007, ADV NEURAL INFORM PR, P513, DOI DOI 10.5555/2188385.2188410; Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613; Sutherland Danica J, 2018, ICLR; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tao CY, 2018, PR MACH LEARN RES, V80; Thomas P. S., 2015, THESIS U MASSACHUSET; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Turner R., 2018, P INT C MACH LEARN P; Uria  B., 2016, J MACH LEARN RES, V17, P7184; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Zhao Shengjia, 2018, ADV NEURAL INFORM PR; Zhou Zhengyuan, 2016, INT C DAT SCI ADV AN	68	17	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902066
C	Hung, SCY; Tu, CH; Wu, CE; Chen, CH; Chan, YM; Chen, CS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hung, Steven C. Y.; Tu, Cheng-Hao; Wu, Cheng-En; Chen, Chien-Hung; Chan, Yi-Ming; Chen, Chu-Song			Compacting, Picking and Growing for Unforgetting Continual Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Continual lifelong learning is essential to many applications. In this paper, we propose a simple but effective approach to continual deep learning. Our approach leverages the principles of deep model compression, critical weights selection, and progressive networks expansion. By enforcing their integration in an iterative manner, we introduce an incremental learning method that is scalable to the number of sequential tasks in a continual learning process. Our approach is easy to implement and owns several favorable characteristics. First, it can avoid forgetting (i.e., learn new tasks while remembering all previous tasks). Second, it allows model expansion but can maintain the model compactness when handling sequential tasks. Besides, through our compaction and selection/expansion mechanism, we show that the knowledge accumulated through learning previous tasks is helpful to build a better model for the new tasks compared to training the models independently with tasks. Experimental results show that our approach can incrementally learn a deep model tackling multiple tasks without forgetting, while the model compactness is maintained with the performance more satisfiable than individual task training.	[Hung, Steven C. Y.] Acad Sinica, Inst Informat Sci, Taipei, Taiwan; MOST Joint Res Ctr AI Technol & All Vista Healthc, Taipei, Taiwan	Academia Sinica - Taiwan	Hung, SCY (corresponding author), Acad Sinica, Inst Informat Sci, Taipei, Taiwan.	brent12052003@gmail.com; andytu455l76@gmail.com; chengen@iis.sinica.edu.tw; redsword26@iis.sinica.edu.tw; yiming@iis.sinica.edu.tw; song@iis.sinica.edu.tw	Chan, Yi-Ming/AAN-4679-2021	Chan, Yi-Ming/0000-0002-3172-3039	 [MOST 108-2634-F-001-004]		We thank the anonymous reviewers and area chair for their constructive comments. This work is supported in part under contract MOST 108-2634-F-001-004.	Aijundi Rahaf, 2019, P CVPR; [Anonymous], 2017, P NATL ACAD SCI; [Anonymous], 2018, ARXIV PREPRINT ARXIV; [Anonymous], 2016, PROGR NEURAL NETWORK; [Anonymous], 2017, IEEE T AFFECTIVE COM; Brahma Pratik Prabhanjan, 2018, P IEEE CVPRW; Cao Q., 2018, P IEEE FG; Chaudhry A., 2018, P ECCV; Dhar Prithviraj, 2019, P ECCV; Eidinger E, 2014, IEEE T INF FOREN SEC, V9, P2170, DOI 10.1109/TIFS.2014.2359646; Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540; Escalera Sergio, 2016, P IEEE CVPRW; Goodfellow I., 2015, TENSORFLOW LARGE SCA; Han S., 2016, P ICLR; Hu W., 2019, P ICLR; Hung Steven CY, 2019, P INT C MULT RETR IC; Kemker Ronald, 2018, P ICLR; Krause J., 2013, 4 INT IEEE WORKSH 3D; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Learned-Miller E., 2016, ADV FACE DETECTION F; Lee S.W., 2017, NIPS; Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081; Liu Weiyang, 2017, P IEEE CVPR; Mallya Arun, 2018, P ECCV; Mallya Arun, 2018, P IEEE CVPR; McClelland J. L., 1995, PSYCHOL REV; Nilsback M.-E., 2008, P IND C COMP VIS GRA; Ostapenko Oleksiy, 2019, P CVPR; Parisi G. I., 2019, NEURAL NETWORKS; Parisi German Ignacio, 2018, P NEURIPS WORKSH CON; Paszke Adam, 2017, P NEURIPS; Pfulb B., 2019, ICLR 2019; Rebuffi Sylvestre-Alvise, 2017, P IEEE CVPR; Riemer Matthew, 2019, P ICLR; Riemer Matthew, 2019, AAAI 2019; Ritter Hippolyt, 2018, NEURIPS; Rosenfeld Amir, 2018, EARLY ACCESS; Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P252, DOI 10.1109/ICCVW.2015.41; Saleh B., 2015, ICDMW; Schwarz Jonathan, 2018, P ICML; Shin Hanul, 2017, P NEURIPS; Thrun S., 1995, INTELLIGENT ROBOTS S; Valkov L., 2018, NEURIPS; Wah CK, 2011, HOUS SOC SER, P1; Wu Chenshen, 2018, P NEURIPS; Xiao Tianjun, 2014, P ACM MM; Yoon J., 2018, P ICLR; Zenke Friedemann, 2017, P ICML, P2; Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342; Zhu M., 2018, P ICLR WORKSH P ICLR WORKSH	51	17	18	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905033
C	Khodadadeh, S; Boloni, L; Shah, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Khodadadeh, Siavash; Boloni, Ladislau; Shah, Mubarak			Unsupervised Meta-Learning for Few-Shot Image Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Few-shot or one-shot learning of classifiers requires a significant inductive bias towards the type of task to be learned. One way to acquire this is by meta-learning on tasks similar to the target task. In this paper, we propose UMTRA, an algorithm that performs unsupervised, model-agnostic meta-learning for classification tasks. The meta-learning step of UMTRA is performed on a flat collection of unlabeled images. While we assume that these images can be grouped into a diverse set of classes and are relevant to the target task, no explicit information about the classes or any labels are needed. UMTRA uses random sampling and augmentation to create synthetic training tasks for meta-learning phase. Labels are only needed at the final target task learning step, and they can be as little as one sample per class. On the Omniglot and Mini-Imagenet few-shot learning benchmarks, UMTRA outperforms every tested approach based on unsupervised learning of representations, while alternating for the best performance with the recent CACTUs algorithm. Compared to supervised model-agnostic meta-learning approaches, UMTRA trades off some classification accuracy for a reduction in the required labels of several orders of magnitude.	[Khodadadeh, Siavash; Boloni, Ladislau] Univ Cent Florida, Dept Comp Sci, Orlando, FL 32816 USA; [Shah, Mubarak] Univ Cent Florida, Ctr Res Comp Vis, Orlando, FL 32816 USA	State University System of Florida; University of Central Florida; State University System of Florida; University of Central Florida	Khodadadeh, S (corresponding author), Univ Cent Florida, Dept Comp Sci, Orlando, FL 32816 USA.	siavash.khodadadeh@knights.ucf.edu; lboloni@cs.ucf.edu; shah@crcv.ucf.edu		Shah, Mubarak/0000-0001-6172-5572	National Science Foundation [IIS-1409823, IIS-1741431]; Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA RD [D17PC00345]	National Science Foundation(National Science Foundation (NSF)); Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA RD	This research is based upon work supported in parts by the National Science Foundation under Grant numbers IIS-1409823 and IIS-1741431 and Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&D Contract No. D17PC00345. The views, findings, opinions, and conclusions or recommendations contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the NSF, ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.	Abu-Mostafa Y., 2012, LEARNING FROM DATA, V4; [Anonymous], 2018, ARXIV180604640; [Anonymous], P INT C LEARN REPR V; Antoniou A., 2019, ARXIV190209884; Bengio Y., 1990, LEARNING SYNAPTIC LE; Berthelot D., 2018, ARXIV180707543; Caron Mathilde, 2018, P EUR C COMP VIS ECC, P132; Cubuk E. D., 2018, ARXIV180509501; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Donahue J., 2016, ARXIV160509782; Finn C., 2017, P INT C MACH LEARN I; Friedman S, 2001, IND REL RES, P1, DOI 10.1097/00054725-200102000-00001; Hariharan B., 2017, P IEEE C COMP VIS PA; Ioffe S, 2015, ARXIV 1502 03167, V32, P448; Ionescu C., 2016, ADV NEURAL INFORM PR, P4331; James G., 2013, INTRO STAT LEARNING, V112; Lake B., 2011, P ANN M COGN SCI SOC; Luo Z., 2017, ADV NEURAL INFORM PR, P165; Meier F, 2018, IEEE INT CONF ROBOT, P2425; Metz, 2018, LEARNING UNSUPERVISE; Miconi T., 2018, ARXIV180402464; Mishra N., 2018, ARXIV170703141; Nichol Alex, 2018, ABS180302999 ARXIV; Oliver A., 2018, ADV NEURAL INFORM PR, V31, P3235; Park E., 2018, ECCV, P569; Ravi S., 2016, P INT C LEARN REPR I; Rupprecht T, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1772, DOI 10.1145/2976749.2989041; Santoro A, 2016, PR MACH LEARN RES, V48; Schmidhuber J., 1987, THESIS TU MUNICH MUC; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Vinyals Oriol, 2016, ARXIV160604080, P3630; Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393	32	17	17	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901073
C	Laidlaw, C; Feizi, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Laidlaw, Cassidy; Feizi, Soheil			Functional Adversarial Attacks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose functional adversarialattacks, a novel class of threat models for crafting adversarial examples to fool machine learning models. Unlike a standard l(p)-ball threat model, a functional adversarial threat model allows only a single function to be used to perturb input features to produce an adversarial example. For example, a functional adversarial attack applied on colors of an image can change all red pixels simultaneously to light red. Such global uniform changes in images can be less perceptible than perturbing pixels of the image individually. For simplicity, we refer to functional adversarial attacks on image colors as ReColorAdv, which is the main focus of our experiments. We show that functional threat models can be combined with existing additive (l(p)) threat models to generate stronger threat models that allow both small, individual perturbations and large, uniform changes to an input. Moreover, we prove that such combinations encompass perturbations that would not be allowed in either constituent threat model. In practice, ReColorAdv can significantly reduce the accuracy of a ResNet-32 trained on CIFAR-10. Furthermore, to the best of our knowledge, combining ReColorAdv with other attacks leads to the strongest existing attack even after adversarial training.	[Laidlaw, Cassidy; Feizi, Soheil] Univ Maryland, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park	Laidlaw, C (corresponding author), Univ Maryland, College Pk, MD 20742 USA.	claidlaw@umd.edu; sfeizi@cs.umd.edu			NSF [CDSE: 1854532, HR001 11990077]	NSF(National Science Foundation (NSF))	This work was supported in part by NSF award CDS&E: 1854532 and award HR001 11990077.	[Anonymous], 2016, CORR; Bhattad Anand, 2019, ARXIV190406347; Brendel Wieland, 2018, INT C LEARN REPR FEB; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Goodfellow I. J., 2015, P ICLR; Hosseini H, 2018, IEEE COMPUT SOC CONF, P1695, DOI 10.1109/CVPRW.2018.00212; Hosseini H, 2017, 2017 16TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P352, DOI 10.1109/ICMLA.2017.0-136; Idelbayev Yerlan, 2018, PROPER RESNET IMPLEM; Jordan M., 2019, ARXIV190208265; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kang Daniel, 2019, ARXIV190501034; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Madry Aleksander, 2018, ICLR; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schmidt L., 2017, ARXIV171202779; Schwiegerling J., 2004, FIELD GUIDE VISUAL O; Song Y., 2018, P 32 INT C NEUR INF, V31, P8322; Szegedy C., 2014, 2014 INT C LEARNING, DOI DOI 10.48550/ARXIV.1312.6199; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Uesato J, 2018, PR MACH LEARN RES, V80; Wong E., 2019, ARXIV190207906; Xiao C., 2018, P INT C LEARN REPR; Zeng Xiaohui, 2019, P IEEE C COMP VIS PA; Zhang H., 2019, ARXIV190104684; Zhang Hongyang, 2019, ICML 2019; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068	32	17	18	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902008
C	Lake, BM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lake, Brenden M.			Compositional generalization through meta sequence-to-sequence learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					People can learn a new concept and use it compositionally, understanding how to "blicket twice" after learning how to "blicket." In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.	[Lake, Brenden M.] NYU, Facebook AI Reasearch, New York, NY 10003 USA	New York University	Lake, BM (corresponding author), NYU, Facebook AI Reasearch, New York, NY 10003 USA.	brenden@nyu.edu						Andreas Jacob, 2019, GOOD ENOUGH COMPOSIT; Bahdanau Dzmitry, 2018, SYSTEMATIC GEN WHAT, P1; Bastings Jasmijn, 2018, P 2018 EMNLP WORKSH, P47, DOI [10.18653/v1/W18-5407, DOI 10.18653/V1/W18-5407]; Bojar O., 2016, SHARED TASK PAPERS, V2, P131, DOI [10.18653/v1/W16-2301, DOI 10.18653/V1/W16-2301]; Chomsky N., 1957, SYNTACTIC STRUCTURES; Dasgupta Ishita, 2018, EVALUATING COMPOSITI; Finn C, 2017, PR MACH LEARN RES, V70; FODOR JA, 1988, COGNITION, V28, P3, DOI 10.1016/0010-0277(88)90031-5; Gandhi Kanishk, 2019, MUTUAL EXCLUSIVITY C; Gershman S. J., 2015, P 37 ANN C COGN SCI; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Grefenstette E, 2015, ADV NEUR IN, V28; Gu Jiatao, 2018, EMPIRICAL METHODS NA; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kingma Diederik P, 2014, INT C MACH LEARN ICM; Lake B, 2018, PR MACH LEARN RES, V80; Lake BM, 2019, CURR OPIN BEHAV SCI, V29, P97, DOI 10.1016/j.cobeha.2019.04.007; Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837; Lake Brenden M., 2019, HUMAN FEW SHOT LEARN; Loula Joao, 2018, REARRANGING FAMILIAR; Luong Minh-Thang, 2015, EMPIRICAL METHODS NA; Marcus Gary, 2018, ARXIV; Marcus Gary, 2003, ALGEBRAIC MIND INTEG; MARKMAN EM, 1988, COGNITIVE PSYCHOL, V20, P121, DOI 10.1016/0010-0285(88)90017-5; Nye M, 2019, PR MACH LEARN RES, V97; Reed S, 2016, PR MACH LEARN RES, V48; Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707; Russin Jake, 2019, ARXIV190409708; Santoro A, 2016, PR MACH LEARN RES, V48; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Sukhbaatar S, 2015, ADV NEUR IN, V28; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wang JW, 2017, ACTA OPHTHALMOL, V95, pE10, DOI 10.1111/aos.13227; Wu Yonghui, 2016, GOOGLES NEURAL MACHI	38	17	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901042
C	Mao, HZ; Negi, P; Narayan, A; Wang, HR; Yang, JC; Wang, HN; Marcus, R; Addanki, R; Khani, M; He, ST; Nathan, V; Cangialosi, F; Venkatakrishnan, SB; Weng, WH; Han, S; Kraska, T; Alizadeh, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mao, Hongzi; Negi, Parimarjan; Narayan, Akshay; Wang, Hanrui; Yang, Jiacheng; Wang, Haonan; Marcus, Ryan; Addanki, Ravichandra; Khani, Mehrdad; He, Songtao; Nathan, Vikram; Cangialosi, Frank; Venkatakrishnan, Shaileshh Bojja; Weng, Wei-Hung; Han, Song; Kraska, Tim; Alizadeh, Mohammad			Park: An Open Platform for Learning-Augmented Computer Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				REINFORCEMENT; ALGORITHM	We present Park, a platform for researchers to experiment with Reinforcement Learning (RL) for computer systems. Using RL for improving the performance of systems has a lot of potential, but is also in many ways very different from, for example, using RL for games. Thus, in this work we first discuss the unique challenges RL for systems has, and then propose Park an open extensible platform, which makes it easier for ML researchers to work on systems problems. Currently, Park consists of 12 real world system-centric optimization problems with one common easy to use interface. Finally, we present the performance of existing RL approaches over those 12 problems and outline potential areas of future work.	[Mao, Hongzi; Negi, Parimarjan; Narayan, Akshay; Wang, Hanrui; Yang, Jiacheng; Wang, Haonan; Marcus, Ryan; Addanki, Ravichandra; Khani, Mehrdad; He, Songtao; Nathan, Vikram; Cangialosi, Frank; Venkatakrishnan, Shaileshh Bojja; Weng, Wei-Hung; Han, Song; Kraska, Tim; Alizadeh, Mohammad] MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Mao, HZ (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA.	park-project@csail.mit.edu	Han, Song/AAR-9464-2020	Han, Song/0000-0002-4186-7618; Bojja Venkatakrishnan, Shaileshh/0000-0001-7355-634X	NSF [CNS-1751009, CNS-1617702]; Google Faculty Research Award; AWS Machine Learning Research Award; Cisco Research Center Award; Alfred P. Sloan Research Fellowship; MIT DSAIL lab.	NSF(National Science Foundation (NSF)); Google Faculty Research Award(Google Incorporated); AWS Machine Learning Research Award; Cisco Research Center Award; Alfred P. Sloan Research Fellowship(Alfred P. Sloan Foundation); MIT DSAIL lab.	thank the anonymous NeurIPS reviewers for their constructive feedback. This work was funded in part by the NSF grants CNS-1751009, CNS-1617702, a Google Faculty Research Award, an AWS Machine Learning Research Award, a Cisco Research Center Award, an Alfred P. Sloan Research Fellowship, and sponsors of the MIT DSAIL lab.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Achiam J, 2017, PR MACH LEARN RES, V70; Addanki Ravichandra, 2018, NIPS MACH LEARN SYST NIPS MACH LEARN SYST; Akhtar Z, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P44, DOI 10.1145/3230543.3230558; Alicherry M, 2012, IEEE INFOCOM SER, P963, DOI 10.1109/INFCOM.2012.6195847; [Anonymous], 2017, ADV NEURAL INFORM PR; Arun V., 2018, NSDI; Astrom K. J., 2006, ADV PID CONTROL, V461; Athuraliya S, 2001, IEEE NETWORK, V15, P48, DOI 10.1109/65.923940; Barroso Luiz Andre, 2013, SYNTHESIS LECT COMPU, V8, P3; Begoli E, 2018, INT CONF MANAGE DATA, P221, DOI 10.1145/3183713.3190662; Berger DS, 2018, HOTNETS-XVII: PROCEEDINGS OF THE 2018 ACM WORKSHOP ON HOT TOPICS IN NETWORKS, P134, DOI 10.1145/3286062.3286082; Berger DS, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P483; Boyan J., 1994, ADV NEURAL INFORM PR, V6; BRAKMO LS, 1995, IEEE J SEL AREA COMM, V13, P1465, DOI 10.1109/49.464716; Brockman G., 2016, OPENAI GYM; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Chen L, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P191, DOI 10.1145/3230543.3230551; Chilimbi T., 2014, OSDI, P571; Claeys M, 2014, IEEE COMMUN LETT, V18, P716, DOI 10.1109/LCOMM.2014.020414.132649; DALEY DJ, 1987, STOCH PROC APPL, V25, P301, DOI 10.1016/0304-4149(87)90208-0; Dong M, 2018, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'18), P343; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Espeholt L, 2018, PR MACH LEARN RES, V80; Feng HH, 2005, PERFORM EVALUATION, V62, P475, DOI 10.1016/j.peva.2005.07.031; Ferguson Andrew D., 2012, P 7 ACM EUROPEAN C C, P99; Floyd S, 1993, IEEE ACM T NETWORK, V1, P397, DOI 10.1109/90.251892; Gao Y, 2018, DESTECH TRANS ENG, P162; Garcia J, 2015, J MACH LEARN RES, V16, P1437; Giaccone P, 2002, IEEE INFOCOM SER, P1160, DOI 10.1109/INFCOM.2002.1019366; Grandl R, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P81; Greenberg A, 2009, ACM SIGCOMM COMP COM, V39, P51, DOI 10.1145/1594977.1592576; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Harchol-Balter M, 2010, PROBAB ENG INFORM SC, V24, P219, DOI 10.1017/S0269964809990246; Hasan S, 2014, IEEE INFOCOM SER, P468; Hesse C., 2017, OPENAI BASELINES; Hester T., 2017, 32 AAAI C ART INT AA; Hollot CV, 2001, IEEE INFOCOM SER, P1510, DOI 10.1109/INFCOM.2001.916647; Huang TY, 2014, ASIA JT CONF INF SEC, P141, DOI 10.1109/AsiaJCIS.2014.31; Hunter DK, 1997, J LIGHTWAVE TECHNOL, V15, P86, DOI 10.1109/50.552116; Hwangbo J, 2019, SCI ROBOT, V4, DOI 10.1126/scirobotics.aau5872; IBM, SPAT IND; Jacobson V., 1988, ACM SIGCOMM COMP COM, P314, DOI [DOI 10.1145/52324.52356, 10.1145/52324.52356, DOI 10.1145/52325.52356]; Jay N., 2018, ARXIV181003259; Kang Daniel, 2018, NEURIPS MLSYS WORKSH; Kipf TN, 2016, P INT C LEARN REPR; Krishnan S., 2018, ARXIV180803196; Krizhevsky A., 2010, UNPUB, V40, P1, DOI DOI 10.1145/3065386; Leis V, 2015, PROC VLDB ENDOW, V9, P204; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Li Y., 2019, REINFORCEMENT LEARNI; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Liu B., 2010, CEC; LU T., 2010, PROC 30 INT C ARTIF, P485; Lyu W., 2018, DAC; Lyu WL, 2018, PR MACH LEARN RES, V80; Maguluri S. T., 2016, STOCHASTIC SYST, V6, P211; Mao H., 2019, P 7 INT C LEARN REPR; Mao H., 2017, P ACM SIGCOMM 2017 C; Mao H., 2018, ARXIV181001963CSSTAT; Mao HZ, 2016, PROCEEDINGS OF THE 15TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS (HOTNETS '16), P50, DOI 10.1145/3005745.3005750; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Marcus R., 2019, ARXIV190403711; McKeown N, 1999, IEEE ACM T NETWORK, V7, P188, DOI 10.1109/90.769767; Mirhoseini A, 2017, PR MACH LEARN RES, V70; Mirhoseini Azalia, 2018, P INT C LEARNING REP; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nagle J., 1984, REQUEST FOR COMMENTS; Nair V, 2010, P 27 INT C MACHINE L, P807; Narayan A., 2018, SIGCOMM; Netravali R., 2015, PROC USENIX ANN TECH, P417; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Nygren E., 2010, OPER SYST REV, V44, P2, DOI DOI 10.1145/1842733.1842736; OpenStreetMap contributors, 2019, US NE DUMP OBT; Ortiz J, 2018, PROCEEDINGS OF THE SECOND WORKSHOP ON DATA MANAGEMENT FOR END-TO-END MACHINE LEARNING, DOI 10.1145/3209889.3209890; Pellegrini F, 2007, LECT NOTES COMPUT SC, V4641, P195; Rajeswaran A., 2017, ARXIV PREPRINT ARXIV; Razavi Behzad, 2002, DESIGN ANALOG CMOS I; Salimans T., 2017, ARXIV170303864; Sandvine, 2018, GLOB INT PHEN; Shah D, 2006, IEEE INFOCOM SER, P1810; Shapira M., 2017, HOTNETS; Silver D, 2018, SCIENCE, V362, P1140, DOI 10.1126/science.aar6404; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Slayton Zack, 2017, Z ORDER INDEXING MUL; Srinivasan R., 1995, TECH REP; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Synopsys, 2019, HSPICE; Verma A, 2014, IEEE INT C CL COMP, P48, DOI 10.1109/CLUSTER.2014.6968735; Verma AK, 2015, PARKINSONS DIS-US, V2015, DOI 10.1155/2015/598028; Vinyals Oriol, 2019, ALPHASTAR MASTERING; Wang H., 2019, ARXIV181202734; Wang H., 2019, TRANSFERABLE AUTOMAT; Wu Y., 2017, INT C LEARN RE UNPUB; Yan FY, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P731; Yin X., 2015, P 2015 ACM C SPEC IN; Zaharia M., 2012, P 9 USENIX C NETWORK, P15, DOI DOI 10.1002/(SICI)1096-9128(199809/11)10:11/133.0.CO;2-H	103	17	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302049
C	Santurkar, S; Tsipras, D; Tran, B; Ilyas, A; Engstrom, L; Madry, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Santurkar, Shibani; Tsipras, Dimitris; Tran, Brandon; Ilyas, Andrew; Engstrom, Logan; Madry, Aleksander			Image Synthesis with a Single (Robust) Classifier	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We show that the basic classification framework alone can be used to tackle some of the most challenging tasks in image synthesis. In contrast to other state-of-the-art approaches, the toolkit we develop is rather minimal: it uses a single, off-the-shelf classifier for all these tasks. The crux of our approach is that we train this classifier to be adversarially robust. It turns out that adversarial robustness is precisely what we need to directly manipulate salient features of the input. Overall, our findings demonstrate the utility of robustness in the broader machine learning context.(2)	[Santurkar, Shibani; Tsipras, Dimitris; Tran, Brandon; Ilyas, Andrew; Engstrom, Logan; Madry, Aleksander] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Santurkar, S (corresponding author), MIT, Cambridge, MA 02139 USA.	shibani@mit.edu; tsipras@mit.edu; btran115@mit.edu; ailyas@mit.edu; engstrom@mit.edu; madry@mit.edu	Tsipras, Dimitris/AAZ-2505-2021		NSF [CCF-1553428, CCF-1563880, CNS-1413920, CNS-1815221, IIS-1447786, IIS-1607189]; Microsoft Corporation; Intel Corporation; MIT-IBM Watson AI Lab research grant; Analog Devices Fellowship	NSF(National Science Foundation (NSF)); Microsoft Corporation(Microsoft); Intel Corporation(Intel Corporation); MIT-IBM Watson AI Lab research grant(International Business Machines (IBM)); Analog Devices Fellowship	Work supported in part by the NSF grants CCF-1553428, CCF-1563880, CNS-1413920, CNS-1815221, IIS-1447786, IIS-1607189, the Microsoft Corporation, the Intel Corporation, the MIT-IBM Watson AI Lab research grant, and an Analog Devices Fellowship.	Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374; Nguyen A, 2016, ADV NEUR IN, V29; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; Bau David, 2019, INT C LEARN REPR ICL; Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972; Brock A., 2019, INT C LEARNING REPRE; BURGER HC, 2012, PROC CVPR IEEE, P2392, DOI DOI 10.1109/CVPR.2012.6247952; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Dabov K., 2007, P 15 EUR SIGN PROC C, V1, P7; Daskalakis C., 2018, P INT C LEARN REPR; Dinh L, 2017, 5 INT C LEARN REPR I; Dinh Laurent, 2014, ARXIV14108516; Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383; Engstrom Logan, 2019, ARXIV PREPRINT ARXIV; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A, 2013, ARXIV13080850; Gulrajani I, 2017, P NIPS 2017; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hensel M, 2017, ADV NEUR IN, V30; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Kingma D.P, P 3 INT C LEARNING R; Kingma Diederik P, 2018, ADV NEURAL INFORM PR; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Madry A., 2018, P ICLR VANC BC CAN; Mordvintsev A, 2015, INCEPTIONISM GOING D; Oygard A., 2015, VISUALIZING GOOGLENE; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T, 2016, ADV NEUR IN, V29; Shmelkov K, 2018, LECT NOTES COMPUT SC, V11206, P218, DOI 10.1007/978-3-030-01216-8_14; Simonyan Karen, 2013, DEEP INSIDE CONVOLUT, P2; Tsipras Dimitris, 2019, ROBUSTNESS MAY BE OD, V1, P2; Tyka Mike, 2016, CLASS VISUALIZATION; Ulyanov D, 2020, INT J COMPUT VISION, V128, P1867, DOI 10.1007/s11263-020-01303-4; van den Oord A, 2016, PR MACH LEARN RES, V48; Vincent P, 2010, J MACH LEARN RES, V11, P3371; WALD A, 1945, ANN MATH, V46, P265, DOI 10.2307/1969022; Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728; Zhang Han, 2018, ARXIV180508318; Zhu Jun-Yan, 2017, ICCV	54	17	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301028
C	Townshend, RJL; Bedi, R; Suriana, PA; Dror, RO		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Townshend, Raphael J. L.; Bedi, Rishi; Suriana, Patricia A.; Dror, Ron O.			End-to-End Learning on 3D Protein Structure for Interface Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Despite an explosion in the number of experimentally determined, atomically detailed structures of biomolecules, many critical tasks in structural biology remain data-limited. Whether performance in such tasks can be improved by using large repositories of tangentially related structural data remains an open question. To address this question, we focused on a central problem in biology: predicting how proteins interact with one another-that is, which surfaces of one protein bind to those of another protein. We built a training dataset, the Database of Interacting Protein Structures (DIPS), that contains biases but is two orders of magnitude larger than those used previously. We found that these biases significantly degrade the performance of existing methods on gold-standard data. Hypothesizing that assumptions baked into the hand-crafted features on which these methods depend were the source of the problem, we developed the first end-to-end learning model for protein interface prediction, the Siamese Atomic Surfacelet Network (SASNet). Using only spatial coordinates and identities of atoms, SASNet outperforms state-of-the-art methods trained on gold-standard structural data, even when trained on only 3% of our new dataset.	[Townshend, Raphael J. L.; Bedi, Rishi; Suriana, Patricia A.; Dror, Ron O.] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Townshend, RJL (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	raphael@cs.stanford.edu; rbedi@cs.stanford.edu; psuriana@stanford.edu; rondror@cs.stanford.edu		Dror, Ron/0000-0002-6418-2793; Suriana, Patricia/0000-0002-0599-5233	Intel; National Science Foundation [1147470]; U.S. Department of Energy Office of Science Graduate Student Research (SCGSR) program; U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Scientific Discovery through Advanced Computing (SciDAC) program; Amazon	Intel(Intel Corporation); National Science Foundation(National Science Foundation (NSF)); U.S. Department of Energy Office of Science Graduate Student Research (SCGSR) program(United States Department of Energy (DOE)); U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Scientific Discovery through Advanced Computing (SciDAC) program; Amazon	The authors thank Guy Amdur, Robin Betz, Stephan Eismann, Scott Hollingsworth, Milind Jagota, Yianni Laloudakis, Naomi Latorraca, Joe Paggi, Reid Pryzant, Joao Rodrigues, and AJ Venkatakrishnan for their discussions and advice. This work was supported by Intel, Amazon, the National Science Foundation Graduate Research Fellowship Program under Grant No. 1147470, the U.S. Department of Energy Office of Science Graduate Student Research (SCGSR) program, and the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Scientific Discovery through Advanced Computing (SciDAC) program.	Ahmad S, 2011, PLOS ONE, V6; Behler J, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.146401; Berman HM, 2000, NUCLEIC ACIDS RES, V28, P235, DOI 10.1093/nar/28.1.235; Bonvin AM, 2006, CURR OPIN STRUC BIOL, V16, P194, DOI 10.1016/j.sbi.2006.02.002; Bromley J., NEURIPS, P737; Cang Z., 2017, ARXIV170400063; de Jesus D. R., 2018, ARXIV180807475; Duvenaud David K, 2015, NEURIPS; Dzamba M., 2015, ARXIV151002855; Esmaielbeiki R, 2016, BRIEF BIOINFORM, V17, P117, DOI 10.1093/bib/bbv027; Fout A., 2017, NEURIPS; Gilmer J, 2017, PR MACH LEARN RES, V70; Gomes J, 2017, ARXIV170310603; Hwang H, 2016, PROTEIN SCI, V25, P159, DOI 10.1002/pro.2744; Jimenez J, 2017, BIOINFORMATICS, V33, P3036, DOI 10.1093/bioinformatics/btx350; Jordan RA, 2012, BMC BIOINFORMATICS, V13, DOI 10.1186/1471-2105-13-41; Kirys T, 2015, BMC BIOINFORMATICS, V16, DOI 10.1186/s12859-015-0672-3; Kondor R., 2018, ARXIV PREPRINT ARXIV; Kuroda D, 2016, STRUCTURE, V24, P1821, DOI 10.1016/j.str.2016.06.025; Kuzminykh D, 2018, MOL PHARMACEUT, V15, P4378, DOI 10.1021/acs.molpharmaceut.7b01134; Mosca R, 2014, NUCLEIC ACIDS RES, V42, pD374, DOI 10.1093/nar/gkt887; Northey TC, 2018, BIOINFORMATICS, V34, P223, DOI 10.1093/bioinformatics/btx585; Porollo A, 2007, PROTEINS, V66, P630, DOI 10.1002/prot.21248; Ragoza M, 2017, J CHEM INF MODEL, V57, P942, DOI 10.1021/acs.jcim.6b00740; Rost B, 1999, PROTEIN ENG, V12, P85, DOI 10.1093/protein/12.2.85; Sanchez-Garcia R., BIOINFORMATICS, V35, P343; Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890; Schutt K. T., 2017, J CHEM THEORY COMPUT, V13, P5255; Smith JS, 2017, CHEM SCI, V8, P3192, DOI 10.1039/c6sc05720a; Thomas Nathaniel, 2018, ARXIV180208219; Torng W, 2017, BMC BIOINFORMATICS, V18, DOI 10.1186/s12859-017-1702-0; Vreven T, 2015, J MOL BIOL, V427, P3031, DOI 10.1016/j.jmb.2015.07.016; Wang W., 2018, ARXIV181202706V1; Weiler M., 2018, NEURIPS; Yang JY, 2013, BIOINFORMATICS, V29, P2588, DOI 10.1093/bioinformatics/btt447; You Jiaxuan, 2018, NEURIPS	38	17	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907031
C	Yildiz, C; Heinonen, M; Lahdesmaki, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yildiz, Cagatay; Heinonen, Markus; Lahdesmaki, Harri			ODE(2)VAE: Deep generative second order ODEs with Bayesian neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present Ordinary Differential Equation Variational Auto-Encoder (ODE(2)VAE), a latent second order ODE model for high-dimensional sequential data. Leveraging the advances in deep generative models, ODE(2)VAE can simultaneously learn the embedding of high dimensional trajectories and infer arbitrarily complex continuous-time latent dynamics. Our model explicitly decomposes the latent space into momentum and position components and solves a second order ODE system, which is in contrast to recurrent neural network (RNN) based time series models and recently proposed black-box ODE techniques. In order to account for uncertainty, we propose probabilistic latent ODE dynamics parameterized by deep Bayesian neural networks. We demonstrate our approach on motion capture, image rotation and bouncing balls datasets. We achieve state-of-the-art performance in long term motion prediction and imputation tasks.	[Yildiz, Cagatay; Heinonen, Markus; Lahdesmaki, Harri] Aalto Univ, Dept Comp Sci, FI-00076 Espoo, Finland	Aalto University	Yildiz, C (corresponding author), Aalto Univ, Dept Comp Sci, FI-00076 Espoo, Finland.	cagatay.yildiz@aalto.fi; markus.o.heinonen@aalto.fi; harri.lahdesmaki@aalto.fi			NSF [EIA-0196217]; Academy of Finland [311584, 313271]	NSF(National Science Foundation (NSF)); Academy of Finland(Academy of Finland)	The data used in this project was obtained from mocap. cs. cmu. edu. The database was created with funding from NSF EIA-0196217. The calculations presented above were performed using computer resources within the Aalto University School of Science Science-IT project. This work has been supported by the Academy of Finland grants no. 311584 and 313271.	Abadi M, 2015, P 12 USENIX S OPERAT; Alemi AA, 2017, ARXIV171100464; Casale Francesco Paolo, 2018, ADV NEURAL INFORM PR, P10390; Chen Changyou, 2018, ICML; Chen T.Q., 2018, ADV NEURAL INFORM PR; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; Damianou A., 2011, ADV NEURAL INFORM PR, P2510; Grathwohl Will, 2018, ARXIV181001367; Heinonen Markus, 2018, P MACHINE LEARNING R, V80, P1959; Higgins M, 2017, PALGR COMMUN, V3, DOI 10.1057/s41599-017-0005-4; Hsieh JT, 2018, ADV NEUR IN, V31; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Karl M., 2016, INT C LEARN REPR; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Li Y., 2017, ARXIV170800065; Lim WY, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P787, DOI 10.1145/3132847.3132995; Lotter William, 2015, ARXIV151106380; Lotter William, 2016, ARXIV160508104; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Schober M, 2019, STAT COMPUT, V29, P99, DOI 10.1007/s11222-017-9798-7; Serban IV, 2017, AAAI CONF ARTIF INTE, P3288; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Sutskever Ilya, 2009, ADV NEURAL INFORM PR, P2; Tolstikhin Ilya, 2017, ARXIV171101558; Tschannen Michael, 2018, ARXIV181205069; Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167; Xiao S, 2018, AAAI CONF ARTIF INTE, P6302; Zhao SS, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3490	33	17	17	1	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905012
C	Yolcu, E; Poczos, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yolcu, Emre; Poczos, Barnabas			Learning Local Search Heuristics for Boolean Satisfiability	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHM	We present an approach to learn SAT solver heuristics from scratch through deep reinforcement learning with a curriculum. In particular, we incorporate a graph neural network in a stochastic local search algorithm to act as the variable selection heuristic. We consider Boolean satisfiability problems from different classes and learn specialized heuristics for each class. Although we do not aim to compete with the state-of-the-art SAT solvers in run time, we demonstrate that the learned heuristics allow us to find satisfying assignments in fewer steps compared to a generic heuristic, and we provide analysis of our results through experiments.	[Yolcu, Emre; Poczos, Barnabas] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Yolcu, E (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	eyolcu@cs.cmu.edu; bapoczos@cs.cmu.edu						Amizadeh S., 2019, INT C LEARN REPR; Ansotegui Carlos, 2012, THEORY APPL SATISFIA, P410; Balint A., 2012, LNCS, P16, DOI DOI 10.1007/978-3-642-31612-8; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Battaglia Peter W, 2018, ARXIV180601261; BAYARDO RJ, 1997, P 14 NAT C ART INT A, P203; Bello I., 2017, ARXIV161109940; Bengio Y., 2018, ARXIV181106128; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Biere A., 2009, HDB SATISFIABILITY; Biere A., 2014, SER DEP COMPUTER SCI, VB-2014-2, P39; Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810; Cook S.A., 1971, P 3 ANN ACM S THEORY, P151, DOI [10.1145/800157.805047, DOI 10.1145/800157.805047]; Crawford JM, 1996, ARTIF INTELL, V81, P31, DOI 10.1016/0004-3702(95)00046-1; Een N, 2004, LECT NOTES COMPUT SC, V2919, P502, DOI 10.1007/978-3-540-24605-3_37; Erdos P., 1959, PUBL MATH-DEBRECEN, V6, P290, DOI DOI 10.2307/1999405; Frohlich A, 2015, AAAI CONF ARTIF INTE, P1136; Fukunaga AS, 2008, EVOL COMPUT, V16, P31, DOI 10.1162/evco.2008.16.1.31; Fukunaga AS, 2004, LECT NOTES COMPUT SC, V3103, P483; Gilmer J, 2017, PR MACH LEARN RES, V70; GORI M, 2005, IEEE IJCNN, P729, DOI DOI 10.1109/IJCNN.2005.1555942; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Illetskova Marketa, 2017, IEEE S SERIES COMPUT, P1, DOI 10.1109/SSCI.2017.8280953; Khalil Elias, 2017, ADV NEURAL INFORM PR, P1; KhudaBukhsh AR, 2016, ARTIF INTELL, V232, P20, DOI 10.1016/j.artint.2015.11.002; Kim JH, 2003, P 35 ANN ACM S THEOR, P213, DOI [10.1145/780542.780576, DOI 10.1145/780542.780576]; Kool Wouter, 7 INT C LEARN REPR I; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lagoudakis M. G., 2001, ELECT NOTES DISCRETE, V9, P344, DOI DOI 10.1016/S1571-0653(04)00332-4; Lauria M, 2017, LECT NOTES COMPUT SC, V10491, P464, DOI 10.1007/978-3-319-66263-3_30; Lederman Gil, 2019, ARXIV180708058; Li ZW, 2018, ADV NEUR IN, V31; Liang JH, 2017, LECT NOTES COMPUT SC, V10491, P119, DOI 10.1007/978-3-319-66263-3_8; Liang JH, 2016, AAAI CONF ARTIF INTE, P3434; Liang JH, 2016, LECT NOTES COMPUT SC, V9710, P123, DOI 10.1007/978-3-319-40970-2_9; Penrose M.D., 2003, RANDOM GEOMETRIC GRA; Russell S., 2009, ARTIF INTELL; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Selman B, 1996, ARTIF INTELL, V81, P17, DOI 10.1016/0004-3702(95)00045-3; Selman B, 1996, DIMACS SERIES DISCRE, P521, DOI [DOI 10.1090/DIMACS/026, 10.1090/dimacs/026/25, DOI 10.1090/DIMACS/026/25]; Selsam D, 2019, LECT NOTES COMPUT SC, V11628, P336, DOI 10.1007/978-3-030-24258-9_24; Selsam Daniel, 2019, 7 INT C LEARN REPR I; Silver D, 2018, SCIENCE, V362, P1140, DOI 10.1126/science.aar6404; Sutton R.S., 1998, INTRO REINFORCEMENT, DOI [10.1109/TNN.1998.712192, DOI 10.1109/TNN.1998.712192]; Tieleman Tijmen, 2012, SLIDES LECT NEURAL N; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xu L, 2008, J ARTIF INTELL RES, V32, P565, DOI 10.1613/jair.2490	48	17	17	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308006
C	Zhang, CQ; Han, ZB; Cui, YJ; Fu, HZ; Zhou, JT; Hu, QH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Changqing; Han, Zongbo; Cui, Yajie; Fu, Huazhu; Zhou, Joey Tianyi; Hu, Qinghua			CPM-Nets: Cross Partial Multi-View Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				FRAMEWORK	Despite multi-view learning progressed fast in past decades, it is still challenging due to the difficulty in modeling complex correlation among different views, especially under the context of view missing. To address the challenge, we propose a novel framework termed Cross Partial Multi-View Networks (CPM-Nets). In this framework, we first give a formal definition of completeness and versatility for multi-view representation and then theoretically prove the versatility of the latent representation learned from our algorithm. To achieve the completeness, the task of learning latent multi-view representation is specifically translated to degradation process through mimicking data transmitting, such that the optimal tradeoff between consistence and complementarity across different views could be achieved. In contrast with methods that either complete missing views or group samples according to view-missing patterns, our model fully exploits all samples and all views to produce structured representation for interpretability. Extensive experimental results validate the effectiveness of our algorithm over existing state-of-the-arts.	[Zhang, Changqing; Han, Zongbo; Cui, Yajie; Hu, Qinghua] Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China; [Zhang, Changqing; Hu, Qinghua] Tianjin Key Lab Machine Learning, Tianjin, Peoples R China; [Fu, Huazhu] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates; [Zhou, Joey Tianyi] ASTAR, Inst High Performance Comp, Singapore, Singapore	Tianjin University; Agency for Science Technology & Research (A*STAR); A*STAR - Institute of High Performance Computing (IHPC)	Zhou, JT (corresponding author), ASTAR, Inst High Performance Comp, Singapore, Singapore.	joey.tianyi.zhou@gmail.com	Fu, Huazhu/A-1411-2014	Fu, Huazhu/0000-0002-9702-5524	National Natural Science Foundation of China [61976151, 61602337, 61732011, 61702358]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was partly supported by National Natural Science Foundation of China (61976151, 61602337, 61732011, 61702358). We also appreciate the discussion with Ganbin Zhou and valuable comments from all the reviewers.	Akaho Shotaro, 2006, CS0609071 ARXIV; Andrew Galen, 2013, ICML; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Cai L, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1158, DOI 10.1145/3219819.3219963; Castrejon L, 2016, PROC CVPR IEEE, P2940, DOI 10.1109/CVPR.2016.321; Chen XH, 2012, PATTERN RECOGN, V45, P2005, DOI 10.1016/j.patcog.2011.11.008; Chung Y.-A., 2018, PROC INT C NEURAL IN, P7365; Dhillon Paramveer, 2011, ADV NEURAL INFORM PR, V24, P199; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar A., 2011, P 28 INT C MACH LEAR, P393, DOI DOI 10.5555/3104482.3104532; Kumar D, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1413; Le Lei, 2018, NIPS, P1; Lee TS, 1996, IEEE T PATTERN ANAL, V18, P959, DOI 10.1109/34.541406; Li SY, 2014, AAAI CONF ARTIF INTE, P1968; [刘威辰 Liu Weichen], 2017, [空军工程大学学报. 自然科学版, Journal of Air Force Engineering University. Natural Science Edition], V18, P1; Liu X., 2018, IEEE TPAMI; Mazumder R, 2010, J MACH LEARN RES, V11, P2287; Mingxia Liu, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9900, P308, DOI 10.1007/978-3-319-46720-7_36; Ngiam Jiquan, 2011, ICML, DOI DOI 10.5555/3104482.3104569; Quoc Le, 2014, P 31 INT C MACHINE L, V32, P1188; Rai P., 2010, P NEURAL INFORM PROC, P1; Rasiwasia N, 2010, ACM MM, DOI DOI 10.1145/1873951.1873987; Shang C, 2017, IEEE INT CONF BIG DA, P766, DOI 10.1109/BigData.2017.8257992; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Tran L., 2017, CVPR; Wah C., 2011, TECH REP; Wang S, 2015, INT CONF MACH LEARN, P883, DOI 10.1109/ICMLC.2015.7340670; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; White Martha, 2012, ADV NEURAL INFORM PR, P1682; Xu C., 2013, ARXIV13045634; Yang Z., 2019, IEEE T IMAGE PROCESS; Yuan Lei, 2012, KDD, P1149; Zhang CQ, 2017, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR.2017.461; Zhang CQ, 2017, IEEE T IMAGE PROCESS, V26, P648, DOI 10.1109/TIP.2016.2627806; Zhang CQ, 2015, IEEE I CONF COMP VIS, P1582, DOI 10.1109/ICCV.2015.185; Zhang H, 2017, PROC CVPR IEEE, P2925, DOI 10.1109/CVPR.2017.312; Zhang HT, 2011, RETROVIROLOGY, V8, DOI 10.1186/1742-4690-8-28; Zhao HD, 2017, AAAI CONF ARTIF INTE, P2921; Zhou JTY, 2019, J MACH LEARN RES, V20; Zhou JT, 2019, ARTIF INTELL, V275, P310, DOI 10.1016/j.artint.2019.06.001	45	17	18	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300051
C	Achille, A; Eccles, T; Matthey, L; Burgess, CP; Watters, N; Lerchner, A; Higgins, I		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Achille, Alessandro; Eccles, Tom; Matthey, Loic; Burgess, Christopher P.; Watters, Nick; Lerchner, Alexander; Higgins, Irina			Life-Long Disentangled Representation Learning with Cross-Domain Latent Homologies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CONNECTIONIST MODELS; MEMORY	Intelligent behaviour in the real-world requires the ability to acquire new knowledge from an ongoing sequence of experiences while preserving and reusing past knowledge. We propose a novel algorithm for unsupervised representation learning from piece-wise stationary visual data: Variational Autoencoder with Shared Embeddings (VASE). Based on the Minimum Description Length principle, VASE automatically detects shifts in the data distribution and allocates spare representational capacity to new knowledge, while simultaneously protecting previously learnt representations from catastrophic forgetting. Our approach encourages the learnt representations to be disentangled, which imparts a number of desirable properties: VASE can deal sensibly with ambiguous inputs, it can enhance its own representations through imagination-based exploration, and most importantly, it exhibits semantically meaningful sharing of latents between different datasets. Compared to baselines with entangled representations, our approach is able to reason beyond surface-level statistics and perform semantically meaningful cross-domain inference.	[Achille, Alessandro; Eccles, Tom; Matthey, Loic; Burgess, Christopher P.; Watters, Nick; Lerchner, Alexander; Higgins, Irina] Univ Calif Los Angeles, DeepMind, Los Angeles, CA 90024 USA	University of California System; University of California Los Angeles	Achille, A (corresponding author), Univ Calif Los Angeles, DeepMind, Los Angeles, CA 90024 USA.	achille@cs.ucla.edu; eccles@google.com; lmatthey@google.com; cpburgess@google.com; nwatters@google.com; lerchner@google.com; irinah@google.com						Achille A., 2017, P ICML WORKSH PRINC; Achille A., 2018, ANN REV CONTROL ROBO, V1; Achille A., 2018, IEEE T PATTERN ANAL, P1, DOI DOI 10.1109/TPAMI.2017.2784440; [Anonymous], 2016, PROGR NEURAL NETWORK; [Anonymous], 2014, ICLR; Ans B, 1997, CR ACAD SCI III-VIE, V320, P989, DOI 10.1016/S0764-4469(97)82472-9; Auchter A, 2017, FRONT BEHAV NEUROSCI, V11, DOI 10.3389/fnbeh.2017.00002; Beattie C., 2016, ARXIV161203801; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Burgess C. P., 2017, NIPS WORKSH LEARN DI; Cichon J, 2015, NATURE, V520, P180, DOI 10.1038/nature14251; Espeholt L., 2018, IMPALA SCALABLE DIST; ETCOFF NL, 1992, COGNITION, V44, P227, DOI 10.1016/0010-0277(92)90002-Y; Freedman DJ, 2001, SCIENCE, V291, P312, DOI 10.1126/science.291.5502.312; French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2; Furlanello T., 2016, ARXIV160602355; Garnelo M., 2016, P ADV NEUR INF PROC; Goodfellow I.J., 2013, EMPIRICAL INVESTIGAT; Grunwald P. D., 2007, MINIMUM DESCRIPTION; Gulyas B., 1993, WENNER GREN INT SERI; He K., 2015, ICCV; Higgins I., 2017, ICLR; Higgins I., 2017, ICML; Karaletsos T., 2016, ICLR; Kavukcuoglu K, 2015, ADV NEURAL INF PROCE, P2017; Kim H., 2017, DISENTANGLING FACTOR; Kingma D.P., 2015, INT C LEARN REPR, P1; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Kumar Abhishek, 2018, ICLR; Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu Y, 2008, J NEUROPHYSIOL, V100, P966, DOI 10.1152/jn.01354.2007; Liu Z., 2015, ICCV; MCCLELLAND JL, 1995, PSYCHOL REV, V102, P419, DOI 10.1037/0033-295X.102.3.419; Milan K., 2016, NIPS; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Necker L.A, 1832, LONDON EDINBURGH PHI, V1, P329, DOI 10.1080/14786443208647909; Nguyen C.V., 2018, INT C LEARNING REPRE; Parisotto E., 2015, ICLR; Przybyszewski AW, 1998, CURR BIOL, V8, pR135, DOI 10.1016/S0960-9822(98)70080-6; Ramapuram Jason, 2017, ARXIV170509847; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; Robins A., 1995, Connection Science, V7, P123, DOI 10.1080/09540099550039318; Ruvolo P., 2013, ICML; Schwarz J., 2018, ICML; Seff A., 2017, NIPS; Shin H., 2017, NIPS; Shwartz-Ziv Ravid, 2017, ARXIV170300810; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Xiao H., 2017, ARXIV170807747; Zenke Friedemann, 2017, ICML	55	17	17	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004043
C	Balasubramanian, K; Ghadimi, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Balasubramanian, Krishnakumar; Ghadimi, Saeed			Zeroth-order (Non)-Convex Stochastic Optimization via Conditional Gradient and Gradient Updates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we propose and analyze zeroth-order stochastic approximation algorithms for nonconvex and convex optimization. Specifically, we propose generalizations of the conditional gradient algorithm achieving rates similar to the standard stochastic gradient algorithm using only zeroth-order information. Furthermore, under a structural sparsity assumption, we first illustrate an implicit regularization phenomenon where the standard stochastic gradient algorithm with zeroth-order information adapts to the sparsity of the problem at hand by just varying the step-size. Next, we propose a truncated stochastic gradient algorithm with zeroth-order information, whose rate depends only poly-logarithmically on the dimensionality.	[Balasubramanian, Krishnakumar] Univ Calif Davis, Dept Stat, Davis, CA 95616 USA; [Ghadimi, Saeed] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA	University of California System; University of California Davis; Princeton University	Balasubramanian, K (corresponding author), Univ Calif Davis, Dept Stat, Davis, CA 95616 USA.	kbala@ucdavis.edu; sghadimi@princeton.edu						Bubeck S., 2015, FDN TRENDS MACHINE L; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Chen P.-Y., 2017, P AISEC CCS 2017, P15, DOI DOI 10.1145/3128572.3140448; Choromanski Krzysztof, 2018, P 35 INT C MACH LEAR; Conn AR, 2009, MOS-SIAM SER OPTIMIZ, V8, P1; Demyanov V F, 1970, APPROXIMATE METHODS; Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Ghadimi Saeed, 2018, MATH PROGRAMMING; Goodfellow I, 2016, DEEP LEARNING; Hazan E., 2012, ICML, P1843; Hazan E, 2016, PR MACH LEARN RES, V48; Hearn Donald, 1982, OPERATIONS RES LETT, V2; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Jain P, 2017, FOUND TRENDS MACH LE, V10, P142, DOI 10.1561/2200000058; Kar P., 2014, ADV NEURAL INFORM PR, P685; Lan GG, 2016, SIAM J OPTIMIZ, V26, P1379, DOI 10.1137/140992382; Mania Horia, 2018, ADV NEURAL INFORM PR; Mockus J., 2012, BAYESIAN APPROACH GL, V37; Mokhtari A., 2018, C ART INT STAT, V84, P1886; Mokhtari A., 2018, ARXIV180409554; Nemirovski A, 1983, WILEY INTERSCIENCE S, V15; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Nesterov Y, 2017, FOUND COMPUT MATH, V17, P527, DOI 10.1007/s10208-015-9296-2; Reddi SJ, 2016, ANN ALLERTON CONF, P1244, DOI 10.1109/ALLERTON.2016.7852377; Rubinstein Reuven Y., 2016, SIMULATION MONTE CAR, V10; Salimans T., 2017, ARXIV170303864; Shamir O., 2013, P C LEARN THEOR, P3; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Spall J.C, 2005, INTRO STOCHASTIC SEA, V65; Wang Y., 2018, INT C ART INT STAT A	34	17	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303045
C	Chan, J; Perrone, V; Spence, JP; Jenkins, PA; Mathieson, S; Song, YS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chan, Jeffrey; Perrone, Valerio; Spence, Jeffrey P.; Jenkins, Paul A.; Mathieson, Sara; Song, Yun S.			A Likelihood-Free Inference Framework for Population Genetic Data using Exchangeable Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				APPROXIMATE BAYESIAN COMPUTATION; STATISTICS	An explosion of high-throughput DNA sequencing in the past decade has led to a surge of interest in population-scale inference with whole-genome data. Recent work in population genetics has centered on designing inference methods for relatively simple model classes, and few scalable general-purpose inference techniques exist for more realistic, complex models. To achieve this, two inferential challenges need to be addressed: (1) population data are exchangeable, calling for methods that efficiently exploit the symmetries of the data, and (2) computing likelihoods is intractable as it requires integrating over a set of correlated, extremely high-dimensional latent variables. These challenges are traditionally tackled by likelihood-free methods that use scientific simulators to generate datasets and reduce them to hand-designed, permutation-invariant summary statistics, often leading to inaccurate inference. In this work, we develop an exchangeable neural network that performs summary statistic-free, likelihood-free inference. Our framework can be applied in a black-box fashion across a variety of simulation-based tasks, both within and outside biology. We demonstrate the power of our approach on the recombination hotspot testing problem, outperforming the state-of-the-art.	[Chan, Jeffrey; Spence, Jeffrey P.; Song, Yun S.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Perrone, Valerio; Jenkins, Paul A.] Univ Warwick, Warwick, England; [Mathieson, Sara] Swarthmore Coll, Swarthmore, PA 19081 USA	University of California System; University of California Berkeley; University of Warwick; Swarthmore College	Chan, J (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	chanjed@berkeley.edu; v.perrone@warwick.ac.uk; spence.jeffrey@berkeley.edu; p.jenkins@warwick.ac.uk; smathie1@swarthmore.edu; yss@berkeley.edu		Jenkins, Paul/0000-0001-7603-4205	NSF Graduate Research Fellowship; EPSRC [EP/L016710/1, EP/L018497/1]; NIH [R01-GM094402]; Packard Fellowship for Science and Engineering; Office of Science of the U.S. Department of Energy [DE-AC02-05CH11231]; NVIDIA Corporation	NSF Graduate Research Fellowship(National Science Foundation (NSF)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Packard Fellowship for Science and Engineering; Office of Science of the U.S. Department of Energy(United States Department of Energy (DOE)); NVIDIA Corporation	We thank Ben Graham for helpful discussions and Yuval Simons for his suggestion to use the decile. This research is supported in part by an NSF Graduate Research Fellowship (JC); EPSRC grants EP/L016710/1 (VP) and EP/L018497/1 (PJ); an NIH grant R01-GM094402 (JC, JPS, SM, and YSS); and a Packard Fellowship for Science and Engineering (YSS). YSS is a Chan Zuckerberg Biohub investigator. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research. This research also used resources of the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231.	Auton A., 2014, ARXIV14034264; Ba J., 2017, P 3 INT C LEARN REPR; Beaumont MA, 2002, GENETICS, V162, P2025; Blum MGB, 2010, STAT COMPUT, V20, P63, DOI 10.1007/s11222-009-9116-0; Boitard S, 2016, PLOS GENET, V12, DOI 10.1371/journal.pgen.1005877; Brutzkus A., 2017, P 34 INT C MACH LEAR, V70, P605; Fearnhead P, 2006, BIOINFORMATICS, V22, P3061, DOI 10.1093/bioinformatics/btl540; Fearnhead P, 2012, J R STAT SOC B, V74, P419, DOI 10.1111/j.1467-9868.2011.01010.x; Flagel Lex, 2018, BIORXIV; Gibbs RA, 2003, NATURE, V426, P789, DOI 10.1038/nature02168; Guo C., 2017, ARXIV170604599; Guttenberg N., 2016, ARXIV161204530; Hey J, 2004, PLOS BIOL, V2, P730, DOI 10.1371/journal.pbio.0020190; Jiang B., 2015, ARXIV151002175; Jin Chi, 2018, ARXIV180309357; Kamm JA, 2016, GENETICS, V203, P1381, DOI 10.1534/genetics.115.184820; Kelleher J, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004842; Kingman JFC., 1982, STOCHASTIC PROCESS A, V13, P235, DOI [10.1016/0304-4149(82)90011-4, DOI 10.1016/0304-4149(82)90011-4]; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402; Li J, 2006, AM J HUM GENET, V79, P628, DOI 10.1086/508066; Papamakarios G., 2016, ARXIV160506376; Pavlidis P, 2010, GENETICS, V185, P907, DOI 10.1534/genetics.110.116459; Petes TD, 2001, NAT REV GENET, V2, P360, DOI 10.1038/35072078; Ravanbakhsh S, 2016, ARXIV PREPRINT ARXIV; Schrider DR, 2015, GENOME BIOL EVOL, V7, P3511, DOI 10.1093/gbe/evv228; Sheehan S, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004845; SHIVASWAMY PK, 2006, P 23 INT C MACH LEAR, P817; Sousa VC, 2009, GENETICS, V181, P1507, DOI 10.1534/genetics.108.098129; Wall J. D., 2016, G3 GENES GENOMES GEN; Wang Y, 2009, P NATL ACAD SCI USA, V106, P6215, DOI 10.1073/pnas.0900418106; Wegmann D, 2009, GENETICS, V182, P1207, DOI 10.1534/genetics.109.102509; Zaheer M., 2017, NEURAL INFORM PROCES	33	17	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA	33244210				2022-12-19	WOS:000461852003018
C	Denevi, G; Ciliberto, C; Stamos, D; Pontil, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Denevi, Giulia; Ciliberto, Carlo; Stamos, Dimitris; Pontil, Massimiliano			Learning To Learn Around A Common Mean	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				BOUNDS	The problem of learning-to-learn (LTL) or meta-learning is gaining increasing attention due to recent empirical evidence of its effectiveness in applications. The goal addressed in LTL is to select an algorithm that works well on tasks sampled from a meta-distribution. In this work, we consider the family of algorithms given by a variant of Ridge Regression, in which the regularizer is the square distance to an unknown mean vector. We show that, in this setting, the LTL problem can be reformulated as a Least Squares (LS) problem and we exploit a novel meta-algorithm to efficiently solve it. At each iteration the meta-algorithm processes only one dataset. Specifically, it firstly estimates the stochastic LS objective function, by splitting this dataset into two subsets used to train and test the inner algorithm, respectively. Secondly, it performs a stochastic gradient step with the estimated value. Under specific assumptions, we present a bound for the generalization error of our meta-algorithm, which suggests the right splitting parameter to choose. When the hyper-parameters of the problem are fixed, this bound is consistent as the number of tasks grows, even if the sample size is kept constant. Preliminary experiments confirm our theoretical findings, highlighting the advantage of our approach, with respect to independent task learning.	[Denevi, Giulia; Pontil, Massimiliano] Ist Italiano Tecnol, Genoa, Italy; [Denevi, Giulia] Univ Genoa, Genoa, Italy; [Ciliberto, Carlo] Imperial Coll London, London, England; [Ciliberto, Carlo; Stamos, Dimitris; Pontil, Massimiliano] UCL, London, England	Istituto Italiano di Tecnologia - IIT; University of Genoa; Imperial College London; University of London; University College London	Denevi, G (corresponding author), Ist Italiano Tecnol, Genoa, Italy.; Denevi, G (corresponding author), Univ Genoa, Genoa, Italy.		Denevi, Giulia/AAF-9385-2021	Denevi, Giulia/0000-0001-7181-9660; /0000-0003-0249-5273	UK Defence Science and Technology Laboratory (Dstl); Engineering and Physical Research Council (EPSRC) [EP/P009069/1]	UK Defence Science and Technology Laboratory (Dstl); Engineering and Physical Research Council (EPSRC)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported in part by the UK Defence Science and Technology Laboratory (Dstl) and Engineering and Physical Research Council (EPSRC) under grant EP/P009069/1. This is part of the collaboration between US DOD, UK MOD and UK EPSRC under the Multidisciplinary University Research Initiative.	Alquier P, 2017, PR MACH LEARN RES, V54, P261; [Anonymous], 2017, P CVPR; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Balcan, 2015, C LEARN THEOR, P191; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Bhatia Rajendra, 1997, MATRIX ANAL, DOI 10.1007/978-1-4612-0653-8; Camoriano R., 2017, INT C ROB AUT; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Cavallanti G, 2010, J MACH LEARN RES, V11, P2901; Ciliberto C., 2017, ADV NEURAL INFORM PR, P1986; Ciliberto C, 2015, PROC CVPR IEEE, P131, DOI 10.1109/CVPR.2015.7298608; Crammer K, 2006, J MACH LEARN RES, V7, P551; Denevi G., 2018, P 34 C UNC ART INT U; Dieuleveut A., 2017, J MACHINE LEARNING R, V18, P3520; Evgeniou T, 2005, J MACH LEARN RES, V6, P615; Finn C, 2017, PR MACH LEARN RES, V70; Franceschi L., 2018, INT C MACH LEARN, V80, P568; Herbster M., 2016, P ADV NEURAL INFORM, P3954; Jacob L., 2009, P 21TH NIPS, P745; Maurer A, 2005, J MACH LEARN RES, V6, P967; Maurer A., 2013, INT C MACH LEARN; Maurer A, 2016, J MACH LEARN RES, V17; Maurer A, 2009, MACH LEARN, V75, P327, DOI 10.1007/s10994-009-5109-7; McDonald A. M., 2016, J MACHINE LEARNING R, V17, P1; Pentina A, 2016, ADV NEUR IN, V29; Pentina A, 2014, PR MACH LEARN RES, V32, P991; Ravi S., 2017, I5 INT C LEARN REPR; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Thrun Sebastian, 1998, LEARNING LEARN, P181, DOI 10.1007/978-1-4615-5529-2; Wu Y., 2014, ROBOTICS AUTONOMOUS	30	17	17	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004070
C	Dziugaite, GK; Roy, DM		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dziugaite, Gintare Karolina; Roy, Daniel M.			Data-dependent PAC-Bayes priors via differential privacy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				BOUNDS	The Probably Approximately Correct (PAC) Bayes framework (McAllester, 1999) can incorporate knowledge about the learning algorithm and (data) distribution through the use of distribution-dependent priors, yielding tighter generalization bounds on data-dependent posteriors. Using this flexibility, however, is difficult, especially when the data distribution is presumed to be unknown. We show how an e-differentially private data-dependent prior yields a valid PAC-Bayes bound, and then show how non-private mechanisms for choosing priors can also yield generalization bounds. As an application of this result, we show that a Gaussian prior mean chosen via stochastic gradient Langevin dynamics (SGLD; Welling and Teh, 2011) leads to a valid PAC-Bayes bound given control of the 2-Wasserstein distance to an epsilon-differentially private stationary distribution. We study our data-dependent bounds empirically, and show that they can be nonvacuous even when other distribution-dependent bounds are vacuous.	[Dziugaite, Gintare Karolina] Univ Cambridge, Element AI, Cambridge, England; [Roy, Daniel M.] Univ Toronto, Vector Inst, Toronto, ON, Canada	University of Cambridge; University of Toronto	Dziugaite, GK (corresponding author), Univ Cambridge, Element AI, Cambridge, England.				EPSRC studentship; NSERC Discovery Grant; Ontario Early Researcher Award	EPSRC studentship(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); NSERC Discovery Grant(Natural Sciences and Engineering Research Council of Canada (NSERC)); Ontario Early Researcher Award(Ministry of Research and Innovation, Ontario)	The authors would like to thank Olivier Catoni, Pascal Germain, Mufan Li, David McAllester, and Alexander Rakhlin, John Shawe-Taylor, for helpful discussions. This research was carried out in part while the authors were visiting the Simons Institute for the Theory of Computing at UC Berkeley. GKD was additionally supported by an EPSRC studentship. DMR was additionally supported by an NSERC Discovery Grant and Ontario Early Researcher Award.	Alquier P, 2018, MACH LEARN, V107, P887, DOI 10.1007/s10994-017-5690-0; BARTLETT P., 2017, SPECTRALLY NORMALIZE; Bassily R., 2014, ARXIV14057085V2CSLG; Bassily R, 2016, ACM S THEORY COMPUT, P1046, DOI 10.1145/2897518.2897566; Begin L, 2016, JMLR WORKSH CONF PRO, V51, P435; Ben London, 2017, ADV NEURAL INFORM PR, P2931; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Catoni O., 2007, LECT NOTES MONOGRAPH, DOI [10.1214/074921707000000391, DOI 10.1214/074921707000000391]; Dimitrakakis C, 2014, LECT NOTES ARTIF INT, V8776, P291, DOI 10.1007/978-3-319-11662-4_21; Dwork C., 2015, ADV NEURAL INF PROCE, P2350; Dwork C, 2008, LECT NOTES COMPUT SC, V4978, P1, DOI 10.1007/978-3-540-79228-4_1; Dwork C, 2006, LECT NOTES COMPUT SC, V4052, P1; Dwork C, 2015, ACM S THEORY COMPUT, P117, DOI 10.1145/2746539.2746580; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dziugaite G. K., 2017, P 33 C UNC ART INT U; Erdogdu M. A., 2018, ARXIV181012361; Germain P., 2016, ADV NEURAL INFORM PR, P1884; Grunwald P. D., 2017, ARXIV171007732; Igel C., 2017, MACHINE LEARNING RES, P466; Kifer D., 2012, J MACH LEARN RES WOR, V23; Langford J., 2001, CMUCS01102; Langford J., 2002, THESIS; Lever G, 2013, THEOR COMPUT SCI, V473, P4, DOI 10.1016/j.tcs.2012.10.013; Mattingly JC, 2002, STOCH PROC APPL, V101, P185, DOI 10.1016/S0304-4149(02)00150-3; Maurer A., 2004, ARXIVCS0411099CSLG; McAllester D. A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P164, DOI 10.1145/307400.307435; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Mehta N. A, 2016, ARXIV160500252; Minami K., 2016, ADV NEURAL INFORM PR, P956; Mir D. J., 2013, THESIS; Neyshabur B., 2017, P INT C LEARN REPR I; Oneto L, 2017, PATTERN RECOGN LETT, V89, P31, DOI 10.1016/j.patrec.2017.02.006; Parrado-Hernandez E, 2012, J MACH LEARN RES, V13, P3507; Raginsky M., 2017, P C LEARN THEOR COLT; Rivasplata O., 2018, ADV NEURAL INFO P SY, V31, P9234; Shawe-Taylor J., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P2, DOI 10.1145/267460.267466; Smith S.L., 2018, P INT C LEARN REPR I; Vollmer SJ, 2016, J MACH LEARN RES, V17, P1; Wang YX, 2015, PR MACH LEARN RES, V37, P2493; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3	41	17	17	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003003
C	Hashimoto, TB; Guu, K; Oren, Y; Liang, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hashimoto, Tatsunori B.; Guu, Kelvin; Oren, Yonatan; Liang, Percy			A Retrieve-and-Edit Framework for Predicting Structured Outputs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					For the task of generating complex outputs such as source code, editing existing outputs can be easier than generating complex outputs from scratch. With this motivation, we propose an approach that first retrieves a training example based on the input (e.g., natural language description) and then edits it to the desired output (e.g., code). Our contribution is a computationally efficient method for learning a retrieval model that embeds the input in a task-dependent way without relying on a hand-crafted metric or incurring the expense of jointly training the retriever with the editor. Our retrieve-and-edit framework can be applied on top of any base model. We show that on a new autocomplete task for GitHub Python code and the Hearthstone cards benchmark, retrieve-and-edit significantly boosts the performance of a vanilla sequence-to-sequence model on both tasks.	[Hashimoto, Tatsunori B.; Oren, Yonatan; Liang, Percy] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Guu, Kelvin] Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Stanford University; Stanford University	Hashimoto, TB (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	thashim@stanford.edu; kguu@stanford.edu; yonatano@stanford.edu; pliang@cs.stanford.edu			DARPA CwC program under ARO prime [W911NF-15-1-0462]	DARPA CwC program under ARO prime	This work was funded by the DARPA CwC program under ARO prime contract no. W911NF-15-1-0462.	Allamanis M, 2015, PR MACH LEARN RES, V37, P2123; Andrew G., 2013, INT C MACH LEARN, p1247?1255; [Anonymous], 2013, NAACL HLT 2013; [Anonymous], 2013, P 51 ANN M ASS COMP; Bahdanau Dzmitry, 2015, ICLR 2015; Balog M., 2016, DEEPCODER LEARNING W; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Chaidaroon S, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P75, DOI 10.1145/3077136.3080816; Cho K., 2014, P 2014 C EMP METH NA, P1724; Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902; Goldenshluger A., 1997, MATH METH STAT, V6, P135; Gu J., 2016, ASS COMPUTATIONAL LI; Gu J., 2017, ARXIV170507267; Guu Kelvin, 2018, T ASS COMPUTATIONAL; Hayati Shirley Anugrah, 2018, P 2018 C EMP METH NA, P925, DOI 10.18653/v1/D18-1111; Hinton G.E., 2011, P EUR S ART NEUR NET, P489; Kipf T., 2018, ARXIV180400891; Lei Tao, 2016, P 2016 C N AM CHAPT, P1279, DOI DOI 10.18653/V1/N16-1153; Li J, 2017, P 2017 C EMP METH NA, P2157; Liang P., 2010, P 27 INT C MACHINE L, P639; Ling W, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P599; Maddison CJ, 2014, PR MACH LEARN RES, V32, P649; Mason R., 2014, COMPUTATIONAL NATURA, P2; Papineni Kishore, 2002, ASS COMPUTATIONAL LI; Rabinovich M., 2017, ASS COMPUTATIONAL LI; Severyn A, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P373, DOI 10.1145/2766462.2767738; Shao Yuanlong, 2017, P 2017 C EMP METH NA, P2210, DOI DOI 10.18653/V1/D17-1235; Shen DH, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2041; Song Yiping, 2016, ARXIV161007149; Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI [DOI 10.1162/NEC0_A_00311, DOI 10.1109/CVPR.2013.49]; Sumita E., 1991, ASS COMPUTATIONAL LI; Sun W., 2018, ARXIV180706473; Tan Ming, 2015, ARXIV151104108; Wu Y., 2016, GOOGLES NEURAL MACHI; Wu Yu, 2018, ARXIV180607042; Xu J., 2018, EMPIRICAL METHODS NA; Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966; Yan R, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P55, DOI 10.1145/2911451.2911542; Yin PC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P440, DOI 10.18653/v1/P17-1041	41	17	17	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004059
C	Klys, J; Snell, J; Zemel, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Klys, Jack; Snell, Jake; Zemel, Richard			Learning Latent Subspaces in Variational Autoencoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Variational autoencoders (VAEs) [10, 20] are widely used deep generative models capable of learning unsupervised latent representations of data. Such representations are often difficult to interpret or control. We consider the problem of unsupervised learning of features correlated to specific labels in a dataset. We propose a VAE-based generative model which we show is capable of extracting features correlated to binary labels in the data and structuring it in a latent subspace which is easy to interpret. Our model, the Conditional Subspace VAE (CSVAE), uses mutual information minimization to learn a low-dimensional latent subspace associated with each label that can easily be inspected and independently manipulated. We demonstrate the utility of the learned representations for attribute manipulation tasks on both the Toronto Face [23] and CelebA [15] datasets.	[Klys, Jack; Snell, Jake; Zemel, Richard] Univ Toronto, Vector Inst, Toronto, ON, Canada	University of Toronto	Klys, J (corresponding author), Univ Toronto, Vector Inst, Toronto, ON, Canada.	jackklys@cs.toronto.edu; jsnell@cs.toronto.edu; zemel@cs.toronto.edu			Samsung; Natural Sciences and Engineering Research Council of Canada	Samsung(Samsung); Natural Sciences and Engineering Research Council of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)CGIAR)	We would like to thank Sageev Oore for helpful discussions. This research was supported by Samsung and the Natural Sciences and Engineering Research Council of Canada.	Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Creswell A., 2017, ARXIV171105175; Edwards Harrison, 2016, INT C LEARN REPR ICL, P3; Ganin Y., 2016, JMLR, V17, P2096; Gomez-Bombarelli R, 2018, ACS CENTRAL SCI, V4, P268, DOI 10.1021/acscentsci.7b00572; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I., 2016, P INT C LEARN REPR; Higgins I., 2017, P INT C LEARN REPR T; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kulkarni TD, 2015, ADV NEUR IN, V28; Lample Guillaume, 2017, ARXIV170600409; Larsen A. B. L., 2015, ARXIV PREPRINT ARXIV; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Louizos C., 2015, ARXIV PREPRINT ARXIV; Mathieu M., 2016, DISENTANGLING FACTOR; Paszke A., 2017, AUTOMATIC DIFFERENTI; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Sohn Kihyuk, 2015, ADV NEURAL INFORM PR, P3483, DOI DOI 10.5555/2969442.2969628; Springenberg Jost Tobias, 2015, ARXIV151106390; Susskind JM., 2010, TORONTO FACE DATABAS, P3; Xiao T., 2018, INT C LEARN REPR WOR; Xiao T., 2018, ARXIV180310562; Zhou Shuchang, 2017, ARXIV170504932	26	17	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001002
C	Kumar, A; Gupta, S; Fouhey, D; Levine, S; Malik, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kumar, Ashish; Gupta, Saurabh; Fouhey, David; Levine, Sergey; Malik, Jitendra			Visual Memory for Robust Path Following	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Humans routinely retrace paths in a novel environment both forwards and backwards despite uncertainty in their motion. This paper presents an approach for doing so. Given a demonstration of a path, a first network generates a path abstraction. Equipped with this abstraction, a second network observes the world and decides how to act to retrace the path under noisy actuation and a changing environment. The two networks are optimized end-to-end at training time. We evaluate the method in two realistic simulators, performing path following and homing under actuation noise and environmental changes. Our experiments show that our approach outperforms classical approaches and other learning based baselines.	[Kumar, Ashish; Gupta, Saurabh; Fouhey, David; Levine, Sergey; Malik, Jitendra] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Kumar, A (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	ashish_kumar@berkeley.edu; sgupta@eecs.berkeley.edu; dfouhey@eecs.berkeley.edu; svlevine@eecs.berkeley.edu; malik@eecs.berkeley.edu						Anderson Peter, 2018, EVALUATION EMBODIED, V1; Armeni Iro, 2016, CVPR; Axelrod  Brian, 2017, RSS; Bowman S., 2017, ICRA; Brahmbhatt Samarth, 2018, CVPR; Canny, 1988, COMPLEXITY ROBOT MOT; Chang Angel, 2017, 3DV; Chaplot Devendra Singh, 2018, ICLR; Clement  Lee, 2017, JFR; Cummins M., 2008, IJRR; Daftry S., 2016, ISER; Davison A., 1998, ECCV; Engel J., 2014, ECCV; Fuentes-Pacheco Jorge, 2015, ARTIFICIAL INTELLIGE; Furgale  Paul, 2010, JFR; Gandhi Dhiraj, 2017, IROS; Giusti A., 2016, RAL; Izadi S., 2011, UIST; Kahn G., 2017, ARXIV170910489; Kar Abhishek, 2017, NIPS; Kavraki Lydia E, 1996, RA; Kendall Alex, 2015, ICCV; Khan A., 2018, INT C LEARN REPR ICL; Koltun, 2018, ICLR, P1; LaValle S. M., 2000, RAPIDLY EXPLORING RA; Levine S., 2017, CVPR; Lowe D. G., 2004, IJCV; Mirowski P., 2018, ARXIV180400168; Mirowski Piotr, 2017, INT C LEARN REPR; Nister D., 2004, CVPR; Parisotto E., 2018, ICLR; Pathak D., 2018, ICLR; Pillai S, 2017, ARXIV170510279; Pomerleau D. A., 1989, NIPS; Sadeghi F., 2017, RSS; Schonberger J. L., 2018, CVPR; Schonberger Johannes Lutz, 2016, ECCV; Schonberger Johannes Lutz, 2016, CVPR; Song Shuran, 2018, P CVPR; Swedish  Tristan, 2018, CVPR; Zhang Jingwei, 2017, ARXIV170609520; Zhou T., 2017, CVPR; Zhu Y., 2017, ICRA	43	17	17	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300071
C	Samadi, S; Tantipongpipat, U; Morgenstern, J; Singh, M; Vempala, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Samadi, Samira; Tantipongpipat, Uthaipon; Morgenstern, Jamie; Singh, Mohit; Vempala, Santosh			The Price of Fair PCA: One Extra Dimension	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DISCRIMINATION	We investigate whether the standard dimensionality reduction technique of PCA inadvertently produces data representations with different fidelity for two different populations. We show on several real-world data sets, PCA has higher reconstruction error on population A than on B (for example, women versus men or lower-versus higher-educated individuals). This can happen even when the data set has a similar number of samples from A and B. This motivates our study of dimensionality reduction techniques which maintain similar fidelity for A and B. We define the notion of Fair PCA and give a polynomial-time algorithm for finding a low dimensional representation of the data which is nearly-optimal with respect to this measure. Finally, we show on real-world data sets that our algorithm can be used to efficiently generate a fair low dimensional representation of the data.	[Samadi, Samira; Tantipongpipat, Uthaipon; Morgenstern, Jamie; Singh, Mohit; Vempala, Santosh] Georgia Tech, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Samadi, S (corresponding author), Georgia Tech, Atlanta, GA 30332 USA.	ssamadi6@gatech.edu; tao@gatech.edu; jamiemmt.cs@gatech.edu; mohitsinghr@gmail.com; vempala@cc.gatech.edu			NSF [CCF-1563838, CCF-1717349, CCF-1717947]	NSF(National Science Foundation (NSF))	This work was supported in part by NSF awards CCF-1563838, CCF-1717349, and CCF-1717947.	Adler P, 2016, IEEE DATA MINING, P1, DOI [10.1109/ICDM.2016.158, 10.1109/ICDM.2016.0011]; Afifi M., 2017, ARXIV170604277; Angwin Julia, 2018, MACHINE BIAS PROPUBL; [Anonymous], 2001, LECT MODERN CONVEX O; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Aswani A.., 2018, ARXIV180203765; Beutel A, 2017, P WORKSH FAIRN ACC T; Buolamwini J., 2018, C FAIRN ACC TRANSP, P77; Calders T, 2010, DATA MIN KNOWL DISC, V21, P277, DOI 10.1007/s10618-010-0190-x; Calmon F., 2017, P 31 INT C NEUR INF, P3992; Chouldechova A, 2017, BIG DATA, V5, P153, DOI 10.1089/big.2016.0047; Crawford Kate, 2017, TROUBLE BIAS; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Ensign Danielle, 2017, WORKSH FAIRN ACC TRA; Ensign Danielle, 2017, ARXIV170609847; Fish B., 2016, P 2016 SIAM INT C DA, P144; Gebru Timnit, 2018, ARXIV180309010; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; Huang G.B., 2008, WORKSHOP FACESREAL L; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; Joseph Matthew, 2016, NIPS, P325; Kamiran Faisal, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P869, DOI 10.1109/ICDM.2010.50; Kamiran F, 2012, IEEE DATA MINING, P924, DOI 10.1109/ICDM.2012.45; Kamiran F, 2012, KNOWL INF SYST, V33, P1, DOI 10.1007/s10115-011-0463-8; Kamishima Toshihiro, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P35, DOI 10.1007/978-3-642-33486-3_3; Kannan S, 2017, EC'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P369, DOI 10.1145/3033274.3085154; Kay M, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3819, DOI 10.1145/2702123.2702520; Kleinberg Jon, 2016, P 8 INN THEOR COMP S; Lau L. C., 2011, ITERATIVE METHODS CO, V46; Lipton Zachary C., 2017, ARXIV171107076; Luong Binh Thanh, 2011, P 17 ACM SIGKDD INT, P502; Madras D, 2018, PR MACH LEARN RES, V80; Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720; Pedreshi D, 2008, P 14 ACM SIGKDD INT, P560, DOI DOI 10.1145/1401890.1401959; Schrijver A., 1998, THEORY LINEAR INTEGE; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Simonite Tom., 2018, IT COMES GORILLAS GO; Sweeney L, 2013, COMMUN ACM, V56, P44, DOI 10.1145/2447976.2447990; Yeh IC, 2009, EXPERT SYST APPL, V36, P2473, DOI 10.1016/j.eswa.2007.12.020; Zafar Muhammad, 2015, CORR; Zemel R., 2013, P INT C MACH LEARN, P325; Zhang B., 2018, ARXIV180107593	45	17	17	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005055
C	Bhatia, K; Jain, P; Kamalaruban, P; Kar, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Bhatia, Kush; Jain, Prateek; Kamalaruban, Parameswaran; Kar, Purushottam			Consistent Robust Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				LEAST TRIMMED SQUARES	We present the first efficient and provably consistent estimator for the robust regression problem. The area of robust learning and optimization has generated a significant amount of interest in the learning and statistics communities in recent years owing to its applicability in scenarios with corrupted data, as well as in handling model mis-specifications. In particular, special interest has been devoted to the fundamental problem of robust linear regression where estimators that can tolerate corruption in up to a constant fraction of the response variables are widely studied. Surprisingly however, to this date, we are not aware of a polynomial time estimator that offers a consistent estimate in the presence of dense, unbounded corruptions. In this work we present such an estimator, called CRR. This solves an open problem put forward in the work of [3]. Our consistency analysis requires a novel two-stage proof technique involving a careful analysis of the stability of ordered lists which may be of independent interest. We show that CRR not only offers consistent estimates, but is empirically far superior to several other recently proposed algorithms for the robust regression problem, including extended Lasso and the TORRENT algorithm. In comparison, CRR offers comparable or better model recovery but with runtimes that are faster by an order of magnitude.	[Bhatia, Kush] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Jain, Prateek] Microsoft Res, Bangalore, Karnataka, India; [Kamalaruban, Parameswaran] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Kar, Purushottam] Indian Inst Technol, Kanpur, Uttar Pradesh, India	University of California System; University of California Berkeley; Microsoft; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Kanpur	Bhatia, K (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	kushbhatia@berkeley.edu; prajain@microsoft.com; kamalaruban.parameswaran@epfl.ch; purushot@cse.iitk.ac.in	Jeong, Yongwook/N-7413-2016; Kar, Purushottam/W-8113-2019	Kar, Purushottam/0000-0003-2096-5267	Research-I Foundation at IIT Kanpur; Microsoft Research India; NSF [IIS-1619362]; Tower Research for research grants; Deep Singh and Daljeet Kaur Faculty Fellowship	Research-I Foundation at IIT Kanpur; Microsoft Research India(Microsoft); NSF(National Science Foundation (NSF)); Tower Research for research grants; Deep Singh and Daljeet Kaur Faculty Fellowship	The authors thank the reviewers for useful comments. PKar is supported by the Deep Singh and Daljeet Kaur Faculty Fellowship and the Research-I Foundation at IIT Kanpur, and thanks Microsoft Research India and Tower Research for research grants. KB gratefully acknowledges the support of the NSF through grant IIS-1619362.	Bhatia Kush, 2015, P 29 ANN C NEUR INF; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Charikar M., 2016, ARXIV161102315CSLG; Chen Yin, 2012, P 26 ANN C NEUR INF; Chen Yudong, 2013, P 30 INT C MACH LEAR; Cherapanamjeri Y., 2016, ARXIV160607315CSLG; Cucker F, 2002, B AM MATH SOC, V39, P1; Davenport M. A., 2009, TREE0906 RIC U DEP E; Garg R., 2009, P 26 INT C MACH LEAR; Jain P., 2014, P 28 ANN C NEUR INF; McWilliams Brian, 2014, 28 ANN C NEUR INF PR; Nasrabadi Nasser M., 2011, ADV NEURAL INFORM PR, P1881; Nguyen NH, 2013, IEEE T INFORM THEORY, V59, P2036, DOI 10.1109/TIT.2012.2232347; Nguyen NH, 2013, IEEE T INFORM THEORY, V59, P2017, DOI 10.1109/TIT.2013.2240435; Rousseeuw P.J., 1987, ROBUST REGRESSION OU; ROUSSEEUW PJ, 1984, J AM STAT ASSOC, V79, P871, DOI 10.2307/2288718; Studer C, 2012, IEEE T INFORM THEORY, V58, P3115, DOI 10.1109/TIT.2011.2179701; Visek JA, 2006, KYBERNETIKA, V42, P1; Visek JA, 2006, KYBERNETIKA, V42, P181; Wright J, 2010, IEEE T INFORM THEORY, V56, P3540, DOI 10.1109/TIT.2010.2048473; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Xu H., 2014, ADV NEURAL INFORM PR, V27; Yang AY, 2013, IEEE T IMAGE PROCESS, V22, P3234, DOI 10.1109/TIP.2013.2262292	23	17	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402016
C	Dann, C; Lattimore, T; Brunskill, E		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dann, Christoph; Lattimore, Tor; Brunskill, Emma			Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Statistical performance bounds for reinforcement learning (RL) algorithms can be critical for high-stakes applications like healthcare. This paper introduces a new framework for theoretically measuring the performance of such algorithms called Uniform-PAC, which is a strengthening of the classical Probably Approximately Correct (PAC) framework. In contrast to the PAC framework, the uniform version may be used to derive high probability regret guarantees and so forms a bridge between the two setups that has been missing in the literature. We demonstrate the benefits of the new framework for finite-state episodic MDPs with a new algorithm that is Uniform-PAC and simultaneously achieves optimal regret and PAC guarantees except for a factor of the horizon.	[Dann, Christoph] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Brunskill, Emma] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Lattimore, Tor] DeepMind, London, England	Carnegie Mellon University; Stanford University	Dann, C (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	cdann@cdann.net; tor.lattimore@gmail.com; ebrun@cs.stanford.edu	Jeong, Yongwook/N-7413-2016		NSF CAREER award	NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD))	We appreciate the support of a NSF CAREER award and a gift from Yahoo.	Agarwal Alekh, 2014, J MACHINE LEARNING R, V32; Audibert JY, 2009, THEOR COMPUT SCI, V410, P1876, DOI 10.1016/j.tcs.2009.01.016; Auer P, 2000, ANN IEEE SYMP FOUND, P270, DOI 10.1109/SFCS.2000.892116; Auer Peter, 2005, P 1 AUSTR COGN VIS W; Azar MG, 2017, PR MACH LEARN RES, V70; Balsubramani Akshay, 2016, UNCERTAINTY ARTIFICI; BARTLETT PL, 2009, P 25 C UNC ART INT, P35; Boucheron S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Bubeck Sebastien, 2012, REGRET ANAL STOCHAST, P138; Dann Christoph, 2015, NEURAL INFORM PROCES; Durrett R., 2010, PROBABILITY THEORY E, V4th ed., DOI 10.1017/CBO9780511779398; Garivier Aurelien, 2016, ADV NEURAL INFORM PR; Garivier Aurelien, 2011, C LEARN THEOR; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Jamieson Kevin, 2013, LIL UCB OPTIMAL EXPL; Jiang N, 2017, PR MACH LEARN RES, V70; Lattimore Tor, 2014, THEORETICAL COMPUTER, V558; Li LH, 2011, MACH LEARN, V82, P399, DOI 10.1007/s10994-010-5225-4; Massart P, 2007, LECT NOTES MATH, V1896, P1, DOI 10.1007/978-3-540-48503-2; oisLavet Vincent Franc, 2015, ARXIV; Pazis J, 2016, AAAI CONF ARTIF INTE, P1977; Srinivas N, 2012, IEEE T INFORM THEORY, V58, P3250, DOI 10.1109/TIT.2011.2182033; Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009; Strehl AL, 2009, J MACH LEARN RES, V10, P2413; Szepesvari C., 2010, INT C MACH LEARN; Weissman T., 2003, HPL200397	26	17	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405077
C	Eleftheriadis, S; Nicholson, TFW; Deisenroth, MP; Hensman, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Eleftheriadis, Stefanos; Nicholson, Thomas F. W.; Deisenroth, Marc P.; Hensman, James			Identification of Gaussian Process State Space Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The Gaussian process state space model (GPSSM) is a non-linear dynamical system, where unknown transition and/or measurement mappings are described by GPs. Most research in GPSSMs has focussed on the state estimation problem, i.e., computing a posterior of the latent state given the model. However, the key challenge in GPSSMs has not been satisfactorily addressed yet: system identification, i.e., learning the model. To address this challenge, we impose a structured Gaussian variational posterior distribution over the latent states, which is parameterised by a recognition model in the form of a bi-directional recurrent neural network. Inference with this structure allows us to recover a posterior smoothed over sequences of data. We provide a practical algorithm for efficiently computing a lower bound on the marginal likelihood using the reparameterisation trick. This further allows for the use of arbitrary kernels within the GPSSM. We demonstrate that the learnt GPSSM can efficiently generate plausible future trajectories of the identified system after only observing a small number of episodes from the true system.	[Eleftheriadis, Stefanos; Nicholson, Thomas F. W.; Deisenroth, Marc P.; Hensman, James] PROWLER Io, Cambridge CB2 1LA, England; [Deisenroth, Marc P.] Imperial Coll London, London, England	Imperial College London	Eleftheriadis, S (corresponding author), PROWLER Io, Cambridge CB2 1LA, England.	stefanos@prowler.io; tom@prowler.io; marc@prowler.io; james@prowler.io	Jeong, Yongwook/N-7413-2016		Google faculty research award	Google faculty research award(Google Incorporated)	Marc P. Deisenroth has been supported by a Google faculty research award.	Al-Shedivat M., 2016, ARXIV161008936; Beal M.J, 2003, THESIS; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Brown EN, 1998, J NEUROSCI, V18, P7411; Calandra R, 2016, IEEE IJCNN, P3338, DOI 10.1109/IJCNN.2016.7727626; Cesar Lincoln C., 2015, INT C LEARN REPR; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Cho Youngmin, 2009, P ADV NEUR INF PROC, V22, P342; Dai Zhenwen, 2015, INT C LEARN REPR; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Deisenroth M.P., 2009, ANN INT C MACH LEARN, P225, DOI 10.1145/1553374.1553403; Deisenroth Marc P., 2012, ADV NEURAL INFORM PR, P2618; Deisenroth MP, 2015, IEEE T PATTERN ANAL, V37, P408, DOI 10.1109/TPAMI.2013.218; Deisenroth MP, 2012, IEEE T AUTOMAT CONTR, V57, P1865, DOI 10.1109/TAC.2011.2179426; Frigola R., 2015, THESIS; Frigola R., 2014, ADV NEURAL INFORM PR, V27, P3680; Frigola R., 2013, ADV NEURAL INFORM PR, V26, P3156; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Kingma D.P, P 3 INT C LEARNING R; Ko J, 2009, AUTON ROBOT, V27, P75, DOI 10.1007/s10514-009-9119-x; Ko Jonathan, 2009, ROBOTICS SCI SYSTEMS; Kocijan J, 2016, ADV IND CONTROL, P1, DOI 10.1007/978-3-319-21021-6; Likar B, 2007, COMPUT CHEM ENG, V31, P142, DOI 10.1016/j.compchemeng.2006.05.011; Ljung L, 1999, SYSTEM IDENTIFICATIO, DOI DOI 10.1002/047134608X.W1046.PUB2; Matthews AGD, 2016, JMLR WORKSH CONF PRO, V51, P231; Matthews AGD, 2017, J MACH LEARN RES, V18, P1; Matthews Alexander G. de G., 2017, THESIS; Murray-Smith R, 2001, P IR SIGN SYST C MAY, P147; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Roberts S, 2013, PHILOS T R SOC A, V371, DOI 10.1098/rsta.2011.0550; Schneider Jeff G., 1997, ADV NEURAL INFORM PR; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Sjoberg J, 1995, AUTOMATICA, V31, P1691, DOI 10.1016/0005-1098(95)00120-8; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Titsias MK, 2014, PR MACH LEARN RES, V32, P1971; Titsias MK, 2010, P 13 INT C ARTIFICIA, V9, P844; Turner R., 2010, W CP, P868; Turner R. Darby, 2011, THESIS; Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167	39	17	17	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405038
C	He, H; Xin, B; Ikehata, S; Wipf, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		He, Hao; Xin, Bo; Ikehata, Satoshi; Wipf, David			From Bayesian Sparsity to Gated Recurrent Nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The iterations of many first-order algorithms, when applied to minimizing common regularized regression functions, often resemble neural network layers with pre-specified weights. This observation has prompted the development of learning-based approaches that purport to replace these iterations with enhanced surrogates forged as DNN models from available training data. For example, important NP-hard sparse estimation problems have recently benefitted from this genre of upgrade, with simple feedforward or recurrent networks ousting proximal gradient-based iterations. Analogously, this paper demonstrates that more powerful Bayesian algorithms for promoting sparsity, which rely on complex multi-loop majorization-minimization techniques, mirror the structure of more sophisticated long short-term memory (LSTM) networks, or alternative gated feedback networks previously designed for sequence prediction. As part of this development, we examine the parallels between latent variable trajectories operating across multiple time-scales during optimization, and the activations within deep network structures designed to adaptively model such characteristic sequences. The resulting insights lead to a novel sparse estimation system that, when granted training data, can estimate optimal solutions efficiently in regimes where other algorithms fail, including practical direction-of-arrival (DOA) and 3D geometry recovery problems. The underlying principles we expose are also suggestive of a learning process for a richer class of multi-loop algorithms in other domains.	[He, Hao] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Xin, Bo; Wipf, David] Microsoft Res, Beijing, Peoples R China; [Ikehata, Satoshi] Natl Inst Informat, Tokyo, Japan	Massachusetts Institute of Technology (MIT); Microsoft; Research Organization of Information & Systems (ROIS); National Institute of Informatics (NII) - Japan	He, H (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	haohe@mit.edu; jimxinbo@gmail.com; satoshi.ikehata@gmail.com; davidwipf@gmail.com	Jeong, Yongwook/N-7413-2016					Andrychowicz M, 2016, ADV NEUR IN, V29; [Anonymous], 2010, J SELECTED TOPICS SI, V4; Baillet S, 2001, IEEE SIGNAL PROC MAG, V18, P14, DOI 10.1109/79.962275; Beck A, 2009, IEEE T IMAGE PROCESS, V18, P2419, DOI 10.1109/TIP.2009.2028250; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Blumensath T., 2009, APPL COMPUTATIONAL H, V27; Blumensath T, 2010, IEEE J-STSP, V4, P298, DOI 10.1109/JSTSP.2010.2042411; BORGEFORS G, 1984, COMPUT VISION GRAPH, V27, P321, DOI 10.1016/0734-189X(84)90035-5; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Candes EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Chung JY, 2015, PR MACH LEARN RES, V37, P2067; Cotter S. F., 2002, IEEE T COMMUNICATION, V50; Dai J., 2017, IEEE SIGNAL PROCESSI, V24; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; FAN J, 2001, J AM STAT ASS, V96; Figueiredo M. A. T., 2002, NIPS; Gers FA, 2000, IEEE IJCNN, P189, DOI 10.1109/IJCNN.2000.861302; Gerstoft P., 2016, IEEE SIGNAL PROCESSI, V23; Gregor K., 2010, P 27 INT C INT C MAC, P399, DOI DOI 10.5555/3104322.3104374; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hunter DR, 2004, AM STAT, V58, P30, DOI 10.1198/0003130042836; Ikehata S., 2012, COMPUTER VISION PATT; Ikehata S, 2014, IEEE T PATTERN ANAL, V36, P1816, DOI 10.1109/TPAMI.2014.2299798; Koutnik J., 2014, INT C MACH LEARN; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Malioutov D, 2005, IEEE T SIGNAL PROCES, V53, P3010, DOI 10.1109/TSP.2005.850882; Manolakis D. G., 2000, STAT ADAPTIVE SIGNAL; Nair V, 2010, P 27 INT C MACHINE L, P807; Sprechmann P, 2015, IEEE T PATTERN ANAL, V37, P1821, DOI 10.1109/TPAMI.2015.2392779; Sriperumbudu B. K., 2012, NEURAL COMPUTATION, V24; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Wang ZY, 2016, AAAI CONF ARTIF INTE, P2194; Wipf D. P., 2012, ADV NERUAL INFORM PR, V24; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Xin B, 2016, ADV NEUR IN, V29; Yang Z, 2013, IEEE T SIGNAL PROCES, V61, P38, DOI 10.1109/TSP.2012.2222378; Zamir A. R., 2016, COMPUTER SCI COMPUTE	40	17	17	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405062
C	Jidling, C; Wahlstrom, N; Wills, A; Schon, TB		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Jidling, Carl; Wahlstrom, Niklas; Wills, Adrian; Schon, Thomas B.			Linearly constrained Gaussian processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				FUNCTIONAL REGRESSION	We consider a modification of the covariance function in Gaussian processes to correctly account for known linear operator constraints. By modeling the target function as a transformation of an underlying function, the constraints are explicitly incorporated in the model such that they are guaranteed to be fulfilled by any sample drawn or prediction made. We also propose a constructive procedure for designing the transformation operator and illustrate the result on both simulated and real-data examples.	[Jidling, Carl; Wahlstrom, Niklas; Schon, Thomas B.] Uppsala Univ, Dept Informat Technol, Uppsala, Sweden; [Wills, Adrian] Univ Newcastle, Sch Engn, Callaghan, NSW, Australia	Uppsala University; University of Newcastle	Jidling, C (corresponding author), Uppsala Univ, Dept Informat Technol, Uppsala, Sweden.	carl.jidling@it.uu.se; niklas.wahlstrom@it.uu.se; adrian.wills@newcastle.edu.au; thomas.schon@it.uu.se	Schön, Thomas B/D-4169-2009; Jeong, Yongwook/N-7413-2016	Schön, Thomas B/0000-0001-5183-234X; Wills, Adrian/0000-0002-8556-6649	Swedish Foundation for Strategic Research (SSF) via the project ASSEMBLE [RIT 15-0012]; Swedish Research Council (VR) [621-2013-5524]	Swedish Foundation for Strategic Research (SSF) via the project ASSEMBLE; Swedish Research Council (VR)(Swedish Research Council)	This research is financially supported by the Swedish Foundation for Strategic Research (SSF) via the project ASSEMBLE (Contract number: RIT 15-0012). The work is also supported by the Swedish Research Council (VR) via the project Probabilistic modeling of dynamical systems (Contract number: 621-2013-5524). We are grateful for the help and equipment provided by the UAS Technologies Lab, Artificial Intelligence and Integrated Computer Systems Division (AIICS) at the Department of Computer and Information Science (IDA), Linkoping University, Sweden. The real data set used in this paper has been collected by some of the authors together with Manon Kok, Arno Solin, and Simo Sarkka. We thank them for allowing us to use this data. We also thank Manon Kok for supporting us with the data processing. Furthermore, we would like to thank Carl Rasmussen and Marc Deisenroth for fruitful discussions on constrained GPs.	Abrahamsen P, 2001, MATH GEOL, V33, P719, DOI 10.1023/A:1011078716252; Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; Andrade-Pacheco Ricardo, 2016, MONITORING SHORT TER, P95; [Anonymous], P 20 INT C MACH LEAR; Constantinescu EM, 2013, INT J UNCERTAIN QUAN, V3, P47, DOI 10.1615/Int.J.UncertaintyQuantification.2012003722; Da Veiga S., 2012, ANN FS TOULOUSE, V21, P529; Ghahramani Z, 2015, NATURE, V521, P452, DOI 10.1038/nature14541; Ginsbourger D, 2016, J STAT PLAN INFER, V170, P117, DOI 10.1016/j.jspi.2015.10.002; Kiraly F. J., 2014, TECHNICAL REPORT; Koyejo O, 2013, INT CONF DAT MIN WOR, P72, DOI 10.1109/ICDMW.2013.150; Luenberger D. G., 1969, OPTIMIZATION VECTOR; Maatouk H, 2017, MATH GEOSCI, V49, P557, DOI 10.1007/s11004-017-9673-2; Navarro A.M., 2016, TECHNICAL REPORT; Nguyen NC, 2016, J COMPUT PHYS, V309, P52, DOI 10.1016/j.jcp.2015.12.035; Nguyen NC, 2015, COMPUT METHOD APPL M, V287, P69, DOI 10.1016/j.cma.2015.01.008; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Ross J., 2013, P 30 INT C MACH LEAR, V28, P1346; Rudovic Ognjen, 2011, P INT C COMP VIS ICC; Salzmann Mathieu, 2010, NEURAL INFORM PROCES; Tran Cuong, 2015, TECHNICAL REPORT; Wahlstrom N., 2015, THESIS	22	17	16	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401025
C	Jun, KS; Bhargava, A; Nowak, R; Willett, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Jun, Kwang-Sung; Bhargava, Aniruddha; Nowak, Robert; Willett, Rebecca			Scalable Generalized Linear Bandits: Online Computation and Hashing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CLASSIFICATION; FEEDBACK	Generalized Linear Bandits (GLBs), a natural extension of the stochastic linear bandits, has been popular and successful in recent years. However, existing GLBs scale poorly with the number of rounds and the number of arms, limiting their utility in practice. This paper proposes new, scalable solutions to the GLB problem in two respects. First, unlike existing GLBs, whose per-time-step space and time complexity grow at least linearly with time t, we propose a new algorithm that performs online computations to enjoy a constant space and time complexity. At its heart is a novel Generalized Linear extension of the Online-to-confidence-set Conversion (GLOC method) that takes any online learning algorithm and turns it into a GLB algorithm. As a special case, we apply GLOC to the online Newton step algorithm, which results in a low-regret GLB algorithm with much lower time and memory complexity than prior work. Second, for the case where the number N of arms is very large, we propose new algorithms in which each next arm is selected via an inner product search. Such methods can be implemented via hashing algorithms (i.e., "hash-amenable") and result in a time complexity sublinear in N. While a Thompson sampling extension of GLOC is hash-amenable, its regret bound for d-dimensional arm sets scales with d(3/2), whereas GLOC's regret bound scales with d. Towards closing this gap, we propose a new hash-amenable algorithm whose regret bound scales with d(5/4). Finally, we propose a fast approximate hash-key computation (inner product) with a better accuracy than the state-of-the-art, which can be of independent interest. We conclude the paper with preliminary experimental results confirming the merits of our methods.	[Jun, Kwang-Sung; Bhargava, Aniruddha; Nowak, Robert; Willett, Rebecca] UW Madison, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison	Jun, KS (corresponding author), UW Madison, Madison, WI 53706 USA.	kjun@discovery.wisc.edu; aniruddha@wisc.edu; rdnowak@wisc.edu; willett@discovery.wisc.edu	Jeong, Yongwook/N-7413-2016	Jun, Kwang-Sung/0000-0001-5483-3161	NSF [IIS-1447449]; MURI grant [2015-05174-04]	NSF(National Science Foundation (NSF)); MURI grant(MURI)	This work was partially supported by the NSF grant IIS-1447449 and the MURI grant 2015-05174-04. The authors thank Yasin Abbasi-Yadkori and Anshumali Shrivastava for providing constructive feedback and Xin Hunt for her contribution at the initial stage.	Abbasi-Yadkori Y., 2012, P INT C ART INT STAT; Abbasi- Yadkori Yasin, 2011, ADV NEURAL INFORM PR, P1; Abeille M, 2017, PR MACH LEARN RES, V54, P176; Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; Agrawal Shipra, 2012, ABS12093 CORR; Ahukorala K., 2015, P 24 ACM INT C INF K, P1703, DOI DOI 10.1145/2806416.2806609; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Crammer K, 2013, MACH LEARN, V90, P347, DOI 10.1007/s10994-012-5321-8; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dani V, 2008, P C LEARN THEOR COLT, P355; Dekel O., 2010, P C LEARNING THEORY; Dekel O, 2012, J MACH LEARN RES, V13, P2655; Filippi S., 2010, NIPS, P586; Gentile C, 2014, J MACH LEARN RES, V15, P2451; Guo RQ, 2016, JMLR WORKSH CONF PRO, V51, P482; Har-Peled S., 2012, THEORY COMPUT, V8, P321, DOI DOI 10.4086/TOC.2012.V008A014; Hazan E, 2016, J MACHINE LEARN RES, V17, P4062; Hofmann Katja, 2011, NIPS WORKSH BAYES OP; Jain P., 2010, P ADV NEUR INF PROC, P928; Kalai Adam Tauman, 2009, P C LEARN THEOR COL; Kannan R, 2008, FOUND TRENDS THEOR C, V4, P157, DOI 10.1561/0400000025; Konyushkova Ksenia, 2013, 21 EUR S ART NEUR NE; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Li Lihong, 2017, ABS17030 CORR; Li Lihong, 2012, P WORKSH ON LIN TRAD, V2, P19; McCullagh P., 1989, GEN LINEAR MODELS, V2nd; Neyshabur B, 2015, PR MACH LEARN RES, V37, P1926; ORABONA F, 2012, AISTATS, P823; Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510; Rusmevichientong P, 2010, MATH OPER RES, V35, P395, DOI 10.1287/moor.1100.0446; Shrivastava A, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P812; Shrivastava Anshumali, 2014, ADV NEURAL INFORM PR, P2321; Slaney M, 2012, P IEEE, V100, P2604, DOI 10.1109/JPROC.2012.2193849; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wang Jingdong, 2014, ABS14082 CORR; Yue Y., 2012, P INT C MACH LEARN I, P1895; Zhang LJ, 2016, PR MACH LEARN RES, V48	41	17	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400010
C	Liu, YY; Shang, FH; Cheng, J; Cheng, H; Jiao, LC		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Liu, Yuanyuan; Shang, Fanhua; Cheng, James; Cheng, Hong; Jiao, Licheng			Accelerated First-order Methods for Geodesically Convex Optimization on Riemannian Manifolds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				FRAMEWORK	In this paper, we propose an accelerated first-order method for geodesically convex optimization, which is the generalization of the standard Nesterov's accelerated method from Euclidean space to nonlinear Riemannian space. We first derive two equations and obtain two nonlinear operators for geodesically convex optimization instead of the linear extrapolation step in Euclidean space. In particular, we analyze the global convergence properties of our accelerated method for geodesically strongly-convex problems, which show that our method improves the convergence rate from O((1 - mu/L)(k) )to O((1 - root mu/L)(k)). Moreover, our method also improves the global convergence rate on geodesically general convex problems from O(1/k) to O(1/k(2)). Finally, we give a specific iterative scheme for matrix Karcher mean problems, and validate our theoretical results with experiments.	[Liu, Yuanyuan; Shang, Fanhua; Cheng, James] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China; [Cheng, Hong] Chinese Univ Hong Kong, Dept Syst Engn & Engn Management, Hong Kong, Peoples R China; [Jiao, Licheng] Xidian Univ, Key Lab Intelligent Percept & Image Understanding, Minist Educ, Sch Artificial Intelligence, Xian, Shaanxi, Peoples R China	Chinese University of Hong Kong; Chinese University of Hong Kong; Xidian University	Shang, FH (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.	yyliu@cse.cuhk.edu.hk; fhshang@cse.cuhk.edu.hk; jcheng@cse.cuhk.edu.hk; hcheng@se.cuhk.edu.hk; lchjiao@mail.xidian.edu.cn	Jeong, Yongwook/N-7413-2016; liu, yuanyuan/GWZ-5838-2022		Hong Kong RGC [CUHK 14206715, 14222816]; Major Research Plan of the National Natural Science Foundation of China [91438201, 91438103]; National Natural Science Foundation of China [61573267]	Hong Kong RGC(Hong Kong Research Grants Council); Major Research Plan of the National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This research is supported in part by Grants (CUHK 14206715 & 14222816) from the Hong Kong RGC, the Major Research Plan of the National Natural Science Foundation of China (Nos. 91438201 and 91438103), and the National Natural Science Foundation of China (No. 61573267).	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448; Attouch H, 2016, SIAM J OPTIMIZ, V26, P1824, DOI 10.1137/15M1046095; Azagra D., 2006, REV MAT COMPLUT; Ba?k, 2014, CONVEX ANAL OPTIMIZA; Barbaresco F., 2009, RADAR; Batchelor PG, 2005, MAGN RESON MED, V53, P221, DOI 10.1002/mrm.20334; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bhatia R, 2007, PRINC SER APPL MATH, P1; Bini DA, 2013, LINEAR ALGEBRA APPL, V438, P1700, DOI 10.1016/j.laa.2011.08.052; Boyd S, 2007, OPTIM ENG, V8, P67, DOI 10.1007/s11081-007-9001-7; Congedo M, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0121423; Fletcher PT, 2007, SIGNAL PROCESS, V87, P250, DOI 10.1016/j.sigpro.2005.12.018; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Lapuyade-Lahorgue J., 2008, RADAR; Liu YY, 2017, AAAI CONF ARTIF INTE, P2287; Meyer G, 2011, J MACH LEARN RES, V12, P593; Moakher M, 2006, J ELASTICITY, V82, P273, DOI 10.1007/s10659-005-9035-z; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; Petersen P., 2016, RIEMANNIAN GEOMETRY, V3; Shang F., 2017, ARXIV170404966; Sra S, 2015, SIAM J OPTIMIZ, V25, P713, DOI 10.1137/140978168; Su WJ, 2016, J MACH LEARN RES, V17; Tseng P., 2008, AACELERATED PROXIMAL; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Yuan XR, 2016, PROCEDIA COMPUT SCI, V80, P2147, DOI 10.1016/j.procs.2016.05.534; Zhang H., 2016, P C LEARN THEOR, P1617; Zhang Hongyi, 2016, ADV NEURAL INFORM PR, V29, P4592	31	17	18	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404091
C	Mujika, A; Meier, F; Steger, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Mujika, Asier; Meier, Florian; Steger, Angelika			Fast-Slow Recurrent Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				LONG-TERM DEPENDENCIES	Processing sequential data of variable length is a major challenge in a wide range of applications, such as speech recognition, language modeling, generative image modeling and machine translation. Here, we address this challenge by proposing a novel recurrent neural network (RNN) architecture, the Fast-Slow RNN (FS-RNN). The FS-RNN incorporates the strengths of both multiscale RNNs and deep transition RNNs as it processes sequential data on different timescales and learns complex transition functions from one time step to the next. We evaluate the FS-RNN on two character level language modeling data sets, Penn Treebank and Hutter Prize Wikipedia, where we improve state of the art results to 1.1 9 and 1.2 5 bits-per-character (BPC), respectively. In addition, an ensemble of two FS-RNNs achieves 1.20 BPC on Hutter Prize Wikipedia outperforming the best known compression algorithm with respect to the BPC measure. We also present an empirical investigation of the learning and network dynamics of the FS-RNN, which explains the improved performance compared to other RNN architectures. Our approach is general as any kind of RNN cell is a possible building block for the FS-RNN architecture, and thus can be flexibly applied to different tasks.	[Mujika, Asier; Meier, Florian; Steger, Angelika] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Mujika, A (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	asierm@ethz.ch; meierflo@inf.ethz.ch; steger@inf.ethz.ch						Bahdanau Dzmitry, 2016, ACOUSTICS SPEECH SIG; Bengio, 2016, ARXIV160901704; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Chung JY, 2015, PR MACH LEARN RES, V37, P2067; DiPietro Robert, 2017, REVISITING NARX RECU; El Hihi Salah, 1995, NIPS, V409; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Graves A., 2014, ARXIV14105401; Graves A, 2013, ARXIV13080850; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Graves Alex, 2016, ARXIV160308983; Ha David, 2016, ARXIV161101578; Hinton, 2016, ARXIV PREPRINT ARXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter S, 1998, INT J UNCERTAIN FUZZ, V6, P107, DOI 10.1142/S0218488598000094; Hochreiter S, 1991, UNTERSUCHUNGEN DYNAM, V91; HUTTER M, 2012, HUMAN KNOWLEDGE COMP; Jaeger H., 2007, DISCOVERING MULTISCA; Jing L., 2016, TUNABLE EFFICIENT UN; Kalchbrenner Nal, 2016, ARXIV161010099; Kingma D.P, P 3 INT C LEARNING R; Koutnik Jan, 2016, ARXIV160308983; Lin TN, 1996, IEEE T NEURAL NETWOR, V7, P1329, DOI 10.1109/72.548162; Mahoney M., 2017, LARGE TEXT COMPRESSI; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Pascanu R., 2013, ARXIV13126026; Robinson AJ, 1987, UTILITY DRIVEN DYNAM; Rocki Kamil, 2016, ARXIV161007675; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P234, DOI 10.1162/neco.1992.4.2.234; Sordoni Alessandro, 2015, P 24 ACM INT C INF K, P553, DOI DOI 10.1145/2806416.2806493; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sutskever I., 2011, P 28 INT C MACH LEAR, P1033, DOI DOI 10.1145/346152.346166; WERBOS PJ, 1988, NEURAL NETWORKS, V1, P339, DOI 10.1016/0893-6080(88)90007-X; Weston J., 2014, ARXIV14103916; Williams R, 1989, NUCCS8927; Zaremba Wojciech, 2014, ABS14092329 CORR; Zilly J.G., 2016, ARXIV PREPRINT ARXIV; Zoph B., 2016, ARXIV161101578	42	17	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405096
C	Ustinova, E; Lempitsky, V		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ustinova, Evgeniya; Lempitsky, Victor			Learning Deep Embeddings with Histogram Loss	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We suggest a loss for learning deep embeddings. The new loss does not introduce parameters that need to be tuned and results in very good embeddings across a range of datasets and problems. The loss is computed by estimating two distribution of similarities for positive (matching) and negative (non-matching) sample pairs, and then computing the probability of a positive pair to have a lower similarity score than a negative pair based on the estimated similarity distributions. We show that such operations can be performed in a simple and piecewise-differentiable manner using 1D histograms with soft assignment operations. This makes the proposed loss suitable for learning deep embeddings using stochastic optimization. In the experiments, the new loss performs favourably compared to recently proposed alternatives.	[Ustinova, Evgeniya; Lempitsky, Victor] Skolkovo Inst Sci & Technol Skoltech, Moscow, Russia	Skolkovo Institute of Science & Technology	Ustinova, E (corresponding author), Skolkovo Inst Sci & Technol Skoltech, Moscow, Russia.				Russian Ministry of Science and Education [RFMEFI57914X0071]	Russian Ministry of Science and Education(Ministry of Education and Science, Russian Federation)	This research is supported by the Russian Ministry of Science and Education grant RFMEFI57914X0071.	Arandjelovic R., 2015, IEEE INT C COMP VIS; Bowman A.W., 1997, APPL SMOOTHING TECHN, V18; Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339; Chechik G, 2010, J MACH LEARN RES, V11, P1109; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Law MT, 2013, IEEE I CONF COMP VIS, P249, DOI 10.1109/ICCV.2013.38; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Lin J., 2015, ABS150104711 CORR; Parkhi Omkar M., 2015, BRIT MACH VIS C; Qian Q, 2015, PROC CVPR IEEE, P3716, DOI 10.1109/CVPR.2015.7298995; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Schultz M, 2004, ADV NEUR IN, V16, P41; Simo-Serra E, 2015, IEEE I CONF COMP VIS, P118, DOI 10.1109/ICCV.2015.22; Song H. O., 2016, COMPUTER VISION PATT; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tadmor Oren, 2016, NIPS; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Wah C., 2011, CALTECHUCSD BIRDS 20; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Yi D., 2014, ARXIV14074979; Zbontar J., 2015, ARXIV151005970; Zheng Liang, 2015, COMP VIS IEEE INT C; Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138	30	17	17	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703100
C	Fairhall, AL; Lewen, GD; Bialek, W; van Steveninck, RRD		Leen, TK; Dietterich, TG; Tresp, V		Fairhall, AL; Lewen, GD; Bialek, W; van Steveninck, RRD			Multiple timescales of adaptation in a neural code	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				INFORMATION; DYNAMICS	Many neural systems extend their dynamic range by adaptation. We examine the timescales of adaptation in the context of dynamically modulated rapidly-varying stimuli, and demonstrate in the fly visual system that adaptation to the statistical ensemble of the stimulus dynamically maximizes information transmission about the time-dependent stimulus. Further, while the rate response has long transients, the adaptation takes place on timescales consistent with optimal variance estimation.	NEC Res Inst, Princeton, NJ 08540 USA	NEC Corporation	Fairhall, AL (corresponding author), NEC Res Inst, 4 Independence Way, Princeton, NJ 08540 USA.							Adrian E.D., 1928, BASIS SENSATION; ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663; Barlow H., 1961, SENSORY COMMUNICATIO, P217; Brenner N, 2000, NEURON, V26, P695, DOI 10.1016/S0896-6273(00)81205-2; DeWeese M, 1998, NEURAL COMPUT, V10, P1179, DOI 10.1162/089976698300017403; LAUGHLIN S, 1981, Z NATURFORSCH C, V36, P910; POTTERS M, 1994, J PHYS I, V4, P1755, DOI 10.1051/jp1:1994219; Rieke F., 1997, SPIKES EXPLORING NEU; Schilstra C, 1999, J EXP BIOL, V202, P1481; Smirnakis SM, 1997, NATURE, V386, P69, DOI 10.1038/386069a0; Strong SP, 1998, PHYS REV LETT, V80, P197, DOI 10.1103/PhysRevLett.80.197; VanHateren JH, 1997, VISION RES, V37, P3407, DOI 10.1016/S0042-6989(97)00105-3; VANSTEVENINCK RRD, 1994, P IEEE C SYST MAN CY, P302	13	17	17	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						124	130						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800018
C	Carreira-Perpinan, MA		Solla, SA; Leen, TK; Muller, KR		Carreira-Perpinan, MA			Reconstruction of sequential data with probabilistic models and continuity constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				SPEECH	We consider the problem of reconstructing a temporal discrete sequence of multidimensional real vectors when part of the data is missing, under the assumption that the sequence was generated by a continuous process. A particular case of this problem is multivariate regression, which is very difficult when the underlying mapping is one-to-many. We propose an algorithm based on a joint probability model of the variables of interest, implemented using a nonlinear latent variable model. Each point in the sequence is potentially reconstructed as any of the modes of the conditional distribution of the missing variables given the present variables (computed using an exhaustive mode search in a Gaussian mixture). Mode selection is determined by a dynamic programming search that minimises a geometric measure of the reconstructed sequence, derived from continuity constraints. We illustrate the algorithm with a toy example and apply it to a real-world inverse problem, the acoustic-to-articulatory mapping. The results show that the algorithm outperforms conditional mean imputation and multilayer perceptrons.	Univ Sheffield, Dept Comp Sci, Sheffield S10 2TN, S Yorkshire, England	University of Sheffield	Carreira-Perpinan, MA (corresponding author), Univ Sheffield, Dept Comp Sci, Sheffield S10 2TN, S Yorkshire, England.							Bartholomew D. J., 1987, LATENT VARIABLE MODE; Bernstein NA., 1967, COORDINATION REGULAT; Bishop, 1995, NEURAL NETWORKS PATT; Bishop CM, 1998, NEURAL COMPUT, V10, P215, DOI 10.1162/089976698300017953; CARREIRAPERPINA.MA, 1999, CS9903 U SHEFF DEP C; GHAHRAMANI Z, 1994, NIPS, V6, P120; HERMANSKY H, 1990, J ACOUST SOC AM, V87, P1738, DOI 10.1121/1.399423; JOSIFOVSKI M, 1999, P EUR 99, P2837; Little RJA, 1987, STAT ANAL MISSING DA; MARCZIN N, 1993, VASCULAR ENDOTHELIUM, V2, P3; RAHIM MG, 1993, J ACOUST SOC AM, V93, P1109, DOI 10.1121/1.405559; Rohwer R, 1996, NEURAL COMPUT, V8, P595, DOI 10.1162/neco.1996.8.3.595; SAUL LK, 1999, NIPS, V11, P751; Schroeter J, 1994, IEEE T SPEECH AUDI P, V2, P133, DOI 10.1109/89.260356; TRESP V, 1995, NEURAL INFORMATION P, V7, P689	15	17	17	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						414	420						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700059
C	Schraudolph, NN; Giannakopoulos, X		Solla, SA; Leen, TK; Muller, KR		Schraudolph, NN; Giannakopoulos, X			Online independent component analysis with local learning rate adaptation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				SEPARATION	Stochastic meta-descent (SMD) is a new technique for online adaptation of local learning rates in arbitrary twice-differentiable systems. Like matrix momentum it uses full second-order information while retaining O(n) computational complexity by exploiting the efficient computation of Hessian-vector products. Here we apply SMD to independent component analysis, and employ the resulting algorithm for the blind separation of time-varying mixtures. By matching individual learning rates to the rate of change in each source signal's mixture coefficients, our technique is capable of simultaneously tracking sources that move at very different, a priori unknown speeds.	IDSIA, CH-6900 Lugano, Switzerland	Universita della Svizzera Italiana	Schraudolph, NN (corresponding author), IDSIA, Corso Elvezia 36, CH-6900 Lugano, Switzerland.	nic@idsia.ch; xavier@idsia.ch						ALMEIDA LB, 1999, PUBLICATIONS NEWTON, pCH6; Amari S, 1996, ADV NEUR IN, V8, P757; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Girolami M, 1997, 1997 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-4, P1788, DOI 10.1109/ICNN.1997.614168; HARMON ME, 1996, WLTR1065 WLAACF; JACOBS RA, 1988, NEURAL NETWORKS, V1, P295, DOI 10.1016/0893-6080(88)90003-2; Karhunen J, 1997, IEEE T NEURAL NETWOR, V8, P486, DOI 10.1109/72.572090; Karhunen J, 1997, 1997 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-4, P2147, DOI 10.1109/ICNN.1997.614238; Kivinen J., 1995, Proceedings of the Twenty-Seventh Annual ACM Symposium on the Theory of Computing, P209, DOI 10.1145/225058.225121; KIVINEN J, 1994, UCSCCRL9416; Murata N, 1997, ADV NEUR IN, V9, P599; PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147; RIEDMILLER M, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P586, DOI 10.1109/ICNN.1993.298623; Schraudolph NN, 1999, NEURAL COMPUT, V11, P853, DOI 10.1162/089976699300016467; Schraudolph NN, 1999, PERSP NEURAL COMP, P151; Schraudolph NN, 1999, IEE CONF PUBL, P569, DOI 10.1049/cp:19991170; SILVA FM, 1990, ADVANCED NEURAL COMPUTERS, P151; Sutton R.S., 1992, P 7 YAL WORKSH AD LE, P161; SUTTON RS, 1992, AAAI-92 PROCEEDINGS : TENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, P171; TOLLENAERE T, 1990, NEURAL NETWORKS, V3, P561, DOI 10.1016/0893-6080(90)90006-7; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270	21	17	17	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						789	795						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700112
C	Isbell, CL; Viola, P		Kearns, MS; Solla, SA; Cohn, DA		Isbell, CL; Viola, P			Restructuring sparse high dimensional data for effective retrieval	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					The task in text retrieval is to find the subset of a collection of documents relevant to a user's information request, usually expressed as a set of words. Classically, documents and queries are represented as vectors of word counts. In its simplest form, relevance is defined to be the dot product between a document and a query vector-a measure of the number of common terms, A central difficulty in text retrieval is that the presence or absence of a word is not sufficient to determine relevance to a query. Linear dimensionality reduction has been proposed as a technique for extracting underlying structure from the document collection. In some domains (such as vision) dimensionality reduction reduces computational complexity. In text retrieval it is more often used to improve retrieval performance. We propose an alternative and novel technique that produces sparse representations constructed from sets of highly-related words. Documents and queries are represented by their distance to these sets, and relevance is measured by the number of common clusters. This technique significantly improves retrieval performance, is efficient to compute and shares properties with the optimal linear projection operator and the independent components of documents.	AT&T Labs Res, Florham Park, NJ 07932 USA	AT&T	Isbell, CL (corresponding author), AT&T Labs Res, 180 Pk Ave,Room A255, Florham Park, NJ 07932 USA.							AMARI S, 1996, ADV NEURAL INFORMATI; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9; FRAKES WB, 1992, INFORMATION RETRIEVA; Golub GH, 1993, MATRIX COMPUTATIONS; KWOK KL, 1996, P 19 ANN INT ACM SIG, P187; OBRIEN GW, 1994, UTCS94259; SAHAMI M, 1996, P 13 INT MACH LEARN; Salton G., 1971, SMART RETRIEVAL SYST; SINGHAL A, 1997, P 20 INT C RES DEV I; TURK M, 1991, P IEEE C COMP VIS PA, P586, DOI DOI 10.1109/CVPR.1991.139758; [No title captured]	12	17	17	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						480	486						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700068
C	Andre, D; Friedman, N; Parr, R		Jordan, MI; Kearns, MJ; Solla, SA		Andre, D; Friedman, N; Parr, R			Generalized prioritized sweeping	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Prioritized sweeping is a model-based reinforcement learning method that attempts to focus an agent's limited computational resources to achieve a good estimate of the value of environment states. To choose effectively where to spend a costly planning step, classic prioritized sweeping uses a simple heuristic to focus computation on the states that are likely to have the largest errors. In this paper, we introduce generalized prioritized sweeping, a principled method for generating such estimates in a representation-specific manner. This allows us to extend prioritized sweeping beyond an explicit, state-based representation to deal with compact representations that are necessary for dealing with large state spaces. We apply this method for generalized model approximators (such as Bayesian networks), and describe preliminary experiments that compare our approach with classical prioritized sweeping.	Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Andre, D (corresponding author), Univ Calif Berkeley, Div Comp Sci, 387 Soda Hall, Berkeley, CA 94720 USA.								0	17	18	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						1001	1007						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700141
C	Davies, S		Mozer, MC; Jordan, MI; Petsche, T		Davies, S			Multidimensional triangulation and interpolation for reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Dynamic Programming, Q-learning and other discrete Markov Decision Process solvers can be applied to continuous d-dimensional state-spaces by quantizing the state space into an array of boxes. This is often problematic above two dimensions: a coarse quantization can lead to poor policies, and fine quantization is too expensive. Possible solutions are variable-resolution discretization, or function approximation by neural nets. A third option, which has been little studied in the reinforcement learning literature, is interpolation on a coarse grid. In this paper we study interpolation techniques that can result in vast improvements in the online behavior of the resulting control systems: multilinear interpolation, and an interpolation algorithm based on an interesting regular triangulation of d-dimensional space. We adapt these interpolators under three reinforcement learning paradigms: (i) offline value iteration with a known model, (ii) Q-learning, and (iii) online value iteration with a previously unknown model learned from data. We describe empirical results, and the resulting implications for practical learning of continuous non-linear dynamic control.			Davies, S (corresponding author), CARNEGIE MELLON UNIV,DEPT COMP SCI,5000 FORBES AVE,PITTSBURGH,PA 15213, USA.								0	17	20	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						1005	1011						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00141
C	Waterhouse, S; Cook, G		Mozer, MC; Jordan, MI; Petsche, T		Waterhouse, S; Cook, G			Ensemble methods for phoneme classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				This paper investigates a number of ensemble methods for improving the performance of phoneme classification for use in a speech recognition system. Two ensemble methods are described; boosting and mixtures of experts, both in isolation and in combination. Results are presented on two speech recognition databases: an isolated word database and a large vocabulary continuous speech database. These results show that principled ensemble methods such as boosting and mixtures provide superior performance to more naive ensemble methods such as averaging.			Waterhouse, S (corresponding author), UNIV CAMBRIDGE,DEPT ENGN,CAMBRIDGE CB2 1PZ,ENGLAND.								0	17	17	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						800	806						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00113
C	Agrawal, A; Amos, B; Barratt, S; Boyd, S; Diamond, S; Kolter, JZ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Agrawal, Akshay; Amos, Brandon; Barratt, Shane; Boyd, Stephen; Diamond, Steven; Kolter, J. Zico			Differentiable Convex Optimization Layers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DESIGN	Recent work has shown how to embed differentiable optimization problems (that is, problems whose solutions can be backpropagated through) as layers within deep learning architectures. This method provides a useful inductive bias for certain problems, but existing software for differentiable optimization layers is rigid and difficult to apply to new settings. In this paper, we propose an approach to differentiating through disciplined convex programs, a subclass of convex optimization problems used by domain-specific languages (DSLs) for convex optimization. We introduce disciplined parametrized programming, a subset of disciplined convex programming, and we show that every disciplined parametrized program can be represented as the composition of an affine map from parameters to problem data, a solver, and an affine map from the solver's solution to a solution of the original problem (a new form we refer to as affine-solver-affine form). We then demonstrate how to efficiently differentiate through each of these components, allowing for end-to-end analytical differentiation through the entire convex program. We implement our methodology in version 1.1 of CVXPY, a popular Python-embedded DSL for convex optimization, and additionally implement differentiable layers for disciplined convex programs in PyTorch and TensorFlow 2.0. Our implementation significantly lowers the barrier to using convex optimization problems in differentiable programs. We present applications in linear machine learning models and in stochastic control, and we show that our layer is competitive (in execution time) compared to specialized differentiable solvers from past work.	[Agrawal, Akshay; Barratt, Shane; Boyd, Stephen; Diamond, Steven] Stanford Univ, Stanford, CA 94305 USA; [Amos, Brandon] Facebook AI, Menlo Pk, CA USA; [Kolter, J. Zico] Carnegie Mellon Univ, Bosch Ctr AI, Pittsburgh, PA 15213 USA	Stanford University; Facebook Inc; Carnegie Mellon University	Agrawal, A (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	akshayka@cs.stanford.edu; bda@fb.com; sbarratt@stanford.edu; boyd@stanford.edu; diamond@cs.stanford.edu; zkolter@cs.cmu.edu			National Science Foundation Graduate Research Fellowship [DGE-1656518]	National Science Foundation Graduate Research Fellowship(National Science Foundation (NSF))	We gratefully acknowledge discussions with Eric Chu, who designed and implemented a code generator for SOCPs [26, 25], Nicholas Moehle, who designed and implemented a basic version of a code generator for convex optimization in unpublished work, and Brendan O'Donoghue. We also would like to thank the anonymous reviewers, who provided us with useful suggestions that improved the paper. S. Barratt is supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-1656518.	Agrawal A., 2018, J CONTROL DECIS, V5, P42, DOI [10.1080/23307706.2017.1397554, DOI 10.1080/23307706.2017.1397554]; Agrawal A., 2019, P SYSTEMS MACHINE LE; Amos B., 2019, ARXIV190608707; Amos B., 2018, ADV NEURAL INFORM PR, P8299; Amos B., 2017, INT C MACH LEARN; Amos Brandon, 2019, THESIS CARNEGIE MELL; Angeris G, 2019, ACS PHOTONICS, V6, P1232, DOI 10.1021/acsphotonics.9b00154; [Anonymous], 2019, MOSEK OPTIMIZATION S; [Anonymous], 2000, COMPUTATIONAL GEOMET; [Anonymous], 2019, J APPL NUMER OPTIM; Barratt S, 2019, ARXIV190405460; Barratt S., 2018, ARXIV181100168; Barratt S., 2019, ARXIV191008615; Barratt Shane, 2018, ARXIV180405098; Belanger D., 2017, INT C MACH LEARN; Ben-Tal A., 2005, Manufacturing & Service Operations Management, V7, P248, DOI 10.1287/msom.1050.0081; Bertsekas D. P., 2005, DYNAMIC PROGRAMMING, V1; Bertsimas D, 2004, LECT NOTES COMPUT SC, V3064, P86; Biggio B, 2018, PATTERN RECOGN, V84, P317, DOI 10.1016/j.patcog.2018.07.023; Boyd Stephen, 2017, Foundations and Trends in Optimization, V3, P1, DOI 10.1561/2400000023; Boyd S, 2004, CONVEX OPTIMIZATION; Boyd S., 2005, OPERATIONS RES, V53; Boyd S. P., 1994, SIAM; Busseti E., 2018, ARXIV181102157; Chu E., 2017, QCML QUADRATIC CONE; Chu E, 2013, 2013 EUROPEAN CONTROL CONFERENCE (ECC), P1547; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Diamond S, 2017, ARXIV170508041; Diamond S, 2016, J MACH LEARN RES, V17; Djolonga J., 2017, ADV NEURAL INFORM PR, P1013; Domahidi A, 2013, 2013 EUROPEAN CONTROL CONFERENCE (ECC), P3077; Domke Justin, 2012, INT C ARTIFICIAL INT; Donti P, 2017, P ADV NEUR INF PROC, V30, P5484; Fiacco A., 1968, NONLINEAR PROGRAMMIN, pxiv+210; Fiacco A. V., 1983, MATH SCI ENG, V165; Finn C, 2017, PR MACH LEARN RES, V70; Fu A, 2017, ARXIV171107582; Geng Z., 2019, ARXIV191009671PHYSIC; Gould S., 2019, ARXIV190904866; Grant M, 2006, NONCON OPTIM ITS APP, V84, P155; GRANT M., 2001, CVX MATLAB SOFTWARE; Grant MC, 2008, LECT NOTES CONTR INF, V371, P95, DOI 10.1007/978-1-84800-155-8_7; Griewank A., 2008, SIAM; Hershenson MD, 2001, IEEE T COMPUT AID D, V20, P1, DOI 10.1109/43.905671; Hoburg W, 2014, AIAA J, V52, P2414, DOI 10.2514/1.J052732; Jagielski M, 2018, P IEEE S SECUR PRIV, P19, DOI 10.1109/SP.2018.00057; Kalman R.E., 1964, J FLUID ENG-T ASME, V86, P51, DOI [10.1115/1.3653115, DOI 10.1115/1.3653115]; Kanno Y., 2011, NONSMOOTH MECH CONVE; Lee Kwonjoon, 2019, ARXIV190403758; Ling C. K., 2018, ARXIV180502777; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Malaviya C., 2018, ARXIV180508241; Mardani M., 2018, ARXIV180603963CSCV; Markowitz H, 1952, J FINANC, V7, P77, DOI 10.1111/j.1540-6261.1952.tb01525.x; Martins AFT, 2016, PR MACH LEARN RES, V48; Martins FT, 2017, P 2017 C EMP METH NA, P349, DOI DOI 10.18653/V1/D17-1036; Metz L., 2016, ARXIV161102163; Miller J., 2015, CVXCANON; Moehle N., 2019, ARXIV190306230; ODonoghue B., 2017, SCS SPLITTING CONIC; PAIGE C. C., 1982, ACM T MATH SOFTWARE, V8, P1; Paszke Adam, 2017, AUTOMATIC DIFFERENTI; Pirnay H, 2012, MATH PROGRAM COMPUT, V4, P307, DOI 10.1007/s12532-012-0043-2; ROBINSON SM, 1980, MATH OPER RES, V5, P43, DOI 10.1287/moor.5.1.43; Stellato B., 2017, ARXIV171108013; Stoyanov Veselin, 2011, P AISTATS; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Udell M., 2014, ARXIV14104821MATHOC; Wang Y, 2011, IEEE T CONTR SYST T, V19, P939, DOI 10.1109/TCST.2010.2056371; Wilder B., 2018, ARXIV180905504; YE YY, 1994, MATH OPER RES, V19, P53, DOI 10.1287/moor.19.1.53	76	16	16	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901022
C	Ansuini, A; Laio, A; Macke, JH; Zoccolan, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ansuini, Alessio; Laio, Alessandro; Macke, Jakob H.; Zoccolan, Davide			Intrinsic dimension of data representations in deep neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep neural networks progressively transform their inputs across multiple processing layers. What are the geometrical properties of the representations learned by these networks? Here we study the intrinsic dimensionality (ID) of data-representations, i.e. the minimal number of parameters needed to describe a representation. We find that, in a trained network, the ID is orders of magnitude smaller than the number of units in each layer. Across layers, the ID first increases and then progressively decreases in the final layers. Remarkably, the ID of the last hidden layer predicts classification accuracy on the test set. These results can neither be found by linear dimensionality estimates (e.g., with principal component analysis), nor in representations that had been artificially linearized. They are neither found in untrained networks, nor in networks that are trained on randomized labels. This suggests that neural networks that can generalize are those that transform the data into low-dimensional, but not necessarily flat manifolds.	[Ansuini, Alessio; Laio, Alessandro; Zoccolan, Davide] Scuola Int Super Studi Avanzati, Trieste, Italy; [Macke, Jakob H.] Tech Univ Munich, Munich, Germany	International School for Advanced Studies (SISSA); Technical University of Munich	Ansuini, A (corresponding author), Scuola Int Super Studi Avanzati, Trieste, Italy.	alessioansuini@gmail.com; laio@sissa.it; macke@tum.de; zoccolan@sissa.it		ansuini, alessio/0000-0002-3117-3532	European Research Council (ERC) [616803LEARN2SEE]; German Research Foundation (DFG) [SFB 1233 (276693517), SFB 1089, SPP 2041]; German Federal Ministry of Education and Research (BMBF, project 'ADMIMEM') [FKZ 01IS18052 A-D]; Human Frontier Science Program [RGY0076/2018]	European Research Council (ERC)(European Research Council (ERC)European Commission); German Research Foundation (DFG)(German Research Foundation (DFG)); German Federal Ministry of Education and Research (BMBF, project 'ADMIMEM')(Federal Ministry of Education & Research (BMBF)); Human Frontier Science Program(Human Frontier Science Program)	This work was supported by a European Research Council (ERC) Consolidator Grant, 616803LEARN2SEE (D.Z.). JHM is funded by the German Research Foundation (DFG) through SFB 1233 (276693517), SFB 1089 and SPP 2041, the German Federal Ministry of Education and Research (BMBF, project `ADMIMEM', FKZ 01IS18052 A-D), and the Human Frontier Science Program (RGY0076/2018).	Achille A, 2018, J MACH LEARN RES, V19; Amsaleg Laurent, 2015, KDD15 P 21 ACM, DOI DOI 10.1145/2783258.2783405; [Anonymous], 2019, CVPR; Arvin AM, 2009, LIVE VARIOLA VIRUS: CONSIDERATIONS FOR CONTINUING RESEARCH, P9; Barrett DGT, 2019, CURR OPIN NEUROBIOL, V55, P55, DOI 10.1016/j.conb.2019.01.007; Basri R., 2016, ARXIV160204723; Brahma PP, 2016, IEEE T NEUR NET LEAR, V27, P1997, DOI 10.1109/TNNLS.2015.2496947; Chung S, 2018, PHYS REV X, V8, DOI 10.1103/PhysRevX.8.031003; Chung S, 2016, PHYS REV E, V93, DOI 10.1103/PhysRevE.93.060301; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Denil Misha, 2013, NIPS, DOI DOI 10.5555/2999792.2999852; DiCarlo JJ, 2007, TRENDS COGN SCI, V11, P333, DOI 10.1016/j.tics.2007.06.010; DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010; Facco E, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-11873-y; Granata D, 2016, SCI REP-UK, V6, DOI 10.1038/srep31377; Gudmundsson GP, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P1, DOI 10.1145/3083187.3083200; He K., 2015, CORR; Huang HP, 2018, PHYS REV E, V98, DOI 10.1103/PhysRevE.98.062313; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lampinen A. K., 2018, ARXIV180910374; LeCun Y, 2010, ATT LAB; Levina E., 2005, ADV NEURAL INFORM PR, V17, P777, DOI DOI 10.5555/2976040.2976138; Ma X., 2018, CORR; Ma Xingjun, 2018, ARXIV180602612; Matteucci G., 2019, J NEUROSCI, P1938; Morcos A., 2018, ADV NEURAL INFORM PR, P5727; Neyshabur B., 2018, ARXIV180512076; Olshausen BA, 2004, CURR OPIN NEUROBIOL, V14, P481, DOI 10.1016/j.conb.2004.07.007; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Raghu Maithra, 2017, ADV NEURAL INFORM PR, P6076; Shwartz-Ziv Ravid, 2017, ARXIV170300810; Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Stringer C, 2018, BIORXIV, DOI [10.1101/374090, DOI 10.1101/374090]; Tafazoli S, 2017, ELIFE, V6, DOI 10.7554/eLife.22794; Vascon S., 2018, EUR C COMP VIS, P577; Yu T., 2018, ARXIV180106801; Zhang C., 2016, CORR	38	16	16	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306015
C	Atzmon, M; Haim, N; Yariv, L; Israelov, O; Maron, H; Lipman, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Atzmon, Matan; Haim, Niv; Yariv, Lior; Israelov, Ofer; Maron, Haggai; Lipman, Yaron			Controlling Neural Level Sets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The level sets of neural networks represent fundamental properties such as decision boundaries of classifiers and are used to model non-linear manifold data such as curves and surfaces. Thus, methods for controlling the neural level sets could find many applications in machine learning. In this paper we present a simple and scalable approach to directly control level sets of a deep neural network. Our method consists of two parts: (i) sampling of the neural level sets, and (ii) relating the samples' positions to the network parameters. The latter is achieved by a sample network that is constructed by adding a single fixed linear layer to the original network. In turn, the sample network can be used to incorporate the level set samples into a loss function of interest. We have tested our method on three different learning tasks: improving generalization to unseen data, training networks robust to adversarial attacks, and curve and surface reconstruction from point clouds. For surface reconstruction, we produce high fidelity surfaces directly from raw 3D point clouds. When training small to medium networks to be robust to adversarial attacks we obtain robust accuracy comparable to state-of-the-art methods.	[Atzmon, Matan; Haim, Niv; Yariv, Lior; Israelov, Ofer; Maron, Haggai; Lipman, Yaron] Weizmann Inst Sci, Rehovot, Israel	Weizmann Institute of Science	Atzmon, M (corresponding author), Weizmann Inst Sci, Rehovot, Israel.				European Research Council (ERC Consolidator Grant, "LiftMatch") [771136]; Israel Science Foundation [1830/17]	European Research Council (ERC Consolidator Grant, "LiftMatch")(European Research Council (ERC)); Israel Science Foundation(Israel Science Foundation)	This research was supported in part by the European Research Council (ERC Consolidator Grant, "LiftMatch" 771136) and the Israel Science Foundation (Grant No. 1830/17).	Ben-Hamu H, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275052; BENISRAEL A, 1966, J MATH ANAL APPL, V15, P243, DOI 10.1016/0022-247X(66)90115-6; Berger M, 2017, COMPUT GRAPH FORUM, V36, P301, DOI 10.1111/cgf.12802; Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Chen Z., 2018, ARXIV181202822; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Ding Gavin Weiguang, 2020, INT C LEARN REPR; Elsayed G., 2018, ADV NEURAL INFORM PR, P842; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Hein M, 2017, NIPS 17; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Krantz S. G., 2012, IMPLICIT FUNCTION TH; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kurakin A, 2016, INT C LEARN REPR SAN; LeCun Y., 2010, MNIST HANDWRITTEN DI; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; Madry Aleksander, 2017, ARXIV; Matyasko A, 2017, IEEE IJCNN, P300, DOI 10.1109/IJCNN.2017.7965869; Mescheder L., 2018, ARXIV181203828; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025; Raman Arora A.B., 2018, INT C LEARN REPR; Sokolic J, 2017, IEEE T SIGNAL PROCES, V65, P4265, DOI 10.1109/TSP.2017.2708039; Sun S., 2015, ARXIV150605232, P148; Tang, 2013, ABS13060239 CORR; Williams Francis, 2018, ARXIV181110943; Wong E, 2018, ADV NEUR IN, V31; Xiao H., 2017, FASHION MNIST NOVEL; Zhang HY, 2019, PR MACH LEARN RES, V97; Zhao HK, 2001, IEEE WORKSHOP ON VARIATIONAL AND LEVEL SET METHODS IN COMPUTER VISION, PROCEEDINGS, P194, DOI 10.1109/VLSM.2001.938900	33	16	16	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302007
C	Beliy, R; Gaziv, G; Hoogi, A; Strappini, F; Golan, T; Irani, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Beliy, Roman; Gaziv, Guy; Hoogi, Assaf; Strappini, Francesca; Golan, Tal; Irani, Michal			From voxels to pixels and back: Self-supervision in natural-image reconstruction from fMRI	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Reconstructing observed images from fMRI brain recordings is challenging. Unfortunately, acquiring sufficient "labeled" pairs of {Image, fMRI} (i.e., images with their corresponding fMRI responses) to span the huge space of natural images is prohibitive for many reasons. We present a novel approach which, in addition to the scarce labeled data (training pairs), allows to train fMRI-to-image reconstruction networks also on "unlabeled" data (i.e., images without fMRI recording, and fMRI recording without images). The proposed model utilizes both an Encoder network (image-to-fMRI) and a Decoder network (fMRI-to-image). Concatenating these two networks back-to-back (Encoder-Decoder & Decoder-Encoder) allows augmenting the training with both types of unlabeled data. Importantly, it allows training on the unlabeled test-fMRI data. This self-supervision adapts the reconstruction network to the new input test-data, despite its deviation from the statistics of the scarce training data.	[Beliy, Roman; Gaziv, Guy; Hoogi, Assaf; Irani, Michal] Weizmann Inst Sci, Dept Comp Sci & Appl Math, IL-76100 Rehovot, Israel; [Strappini, Francesca] Weizmann Inst Sci, Dept Neurobiol, IL-76100 Rehovot, Israel; [Golan, Tal] Columbia Univ, Zuckerman Inst, New York, NY 10027 USA	Weizmann Institute of Science; Weizmann Institute of Science; Columbia University	Beliy, R (corresponding author), Weizmann Inst Sci, Dept Comp Sci & Appl Math, IL-76100 Rehovot, Israel.	roman.beliy@weizmann.ac.il; guy.gaziv@weizmann.ac.il; assaf.hoogi@weizmann.ac.il; francescastrappini@gmail.com; tal.golan@columbia.edu; michal.irani@weizmann.ac.il	Hoogi, Assaf/AAF-9339-2022; Golan, Tal/E-6406-2017; Hoogi, Assaf/AFK-2033-2022	Golan, Tal/0000-0002-7940-7473; Hoogi, Assaf/0000-0002-4542-6254	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [788535]	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC))	This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 788535).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 1998, STAT LEARNING THEORY, V16; Chan TE, 2018, FRONT AGING NEUROSCI, V10, DOI 10.3389/fnagi.2018.00146; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Eickenberg M, 2017, NEUROIMAGE, V152, P184, DOI 10.1016/j.neuroimage.2016.10.001; Gandelsman Y., 2018, QUOT DOUBLE DIP QUOT; Gao Y, 2017, IEEE T IMAGE PROCESS, V26, P2545, DOI 10.1109/TIP.2017.2675341; Ghaemmaghami P, 2017, INT CONF ACOUST SPEE, P969, DOI 10.1109/ICASSP.2017.7952300; Glorot X., 2010, PROC MACH LEARN RES, P249; Guclu U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015; Horikawa T, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms15037; Horikawa T, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00004; Kay KN, 2008, NATURE, V452, P352, DOI 10.1038/nature06713; Lin Y., 2019, TECH REP; Marcacini RM, 2018, DECIS SUPPORT SYST, V114, P70, DOI 10.1016/j.dss.2018.08.009; Naselaris T, 2009, NEURON, V63, P902, DOI 10.1016/j.neuron.2009.09.006; Nishimoto S, 2011, CURR BIOL, V21, P1641, DOI 10.1016/j.cub.2011.08.031; Raina R., 2007, LEARNING, P759, DOI DOI 10.1145/1273496.1273592; Richard H., 2018, TECH REP; Rossi A, 2018, LECT NOTES ARTIF INT, V11081, P201, DOI 10.1007/978-3-319-99978-4_16; Seeliger K, 2018, NEUROIMAGE, V181, P775, DOI 10.1016/j.neuroimage.2018.07.043; Shen G., 2018, BIORXIV; Shen GH, 2019, PLOS COMPUT BIOL, V15, DOI 10.1371/journal.pcbi.1006633; Shocher A., 2018, ZERO SHOT SUPER RESO; Shocher A., 2018, INTERNAL DISTRIBUTIO; St-Yves G., 2018, BIORXIV; Tewari Ayush, 2017, IEEE INT C COMP VIS; Wen HG, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-22160-9; Wen HG, 2018, CEREB CORTEX, V28, P4136, DOI 10.1093/cercor/bhx268; Zhang R, 2017, PROC CVPR IEEE, P645, DOI 10.1109/CVPR.2017.76; Zhou S., 2018, BIORXIV	32	16	16	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306051
C	Cai, XY; Xu, TY; Yi, JF; Huang, JZ; Rajasekaran, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cai, Xingyu; Xu, Tingyang; Yi, Jinfeng; Huang, Junzhou; Rajasekaran, Sanguthevar			DTWNet: a Dynamic Time Warping Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SERIES	Dynamic Time Warping (DTW) is widely used as a similarity measure in various domains. Due to its invariance against warping in the time axis, DTW provides more meaningful discrepancy measurements between two signals than other distance measures. In this paper, we propose a novel component in an artificial neural network. In contrast to the previous successful usage of DTW as a loss function, the proposed framework leverages DTW to obtain a better feature extraction. For the first time, the DTW loss is theoretically analyzed, and a stochastic backpropogation scheme is proposed to improve the accuracy and efficiency of the DTW learning. We also demonstrate that the proposed framework can be used as a data analysis tool to perform data decomposition.	[Cai, Xingyu; Rajasekaran, Sanguthevar] Univ Connecticut, Storrs, CT 06269 USA; [Xu, Tingyang; Huang, Junzhou] Tencent AI Lab, Bellevue, WA USA; [Yi, Jinfeng] JD Com AI Lab, Beijing, Peoples R China	University of Connecticut	Cai, XY (corresponding author), Univ Connecticut, Storrs, CT 06269 USA.		Xu, Tingyang/AHA-6587-2022; Xu, Tingyang/HGU-8709-2022					[Anonymous], 2018, [No title captured]; Berndt D. J., 1994, P 3 INT C KNOWL DISC, P359; Blondel Mathieu, 2017, ARXIV170301541; Chang Chien-Yi, 2019, ARXIV190102598; Chen Y, 2015, UCR TIME SERIES CLAS; Du Simon S, 2018, GRADIENT DESCENT FIN; Giraldo Sergio, 2018, 2018 26 SIGN PROC CO, P1; Gold O, 2018, ACM T ALGORITHMS, V14, DOI 10.1145/3230734; Holt G. A. T., 2007, P 19 ANN C ADV SCH C, V300, P1; Kate RJ, 2016, DATA MIN KNOWL DISC, V30, P283, DOI 10.1007/s10618-015-0418-x; Keogh E, 2005, KNOWL INF SYST, V7, P358, DOI 10.1007/s10115-004-0154-9; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lei Hansheng, 2004, COMP VIS PATT REC WO, P76; Lemire D, 2009, PATTERN RECOGN, V42, P2169, DOI 10.1016/j.patcog.2008.11.030; Mei JY, 2016, IEEE T CYBERNETICS, V46, P1363, DOI 10.1109/TCYB.2015.2426723; Muda L., 2010, J COMPUTING; Mueen A, 2018, KNOWL INF SYST, V54, P237, DOI 10.1007/s10115-017-1119-0; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Petitjean F, 2012, THEOR COMPUT SCI, V414, P76, DOI 10.1016/j.tcs.2011.09.029; Sakurai Y., 2007, P INT C DAT, P1046, DOI DOI 10.1109/ICDE.2007.368963; Shah M, 2016, PROCEEDINGS OF THE THIRD ACM IKDD CONFERENCE ON DATA SCIENCES (CODS), DOI 10.1145/2888451.2888456; Shokoohi-Yekta M, 2017, DATA MIN KNOWL DISC, V31, P1, DOI 10.1007/s10618-016-0455-0; Steffen Peter, 2009, Parallel Processing and Applied Mathematics. 8th International Conference, PPAM 2009. Revised Selected Papers, P290; Varatharajan R, 2018, COMPUT ELECTR ENG, V70, P447, DOI 10.1016/j.compeleceng.2017.05.035; Wang FY, 2017, IEEE-CAA J AUTOMATIC, V4, P1, DOI 10.1109/JAS.2017.7510310	25	16	16	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903029
C	Koh, PW; Ang, KS; Teo, HHK; Liang, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Koh, Pang Wei; Ang, Kai-Siang; Teo, Hubert H. K.; Liang, Percy			On the Accuracy of Influence Functions for Measuring Group Effects	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Influence functions estimate the effect of removing a training point on a model without the need to retrain. They are based on a first-order Taylor approximation that is guaranteed to be accurate for sufficiently small changes to the model, and so are commonly used to study the effect of individual points in large datasets. However, we often want to study the effects of large groups of training points, e.g., to diagnose batch effects or apportion credit between different data sources. Removing such large groups can result in significant changes to the model. Are influence functions still accurate in this setting? In this paper, we find that across many different types of groups and for a range of real-world datasets, the predicted effect (using influence functions) of a group correlates surprisingly well with its actual effect, even if the absolute and relative errors are large. Our theoretical analysis shows that such strong correlation arises only under certain settings and need not hold in general, indicating that real-world datasets have particular properties that allow the influence approximation to be accurate.	[Koh, Pang Wei; Ang, Kai-Siang; Teo, Hubert H. K.; Liang, Percy] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Koh, PW (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	pangwei@cs.stanford.edu; kaiang@.stanford.edu; hteo@.stanford.edu; pliang@cs.stanford.edu			Open Philanthropy Project Award; Facebook Fellowship Program	Open Philanthropy Project Award; Facebook Fellowship Program(Facebook Inc)	We are grateful to Zhenghao Chen, Brad Efron, Jean Feng, Tatsunori Hashimoto, Robin Jia, Stephen Mussmann, Aditi Raghunathan, Marco Tulio Ribeiro, Noah Simon, Jacob Steinhardt, and Jian Zhang for helpful discussions and comments. We are further indebted to Ryan Giordano, Ruoxi Jia, andWill Stephenson for discussion about prior work, and Samuel Bowman, Braden Hancock, Emma Pierson, and Pranav Rajpurkar for their assistance with applications and datasets. This work was funded by an Open Philanthropy Project Award. PWK was supported by the Facebook Fellowship Program.	Arrieta-Ibarra I, 2018, AEA PAP P, V108, P38, DOI 10.1257/pandp.20181003; Boyd S, 2004, CONVEX OPTIMIZATION; Brunet M., 2018, ARXIV181003611; COOK RD, 1977, TECHNOMETRICS, V19, P15, DOI 10.2307/1268249; Debruyne M, 2008, J MACH LEARN RES, V9, P2377; FAN QC, 2018, ADV NEURAL INFORM PR, P3539; Geva M., 2019, EMPIRICAL METHODS NA; Ghorbani A., 2019, ARXIV190402868; Giordano R, 2019, P 22 INT C ART INT S, P1139; Giordano R., 2019, ARXIV190712116; HAMPEL FR, 1974, J AM STAT ASSOC, V69, P383, DOI 10.2307/2285666; Hampel FR., 2011, WILEY SERIES PROBABI; Hancock B., 2018, ASS COMPUTATIONAL LI; Hayes J., 2018, P ADV NEURAL INFORM, P6604; Jaeckel L. A., 1972, INFINITESIMAL UNPUB; Jia R., 2019, ARXIV190210275; Khanna R., 2019, P 22 INT C ART INT S, P3382; Koh P. W., 2019, ARXIV181100741; Koh P. W., 2017, INT C MACH LEARN ICM; Langmead B, 2010, GENOME BIOL, V11, DOI 10.1186/gb-2010-11-8-r83; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu Y, 2014, PR MACH LEARN RES, V32; Metsis V., 2006, CEAS, V17, P28; PREGIBON D, 1981, ANN STAT, V9, P705, DOI 10.1214/aos/1176345513; Rad K., 2018, ARXIV180110243; Ratner A, 2016, ADV NEUR IN, V29; Schulam P., 2019, 22 INT C ART INT STA, V89, P1022; Steinhardt J., 2017, ADV NEURAL INFORM PR; Strack B, 2014, BIOMED RES INT, V2014, DOI 10.1155/2014/781670; Wang H, 2019, ARXIV190110501; Wang K, 2015, I S MOD ANAL SIM COM, P154, DOI 10.1109/MASCOTS.2015.34; Williams A., 2018, P C N AM CHAPT ASS C, V1, P1112, DOI DOI 10.18653/V1/N18-1101; Zhou J., 2019, C HUM FACT COMP SYST	33	16	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305027
C	Li, SY; Wang, R; Tang, MX; Zhang, CJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Siyuan; Wang, Rui; Tang, Minxue; Zhang, Chongjie			Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Hierarchical Reinforcement Learning (HRL) is a promising approach to solving long-horizon problems with sparse and delayed rewards. Many existing HRL algorithms either use pre-trained low-level skills that are unadaptable, or require domain-specific information to define low-level rewards. In this paper, we aim to adapt low-level skills to downstream tasks while maintaining the generality of reward design. We propose an HRL framework which sets auxiliary rewards for low-level skill training based on the advantage function of the high-level policy. This auxiliary reward enables efficient, simultaneous learning of the high-level policy and low-level skills without using task-specific knowledge. In addition, we also theoretically prove that optimizing low-level skills with this auxiliary reward will increase the task return for the joint policy. Experimental results show that our algorithm dramatically outperforms other state-of-the-art HRL methods in Mujoco domains(2). We also find both low-level and high-level policies trained by our algorithm transferable.	[Li, Siyuan; Zhang, Chongjie] Tsinghua Univ, IIIS, Beijing, Peoples R China; [Wang, Rui; Tang, Minxue] Tsinghua Univ, Beijing, Peoples R China	Tsinghua University; Tsinghua University	Li, SY (corresponding author), Tsinghua Univ, IIIS, Beijing, Peoples R China.	sy-li17@mails.tsinghua.edu.cn; rui1@stanford.edu; tangmx16@mails.tsinghua.edu.cn; chongjie@tsinghua.edu.cn			Huawei Noah's Ark Lab [YBN2018055043]	Huawei Noah's Ark Lab(Huawei Technologies)	The authors would like to thank the anonymous reviewers for their valuable comments and helpful suggestions. The work is supported by Huawei Noah's Ark Lab under Grant No. YBN2018055043.	Al-Emran Mostafa, 2015, INT J COMPUTING DIGI, V4; Azizzadenesheli K., 2018, ARXIV181007900; Chentanez, 2005, ADV NEURAL INFORM PR, P1281, DOI DOI 10.21236/ADA440280; Devlin SM, 2012, P 11 INT C AUTONOMOU, V1, P433; Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639; Duan Yan, 2016, ARXIV160406778; Dwiel Zach, 2019, HIERARCHICAL POLICY; Eysenbach B., 2018, DIVERSITY IS ALL YOU; Florensa Carlos, 2017, P 34 INT C MACH LEAR; Frans Kevin, 2018, INT C LEARN REPR; Ghosh D, 2018, 7 INT C LEARN REPR; Gu SX, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (ICMA), P387, DOI 10.1109/ICMA.2017.8015848; Gupta A., 2017, ARXIV170302949; Heess Nicolas, 2016, LEARNING TRANSFER MO; Hinton, 1993, ADV NEURAL INFORM PR, V5, P271; Jang E., 2017, ICLR; Konidaris G., 2006, P 23 INT C MACH LEAR, P489, DOI DOI 10.1145/1143844.1143906; Konidaris G, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P895; Kulkarni T. D., 2016, PROC 30 INT C NEURAL, P3675; Levy A., 2018, PROC INT C LEARN REP; Li Siyuan, 2018, ARXIV180603793; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nachum O., 2018, NEURIPS; Nachum O., 2018, NEAR OPTIMAL REPRESE; Schulman J., 2015, ICML; Schulman John, 2015, ARXIV150602438; Shu Tianmin, 2017, ARXIV171207294; Sutton R. S., 1985, TEMPORAL CREDIT ASSI; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Tai L, 2017, IEEE INT C INT ROBOT, P31; Vezhnevets AS, 2017, PR MACH LEARN RES, V70	34	16	16	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301040
C	Liu, FL; Liu, YX; Ren, XC; He, XD; Sun, X		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Fenglin; Liu, Yuanxin; Ren, Xuancheng; He, Xiaodong; Sun, Xu			Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In vision-and-language grounding problems, fine-grained representations of the image are considered to be of paramount importance. Most of the current systems incorporate visual features and textual concepts as a sketch of an image. However, plainly inferred representations are usually undesirable in that they are composed of separate components, the relations of which are elusive. In this work, we aim at representing an image with a set of integrated visual regions and corresponding textual concepts, reflecting certain semantics. To this end, we build the Mutual Iterative Attention (MIA) module, which integrates correlated visual features and textual concepts, respectively, by aligning the two modalities. We evaluate the proposed approach on two representative vision-and-language grounding tasks, i.e., image captioning and visual question answering. In both tasks, the semantic-grounded image representations consistently boost the performance of the baseline models under all metrics across the board. The results demonstrate that our approach is effective and generalizes well to a wide range of models for image-related applications.(2)	[Liu, Fenglin] Peking Univ, Sch ECE, ADSPLAB, Shenzhen, Peoples R China; [Ren, Xuancheng; Sun, Xu] Peking Univ, Sch EECS, MOE Key Lab Computat Linguist, Beijing, Peoples R China; [Liu, Yuanxin] Chinese Acad Sci, Inst Informat Engn, Beijing, Peoples R China; [Liu, Yuanxin] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing, Peoples R China; [He, Xiaodong] JD AI Res, Beijing, Peoples R China	Peking University; Peking University; Chinese Academy of Sciences; Institute of Information Engineering, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Liu, FL (corresponding author), Peking Univ, Sch ECE, ADSPLAB, Shenzhen, Peoples R China.	fenglinliu98@pku.edu.cn; liuyuanxin@iie.ac.cn; renxc@pku.edu.cn; xiaodong.he@jd.com; xusun@pku.edu.cn			National Natural Science Foundation of China [61673028]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by National Natural Science Foundation of China (No. 61673028). We thank all the anonymous reviewers for their constructive comments and suggestions. Xu Sun is the corresponding author of this paper.	Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Ba L. J., 2016, CORRABS160706450; Banerjee Satanjeev, 2005, P ACL WORKSH INTR EX, P65; Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285; Chen X, 2015, CORR, V1504, P325; Nguyen DK, 2018, PROC CVPR IEEE, P6087, DOI 10.1109/CVPR.2018.00637; Fang H., 2015, C COMP VIS PATT REC; Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kim JH, 2018, ADV NEUR IN, V31; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Lin C.-Y., 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.3115/V1/D14-1020; Liu FL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5095; Liu Fenglin, 2018, EMNLP; Lu J., 2018, CVPR; Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345; Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232; Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wu Q., 2016, P CVPR; Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094; Yao T., 2018, P EUR C COMP VIS ECC, P684; Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Zhang C, 2006, ADV NEURAL INFORM PR, P1417	36	16	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306081
C	Tran, NT; Tran, VH; Nguyen, NB; Yang, LX; Cheung, NM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ngoc-Trung Tran; Viet-Hung Tran; Ngoc-Bao Nguyen; Yang, Linxiao; Cheung, Ngai-Man			Self-supervised GAN: Analysis and Improvement with Multi-class Minimax Game	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Self-supervised (SS) learning is a powerful approach for representation learning using unlabeled data. Recently, it has been applied to Generative Adversarial Networks (GAN) training. Specifically, SS tasks were proposed to address the catastrophic forgetting issue in the GAN discriminator. In this work, we perform an in-depth analysis to understand how SS tasks interact with learning of generator. From the analysis, we identify issues of SS tasks which allow a severely mode-collapsed generator to excel the SS tasks. To address the issues, we propose new SS tasks based on a multi-class minimax game. The competition between our proposed SS tasks in the game encourages the generator to learn the data distribution and generate diverse samples. We provide both theoretical and empirical analysis to support that our proposed SS tasks have better convergence property. We conduct experiments to incorporate our proposed SS tasks into two different GAN baseline models. Our approach establishes state-of-the-art FID scores on CIFAR-10, CIFAR-100, STL-10, CelebA, Imagenet 32 x 32 and Stacked-MNIST datasets, outperforming existing works by considerable margins in some cases. Our unconditional GAN model approaches performance of conditional GAN without using labeled data. Our code: https //github.com/tntrung/msgan	[Ngoc-Trung Tran; Viet-Hung Tran; Ngoc-Bao Nguyen; Yang, Linxiao; Cheung, Ngai-Man] Singapore Univ Technol & Design SUTD, Singapore, Singapore	Singapore University of Technology & Design	Cheung, NM (corresponding author), Singapore Univ Technol & Design SUTD, Singapore, Singapore.	ngaiman-cheung@sutd.edu.sg			ST Electronics; National Research Foundation(NRF), Prime Minister's Office, Singapore under Corporate Laboratory @ University Scheme (Programme Title: STEE Infosec -SUTD Corporate Laboratory); National Research Foundation Singapore under its Al Singapore Programme [AISG-1OOE2018-005]; Energy Market Authority [NRF2017EWT-EP003-06 1]; SUTD project [PIE-SGP-AI-2018-01]	ST Electronics; National Research Foundation(NRF), Prime Minister's Office, Singapore under Corporate Laboratory @ University Scheme (Programme Title: STEE Infosec -SUTD Corporate Laboratory)(Singapore University of Technology & DesignNational Research Foundation, Singapore); National Research Foundation Singapore under its Al Singapore Programme; Energy Market Authority; SUTD project(Singapore University of Technology & Design)	This work was supported by ST Electronics and the National Research Foundation(NRF), Prime Minister's Office, Singapore under Corporate Laboratory @ University Scheme (Programme Title: STEE Infosec -SUTD Corporate Laboratory). This research was also supported by the National Research Foundation Singapore under its Al Singapore Programme [Award Number: AISG-1OOE2018-005]. This research was also supported in part by the Energy Market Authority (EP award no. NRF2017EWT-EP003-06 1). This project was also supported by SUTD project PIE-SGP-AI-2018-01.	[Anonymous], 2017, ARXIV170903831; [Anonymous], 2018, ARXIV180306107; Arjovsky Mart<prime>in, 2017, P 5 INT C LEARN REPR; Boesen Anders, 2015, ARXIV151209300; Brock Andrew, 2018, ARXIV180911096; Che Tong, 2016, ARXIV161202136; Chen TS, 2019, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2019.00632; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Donahue J., 2016, ARXIV160509782; Dumoulin Vincent, 2016, ARXIV E PRINTS; Ghosh A, 2018, PROC CVPR IEEE, P8513, DOI 10.1109/CVPR.2018.00888; Gidaris Spyros, 2018, ARXIV180307728; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow Ian, 2016, NIPS 2016 TUTORIAL G; Gulrajani I, 2017, P NIPS 2017; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Karras T., 2017, PROGR GROWING GANS I; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma Diederik P, 2013, ARXIV13126114; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Kodali Naveen, 2017, ARXIV170507215; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lee K.S., 2019, NEURIPS 2019 WORKSH; Li Chunyuan, 2017, NIPS; Lim S. K., 2018, P IEEE INT C DAT MIN; Liu XY, 2019, PROC CVPR IEEE, P4268, DOI 10.1109/CVPR.2019.00440; Makhzani A., 2015, ARXIV151105644; Metz Luke, 2017, ICLR; Noroozi M, 2017, IEEE I CONF COMP VIS, P5899, DOI 10.1109/ICCV.2017.628; Odena A, 2017, PR MACH LEARN RES, V70; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Petzka H., 2017, ARXIV170908894; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S, 2016, PR MACH LEARN RES, V48; Salimans T, 2016, ADV NEUR IN, V29; Schlegl T., 2017, CORR; Tran Ngoc-Trung, 2019, AAAI; Tran Ngoc-Trung, 2019, ARXIV190505469 CORR; Xiang S, 2017, ARXIV170403971; Yang LX, 2019, IEEE I CONF COMP VIS, P6449, DOI 10.1109/ICCV.2019.00654; Yazici Y., 2018, INT C LEARN REPR ICL; Zenke F, 2017, PR MACH LEARN RES, V70; Zhang Han, 2018, ARXIV180508318; Zhang Hanwang, 2017, PROC CVPR IEEE, P5532, DOI [DOI 10.1109/CVPR.2017.331, DOI 10.1109/CVPR.2018.00611]; Zhang R, 2017, PROC CVPR IEEE, P645, DOI 10.1109/CVPR.2017.76; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhu Jun-Yan, 2017, ICCV	54	16	16	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904084
C	Sun, Y; Liu, JM; Kamilov, US		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sun, Yu; Liu, Jiaming; Kamilov, Ulugbek S.			Block Coordinate Regularization by Denoising	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				THRESHOLDING ALGORITHM; INVERSE PROBLEMS; IMAGE; RECONSTRUCTION; CONVERGENCE; SHRINKAGE; SPARSE	We consider the problem of estimating a vector from its noisy measurements using a prior specified only through a denoising function. Recent work on plug-and-play priors (PnP) and regularization-by-denoising (RED) has shown the state-of-the-art performance of estimators under such priors in a range of imaging tasks. In this work, we develop a new block coordinate RED algorithm that decomposes a large-scale estimation problem into a sequence of updates over a small subset of the unknown variables. We theoretically analyze the convergence of the algorithm and discuss its relationship to the traditional proximal optimization. Our analysis complements and extends recent theoretical results for RED-based estimation methods. We numerically validate our method using several denoiser priors, including those based on convolutional neural network (CNN) denoisers.	[Sun, Yu; Liu, Jiaming; Kamilov, Ulugbek S.] Washington Univ, St Louis, MO 63110 USA	Washington University (WUSTL)	Sun, Y (corresponding author), Washington Univ, St Louis, MO 63110 USA.	sun.yu@wustl.edu; jiaming.liu@wustl.edu; kamilov@wustl.edu			NSF [CCF-1813910]; NVIDIA Corporation	NSF(National Science Foundation (NSF)); NVIDIA Corporation	This material is based upon work supported in part by NSF award CCF-1813910 and by NVIDIA Corporation with the donation of the Titan Xp GPU for research. The authors thank B. Wohlberg as well as anonymous reviewers for insightful comments.	Afonso MV, 2010, IEEE T IMAGE PROCESS, V19, P2345, DOI 10.1109/TIP.2010.2047910; Bauschke HH, 2017, CONVEX ANAL MONOTONE, DOI 10.1007/978-3-319-48311-5; Beck A, 2013, SIAM J OPTIMIZ, V23, P2037, DOI 10.1137/120887679; Beck A, 2009, IEEE T IMAGE PROCESS, V18, P2419, DOI 10.1109/TIP.2009.2028250; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bect J, 2004, LECT NOTES COMPUT SC, V2034, P1; Bigdeli SA, 2017, ADV NEUR IN, V30; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Buades A, 2010, SIAM REV, V52, P113, DOI 10.1137/090773908; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Chan SH, 2017, IEEE T COMPUT IMAG, V3, P84, DOI 10.1109/TCI.2016.2629286; Chow YT, 2017, SIAM J SCI COMPUT, V39, pA1280, DOI 10.1137/16M1102653; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Danielyan A, 2012, IEEE T IMAGE PROCESS, V21, P1715, DOI 10.1109/TIP.2011.2176954; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Farrens S, 2017, ASTRON ASTROPHYS, V601, DOI 10.1051/0004-6361/201629709; Fercoq O., 2018, LECT NOTES OPTIMIZAT; Figueiredo MAT, 2003, IEEE T IMAGE PROCESS, V12, P906, DOI 10.1109/TIP.2003.814255; Fletcher AK, 2018, ADV NEUR IN, V31; Gouk H., 2018, ARXIV180404368; Han Y, 2018, MAGN RESON MED, V80, P1189, DOI 10.1002/mrm.27106; Jin KH, 2017, IEEE T IMAGE PROCESS, V26, P4509, DOI 10.1109/TIP.2017.2713099; Kak A.V., 1988, PRINCIPLES COMPUTERI, DOI DOI 10.1137/1.9780898719277.CH7; Kamilov US, 2017, IEEE SIGNAL PROC LET, V24, P1872, DOI 10.1109/LSP.2017.2763583; Knoll F, 2011, MAGN RESON MED, V65, P480, DOI 10.1002/mrm.22595; Mataev G., 2019, P IEEE CVF INT C COM; Metzler CA, 2016, IEEE INT CONF MULTI; Metzler CA, 2018, PR MACH LEARN RES, V80; Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Ng MK, 2010, SIAM J SCI COMPUT, V32, P2710, DOI 10.1137/090774823; Niu F., 2011, P ADV NEUR INF PROC, V24; Ono S, 2017, IEEE SIGNAL PROC LET, V24, P1108, DOI 10.1109/LSP.2017.2710233; Parikh N., 2014, FDN TRENDS OPTIM, V1, P127, DOI DOI 10.1561/2400000003; Peng ZM, 2016, ANN MATH SCI APPL, V1, P57, DOI 10.4310/AMSA.2016.v1.n1.a2; Peng ZM, 2016, SIAM J SCI COMPUT, V38, pA2851, DOI 10.1137/15M1024950; Rockafellar RT., 1998, VARIATIONAL ANAL, DOI 10.1007/978-3-642-02431-3; Romano Y, 2017, SIAM J IMAGING SCI, V10, P1804, DOI 10.1137/16M1102884; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Ryu EK, 2019, PR MACH LEARN RES, V97; Ryu EK, 2016, APPL COMPUT MATH-BAK, V15, P3; Sedghi H., 2019, INT C LEARN REPR ICL; Sreehari S, 2016, IEEE T COMPUT IMAG, V2, P408, DOI 10.1109/TCI.2016.2599778; Sun Y., 2019, ARXIV190505113CSCV; Sun Y, 2019, IEEE T COMPUT IMAG, V5, P395, DOI 10.1109/TCI.2019.2893568; Tan J, 2015, IEEE T SIGNAL PROCES, V63, P2085, DOI 10.1109/TSP.2015.2408558; Teodoro AM, 2019, IEEE T IMAGE PROCESS, V28, P451, DOI 10.1109/TIP.2018.2869727; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105; Venkatakrishnan S. V., 2013, P IEEE GLOB C SIGN P; Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3; Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI [10.1109/TIP.2016.2621478, 10.1109/TIP.2017.2689999]; Yu Y.-L., 2013, P ADV NEUR INF PROC, V26; Zbontar, 2018, ARXIV181108839; ZHANG K, 2017, PROC CVPR IEEE, P2808, DOI DOI 10.1109/CVPR.2017.300; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	65	16	16	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300035
C	Verma, G; Swami, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Verma, Gunjan; Swami, Ananthram			Error Correcting Output Codes Improve Probability Estimation and Adversarial Robustness of Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Modern machine learning systems are susceptible to adversarial examples; inputs which clearly preserve the characteristic semantics of a given class, but whose classification is (usually confidently) incorrect. Existing approaches to adversarial defense generally rely on modifying the input, e.g. quantization, or the learned model parameters, e.g. via adversarial training. However, recent research has shown that most such approaches succumb to adversarial examples when different norms or more sophisticated adaptive attacks are considered. In this paper, we propose a fundamentally different approach which instead changes the way the output is represented and decoded. This simple approach achieves state-of-the-art robustness to adversarial examples for L-2 and L-infinity based adversarial perturbations on MNIST and CIFAR10. In addition, even under strong white-box attacks, we find that our model often assigns adversarial examples a low probability; those with high probability are often interpretable, i.e. perturbed towards the perceptual boundary between the original and adversarial class. Our approach has several advantages: it yields more meaningful probability estimates, is extremely fast during training and testing, requires essentially no architectural changes to existing discriminative learning pipelines, is wholly complementary to other defense approaches including adversarial training, and does not sacrifice benign test set performance.	[Verma, Gunjan; Swami, Ananthram] CCDC Army Res Lab, Adelphi, MD 20783 USA		Verma, G (corresponding author), CCDC Army Res Lab, Adelphi, MD 20783 USA.	gunjan.verma.civ@mail.mil; ananthram.swami.civ@mail.mil						Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; [Anonymous], 2017, ARXIV171209196; [Anonymous], 2019, DARPA PROGRAM GUARAN; Athalye A., 2018, P 35 INT C MACH LEAR; Blaser R, 2016, J MACH LEARN RES, V17; Buckman J., 2018, INT C LEARN REPR; Carlini N., 2019, CORR; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Cohen Jeremy M, 2019, ARXIV190202918; Dhillon GS., 2018, P 6 INT C LEARN REPR; Dietterich TG, 1994, J ARTIF INTELL RES, V2, P263; Escalera S, 2010, J MACH LEARN RES, V11, P661; Gal Y., 2018, ARXIV180308533; Goodfellow I. J., 2014, ARXIV14126572; Guo CA, 2017, PR MACH LEARN RES, V70; Madry A., 2018, ARXIV PREPRINT ARXIV; Papernot N., 2016, ARXIV160202697; Schmidt L., 2017, ARXIV171202779; Schott Lukas, 2018, 1 ADVERSARIALLY ROBU; Song Y, 2017, ARXIV171010766; Tao G., 2018, ADV NEURAL INFORM PR, P7717; Tramer F., 2017, ARXIV170507204; Tsymbal A., 2005, Information Fusion, V6, P83, DOI 10.1016/j.inffus.2004.04.003; Wong E, 2018, ARXIV180512514; Wong Eric, 2017, ARXIV PREPRINT ARXIV; Xu W., 2017, P 25 ANN NETW DISTR, DOI DOI 10.14722/NDSS.2018.23198; Zhang H., 2019, ARXIV190104684; Zheng TH, 2019, AAAI CONF ARTIF INTE, P2253	28	16	16	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900026
C	Yu, M; Yang, ZR; Kolar, M; Wang, ZR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yu, Ming; Yang, Zhuoran; Kolar, Mladen; Wang, Zhaoran			Convergent Policy Optimization for Safe Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COMPREHENSIVE SURVEY; GO; GRADIENT; GAME	We study the safe reinforcement learning problem with nonlinear function approximation, where policy optimization is formulated as a constrained optimization problem with both the objective and the constraint being nonconvex functions. For such a problem, we construct a sequence of surrogate convex constrained optimization problems by replacing the nonconvex functions locally with convex quadratic functions obtained from policy gradient estimators. We prove that the solutions to these surrogate problems converge to a stationary point of the original nonconvex problem. Furthermore, to extend our theoretical results, we apply our algorithm to examples of optimal control and multi-agent reinforcement learning with safety constraints.	[Yu, Ming; Kolar, Mladen] Univ Chicago, Booth Sch Business, Chicago, IL 60637 USA; [Yang, Zhuoran] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA; [Wang, Zhaoran] Northwestern Univ, Dept Ind Engn & Management Sci, Evanston, IL USA	University of Chicago; Princeton University; Northwestern University	Yu, M (corresponding author), Univ Chicago, Booth Sch Business, Chicago, IL 60637 USA.	ming93@uchicago.edu	Wang, Zhaoran/P-7113-2018					Achiam J, 2017, PR MACH LEARN RES, V70; Altman E., 1999, STOCH MODEL SER; Ammar HB, 2015, PR MACH LEARN RES, V37, P2361; Amodei D., 2016, CONCRETE PROBLEMS AI; Anderson B. D., 2007, OPTIMAL CONTROL LINE; [Anonymous], 2018, REINFORCEMENT LEARNI, DOI 10.1016/S1364-6613(99)01331-5; Bai Y., 2019, ARXIV190512849; Basar T., 2018, ARXIV181203239; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Berkenkamp Felix, 2017, ADV NEURAL INFORM PR, P908; Borkar VS, 1997, SYST CONTROL LETT, V29, P291, DOI 10.1016/S0167-6911(97)90015-3; Boutilier C, 1996, THEORETICAL ASPECTS OF RATIONALITY AND KNOWLEDGE, P195; Bradtke S. J., 1993, ADV NEURAL INFORM PR, P295; BRADTKE SJ, 1994, PROCEEDINGS OF THE 1994 AMERICAN CONTROL CONFERENCE, VOLS 1-3, P3475; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Busoniu L, 2008, IEEE T SYST MAN CY C, V38, P156, DOI 10.1109/TSMCC.2007.913919; Cai Qi, 2019, ARXIV190510027; Chow Y, 2017, J MACHINE LEARNING R, V18, P1; Chow Y., 2018, ARXIV180507708; Dean S., 2017, ARXIV171001688; Dunford N., 1958, LINEAR OPERATORS 1, V7; Evans L. C., 2005, LECT NOTES; Fazel M., 2018, INT C MACHINE LEARNI, P1466; Fisac J. F., 2018, IEEE T AUTOMATIC CON; Garcia J, 2015, J MACH LEARN RES, V16, P1437; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grondman I, 2012, IEEE T SYST MAN CY C, V42, P1291, DOI 10.1109/TSMCC.2012.2218595; He JY, 2019, PR MACH LEARN RES, V89; He Jingyu, 2019, TECHNICAL REPORT; Huang Jessie, 2018, ARXIV180508313; Kelley J.L, 2017, GEN TOPOLOGY; Konda VR, 2000, ADV NEUR IN, V12, P1008; Lacotte J., 2018, ARXIV180804468; Lee D., 2018, 14 ART INT INT DIG E; Leonard Adolphs, 2018, THESIS; LI Y., 2017, ARXIV; Liu A., 2018, ARXIV180108266; Liu B., 2019, ADV NEUR IN; Matthew Kretchmar R, 2002, 6 WORLD C SYST CYB I; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nair A., 2015, ARXIV150704296; Paternain S., 2018, THESIS; Peng P., 2017, ARXIV170310069; Pirotta M, 2015, MACH LEARN, V100, P255, DOI 10.1007/s10994-015-5484-1; Prashanth LA, 2016, MACH LEARN, V105, P367, DOI 10.1007/s10994-016-5569-5; Recht B., 2018, ARXIV180609460; Rhee CH, 2015, OPER RES, V63, P1026, DOI 10.1287/opre.2015.1404; RUSZCZYNSKI A, 1980, MATH PROGRAM, V19, P220, DOI 10.1007/BF01581643; Scharpff J, 2016, AAAI CONF ARTIF INTE, P3174; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Scutari G, 2014, IEEE T SIGNAL PROCES, V62, P641, DOI 10.1109/TSP.2013.2293126; Silver D, 2018, SCIENCE, V362, P1140, DOI 10.1126/science.aar6404; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sun P., 2018, ARXIV180907193, V1809; Sun Y, 2017, IEEE T SIGNAL PROCES, V65, P794, DOI 10.1109/TSP.2016.2601299; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Tessler C., 2018, ARXIV180511074; Turchetta M., 2016, ADV NEURAL INFORM PR, P4312; Vinyals Oriol, 2017, ARXIV170804782; Wang L., 2019, ARXIV190901150; Wen M., 2018, P INT C NEUR INF PRO, P7461; Xu S., 2019, P 15 AAAI C ART INT; Yang Y, 2016, IEEE T SIGNAL PROCES, V64, P2949, DOI 10.1109/TSP.2016.2531627; Yang Z., 2019, ARXIV190706246; Yang Z., 2018, ARXIV180600877; Zhang K., 2018, ARXIV181202783	69	16	16	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303015
C	Zhu, H; Chang, DQ; Xu, ZR; Zhang, PY; Li, X; He, J; Li, H; Xu, J; Gai, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhu, Han; Chang, Daqing; Xu, Ziru; Zhang, Pengye; Li, Xiang; He, Jie; Li, Han; Xu, Jian; Gai, Kun			Joint Optimization of Tree-based Index and Deep Model for Recommender Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Large-scale industrial recommender systems are usually confronted with computational problems due to the enormous corpus size. To retrieve and recommend the most relevant items to users under response time limits, resorting to an efficient index structure is an effective and practical solution. The previous work Tree-based Deep Model (TDM) [34] greatly improves recommendation accuracy using tree index. By indexing items in a tree hierarchy and training a user-node preference prediction model satisfying a max-heap like property in the tree, TDM provides logarithmic computational complexity w.r.t. the corpus size, enabling the use of arbitrary advanced models in candidate retrieval and recommendation. In tree-based recommendation methods, the quality of both the tree index and the user-node preference prediction model determines the recommendation accuracy for the most part. We argue that the learning of tree index and preference model has interdependence. Our purpose, in this paper, is to develop a method to jointly learn the index structure and user preference prediction model. In our proposed joint optimization framework, the learning of index and user preference prediction model are carried out under a unified performance measure. Besides, we come up with a novel hierarchical user preference representation utilizing the tree index hierarchy. Experimental evaluations with two large-scale real-world datasets show that the proposed method improves recommendation accuracy significantly. Online A/B test results at a display advertising platform also demonstrate the effectiveness of the proposed method in production environments.	[Zhu, Han; Chang, Daqing; Xu, Ziru; Zhang, Pengye; Li, Xiang; He, Jie; Li, Han; Xu, Jian; Gai, Kun] Alibaba Grp, Beijing, Peoples R China; [Xu, Ziru] Tsinghua Univ, Sch Software, Beijing, Peoples R China	Alibaba Group; Tsinghua University	Zhu, H (corresponding author), Alibaba Grp, Beijing, Peoples R China.	zhuhan.zh@alibaba-inc.com; daqing.cdq@alibaba-inc.com; ziru.xzr@alibaba-inc.com; pengye.zpy@alibaba-inc.com; yushi.lx@alibaba-inc.com; jay.hj@alibaba-inc.com; lihan.lh@alibaba-inc.com; xiyu.xj@alibaba-inc.com; jingshi.gk@alibaba-inc.com						Agrawal Rahul, 2013, WWW 13, P13; [Anonymous], 2013, SER JMLR WORKSHOP C; Beutel A, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P46, DOI 10.1145/3159652.3159727; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Cao Y, 2016, AAAI CONF ARTIF INTE, P3457; Covington P, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P191, DOI 10.1145/2959100.2959190; Daume H, 2017, PR MACH LEARN RES, V70; Davidson J., 2010, P 4 ACM C RECOMMENDE, P293, DOI DOI 10.1145/1864708.1864770; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; Han L., 2018, ICML, P1885; He RN, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P507, DOI 10.1145/2872427.2883037; He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569; Jain H, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P935, DOI 10.1145/2939672.2939756; Jin JQ, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2193, DOI 10.1145/3269206.3272021; Johnson Jeff, 2017, BILLION SCALE SIMILA; Kingma D.P., 2015, INT C LEARN REPR, P1; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Lian JX, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1754, DOI 10.1145/3219819.3220023; Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344; Liu T., 2004, NIPS 2004, P825; McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755; Morin F., 2005, AISTATS; Okura S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1933, DOI 10.1145/3097983.3098108; Pi Q, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2671, DOI 10.1145/3292500.3330666; Prabhu Y, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P993, DOI 10.1145/3178876.3185998; Prabhu Y, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P263, DOI 10.1145/2623330.2623651; Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127; Salakhutdinov R., 2007, ADV NEURAL INF PROCE, V20, P1257; Sarwar Badrul, 2001, P 10 INT C WORLD WID, P285, DOI 10.1145/371920.372071; Sun A., 2017, ARXIV170707435; Zhou GR, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1059, DOI 10.1145/3219819.3219823; Zhou Guorui, 2018, ARXIV180903672; Zhu H, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1079, DOI 10.1145/3219819.3219826; Zhu H, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2191, DOI 10.1145/3097983.3098134	34	16	16	7	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304002
C	Domke, J; Sheldon, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Domke, Justin; Sheldon, Daniel			Importance Weighting and Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI's practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions.	[Domke, Justin; Sheldon, Daniel] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA; [Sheldon, Daniel] Mt Holyoke Coll, Dept Comp Sci, S Hadley, MA 01075 USA	University of Massachusetts System; University of Massachusetts Amherst; Mount Holyoke College	Domke, J (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.				National Science Foundation [1617533]	National Science Foundation(National Science Foundation (NSF))	We thank Tom Rainforth for insightful comments regarding asymptotics and Theorem 3 and Linda Siew Li Tan for comments regarding Lemma 7. This material is based upon work supported by the National Science Foundation under Grant No. 1617533.	Agakov FV, 2004, LECT NOTES COMPUT SC, V3316, P561; Bachman Philip, 2015, NIPS WORKSH ADV APPR; Bamler Robert, 2017, NIPS; Bickel P.J., 2015, MATH STAT BASIC IDEA, VI; Burda Y., 2015, IMPORTANCE WEIGHTED; Cremer Chris, 2017, REINTERPRETING IMPOR; Dieng A. B., 2017, ADV NEURAL INFORM PR, P2729; Fang K.T., 1990, MONOGRAPHS STAT APPL, V36; GILKS WR, 1994, J R STAT SOC D-STAT, V43, P169, DOI DOI 10.2307/2348941; Kingma D., ICLR; Kucukelbir A, 2017, J MACH LEARN RES, V18, P1; Le Tuan Anh, 2018, ICLR; Maddison C. J., 2017, ADV NEURAL INFORM PR, P6576; Marcinkiewicz, 1937, FUND MATH, V29, P60, DOI [10.4064/fm-29-1-60-90, DOI 10.4064/FM-29-1-60-90]; Minka T., 2005, TECH REP; Minka T. P., 2001, UAI; Naesseth C. A., 2018, AISTATS; Rainforth Tom, TIGHTER VARIATIONAL; Ranganath R., 2014, AISTATS; Romon Gabriel, 2018, BOUNDS MOMENTS SAMPL; Ruiz Francisco J. R., 2016, UAI; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251; Stan Development Team, 2017, MOD LANG US GUID REF	24	16	16	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304048
C	Nguyen, DT; Kumar, A; Lau, HC		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Duc Thien Nguyen; Kumar, Akshat; Lau, Hoong Chuin			Credit Assignment For Collective Multiagent RL With Global Rewards	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				EXPLOITING ANONYMITY	Scaling decision theoretic planning to large multiagent systems is challenging due to uncertainty and partial observability in the environment. We focus on a multiagent planning model subclass, relevant to urban settings, where agent interactions are dependent on their "collective influence" on each other, rather than their identities. Unlike previous work, we address a general setting where system reward is not decomposable among agents. We develop collective actor-critic RL approaches for this setting, and address the problem of multiagent credit assignment, and computing low variance policy gradient estimates that result in faster convergence to high quality solutions. We also develop difference rewards based credit assignment methods for the collective setting. Empirically our new approaches provide significantly better solutions than previous methods in the presence of global rewards on two real world problems modeling taxi fleet optimization and multiagent patrolling, and a synthetic grid navigation domain.	[Duc Thien Nguyen; Kumar, Akshat; Lau, Hoong Chuin] Singapore Management Univ, Sch Informat Syst, 80 Stamford Rd, Singapore 178902, Singapore	Singapore Management University	Nguyen, DT (corresponding author), Singapore Management Univ, Sch Informat Syst, 80 Stamford Rd, Singapore 178902, Singapore.	dtnguyen.2014@smu.edu.sg; akshatkumar@smu.edu.sg; hclau@smu.edu.sg	LAU, Hoong Chuin/E-8556-2012	LAU, Hoong Chuin/0000-0002-5326-411X	National Research Foundation Singapore under its Corp Lab @ University scheme; Fujitsu Limited; A*STAR graduate scholarship	National Research Foundation Singapore under its Corp Lab @ University scheme(National Research Foundation, Singapore); Fujitsu Limited; A*STAR graduate scholarship(Agency for Science Technology & Research (A*STAR))	This research project is supported by National Research Foundation Singapore under its Corp Lab @ University scheme and Fujitsu Limited. First author is also supported by A*STAR graduate scholarship.	Agogino AK., 2004, AAMAS, V2, P980; Agussurja L, 2018, AAAI CONF ARTIF INTE, P6086; Amato C, 2015, IEEE INT CONF ROBOT, P1241, DOI 10.1109/ICRA.2015.7139350; Amato C, 2010, AUTON AGENT MULTI-AG, V21, P293, DOI 10.1007/s10458-009-9103-z; Asadi Kavosh, 2017, ARXIV170900503; Becker R, 2004, J ARTIF INTELL RES, V22, P423, DOI 10.1613/jair.1497; Becker Raphen, 2004, P 3 INT C AUT AG MUL, P302; Bernstein DS, 2002, MATH OPER RES, V27, P819, DOI 10.1287/moor.27.4.819.297; Chang YH, 2004, ADV NEUR IN, V16, P807; Chase J, 2019, IEEE T DEPEND SECURE, V16, P565, DOI 10.1109/TDSC.2017.2703626; Ciosek Kamil, 2018, AAAI C ART INT; Devlin S, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P165; Diaconis P, 1980, STUDIES INDUCTIVE LO, V2, P233; Dibangoye Jilles Steeve, 2018, INT C MACH LEARN, P1241; Nguyen DT, 2017, AAAI CONF ARTIF INTE, P3036; Durfee Ed, 2013, MULTIAGENT SYSTEMS, P485; Foerster JN, 2018, AAAI CONF ARTIF INTE, P2974; Guestrin C, 2002, ADV NEUR IN, V14, P1523; Gupta T, 2018, AAAI CONF ARTIF INTE, P6186; Kok JR, 2006, J MACH LEARN RES, V7, P1789; Konda VR, 2003, SIAM J CONTROL OPTIM, V42, P1143, DOI 10.1137/S0363012901385691; Liu LP, 2014, PR MACH LEARN RES, V32, P1602; Lowe R., 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295385; Nair R., 2005, AAAI, P133; Nguyen D. T., 2017, ADV NEURAL INFORM PR, P4322; Niepert M, 2014, AAAI CONF ARTIF INTE, P2467; Poupart P, 2004, ADV NEUR IN, V16, P823; Rashid T., 2018, INTERNATIONAL CONFER, P4292, DOI [10.48550/arXiv.1803.11485, DOI 10.48550/ARXIV.1803.11485]; Robbel P, 2016, AAAI CONF ARTIF INTE, P2537; Sheldon D. R., 2011, ADV NEURAL INFORM PR, P1161; Silver D, 2014, PR MACH LEARN RES, V32; Sonu E, 2015, P I C AUTOMAT PLAN S, P202; Spaan T. J., 2008, P 7 INT C AUT AG MUL, P525; Sun T, 2015, PR MACH LEARN RES, V37, P853; Sunehag P., 2017, ARXIV170605296; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Tan M., 1993, P 10 INT C MACHINE L, P330, DOI DOI 10.1016/B978-1-55860-307-3.50049-6; Tumer K, 2009, ADV COMPLEX SYST, V12, P475, DOI 10.1142/S0219525909002295; Tumer Kagan, 2007, AUTON AGENT MULTI-AG; Turner K., 2002, Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems, P378; Varakantham P. R., 2012, AAAI C ART INT, P1471; Varakantham P, 2014, AAAI CONF ARTIF INTE, P2505; Witwicki Stefan J., 2010, INT C AUT PLANN SCHE, P185; Yang Jiachen, 2018, INT C LEARN REPR; Yang Yaodong, 2018, ARXIV PREPRINT ARXIV, P5571, DOI DOI 10.1115/FEDSM2018-83038; Zhang Chongjie, 2011, AAAI C ART INT; Zhang Y, 2013, 2013 INTERNATIONAL CONFERENCE ON MECHANICAL AND AUTOMATION ENGINEERING (MAEE 2013), P110, DOI 10.1109/MAEE.2013.37	48	16	16	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002063
C	Hand, P; Leong, O; Voroninski, V		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hand, Paul; Leong, Oscar; Voroninski, Vladislav			Phase Retrieval Under a Generative Prior	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SIGNAL RECOVERY; CRYSTALLOGRAPHY	We introduce a novel deep learning inspired formulation of the phase retrieval problem, which asks to recover a signal y(0) is an element of 2 R-n from m quadratic observations, under structural assumptions on the underlying signal. As is common in many imaging problems, previous methodologies have considered natural signals as being sparse with respect to a known basis, resulting in the decision to enforce a generic sparsity prior. However, these methods for phase retrieval have encountered possibly fundamental limitations, as no computationally efficient algorithm for sparse phase retrieval has been proven to succeed with fewer than O (k(2) log n) generic measurements, which is larger than the theoretical optimum of O (k log n). In this paper, we propose a new framework for phase retrieval by modeling natural signals as being in the range of a deep generative neural network G : R-k -> R-n. We introduce an empirical risk formulation that has favorable global geometry for gradient methods, as soon as m = O (kd(2) log n), under the model of a d-layer fully-connected neural network with random weights. Specifically, when suitable deterministic conditions on the generator and measurement matrix are met, we construct a descent direction for any point outside of a small neighborhood around the true k-dimensional latent code and a negative multiple thereof. This formulation for structured phase retrieval thus benefits from two effects: generative priors can more tightly represent natural signals than sparsity priors, and this empirical risk formulation can exploit those generative priors at an information theoretically optimal sample complexity, unlike for a sparsity prior. We corroborate these results with experiments showing that exploiting generative models in phase retrieval tasks outperforms both sparse and general phase retrieval methods.	[Hand, Paul] Northeastern Univ, Boston, MA 02115 USA; [Leong, Oscar] Rice Univ, Houston, TX 77251 USA; [Voroninski, Vladislav] Helm Ai, Menlo Pk, CA USA	Northeastern University; Rice University	Hand, P (corresponding author), Northeastern Univ, Boston, MA 02115 USA.	p.hand@northeastern.edu; oscar.f.leong@rice.edu; vlad@helm.ai			NSF Graduate Research Fellowship [DGE-1450681]; NSF [DMS-1464525]	NSF Graduate Research Fellowship(National Science Foundation (NSF)); NSF(National Science Foundation (NSF))	OL acknowledges support by the NSF Graduate Research Fellowship under Grant No. DGE-1450681. PH acknowledges funding by the grant NSF DMS-1464525.	[Anonymous], 1962, MATH SCAND; [Anonymous], 2012, INTRO NONASYMPTOTIC; Arora  Sanjeev, 2015, ABS151105653 CORR; Ba J., 2017, P 3 INT C LEARN REPR; Bahmani S., 2015, P ADV NEUR INF PROC, V28, P523; Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x; Bora A., 2017, ARXIV170303208; Bunk O, 2007, ACTA CRYSTALLOGR A, V63, P306, DOI 10.1107/S0108767307021903; Cai TT, 2016, ANN STAT, V44, P2221, DOI 10.1214/16-AOS1443; Candes EJ, 2014, FOUND COMPUT MATH, V14, P1017, DOI 10.1007/s10208-013-9162-z; Candes Emmanuel J., 2017, IEEE T INFORM THEORY, V61, P195; Chandra R., 2017, AS C SIGN SYST COMP; Dainty JC, 1987, IMAG RECOVERY THEORY, V13, P231; Drury S. W., 2001, HONOURS ANAL LECT NO; FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758; GERCHBERG RW, 1972, OPTIK, V35, P237; Goldstein Tom, 2016, ARXIV161007531; Gunaydin  Harun, 2017, ARXIV170504286; Hand P., 2017, ARXIV170507576; Jaganathan K, 2013, IEEE INT SYMP INFO, P1022, DOI 10.1109/ISIT.2013.6620381; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee J, 2018, INT C LEARN REPR ICL; Li XD, 2013, SIAM J MATH ANAL, V45, P3019, DOI 10.1137/120893707; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Mark A, 2013, PROOF RIP SUBGAUSSIA; Miao JW, 2008, ANNU REV PHYS CHEM, V59, P387, DOI 10.1146/annurev.physchem.59.032607.093642; MILLANE RP, 1990, J OPT SOC AM A, V7, P394, DOI 10.1364/JOSAA.7.000394; Plan Y, 2013, COMMUN PUR APPL MATH, V66, P1275, DOI 10.1002/cpa.21442; Sun J, 2016, IEEE INT SYMP INFO, P2379, DOI 10.1109/ISIT.2016.7541725; Voroninski V., 2016, COMPRESSED SENSING P; Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9; Walther A., 1963, OPT ACTA, V10, P41, DOI DOI 10.1080/713817747; Wang G, 2018, IEEE T INFORM THEORY, V64, P773, DOI 10.1109/TIT.2017.2756858; Wang G, 2018, IEEE T SIGNAL PROCES, V66, P479, DOI 10.1109/TSP.2017.2771733; Yeh LH, 2015, OPT EXPRESS, V23, P33214, DOI 10.1364/OE.23.033214	37	16	16	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003067
C	Hong, ZW; Shann, TY; Su, SY; Chang, YH; Fu, TJ; Lee, CY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hong, Zhang-Wei; Shann, Tzu-Yun; Su, Shih-Yang; Chang, Yi-Hsiang; Fu, Tsu-Jui; Lee, Chun-Yi			Diversity-Driven Exploration Strategy for Deep Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off-and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure regularization to the loss function, the proposed methodology significantly enhances an agent's exploratory behavior, and thus prevents the policy from being trapped in local optima. We further propose an adaptive scaling strategy to enhance the performance. We demonstrate the effectiveness of our method in huge 2D gridworlds and a variety of benchmark environments, including Atari 2600 and MuJoCo. Experimental results validate that our method outperforms baseline approaches in most tasks in terms of mean scores and exploration efficiency.	[Hong, Zhang-Wei; Shann, Tzu-Yun; Su, Shih-Yang; Chang, Yi-Hsiang; Fu, Tsu-Jui; Lee, Chun-Yi] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu, Taiwan	National Tsing Hua University	Hong, ZW (corresponding author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu, Taiwan.	williamd4112@gapp.nthu.edu.tw; arielshann@gapp.nthu.edu.tw; at7788546@gapp.nthu.edu.tw; shawn420@gapp.nthu.edu.tw; rayfu1996ozig@gapp.nthu.edu.tw; cylee@gapp.nthu.edu.tw		Lee, Chun-Yi/0000-0002-4680-4800	Ministry of Science and Technology (MOST) in Taiwan; MediaTek Inc.; NVIDIA Corporation	Ministry of Science and Technology (MOST) in Taiwan(Ministry of Science and Technology, Taiwan); MediaTek Inc.; NVIDIA Corporation	The authors would like to thank Ministry of Science and Technology (MOST) in Taiwan and MediaTek Inc. for their funding support, and NVIDIA Corporation and NVAITC for their support of GPUs.	Bellemare M., 2016, NEURIPS; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Conti, 2017, ARXIV171206560; Fortunato, 2018, P INT C LEARN REPR I; Haarnoja T, 2018, PR MACH LEARN RES, V80; Houthooft R., 2016, ADV NEURAL INFORM PR, P1109; Lehman J, 2011, GENET EVOL COMPUT, P37; Lehman J, 2011, GECCO-2011: PROCEEDINGS OF THE 13TH ANNUAL GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P211; Lehman J, 2011, EVOL COMPUT, V19, P189, DOI 10.1162/EVCO_a_00025; Lillicrap, 2016, ARXIV150902971; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Osband Ian, 2017, ARXIV170307608; Pathak D., 2017, ICML 17, P2778; Peters J, 2010, AAAI CONF ARTIF INTE, P1607; Plappert, 2018, P INT C LEARN REPR I; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Schulman John, 2017, ARXIV170706347; Silver D, 2014, PR MACH LEARN RES, V32; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Stadie B. C., 2015, ARXIV150700814; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tang HL, 2017, INT CONF MEAS, P1, DOI [10.1109/ICMTMA.2017.0009, 10.1109/ICMTMA.2017.8]; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; van den Oord Aaron, 2016, ARXIV160605328; Wu YH, 2017, ADV NEUR IN, V30; Zhang M, 2016, IEEE INT CONF ROBOT, P520, DOI 10.1109/ICRA.2016.7487174	28	16	16	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005010
C	Jun, KS; Li, LH; Ma, YZ; Zhu, XJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jun, Kwang-Sung; Li, Lihong; Ma, Yuzhe; Zhu, Xiaojin			Adversarial Attacks on Stochastic Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study adversarial attacks that manipulate the reward signals to control the actions chosen by a stochastic multi-armed bandit algorithm. We propose the first attack against two popular bandit algorithms: 6-greedy and UCB, without knowledge of the mean rewards. The attacker is able to spend only logarithmic effort, multiplied by a problem-specific parameter that becomes smaller as the bandit problem gets easier to attack. The result means the attacker can easily hijack the behavior of the bandit algorithm to promote or obstruct certain actions, say, a particular medical treatment. As bandits are seeing increasingly wide use in practice, our study exposes a significant security threat.	[Jun, Kwang-Sung] Boston Univ, Boston, MA 02215 USA; [Li, Lihong] Google Brain, Mountain View, CA USA; [Ma, Yuzhe; Zhu, Xiaojin] UW Madison, Madison, WI USA	Boston University; Google Incorporated; University of Wisconsin System; University of Wisconsin Madison	Jun, KS (corresponding author), Boston Univ, Boston, MA 02215 USA.	kwangsung.jun@gmail.com; lihong@google.com; ma234@wisc.edu; jerryzhu@cs.wisc.edu	Ma, Yuzhe/CAF-7203-2022	Jun, Kwang-Sung/0000-0001-5483-3161	NSF [1837132, 1545481, 1704117, 1623605, 1561512]; MADLab AF Center of Excellence [FA9550-18-1-0166]	NSF(National Science Foundation (NSF)); MADLab AF Center of Excellence	This work is supported in part by NSF 1837132, 1545481, 1704117, 1623605, 1561512, and the MADLab AF Center of Excellence FA9550-18-1-0166.	Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Agarwal Alekh, 2016, CORR; Agrawal S., 2012, COLT, V39, P1; Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Chapelle O, 2015, ACM T INTEL SYST TEC, V5, DOI 10.1145/2532128; Dorigo M., 1997, ROBOT SHAPING EXPT B; Even-Dar E, 2009, MATH OPER RES, V34, P726, DOI 10.1287/moor.1090.0396; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Goodfellow Ian, 2017, 5 INT C LEARN REPR I; Greenewald K., 2017, ADV NEURAL INFORM PR, P5979; Kuleshov V., 2014, CORR; Kveton B, 2015, PR MACH LEARN RES, V37, P767; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Lykouris T, 2018, ACM S THEORY COMPUT, P114, DOI 10.1145/3188745.3188918; Ma Y., 2018, C DEC GAM THEOR SEC; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Zhu X., 2018, ARXIV PREPRINT ARXIV	24	16	16	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303062
C	Mordan, T; Thome, N; Henaff, G; Cord, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mordan, Taylor; Thome, Nicolas; Henaff, Gilles; Cord, Matthieu			Revisiting Multi-Task Learning with ROCK: a Deep Residual Auxiliary Block for Visual Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Multi-Task Learning (MTL) is appealing for deep learning regularization. In this paper, we tackle a specific MTL context denoted as primary MTL, where the ultimate goal is to improve the performance of a given primary task by leveraging several other auxiliary tasks. Our main methodological contribution is to introduce ROCK, a new generic multi-modal fusion block for deep learning tailored to the primary MTL context. ROCK architecture is based on a residual connection, which makes forward prediction explicitly impacted by the intermediate auxiliary representations. The auxiliary predictor's architecture is also specifically designed to our primary MTL context, by incorporating intensive pooling operators for maximizing complementarity of intermediate representations. Extensive experiments on NYUv2 dataset (object detection with scene classification, depth prediction, and surface normal estimation as auxiliary tasks) validate the relevance of the approach and its superiority to flat MTL approaches. Our method outperforms state-of-the-art object detection models on NYUv2 dataset by a large margin, and is also able to handle large-scale heterogeneous inputs (real and synthetic images) with missing annotation modalities.	[Mordan, Taylor; Cord, Matthieu] Sorbonne Univ, CNRS, Lab Informat Paris 6, LIP6, F-75005 Paris, France; [Mordan, Taylor; Henaff, Gilles] Thales Land & Air Syst, 2 Ave Gay Lussac, F-78990 Elancourt, France; [Thome, Nicolas] Conservatoire Natl Arts & Metiers, CEDRIC, 292 Rue St Martin, F-75003 Paris, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite; Thales Group; heSam Universite; Conservatoire National Arts & Metiers (CNAM); Institut Polytechnique de Paris	Mordan, T (corresponding author), Sorbonne Univ, CNRS, Lab Informat Paris 6, LIP6, F-75005 Paris, France.; Mordan, T (corresponding author), Thales Land & Air Syst, 2 Ave Gay Lussac, F-78990 Elancourt, France.	taylor.mordan@lip6.fr; nicolas.thome@cnam.fr; gilles.henaff@fr.thalesgroup.com; matthieu.cord@lip6.fr						Azizpour H, 2016, IEEE T PATTERN ANAL, V38, P1790, DOI 10.1109/TPAMI.2015.2500224; Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285; Bilen H, 2016, ADV NEUR IN, V29; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai Jifeng, 2016, ADV NEURAL INFORM PR, P379, DOI DOI 10.1016/J.JPOWSOUR.2007.02.075; Dharmasiri  Thanuja, 2017, P INT C INT ROB SYST, P5; Droniou  Alain, 2013, P INT C MACH LEARN I, P5; Durand T, 2016, PROC CVPR IEEE, P4743, DOI 10.1109/CVPR.2016.513; Durand T, 2015, IEEE I CONF COMP VIS, P2713, DOI 10.1109/ICCV.2015.311; Durand Thibaut, 2017, CVPR, DOI DOI 10.1109/CVPR.2017.631; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Fu H., 2017, ARXIV PREPRINT ARXIV; Fukui Akira, 2016, ARXIV160601847; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Gupta Saurabh, 2015, ARXIV150204652; Hane C, 2015, PROC CVPR IEEE, P381, DOI 10.1109/CVPR.2015.7298635; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hoffman J, 2016, PROC CVPR IEEE, P826, DOI 10.1109/CVPR.2016.96; Kingma D.P, P 3 INT C LEARNING R; Kokkinos I, 2017, PROC CVPR IEEE, P5454, DOI 10.1109/CVPR.2017.579; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Ladicky L, 2014, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2014.19; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Li J, 2017, IEEE I CONF COMP VIS, P3392, DOI 10.1109/ICCV.2017.365; Liu BY, 2010, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2010.5539823; Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2; Lu YX, 2017, PROC CVPR IEEE, P1131, DOI 10.1109/CVPR.2017.126; Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539; Meyerson E., 2018, P INT C LEARN REPR I; Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433; Mordan T, 2019, INT J COMPUT VISION, V127, P1659, DOI 10.1007/s11263-018-1109-z; Park  Seong-Jin, 2017, P IEEE INT C COMP VI, P5; Pechyony D., 2010, ADV NEURAL INFORM PR, P1894; Ren Zhongzheng, 2018, CVPR; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sharmanska  Viktoriia, 2014, ARXIV14100389, P2; Sharmanska Viktoriia, 2013, ICCV; Shi  Zhiyuan, 2017, P IEEE C COMP VIS PA, P2; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Spinello L, 2012, IEEE INT CONF ROBOT, P4469, DOI 10.1109/ICRA.2012.6225137; Vapnik V, 2015, J MACH LEARN RES, V16, P2023; Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042; Wang C., 2016, P IEEE C COMP VIS PA, P51; Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897; Yang Yongxin, 2017, P INT C MACH REPR; Zhang Yuting, 2017, CVPR	51	16	16	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301031
C	Smith, E; Fujimoto, S; Meger, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Smith, Edward; Fujimoto, Scott; Meger, David			Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SUPERRESOLUTION; RECOGNITION	We consider the problem of scaling deep generative shape models to high-resolution. Drawing motivation from the canonical view representation of objects, we introduce a novel method for the fast up-sampling of 3D objects in voxel space through networks that perform super-resolution on the six orthographic depth projections. This allows us to generate high-resolution objects with more efficient scaling than methods which work directly in 3D. We decompose the problem of 2D depth super-resolution into silhouette and depth prediction to capture both structure and fine detail. This allows our method to generate sharp edges more easily than an individual network. We evaluate our work on multiple experiments concerning high-resolution 3D objects, and show our system is capable of accurately predicting novel objects at resolutions as large as 512 x 512 x 512 - the highest resolution reported for this task. We achieve state-of-the-art performance on 3D object reconstruction from RGB images on the ShapeNet dataset, and further demonstrate the first effective 3D super-resolution method.	[Smith, Edward; Fujimoto, Scott; Meger, David] McGill Univ, Montreal, PQ, Canada	McGill University	Smith, E (corresponding author), McGill Univ, Montreal, PQ, Canada.	edward.smith@mail.mcgill.ca; scott.fujimoto@mail.mcgill.ca; dmeger@cim.mcgill.ca						Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2018, ARXIV180401654; Chang A. X., 2015, ARXIV PREPRINT ARXIV; Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Denton T, 2004, INT C PATT RECOG, P273, DOI 10.1109/ICPR.2004.1334159; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Fan H., 2017, C COMP VIS PATT REC, V38; Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hane Christian, 2017, ARXIV170400710; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hui T.-W., 2016, DEPTH MAP SUPER RESO, P353, DOI DOI 10.1007/978-3-319-46487-9_22; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Kar A., 2017, ADV NEURAL INFORM PR; Karras Tero, 2018, INT C LEARN REPR; Kato H., 2017, ARXIV171107566; Kazhdan M., 2003, Symposium on Geometry Processing, P156; Knopp J, 2010, LECT NOTES COMPUT SC, V6316, P589, DOI 10.1007/978-3-642-15567-3_43; KOENDERINK JJ, 1976, BIOL CYBERN, V24, P51, DOI 10.1007/BF00365595; Ledig C., 2016, IEEE COMPUTER SOC; Li Y., 2016, ADV NEURAL INFORM PR, P307; Liu Jerry, 2017, ARXIV170605170; Luong QT, 1996, COMPUT VIS IMAGE UND, V64, P193, DOI 10.1006/cviu.1996.0055; Mac Aodha O, 2012, LECT NOTES COMPUT SC, V7574, P71, DOI 10.1007/978-3-642-33712-3_6; Macrini D, 2002, INT C PATT RECOG, P24, DOI 10.1109/ICPR.2002.1047786; Mathieu M., 2016, INT C LEARN REPR ICL; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; MURASE H, 1995, INT J COMPUT VISION, V14, P5, DOI 10.1007/BF01421486; Osendorfer C, 2014, LECT NOTES COMPUT SC, V8836, P250, DOI 10.1007/978-3-319-12643-2_31; Park J, 2011, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2011.6126423; Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Pontes J. K., 2017, ARXIV171110669; Qi C.R., 2017, P 31 ANN C NEUR INF, P4; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Riegler Gernot, 2017, P INT C 3D VIS; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132; Sharma A, 2016, LECT NOTES COMPUT SC, V9915, P236, DOI 10.1007/978-3-319-49409-8_20; Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802; Shin Daeyun, 2018, IEEE C COMP VIS PATT; Smith Edward J., 2017, ABS170709557 CORR; Soltani A. A., SYNTHESIZING 3D SHAP; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230; Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50; Wu JJ, 2017, ADV NEUR IN, V30; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625	49	16	16	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001005
C	Woodworth, B; Wang, JL; Smith, A; McMahan, B; Srebro, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Woodworth, Blake; Wang, Jialei; Smith, Adam; McMahan, Brendan; Srebro, Nathan			Graph Oracle Models, Lower Bounds, and Gaps for Parallel Stochastic Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We suggest a general oracle-based framework that captures different parallel stochastic optimization settings described by a dependency graph, and deriv e generic lower bounds in terms of this graph. We then use the framework and derive lower bounds for several specific parallel optimization settings, including delayed updates and parallel processing with intermittent communication. We highlight gaps between lower and upper bounds on the oracle complexity, and cases where the "natural" algorithms are not known to be optimal.	[Woodworth, Blake; Srebro, Nathan] Toyota Technol Inst, Chicago, IL 60637 USA; [Wang, Jialei] Two Sigma Investments, New York, NY USA; [Smith, Adam] Boston Univ, Boston, MA 02215 USA; [McMahan, Brendan] Google, Menlo Pk, CA USA	Toyota Technological Institute - Chicago; Boston University; Google Incorporated	Woodworth, B (corresponding author), Toyota Technol Inst, Chicago, IL 60637 USA.	blake@ttic.edu; jialei.wang@twosigma.com; ads22@bu.edu; mcmahan@google.com; nati@ttic.edu	Smith, Adam/GPS-8322-2022		NSF-BSF [1718970]; Google Research Award; NSF Graduate Research Fellowship [1754881]; NSF [IIS-1447700, AF-1763786]; Sloan Foundation research award	NSF-BSF; Google Research Award(Google Incorporated); NSF Graduate Research Fellowship(National Science Foundation (NSF)); NSF(National Science Foundation (NSF)); Sloan Foundation research award	We would like to thank Ohad Shamir for helpful discussions. This work was partially funded by NSF-BSF award 1718970 ("Convex and Non-Convex Distributed Learning") and a Google Research Award. BW is supported by the NSF Graduate Research Fellowship under award 1754881. AS was supported by NSF awards IIS-1447700 and AF-1763786, as well as a Sloan Foundation research award.	Agarwal A, 2011, ADV NEURAL INFORM PR, P873; Agarwal N., 2017, ARXIV171010329; [Anonymous], 2001, STUDIES COMPUTATIONA; Arjevani Y., 2015, ADV NEURAL INFORM PR, V28, P1756; Arjevani Yossi, 2020, ALGORITHMIC LEARNING; Bauschke H. H., 2017, CONVEX ANAL MONOTONE, V2011; Bertsekas D.P., 1989, PARALLEL DISTRIBUTED, V23; Braverman M, 2016, ACM S THEORY COMPUT, P1011, DOI 10.1145/2897518.2897582; Cotter A., 2011, P NEURIPS GRAN SPAIN; Duchi J. C., 2018, P 31 C LEARNING THEO, P3065; Feyzmahdavian HR, 2016, IEEE T AUTOMAT CONTR, V61, P3740, DOI 10.1109/TAC.2016.2525015; Garg A., 2014, ADV NEURAL INFORM PR, V27, P2726, DOI [https://doi.org/10.1109/hipc.1997.634533, DOI 10.1109/HIPC.1997.634533]; Guzman C, 2015, J COMPLEXITY, V31, P1, DOI 10.1016/j.jco.2014.08.003; Hinder O., 2017, ARXIV171011606; Lee J. D., 2017, J MACH LEARN RES, V18, P1; McMahan B., 2017, P 20 INT C ART INT S, P1273; Shalev-Shwartz S., 2008, P 25 INT C MACH LEAR, V307, P928, DOI DOI 10.1145/1390156.1390273; SLUD EV, 1977, ANN PROBAB, V5, P404, DOI 10.1214/aop/1176995801; Sra S, 2016, JMLR WORKSH CONF PRO, V51, P957; Streeter M, 2014, ADV NEURAL INFORM PR, P2915; Wang J., 2017, P MACHINE LEARNING R, P1882; Woodworth  Blake, 2017, ARXIV170903594 ARXIV; Zhang, 2013, ADV NEURAL INFORM PR, P315; Zhang Y., 2013, NEURAL INFORM PROCES, P2328	29	16	16	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003009
C	Wydmuch, M; Jasinska, K; Kuznetsov, M; Busa-Fekete, R; Dembczynski, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wydmuch, Marek; Jasinska, Kalina; Kuznetsov, Mikhail; Busa-Fekete, Robert; Dembczynski, Krzysztof			A no-regret generalization of hierarchical softmax to extreme multi-label classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Extreme multi-label classification (XMLC) is a problem of tagging an instance with a small subset of relevant labels chosen from an extremely large pool of possible labels. Large label spaces can be efficiently handled by organizing labels as a tree, like in the hierarchical softmax (HSM) approach commonly used for multi-class problems. In this paper, we investigate probabilistic label trees (PLTs) that have been recently devised for tackling XMLC problems. We show that PLTs are a no-regret multi-label generalization of HSM when precision@k is used as a model evaluation metric. Critically, we prove that pick-one-label heuristic-a reduction technique from multi-label to multi-class that is routinely used along with HSM-is not consistent in general. We also show that our implementation of PLTs, referred to as EXTREMETEXT (XT), obtains significantly better results than HSM with the pick-one-label heuristic and XML-CNN, a deep network specifically designed for XMLC problems. Moreover, XT is competitive to many state-of-the-art approaches in terms of statistical performance, model size and prediction time which makes it amenable to deploy in an online system.	[Wydmuch, Marek; Jasinska, Kalina; Dembczynski, Krzysztof] Poznan Univ Tech, Inst Comp Sci, Poznan, Poland; [Kuznetsov, Mikhail; Busa-Fekete, Robert] Yahoo Res, New York, NY USA	Poznan University of Technology	Wydmuch, M (corresponding author), Poznan Univ Tech, Inst Comp Sci, Poznan, Poland.	mwydmuch@cs.put.poznan.pl; kjasinska@cs.put.poznan.pl; kuznetsov@oath.com; busafekete@oath.com; kdembczynski@cs.put.poznan.pl	Wydmuch, Marek/X-9993-2019; Jasinska, Kalina/O-3914-2014	Wydmuch, Marek/0000-0002-6598-6304; 	Polish National Science Center [2017/25/N/ST6/00747]; Polish Ministry of Science and Higher Education [09/91/DSPB/0651]	Polish National Science Center; Polish Ministry of Science and Higher Education(Ministry of Science and Higher Education, Poland)	The work of Kalina Jasinska was supported by the Polish National Science Center under grant no. 2017/25/N/ST6/00747. The work of Krzysztof Dembczynski was supported by the Polish Ministry of Science and Higher Education under grant no. 09/91/DSPB/0651. Computational experiments have been performed in Poznan Supercomputing and Networking Center.	Agarwal S, 2014, J MACH LEARN RES, V15, P1653; Agrawal R., 2013, WWW, P13, DOI DOI 10.1145/2488388.2488391; Babbar R, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P721, DOI 10.1145/3018661.3018741; Bengio Samy, 2010, ADV NEURAL INFORM PR, V1, P163, DOI [10.5555/2997189.2997208, DOI 10.5555/2997189.2997208]; Beygelzimer A., 2009, P 25 C UNC ART INT, P51; Beygelzimer A, 2009, LECT NOTES ARTIF INT, V5809, P247, DOI 10.1007/978-3-642-04414-4_22; Bhatia Kush, 2015, ADV NEURAL INFORM PR, P730, DOI DOI 10.5555/2969239.2969321; Choromanska A. E., 2015, ADV NEURAL INFORM PR, P55; Dekel O., 2010, P 13 INT C ART INT S, V9, P137; Dembczynski K., 2010, P 27 INT C MACH LEAR, P279; Dembczynski K, 2017, PR MACH LEARN RES, V70; Dembczynski K, 2012, FRONT ARTIF INTEL AP, V242, P294, DOI 10.3233/978-1-61499-098-7-294; Dembczyski K, 2016, P 2016 EUR C MACH LE, P511, DOI DOI 10.1007/978-3-319-46227-132; Deng Jia, 2011, ADV NEURAL INFORM PR, V1, P567, DOI [10.5555/2986459.2986523, DOI 10.5555/2986459.2986523]; Fox J., 2015, APPL REGRESSION ANAL; Jasinska K, 2016, PR MACH LEARN RES, V48; Jernite Y, 2017, PR MACH LEARN RES, V70; Joulin A., 2017, P INT C MACH LEARN I, P1302; Joulin Armand, 2016, ARXIV160701759; Kumar A, 2013, MACH LEARN, V92, P65, DOI 10.1007/s10994-013-5371-6; KURZYNSKI MW, 1988, PATTERN RECOGN, V21, P355, DOI 10.1016/0031-3203(88)90049-0; Langford J., 2007, VOWPAL WABBIT; Li C.-L., 2014, P 31TH INT C MACHINE, V32, P423; Liu JZ, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P115, DOI 10.1145/3077136.3080834; Mikolov T, 2013, P 26 INT C NEURAL IN, P3111; Mineiro P, 2015, LECT NOTES ARTIF INT, V9284, P37, DOI 10.1007/978-3-319-23528-8_3; Morin F., 2005, PROC INT WORKSHOP AR, P246; Narasimhan H., 2014, ADV NEURAL INF PROC, V27, P1493; Niculescu-Mizil A, 2017, PR MACH LEARN RES, V54, P1448; Prabhu Y, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P993, DOI 10.1145/3178876.3185998; Prabhu Y, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P263, DOI 10.1145/2623330.2623651; Shrivastava A, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P812; Tagami Yukihiro, 2017, P 23 ACM SIGKDD INT, P455; Vijayanarasimhan S., 2014, ABS14127479 CORR; Ye N., 2012, P 29 INT C MACH LEAR; Yen IEH, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P545, DOI 10.1145/3097983.3098083; Yu HF, 2014, PR MACH LEARN RES, V32	38	16	16	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000082
C	Bhattacharjee, P; Das, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Bhattacharjee, Prateep; Das, Sukhendu			Temporal Coherency based Criteria for Predicting Video Frames using Deep Multi-stage Generative Adversarial Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NORMALIZED CROSS-CORRELATION	Predicting the future from a sequence of video frames has been recently a sought after yet challenging task in the field of computer vision and machine learning. Although there have been efforts for tracking using motion trajectories and flow features, the complex problem of generating unseen frames has not been studied extensively. In this paper, we deal with this problem using convolutional models within a multi-stage Generative Adversarial Networks (GAN) framework. The proposed method uses two stages of GANs to generate crisp and clear set of future frames. Although GANs have been used in the past for predicting the future, none of the works consider the relation between subsequent frames in the temporal dimension. Our main contribution lies in formulating two objective functions based on the Normalized Cross Correlation (NCC) and the Pairwise Contrastive Divergence (PCD) for solving this problem. This method, coupled with the traditional L1 loss, has been experimented with three real-world video datasets viz. Sports-1M, UCF-101 and the KITTI. Performance analysis reveals superior results over the recent state-of-the-art methods.	[Bhattacharjee, Prateep; Das, Sukhendu] Indian Inst Technol Madras, Dept Comp Sci & Engn, Visualizat & Percept Lab, Madras, Tamil Nadu, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Madras	Bhattacharjee, P (corresponding author), Indian Inst Technol Madras, Dept Comp Sci & Engn, Visualizat & Percept Lab, Madras, Tamil Nadu, India.	prateepb@cse.iitm.ac.in; sdas@iitm.ac.in	Das, Sukhendu/AAP-8630-2020; Jeong, Yongwook/N-7413-2016	Das, Sukhendu/0000-0002-2823-9211; 				Bovik A, 2009, ESSENTIAL GUIDE TO VIDEO PROCESSING, 2ND EDITION, P1; BRIECHLE K, 2001, AEROSPACE DEFENSE SE, V4387, P95; Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goroshin R, 2015, IEEE I CONF COMP VIS, P4086, DOI 10.1109/ICCV.2015.465; Hadsell R, 2006, IEEE C COMP VIS PATT, V2, P1735; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kingma D.P, P 3 INT C LEARNING R; Liu Ziwei, 2017, ARXIV170202463; Luo JW, 2010, IEEE T ULTRASON FERR, V57, P1347, DOI 10.1109/TUFFC.2010.1554; Mathieu Michael, 2016, ICLR; Mittal A., 2016, ADV NEURAL INFORM PR, P2675; Mobahi H., 2009, P 26 ANN INT C MACHI, P737, DOI DOI 10.1145/1553374.1553469; Nakhmani A, 2013, PATTERN RECOGN LETT, V34, P315, DOI 10.1016/j.patrec.2012.10.025; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ranzato MarcAurelio, 2014, ARXIV14126604; Sedaghat N., 2016, NEXT FLOW HYBRID MUL; Soomro K., 2012, ARXIV; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53	25	16	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404033
C	Hsu, D; Shi, K; Sun, XR		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hsu, Daniel; Shi, Kevin; Sun, Xiaorui			Linear regression without correspondence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This article considers algorithmic and statistical aspects of linear regression when the correspondence between the covariates and the responses is unknown. First, a fully polynomial-time approximation scheme is given for the natural least squares optimization problem in any constant dimension. Next, in an average-case and noise-free setting where the responses exactly correspond to a linear function of i.i.d. draws from a standard multivariate normal distribution, an efficient algorithm based on lattice basis reduction is shown to exactly recover the unknown linear function in arbitrary dimension. Finally, lower bounds on the signal-to-noise ratio are established for approximate recovery of the unknown linear function by any estimator.	[Hsu, Daniel; Shi, Kevin] Columbia Univ, New York, NY 10027 USA; [Sun, Xiaorui] Microsoft Res, Redmond, WA USA	Columbia University; Microsoft	Hsu, D (corresponding author), Columbia Univ, New York, NY 10027 USA.	djhsu@cs.columbia.edu; kshi@cs.columbia.edu; xiaoruisun@cs.columbia.edu	Jeong, Yongwook/N-7413-2016	Hsu, Daniel/0000-0002-3495-7113	NSF [DMR-1534910, IIS-1563785]; Sloan Research Fellowship; Simons Foundation [320173]; Bloomberg Data Science Research Grant	NSF(National Science Foundation (NSF)); Sloan Research Fellowship(Alfred P. Sloan Foundation); Simons Foundation; Bloomberg Data Science Research Grant	We are grateful to Ashwin Pananjady, Michal Derezinski, and Manfred Warmuth for helpful discussions. DH was supported in part by NSF awards DMR-1534910 and IIS-1563785, a Bloomberg Data Science Research Grant, and a Sloan Research Fellowship. XS was supported in part by a grant from the Simons Foundation (#320173 to Xiaorui Sun). This work was done in part while DH and KS were research visitors and XS was a research fellow at the Simons Institute for the Theory of Computing.	Andoni Alexandr, 2017, C LEARN THEOR; [Anonymous], 2017, ARXIV170501342; Avron H, 2013, SIAM J MATRIX ANAL A, V34, P1464, DOI 10.1137/120867287; Bobkov S., 2014, PREPRINT; Boutsidis C, 2013, IEEE T INFORM THEORY, V59, P6880, DOI 10.1109/TIT.2013.2272457; DAVIDSON K. R, 2001, HDB GEOMETRY BANACH, V1, P131; Derezinski Michal, 2017, ARXIV170506908; Elhami Golnooshsadat, 2017, P 42 IEEE INT C AC S; FRIEZE AM, 1986, SIAM J COMPUT, V15, P536, DOI 10.1137/0215038; Garey M.R., 1979, COMPUTERS INTRACTABI; HAN TS, 1994, IEEE T INFORM THEORY, V40, P1247, DOI 10.1109/18.335943; LAGARIAS JC, 1985, J ACM, V32, P229, DOI 10.1145/2455.2461; Laurent B, 2000, ANN STAT, V28, P1302; LECAM L, 1973, ANN STAT, V1, P38, DOI 10.1214/aos/1193342380; Ledoux, 2000, CONCENTRATION MEASUR; LENSTRA AK, 1982, MATH ANN, V261, P515, DOI 10.1007/BF01457454; Pananjady A, 2016, ANN ALLERTON CONF, P417, DOI 10.1109/ALLERTON.2016.7852261; Pananjady Ashwin, 2017, ARXIV170407461; Reiss R.-D., 2012, APPROXIMATE DISTRIBU; Rudelson M., 2010, ARXIV10032990; Unnikrishnan J., 2015, ARXIV151200115; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060; Yu B., 1997, FESTSCHRIFT L LECAM, P423	24	16	16	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401055
C	Rothe, A; Lake, BM; Gureckis, TM		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Rothe, Anselm; Lake, Brenden M.; Gureckis, Todd M.			Question Asking as Program Generation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					A hallmark of human intelligence is the ability to ask rich, creative, and revealing questions. Here we introduce a cognitive model capable of constructing human-like questions. Our approach treats questions as formal programs that, when executed on the state of the world, output an answer. The model specifies a probability distribution over a complex, compositional space of programs, favoring concise programs that help the agent learn in the current context. We evaluate our approach by modeling the types of open-ended questions generated by humans who were attempting to learn about an ambiguous situation in a game. We find that our model predicts what questions people will ask, and can creatively produce novel questions that were not present in the training set. In addition, we compare a number of model variants, finding that both question informativeness and complexity are important for producing human-like questions.	[Rothe, Anselm; Lake, Brenden M.; Gureckis, Todd M.] NYU, Dept Psychol, New York, NY 10003 USA; [Lake, Brenden M.] NYU, Ctr Data Sci, New York, NY 10003 USA	New York University; New York University	Rothe, A (corresponding author), NYU, Dept Psychol, New York, NY 10003 USA.	anselm@nyu.edu; brenden@nyu.edu; todd.gureckis@nyu.edu	Jeong, Yongwook/N-7413-2016	Gureckis, Todd/0000-0002-7139-4778	NSF [BCS-1255538]; John Templeton Foundation Varieties of Understanding project; John S. McDonnell Foundation Scholar Award; Moore-Sloan Data Science Environment at NYU	NSF(National Science Foundation (NSF)); John Templeton Foundation Varieties of Understanding project; John S. McDonnell Foundation Scholar Award; Moore-Sloan Data Science Environment at NYU	We thank Chris Barker, Sam Bowman, Noah Goodman, and Doug Markant for feedback and advice. This research was supported by NSF grant BCS-1255538, the John Templeton Foundation Varieties of Understanding project, a John S. McDonnell Foundation Scholar Award to TMG, and the Moore-Sloan Data Science Environment at NYU.	Bordes A., 2016, ABS160507683 CORR; Chouinard MM, 2007, MONOGR SOC RES CHILD, V72, P1; Fodor J. A, 1975, LANGUAGE THOUGHT; FODOR JA, 1988, COGNITION, V28, P3, DOI 10.1016/0010-0277(88)90031-5; Goodman ND, 2015, CONCEPTUAL MIND: NEW DIRECTIONS IN THE STUDY OF CONCEPTS, P623; Groenendijk Jeroen Antonius Gerardus, 1984, THESIS; Gureckis T. M., 2009, P 31 ANN C COGN SCI; Gureckis TM, 2012, PERSPECT PSYCHOL SCI, V7, P464, DOI 10.1177/1745691612454304; Hawkins Robert D., 2015, COGNITIVE SCI; Jacobson P., 2014, COMPOSITIONAL SEMANT; Jain U., 2017, CREATIVITY GENERATIN; Johnson R, 2017, INT CONF PERVAS COMP; Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Marcus Gary, 2003, ALGEBRAIC MIND INTEG; Mostafazadeh N, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1802; Piantadosi ST, 2012, COGNITION, V123, P199, DOI 10.1016/j.cognition.2011.11.005; Roberts C., 1996, OSU WORKING PAPERS L, V49, P91, DOI DOI 10.3765/SP.5.6; Rothe A, 2016, P 38 ANN C COGN SCI; Settles Burr, 2012, ACTIVE LEARNING, DOI 10.2200/s00429ed1v01y201207aim018; Strub F, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2765; Vijayakumar Ashwin K, 2016, ARXIV161002424; Young S, 2013, P IEEE, V101, P1160, DOI 10.1109/JPROC.2012.2225812	25	16	16	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401009
C	Schwartz, I; Schwing, AG; Hazan, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Schwartz, Idan; Schwing, Alexander G.; Hazan, Tamir			High-Order Attention Models for Visual Question Answering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The quest for algorithms that enable cognitive abilities is an important part of machine learning. A common trait in many recently investigated cognitive-like tasks is that they take into account different data modalities, such as visual and textual input. In this paper we propose a novel and generally applicable form of attention mechanism that learns high-order correlations between various data modalities. We show that high-order correlations effectively direct the appropriate attention to the relevant elements in the different data modalities that are required to solve the joint task. We demonstrate the effectiveness of our high-order attention mechanism on the task of visual question answering (VQA), where we achieve state-of-the-art performance on the standard VQA dataset.	[Schwartz, Idan] Technion, Dept Comp Sci, Haifa, Israel; [Schwing, Alexander G.] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL USA; [Hazan, Tamir] Technion, Dept Ind Engn & Management, Haifa, Israel	Technion Israel Institute of Technology; University of Illinois System; University of Illinois Urbana-Champaign; Technion Israel Institute of Technology	Schwartz, I (corresponding author), Technion, Dept Comp Sci, Haifa, Israel.	idansc@cs.technion.ac.il; aschwing@illinois.edu; tamir.hazan@gmail.com	Jeong, Yongwook/N-7413-2016		Israel Science Foundation [948/15]; National Science Foundation [1718221]	Israel Science Foundation(Israel Science Foundation); National Science Foundation(National Science Foundation (NSF))	This research was supported in part by The Israel Science Foundation (grant No. 948/15). This material is based upon work supported in part by the National Science Foundation under Grant No. 1718221. We thank Nvidia for providing GPUs used in this research.	Andreas J, 2016, ARXIV160101705; [Anonymous], 2016, ARXIV160603556; Antol S., 2015, ICCV; Bahdanau Dzmitry, 2015, NEURAL MACHINE TRANS; Charikar Moses, 2002, ICALP; Collobert R., 2011, BIGLEARN NIPS WORKSH; Fukui A., 2016, EMNLP; Hermann KM, 2015, ADV NEUR IN, V28; Jabri A., 2016, ECCV; Jain U., 2017, CVPR; Kazemi Vahid, 2017, ARXIV170403162; Kim J, 2017, ICLR; Kim J.-H., 2016, NIPS; Kim Y., 2017, 5 INT C LEARN REPR I; Lu J., 2016, NIPS; Ma L., 2015, ARXIV150600333; Malinowski M., 2015, P INT C COMP VIS; Mostafazadeh N., 2016, ARXIV160306059; Nam H., 2016, DUAL ATTENTION NETWO; Noh Hyeonwoo, 2016, ARXIV160603647; Pham Ninh, 2013, SIGKDD; Rocktaschel Tim, 2016, P ICLR; Shih K. J., 2016, CVPR; Xiong C., 2016, ARXIV160301417; Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28; Xu K., 2015, ICML; Yang Z., 2016, CVPR; Yin W., 2015, ARXIV151205193; Zhu Y., 2016, CVPR	29	16	17	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403071
C	Patel, AB; Nguyen, T; Baraniuk, RG		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Patel, Ankit B.; Tan Nguyen; Baraniuk, Richard G.			A Probabilistic Framework for Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We develop a probabilistic framework for deep learning based on the Deep Rendering Mixture Model (DRMM), a new generative probabilistic model that explicitly capture variations in data due to latent task nuisance variables. We demonstrate that max-sum inference in the DRMM yields an algorithm that exactly reproduces the operations in deep convolutional neural networks (DCNs), providing a first principles derivation. Our framework provides new insights into the successes and shortcomings of DCNs as well as a principled route to their improvement. DRMM training via the Expectation-Maximization (EM) algorithm is a powerful alternative to DCN back-propagation, and initial training results are promising. Classification based on the DRMM and other variants outperforms DCNs in supervised digit classification, training 2-3. faster while achieving similar accuracy. Moreover, the DRMM is applicable to semi-supervised and unsupervised learning tasks, achieving results that are state-of-the-art in several categories on the MNIST benchmark and comparable to state of the art on the CIFAR10 benchmark.	[Patel, Ankit B.] Rice Univ, Baylor Coll Med, Houston, TX 77251 USA; [Tan Nguyen; Baraniuk, Richard G.] Rice Univ, Houston, TX 77251 USA	Baylor College of Medicine; Rice University; Rice University	Patel, AB (corresponding author), Rice Univ, Baylor Coll Med, Houston, TX 77251 USA.	ankitp@bcm.edu; mn15@rice.edu; richb@rice.edu	Baraniuk, Richard/ABA-1743-2020	Nguyen, Tan/0000-0002-6408-5416	IARPA via DoI/IBC [D16PC00003]; NSF [CCF-1527501]; AFOSR [FA9550-14-1-0088]; ARO [W911NF-15-1-0316]; ONR [N00014-12-1-0579]; NSF; NSF IGERT Training Grant [DGE-1250104]	IARPA via DoI/IBC; NSF(National Science Foundation (NSF)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); ARO; ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); NSF IGERT Training Grant	Thanks to Xaq Pitkow and Ben Poole for helpful feedback. ABP and RGB were supported by IARPA via DoI/IBC contract D16PC00003. <SUP>1</SUP> RGB was supported by NSF CCF-1527501, AFOSR FA9550-14-1-0088, ARO W911NF-15-1-0316, and ONR N00014-12-1-0579. TN was supported by an NSF Graduate Reseach Fellowship and NSF IGERT Training Grant (DGE-1250104).	Anselmi F., 2013, MAGIC MAT THEORY DEE; Arora S, 2013, ARXIV13106343; Bernardo J. M., 2007, BAYESIAN STAT, V8, P3, DOI DOI 10.1007/978-3-642-93220-5_6; Bishop C.M, 2006, PATTERN RECOGN; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; Goodfellow I. J., 2013, ARXIV13024389; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee D., 2013, INT C MACH LEARN ICM; Maaloe L, 2016, PR MACH LEARN RES, V48; Makhzani Alireza, 2015, ADV NEURAL INFORM PR, P2773; Montufar G.F., 2014, ADV NEURAL INF PROCE, V27, P2924, DOI DOI 10.5555/2969033.2969153; Patel A. B., 2015, ARXIV150400641; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Rifai S., 2011, ADV NEURAL INFORM PR, V24, P2294; Rifai S., 2011, P 28 INT C MACH LEAR, P833; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T, 2016, ADV NEUR IN, V29; Sheikh AS, 2014, J MACH LEARN RES, V15, P2653; Soatto S., 2016, INT C LEARN REPR; Springenberg J.T., 2014, ARXIV14126806; Springenberg Jost Tobias, 2015, ARXIV151106390; Tang Y., 2012, ARXIV12064635, P1419, DOI DOI 10.48550/ARXIV.1206.6445; Taylor G., 2016, ARXIV160502026; van den Oord A., 2014, NIPS, P3518; Vapnik VN, 1998, STAT LEARNING THEORY, DOI DOI 10.1007/978-1-4419-1428-6_5864; Zhao J, 2016, 2016 IEEE MTT-S INTERNATIONAL WIRELESS SYMPOSIUM (IWS), DOI 10.1109/ICSSSM.2016.7538614	31	16	16	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701088
C	Wang, H; Shi, XJ; Yeung, DY		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Hao; Shi, Xingjian; Yeung, Dit-Yan			Natural-Parameter Networks: A Class of Probabilistic Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are often prone to overfitting. One effective way to alleviate this problem is to exploit the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the weights and neurons according to the data, as is often done in probabilistic graphical models. To address these problems, we propose a class of probabilistic neural networks, dubbed natural-parameter networks (NPN), as a novel and lightweight Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family distributions to model the weights and neurons. Different from traditional NN and BNN, NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As a Bayesian treatment, efficient backpropagation (BP) is performed to learn the natural parameters for the distributions over both the weights and neurons. The output distributions of each layer, as byproducts, may be used as second-order representations for the associated tasks such as link prediction. Experiments on real-world datasets show that NPN can achieve state-of-the-art performance.	[Wang, Hao; Shi, Xingjian; Yeung, Dit-Yan] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China	Hong Kong University of Science & Technology	Wang, H (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.	hwangaz@cse.ust.hk; xshiab@cse.ust.hk; dyyeung@cse.ust.hk						Balan Anoop Korattikara, 2015, ADV NEURAL INFORM PR, P3; Bastien F., 2012, DEEP LEARN UNS FEAT; Bishop CM, 2006, PATTERN RECOGNITION; Blei, 2011, P 17 ACM SIGKDD INT, P448, DOI DOI 10.1145/2020408.2020480; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3; CLARK CE, 1961, OPER RES, V9, P145, DOI 10.1287/opre.9.2.145; David J. MacKay, 1992, NEURAL COMPUTATION; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Henao R., 2015, NIPS; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1699; Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lee DD, 2001, ADV NEUR IN, V13, P556; Leskovec J, 2014, SNAP DATASETS STANFO; Neal R. M., 2012, BAYESIAN LEARNING NE; Neal R. M., 1990, LEARNING STOCHASTIC, V64, P1577; Ranganath R, 2015, JMLR WORKSH CONF PRO, V38, P762; Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Wang H., 2016, TKDE; Wang H, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1235, DOI 10.1145/2783258.2783273; Wang Hao, 2013, IJCAI; Zhou Mingyuan, 2012, AISTATS, P1462	26	16	16	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702026
C	Maddison, CJ; Tarlow, D; Minka, T		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Maddison, Chris J.; Tarlow, Daniel; Minka, Tom			A* Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The problem of drawing samples from a discrete distribution can be converted into a discrete optimization problem [1, 2, 3, 4]. In this work, we show how sampling from a continuous distribution can be converted into an optimization problem over continuous space. Central to the method is a stochastic process recently described in mathematical statistics that we call the Gumbel process. We present a new construction of the Gumbel process and A. Sampling, a practical generic sampling algorithm that searches for the maximum of a Gumbel process using A. search. We analyze the correctness and convergence time of A* Sampling and demonstrate empirically that it makes more efficient use of bound and likelihood evaluations than the most closely related adaptive rejection sampling-based algorithms.	[Maddison, Chris J.] Univ Toronto, Dept Comp Sci, Toronto, ON, Canada; [Tarlow, Daniel; Minka, Tom] Microsoft Res, Redmond, WA USA	University of Toronto; Microsoft	Maddison, CJ (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.	cmaddis@cs.toronto.edu; dtarlow@microsoft.com; minkal@microsoft.com			NSERC	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC))	This research was supported by NSERC. We thank James Martens and Radford Neal for helpful discussions, Elad Mezuman for help developing early ideas related to this work, and Roger Grosse for suggestions that greatly improved this work.	Chang MMJ, 2008, CVPR, P1, DOI [10.1002/9780470727010.CH1, DOI 10.1109/ARSO.2008.4653593.]; DYMETMAN M, 2012, ARXIV12070742; Ermon Stefano, 2013, ADV NEURAL INFORM PR, P2085; GILKS WR, 1992, J R STAT SOC C-APPL, V41, P337; Gumbel E. J., 1954, STAT THEORY EXTREME; Hansen Eldon, 2003, GLOBAL OPTIMIZATION, V264; Hazan T., 2012, ICML; Hazan T, 2013, ADV NEURAL INFORM PR, P1268; Kroshko Dmitrey, 2014, FUNCDESIGNER; Malmberg Hannes, 2013, THESIS; Mansinghka Vikash, 2009, 12 INT C ART INT STA, P400; Mateescu Robert Eugeniu, 2007, THESIS; Minka T.P., 2001, P 17 C UNC ART INT, P362; Mira A, 2001, J R STAT SOC B, V63, P593, DOI 10.1111/1467-9868.00301; Mitha Faheem, 2003, THESIS; Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461; Papandreou G., 2010, ADV NEURAL INF PROCE, V23, P1858; Papandreou G, 2011, IEEE I CONF COMP VIS, P193, DOI 10.1109/ICCV.2011.6126242; Propp JG, 1996, RANDOM STRUCT ALGOR, V9, P223, DOI 10.1002/(SICI)1098-2418(199608/09)9:1/2<223::AID-RSA14>3.0.CO;2-O; TARLOW D, 2012, AISTATS, P21; YELLOTT JI, 1977, J MATH PSYCHOL, V15, P109, DOI 10.1016/0022-2496(77)90026-8	21	16	16	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102010
C	Sun, Y; Chen, YH; Wang, XG; Tang, XO		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Sun, Yi; Chen, Yuheng; Wang, Xiaogang; Tang, Xiaoou			Deep Learning Face Representation by Joint Identification-Verification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 features extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 features extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset [11], 99.15% face verification accuracy is achieved. Compared with the best previous deep learning result [20] on LFW, the error rate has been significantly reduced by 67%.	[Sun, Yi; Tang, Xiaoou] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China; [Chen, Yuheng] SenseTime Grp, Hong Kong, Peoples R China; [Wang, Xiaogang] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China; [Wang, Xiaogang; Tang, Xiaoou] Chinese Acad Sci, Shenzhen Inst Adv Technol, Beijing, Peoples R China	Chinese University of Hong Kong; Chinese University of Hong Kong; Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS	Sun, Y (corresponding author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China.	sy011@ie.cuhk.edu.hk; chyh1990@gmail.com; xgwang@ee.cuhk.edu.hk; xtang@ie.cuhk.edu.hk						[Anonymous], 2014, P CVPR; [Anonymous], 2014, P CVPR; [Anonymous], P ICCV; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; Cao X., 2013, P ICCV; Chen D., 2012, P ECCV; Chen D., 2013, P CVPR; Chopra S., 2005, P CVPR; Guillaumin M., 2009, P ICCV; Hadsell R., 2006, P CVPR; Hu J., 2014, P CVPR; Huang C, 2011, TR115 NEC; Huang G. B., 2012, P CVPR; Huang G.B., 2008, WORKSHOP FACESREAL L; LeCun Y., 1998, P IEEE; Lu C., 2014, ARXIV14043840; Mignon Alexis, 2012, P CVPR; Mobahi H., 2009, P ICML; Moghaddam B, 2000, PATTERN RECOGN, V33, P1771, DOI 10.1016/S0031-3203(99)00179-X; Nair V., 2010, 27 INT C MACH LEARN, P807, DOI DOI 10.5555/3104322.3104425; Sun Y., 2014, ARXIV14064773; Tang X., 2014, P NIPS; Wang X., 2003, P ICCV; Wang XG, 2004, IEEE T PATTERN ANAL, V26, P1222, DOI 10.1109/TPAMI.2004.57; Xiong X., 2013, P CVPR; Zhang T, 2011, IEEE T INFORM THEORY, V57, P4689, DOI 10.1109/TIT.2011.2146690; Zhu Z, 2013, P ICCV	27	16	16	4	27	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101087
C	Bagnell, JA; Kakade, S; Ng, AY; Schneider, J		Thrun, S; Saul, K; Scholkopf, B		Bagnell, JA; Kakade, S; Ng, AY; Schneider, J			Policy search by dynamic programming	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We consider the policy search approach to reinforcement learning. We show that if a "baseline distribution" is given (indicating roughly how often we expect a good policy to visit each state), then we can derive a policy search algorithm that terminates in a finite number of steps, and for which we can provide non-trivial performance guarantees. We also demonstrate this algorithm on several grid-world POMDPs, a planar biped walking robot, and a double-pole balancing problem.	Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Bagnell, JA (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.							AMALDI E, 1998, THEORETICAL COMP SCI; ATKESON C, 2003, NIPS, V15; KAKADE S, 2002, P 19 INT C MACH LEAR; Kakade S. M., 2003, THESIS U LONDON LOND; KEARNS M, 1999, NIPS, V12; LITTMAN M, 1994, P 3 C SIM AD BEH; NG AY, 2000, P 16 C UNC ART INT; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	8	16	16	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						831	838						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500104
C	Campbell, WM; Campbell, JP; Reynolds, DA; Jones, DA; Leek, TR		Thrun, S; Saul, K; Scholkopf, B		Campbell, WM; Campbell, JP; Reynolds, DA; Jones, DA; Leek, TR			Phonetic speaker recognition with support vector machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				VERIFICATION	A recent area of significant progress in speaker recognition is the use of high level features-idiolect, phonetic relations, prosody, discourse structure, etc. A speaker not only has a distinctive acoustic sound but uses language in a characteristic manner. Large corpora of speech data available in recent years allow experimentation with long term statistics of phone patterns, word patterns, etc. of an individual. We propose the use of support vector machines and term frequency analysis of phone sequences to model a given speaker. To this end, we explore techniques for text categorization applied to the problem. We derive a new kernel based upon a linearization of likelihood ratio scoring. We introduce a new phone-based SVM speaker recognition approach that halves the error rate of conventional phone-based approaches.	MIT, Lincoln Lab, Lexington, MA 02420 USA	Lincoln Laboratory; Massachusetts Institute of Technology (MIT)	Campbell, WM (corresponding author), MIT, Lincoln Lab, Lexington, MA 02420 USA.	wcampbell@ll.mit.edu; jpc@ll.mit.edu; dar@ll.mit.edu; daj@ll.mit.edu; tleek@ll.mit.edu						Adami AG, 2003, INT CONF ACOUST SPEE, P788; Andrews WD, 2002, INT CONF ACOUST SPEE, P149; Campbell WM, 2002, INT CONF ACOUST SPEE, P161; Collobert R, 2001, J MACH LEARN RES, V1, P143, DOI 10.1162/15324430152733142; Doddington G. R., 2001, P EUR, P2521; Joachims T., 2002, LEARNING CLASSIFY TE; Klusacek D, 2003, INT CONF ACOUST SPEE, P804; *LING DAT CONS, SWITCHB 2 CORP; MARTIN A, 1997, P EUROSPEECH, P1895; PRZYBOCKI M, NIST YEAR 2003 SPEAK; Quatieri TF, 2000, IEEE T SPEECH AUDI P, V8, P567, DOI 10.1109/89.861376; Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361; SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0; Schmidt-Nielsen A, 2000, DIGIT SIGNAL PROCESS, V10, P249, DOI 10.1006/dspr.1999.0356; Zissman MA, 1996, IEEE T SPEECH AUDI P, V4, P31, DOI 10.1109/TSA.1996.481450	15	16	18	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1377	1384						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500171
C	Graepel, T; Herbrich, R		Thrun, S; Saul, K; Scholkopf, B		Graepel, T; Herbrich, R			Invariant pattern recognition by semidefinite programming machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Knowledge about, local invariances with respect to given pattern transformations can greatly improve the accuracy of classification. Previous approaches are either based on regularisation or on the generation of virtual (transformed) examples. We develop a new framework for learning linear classifiers under known transformations based on semidefinite programming. We present a new learning algorithm the Semidefinite Programming Machine (SDPM)-which is able to find a maximum margin hyperplane when the training examples are polynomial trajectories instead of single points. The solution is found to be sparse in dual variables and allows to identify those points on the trajectory with minimal real-valued output as virtual support vectors. Extensions to segments of trajectories, to more than one transformation parameter, and to learning with kernels are discussed. In experiments we rise a Taylor expansion to locally approximate rotational invariance in pixel images from USPS and find improvements over known methods.	Microsoft Res Ltd, Cambridge, England	Microsoft	Graepel, T (corresponding author), Microsoft Res Ltd, Cambridge, England.							Chapelle O, 2002, ADV NEUR IN, V14, P609; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; GRAEPEL T, 2004, ADV NEURAL INFORMATI, V16; NEMIROVSKI A, 2002, LECT NOTES C O R E S; Nesterov Y., 2000, HIGH PERFORMANCE OPT, V33, P405, DOI [DOI 10.1007/978-1-4757-3216-0_17, 10.1007/978-1-4757-3216-017, DOI 10.1007/978-1-4757-3216-017]; ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519; Scholkopf Bernhard, 1997, SUPPORT VECTOR LEARN; SIMARD P, 1998, NEUROL NETWORKS TRIC; Vandenberghe L, 1996, SIAM REV, V38, P49, DOI 10.1137/1038003	9	16	16	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						33	40						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500005
C	Ricks, B; Ventura, D		Thrun, S; Saul, K; Scholkopf, B		Ricks, B; Ventura, D			Training a quantum neural network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Most proposals for quantum neural networks have skipped over the problem of how to train the networks. The mechanics of quantum computing are different enough from classical computing that the issue of training should be treated in detail. We propose a simple quantum neural network and a training method for it. It can be shown that this algorithm works in quantum systems. Results on several real-world data sets show that this algorithm can train the proposed quantum neural networks, and that it has some advantages over classical learning algorithms.	Brigham Young Univ, Dept Comp Sci, Provo, UT 84602 USA	Brigham Young University	Ricks, B (corresponding author), Brigham Young Univ, Dept Comp Sci, Provo, UT 84602 USA.	cyberbob@cs.byu.edu; ventura@cs.byu.edu						Altaisky M. V, 2001, QUANTUM NEURAL NETWO; Behrman E. C., 2002, QUANTUM NEURAL NETWO; Boyer M, 1996, P 4 WORKSH PHYS COMP, P36; EHRMAN EC, 1996, P 4 WORKSH PHYS COMP, P22; Ezhov A., 2000, FUTURE DIRECTIONS IN; Ezhov AA, 2000, INFORM SCIENCES, V128, P271, DOI 10.1016/S0020-0255(00)00057-8; FUJITA Y, 2002, QUANTUM GAUGED NEURA; Grover L. K., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, P212, DOI 10.1145/237814.237866; Grover LK, 1997, PHYS REV LETT, V79, P325, DOI 10.1103/PhysRevLett.79.325; Jozsa R, 1998, GEOMETRIC UNIVERSE, P369; Narayanan A, 2000, INFORM SCIENCES, V128, P231, DOI 10.1016/S0020-0255(00)00055-4; Newman C. B. D., 1998, UCI REPOSITORY MACHI; Nielsen M.A., 2000, QUANTUM COMPUTATION; SHAFEE F, 2002, NEURAL NETWORKS C NO; Shor PW, 1997, SIAM J COMPUT, V26, P1484, DOI [10.1137/S0097539795293172, 10.1137/S0036144598347011]; Vedral V, 1996, PHYS REV A, V54, P147, DOI 10.1103/PhysRevA.54.147; Vedral V, 1997, PHYS REV LETT, V78, P2275, DOI 10.1103/PhysRevLett.78.2275; Ventura D, 2000, INFORM SCIENCES, V124, P273, DOI 10.1016/S0020-0255(99)00101-2; ZARNDT F, 1995, THESIS BRIGHAM YOUNG	20	16	16	0	4	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1019	1026						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500127
C	Rosset, S; Zhu, J; Hastie, T		Thrun, S; Saul, K; Scholkopf, B		Rosset, S; Zhu, J; Hastie, T			Margin maximizing loss functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Margin maximizing properties play an important role in the analysis of classification models, such as boosting and support vector machines. Margin maximization is theoretically interesting because it facilitates generalization error analysis, and practically interesting because it presents a clear geometric interpretation of the models being built. We formulate and prove a sufficient condition for the solutions of regularized loss functions to converge to margin maximizing separators, as the regularization vanishes. This condition covers the hinge loss of SVM, the exponential loss of AdaBoost and logistic regression loss, We also generalize it to multi-class classification problems, and present margin maximizing multiclass versions of logistic regression and support vector machines.	IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Rosset, S (corresponding author), IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.	srosset@us.ibm.com; jizhu@umich.edu; hastie@stat.stanford.edu		Hastie, Trevor/0000-0002-0164-3142				Bartlett P. L., 2003, CONVEXITY CLASSIFICA; Breiman L, 1999, NEURAL COMPUT, V11, P1493, DOI 10.1162/089976699300016106; Freund Y., 1995, P 2 EUR C COMP LEARN; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; GROVE AJ, 1998, P 15 NAT C AI; Hastie T., 2009, ELEMENTS STAT LEARNI, V2nd, DOI DOI 10.1007/978-0-387-21606-5; Mangasarian OL, 1999, OPER RES LETT, V24, P15, DOI 10.1016/S0167-6377(98)00049-2; ROSSET R, 2003, BOOSTING REGULARIZED; Schapire RE, 1998, ANN STAT, V26, P1651; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Weston J., 1998, CSDTR9804 U LOND DEP	11	16	16	0	3	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1237	1244						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500154
C	Smola, AJ; Vishwanathan, SVN; Eskin, E		Thrun, S; Saul, K; Scholkopf, B		Smola, AJ; Vishwanathan, SVN; Eskin, E			Laplace propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We present a novel method for approximate inference in Bayesian models and regularized risk functionals. It is based on the propagation of mean and variance derived from the Laplace approximation of conditional probabilities in factorizing distributions, much akin to Minka's Expectation Propagation. In the jointly normal case, it coincides with the latter and belief propagation, whereas in the general case, it provides an optimization strategy containing Support Vector chunking, the Bayes Committee Machine, and Gaussian Process chunking as special cases.	Australian Natl Univ, Machine Learning Grp, Canberra, ACT 0200, Australia	Australian National University	Smola, AJ (corresponding author), Australian Natl Univ, Machine Learning Grp, Canberra, ACT 0200, Australia.	smola@axiom.anu.edu.au; vishy@axiom.anu.edu.au; eeskin@cs.huji.ac.il						COLLOBERT R, 2002, ADV NEURAL INFORMATI; Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619; Joachims T, 1999, ADVANCES IN KERNEL METHODS, P169; Jordan MI, 1998, NATO ADV SCI I D-BEH, V89, P105; Minka T. P., 2001, THESIS MIT MEDIA LAB; Newman C. B. D., 1998, UCI REPOSITORY MACHI; Pearl J., 1984, INTELLIGENT SEARCH S; Platt J, 1998, MSRTR9814; Tresp V, 2000, NEURAL COMPUT, V12, P2719, DOI 10.1162/089976600300014908	9	16	16	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						441	448						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500056
C	Dietterich, TG; Wang, X		Dietterich, TG; Becker, S; Ghahramani, Z		Dietterich, TG; Wang, X			Batch value function approximation via support vectors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We present three ways of combining linear programming with the kernel trick to find value function approximations for reinforcement learning. One formulation is based on SVM regression; the second is based on the Bellman equation; and the third seeks only to ensure that good moves have an advantage over bad moves. All formulations attempt to minimize the number of support vectors while fitting the data. Experiments in a difficult, synthetic maze problem show that all three formulations give excellent performance, but the advantage formulation is much easier to train. Unlike policy gradient methods, the kernel methods described here can easily adjust the complexity of the function approximator to fit the complexity of the value function.	Oregon State Univ, Dept Comp Sci, Corvallis, OR 97331 USA	Oregon State University	Dietterich, TG (corresponding author), Oregon State Univ, Dept Comp Sci, Corvallis, OR 97331 USA.							BAIRD LC, 1993, 931146 WRIGHT PATT A; Moll R, 1999, ADV NEUR IN, V11, P1017; TESAURO G, 1995, COMMUN ACM, V38, P58, DOI 10.1145/203330.203343; UTGOFF PE, 1987, ICML 87, P115; Vapnik V., 2000, NATURE STAT LEARNING; Zhang W, 1995, INT JOINT C ART INT, P1114	6	16	16	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1491	1498						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100185
C	Mozer, MC; Colagrosso, MD; Huber, DE		Dietterich, TG; Becker, S; Ghahramani, Z		Mozer, MC; Colagrosso, MD; Huber, DE			A rational analysis of cognitive control in a speeded discrimination task	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We are interested in the mechanisms by which individuals monitor and adjust their performance of simple cognitive tasks. We model a speeded discrimination task in which individuals are asked to classify a sequence of stimuli (Jones & Braver, 2001). Response conflict arises when one stimulus class is infrequent relative to another, resulting in more errors and slower reaction times for the infrequent class. How do control processes modulate behavior based on the relative class frequencies? We explain performance from a rational perspective that casts the goal of individuals as minimizing a cost that depends both on error rate and reaction time. With two additional assumptions of rationality-that class prior probabilities are accurately estimated and that inference is optimal subject to limitations on rate of information transmission-we obtain a good fit to overall RT and error data, as well as trial-by-trial variations in performance.	Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Mozer, MC (corresponding author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.							BOTVINICK MM, 2001, UNPUB EVALUATING DEM; COLAGROSSO M, THESIS; JONES AD, 2001, UNPUB SEQUENTIAL MOD; MOZER MC, 2000, 41 ANN M PSYCH SOC N; YEUNG N, 2000, UNPUB NEURAL BASIS E	5	16	16	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						51	57						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100007
C	Murphy, KP; Paskin, MA		Dietterich, TG; Becker, S; Ghahramani, Z		Murphy, KP; Paskin, MA			Linear time inference in hierarchical HMMs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				HIDDEN MARKOV-MODELS; RECOGNITION	The hierarchical hidden Markov model (HHMM) is a generalization of the hidden Markov model (HMM) that models sequences with structure at many length/time scales [FST98]. Unfortunately, the original inference algorithm is rather complicated, and takes O(T-3) time, where T is the length of the sequence, making it impractical for many domains. In this paper, we show how HHMMs are a special kind of dynamic Bayesian network (DBN), and thereby derive a much simpler inference algorithm, which only takes O(T) time. Furthermore, by drawing the connection between HHMMs and DBNs, we enable the application of many standard approximation techniques to further speed up inference.	Univ Calif Berkeley, Dept Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Murphy, KP (corresponding author), Univ Calif Berkeley, Dept Comp Sci, Berkeley, CA 94720 USA.							Boutilier Craig, 1996, UAI; BUI H, 2000, AAAI; BUI H, 2001, INT J PATTERN REC AI; Fine S, 1998, MACH LEARN, V32, P41, DOI 10.1023/A:1007469218079; HOEY J, 2001, ICCV WORKSH DET REC; Hu M., 2000, HIERARCHICAL HMM IMP; Ivanov YA, 2000, IEEE T PATTERN ANAL, V22, P852, DOI 10.1109/34.868686; JORDAN MI, 1996, NIPS; Kjaerulff U. B., 1990, R9009 AALB U DEP MAT; Lari K., 1990, Computer Speech and Language, V4, P35, DOI 10.1016/0885-2308(90)90022-X; MOORE D, 2001, CVPR WORKSH MOD VS E; MURPHY K, 1999, PEARLS ALGORITHM MUL; MURPHY K, 2001, APPLYING JUNCTION TR; Murphy K., 2001, UAI; NEFIAN A, 2000, IEEE INT C IM PROC; Parr Ronald, 1997, NIPS; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Saul LK, 1999, MACH LEARN, V37, P75, DOI 10.1023/A:1007649326333; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; THEOCHAROUS G, 2001, ICRA; ZWEIG G, 1997, THESIS UC BERKELEY	22	16	16	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						833	840						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100104
C	Arcas, BAY; Fairhall, AL; Bialek, W		Leen, TK; Dietterich, TG; Tresp, V		Arcas, BAY; Fairhall, AL; Bialek, W			What can a single neuron compute?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					In this paper we formulate a description of the computation performed by a neuron as a combination of dimensional reduction and nonlinearity. We implement this description for the Hodgkin-Huxley model, identify the most relevant dimensions and find the nonlinearity. A two dimensional description already captures a significant fraction of the information that spikes carry about dynamic inputs. This description also shows that computation in the Hodgkin-Huxley model is more complex than a simple integrate-and-fire or perceptron model.	Princeton Univ, Rare Books Lib, Princeton, NJ 08544 USA	Princeton University	Arcas, BAY (corresponding author), Princeton Univ, Rare Books Lib, Princeton, NJ 08544 USA.							[Anonymous], [No title captured]; BIALEK W, 1988, P R SOC LOND B, V234; BRENNER N, 2000, NEURAL COMP, V12; Hodgkin A. L., 1952, J PHYSL, V117; Rieke F., 1997, SPIKES EXPLORING NEU; SCHNEIDMAN E, 1998, NEURAL COMP, V10	6	16	16	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						75	81						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800011
C	Domeniconi, C; Peng, J; Gunopulos, D		Leen, TK; Dietterich, TG; Tresp, V		Domeniconi, C; Peng, J; Gunopulos, D			An adaptive metric machine for pattern classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				NEAREST-NEIGHBOR CLASSIFICATION	Nearest neighbor classification assumes locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with finite samples due to the curse of dimensionality. Severe bias can be introduced under these conditions when using the nearest neighbor rule. We propose a locally adaptive nearest neighbor classification method to try to minimize bias. We use a Chi-squared distance analysis to compute a flexible metric for producing neighborhoods that axe elongated along less relevant feature dimensions and constricted along most influential ones. As a result, the class conditional probabilities tend to be smoother in the modified neighborhoods, whereby better classification performance can be achieved. The efficacy of our method is validated and compared against other techniques using a variety of real world data.	Univ Calif Riverside, Dept Comp Sci, Riverside, CA 92521 USA	University of California System; University of California Riverside	Domeniconi, C (corresponding author), Univ Calif Riverside, Dept Comp Sci, Riverside, CA 92521 USA.			Gunopulos, Dimitrios/0000-0001-6339-1879				Atkeson CG, 1997, ARTIF INTELL REV, V11, P11, DOI 10.1023/A:1006559212014; Bellman R., 1961, ADAPTIVE CONTROL PRO; CLEVELAND WS, 1988, J AM STAT ASSOC, V83, P596, DOI 10.2307/2289282; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Domeniconi C, 2000, PROC CVPR IEEE, P517, DOI 10.1109/CVPR.2000.855863; Duda R.O., 1973, J ROYAL STAT SOC SER; Friedman J. H., 1994, FLEXIBLE METRIC NEAR; Hastie T, 1996, IEEE T PATTERN ANAL, V18, P607, DOI 10.1109/34.506411; LOWE DG, 1995, NEURAL COMPUT, V7, P72, DOI 10.1162/neco.1995.7.1.72; Merz C., 1996, UCI REPOSITORY MACHI; MYLES JP, 1990, PATTERN RECOGN, V23, P1291, DOI 10.1016/0031-3203(90)90123-3; QUINLIN JR, 1993, C4 5 PROGRAMS MACHIN	12	16	16	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						458	464						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800065
C	Saon, G; Padmanabhan, M		Leen, TK; Dietterich, TG; Tresp, V		Saon, G; Padmanabhan, M			Minimum Bayes error feature selection for continuous speech recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We consider the problem of designing a linear transformation 0 E of rank p less than or equal to n, which projects the features of a classifier x is an element of R-n onto y = Ox is an element of IPP such as to achieve minimum Bayes error (or probability of misclassification). Two avenues will be explored: the first is to maximize the theta -average divergence between the class densities and the second is to, minimize the union Bhattacharyya bound in the range of theta. While both approaches yield similar performance in practice, they outperform standard LDA features and show a 10% relative improvement in the word error rate over state-of-the-art cepstral features on a large vocabulary telephony speech recognition task.	IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Saon, G (corresponding author), IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.							DECELL HP, 1972, P PURD U C MACH PROC; Duda R.O., 1973, J ROYAL STAT SOC SER; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; HAEBUMBACH R, 1992, P ICASSP, V1, P13; KULLBACK S, 1968, INFORMATION THEORY S; Kumar N, 1998, SPEECH COMMUN, V26, P283, DOI 10.1016/S0167-6393(98)00061-2; PADMANABHAN M, 1999, P EUROSPEECH 99 BUD; Saon G., 2000, P ICASSP 2000 IST TU; Searle S., 1982, MATRIX ALGEBRA USEFU	9	16	16	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						800	806						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800113
C	Slaney, M; Covell, M		Leen, TK; Dietterich, TG; Tresp, V		Slaney, M; Covell, M			FaceSync: A linear operator for measuring synchronization of video facial images and audio tracks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					FaceSync is an optimal linear algorithm that finds the degree of synchronization between the audio and image recordings of a human speaker. Using canonical correlation, it finds the best direction to combine all the audio and image data, projecting them onto a single axis. FaceSync uses Pearson's correlation to measure the degree of synchronization between the audio and image data. We derive the optimal linear transform to combine the audio and visual information and describe an implementation that avoids the numerical problems caused by computing the correlation matrices.			Slaney, M (corresponding author), IBM Corp, Almaden Res Ctr, 650 Harry Rd, San Jose, CA 95120 USA.			Slaney, Malcolm/0000-0001-9733-4864				Bregler C., 2007, P ACM SIGGRAPH, P353; COVELL M, 1996, P INT C IM PROC LAUS, V3, P471; HERSHEY J, IN PRESS ADV NEURAL, V12; Rabiner L., 1993, FUNDAMENTALS SPEECH; Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647; Scharf LL, 1998, IEEE T SIGNAL PROCES, V46, P647, DOI 10.1109/78.661332; SUGAMURA N, 1986, SPEECH COMMUNICATION, V4; Yehia H, 1998, SPEECH COMMUN, V26, P23, DOI 10.1016/S0167-6393(98)00048-X	8	16	20	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						814	820						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800115
C	Mozer, MC; Wolniewicz, R; Grimes, DB; Johnson, E; Kaushansky, H		Solla, SA; Leen, TK; Muller, KR		Mozer, MC; Wolniewicz, R; Grimes, DB; Johnson, E; Kaushansky, H			Churn reduction in the wireless industry	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Competition in the wireless telecommunications industry is rampant. To maintain profitability, wireless carriers must control chum, the loss of subscribers who switch from one carrier to another. We explore statistical techniques for churn prediction and, based on these predictions, an optimal policy for identifying customers to whom incentives should be offered to increase retention. Our experiments are based on a data base of nearly 47,000 U.S. domestic subscribers, and includes information about their usage, billing, credit, application, and complaint history. We show that under a wide variety of assumptions concerning the cost of intervention and the retention rate resulting from intervention, churn prediction and remediation can yield significant savings to a carrier We also show the importance of a data representation crafted by domain experts.	Athene Software, Boulder, CO 80302 USA		Mozer, MC (corresponding author), Athene Software, 2060 Broadway,Suite 300, Boulder, CO 80302 USA.							Bishop, 1995, NEURAL NETWORKS PATT; FOWLKES AJ, 1999, EFFECT CHURN VALUE I; GREEN DM, 1966, SIGNAL DETECTION THE; LUNA L, 1998, RADIO COMMUNICATIONS; POWER JD, 1998, 1998 RESIDENTIAL WIR	5	16	20	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						935	941						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700132
C	Weiss, Y; Freeman, WT		Solla, SA; Leen, TK; Muller, KR		Weiss, Y; Freeman, WT			Correctness of belief propagation in Gaussian graphical models of arbitrary topology	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Local "belief propagation" rules of the sort proposed by Pearl [15] are guaranteed to converge to the correct posterior probabilities in singly connected graphical models. Recently, a number of researchers have empirically demonstrated good performance of "loopy belief propagation"-using these same rules on graphs with loops. Perhaps the most dramatic instance is the near Shannon-limit performance of ''Turbo codes", whose decoding algorithm is equivalent to loopy belief propagation. Except for the case of graphs with a single loop, there has been little theoretical understanding of the performance of loopy propagation. Here we analyze belief propagation in networks with arbitrary topologies when the nodes in the graph describe jointly Gaussian random variables. We give an analytical formula relating the true posterior probabilities with those calculated using loopy propagation. We give sufficient conditions for convergence and show that when belief propagation converges it gives the correct posterior means for all graph topologies, not just networks with a single loop. The related "max-product" belief propagation algorithm finds the maximum posterior probability estimate for singly connected networks. We show that, even for non-Gaussian probability distributions, the convergence points of the max-product algorithm in loopy networks are maxima over a particular large local neighborhood of the posterior probability. These results help clarify the empirical performance results and motivate using the powerful belief propagation algorithm in a broader class of networks.	Univ Calif Berkeley, Dept Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Weiss, Y (corresponding author), Univ Calif Berkeley, Dept Comp Sci, 485 Soda Hall, Berkeley, CA 94720 USA.							AJI S, 1998, P 1998 ISIT; BERROU C, 1993, P IEEE INT COMM C 93; Cowell R., 1998, LEARNING GRAPHICAL M; FORNEY GD, 1998, 1998 INF THEOR WORKS; FREEMAN WT, 1999, ADV NEURAL INFORMATI, V11; FREEMAN WT, 1999, 9939 MERL; FREY BJ, 2000, IN PRESS ADV NEURAL, V12; FREY BJ, 1998, BAYESIAN NETWORKS PA; Gallager RG, 1963, LOW DENSITY PARITY C; Kschischang FR, 1998, IEEE J SEL AREA COMM, V16, P219, DOI 10.1109/49.661110; McEliece R. J., 1995, Proceedings. Thirty-Third Annual Allerton Conference on Communication, Control, and Computing, P366; McEliece RJ, 1998, IEEE J SEL AREA COMM, V16, P140, DOI 10.1109/49.661103; Murphy K. P., 1999, P UNC AI; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; RUSMEVICHIENTON.P, 2000, IN PRESS ADV NEURAL, V12; Strang G., 1986, INTRO APPL MATH; WEISS Y, 1999, UCBCSD991046; WEISS Y, 2000, IN PRESS NEURAL COMP; WEISS Y, 1997, 1616 MIT AI LAB; WIBERG N, 1996, THESIS U LINKOPING S	20	16	16	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						673	679						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700096
C	Bhushan, N; Shadmehr, R		Kearns, MS; Solla, SA; Cohn, DA		Bhushan, N; Shadmehr, R			Evidence for a forward dynamics model in human adaptive motor control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Based on computational principles, the concept of an internal model for adaptive control has been divided into a forward and an inverse model. However, there is as yet little evidence that learning control by the CNS is through adaptation of one or the other. Here we examine two adaptive control architectures, one based only on the inverse model and other based on a combination of forward and inverse models. We then show that for reaching movements of the hand in novel force fields, only the learning of the forward model results in key characteristics of performance that match the kinematics of human subjects. In contrast, the adaptive control system that relies only on the inverse model fails to produce the kinematic patterns observed in the subjects, despite the fact that it is more stable. Our results provide evidence that learning control of novel dynamics is via formation of a forward model.	Johns Hopkins Univ, Dept Biomed Engn, Baltimore, MD 21205 USA	Johns Hopkins University	Bhushan, N (corresponding author), Johns Hopkins Univ, Dept Biomed Engn, Baltimore, MD 21205 USA.							BHUSHAN N, 1999, IN PRESS BIOL CYBERA; JORDAN MI, 1994, J COGNITIVE NEUROSCI, V6, P359, DOI 10.1162/jocn.1994.6.4.359; JORDAN MI, 1992, COGNITIVE SCI, V16, P307, DOI 10.1207/s15516709cog1603_1; Miall RC, 1996, NEURAL NETWORKS, V9, P1265, DOI 10.1016/S0893-6080(96)00035-4; Narendra K S, 1990, IEEE Trans Neural Netw, V1, P4, DOI 10.1109/72.80202; SHADMEHR R, 1994, J NEUROSCI, V14, P3208; Shadmehr R, 1997, J NEUROSCI, V17, P409, DOI 10.1523/JNEUROSCI.17-01-00409.1997; Soechting JF, 1997, J BIOMECH ENG-T ASME, V119, P93, DOI 10.1115/1.2796071; WOLPERT DM, 1995, SCIENCE, V269, P1880, DOI 10.1126/science.7569931	9	16	16	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						3	9						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700001
C	Friedman, N; Singer, Y		Kearns, MS; Solla, SA; Cohn, DA		Friedman, N; Singer, Y			Efficient Bayesian parameter estimation in large discrete domains	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				NETWORKS	We examine the problem of estimating the parameters of a multinomial distribution over a large number of discrete outcomes, most of which do not appear in the training data. We analyze this problem from a Bayesian perspective and develop a hierarchical prior that incorporates the assumption that the observed outcomes constitute only a small subset of the possible outcomes. We show how to efficiently perform exact inference with this form of hierarchical prior and compare it to standard approaches.	Hebrew Univ Jerusalem, IL-91905 Jerusalem, Israel	Hebrew University of Jerusalem	Friedman, N (corresponding author), Hebrew Univ Jerusalem, IL-91905 Jerusalem, Israel.							Bauman Peto L.C., 1995, NAT LANG ENG, V1, P289; BUNTINE W, 1993, ARTIFICIAL INTELLIGE; CLARKE BS, 1994, J STAT PLAN INFER, V41, P37, DOI 10.1016/0378-3758(94)90153-8; COOPER GF, 1992, MACH LEARN, V9, P309, DOI 10.1023/A:1022649401552; DeGroot M. H., 1970, OPTIMAL STAT DECISIO; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; RABINOWICZ E, 1986, TRIBOLOGY MECHANICS, V3, P1; RISTAD E, 1995, CSTR49595; Singer Y, 1997, NEURAL COMPUT, V9, P1711, DOI 10.1162/neco.1997.9.8.1711; WILLEMS FMJ, 1995, IEEE T INFORM THEORY, V41, P653, DOI 10.1109/18.382012; WITTEN IH, 1991, IEEE T INFORM THEORY, V37, P1085, DOI 10.1109/18.87000	12	16	17	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						417	423						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700059
C	Rodriguez, P; Wiles, J		Jordan, MI; Kearns, MJ; Solla, SA		Rodriguez, P; Wiles, J			Recurrent neural networks can learn to implement symbol-sensitive counting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Recently researchers have derived formal complexity analysis of analog computation in the setting of discrete-time dynamical systems. As an empirical constrast, training recurrent neural networks (RNNs) produces self-organized systems that are realizations of analog mechanisms. Previous work showed that a RNN can learn to process a simple context-free language (CFL) by counting. Herein, we extend that work to show that a RNN can learn a harder CFL, a simple palindrome, by organizing its resources into a symbol-sensitive counting solution, and we provide a dynamical systems analysis which demonstrates how the network can not only count, but also copy and store counting information.	Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Rodriguez, P (corresponding author), Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA.		Wiles, Janet/C-1989-2008	Wiles, Janet/0000-0002-4051-4116					0	16	16	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						87	93						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700013
C	Sahani, M; Pezaris, JS; Andersen, RA		Jordan, MI; Kearns, MJ; Solla, SA		Sahani, M; Pezaris, JS; Andersen, RA			On the separation of signals from neighboring cells in tetrode recordings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We discuss a solution to the problem of separating waveforms produced by multiple cells in an extracellular neural recording. We take an explicitly probabilistic approach, using latent-variable models of varying sophistication to describe the distribution of waveforms produced by a single cell. The models range from a single Gaussian distribution of waveforms for each cell to a mixture of hidden Markov models. We stress the overall statistical structure of the approach, allowing the details of the generative model chosen to depend on the specific neural preparation.	CALTECH, Pasadena, CA 91125 USA	California Institute of Technology	Sahani, M (corresponding author), CALTECH, 216-76 Caltech, Pasadena, CA 91125 USA.								0	16	17	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						222	228						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700032
C	Bishop, CM; Qazaz, CS		Mozer, MC; Jordan, MI; Petsche, T		Bishop, CM; Qazaz, CS			Regression with input-dependent noise: A Bayesian treatment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				In most treatments of the regression problem it is assumed that the distribution of target data can be described by a deterministic function of the inputs, together with additive Gaussian noise having constant variance. The use of maximum likelihood to train such models then corresponds to the minimization of a sum-of-squares error function. In many applications a more realistic model would allow the noise variance itself to depend on the input variables. However, the use of maximum likelihood to train such models would give highly biased results. In this paper we show how a Bayesian treatment can allow for an input-dependent variance while overcoming the bias of maximum likelihood.			Bishop, CM (corresponding author), ASTON UNIV,NEURAL COMP RES GRP,BIRMINGHAM B4 7ET,W MIDLANDS,ENGLAND.								0	16	16	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						347	353						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00049
C	Elisseeff, A; PaugamMoisy, H		Mozer, MC; Jordan, MI; Petsche, T		Elisseeff, A; PaugamMoisy, H			Size of multilayer networks for exact learning: Analytic approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				This article presents a new result about the size of a multilayer neural network computing real outputs for exact learning of a finite set of real samples. The architecture of the network is feedforward, with one hidden layer and several outputs. Starting from a fixed training set, we consider the network as a function of its weights. We derive, for a wide family of transfer functions, a lower and an upper bound on the number of hidden units for exact learning, given the size of the dataset and the dimensions of the input and output spaces.			Elisseeff, A (corresponding author), ECOLE NORMALE SUPER LYON,DEPT MATH & INFORMAT,46 ALLEE ITALIE,F-69364 LYON 07,FRANCE.								0	16	17	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						162	168						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00023
C	Ji, CY; Ma, S		Mozer, MC; Jordan, MI; Petsche, T		Ji, CY; Ma, S			Combined weak classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				To obtain classification systems with both good generalization performance and efficieney in space and time, we propose a learning method based on combinations of weak classifiers, where weak classifiers are linear classifiers (perceptrons) which can do a little better than malting random guesses. A randomized algorithm is proposed to find the weak classifiers. They are then combined through a majority vote. As demonstrated through systematic experiments, the method developed is able to obtain combinations of weak classifiers with good generalization performance and a fast training time on a variety of test problems and real applications.			Ji, CY (corresponding author), RENSSELAER POLYTECH INST,DEPT ELECT COMP & SYST ENGN,TROY,NY 12180, USA.								0	16	16	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						494	500						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00070
C	Gordon, GJ		Touretzky, DS; Mozer, MC; Hasselmo, ME		Gordon, GJ			Stable fitted reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						CARNEGIE MELLON UNIV,DEPT COMP SCI,PITTSBURGH,PA 15213	Carnegie Mellon University									0	16	15	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						1052	1058						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00148
C	Levin, AU		Touretzky, DS; Mozer, MC; Hasselmo, ME		Levin, AU			Stock selection via nonlinear multi-factor models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						BZW BARCLAYS GLOBAL INVESTORS,ADV STRATEGIES & RES GRP,SAN FRANCISCO,CA 94105	Barclays									0	16	19	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						966	972						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00136
C	Stevens, CF; Zador, A		Touretzky, DS; Mozer, MC; Hasselmo, ME		Stevens, CF; Zador, A			Information through a spiking neuron	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SALK INST BIOL STUDIES,MNL S,LA JOLLA,CA 92037	Salk Institute									0	16	16	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						75	81						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00011
C	Viola, P; Schraudolph, NN; Sejnowski, TJ		Touretzky, DS; Mozer, MC; Hasselmo, ME		Viola, P; Schraudolph, NN; Sejnowski, TJ			Empirical entropy manipulation for real-world problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SALK INST BIOL STUDIES,COMPUTAT NEUROBIOL LAB,LA JOLLA,CA 92037	Salk Institute			Sejnowski, Terrence/AAV-5558-2021						0	16	17	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						851	857						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00120
C	SINGH, SP		MOODY, JE; HANSON, SJ; LIPPMANN, RP		SINGH, SP			THE EFFICIENT LEARNING OF MULTIPLE TASK SEQUENCES	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	16	16	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						251	258						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00031
C	Kim, M; Tack, J; Hwang, SJ		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Kim, Minseon; Tack, Jihoon; Hwang, Sung Ju			Adversarial Self-Supervised Contrastive Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Existing adversarial learning approaches mostly use class labels to generate adversarial samples that lead to incorrect predictions, which are then used to augment the training of the model for improved robustness. While some recent works propose semi-supervised adversarial learning methods that utilize unlabeled data, they still require class labels. However, do we really need class labels at all, for adversarially robust training of deep neural networks? In this paper, we propose a novel adversarial attack for unlabeled data, which makes the model confuse the instance-level identities of the perturbed data samples. Further, we present a self-supervised contrastive learning framework to adversarially train a robust neural network without labeled data, which aims to maximize the similarity between a random augmentation of a data sample and its instance-wise adversarial perturbation. We validate our method, Robust Contrastive Learning (RoCL), on multiple benchmark datasets, on which it obtains comparable robust accuracy over state-of-the-art supervised adversarial learning methods, and significantly improved robustness against the black box and unseen types of attacks. Moreover, with further joint fine-tuning with supervised adversarial loss, RoCL obtains even higher robust accuracy over using self-supervised learning alone. Notably, RoCL also demonstrate impressive results in robust transfer learning.	[Kim, Minseon; Tack, Jihoon; Hwang, Sung Ju] Korea Adv Inst Sci & Technol, Daejeon, South Korea; [Hwang, Sung Ju] AITRICS, Seoul, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Kim, M (corresponding author), Korea Adv Inst Sci & Technol, Daejeon, South Korea.	minseonkim@kaist.ac.kr; jihoontack@kaist.ac.kr; sjhwang82@kaist.ac.kr			Institute of Information & communications Technology Planning & Evaluation (IITP) - Korea government (MSIT) [2020-0-00153]; Samsung Research Funding Center of Samsung Electronics [SRFC-IT1502-51]	Institute of Information & communications Technology Planning & Evaluation (IITP) - Korea government (MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); Samsung Research Funding Center of Samsung Electronics(Samsung)	This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2020-0-00153) and Samsung Research Funding Center of Samsung Electronics (No. SRFC-IT1502-51). We thank Sihyun Yu, Seanie Lee, and Hayeon Lee for providing helpful feedbacks and suggestions in preparing an earlier version of the manuscript. We also thank the anonymous reviewers for their insightful comments and suggestions.	Athalye A, 2018, PR MACH LEARN RES, V80; Bachman P, 2019, ADV NEUR IN, V32; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Carmon Y, 2019, ADV NEUR IN, V32; Chen K., 2019, ARXIV191106470; Chen T., 2020, P IEEE C COMP VIS PA; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cohen J, 2019, PR MACH LEARN RES, V97; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Dosovitskiy A, 2016, IEEE T PATTERN ANAL, V38, P1734, DOI 10.1109/TPAMI.2015.2496141; Gidaris Spyros, 2018, ARXIV180307728; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Goyal Priya, 2017, ARXIV170602677; He K., 2020, P IEEECVF C COMPUTER, P9729; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hendrycks D., 2019, ADV NEURAL INFORM PR, V6; Hendrycks D., 2019, ICLR, P1; Hendrycks D, 2019, PR MACH LEARN RES, V97; Ilyas A., 2019, P ADV NEUR INF PROC; Khosla Prannay, 2020, ADV NEURAL INFORM PR; Kolesnikov A, 2019, REVISITING SELF SUPE; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kurakin A., 2016, ARXIV PREPRINT ARXIV; LIU KS, 2019, ADV NEURAL INFORM PR, P459, DOI DOI 10.1109/ICDM.2019.00056; Loshchilov I., 2017, P INT C LEARNING REP; Madaan Divyam, 2020, P 37 INT C MACH LEAR; Madry Aleksander, 2017, ARXIV; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Naseer Muzammal, 2020, IEEE CVF C COMP VIS; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Salman Hadi, 2019, ADV NEURAL INFORM PR; Shafahi A., 2020, INT C LEARN REPR; Shan YH, 2020, INT J MACH LEARN CYB, V11, P1825, DOI 10.1007/s13042-020-01074-x; Stanforth R., 2019, ADV NEURAL INFORM PR; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tian Y., 2020, ECCV, P776, DOI [10.48550/arXiv.1906.05849, DOI 10.1007/978-3-030-58621-8_45]; Tian Y., 2020, ARXIV200510243, V33, P6827; Tramer F, 2019, ADV NEUR IN, V32; Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393; Yin D., 2019, ADV NEURAL INFORM PR; You Yang, 2017, ARXIV170803888, V6, P12; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang HX, 2019, AIP CONF PROC, V2154, DOI 10.1063/1.5125364; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zheng S, 2016, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2016.485	47	15	15	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000066
C	Cao, JZ; Mo, LY; Zhang, YF; Jia, K; Shen, CH; Tan, MK		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cao, Jiezhang; Mo, Langyuan; Zhang, Yifan; Jia, Kui; Shen, Chunhua; Tan, Mingkui			Multi-marginal Wasserstein GAN	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Multiple marginal matching problem aims at learning mappings to match a source domain to multiple target domains and it has attracted great attention in many applications, such as multi-domain image translation. However, addressing this problem has two critical challenges: (i) Measuring the multi-marginal distance among different domains is very intractable; (ii) It is very difficult to exploit cross-domain correlations to match the target domain distributions. In this paper, we propose a novel Multi-marginal Wasserstein GAN (MWGAN) to minimize Wasserstein distance among domains. Specifically, with the help of multi-marginal optimal transport theory, we develop a new adversarial objective function with inner- and inter-domain constraints to exploit cross-domain correlations. Moreover, we theoretically analyze the generalization performance of MWGAN, and empirically evaluate it on the balanced and imbalanced translation tasks. Extensive experiments on toy and real-world datasets demonstrate the effectiveness of MWGAN.	South China Univ Technol, Peng Cheng Lab, Guangzhou, Peoples R China; [Cao, Jiezhang; Mo, Langyuan; Zhang, Yifan; Jia, Kui; Shen, Chunhua; Tan, Mingkui] Univ Adelaide, South China Univ Technol, Peng Cheng Lab, Adelaide, SA, Australia	South China University of Technology; University of Adelaide	Tan, MK (corresponding author), Univ Adelaide, South China Univ Technol, Peng Cheng Lab, Adelaide, SA, Australia.	secaojiezhang@mail.scut.edu.cn; selymo@mail.scut.edu.cn; sezyifan@mail.scut.edu.cn; kuijia@scut.edu.cn; chunhua.shen@adelaide.edu.au; mingkuitan@scut.edu.cn			Guangdong Provincial Scientific and Technological Funds [2018B010107001]; National Natural Science Foundation of China (NSFC) [61602185]; NSFC [61836003]; Fundamental Research Funds for the Central Universities [D2191240]; Program for Guangdong Introducing Innovative and Enterpreneurial Teams [2017ZT07X183]; Tencent AI Lab Rhino-Bird Focused Research Program [JR201902]; Microsoft Research Asia (MSRA Collaborative Research Program 2019)	Guangdong Provincial Scientific and Technological Funds; National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); NSFC(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Program for Guangdong Introducing Innovative and Enterpreneurial Teams; Tencent AI Lab Rhino-Bird Focused Research Program; Microsoft Research Asia (MSRA Collaborative Research Program 2019)(Microsoft)	This work is partially funded by Guangdong Provincial Scientific and Technological Funds under Grants 2018B010107001, National Natural Science Foundation of China (NSFC) 61602185, key project of NSFC (No. 61836003), Fundamental Research Funds for the Central Universities D2191240, Program for Guangdong Introducing Innovative and Enterpreneurial Teams 2017ZT07X183, and Tencent AI Lab Rhino-Bird Focused Research Program (No. JR201902). This work is also partially funded by Microsoft Research Asia (MSRA Collaborative Research Program 2019).	Adler J., 2018, ADV NEURAL INFORM PR; Almahairi A, 2018, PR MACH LEARN RES, V80; Arjovsky M, 2017, PR MACH LEARN RES, V70; Brock A., 2019, INT C LEARNING REPRE; Cao JZ, 2018, PR MACH LEARN RES, V80; Cao J, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON REAL-TIME COMPUTING AND ROBOTICS (RCAR), P549; Chen X, 2019, EXP EYE RES, V180, P164, DOI 10.1016/j.exer.2018.12.018; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Farnia F, 2018, ADV NEUR IN, V31; Frogner C., 2015, LEARNING WASSERSTEIN; Galanti T., 2018, ARXIV180708501; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397; Genevay A, 2018, PR MACH LEARN RES, V84; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Guo Y, 2019, ADV NEUR IN, V32; Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352; Guo Yong, 2018, ARXIV180907099; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He Zhenliang, 2017, ARXIV171110678; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Hui L, 2018, INT C PATT RECOG, P2044, DOI 10.1109/ICPR.2018.8545169; Kazemi H., 2018, ADV NEURAL INFORM PR, P10348; Kim T, 2017, PR MACH LEARN RES, V70; Kingma D.P, P 3 INT C LEARNING R; Liu AH, 2018, ADV NEUR IN, V31; Liu HD, 2018, PR MACH LEARN RES, V80; Liu Ming-Yu, 2017, NIPS; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Mao Xudong, 2017, ICCV, DOI [10.1109/ICCV.2017.304, DOI 10.1109/ICCV.2017.304]; Mathieu Michael, 2016, ICLR; Pan X., 2018, INT C MACH LEARN; Petzka Henning, 2018, ICLR; Sanjabi M, 2018, ADV NEUR IN, V31; Santambrogio F., 2015, OPTIMAL TRANSPORT AP, P99; Song CJ, 2019, ADV NEUR IN, V32; Villani C., 2008, OPTIMAL TRANSPORT OL; Wang C, 2019, IEEE T EVOLUTIONARY; Wu Z., 2020, AAAI C ART INT; Xie SA, 2018, PR MACH LEARN RES, V80; Yan YG, 2019, AAAI CONF ARTIF INTE, P5605; Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719; Zeng RH, 2019, IEEE T IMAGE PROCESS, V28, P5797, DOI 10.1109/TIP.2019.2922108; Zhang YF, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2768, DOI 10.1145/3219819.3219948; Zhu Jun-Yan, 2017, ICCV; Zhuang ZW, 2018, ADV NEUR IN, V31	53	15	16	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301073
C	Chen, ZD; Villar, S; Chen, L; Bruna, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Zhengdao; Villar, Soledad; Chen, Lei; Bruna, Joan			On the equivalence between graph isomorphism testing and function approximation with GNNs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Graph neural networks (GNNs) have achieved lots of success on graph-structured data. In light of this, there has been increasing interest in studying their representation power. One line of work focuses on the universal approximation of permutation-invariant functions by certain classes of GNNs, and another demonstrates the limitation of GNNs via graph isomorphism tests. Our work connects these two perspectives and proves their equivalence. We further develop a framework of the representation power of GNNs with the language of sigma-algebra, which incorporates both viewpoints. Using this framework, we compare the expressive power of different classes of GNNs as well as other methods on graphs. In particular, we prove that order-2 Graph G-invariant networks fail to distinguish non-isomorphic regular graphs with the same degree. We then extend them to a new architecture, Ring-GNN, which succeeds in distinguishing these graphs as well as for tasks on real-world datasets.	[Chen, Zhengdao; Chen, Lei] NYU, Courant Inst Math Sci, New York, NY 10003 USA; [Villar, Soledad; Bruna, Joan] NYU, Ctr Data Sci, Courant Inst Math Sci, New York, NY 10003 USA	New York University; New York University	Chen, ZD (corresponding author), NYU, Courant Inst Math Sci, New York, NY 10003 USA.	zc1216@nyu.edu; soledad.villar@nyu.edu; lc3909@nyu.edu; bruna@cims.nyu.edu			NSF [RI-IIS 1816753]; NSF CAREER CIF [1845360]; Alfred P. Sloan Fellowship; Samsung GRP; EOARD [FA9550-18-1-7007]; Simons Collaboration Algorithms and Geometry; Samsung Electronics	NSF(National Science Foundation (NSF)); NSF CAREER CIF; Alfred P. Sloan Fellowship(Alfred P. Sloan Foundation); Samsung GRP(Samsung); EOARD; Simons Collaboration Algorithms and Geometry; Samsung Electronics(Samsung)	We would like to thank Haggai Maron and Thomas Kipf for fruitful discussions and for pointing us towards G-invariant networks as powerful models to study representational power in graphs. We thank Prof. Michael M. Bronstein for supporting this research with computing resources. This work was partially supported by NSF grant RI-IIS 1816753, NSF CAREER CIF 1845360, the Alfred P. Sloan Fellowship, Samsung GRP and Samsung Electronics. SV was partially funded by EOARD FA9550-18-1-7007 and the Simons Collaboration Algorithms and Geometry.		0	15	15	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907054
C	Cutkosky, A; Orabona, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cutkosky, Ashok; Orabona, Francesco			Momentum-Based Variance Reduction in Non-Convex SGD	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Variance reduction has emerged in recent years as a strong competitor to stochastic gradient descent in non-convex problems, providing the first algorithms to improve upon the converge rate of stochastic gradient descent for finding first-order critical points. However, variance reduction techniques typically require carefully tuned learning rates and willingness to use excessively large "mega-batches" in order to achieve their improved results. We present a new algorithm, STORM, that does not require any batches and makes use of adaptive learning rates, enabling simpler implementation and less hyperparameter tuning. Our technique for removing the batches uses a variant of momentum to achieve variance reduction in non-convex optimization. On smooth losses F, STORM finds a point x with E{vertical bar vertical bar del F(x)vertical bar vertical bar] <= O(1 / root T + sigma(1/3) / T-1/3) in T iterations with sigma(2) variance in the gradients, matching the optimal rate and without requiring knowledge of sigma.	[Cutkosky, Ashok] Google Res, Mountain View, CA 94043 USA; [Orabona, Francesco] Boston Univ, Boston, MA 02215 USA	Google Incorporated; Boston University	Cutkosky, A (corresponding author), Google Res, Mountain View, CA 94043 USA.	ashok@cutkosky.com; francesco@orabona.com		Cutkosky, Ashok/0000-0002-3822-3029				Abadi M, 2015, P 12 USENIX S OPERAT; Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Cutkosky A, 2018, ADV NEUR IN, V31; Defazio Aaron, 2018, ARXIV181204529; Dieuleveut A, 2017, J MACH LEARN RES, V18; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Foster Dylan J, 2019, ARXIV191202365; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hogan E, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS: BIG DATA, EMERGENT THREATS, AND DECISION-MAKING IN SECURITY INFORMATICS, P315, DOI 10.1109/ISI.2013.6578850; Jain P., 2018, C LEARN THEOR, V75, P545; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Mahdavi Mehrdad, 2013, ADV NEURAL INFORM PR, P674; Nguyen L. M., 2017, ARXIV170507261; Nguyen L. M., 2018, ARXIV181110105; Nguyen LM, 2017, PR MACH LEARN RES, V70; Reddi SJ, 2016, PR MACH LEARN RES, V48; Reddi Sashank J., 2018, INT C LEARN REPR; RUSZCZYNSKI A, 1983, IEEE T AUTOMAT CONTR, V28, P1097, DOI 10.1109/TAC.1983.1103184; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tran-Dinh Q., 2019, ARXIV PREPRINT ARXIV; Vaswani A., 2018, MT RES TRACK; Wang C., 2013, ADV NEURAL INFORM PR, P181; Ward R., 2018, ARXIV180601811; Yuan K, 2016, J MACH LEARN RES, V17; Zhang L., 2013, ADV NEURAL INFORM PR, P980	31	15	15	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906085
C	Gu, XY; Akoglu, L; Rinaldo, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gu, Xiaoyi; Akoglu, Leman; Rinaldo, Alessandro			Statistical Analysis of Nearest Neighbor Methods for Anomaly Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SUPPORT	Nearest-neighbor (NN) procedures are well studied and widely used in both supervised and unsupervised learning problems. In this paper we are concerned with investigating the performance of NN-based methods for anomaly detection. We first show through extensive simulations that NN methods compare favorably to some of the other state-of-the-art algorithms for anomaly detection based on a set of benchmark synthetic datasets. We further consider the performance of NN methods on real datasets, and relate it to the dimensionality of the problem. Next, we analyze the theoretical properties of NN-methods for anomaly detection by studying a more general quantity called distance-to-measure (DTM), originally developed in the literature on robust geometric and topological inference. We provide finite-sample uniform guarantees for the empirical DTM and use them to derive misclassification rates for anomalous observations under various settings. In our analysis we rely on Huber's contamination model and formulate mild geometric regularity assumptions on the underlying distribution of the data.	[Gu, Xiaoyi; Rinaldo, Alessandro] Carnegie Mellon Univ, Dept Stat & Data Sci, Pittsburgh, PA 15213 USA; [Akoglu, Leman] Carnegie Mellon Univ, Heinz Coll Informat Syst & Publ Policy, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	Gu, XY (corresponding author), Carnegie Mellon Univ, Dept Stat & Data Sci, Pittsburgh, PA 15213 USA.	xgu1@andrew.cmu.edu; lakoglu@andrew.cmu.edu; arinaldo@cmu.edu						Angiulli F., 2002, Principles of Data Mining and Knowledge Discovery. 6th European Conference, PKDD 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2431), P15; Bandaragoda TR, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON DATA MINING WORKSHOP (ICDMW), P698, DOI 10.1109/ICDMW.2014.70; Bartlett P. L., 2011, ADV NEURAL INFORM PR, P478; Breunig MM, 2000, SIGMOD REC, V29, P93, DOI 10.1145/335191.335388; Campos GO, 2016, DATA MIN KNOWL DISC, V30, P891, DOI 10.1007/s10618-015-0444-8; Chazal F, 2018, J MACH LEARN RES, V18; Chazal F, 2015, J MACH LEARN RES, V16, P3603; Chazal F, 2011, FOUND COMPUT MATH, V11, P733, DOI 10.1007/s10208-011-9098-0; Chen Jinghui, 2017, P 2017 SIAM INT C DA, P90, DOI DOI 10.1137/1.9781611974973.11; Cuevas A, 1997, ANN STAT, V25, P2300; Emmott Andrew, 2015, ARXIV150301158; Falcao F, 2019, SAC '19: PROCEEDINGS OF THE 34TH ACM/SIGAPP SYMPOSIUM ON APPLIED COMPUTING, P318, DOI 10.1145/3297280.3297314; Frank A., 2010, UCI MACHINE LEARNING; Goldstein M, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0152173; Guan Leying, 2019, ARXIV190504396; Hero A. O., 2007, ADV NEURAL INFORM PR, P585; Huber P. J., 1992, ROBUST ESTIMATION LO, P492, DOI [10.1007/978-1-4612-4380-9_35, DOI 10.1007/978-1-4612-4380-9_35]; HUBER PJ, 1965, ANN MATH STAT, V36, P1753, DOI 10.1214/aoms/1177699803; Kim J, 2012, J MACH LEARN RES, V13, P2529; Li XJ, 2015, PROCEEDINGS OF THE 18TH ASIA PACIFIC SYMPOSIUM ON INTELLIGENT AND EVOLUTIONARY SYSTEMS, VOL 1, P433, DOI 10.1007/978-3-319-13359-1_34; Liu F, 2008, ACTA HORTIC, V792, P413, DOI 10.17660/ActaHortic.2008.792.48; Pang GS, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON DATA MINING WORKSHOP (ICDMW), P623, DOI 10.1109/ICDMW.2015.62; Pevny T, 2016, MACH LEARN, V102, P275, DOI 10.1007/s10994-015-5521-0; Ramaswamy Sridhar, 2000, SIGMOD C; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; Sugiyama Mahito, 2013, RAPID DISTANCE BASED, P467; Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49; Ting KM, 2017, MACH LEARN, V106, P55, DOI 10.1007/s10994-016-5586-4; Wang YZ, 2018, PR MACH LEARN RES, V80; Wu Mingxi, 2006, OUTLIER DETECTION SA, P767; Zimek Arthur, 2012, Statistical Analysis and Data Mining, V5, P363, DOI 10.1002/sam.11161	31	15	15	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902054
C	Lin, X; Zhen, HL; Li, ZH; Zhang, QF; Kwong, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lin, Xi; Zhen, Hui-Ling; Li, Zhenhua; Zhang, Qingfu; Kwong, Sam			Pareto Multi-Task Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Multi-task learning is a powerful method for solving multiple correlated tasks simultaneously. However, it is often impossible to find one single solution to optimize all the tasks, since different tasks might conflict with each other. Recently, a novel method is proposed to find one single Pareto optimal solution with good trade-off among different tasks by casting multi-task learning as multiobjective optimization. In this paper, we generalize this idea and propose a novel Pareto multi-task learning algorithm (Pareto MTL) to find a set of well-distributed Pareto solutions which can represent different trade-offs among different tasks. The proposed algorithm first formulates a multi-task learning problem as a multiobjective optimization problem, and then decomposes the multiobjective optimization problem into a set of constrained subproblems with different trade-off preferences. By solving these subproblems in parallel, Pareto MTL can find a set of well-representative Pareto optimal solutions with different trade-off among all tasks. Practitioners can easily select their preferred solution from these Pareto solutions, or use different trade-off solutions for different situations. Experimental results confirm that the proposed algorithm can generate well-representative solutions and outperform some state-of-the-art algorithms on many multi-task learning applications.	[Lin, Xi; Zhen, Hui-Ling; Zhang, Qingfu; Kwong, Sam] City Univ Hong Kong, Hong Kong, Peoples R China; [Li, Zhenhua] Nanjing Univ Aeronaut & Astronaut, Nanjing, Peoples R China	City University of Hong Kong; Nanjing University of Aeronautics & Astronautics	Lin, X (corresponding author), City Univ Hong Kong, Hong Kong, Peoples R China.	xi.lin@my.cityu.edu.hk; huilzhen@um.cityu.edu.hk; zhenhua.li@nuaa.edu.cn; qingfu.zhang@cityu.edu.hk; cssamk@cityu.edu.hk	Kwong, Sam/C-9319-2012	Kwong, Sam/0000-0001-7484-7261; Lin, Xi/0000-0001-5298-6893	Natural Science Foundation of China [61876163, 61672443]; ANR/RGC Joint Research Scheme - Research Grants Council of the Hong Kong Special Administrative Region, China [A-CityU101/16]; ANR/RGC Joint Research Scheme - France National Research Agency [A-CityU101/16]; Hong Kong RGC General Research Funds [9042489 (CityU 11206317), 9042322 (CityU 11200116)]	Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); ANR/RGC Joint Research Scheme - Research Grants Council of the Hong Kong Special Administrative Region, China(French National Research Agency (ANR)); ANR/RGC Joint Research Scheme - France National Research Agency(French National Research Agency (ANR)); Hong Kong RGC General Research Funds	This work was supported by the Natural Science Foundation of China under Grant 61876163 and Grant 61672443, ANR/RGC Joint Research Scheme sponsored by the Research Grants Council of the Hong Kong Special Administrative Region, China and France National Research Agency (Project No. A-CityU101/16), and Hong Kong RGC General Research Funds under Grant 9042489 (CityU 11206317) and Grant 9042322 (CityU 11200116).	[Anonymous], 2017, NUMERICAL EVOLUTIONA; Boyd S, 2004, CONVEX OPTIMIZATION; Cai H., 2019, PROC INT C LEARN REP; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chen Z, 2018, PR MACH LEARN RES, V80; Ddsiddri Jean-Antoine, 2012, EUR C COMP METH APPL; Deb K., 2001, MULTIOBJECTIVE OPTIM, V16; Dong J. D., 2018, P EUR C COMP VIS ECC, P517; Elsken Thomas, 2019, INT C LEARN REPR; Fliege J, 2000, MATH METHOD OPER RES, V51, P479, DOI 10.1007/s001860000043; Fliege J, 2016, SIAM J OPTIMIZ, V26, P2091, DOI 10.1137/15M1016424; Hernandez-Lobato D, 2016, PR MACH LEARN RES, V48; Huang Xinyu, 2018, P IEEE C COMP VIS PA; Huang Z., 2015, 16 ANN C INT SPEECH; Kendall A., 2018, P IEEE C COMP VIS PA; Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336; Kim J, 2017, INTERSPEECH, P1113, DOI 10.21437/Interspeech.2017-736; Kokkinos I., 2017, P IEEE C COMP VIS PA, P6129; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu Hai-Lin, 2017, IEEE T EVOLUTIONARY, V18, P450; Ruder S., 2017, ARXIV; Sabour S., 2017, 31 C NEUR INF PROC S, P3856; Sener O, 2018, ADV NEUR IN, V31; Shah A., 2016, INT C MACH LEARN ICM, P1919; Subramanian S., 2018, INT C LEARN REPR; Trivedi A, 2017, IEEE T EVOLUT COMPUT, V21, P440, DOI 10.1109/TEVC.2016.2608507; Tynkkynen LK, 2012, BMC HEALTH SERV RES, V12, DOI 10.1186/1472-6963-12-201; Wang P., 2019, IEEE T PATTERN ANAL; Wang P, 2018, PROC CVPR IEEE, P5860, DOI 10.1109/CVPR.2018.00614; Wiskel JB, 2018, PR INT PIPELINE CONF; Xiao H., 2017, ARXIV 170807747; Zamir AR, 2018, PROC CVPR IEEE, P3712, DOI 10.1109/CVPR.2018.00391; Zhang QF, 2007, IEEE T EVOLUT COMPUT, V11, P712, DOI 10.1109/TEVC.2007.892759; Zhang Yu, 2017, ARXIV170708114, DOI DOI 10.1109/TKDE.2021.3070203; Zhang YY, 2018, ASIAPAC SIGN INFO PR, P1771, DOI 10.23919/APSIPA.2018.8659587; Zitzler E, 1999, IEEE T EVOLUT COMPUT, V3, P257, DOI 10.1109/4235.797969; Zitzler E., 1999, EVOLUTIONARY ALGORIT, V63; Zuluaga Marcela, 2013, P INT C MACH LEARN A, V28, P462	39	15	15	0	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903065
C	Luise, G; Salzo, S; Pontil, M; Ciliberto, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Luise, Giulia; Salzo, Saverio; Pontil, Massimiliano; Ciliberto, Carlo			Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MINIMIZATION	We present a novel algorithm to estimate the barycenter of arbitrary probability distributions with respect to the Sinkhorn divergence. Based on a Frank-Wolfe optimization strategy, our approach proceeds by populating the support of the barycenter incrementally, without requiring any pre-allocation. We consider discrete as well as continuous distributions, proving convergence rates of the proposed algorithm in both settings. Key elements of our analysis are a new result showing that the Sinkhorn divergence on compact domains has Lipschitz continuous gradient with respect to the Total Variation and a characterization of the sample complexity of Sinkhorn potentials. Experiments validate the effectiveness of our method in practice.	[Luise, Giulia; Pontil, Massimiliano] UCL, Dept Comp Sci, London, England; [Salzo, Saverio; Pontil, Massimiliano] Ist Italiano Tecnol, CSML, Genoa, Italy; [Ciliberto, Carlo] Imperial Coll London, Dept Elect & Elect Engn, London, England	University of London; University College London; Istituto Italiano di Tecnologia - IIT; Imperial College London	Luise, G (corresponding author), UCL, Dept Comp Sci, London, England.	g.luise.16@ucl.ac.uk; saverio.salzo@iit.it; m.pontil@cs.ucl.ac.uk; c.ciliberto@ic.ac.uk	Salzo, Saverio/AAZ-7481-2021	Salzo, Saverio/0000-0003-0494-9101				Adams R., 2003, SOBOLEV SPACES, V2nd; Agueh M, 2011, SIAM J MATH ANAL, V43, P904, DOI 10.1137/100805741; Aliprantis K., 2006, INFINITE DIMENSIONAL; [Anonymous], 2018, J MACHINE LEARNING R; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Bach Francis, 2012, ARXIV12034523; Benamou Jean-David, 2015, SIAM J SCI COMPUT, V37, P2; Bonnans J., 2013, PERTURBATION ANAL OP; Boyd N, 2017, SIAM J OPTIMIZ, V27, P616, DOI 10.1137/15M1035793; Bredies K, 2013, ESAIM CONTR OPTIM CA, V19, P190, DOI 10.1051/cocv/2011205; Chizat L, 2018, MATH COMPUT, V87, P2563, DOI 10.1090/mcom/3303; Chouquet G., 1969, LECT ANAL, VII; Claici S., 2018, ARXIV E PRINTS; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi M, 2014, PR MACH LEARN RES, V32, P685; Demyanov V. F., 1967, J SIAM CONTROL, V5, P280; DEMYANOV VF, 1968, SIAM J CONTROL, V6, P73, DOI 10.1137/0306006; DEVROYE L, 1990, ANN STAT, V18, P1496, DOI 10.1214/aos/1176347765; Dognin Pierre, 2019, ARXIV190204999; DUNN JC, 1978, J MATH ANAL APPL, V62, P432, DOI 10.1016/0022-247X(78)90137-3; Dvurechenskii P., 2018, ADV NEURAL INFORM PR, P10760; Feydy Jean, 2019, INT C ART INT STAT A; FRANKLIN J, 1989, LINEAR ALGEBRA APPL, V114, P717, DOI 10.1016/0024-3795(89)90490-4; Genevay A., 2016, P NEUR INF PROC SYST, P3440; Genevay A, 2018, PR MACH LEARN RES, V84; Genevay Aude, 2018, INT C ART INT STAT A; Gramfort A, 2015, Inf Process Med Imaging, V24, P261, DOI 10.1007/978-3-319-19992-4_20; Jaggi M., 2013, P 30 INT C MACHINE L, P427; KANTOROVICH L, 1942, DOKLADY AKAD NAUK US; KNOPP P, 1968, CANADIAN J MATH, V20, P855, DOI 10.4153/CJM-1968-082-4; Lacoste-Julien S., 2015, ARXIV150102056; Lemmens B., 2013, ARXIV13047921; Lemmens B., 2012, NONLINEAR PERRON FRO, V189; MENON MV, 1967, P AM MATH SOC, V18, P244, DOI 10.2307/2035271; Muandet K, 2017, FOUND TRENDS MACH LE, V10, P1, DOI 10.1561/2200000060; NUSSBAUM RD, 1988, MEM AM MATH SOC, V75, P1; NUSSBAUM RD, 1993, J FUNCT ANAL, V115, P45, DOI 10.1006/jfan.1993.1080; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Pinelis I., 1994, ANN PROBAB, V22, P1679, DOI [10.1214/aop/1176988477, DOI 10.1214/AOP/1176988477]; Rabin J, 2012, LECT NOTES COMPUT SC, V6667, P435, DOI 10.1007/978-3-642-24785-9_37; SINKHORN R, 1967, PAC J MATH, V21, P343, DOI 10.2140/pjm.1967.21.343; Smale S, 2007, CONSTR APPROX, V26, P153, DOI 10.1007/s00365-006-0659-y; Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963; Solomon Justin, 2014, P 31 INT C INT C MAC, V32; Song Le, 2008, LEARNING VIA HILBERT; Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Staib M., 2017, ADV NEURAL INFORM PR, P2647; Stein E.M, 2016, SINGULAR INTEGRALS D, V30; Villani C., 2008, OPTIMAL TRANSPORT OL; Wendland H., 2004, SCATTERED DATA APPRO, V17; Ye JB, 2017, IEEE T SIGNAL PROCES, V65, P2317, DOI 10.1109/TSP.2017.2659647; YURINSKII VV, 1976, J MULTIVARIATE ANAL, V6, P473, DOI 10.1016/0047-259X(76)90001-4	54	15	15	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900086
C	Mettes, P; van der Pol, E; Snoek, CGM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mettes, Pascal; van der Pol, Elise; Snoek, Cees G. M.			Hyperspherical Prototype Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				POINTS	This paper introduces hyperspherical prototype networks, which unify classification and regression with prototypes on hyperspherical output spaces. For classification, a common approach is to define prototypes as the mean output vector over training examples per class. Here, we propose to use hyperspheres as output spaces, with class prototypes defined a priori with large margin separation. We position prototypes through data-independent optimization, with an extension to incorporate priors from class semantics. By doing so, we do not require any prototype updating, we can handle any training size, and the output dimensionality is no longer constrained to the number of classes. Furthermore, we generalize to regression, by optimizing outputs as an interpolation between two prototypes on the hypersphere. Since both tasks are now defined by the same loss function, they can be jointly trained for multi-task problems. Experimentally, we show the benefit of hyperspherical prototype networks for classification, regression, and their combination over other prototype methods, softmax cross-entropy, and mean squared error approaches.	[Mettes, Pascal; Snoek, Cees G. M.] Univ Amsterdam, ISIS Lab, Amsterdam, Netherlands; [van der Pol, Elise] Univ Amsterdam, UvA Bosch Delta Lab, Amsterdam, Netherlands	University of Amsterdam; University of Amsterdam	Mettes, P (corresponding author), Univ Amsterdam, ISIS Lab, Amsterdam, Netherlands.							Allen KR, 2019, PR MACH LEARN RES, V97; Barz Bjorn, 2019, ARXIV PREPRINT ARXIV; Bojanowski P, 2017, PR MACH LEARN RES, V70; Boney R., 2017, SEMISUPERVISED FEW S; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chintala S, 2017, APPL COMPUT HARMON A, V42, P154, DOI 10.1016/j.acha.2016.06.005; Chris Burges T.S., 2005, P 22 INT MACH LEARN, DOI 10.1145/1102351.1102363; Ganea Octavian-Eugen, 2018, NEURIPS, V2, P3; Gao TY, 2019, AAAI CONF ARTIF INTE, P6407; Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459; Guerriero S., 2018, ICLR WS, P1; Hasnat Abul, 2017, MISES FISHER MIXTURE; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; HICKS JS, 1959, COMMUN ACM, V2, P17, DOI 10.1145/377939.377945; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jetley S., 2015, BRIT MACH VIS C BMVC BRIT MACH VIS C BMVC; Li X, 2019, SIGNAL PROCESS-IMAGE, V74, P242, DOI 10.1016/j.image.2019.02.011; Liu W., 2017, P IEEE C COMPUTER VI, P212; Liu WY, 2018, ADV NEUR IN, V31; Liu WY, 2017, ADV NEUR IN, V30; Liu WY, 2016, PR MACH LEARN RES, V48; Liu Weiyang, 2018, CVPR; Luo C., 2017, ARXIV170205870; Mensink T, 2013, IEEE T PATTERN ANAL, V35, P2624, DOI 10.1109/TPAMI.2013.83; Mikolov T., 2013, ARXIV; Mitchell TM, 1980, CBMTR117; Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47; MULLER ME, 1959, COMMUN ACM, V2, P19, DOI 10.1145/377939.377946; Musin OR, 2015, EXP MATH, V24, P460, DOI 10.1080/10586458.2015.1022842; Nickel M, 2017, ADV NEUR IN, V30; Pan YW, 2019, PROC CVPR IEEE, P2234, DOI 10.1109/CVPR.2019.00234; Perrot M., 2015, ADV NEURAL INFORM PR, P1810; Saff EB, 1997, MATH INTELL, V19, P5, DOI 10.1007/BF03024331; Seth Harshita, 2019, PROTOTYPICAL METRIC; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Soudry D., 2018, ICLR, P1; Strezoski Gjorgji, 2017, ABS170800684 CORR; Tammes PML, 1930, RECL TRAV BOT NEERL, V27, P1; Tifrea Alexandru, 2019, ICLR; Vapnik V, 2015, J MACH LEARN RES, V16, P2023; Wah Catherine, 2011, CALTECH UCSD BIRDS 2; Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359; Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Yang H., 2018, CVPR; Zheng YT, 2018, PROC CVPR IEEE, P5089, DOI 10.1109/CVPR.2018.00534	46	15	15	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301047
C	Yang, DR; Zhao, L; Lin, ZC; Qin, T; Bian, J; Liu, TY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Derek; Zhao, Li; Lin, Zichuan; Qin, Tao; Bian, Jiang; Liu, Tieyan			Fully Parameterized Quantile Function for Distributional Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MARKOV DECISION-PROCESSES; ENVIRONMENT; VARIANCE	Distributional Reinforcement Learning (RL) differs from traditional RL in that, rather than the expectation of total returns, it estimates distributions and has achieved state-of-the-art performance on Atari Games. The key challenge in practical distributional RL algorithms lies in how to parameterize estimated distributions so as to better approximate the true continuous distribution. Existing distributional RL algorithms parameterize either the probability side or the return value side of the distribution function, leaving the other side uniformly fixed as in C51, QR-DQN or randomly sampled as in IQN. In this paper, we propose fully parameterized quantile function that parameterizes both the quantile fraction axis (i.e., the x-axis) and the value axis (i.e., y-axis) for distributional RL. Our algorithm contains a fraction proposal network that generates a discrete set of quantile fractions and a quantile value network that gives corresponding quantile values. The two networks are jointly trained to find the best approximation of the true distribution. Experiments on 55 Atari Games show that our algorithm significantly outperforms existing distributional RL algorithms and creates a new record for the Atari Learning Environment for non-distributed agents.	[Yang, Derek] Univ Calif San Diego, La Jolla, CA 92093 USA; [Yang, Derek; Zhao, Li; Qin, Tao; Bian, Jiang; Liu, Tieyan] Microsoft Res, Redmond, WA USA; [Lin, Zichuan] Tsinghua Univ, Beijing, Peoples R China	University of California System; University of California San Diego; Microsoft; Tsinghua University	Yang, DR (corresponding author), Univ Calif San Diego, La Jolla, CA 92093 USA.	dyang1206@gmail.com; lizo@microsoft.com; linzc16@mails.tsinghua.edu.cn; taoqin@microsoft.com; jiang.bian@microsoft.com; tyliu@microsoft.com		Qin, Tao/0000-0002-9095-0776				[Anonymous], LEARNING DELAYED REW; Barth-Maron G, 2018, INT C LEARN REPR; Bellemare MG, 2017, PR MACH LEARN RES, V70; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Bellman RE, 1957, DYNAMIC PROGRAMMING; Dabney W, 2018, AAAI CONF ARTIF INTE, P2892; Hessel M, 2018, AAAI CONF ARTIF INTE, P3215; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; JAQUETTE SC, 1973, ANN STAT, V1, P496, DOI 10.1214/aos/1176342415; Kapturowski S., 2018, RECURRENT EXPERIENCE; Kumar Saurabh, 2018, DOPAMINE RES FRAMEWO; Li YL, 2016, 2016 9TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2016), P1995; Machado MC, 2018, J ARTIF INTELL RES, V61, P523, DOI 10.1613/jair.5699; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Morimura T., 2010, PROC 27 INT C MACH L, P799; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Schaul T., 2016, INT C LEARN REPR ICL; SOBEL MJ, 1982, J APPL PROBAB, V19, P794, DOI 10.2307/3213832; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Tang YH, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2710; Teh, 2018, P MACHINE LEARNING R, P29; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; WHITE DJ, 1988, J OPTIMIZ THEORY APP, V56, P1, DOI 10.1007/BF00938524	25	15	15	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306022
C	Fusi, N; Sheth, R; Elibol, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Fusi, Nicolo; Sheth, Rishit; Elibol, Melih			Probabilistic Matrix Factorization for Automated Machine Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In order to achieve state-of-the-art performance, modern machine learning techniques require careful data pre-processing and hyperparameter tuning. Moreover, given the ever increasing number of machine learning models being developed, model selection is becoming increasingly important. Automating the selection and tuning of machine learning pipelines, which can include different data preprocessing methods and machine learning models, has long been one of the goals of the machine learning community. In this paper, we propose to solve this meta-learning task by combining ideas from collaborative filtering and Bayesian optimization. Specifically, we use a probabilistic matrix factorization model to transfer knowledge across experiments performed in hundreds of different datasets and use an acquisition function to guide the exploration of the space of possible pipelines. In our experiments, we show that our approach quickly identifies high-performing pipelines across a wide range of datasets, significantly outperforming the current state-of-the-art.	[Fusi, Nicolo; Sheth, Rishit; Elibol, Melih] Microsoft Res, Cambridge, England; [Elibol, Melih] Univ Calif Berkeley, EECS, Berkeley, CA 94720 USA	Microsoft; University of California System; University of California Berkeley	Fusi, N (corresponding author), Microsoft Res, Cambridge, England.	nfusi@microsoft.com; rishet@microsoft.com; elibol@cs.berkeley.edu						[Anonymous], 2008, P 25 INT C MACH LEAR; Bergstra J., 2013, JMLR WORKSHOP C P IC, V28, P115, DOI [10.5555/3042817.3042832, DOI 10.5555/3042817.3042832]; Bergstra J, 2012, J MACH LEARN RES, V13, P281; Bergstra James S, 2011, ADV NEURAL INFORM PR, P2546, DOI [10.5555/2986459.2986743, DOI 10.5555/2986459.2986743]; Feurer M, 2015, ADV NEUR IN, V28; Feurer M, 2015, AAAI CONF ARTIF INTE, P1128; Grunewalder S., 2010, J MACH LEARN RES, P273; Guyon I., 2016, P 2016 WORKSHOP AUTO, P21; Lawrence N, 2005, J MACH LEARN RES, V6, P1783; Lawrence Neil, 2009, ICML; Leite Rui, 2012, Machine Learning and Data Mining in Pattern Recognition. Proceedings 8th International Conference, MLDM 2012, P117, DOI 10.1007/978-3-642-31537-4_10; Li L, 2016, ARXIV160306560; Malitsky Y., 2014, 7 ANN S COMB SEARCH; Misir M, 2017, ARTIF INTELL, V244, P291, DOI 10.1016/j.artint.2016.12.001; Mokus J., 1975, LECT NOTES COMPUTER, P400, DOI [10.1007/3-540-07165-2_55, DOI 10.1007/3-540-07165-2_55, DOI 10.1007/978-3-662-38527-2_55, 10.1007/3-540-07165-2, DOI 10.1007/3-540-07165-2]; Osborne M, 2009, LEARNING INTELLIGENT, P1; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Perrone V, 2017, ARXIV171202902; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Reif M, 2012, MACH LEARN, V87, P357, DOI 10.1007/s10994-012-5286-7; Schilling N, 2015, LECT NOTES ARTIF INT, V9285, P87, DOI 10.1007/978-3-319-23525-7_6; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Springenberg J.T., 2016, P ADV NEURAL INFORM, P4134; Srinivas N., 2009, P 27 INT C MACHINE L, P1015; Stern D, 2010, AAAI CONF ARTIF INTE, P179; Swersky K., 2013, ADV NEURAL INFORM PR, P2004, DOI DOI 10.1038/S41598-021-83582-6; Vanschoren J., 2013, ACM SIGKDD EXPLOR NE, V15, P49, DOI [10.1145/2641190.2641198, DOI 10.1145/2641190.2641198]; Wistuba M., 2015, METASEL PKDDECML, V145, P15, DOI 10.5555/3053836.3053842	30	15	15	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303035
C	Gao, HY; Wang, ZY; Ji, SW		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gao, Hongyang; Wang, Zhengyang; Ji, Shuiwang			ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Convolutional neural networks (CNNs) have shown great capability of solving various artificial intelligence tasks. However, the increasing model size has raised challenges in employing them in resource-limited applications. In this work, we propose to compress deep models by using channel-wise convolutions, which replace dense connections among feature maps with sparse ones in CNNs. Based on this novel operation, we build light-weight CNNs known as ChannelNets. ChannelNets use three instances of channel-wise convolutions; namely group channel-wise convolutions, depth-wise separable channel-wise convolutions, and the convolutional classification layer. Compared to prior CNNs designed for mobile devices, ChannelNets achieve a significant reduction in terms of the number of parameters and computational cost without loss in accuracy. Notably, our work represents the first attempt to compress the fully-connected classification layer, which usually accounts for about 25% of total parameters in compact CNNs. Experimental results on the ImageNet dataset demonstrate that ChannelNets achieve consistently better performance compared to prior methods.	[Gao, Hongyang; Wang, Zhengyang; Ji, Shuiwang] Texas A&M Univ, College Stn, TX 77843 USA	Texas A&M University System; Texas A&M University College Station	Gao, HY (corresponding author), Texas A&M Univ, College Stn, TX 77843 USA.	hongyang.gao@tamu.edu; zhengyang.wang@tamu.edu; sji@tamu.edu	Wang, Zhengyang/T-4824-2019	Wang, Zhengyang/0000-0002-5146-2884; Ji, Shuiwang/0000-0002-4205-4563	National Science Foundation [IIS-1633359, DBI-1641223]	National Science Foundation(National Science Foundation (NSF))	This work was supported in part by National Science Foundation grants IIS-1633359 and DBI-1641223.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Chen WL, 2015, PR MACH LEARN RES, V37, P2285; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Howard A.G., 2017, MOBILENETS EFFICIENT; Iandola F.N., 2016, ARXIV; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lebedev V., 2015, 3 INT C LEARNING REP; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954]; Luong A, 2016, P 20 SIGNLL C COMP N, P291, DOI DOI 10.18653/V1/K16-1029; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Sandler Mark, 2018, ARXIV180104381, DOI DOI 10.1109/CVPR.2018.00474; Sifre Laurent, 2014, THESIS; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Wu JX, 2016, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2016.521; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716	24	15	16	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305023
C	Gupta, A; Murali, A; Gandhi, D; Pinto, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gupta, Abhinav; Murali, Adithyavairavan; Gandhi, Dhiraj; Pinto, Lerrel			Robot Learning in Homes: Improving Generalization and Reducing Dataset Bias	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Data-driven approaches to solving robotic tasks have gained a lot of traction in recent years. However, most existing policies are trained on large-scale datasets collected in curated lab settings. If we aim to deploy these models in unstructured visual environments like people's homes, they will be unable to cope with the mismatch in data distribution. In such light, we present the first systematic effort in collecting a large dataset for robotic grasping in homes. First, to scale and parallelize data collection, we built a low cost mobile manipulator assembled for under 3 K USD. Second, data collected using low cost robots suffer from noisy labels due to imperfect execution and calibration errors. To handle this, we develop a framework which factors out the noise as a latent variable. Our model is trained on 28 K grasps collected in several houses under an array of different environmental conditions. We evaluate our models by physically executing grasps on a collection of novel objects in multiple unseen homes. The models trained with our home dataset showed a marked improvement of 43.7% over a baseline model trained with data collected in lab. Our architecture which explicitly models the latent noise in the dataset also performed 10% better than one that did not factor out the noise. We hope this effort inspires the robotics community to look outside the lab and embrace learning based approaches to handle inaccurate cheap robots.	[Gupta, Abhinav; Murali, Adithyavairavan; Gandhi, Dhiraj; Pinto, Lerrel] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Gupta, A; Murali, A; Gandhi, D; Pinto, L (corresponding author), Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.	abhinavg@cs.cmu.edu; amurali@cs.cmu.edu; dgandhi@cs.cmu.edu; lerrelp@cs.cmu.edu			ONR MURI [N000141612007]; Sloan Research Fellowship; Uber Fellowship	ONR MURI(MURIOffice of Naval Research); Sloan Research Fellowship(Alfred P. Sloan Foundation); Uber Fellowship	This work was supported by ONR MURI N000141612007. Abhinav was supported in part by Sloan Research Fellowship and Adithya was partly supported by a Uber Fellowship.	Agarwal P., 2016, LEARNING POKE POKING; Bicchi A., 2000, INT C ROB AUT; Bohg J., 2014, IEEE T ROBOTICS; Bousmalis K, 2018, IEEE INT CONF ROBOT, P4243; Calandra R., 2017, C ROB LEARN; Chen  Tao, 2018, NUERAL INFORM PROCES; Dahl  George, 2010, IEEE T AUDIO SPEECH; Deisenroth M., 2011, RSS; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Erickson  Zackory, 2017, C ROB LEARN; Fang K, 2018, IEEE INT CONF ROBOT, P3516; Finn C., 2016, ADV NEURAL INFORM PR; Frenay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894; Hawes N, 2017, IEEE ROBOT AUTOM MAG, V24, P146, DOI 10.1109/MRA.2016.2636359; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Josh T, 2017, DOMAIN RANDOMIZATION; Keselman L., 2017, ARXIV170505548; Kingma D.P, P 3 INT C LEARNING R; Lenz I., 2015, IJRR; Levine S., 2016, ABS160302199 CORR; Levine S, 2016, J MACH LEARN RES, V17; Mahler  J., 2017, RSS; Mahler J, 2016, IEEE INT CONF ROBOT, P1957, DOI 10.1109/ICRA.2016.7487342; Misra I., 2016, CVPR; Murali  Adithyavairavan, 2018, INT S EXP ROB; Murali  Adithyavairavan, 2018, INT C ROB AUT; Nair  Ashvin, 2017, INT C ROB AUT; Nettleton DF, 2010, ARTIF INTELL REV, V33, P275, DOI 10.1007/s10462-010-9156-z; NGUYEN VD, 1988, INT J ROBOT RES, V7, P3, DOI 10.1177/027836498800700301; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Peng Xue Bin, 2017, ARXIV171006537; Pinto L, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Pinto  Lerrel, 2017, INT C ROB AUT; Pinto  Lerrel, 2015, ABS150906825 CORR; Redmon J., 2017, P IEEE C COMPUTER VI, P7263, DOI DOI 10.1109/CVPR.2017.690; Sharma  Pratyusha, 2018, ARXIV181007121; Sun Chen, 2017, ICCV; Veloso M, 2012, IEEE INT C INT ROBOT, P5446, DOI 10.1109/IROS.2012.6386300; Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885; Yahya A., 2017, IROS	40	15	15	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003063
C	Hajiramezanali, E; Dadaneh, SZ; Karbalayghareh, A; Zhou, MY; Qian, XN		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hajiramezanali, Ehsan; Dadaneh, Siamak Zamani; Karbalayghareh, Alireza; Zhou, Mingyuan; Qian, Xiaoning			Bayesian multi-domain learning for cancer subtype discovery from next-generation sequencing count data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Precision medicine aims for personalized prognosis and therapeutics by utilizing recent genome-scale high-throughput profiling techniques, including next-generation sequencing (NGS). However, translating NGS data faces several challenges. First, NGS count data are often overdispersed, requiring appropriate modeling. Second, compared to the number of involved molecules and system complexity, the number of available samples for studying complex disease, such as cancer, is often limited, especially considering disease heterogeneity. The key question is whether we may integrate available data from all different sources or domains to achieve reproducible disease prognosis based on NGS count data. In this paper, we develop a Bayesian Multi-Domain Learning (BMDL) model that derives domain-dependent latent representations of overdispersed count data based on hierarchical negative binomial factorization for accurate cancer subtyping even if the number of samples for a specific cancer type is small. Experimental results from both our simulated and NGS datasets from The Cancer Genome Atlas (TCGA) demonstrate the promising potential of BMDL for effective multi-domain learning without "negative transfer" effects often seen in existing multi-task learning and transfer learning methods.	[Hajiramezanali, Ehsan; Dadaneh, Siamak Zamani; Karbalayghareh, Alireza; Qian, Xiaoning] Texas A&M Univ, College Stn, TX 77843 USA; [Zhou, Mingyuan] Univ Texas Austin, Austin, TX 78712 USA	Texas A&M University System; Texas A&M University College Station; University of Texas System; University of Texas Austin	Hajiramezanali, E (corresponding author), Texas A&M Univ, College Stn, TX 77843 USA.	ehsanr@tamu.edu; siamak@tamu.edu; alireza.kg@tamu.edu; Mingyuan.Zhou@mccombs.utexas.edu; xqian@ece.tamu.edu	Zhou, Mingyuan/AAE-8717-2021		NSF [CCF-1553281, IIS-1812641, IIS-1812699]	NSF(National Science Foundation (NSF))	We would like to thank Dr. Sahar Yarian for insightful discussions. We also thank Texas A&M High Performance Research Computing and Texas Advanced Computing Center for providing computational resources to perform experiments in this work. This work was supported in part by the NSF Awards CCF-1553281, IIS-1812641, and IIS-1812699.	[Anonymous], 2012, INT C MACH LEARN; Argyriou A., 2007, NIPS, V19, P41, DOI DOI 10.1007/S10994-007-5040-8; Chelba C, 2006, COMPUT SPEECH LANG, V20, P382, DOI 10.1016/j.csl.2005.05.005; Chin L, 2008, NATURE, V455, P1061, DOI 10.1038/nature07385; Dadaneh SZ, 2018, J AM STAT ASSOC, V113, P81, DOI 10.1080/01621459.2017.1328358; Griffiths T.L., 2006, ADV NEURAL INFORM PR, P475; Hajiramezanali Ehsan, 2018, ARXIV180302527; Jacob L., 2009, P 21TH NIPS, P745; Kang Z., 2011, P INT C MACH LEARN, V2, P4; Karbalayghareh Alireza, 2018, IEEE T SIGNAL PROCES; Kumar A., 2012, ICML; Le Cam L., 1960, PAC J MATH, V10, P1181, DOI DOI 10.2140/PJM.1960.10.1181; Love MI, 2014, GENOME BIOL, V15, DOI 10.1186/s13059-014-0550-8; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pardoe D., 2010, ICML, P863; Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059; Rai P., 2010, INT C ART INT STAT, P613; Scholkopf B., 2001, LEARNING KERNELS SUP; Teh Yee W, 2005, ADV NEURAL INFORM PR, P1385, DOI DOI 10.1198/016214506000000302; Thibaux Romain, 2007, INT C ART INT STAT, P564; Williamson  S., 2010, IBP COMPOUND DIRICHL; Xue Y, 2007, J MACH LEARN RES, V8, P35; Zhou M, 2009, ADV NEURAL INFORM PR, P2295; Zhou M, 2012, ADV NEURAL INFORM PR; Zhou MY, 2018, BAYESIAN ANAL, V13, P1061, DOI 10.1214/17-BA1070; Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211	27	15	15	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003065
C	Logeswaran, L; Lee, H; Bengio, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Logeswaran, Lajanugen; Lee, Honglak; Bengio, Samy			Content preserving text generation with attribute controls	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this work, we address the problem of modifying textual attributes of sentences. Given an input sentence and a set of attribute labels, we attempt to generate sentences that are compatible with the conditioning information. To ensure that the model generates content compatible sentences, we introduce a reconstruction loss which interpolates between auto-encoding and back-translation loss components. We propose an adversarial loss to enforce generated samples to be attribute compatible and realistic. Through quantitative, qualitative and human evaluations we demonstrate that our model is capable of generating fluent sentences that better reflect the conditioning information compared to prior methods. We further demonstrate that the model is capable of simultaneously controlling multiple attributes.	[Logeswaran, Lajanugen] Univ Michigan, Ann Arbor, MI 48109 USA; [Lee, Honglak; Bengio, Samy] Google Brain, Mountain View, CA USA	University of Michigan System; University of Michigan; Google Incorporated	Logeswaran, L (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	llajan@umich.edu; honglak@google.com; bengio@google.com						Artetxe Mikel, 2017, ARXIV171011041; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Chen Xi, 2016, ARXIV161102731; Chung JY, 2015, PR MACH LEARN RES, V37, P2067; Diao QM, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P193, DOI 10.1145/2623330.2623758; Fu Zhenxin, 2017, ARXIV171106861; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He Di, 2016, NEURAL INFORM PROCES, P2; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu ZT, 2017, PR MACH LEARN RES, V70; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jozefowicz Rafal, 2016, ARXIV160202410; Kikuchi Yuta, 2016, P 2016 C EMP METH NA, P1328, DOI DOI 10.18653/V1/D16-1140; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kiros R., 2014, NIPS 14, P2348; Lample Guillaume, 2017, INT C LEARN REPR; Li J., 2018, ACL; Li JW, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P994; Maas A., 2011, P 49 ANN M ASS COMPU, P142; Miyato Takeru, 2018, ARXIV180205637; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Prabhumoye S, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P866; Radford A., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1704.01444; Ramm A, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017): SYSTEM DEMONSTRATIONS, P1, DOI 10.18653/v1/P17-4001; Reed S, 2016, PR MACH LEARN RES, V48; Sennrich Rico, 2016, P 2016 C N AM CHAPT, P35; Shen Tianxiao, 2017, P NIPS, P6830; Shetty Rakshith, 2017, ARXIV171101921; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; van den Oord A, 2016, PR MACH LEARN RES, V48; Xu Wei, 2012, COLING; Yamagishi Hayahide, 2016, P 3 WORKSH AS TRANSL, P203; Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47	34	15	15	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305014
C	Rudi, A; Calandriello, D; Carratino, L; Rosasco, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Rudi, Alessandro; Calandriello, Daniele; Carratino, Luigi; Rosasco, Lorenzo			On Fast Leverage Score Sampling and Optimal Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Leverage score sampling provides an appealing way to perform approximate computations for large matrices. Indeed, it allows to derive faithful approximations with a complexity adapted to the problem at hand. Yet, performing leverage scores sampling is a challenge in its own right requiring further approximations. In this paper, we study the problem of leverage score sampling for positive definite matrices defined by a kernel. Our contribution is twofold. First we provide a novel algorithm for leverage score sampling and second, we exploit the proposed method in statistical learning by deriving a novel solver for kernel ridge regression. Our main technical contribution is showing that the proposed algorithms are currently the most efficient and accurate for these problems.	[Rudi, Alessandro] INRIA Sierra team, ENS, Paris, France; [Calandriello, Daniele] LCSL IIT, Genoa, Italy; [Calandriello, Daniele; Rosasco, Lorenzo] MIT, Genoa, Italy; [Carratino, Luigi] Univ Genoa, Genoa, Italy; [Rosasco, Lorenzo] Univ Genoa, LCSL IIT, Genoa, Italy	UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); University of Genoa; University of Genoa	Rudi, A (corresponding author), INRIA Sierra team, ENS, Paris, France.	alessandro.rudi@inria.fr; daniele.calandriello@iit.it		Carratino, Luigi/0000-0001-9947-9944	Center for Brains, Minds and Machines (CBMM) - NSF STC award [CCF-1231216]; Italian Institute of Technology; AFOSR [FA9550-17-1-0390, BAA-AFRL-AFOSR-2016-0007]; EU H2020-MSCA-RISE project NoMADS [DLV-777826]; European Research Council [SEQUOIA 724063]	Center for Brains, Minds and Machines (CBMM) - NSF STC award; Italian Institute of Technology(Istituto Italiano di Tecnologia - IIT); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); EU H2020-MSCA-RISE project NoMADS; European Research Council(European Research Council (ERC)European Commission)	This material is based upon work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216, and the Italian Institute of Technology. We gratefully acknowledge the support of NVIDIA Corporation for the donation of the Titan Xp GPUs and the Tesla k40 GPU used for this research. L. R. acknowledges the support of the AFOSR projects FA9550-17-1-0390 and BAA-AFRL-AFOSR-2016-0007 (European Office of Aerospace Research and Development), and the EU H2020-MSCA-RISE project NoMADS - DLV-777826. A. R. acknowledges the support of the European Research Council (grant SEQUOIA 724063).	Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Arora R, 2012, ANN ALLERTON CONF, P861, DOI 10.1109/Allerton.2012.6483308; Bach Francis, 2013, C LEARN THEOR; Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308; Calandriello D., 2017, P 34 INT C MACH LEAR, V70, P645; Calandriello D., 2017, ADV NEURAL INFORM PR, V30, P6140; Calandriello Daniele, 2017, AISTATS; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Carratino Luigi, 2018, ADV NEURAL INFORM PR, P10213; Ciliberto C., 2017, ADV NEURAL INFORM PR, P1986; Ciliberto Carlo, 2016, ADV NEURAL INFORM PR, V29, P4412; Drineas P, 2012, J MACH LEARN RES, V13, P3475; El Alaoui A, 2015, ADV NEUR IN, V28; Korba Anna, 2018, ADV NEURAL INFORM PR, P9008; Lin J., 2018, APPL COMPUTATIONAL H, DOI 10.1016/j.acha.2018.09.009.; Musco Cameron, 2017, NIPS; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Roux N.L., 2012, ADV NEURAL INFORM PR, V25, P2663; Rudi A., 2015, ADV NEURAL INFORM PR, V28, P1657, DOI DOI 10.5555/2969239.2969424; Rudi A., 2017, ADV NEURAL INFORM PR, V30, P3891; Rudi A., 2017, ADV NEURAL INFORM PR, V30, P3215; Scholkopf B., 2001, LEARNING KERNELS SUP; Smola Alex J, 2000, SPARSE GREEDY MATRIX; Steinwart I., 2008, SUPPORT VECTOR MACHI; Steinwart I., 2009, P C LEARN THEOR COLT; Tropp J. A., 2012, TECHNICAL REPORT; Williams Christopher, 2001, NEURAL INFORM PROCES; Woodruff David P., 2014, ARXIV14114357	30	15	15	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000020
C	Shavitt, I; Segal, E		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shavitt, Ira; Segal, Eran			Regularization Learning Networks: Deep Learning for Tabular Datasets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Despite their impressive performance, Deep Neural Networks (DNNs) typically underperform Gradient Boosting Trees (GBTs) on many tabular-dataset learning tasks. We propose that applying a different regularization coefficient to each weight might boost the performance of DNNs by allowing them to make more use of the more relevant inputs. However, this will lead to an intractable number of hyperparameters. Here, we introduce Regularization Learning Networks (RLNs), which overcome this challenge by introducing an efficient hyperparameter tuning scheme which minimizes a new Counterfactual Loss. Our results show that RLNs significantly improve DNNs on tabular datasets, and achieve comparable results to GBTs, with the best performance achieved with an ensemble that combines GBTs and RLNs. RLNs produce extremely sparse networks, eliminating up to 99.8% of the network edges and 82% of the input features, thus providing more interpretable models and reveal the importance that the network assigns to different inputs. RLNs could efficiently learn a single network in datasets that comprise both tabular and unstructured data, such as in the setting of medical imaging accompanied by electronic health records. An open source implementation of RLN can be found at https: //github.com/irashavitt/regularization_ learning_networks.	[Shavitt, Ira; Segal, Eran] Weizmann Inst Sci, Rehovot, Israel	Weizmann Institute of Science	Shavitt, I (corresponding author), Weizmann Inst Sci, Rehovot, Israel.	irashavitt@gmail.com; eran.segal@weizmann.ac.il	Segal, Eran/AAF-4855-2019					[Anonymous], 2016, CORR; Beam David, 2015, ROSSMANN STORE SALES; Belkhayat Kamil, 2018, XGBOOST LGBM PORTO S; Bengio Y., 2007, SCALING LEARNING ALG; Bengio Y., REPRESENTATION LEARN; Bengio Yoshua, 1999, GRADIENT BASED OPTIM, P1; Bergstra J. S., 2011, P 24 INT C NEUR INF, P2546, DOI DOI 10.1145/3065386; Cai Hengxing, 2018, KDD CUP 2018 TRAVEL; Chen T., XGBOOST SCALABLE TRE; Chiu Chung-Cheng, STATE OF THE ART SPE; Garson D.G., 1991, AI EXPERT, V6, P47; Goodfellow I.J., EXPLAINING HARNESSIN; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goodman B, EUROPEAN UNION REGUL; Hinton G. E., DISTRIBUTED REPRESEN; Hooker S., EVALUATING FEATURE I; Johnson M., T ASSOC COMPUT LING; Krizhevsky A., IMAGENET CLASSIFICAT; Luketina Jelena, SCALABLE GRADIENT BA; Maclaurin Dougal, GRADIENT BASED HYPER; Miotto Riccardo, 2016, DEEP PATIENT UNSUPER; Papernot Nicolas, LIMITATIONS DEEP LEA; Rajkomar Alvin, 2018, NPJ DIGITAL MED, V1; Shrikumar Avanti, NOT JUST BLACK BOX L, V6, P4; Smith Leslie N., DISCIPLINED APPROACH; Snoek J., 2012, P NIPS, P1, DOI DOI 10.48550/ARXIV.1206.2944; Sundararajan Mukund, GRADIENTS COUNTERFAC, V6, P4; Suzuki Kenji, RADIOLOGICAL PHYS TE, V10, P7; Yann L., MNIST DATABASE HANDW; Yide Huang, TRAVEL TIME PREDICTI	32	15	15	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301037
C	Si, XJ; Dai, HJ; Raghothaman, M; Naik, M; Song, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Si, Xujie; Dai, Hanjun; Raghothaman, Mukund; Naik, Mayur; Song, Le			Learning Loop Invariants for Program Verification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					A fundamental problem in program verification concerns inferring loop invariants. The problem is undecidable and even practical instances are challenging. Inspired by how human experts construct loop invariants, we propose a reasoning framework CODE2INV that constructs the solution by multi-step decision making and querying an external program graph memory block. By training with reinforcement learning, CODE2INv captures rich program features and avoids the need for ground truth solutions as supervision. Compared to previous learning tasks in domains with graph-structured data, it addresses unique challenges, such as a binary objective function and an extremely sparse reward that is given by an automated theorem prover only after the complete loop invariant is proposed. We evaluate CODE2INV on a suite of 133 benchmark problems and compare it to three state-of-the-art systems. It solves 106 problems compared to 73 by a stochastic search-based system, 77 by a heuristic search-based system, and 100 by a decision tree learning-based system. Moreover, the strategy learned can be generalized to new programs: compared to solving new instances from scratch, the pre-trained agent is more sample efficient in finding solutions.	[Si, Xujie; Raghothaman, Mukund; Naik, Mayur] Univ Penn, Philadelphia, PA 19104 USA; [Dai, Hanjun; Song, Le] Georgia Tech, Atlanta, GA USA; [Song, Le] Ant Financial, Hangzhou, Zhejiang, Peoples R China	University of Pennsylvania; University System of Georgia; Georgia Institute of Technology	Si, XJ (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.	xsi@cis.upenn.edu; hanjundai@gatech.edu; rmukund@cis.upenn.edu; mhnaik@cis.upenn.edu; lsong@cc.gatech.edu	Dai, Hanjun/AAQ-8943-2021		DARPA [FA8750-15-2-0009]; NSF [CCF-1526270, IIS-1350983, IIS-1639792, CNS-1704701]; ONR [N00014-15-1-2340]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	We thank the anonymous reviewers for insightful comments. We thank Ningning Xie for useful feedback. This research was supported in part by DARPA FA8750-15-2-0009, NSF (CCF-1526270, IIS-1350983, IIS-1639792, CNS-1704701) and ONR N00014-15-1-2340.	Albarghouthi Aws, 2013, P INT C COMP AID VER; Allamanis Miltiadis, 2018, P INT C LEARN REPR I; Alur Rajeev, 2013, P FORM METH COMP AID; Alur Rajeev, 2017, SCALING ENUMERATIVE; Appel Andrew W., 2011, P 20 EUR S PROGR ESO; Balog Matej, 2017, P INT C LEARN REPR I; Bello Irwan, 2016, CORR; Bielik Pavol, 2016, P INT C MACH LEARN I; Bounov Dimitar, 2018, P 2018 CHI C HUM FAC; Brockschmidt Marc, 2017, P STAT AN S SAS; Bunel R., 2018, INT C LEARN REPR; Chen Liang, 2016, ARXIV161100020; Chen X., 2018, P 32 C NEUR INF PROC; Colon Michael A., 2003, P INT C COMP AID VER; Cook Byron, 2011, COMMUNICATIONS ACM, V54; Cytron Ron, 1991, ACM T PROGRAM LANG S, V13; Dai H., 2018, P INT C LEARN REPR I; Dai Hanjun, 2016, P INT C MACH LEARN I; DEMOURA L, 2008, P 14 INT C TOOLS ALG; Devlin Jacob, 2017, P INT C MACH LEARN I; Dillig Isil, 2013, P ACM C OBJ OR PROGR; Duvenaud D., 2015, P C NEUR INF PROC SY; Fahndrich Manuel, 2010, P 2010 INT C FORM VE; Garg Pranav, 2016, P ACM S PRINC PROGR; Garg Pranav, 2014, P INT C COMP AID VER; Gulwani Sumit, 2011, P ACM C PROGR LANG D; Gulwani Sumit, 2007, P ACM S PRINC PROGR; Khalil Elias B., 2017, P C NEUR INF PROC SY; Khalil Elias Boutros, 2016, LEARNING BRANCH MIXE; Kusner Matt J, 2017, P INT C MACH LEARN I; Maddison C. J., 2014, P INT C MACH LEARN I; Manna Zohar, 1971, COMMUNICATIONS ACM; Miller Alexander, 2016, P C EMP METH NAT LAN; Murali Vijayaraghavan, 2018, P INT C LEARN REPR I; Nguyen Anh Tuan, 2015, P INT C SOFTW ENG IC; Padhi Saswat, 2016, P ACM C PROGR LANG D; Parisotto E., 2016, P INT C LEARN REPR I; Parisotto Emilio, 2016, ARXIV161101855; REYNOLDS J, 2002, P IEEE S LOG COMP SC; Sankaranarayanan Sriram, 2004, P ACM S PRINC PROGR; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Schkufza Eric, 2013, STOCHASTIC SUPEROPTI; Selsam D., 2018, ARXIV180203685; Sharma Rahul, 2014, P INT C COMP AID VER; Sharma Rahul, 2013, P EUR S PROGR ESOP; Sharma Rahul, 2011, P INT C COMP AID VER; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Sukhbaatar Sainbayar, 2015, P C NEUR INF PROC SY; Tai Kai Sheng, 2015, P ASS COMP LING ACL; [No title captured]	50	15	17	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002031
C	van Merrienboer, B; Breuleux, O; Bergeron, A; Lamblin, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		van Merrienboer, Bart; Breuleux, Olivier; Bergeron, Arnaud; Lamblin, Pascal			Automatic differentiation in ML: Where we are and where we should be going	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHMS	We review the current state of automatic differentiation (AD) for array programming in machine learning (ML), including the different approaches such as operator overloading (OO) and source transformation (ST) used for AD, graph-based intermediate representations for programs, and source languages. Based on these insights, we introduce a new graph-based intermediate representation (IR) which specifically aims to efficiently support fully-general AD for array programming. Unlike existing dataflow programming representations in ML frameworks, our IR naturally supports function calls, higher-order functions and recursion, making ML models easier to implement. The ability to represent closures allows us to perform AD using ST without a tape, making the resulting derivative (adjoint) program amenable to ahead-of-time optimization using tools from functional language compilers, and enabling higher-order derivatives. Lastly, we introduce a proof of concept compiler toolchain called Myia which uses a subset of Python as a front end.	[van Merrienboer, Bart; Lamblin, Pascal] Google Brain, Mila, Montreal, PQ, Canada; [Breuleux, Olivier; Bergeron, Arnaud] Mila, Montreal, PQ, Canada		van Merrienboer, B (corresponding author), Google Brain, Mila, Montreal, PQ, Canada.	bartvm@google.com; breuleuo@iro.umontreal.ca; bergearn@iro.umontreal.ca; lamblinp@google.com						Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Akidau T, 2015, PROC VLDB ENDOW, V8, P1792; Barendsen E., 1993, Foundations of Software Technology and Theoretical Computer Science. 13th Conference Proceedings, P41; Baydin AG, 2018, J MACH LEARN RES, V18; Baydin Atilim Gunes, 2016, ABS161103423 ARXIV; Behnel S, 2011, COMPUT SCI ENG, V13, P31, DOI 10.1109/MCSE.2010.118; Bell Bradley M, 2012, COMPUTATIONAL INFRAS, V57; Bischof C, 1996, IEEE COMPUT SCI ENG, V3, P18, DOI 10.1109/99.537089; Bischof Christian H, 2000, TECHNICAL REPORT; Chen  Tianqi, 2018, ABS180204799 ARXIV; Chen  Tianqi, 2015, ABS151201274 ARXIV; CLICK C, 1995, SIGPLAN NOTICES, V30, P35, DOI 10.1145/202530.202534; Clifford W.K., 1873, P LOND MATH SOC, V4, P381, DOI [10.1112/plms/s1-4.1.381, DOI 10.1112/PLMS/S1-4.1.381]; de Vries E, 2008, LECT NOTES COMPUT SC, V5083, P201, DOI 10.1007/978-3-540-85373-2_12; FLANAGAN C, 1993, SIGPLAN NOTICES, V28, P237, DOI 10.1145/173262.155113; Griewank A, 2008, OTHER TITL APPL MATH, V105, P1, DOI 10.1137/1.9780898717761; Griewank A, 1996, ACM T MATH SOFTWARE, V22, P131, DOI 10.1145/229473.229474; Hascoet L, 2013, ACM T MATH SOFTWARE, V39, DOI 10.1145/2450153.2450158; Hascoet  Laurent, 2017, NIPS AUT WORKSH; Hascoet  Laurent, 2003, RR4856 INRIA; Johnston WM, 2004, ACM COMPUT SURV, V36, P1, DOI 10.1145/1013208.1013209; Kai Sheng Tai, 2015, ARXIV150300075; Lam S.K., 2015, P 2 WORKSHOP LLVM CO, P1, DOI [10.1145/2833157.2833162, DOI 10.1145/2833157.2833162]; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Leissa R, 2015, INT SYM CODE GENER, P202, DOI 10.1109/CGO.2015.7054200; Lindenmaier  Gotz, 2005, TECH REP, V8; Maclaurin D., 2015, ICML 2015 AUTOML WOR; Merrienboer Bart van, 2017, ABS171102712 ARXIV; Naumann U, 2008, MATH PROGRAM, V112, P427, DOI 10.1007/s10107-006-0042-z; Paszke Adam, 2017, AUTOMATIC DIFFERENTI; Pearlmutter BA, 2008, ACM T PROGR LANG SYS, V30, DOI 10.1145/1330017.1330018; Revels  Jarrett, 2016, ABS160707892 ARXIV; Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003; Shivers Olin, 1991, THESIS; Siskind Jeffrey Mark, 2016, ABS161103416 ARXIV; Siskind Jeffrey Mark, 2008, TRECE0801 PURD U; Theano Development Team, 2016, ABS160502688 ARXIV T; Tokui S, 2015, P WORKSH MACH LEARN, V5; van der Walt S, 2011, COMPUT SCI ENG, V13, P22, DOI 10.1109/MCSE.2011.37; Wang F., 2018, ICLR WORKSH TRACK; Wang M, 2016, MATH PROGRAM COMPUT, V8, P393, DOI 10.1007/s12532-016-0100-3; Wei  Richard, 2017, URBANA, V51, P61801	42	15	15	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003032
C	Wu, H; Mardt, A; Pasquali, L; Noe, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wu, Hao; Mardt, Andreas; Pasquali, Luca; Noe, Frank			Deep Generative Markov State Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MOLECULAR-DYNAMICS SIMULATIONS; KINETICS	We propose a deep generative Markov State Model (DeepGenMSM) learning framework for inference of metastable dynamical systems and prediction of trajectories. After unsupervised training on time series data, the model contains (i) a probabilistic encoder that maps from high-dimensional configuration space to a small-sized vector indicating the membership to metastable (long-lived) states, (ii) a Markov chain that governs the transitions between metastable states and facilitates analysis of the long-time dynamics, and (iii) a generative part that samples the conditional distribution of configurations in the next time step. The model can be operated in a recursive fashion to generate trajectories to predict the system evolution from a defined starting state and propose new configurations. The DeepGenMSM is demonstrated to provide accurate estimates of the long-time kinetics and generate valid distributions for molecular dynamics (MD) benchmark systems. Remarkably, we show that DeepGenMSMs are able to make long timesteps in molecular configuration space and generate physically realistic structures in regions that were not seen in training data.	[Wu, Hao; Mardt, Andreas; Pasquali, Luca; Noe, Frank] Free Univ Berlin, Dept Math & Comp Sci, D-14195 Berlin, Germany; [Wu, Hao] Tongji Univ, Sch Math Sci, Shanghai 200092, Peoples R China	Free University of Berlin; Tongji University	Noe, F (corresponding author), Free Univ Berlin, Dept Math & Comp Sci, D-14195 Berlin, Germany.	frank.noe@fu-berlin.de	Noe, Frank/Y-2766-2019		European Research Commission (ERC CoG "ScaleCell"); Deutsche Forschungsgemeinschaft [CRC 1114/A04, Transregio 186/A12, 825/4-1]; 1000-Talent Program of Young Scientists in China	European Research Commission (ERC CoG "ScaleCell"); Deutsche Forschungsgemeinschaft(German Research Foundation (DFG)); 1000-Talent Program of Young Scientists in China	This work was funded by the European Research Commission (ERC CoG "ScaleCell"), Deutsche Forschungsgemeinschaft (CRC 1114/A04, Transregio 186/A12, NO 825/4-1, Dynlon P8), and the "1000-Talent Program of Young Scientists in China".	Abadi M, 2015, P 12 USENIX S OPERAT; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bowman G. R., 2014, ADV EXPT MED BIOL, V797; Bowman GR, 2010, J CHEM THEORY COMPUT, V6, P787, DOI 10.1021/ct900620b; Brunton SL, 2016, P NATL ACAD SCI USA, V113, P3932, DOI 10.1073/pnas.1517384113; Djork-Arn, ICLR 2016; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hernandez C. X., 2017, ARXIV171108576; Kingma D.P, P 3 INT C LEARNING R; Korda M, 2018, J NONLINEAR SCI, V28, P687, DOI 10.1007/s00332-017-9423-0; Kube S, 2007, J CHEM PHYS, V126, DOI 10.1063/1.2404953; Li QX, 2017, CHAOS, V27, DOI 10.1063/1.4993854; Lusch B., 2017, ARXIV171209707; Mardt A, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02388-1; Mezic I, 2005, NONLINEAR DYNAM, V41, P309, DOI 10.1007/s11071-005-2824-x; Noe F, 2015, J CHEM THEORY COMPUT, V11, P5002, DOI 10.1021/acs.jctc.5b00553; Noe F, 2013, MULTISCALE MODEL SIM, V11, P635, DOI 10.1137/110858616; Otto S.E., 2017, ARXIV171201378; Paszke A., 2017, AUTOMATIC DIFFERENTI; Plattner N, 2017, NAT CHEM, V9, P1005, DOI [10.1038/NCHEM.2785, 10.1038/nchem.2785]; Prinz JH, 2011, J CHEM PHYS, V134, DOI 10.1063/1.3565032; Ribeiro JML, 2018, J CHEM PHYS, V149, DOI 10.1063/1.5025487; Sarich M., 2013, COURANT LECT NOTES; Sarich M, 2010, MULTISCALE MODEL SIM, V8, P1154, DOI 10.1137/090764049; Schmid P.J., 2008, 61 ANN M APS DIV FLU, P208; Swope WC, 2004, J PHYS CHEM B, V108, P6582, DOI 10.1021/jp037422q; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szekely G.J., 2004, INTERSTAT, V5, P1249; Tu J.H., 2014, J COMPUT DYN, V1, P391, DOI [10.3934/jcd.2014.1.391, DOI 10.3934/JCD.2014.1.391]; Wehmeyer C., 2017, ARXIV171011239; Wu H., 2017, ARXIV170704659	31	15	15	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304002
C	Yuan, JJ; Lamperski, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yuan, Jianjun; Lamperski, Andrew			Online convex optimization for cumulative constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				REGRET	We propose the algorithms for online convex optimization which lead to cumulative squared constraint violations of the form Sigma(T)(t=1) [g(x(t))](+))(2) = O (T1-beta), where beta is an element of (0, 1). Previous literature has focused on long-term constraints of the form Sigma(T)(t=1) g(x(t)). There, strictly feasible solutions can cancel out the effects of violated constraints. In contrast, the new form heavily penalizes large constraint violations and cancellation effects cannot occur. Furthermore, useful bounds on the single step constraint violation [g(x(t))](+) are derived. For convex objectives, our regret bounds generalize existing bounds, and for strongly convex objectives we give improved regret bounds. In numerical experiments, we show that our algorithm closely follows the constraint boundary leading to low cumulative violation.	[Yuan, Jianjun; Lamperski, Andrew] Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities	Yuan, JJ (corresponding author), Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA.	yuanx270@umn.edu; alampers@umn.edu						[Anonymous], 2018, ARXIV180105039; Blum A, 2004, THEOR COMPUT SCI, V324, P137, DOI 10.1016/j.tcs.2004.05.012; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Crammer K, 2006, J MACH LEARN RES, V7, P551; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Diamond S, 2016, J MACH LEARN RES, V17; Duchi J., 2008, PROC 25 INT C MACH L, P272; Duchi J. C., 2010, COLT, P14; Jenatton R, 2016, PR MACH LEARN RES, V48; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Y., 2018, ARXIV180107780; Mahdavi M, 2012, J MACH LEARN RES, V13, P2503; MAIRAL J., 2009, P 26 ANN INT C MACH, P689, DOI [10.1145/1553374.1553463, DOI 10.1145/1553374.1553463]; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Senthil K, 2010, INT J COMPUTER APPL; Yu H., 2017, ADV NEURAL INFORM PR, P1428; Yuan JJ, 2017, P AMER CONTR CONF, P4448, DOI 10.23919/ACC.2017.7963640; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	21	15	15	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000062
C	Zhang, JZ; Bareinboim, E		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Junzhe; Bareinboim, Elias			Equality of Opportunity in Classification: A Causal Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				RISK	The Equalized Odds (for short, EO) is one of the most popular measures of discrimination used in the supervised learning setting. It ascertains fairness through the balance of the misclassification rates (false positive and negative) across the protected groups - e.g., in the context of law enforcement, an African-American defendant who would not commit a future crime will have an equal opportunity of being released, compared to a non-recidivating Caucasian defendant. Despite this noble goal, it has been acknowledged in the literature that statistical tests based on the EO are oblivious to the underlying causal mechanisms that generated the disparity in the first place (Hardt et al. 2016). This leads to a critical disconnect between statistical measures readable from the data and the meaning of discrimination in the legal system, where compelling evidence that the observed disparity is tied to a specific causal process deemed unfair by society is required to characterize discrimination. The goal of this paper is to develop a principled approach to connect the statistical disparities characterized by the EO and the underlying, elusive, and frequently unobserved, causal mechanisms that generated such inequality. We start by introducing a new family of counterfactual measures that allows one to explain the misclassification disparities in terms of the underlying mechanisms in an arbitrary, non-parametric structural causal model. This will, in turn, allow legal and data analysts to interpret currently deployed classifiers through causal lens, linking the statistical disparities found in the data to the corresponding causal processes. Leveraging the new family of counterfactual measures, we develop a learning procedure to construct a classifier that is statistically efficient, interpretable, and compatible with the basic human intuition of fairness. We demonstrate our results through experiments in both real (COMPAS) and synthetic datasets.	[Zhang, Junzhe; Bareinboim, Elias] Purdue Univ, W Lafayette, IN 47907 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus	Zhang, JZ (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.	zhang745@purdue.edu; eb@purdue.edu	Zhang, Junzhe/GON-5782-2022	Zhang, Junzhe/0000-0002-1309-0151	IBM Research; Adobe Research; NSF [IIS-1704352, IIS-1750807]	IBM Research(International Business Machines (IBM)); Adobe Research; NSF(National Science Foundation (NSF))	This research is supported in parts by grants from IBM Research, Adobe Research, NSF IIS-1704352, and IIS-1750807 (CAREER).	Angwin J., 2016, PROPUBLICA, V23; Bareinboim E, 2016, P NATL ACAD SCI USA, V113, P7345, DOI 10.1073/pnas.1510507113; Boyd S, 2004, CONVEX OPTIMIZATION; Brennan T, 2009, CRIM JUSTICE BEHAV, V36, P21, DOI 10.1177/0093854808326545; Chouldechova A, 2017, BIG DATA, V5, P153, DOI 10.1089/big.2016.0047; Goh G, 2016, ADV NEUR IN, V29; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Khandani AE, 2010, J BANK FINANC, V34, P2767, DOI 10.1016/j.jbankfin.2010.06.001; Kilbertus Niki, 2017, ADV NEURAL INFORM PR, P656; Kusner Matt J, 2017, ADV NEURAL INFORM PR, V30, P4066, DOI DOI 10.5555/3294996.3295162; Lunceford JK, 2004, STAT MED, V23, P2937, DOI 10.1002/SIM.1903; Mahoney JF, 2007, Patent No. [US7287008 B1, 7287008]; Mancuhan K, 2014, ARTIF INTELL LAW, V22, P211, DOI 10.1007/s10506-014-9156-4; Nabi R., 2018, P 32 AAAI C ART INT; Pearl J., 2001, P 17 C ONUNCERTAINTY, P411; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Pearl J, 2016, J CAUSAL INFERENCE, V4, DOI 10.1515/jci-2016-0021; PUDIL P, 1994, PATTERN RECOGN LETT, V15, P1119, DOI 10.1016/0167-8655(94)90127-9; Shpitser Ilya, 2010, P 26 C UNC ART INT, p[527, 527]; Sweeney L., 2013, DISCRIMINATION ONLIN, V11, P10, DOI [DOI 10.1145/2460276.2460278, 10.1145/2460276.2460278]; van der Zander B., 2014, P 30 C UNC ART INT; Woodworth B., 2017, ARXIV PREPRINT ARXIV; Wright S, 1934, ANN MATH STAT, V5, P161, DOI 10.1214/aoms/1177732676; Zhang J., 2018, R37 AI LAB PURD U; Zhang J, 2018, 32 AAAI C ART INT; Zhang JZ, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P653; Zhang L, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3929	29	15	15	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303065
C	Zhong, ZS; Shen, TC; Yang, YB; Zhang, C; Lin, ZC		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhong, Zhisheng; Shen, Tiancheng; Yang, Yibo; Zhang, Chao; Lin, Zhouchen			Joint Sub-bands Learning with Clique Structures for Wavelet Domain Super-Resolution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NETWORK	Convolutional neural networks (CNNs) have recently achieved great success in single-image super-resolution (SISR). However, these methods tend to produce over-smoothed outputs and miss some textural details. To solve these problems, we propose the Super-Resolution CliqueNet (SRCliqueNet) to reconstruct the high resolution (HR) image with better textural details in the wavelet domain. The proposed SRCliqueNet firstly extracts a set of feature maps from the low resolution (LR) image by the clique blocks group. Then we send the set of feature maps to the clique up-sampling module to reconstruct the HR image. The clique upsampling module consists of four sub-nets which predict the high resolution wavelet coefficients of four sub-bands. Since we consider the edge feature properties of four sub-bands, the four sub-nets are connected to the others so that they can learn the coefficients of four sub-bands jointly. Finally we apply inverse discrete wavelet transform (IDWT) to the output of four sub-nets at the end of the clique up-sampling module to increase the resolution and reconstruct the HR image. Extensive quantitative and qualitative experiments on benchmark datasets show that our method achieves superior performance over the state-of-the-art methods.	[Zhong, Zhisheng; Shen, Tiancheng; Yang, Yibo; Zhang, Chao; Lin, Zhouchen] Peking Univ, Sch EECS, Key Lab Machine Percept MOE, Beijing, Peoples R China; [Shen, Tiancheng; Yang, Yibo] Peking Univ, Acad Adv Interdisciplinary Studies, Beijing, Peoples R China; [Lin, Zhouchen] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai, Peoples R China	Peking University; Peking University; Shanghai Jiao Tong University	Zhang, C (corresponding author), Peking Univ, Sch EECS, Key Lab Machine Percept MOE, Beijing, Peoples R China.	zszhong@pku.edu.cn; tianchengshen@pku.edu.cn; ibo@pku.edu.cn; c.zhang@pku.edu.cn; zlin@pku.edu.cn	Yang, Yibo/AAW-4088-2021	Yang, Yibo/0000-0003-0530-7231	National Basic Research Program of China (973 Program) [2015CB352502, 2015CB352303]; National Natural Science Foundation (NSF) of China [61625301, 61731018, 61671027]; Qualcomm; Microsoft Research Asia	National Basic Research Program of China (973 Program)(National Basic Research Program of China); National Natural Science Foundation (NSF) of China(National Natural Science Foundation of China (NSFC)); Qualcomm; Microsoft Research Asia(Microsoft)	This research is partially supported by National Basic Research Program of China (973 Program) (grant nos. 2015CB352502 and 2015CB352303), National Natural Science Foundation (NSF) of China (grant nos. 61625301, 61731018 and 61671027), Qualcomm and Microsoft Research Asia.	Adelson E.H., 1984, RCA ENG, V29, P33; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Bee Lim, 2017, CVPR WORKSH; Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135; Chan RH, 2003, SIAM J SCI COMPUT, V24, P1408, DOI 10.1137/S1064827500383123; Denton Emily L, 2015, NEURIPS, V2, P4; Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187; Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Hui Zheng, 2018, CVPR; Ji H, 2009, IEEE T PATTERN ANAL, V31, P649, DOI 10.1109/TPAMI.2008.103; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar N, 2017, PATTERN RECOGN LETT, V90, P65, DOI 10.1016/j.patrec.2017.03.014; Lai Wei-Sheng, 2017, PROC CVPR IEEE, P624, DOI DOI 10.1109/CVPR.2017.618; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Mallat S, 1996, P IEEE, V84, P604, DOI 10.1109/5.488702; Robinson MD, 2010, IEEE T IMAGE PROCESS, V19, P2669, DOI 10.1109/TIP.2010.2050107; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Stankovic RS, 2003, COMPUT ELECTR ENG, V29, P25, DOI 10.1016/S0045-7906(01)00011-8; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486; Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298; Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149; Timofte R, 2016, PROC CVPR IEEE, P1865, DOI 10.1109/CVPR.2016.206; Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang ZF, 2016, INT C NETWB INFO, P370, DOI 10.1109/NBiS.2016.7; Yibo Yang, 2018, CVPR; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474; Zeyde Roman, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47; Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344	41	15	16	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300016
C	Devraj, AM; Meyn, SP		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Devraj, Adithya M.; Meyn, Sean P.			Zap Q-Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				STOCHASTIC-APPROXIMATION; CONVERGENCE-RATES	The Zap Q-learning algorithm introduced in this paper is an improvement of Watkins' original algorithm and recent competitors in several respects. It is a matrix-gain algorithm designed so that its asymptotic variance is optimal Moreover, an ODE analysis suggests that the transient behavior is a close match to a deterministic Newton-Raphson implementation. This is made possible by a two time-scale update equation for the matrix gain sequence. The analysis suggests that the approach will lead to stable and efficient computation even for non-ideal parameterized settings. Numerical experiments confirm the quick convergence, even in such non-ideal cases.	[Devraj, Adithya M.; Meyn, Sean P.] Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32608 USA	State University System of Florida; University of Florida	Devraj, AM (corresponding author), Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32608 USA.	adithyamdevraj@ufl.edu; meyn@ece.ufl.edu	Jeong, Yongwook/N-7413-2016		National Science Foundation [EPCN-1609131, CPS-1259040]	National Science Foundation(National Science Foundation (NSF))	This research was supported by the National Science Foundation under grants EPCN-1609131 and CPS-1259040.	Barman K, 2008, SYST CONTROL LETT, V57, P784, DOI 10.1016/j.sysconle.2008.03.003; Benveniste A., 1990, APPL MATH, V22; Boyan JA, 2002, MACH LEARN, V49, P233, DOI 10.1023/A:1017936530646; Choi D, 2006, DISCRETE EVENT DYN S, V16, P207, DOI 10.1007/s10626-006-8134-8; Devraj A.M., 2017, ARXIV E PRINTS; Givchi A., 2015, AS C MACH LEARN, P159; Glynn PW, 2002, STAT PROBABIL LETT, V56, P143, DOI 10.1016/S0167-7152(01)00158-4; Konda VR, 2004, ANN APPL PROBAB, V14, P796, DOI 10.1214/105051604000000116; Kushner H.J., 1997, APPL MATH, V35; Lund RB, 1996, ANN APPL PROBAB, V6, P218; MA DJ, 1990, STOCH PROC APPL, V35, P27, DOI 10.1016/0304-4149(90)90120-H; Mehta P, 2009, IEEE DECIS CONTR P, P3598, DOI 10.1109/CDC.2009.5399753; Meyn SP, 1994, ANN APPL PROBAB, V4, P981, DOI 10.1214/aoap/1177004900; Moulines E., 1981, ADV NEURAL INFORM PR, V24, p[451, 1133]; Pan YC, 2017, AAAI CONF ARTIF INTE, P2464; Polyak B.T., 1990, AVTOMATIKA TELEMEKHA, V7, P98; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; RUPPERT D, 1985, ANN STAT, V13, P236, DOI 10.1214/aos/1176346589; Ruppert David, 1988, EFFICIENT ESTIMATION; Szepesvari C., 1997, NEURAL INFORM PROCES, V10, P1064; Tsitsiklis JN, 1999, IEEE T AUTOMAT CONTR, V44, P1840, DOI 10.1109/9.793723; TSITSIKLIS JN, 1994, MACH LEARN, V16, P185, DOI 10.1023/A:1022689125041; Watkins CJCH., 1989, THESIS; YAO H. S., 2008, P 25 INT C MACH LEAR, P1208; Yao HS, 2009, IEEE DECIS CONTR P, P1181, DOI 10.1109/CDC.2009.5400370; Yu HZ, 2013, ANN OPER RES, V208, P95, DOI 10.1007/s10479-012-1128-z	31	15	15	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402028
C	Soltanolkotabi, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Soltanolkotabi, Mandi			Learning ReLUs via Gradient Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this paper we study the problem of learning Rectified Linear Units (ReLUs) which are functions of the form x bar right arrow max(0,< w, x >) with W is an element of R-d denoting the weight vector. We study this problem in the high-dimensional regime where the number of observations are fewer than the dimension of the weight vector. We assume that the weight vector belongs to some closed set (convex or nonconvex) which captures known side-information about its structure. We focus on the realizable model where the inputs are chosen i.i.d. from a Gaussian distribution and the labels are generated according to a planted weight vector. We show that projected gradient descent, when initialized at 0, converges at a linear rate to the planted model with a number of samples that is optimal up to numerical constants. Our results on the dynamics of convergence of these very shallow neural nets may provide some insights towards understanding the dynamics of deeper architectures.	[Soltanolkotabi, Mandi] Univ Southern Calif, Ming Hsieh Dept Elect Engn, Los Angeles, CA 90007 USA	University of Southern California	Soltanolkotabi, M (corresponding author), Univ Southern Calif, Ming Hsieh Dept Elect Engn, Los Angeles, CA 90007 USA.	soltanol@usc.edu	Jeong, Yongwook/N-7413-2016					Amelunxen D., 2014, INFORM INFERENCE; [Anonymous], 2016, ARXIV161007108; Brutzkus A., 2017, INT C MACH LEARN ICM; Collobert R., 2008, AUNIFIED ARCHITECTUR, P160; Ganti R., 2015, ARXIV150608910; GOEL S., 2016, ARXIV161110258; Gordon Y., 1988, MILMANS INEQUALITY R; Haeffele B. D., 2015, ARXIV PREPRINT ARXIV; Horowitz JL, 1996, J AM STAT ASSOC, V91, P1632, DOI 10.2307/2291590; ICHIMURA H, 1993, J ECONOMETRICS, V58, P71, DOI 10.1016/0304-4076(93)90114-K; Kakade S., 2011, ADV NEURAL INFORM PR; KALAI A. T., 2009, COLT; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LI Y., 2017, ARXIV170509886; Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382; Nguyen Q., 2017, ARXIV170408045; Oymak S., 2015, ARXIV150704793; Poston T., 1991, IJCNN-91-Seattle: International Joint Conference on Neural Networks (Cat. No.91CH3049-4), P173, DOI 10.1109/IJCNN.1991.155333; Soltanolkotabi M., 2017, THEORETICAL INSIGHTS, V07; Soltanolkotabi M, 2017, ARXIV170206175; Soltanolkotabi Mahdi, 2017, ARXIV170504591; Tian Y., 2017, INT C MACH LEARN ICM; Zhong  Kai, 2017, ARXIV170603175	25	15	15	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402006
C	Song, JM; Zhao, SJ; Ermon, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Song, Jiaming; Zhao, Shengjia; Ermon, Stefano			A-NICE-MC: Adversarial Training for MCMC	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MARKOV-CHAIN	Existing Markov Chain Monte Carlo (MCMC) methods are either based on general-purpose and domain-agnostic schemes, which can lead to slow convergence, or problem-specific proposals hand-crafted by an expert. In this paper, we propose A-NICE-MC, a novel method to automatically design efficient Markov chain kernels tailored for a specific domain. First, we propose an efficient likelihood-free adversarial training method to train a Markov chain and mimic a given data distribution. Then, we leverage flexible volume preserving flows to obtain parametric kernels for MCMC. Using a bootstrap approach, we show how to train efficient Markov chains to sample from a prescribed posterior distribution by iteratively improving the quality of both the model and the samples. Empirical results demonstrate that A-NICE-MC combines the strong guarantees of MCMC with the expressiveness of deep neural networks, and is able to significantly outperform competing methods such as Hamiltonian Monte Carlo.	[Song, Jiaming; Zhao, Shengjia; Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Song, JM (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	tsong@cs.stanford.edu; zhaosj12@cs.stanford.edu; ermon@cs.stanford.edu	Jeong, Yongwook/N-7413-2016		Intel Corporation; NSF [1651565, 1522054, 1733686]; TRI; FLI	Intel Corporation(Intel Corporation); NSF(National Science Foundation (NSF)); TRI; FLI	This research was funded by Intel Corporation, TRI, FLI and NSF grants 1651565, 1522054, 1733686. The authors would like to thank Daniel Levy for discussions on the NICE proposal proof, Yingzhen Li for suggestions on the training procedure and Aditya Grover for suggestions on the implementation.	Abadi M, 2015, P 12 USENIX S OPERAT; Arjovsky M., 2017, ARXIV170107875; Bengio Y., 2014, DEEP GENERATIVE STOC; Bordes F., 2017, ICLR; Boyd S, 2004, SIAM REV, V46, P667, DOI 10.1137/s0036144503423264; Brooks SP, 1998, J COMPUT GRAPH STAT, V7, P434, DOI 10.2307/1390675; Dinh Laurent, 2014, ARXIV14108516; Efron B., 1994, INTRO BOOTSTRAP; Ermon S, 2014, AAAI CONF ARTIF INTE, P849; Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gorham J., 2016, ARXIV161106972; Gorham J, 2017, PR MACH LEARN RES, V70; Gorham J, 2015, ADV NEUR IN, V28; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; Grover A., 2017, ARXIV170508868; Gulrajani I, 2017, P NIPS 2017; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; JAKOB W, 2012, ACM T GRAPHIC, V31; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Landau D. P., 2014, GUIDE MONTE CARLO SI; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; MAHENDRAN N, 2012, J MACHINE LEARNING R, V22, P751; Mohamed Shakir, 2016, ARXIV161003483; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ranganath R, 2014, JMLR WORKSH CONF PRO, V33, P814; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Russell S., 2001, P 17 C UNC ART INT, P120; Salimans T, 2016, ADV NEUR IN, V29; Salimans T, 2015, PR MACH LEARN RES, V37, P1218; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Zhu Jun-Yan, 2017, ICCV	39	15	15	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405022
C	Wang, Y; Solus, L; Yang, KD; Uhler, C		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wang, Yuhao; Solus, Liam; Yang, Karren Dai; Uhler, Caroline			Permutation-based Causal Inference Algorithms with Interventions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MARKOV EQUIVALENCE CLASSES; NETWORKS	Learning directed acyclic graphs using both observational and interventional data is now a fundamentally important problem due to recent technological developments in genomics that generate such single-cell gene expression data at a very large scale. In order to utilize this data for learning gene regulatory networks, efficient and reliable causal inference algorithms are needed that can make use of both observational and interventional data. In this paper, we present two algorithms of this type and prove that both are consistent under the faithfulness assumption. These algorithms are interventional adaptations of the Greedy SP algorithm and are the first algorithms using both observational and interventional data with consistency guarantees. Moreover, these algorithms have the advantage that they are nonparametric, which makes them useful also for analyzing non-Gaussian data. In this paper, we present these two algorithms and their consistency guarantees, and we analyze their performance on simulated data, protein signaling data, and single-cell gene expression data.	[Wang, Yuhao; Uhler, Caroline] MIT, Lab Informat & Decis Syst, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Wang, Yuhao; Yang, Karren Dai; Uhler, Caroline] MIT, Inst Data Syst & Soc, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Solus, Liam] KTH Royal Inst Technol, Dept Math, Stockholm, Sweden; [Yang, Karren Dai] MIT, Broad Inst MIT & Harvard, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); Royal Institute of Technology; Harvard University; Massachusetts Institute of Technology (MIT); Broad Institute	Wang, Y (corresponding author), MIT, Lab Informat & Decis Syst, 77 Massachusetts Ave, Cambridge, MA 02139 USA.; Wang, Y (corresponding author), MIT, Inst Data Syst & Soc, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	yuhaow@mit.edu; solus@kth.se; karren@mit.edu; cuhler@mit.edu	Jeong, Yongwook/N-7413-2016		DARPA [W911NF-16-1-0551]; ONR [N00014-17-1-2147]; NSF Mathematical Sciences Postdoctoral Research Fellowship [DMS - 1606407]; MIT Department of Biological Engineering; NSF [1651995]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); ONR(Office of Naval Research); NSF Mathematical Sciences Postdoctoral Research Fellowship(National Science Foundation (NSF)); MIT Department of Biological Engineering; NSF(National Science Foundation (NSF))	Yuhao Wang was supported by DARPA (W911NF-16-1-0551) and ONR (N00014-17-1-2147). Liam Solus was supported by an NSF Mathematical Sciences Postdoctoral Research Fellowship (DMS - 1606407). Karren Yang was supported by the MIT Department of Biological Engineering. Caroline Uhler was partially supported by DARPA (W911NF-16-1-0551), NSF (1651995) and ONR (N00014-17-1-2147). We thank Dr. Sofia Triantafillou from the University of Crete for helping us run COmbINE.	Andersson SA, 1997, ANN STAT, V25, P505; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; Chickering D. M., 1995, P 11 C UNC ART INT; Dixit A, 2016, CELL, V167, P1853, DOI 10.1016/j.cell.2016.11.038; Friedman N, 2000, J COMPUT BIOL, V7, P601, DOI 10.1089/106652700750050961; Garber M, 2012, MOL CELL, V47, P810, DOI 10.1016/j.molcel.2012.07.030; Hauser A, 2015, J R STAT SOC B, V77, P291, DOI 10.1111/rssb.12071; Hauser A, 2012, J MACH LEARN RES, V13, P2409; Hyttinen A., 2014, CONSTRAINT BASED CAU; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Macosko EZ, 2015, CELL, V161, P1202, DOI 10.1016/j.cell.2015.05.002; Magliacane S., 2016, ADV NEURAL INFORM PR; Meek C., 1997, PHD THESIS; Meinshausen N, 2016, P NATL ACAD SCI USA, V113, P7361, DOI 10.1073/pnas.1510493113; Nandy P., 2015, ARXIV150702608; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Rau A, 2013, BMC SYST BIOL, V7, DOI 10.1186/1752-0509-7-111; Robins JM, 2000, EPIDEMIOLOGY, V11, P550, DOI 10.1097/00001648-200009000-00011; Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809; Solus Liam, 2017, ARXIV170203530; Tillman R. E., 2009, ADV NEURAL INFORM PR; Triantafillou S, 2015, J MACH LEARN RES, V16, P2147; Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7	26	15	15	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405087
C	Battaglia, PW; Pascanu, R; Lai, M; Rezende, D; Kavukcuoglu, K		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Battaglia, Peter W.; Pascanu, Razvan; Lai, Matthew; Rezende, Danilo; Kavukcuoglu, Koray			Interaction Networks for Learning about Objects, Relations and Physics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.	[Battaglia, Peter W.; Pascanu, Razvan; Lai, Matthew; Rezende, Danilo; Kavukcuoglu, Koray] Google DeepMind, London N1C 4AG, England	Google Incorporated	Battaglia, PW (corresponding author), Google DeepMind, London N1C 4AG, England.	peterbattaglia@google.com; razp@google.com; matthewlai@google.com; danilor@google.com; korayk@google.com						Andreas Jacob, 2016, ARXIV160101705, P1545, DOI [DOI 10.18653/V1/N16-1181, 10.18653/v1/N16-1181]; Baraff D, 2001, SIGGRAPH COURSE NOTE, V2, P2; Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110; Craik Kenneth James Williams, 1943, NATURE EXPLANATION, P1; Fragkiadaki K., 2016, ICLR; GARDIN F, 1989, ARTIF INTELL, V38, P139, DOI 10.1016/0004-3702(89)90055-6; Ghahramani Z, 2015, NATURE, V521, P452, DOI 10.1038/nature14541; Grzeszczuk R., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P9, DOI 10.1145/280814.280816; Hayes PJ, 1978, NAIVE PHYS MANIFESTO; Hegarty M, 2004, TRENDS COGN SCI, V8, P280, DOI 10.1016/j.tics.2004.04.001; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Johnson Laird P.N., 1983, MENTAL MODELS COGNIT; Kingma D.P, P 3 INT C LEARNING R; Ladicky L, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818129; Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lerer A, 2016, PR MACH LEARN RES, V48; Li Wenbin, 2016, ARXIV160400066; Mottaghi R., 2016, ARXIV160305600; Mottaghi R., 2015, ARXIV151104048; Reed Scott, 2016, ICLR; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003; Socher R., 2011, ADV NEURAL INF PROCE, V24, P1; SPELKE ES, 1992, PSYCHOL REV, V99, P605, DOI 10.1037/0033-295X.99.4.605; Sutskever I, 2009, ADV NEURAL INFORM PR, P1593; Tenenbaum JB, 2011, SCIENCE, V331, P1279, DOI 10.1126/science.1192788; Winston P, 1975, PSYCHOL COMPUTER VIS, V73; Wu J., 2015, ADV NEURAL INF PROCE, V28, P1, DOI DOI 10.1007/978-3-319-26532-2_15	29	15	15	4	18	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703054
C	Bluche, T		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bluche, Theodore			Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Offline handwriting recognition systems require cropped text line images for both training and recognition. On the one hand, the annotation of position and transcript at line level is costly to obtain. On the other hand, automatic line segmentation algorithms are prone to errors, compromising the subsequent recognition. In this paper, we propose a modification of the popular and efficient Multi-Dimensional Long Short-Term Memory Recurrent Neural Networks (MDLSTM-RNNs) to enable end-to-end processing of handwritten paragraphs. More particularly, we replace the collapse layer transforming the two-dimensional representation into a sequence of predictions by a recurrent version which can select one line at a time. In the proposed model, a neural network performs a kind of implicit line segmentation by computing attention weights on the image representation. The experiments on paragraphs of Rimes and IAM databases yield results that are competitive with those of networks trained at line level, and constitute a significant step towards end-to-end transcription of full documents.	[Bluche, Theodore] A2iA SAS, 39 Rue Bienfaisance, F-75008 Paris, France		Bluche, T (corresponding author), A2iA SAS, 39 Rue Bienfaisance, F-75008 Paris, France.	tb@a2ia.com						Augustin E., 2006, P WORKSH FRONT HANDW; Ba J., 2014, ARXIV; Bahdanau Dzmitry, 2015, NEURAL MACHINE TRANS; BENGIO Y, 1995, NEURAL COMPUT, V7, P1289, DOI 10.1162/neco.1995.7.6.1289; Bluche T., 2014, INT C FRONT HANDWR R; Bluche T, 2016, 160403286 ARXIV; Bluche T., 2015, THESIS; Bosch V, 2012, INT CONF FRONT HAND, P201, DOI 10.1109/ICFHR.2012.274; Brunessaux S, 2014, 2014 11TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS (DAS 2014), P349, DOI 10.1109/DAS.2014.58; Chorowski I. K., 2015, ADV NEURAL INFORM PR, V28, P577, DOI DOI 10.1016/0167-739X(94)90007-8; Delakis M, 2008, VISAPP 2008: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 2, P290; Doetsch Patrick, 2014, FAST ROBUST TRAINING; FUKUSHIMA K, 1987, APPL OPTICS, V26, P4985, DOI 10.1364/AO.26.004985; Gatos B, 2014, 2014 11TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS (DAS 2014), P237, DOI 10.1109/DAS.2014.23; Graves A., 2008, ADV NEURAL INFORM PR, V21, P545; Graves A., 2006, P INT C MACH LEARN I; Graves Alex, 2013, ARXIV13080850 CORR; Gregor K., 2015, ARXIV150204623; Hebert D, 2011, PROC INT CONF DOC, P493, DOI 10.1109/ICDAR.2011.105; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Johnson J., 2015, ARXIV151107571; Jung K, 2001, PATTERN RECOGN LETT, V22, P1503, DOI 10.1016/S0167-8655(01)00096-4; Kaltenmeier A., 1993, Proceedings of the Second International Conference on Document Analysis and Recognition (Cat. No.93TH0578-5), P139, DOI 10.1109/ICDAR.1993.395764; Kozielski M, 2013, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2013.190; Lee Chen-Yu, 2016, ARXIV160303101; Likforman-Sulem L, 2007, INT J DOC ANAL RECOG, V9, P123, DOI 10.1007/s10032-006-0023-z; Marti U.-V., 2002, International Journal on Document Analysis and Recognition, V5, P39, DOI 10.1007/s100320200071; Messina R., 2014, 11 IAPR WORKSH DOC A, P212; Moysset Bastien, 2015, INT C DOC AN REC ICD; Moysset Bastien, 2015, INT WORKSH HIST DOC; Sanchez J. A., 2014, INT C FRONT HANDWR R; Tieleman Tijmen, 2012, LECT 65 RMSPROP DIVI, V4; Tong A., 2014, 11 IAPR WORKSH DOC A; Vinciarelli A, 2004, IEEE T PATTERN ANAL, V26, P709, DOI 10.1109/TPAMI.2004.14; Pham V, 2014, INT CONF FRONT HAND, P285, DOI 10.1109/ICFHR.2014.55; Xu K, 2015, ARXIV150203044; Zhang X, 2013, ARXIV PREPRINT ARXIV	38	15	15	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702004
C	Chen, XY; Liu, C; Shin, R; Song, D; Chen, MC		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chen, Xinyun; Liu, Chang; Shin, Richard; Song, Dawn; Chen, Mingcheng			Latent Attention For If-Then Program Synthesis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Automatic translation from natural language descriptions into programs is a long-standing challenging problem. In this work, we consider a simple yet important sub-problem: translation from textual descriptions to If-Then programs. We devise a novel neural network architecture for this task which we train end-toend. Specifically, we introduce Latent Attention, which computes multiplicative weights for the words in the description in a two-stage process with the goal of better leveraging the natural language structures that indicate the relevant parts for predicting program elements. Our architecture reduces the error rate by 28.57% compared to prior art [3]. We also propose a one-shot learning scenario of If-Then program synthesis and simulate it with our existing dataset. We demonstrate a variation on the training procedure for this scenario that outperforms the original procedure, significantly closing the gap to the model trained with all data.	[Chen, Xinyun] Shanghai Jiao Tong Univ, Shanghai, Peoples R China; [Liu, Chang; Shin, Richard; Song, Dawn] Univ Calif Berkeley, Berkeley, CA USA; [Chen, Mingcheng] UIUC, Champaign, IL USA	Shanghai Jiao Tong University; University of California System; University of California Berkeley; University of Illinois System; University of Illinois Urbana-Champaign	Chen, XY (corresponding author), Shanghai Jiao Tong Univ, Shanghai, Peoples R China.		Chen, Xinyun/ABZ-9877-2022		National Science Foundation [TWC-1409915]; DARPA grant [FA8750-15-2-0104]	National Science Foundation(National Science Foundation (NSF)); DARPA grant(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	We thank the anonymous reviewers for their valuable comments. This material is based upon work partially supported by the National Science Foundation under Grant No. TWC-1409915, and a DARPA grant FA8750-15-2-0104. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation and DARPA.	[Anonymous], 2014, SIGMOD; [Anonymous], MOBISYS; Artzi Yoav, 2015, EMNLP; Beltagy I., 2016, ACL; Berant Jonathan, 2013, P 2013 C EMP METH NA, P1533; Branavan S. R., 2009, ACL; Chung J., 2014, ARXIV14123555; DONG L, 2016, ACL, DOI DOI 10.18653/V1/P16-1004; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Jones Bevan, 2012, P ACL; Kate R. J., 2005, AAAI; Kingma D.P, P 3 INT C LEARNING R; Kushman N., 2013, NAACL; Lei T., 2013, ACL; Ling W., 2016, CORR; Quirk C., 2015, ACL; Sukhbaatar S, 2015, ADV NEUR IN, V28; Vinyals Oriol, 2015, NIPS; Wong Y. W., 2006, NAACL; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Zaremba Wojciech, 2014, ABS14092329 CORR; Zelle JM, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P1050	22	15	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702031
C	He, D; Xia, YC; Qin, T; Wang, LW; Yu, NH; Liu, TY; Ma, WY		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		He, Di; Xia, Yingce; Qin, Tao; Wang, Liwei; Yu, Nenghai; Liu, Tie-Yan; Ma, Wei-Ying			Dual Learning for Machine Translation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation dual-NMT. Experiments show that dual-NMT works very well on English <-> French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.	[He, Di; Wang, Liwei] Peking Univ, Key Lab Machine Percept MOE, Sch EECS, Beijing, Peoples R China; [Xia, Yingce; Yu, Nenghai] Univ Sci & Technol China, Hefei, Anhui, Peoples R China; [Qin, Tao; Liu, Tie-Yan; Ma, Wei-Ying] Microsoft Res, Redmond, WA USA; [Xia, Yingce] Microsoft Res Asia, Beijing, Peoples R China	Peking University; Chinese Academy of Sciences; University of Science & Technology of China, CAS; Microsoft; Microsoft; Microsoft Research Asia	He, D (corresponding author), Peking Univ, Key Lab Machine Percept MOE, Sch EECS, Beijing, Peoples R China.	dih@cis.pku.edu.cn; xiayingc@mail.ustc.edu.cn; taoqin@microsoft.com; wanglw@cis.pku.edu.cn; ynh@ustc.edu.cn; tie-yan.liu@microsoft.com; wyma@microsoft.com			National Basic Research Program of China (973 Program) [2015CB352502]; NSFC [61573026]; MOE-Microsoft Key Laboratory of Statistics and Machine Learning, Peking University	National Basic Research Program of China (973 Program)(National Basic Research Program of China); NSFC(National Natural Science Foundation of China (NSFC)); MOE-Microsoft Key Laboratory of Statistics and Machine Learning, Peking University	This work was partially supported by National Basic Research Program of China (973 Program) (grant no. 2015CB352502), NSFC (61573026) and the MOE-Microsoft Key Laboratory of Statistics and Machine Learning, Peking University. We would like to thank Yiren Wang, Fei Tian, Li Zhao and Wei Chen for helpful discussions, and the anonymous reviewers for their valuable comments on our paper.	[Anonymous], 2010, INTERSPEECH; Bahdanau Dzmitry, 2015, ICLR; Brants T., 2007, LARGE LANGUAGE MODEL; Cho K., 2014, P 2014 C EMP METH NA, P1724; Gulcehre Caglar, 2015, USING MONOLINGUAL CO; Jean S, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1; Koehn P., 2003, P 2003 C N AM CHAPT, P127, DOI DOI 10.3115/1073445.1073462; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Ranzato MarcAurelio, 2015, ARXIV; Rush Alexander M, 2015, P 2015 C EMP METH NA, P379, DOI DOI 10.18653/V1/D15-1044; Sennrich R., 2016, ACL; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Ueffing N., 2008, MACHINE TRANSLATION; Zeiler M.D, 2012, CORR ABS12125701	15	15	15	0	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703105
C	Ba, J; Grosse, R; Salakhutdinov, R; Frey, B		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Ba, Jimmy; Grosse, Roger; Salakhutdinov, Ruslan; Frey, Brendan			Learning Wake-Sleep Recurrent Attention Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Despite their success, convolutional neural networks are computationally expensive because they must examine all image locations. Stochastic attention-based models have been shown to improve computational efficiency at test time, but they remain difficult to train because of intractable posterior inference and high variance in the stochastic gradient estimates. Borrowing techniques from the literature on training deep generative models, we present the Wake-Sleep Recurrent Attention Model, a method for training stochastic attention networks which improves posterior inference and which reduces the variability in the stochastic gradients. We show that our method can greatly speed up the training time for stochastic attention networks in the domains of image classification and caption generation.	[Ba, Jimmy; Grosse, Roger; Salakhutdinov, Ruslan; Frey, Brendan] Univ Toronto, Toronto, ON, Canada	University of Toronto	Ba, J (corresponding author), Univ Toronto, Toronto, ON, Canada.	jimmy@psi.toronto.edu; rgrosse@cs.toronto.edu; rsalskhu@cs.toronto.edu; frey@psi.toronto.edu			Fields Institute; ONR [N00014-14-1-0232]; Samsung	Fields Institute; ONR(Office of Naval Research); Samsung(Samsung)	This work was supported by the Fields Institute, Samsung, ONR Grant N00014-14-1-0232 and the hardware donation of NVIDIA Corporation.	Ba J., 2015, ICLR 2015 C TRACK P; Bornschein Jorg, 2014, ARXIV14062751; Burda Yuri, 2015, ICLR; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; Denil M, 2012, NEURAL COMPUT, V24, P2151, DOI 10.1162/NECO_a_00312; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Graves A., 2014, ARXIV14105401; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Larochelle H., 2010, ADV NEURAL INFORM PR, P1243; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Mnih V., 2014, NEURAL INFORM PROCES, DOI DOI 10.48550/ARXIV.1406.6247; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Paisley J., 2012, ARXIV12066430; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Tang Y., 2014, ADV NEURAL INFORM PR, P1808; Tang Yichuan, 2013, ADV NEURAL INFORM PR, P530; Weaver Lex., 2001, P 17 C UNC ART INT U, P538; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Zaremba Wojciech, 2015, ARXIV150500521	28	15	15	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102010
C	Li, CX; Zhu, J; Shi, TL; Zhang, B		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Li, Chongxuan; Zhu, Jun; Shi, Tianlin; Zhang, Bo			Max-Margin Deep Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Deep generative models (DGMs) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability. However, little work has been done on examining or empowering the discriminative ability of DGMs on making accurate predictions. This paper presents max-margin deep generative models (mmDGMs), which explore the strongly discriminative principle of max-margin learning to improve the discriminative power of DGMs, while retaining the generative capability. We develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objective. Empirical results on MNIST and SVHN datasets demonstrate that (1) max-margin learning can significantly improve the prediction performance of DGMs and meanwhile retain the generative ability; and (2) mmDGMs are competitive to the state-of-the-art fully discriminative networks by employing deep convolutional neural networks (CNNs) as both recognition and generative models.	[Li, Chongxuan; Zhu, Jun; Zhang, Bo] Tsinghua Univ, Ctr Bioinspired Comp Res, State Key Lab Intell Tech & Sys, Dept Comp Sci & Tech,TNList Lab, Beijing 100084, Peoples R China; [Shi, Tianlin] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Tsinghua University; Stanford University	Li, CX (corresponding author), Tsinghua Univ, Ctr Bioinspired Comp Res, State Key Lab Intell Tech & Sys, Dept Comp Sci & Tech,TNList Lab, Beijing 100084, Peoples R China.	licx14@mails.tsinghua.edu.cn; dcszj@tsinghua.edu.cn; st1501@gmail.com; dcszb@tsinghua.edu.cn			National Basic Research Program (973 Program) of China [2013CB329403, 2012CB316301]; National NSF of China [61322308, 61332007]; Tsinghua TNList Lab Big Data Initiative; Tsinghua Initiative Scientific Research Program [20121088071, 20141080934]	National Basic Research Program (973 Program) of China(National Basic Research Program of China); National NSF of China(National Natural Science Foundation of China (NSFC)); Tsinghua TNList Lab Big Data Initiative; Tsinghua Initiative Scientific Research Program	The work was supported by the National Basic Research Program (973 Program) of China (Nos. 2013CB329403, 2012CB316301), National NSF of China (Nos. 61322308, 61332007), Tsinghua TNList Lab Big Data Initiative, and Tsinghua Initiative Scientific Research Program (Nos. 20121088071, 20141080934).	Altun Y, 2003, ICML; Ba J., 2017, P 3 INT C LEARN REPR; Bastien F., 2012, DEEP LEARN UNS FEAT; Bengio Y., 2014, ICML; Chen N, 2012, IEEE T PATTERN ANAL, V34, P2365, DOI 10.1109/TPAMI.2012.64; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Dosovitskiy A., 2014, ARXIV14115928; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gregor K., 2014, ICML; Hofmann T., 2004, P 21 INT C MACH LEAR, P104, DOI 10.1145/1015330.1015341; Kingma D.P., 2015, INT C LEARN REPR, P1; Kingma D. P., 2014, NIPS; Larochelle H., 2011, AISTATS; LeCun Y., 1998, P IEEE; Lee C., 2015, AISTATS; Lee H., 2009, ICML; Lin M., 2014, ICLR; Little R. J., 1987, JMLR, V539; Miller K., 2012, AISTATS; Mnih A., 2014, ICML; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Ranzato Marc'Aurelio, 2011, CVPR; Rezende D.J., 2014, PROC INT CONFER ENCE; Salakhutdinov R., 2009, AISTATS; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251; Sermanet Pierre, 2012, ICPR; Shalev-Shwartz S., 2011, MATH PROGRAMMING B; Tang Y., 2013, ICML; Taskar B., 2003, ICML; Taskar Ben, 2003, NIPS; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Warde-Farley D., 2013, ICML; Zeiler M., 2013, ICLR; Zhu J., 2008, NIPS; Zhu J, 2014, J MACH LEARN RES, V15, P1799; Zhu J, 2014, J MACH LEARN RES, V15, P1073; Zhu J, 2012, J MACH LEARN RES, V13, P2237	37	15	16	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102106
C	Blum, A; Haghtalab, N; Procaccia, AD		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Blum, Avrim; Haghtalab, Nika; Procaccia, Ariel D.			Learning Optimal Commitment to Overcome Insecurity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Game-theoretic algorithms for physical security have made an impressive real-world impact. These algorithms compute an optimal strategy for the defender to commit to in a Stackelberg game, where the attacker observes the defender's strategy and best-responds. In order to build the game model, though, the payoffs of potential attackers for various outcomes must be estimated; inaccurate estimates can lead to significant inefficiencies. We design an algorithm that optimizes the defender's strategy with no prior information, by observing the attacker's responses to randomized deployments of resources and learning his priorities. In contrast to previous work, our algorithm requires a number of queries that is polynomial in the representation of the game.	[Blum, Avrim; Haghtalab, Nika; Procaccia, Ariel D.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Blum, A (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	avrim@cs.cmu.edu; nika@cmu.edu; arielpro@cs.cmu.edu			National Science Foundation [CCF-1116892, CCF-1101215, CCF-1215883, IIS-1350598]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported by the National Science Foundation under grants CCF-1116892, CCF-1101215, CCF-1215883, and IIS-1350598.	Conitzer V., 2006, P 7 ACM C ELECT COMM, P82, DOI DOI 10.1145/1134707.1134717; Fukuda K., 1996, Combinatorics and Computer Science. 8th Franco-Japanese and 4th Franco-Chinese Conference. Selected Papers, P91; GACS P, 1981, MATH PROGRAM STUD, V14, P61, DOI 10.1007/BFb0120921; Grtschel M., 1993, GEOMETRIC ALGORITHMS; Kalai AT, 2006, MATH OPER RES, V31, P253, DOI 10.1287/moor.1060.0194; Kiekintveld C., 2011, 10 INT C AUT AG MULT, P1005; Korzhyk D, 2010, AAAI CONF ARTIF INTE, P805; Korzhyk D, 2011, J ARTIF INTELL RES, V41, P297, DOI 10.1613/jair.3269; Letchford J, 2009, LECT NOTES COMPUT SC, V5814, P250, DOI 10.1007/978-3-642-04645-2_23; Marecki J., 2012, P 11 INT C AUT AG MU, V2, P821; MOTZKIN THEODORE S., 1953, CONTRIBUTIONS THEORY, V28, P51; Paruchuri P., 2008, P 7 INT JOINT C AUTO, V2, P895; Pita J, 2010, ARTIF INTELL, V174, P1142, DOI 10.1016/j.artint.2010.07.002; Tambe M., 2012, SECURITY GAME THEORY	14	15	15	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103037
