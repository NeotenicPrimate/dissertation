PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
C	Malkomes, G; Kusner, MJ; Chen, WL; Weinberger, KQ; Moseley, B		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Malkomes, Gustavo; Kusner, Matt J.; Chen, Wenlin; Weinberger, Kilian Q.; Moseley, Benjamin			Fast Distributed k-Center Clustering with Outliers on Massive Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Clustering large data is a fundamental problem with a vast number of applications. Due to the increasing size of data, practitioners interested in clustering have turned to distributed computation methods. In this work, we consider the widely used k center clustering problem and its variant used to handle noisy data, k-center with outliers. In the noise-free setting we demonstrate how a previously-proposed distributed method is actually an O(1)-approximation algorithm, which accurately explains its strong empirical performance. Additionally, in the noisy setting, we develop a novel distributed algorithm that is also an O(1)-approximation. These algorithms are highly parallel and lend themselves to virtually any distributed computing framework. We compare each empirically against the best known sequential clustering methods and show that both distributed algorithms are consistently close to their sequential versions. The algorithms are all one can hope for in distributed settings: they are fast, memory efficient and they match their sequential counterparts.	[Malkomes, Gustavo; Kusner, Matt J.; Chen, Wenlin; Moseley, Benjamin] Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA; [Weinberger, Kilian Q.] Cornell Univ, Dept Comp Sci, Ithaca, NY 14850 USA	Washington University (WUSTL); Cornell University	Malkomes, G (corresponding author), Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.	luizgustavo@wustl.edu; mkusner@wustl.edu; wenlinchen@wustl.edu; kqw4@cornell.edu; bmoseley@wustl.edu			CAPES/BR; NSF [IIA-1355406, IIS-1149882, EFRI-1137211]; Google; Yahoo	CAPES/BR(Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES)); NSF(National Science Foundation (NSF)); Google(Google Incorporated); Yahoo	GM was supported by CAPES/BR; MJK and KQW were supported by the NSF grants IIA-1355406, IIS-1149882, EFRI-1137211; and BM was supported by the Google and Yahoo Research Awards.	Agarwal PK, 2008, LECT NOTES COMPUT SC, V5193, P64, DOI 10.1007/978-3-540-87744-8_6; Aggarwal Charu Chandra, 2004, US Patent, Patent No. [6714975, 6,714,975]; Ailon N., 2009, PROC 22 ADV NEURAL I, P10; Andoni A, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P574, DOI 10.1145/2591796.2591805; Bahmani B, 2012, PROC VLDB ENDOW, V5, P454, DOI 10.14778/2140436.2140442; Bahmani B, 2012, PROC VLDB ENDOW, V5, P622, DOI 10.14778/2180912.2180915; Balcan M.F., 2013, P 26 INT C NEURAL IN, P1995; Barbosa R, 2015, PR MACH LEARN RES, V37, P1236; Broder A, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P233, DOI 10.1145/2556195.2556260; Charikar M, 2001, SIAM PROC S, P642; Chen M., 2012, P INT C ART INT STAT, P218; Chierichetti, 2010, P 19 INT C WORLD WID, P231, DOI 10.1145/1772690.1772715.; Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137; Ene A., 2011, SIGKDD, DOI DOI 10.1145/2020408.2020515; Feldman J, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P710; GONZALEZ TF, 1985, THEOR COMPUT SCI, V38, P293, DOI 10.1016/0304-3975(85)90224-5; Guha S, 2003, IEEE T KNOWL DATA EN, V15, P515, DOI 10.1109/TKDE.2003.1198387; Guha S, 2004, NETWORK THEORY APPLI, V11, P35; Hassani M., 2009, KDD WORKSH KNOWL DIS, P39; HOCHBAUM DS, 1985, MATH OPER RES, V10, P180, DOI 10.1287/moor.10.2.180; KARLOFF H, 2010, SODA, V135, P938; Kaufman L., 1990, FINDING GROUPS DATA; Kumar R., 2013, PROC SEG ANN M, P1, DOI [10.1145/2809814, DOI 10.1145/2809814]; MeCutchen RM, 2008, LECT NOTES COMPUT SC, V5171, P165; Shindler M., 2011, ADV NEURAL INFORM PR, P2375; Suri S., 2011, P 20 INT C WORLD WID, P607, DOI DOI 10.1145/1963405.1963491; Tsanas A, 2010, INT CONF ACOUST SPEE, P594, DOI 10.1109/ICASSP.2010.5495554; Tyree S., 2011, P 20 INT C WORLD WID, P387, DOI DOI 10.1145/1963405.1963461; Zamir O., 1997, Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, P287; Zhao Z, 2012, INT PARALL DISTRIB P, P390, DOI 10.1109/IPDPS.2012.44	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103035
C	Mazumdar, A; Rawat, AS		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Mazumdar, Arya; Rawat, Ankit Singh			Associative Memory via a Sparse Recovery Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				NEURAL-NETWORKS; ALGORITHMS	An associative memory is a structure learned from a dataset M of vectors (signals) in a way such that, given a noisy version of one of the vectors as input, the nearest valid vector from M(nearest neighbor) is provided as output, preferably via a fast iterative algorithm. Traditionally, binary (or q-ary) Hopfield neural networks are used to model the above structure. In this paper, for the first time, we propose a model of associative memory based on sparse recovery of signals. Our basic premise is simple. For a dataset, we learn a set of linear constraints that every vector in the dataset must satisfy. Provided these linear constraints possess some special properties, it is possible to cast the task of finding nearest neighbor as a sparse recovery problem. Assuming generic random models for the dataset, we show that it is possible to store super-polynomial or exponential number of n-length vectors in a neural network of size O(n). Furthermore, given a noisy version of one of the stored vectors corrupted in near-linear number of coordinates, the vector can be correctly recalled using a neurally feasible algorithm.	[Mazumdar, Arya] Univ Minnesota Twin Cities, Dept ECE, Minneapolis, MN 55455 USA; [Rawat, Ankit Singh] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA	University of Minnesota System; University of Minnesota Twin Cities; Carnegie Mellon University	Mazumdar, A (corresponding author), Univ Minnesota Twin Cities, Dept ECE, Minneapolis, MN 55455 USA.	arya@umn.edu; asrawat@andrew.cmu.edu	Rawat, Ankit Singh/V-3483-2019	Rawat, Ankit Singh/0000-0001-9790-6500				Agarwal Alekh, 2013, ABS13107991 CORR; Arora S., 2015, ABS150300778 CORR; Arora S., 2013, ARXIV13086273; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Candes EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Foucart S., 2013, MATH INTRO COMPRESSI, P1, DOI DOI 10.1007/978-0-8176-4948-7; Gripon V, 2011, IEEE T NEURAL NETWOR, V22, P1087, DOI 10.1109/TNN.2011.2146789; GROSS DJ, 1984, NUCL PHYS B, V240, P431, DOI 10.1016/0550-3213(84)90237-2; Hebb D. O., 2005, ORG BEHAV NEUROPSYCH; Hillar Christopher, 2014, ARXIV14114625; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; Hu T, 2012, NEURAL COMPUT, V24, P2852, DOI 10.1162/NECO_a_00353; Jankowski S, 1996, IEEE T NEURAL NETWOR, V7, P1491, DOI 10.1109/72.548176; Karbasi A., 2014, ABS14076513 CORR; Kumar K. R., 2011, IEEE Information Theory Workshop (ITW 2011), P80, DOI 10.1109/ITW.2011.6089532; Maleki A, 2009, ANN ALLERTON CONF, P236, DOI 10.1109/ALLERTON.2009.5394802; MCELIECE RJ, 1987, IEEE T INFORM THEORY, V33, P461, DOI 10.1109/TIT.1987.1057328; Muezzinoglu MK, 2003, IEEE T NEURAL NETWOR, V14, P891, DOI 10.1109/TNN.2003.813844; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Posner E. C., 1985, JPL TDA PROGR REPORT, V42-83, P209; Salavati A. H., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1064, DOI 10.1109/ISIT.2012.6283014; TANAKA F, 1980, J PHYS F MET PHYS, V10, P2769, DOI 10.1088/0305-4608/10/12/017; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018; Yin WT, 2008, SIAM J IMAGING SCI, V1, P143, DOI 10.1137/070703983	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102064
C	Meshi, O; Mahdavi, M; Schwing, AG		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Meshi, Ofer; Mahdavi, Mehrdad; Schwing, Alexander G.			Smooth and Strong: MAP Inference with Linear Convergence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				BELIEF PROPAGATION; RELAXATIONS	Maximum a-posteriori (MAP) inference is an important task for many applications. Although the standard formulation gives rise to a hard combinatorial optimization problem, several effective approximations have been proposed and studied in recent years. We focus on linear programming (LP) relaxations, which have achieved state-of-the-art performance in many applications. However, optimization of the resulting program is in general challenging due to non-smoothness and complex non-separable constraints. Therefore, in this work we study the benefits of augmenting the objective function of the relaxation with strong convexity. Specifically, we introduce strong convexity by adding a quadratic term to the LP relaxation objective. We provide theoretical guarantees for the resulting programs, bounding the difference between their optimal value and the original optimum. Further, we propose suitable optimization algorithms and analyze their convergence.	[Meshi, Ofer; Mahdavi, Mehrdad] TTI Chicago, Chicago, IL 60637 USA; [Schwing, Alexander G.] Univ Toronto, Toronto, ON, Canada	University of Toronto	Meshi, O (corresponding author), TTI Chicago, Chicago, IL 60637 USA.							Belanger D., 2014, UAI; Borenstein E., 2004, CVPR; Duchi J., 2008, PROC 25 INT C MACH L, P272; Figueiredo Mario A. T., 2011, P 28 INT C MACH LEAR, P169; Frank M., 1956, ALGORITHM QUADRATIC, V3, P95; Garber D., 2013, ARXIV13014666; Globerson A., 2008, NIPS; Hazan T, 2010, IEEE T INFORM THEORY, V56, P6294, DOI 10.1109/TIT.2010.2079014; Johnson J. K., 2008, THESIS; Kappes J. H., 2012, CVPR; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Komodakis N., 2010, IEEE PAMI; Kumar MP, 2009, J MACH LEARN RES, V10, P71; Lacoste-Julien S., 2013, P 30 INT C MACH LEAR, P53; Li Y., 2014, P 31 INT C MACH LEAR, P1368; Meshi O., 2011, ECML; Meshi O., 2015, AISTATS; Meshi O., 2012, NIPS, P3023; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Prusa D, 2013, PROC CVPR IEEE, P1738, DOI 10.1109/CVPR.2013.227; Ravikumar P, 2010, J MACH LEARN RES, V11, P1043; Savchynskyy B., 2011, CVPR; Schwing A. G., 2014, P ICML; Schwing A. G., 2012, P NIPS; Shalev-Shwartz, 2014, ICML; Sontag D, 2012, OPTIMIZATION FOR MACHINE LEARNING, P219; Tarlow Daniel, 2010, INT C ART INT STAT, P812; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938; Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036; Werner T, 2010, IEEE T PATTERN ANAL, V32, P1474, DOI 10.1109/TPAMI.2009.134; Yanover C, 2006, J MACH LEARN RES, V7, P1887	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100078
C	Mirzazadeh, F; Ravanbakhsh, S; Ding, N; Schuurmans, D		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Mirzazadeh, Farzaneh; Ravanbakhsh, Siamak; Ding, Nan; Schuurmans, Dale			Embedding Inference for Structured Multilabel Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				CLASSIFICATION	A key bottleneck in structured output prediction is the need for inference during training and testing, usually requiring some form of dynamic programming. Rather than using approximate inference or tailoring a specialized inference method for a particular structure-standard responses to the scaling challenge-we propose to embed prediction constraints directly into the learned representation. By eliminating the need for explicit inference a more scalable approach to structured output prediction can be achieved, particularly at test time. We demonstrate the idea for multi-label prediction under subsumption and mutual exclusion constraints, where a relationship to maximum margin structured output prediction can be established. Experiments demonstrate that the benefits of structured output training can still be realized even after inference has been eliminated.	[Mirzazadeh, Farzaneh; Ravanbakhsh, Siamak; Schuurmans, Dale] Univ Alberta, Edmonton, AB, Canada; [Ding, Nan] Google, Menlo Pk, CA USA	University of Alberta; Google Incorporated	Mirzazadeh, F (corresponding author), Univ Alberta, Edmonton, AB, Canada.	mirzazad@ualberta.ca; mravanba@ualberta.ca; dingnan@google.com; daes@ualberta.ca						BakIr G., 2007, PREDICTING STRUCTURE; Bi W., 2012, NEURAL INFORM PROCES; Cisse M., 2013, P ADV NEUR INF PROC; Daume H, 2009, MACH LEARN, V75, P297, DOI 10.1007/s10994-009-5106-x; Dembczynski K., 2010, P ICML; Dembczynski K, 2012, MACH LEARN, V88, P5, DOI 10.1007/s10994-012-5285-8; Deng J., 2014, P ECCV; Deng J., 2010, P EUR C COMP VIS ECC; Guo Y., 2011, AAAI; Haeffele B., 2014, INT C MACH LEARN ICM; Hariharan B, 2012, MACH LEARN, V88, P127, DOI 10.1007/s10994-012-5291-x; Jancsary J., 2013, P INT C MACH LEARN I; Joachims T, 1999, ICML; Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359; Kadri H., 2013, P INT C MACH LEARN I; Kae Andrew, 2013, P CVPR; Kapoor A., 2012, P ADV NEUR INF PROC; Klimt B., 2004, ECML; Lafferty J., 2001, P 18 INT C MACH LEAR; Li Q., 2013, P INT C MACH LEARN I; Lin Z., 2014, P INT C MACH LEARN I; Makela M., 2003, TECHNICAL REPORT; Mirzazadeh F., 2014, P AAAI; Rousu J, 2006, J MACH LEARN RES, V7, P1601; Srikumar V, 2014, ADV NEUR IN, V27; Sun X., 2014, P NIPS; Taskar B., 2004, THESIS; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34; Weinberger K., 2008, NEURAL INFORM PROCES; Weston J., 2011, INT JOINT C ART INT	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100043
C	Mittal, H; Mahajan, A; Gogate, V; Singla, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Mittal, Happy; Mahajan, Anuj; Gogate, Vibhav; Singla, Parag			Lifted Inference Rules with Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Lifted inference rules exploit symmetries for fast reasoning in statistical relational models. Computational complexity of these rules is highly dependent on the choice of the constraint language they operate on and therefore coming up with the right kind of representation is critical to the success of lifted inference. In this paper, we propose a new constraint language, called setineq, which allows subset, equality and inequality constraints, to represent substitutions over the variables in the theory. Our constraint formulation is strictly more expressive than existing representations, yet easy to operate on. We reformulate the three main lifting rules: decomposer, generalized binomial and the recently proposed single occurrence for MAP inference, to work with our constraint representation. Experiments on benchmark MLNs for exact and sampling based inference demonstrate the effectiveness of our approach over several other existing techniques.	[Mittal, Happy; Mahajan, Anuj; Singla, Parag] IIT Delhi, Dept Comp Sci & Engn, Hauz Khas, New Delhi 110016, India; [Gogate, Vibhav] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75080 USA	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Delhi; University of Texas System; University of Texas Dallas	Mittal, H (corresponding author), IIT Delhi, Dept Comp Sci & Engn, Hauz Khas, New Delhi 110016, India.	happy.mittal@cse.iitd.ac.in; anujmahajan.iitd@gmail.com; vgogate@hlt.utdallas.edu; parags@cse.iitd.ac.in			TCS Research Scholar Program; DARPA Probabilistic Programming for Advanced Machine Learning Program [FA8750-14-C-0005]; Google travel grant	TCS Research Scholar Program; DARPA Probabilistic Programming for Advanced Machine Learning Program; Google travel grant(Google Incorporated)	Happy Mittal was supported by TCS Research Scholar Program. Vibhav Gogate was partially supported by the DARPA Probabilistic Programming for Advanced Machine Learning Program under AFRL prime contract number FA8750-14-C-0005. Parag Singla is being supported by Google travel grant to attend the conference. We thank Somdeb Sarkhel for helpful discussions.	Apsel U, 2014, AAAI CONF ARTIF INTE, P2403; Braz RD, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1319; Bui Hung Hai, 2013, UAI, P132; de Salvo Braz R., 2007, INTRO STAT RELATIONA; Domingos P., 2009, SYNTHESIS LECT ARTIF; Gogate V., 2011, UAI 2011 P 27 C UNC, P256; Gogate V., 2012, P 26 ANN C NEUR INF, P1664; Gogate Vibhav, 2012, P AAAI 12, P1910; Jha A., 2010, P 24 ANN C NEUR INF, P973; Kersting K., 2009, P 25 C UNC ART INT, P277; Kisynski J., 2009, P UAI 09; Mihalkova Lilyana, 2007, P 24 INT C MACH LEAR, P625, DOI DOI 10.1145/1273496.1273575.ICML'07; Milch B., 2008, P AAAI 08; Mittal H., 2014, P NIPS 14, P649; Mladenov M., 2015, P UAI 15; Mladenov M, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P603; Poole D., 2003, P INT JOINT C ART IN, P985; Russell S. J., 2010, ARTIF INTELL, V3rd; Singla P., 2008, P 23 AAAI C ART INT, V8, P1094; Singla P, 2014, AAAI CONF ARTIF INTE, P2497; Taghipour N., 2012, P AISTATS 12; Van den Broeck G., 2011, P IJCAI 11; Van den Broeck G., 2013, THESIS; Van den Broeck G., 2013, P NIPS 13; Van den Broeck G., 2012, P AAAI 12; VandenBroeck G, 2011, ADV NEURAL INFORM PR, V24, P1386	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102049
C	Montanari, A; Reichman, D; Zeitouni, O		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Montanari, Andrea; Reichman, Daniel; Zeitouni, Ofer			On the Limitation of Spectral Methods: From the Gaussian Hidden Clique Problem to Rank-One Perturbations of Gaussian Tensors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We consider the following detection problem: given a realization of a symmetric matrix X of dimension n, distinguish between the hypothesis that all upper triangular variables are i.i.d. Gaussians variables with mean 0 and variance 1 and the hypothesis that there is a planted principal submatrix B of dimension L for which all upper triangular variables are i.i.d. Gaussians with mean 1 and variance 1, whereas all other upper triangular elements of X not in B are i.i.d. Gaussians variables with mean 0 and variance 1. We refer to this as the ` Gaussian hidden clique problem'. When L = (1 + epsilon)root n (epsilon > 0), it is possible to solve this detection problem with probability 1 o(n) (1) by computing the spectrum of X and considering the largest eigenvalue of X. We prove that when L < (1 epsilon)root n no algorithm that examines only the eigenvalues of X can detect the existence of a hidden Gaussian clique, with error probability vanishing as n -> infinity. The result above is an immediate consequence of a more general result on rank-one perturbations of k-dimensional Gaussian tensors. In this context we establish a lower bound on the critical signal-to-noise ratio below which a rank-one signal cannot be detected.	[Montanari, Andrea] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA; [Montanari, Andrea] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Reichman, Daniel] Univ Calif Berkeley, Dept Cognit & Brain Sci, Berkeley, CA 94720 USA; [Zeitouni, Ofer] Weizmann Inst Sci, Fac Math, IL-76100 Rehovot, Israel; [Zeitouni, Ofer] NYU, Courant Inst, New York, NY 10003 USA	Stanford University; Stanford University; University of California System; University of California Berkeley; Weizmann Institute of Science; New York University	Montanari, A (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.	montanari@stanford.edu; daniel.reichman@gmail.com; ofer.zeitouni@weizmann.ac.il	zeitouni, ofer/O-7764-2019	zeitouni, ofer/0000-0002-2520-1525				Addario-Berry L, 2010, ANN STAT, V38, P3063, DOI 10.1214/10-AOS817; Alon N, 1998, RANDOM STRUCT ALGOR, V13, P457, DOI 10.1002/(SICI)1098-2418(199810/12)13:3/4<457::AID-RSA14>3.0.CO;2-W; ANDERSON G. W., 2010, INTRO RANDOM MATRICE, V118; Arias-Castro E, 2008, ANN STAT, V36, P1726, DOI 10.1214/07-AOS526; Auffinger A, 2013, COMMUN PUR APPL MATH, V66, P165, DOI 10.1002/cpa.21422; Balakrishnan S., 2011, NEURIPS 2011 WORKSH, V4; Bhamidi S., ARXIV12112284; Dembo A., ARXIV14094606; Deshpande Y., 2014, FDN COMPUTATIONAL MA, P1; Feige U, 2000, RANDOM STRUCT ALGOR, V16, P195, DOI 10.1002/(SICI)1098-2418(200003)16:2<195::AID-RSA5>3.0.CO;2-A; Feral D, 2007, COMMUN MATH PHYS, V272, P185, DOI 10.1007/s00220-007-0209-3; FUREDI Z, 1981, COMBINATORICA, V1, P233, DOI 10.1007/BF02579329; GRIMMETT GR, 1975, MATH PROC CAMBRIDGE, V77, P313, DOI 10.1017/S0305004100051124; Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025; JERRUM M, 1992, RANDOM STRUCT ALGOR, V3, P347, DOI 10.1002/rsa.3240030402; Kolar M., 2011, ADV NEURAL INFORM PR, V24, P909; Ma Z, ARXIV13095914; Onatski A, 2013, ANN STAT, V41, P1204, DOI 10.1214/13-AOS1100; Richard E., 2014, P ADV NEUR INF PROC, P2897; Talagrand M, 2006, PROBAB THEORY REL, V134, P339, DOI 10.1007/s00440-005-0433-8; WATERHOUSE WC, 1990, LINEAR ALGEBRA APPL, V128, P97, DOI 10.1016/0024-3795(90)90284-J	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100087
C	Moore, DA; Russell, SJ		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Moore, David A.; Russell, Stuart J.			Gaussian Process Random Fields	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Gaussian processes have been successful in both supervised and unsupervised machine learning tasks, but their computational complexity has constrained practical applications. We introduce a new approximation for large-scale Gaussian processes, the Gaussian Process Random Field (GPRF), in which local GPs are coupled via pairwise potentials. The GPRF likelihood is a simple, tractable, and parallelizeable approximation to the full GP marginal likelihood, enabling latent variable modeling and hyperparameter selection on large datasets. We demonstrate its effectiveness on synthetic spatial data as well as a real-world application to seismic event location.	[Moore, David A.; Russell, Stuart J.] Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94709 USA	University of California System; University of California Berkeley	Moore, DA (corresponding author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94709 USA.	dmoore@cs.berkeley.edu; russell@cs.berkeley.edu			DTRA [HDTRA-11110026]; Microsoft Research	DTRA(United States Department of DefenseDefense Threat Reduction Agency); Microsoft Research(Microsoft)	We thank the anonymous reviewers for their helpful suggestions. This work was supported by DTRA grant #HDTRA-11110026, and by computing resources donated by Microsoft Research under an Azure for Research grant.	[Anonymous], 2012, GPY GAUSSIAN PROCESS; BESAG J, 1975, J ROY STAT SOC D-STA, V24, P179, DOI 10.2307/2987782; Chalupka K, 2013, J MACH LEARN RES, V14, P333; Damianou Andreas C., 2015, J MACHINE LEARNING R; Deisenroth Marc Peter, 2015, INT C MACH LEARN ICM; Ferris B, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2480; Gal Yarin, 2014, ADV NEURAL INFORM PR; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; International Seismological Centre, 2015, ON LIN B, DOI 10.31905/D808B830; Koller D., 2009, PROBABILISTIC GRAPHI; Lawrence N. D., 2004, ADV NEURAL INFORM PR; Lawrence Neil D, 2007, INT C ART INT STAT A; McNames J, 2001, IEEE T PATTERN ANAL, V23, P964, DOI 10.1109/34.955110; Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467; Nguyen TV, 2014, PR MACH LEARN RES, V32; Nguyen-Tuong D, 2009, ADV ROBOTICS, V23, P2015, DOI 10.1163/016918609X12529286896877; Park C, 2011, J MACH LEARN RES, V12, P1697; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rasmussen CE, 2002, ADV NEUR IN, V14, P881; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Snelson E., 2007, ARTIFICIAL INTELLIGE; Titsias M.K., 2009, J MACH LEARN RES; Titsias Michalis, 2010, INT C ART INT STAT A; Tresp V, 2000, NEURAL COMPUT, V12, P2719, DOI 10.1162/089976600300014908; Vanhatalo Jarno, 2008, UNCERTAINTY ARTIFICI; Yedidia J. S., 2001, TR200116 MITS EL RES; Zhong Guoqiang, 2010, AAAI C ART INT	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102023
C	Oh, S; Thekumparampil, KK; Xu, JM		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Oh, Sewoong; Thekumparampil, Kiran K.; Xu, Jiaming			Collaboratively Learning Preferences from Ordinal Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					In personalized recommendation systems, it is important to predict preferences of a user on items that have not been seen by that user yet. Similarly, in revenue management, it is important to predict outcomes of comparisons among those items that have never been compared so far. The MultiNomial Logit model, a popular discrete choice model, captures the structure of the hidden preferences with a low-rank matrix. In order to predict the preferences, we want to learn the underlying model from noisy observations of the low-rank matrix, collected as revealed preferences in various forms of ordinal data. A natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization. We present the convex relaxation approach in two contexts of interest: collaborative ranking and bundled choice modeling. In both cases, we show that the convex relaxation is minimax optimal. We prove an upper bound on the resulting error with finite samples, and provide a matching information-theoretic lower bound.	[Oh, Sewoong; Thekumparampil, Kiran K.] Univ Illinois, Champaign, IL 61820 USA; [Xu, Jiaming] UPenn, Wharton Sch, Philadelphia, PA USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Pennsylvania	Oh, S (corresponding author), Univ Illinois, Champaign, IL 61820 USA.	swoh@illinois.edu; thekump2@illinois.edu; jiamingx@wharton.upenn.edu			NSF CMMI award [MES-1450848]; NSF SaTC award [CNS-1527754]	NSF CMMI award; NSF SaTC award(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE))	This research is supported in part by NSF CMMI award MES-1450848 and NSF SaTC award CNS-1527754.	Agarwal A., 2010, P ADV NEUR INF PROC, P37; Ammar A., 2014, P ACM SIGMETRICS INT; Azari H, 2012, NIPS 12, P126; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Caron F, 2012, J COMPUT GRAPH STAT, V21, P174, DOI 10.1080/10618600.2012.638220; Ding W., 2014, TECHNICAL REPORT; Gormley IC, 2009, BAYESIAN ANAL, V4, P265, DOI 10.1214/09-BA410; Guiver J., 2009, P 26 ANN INT C MACHI, P377; Hajek S., 2014, ADV NEURAL INFORM PR, P1475; Hunter DR, 2004, ANN STAT, V32, P384; Ledoux M., 2005, CONCENTRATION MEASUR; Liu T-Y., 2009, FOUND TRENDS INF RET, V3, P225, DOI DOI 10.1561/1500000016; Lu Yu, 2014, ARXIV14100860; Luce D., 1959, INDIVIDUAL CHOICE BE; Marschak J., 1960, P S MATH METH SOC SC, V7, P19; Negahban S., 2012, J MACHINE LEARNING R; Negahban Sahand, 2012, NIPS, P2483; Oh S., 2014, ADV NEURAL INFORM PR, P595; Pardo B., 2002, Computer Music Journal, V26, P27, DOI 10.1162/014892602760137167; Park D., 2015, PREFERENCE COMPLETIO; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Soufiani H. A., 2013, ADV NEURAL INFORM PR, P73; Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288; Tropp J., 2011, USER FRIENDLY TAIL B; Van de Geer S, 2000, EMPIRICAL PROCESSES, V6; Wu R., 2015, ARXIV150204631	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101077
C	Ohsaka, N; Yoshida, Y		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Ohsaka, Naoto; Yoshida, Yuichi			Monotone k-Submodular Function Maximization with Size Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				FUNCTION SUBJECT	A k-submodular function is a generalization of a submodular function, where the input consists of k disjoint subsets, instead of a single subset, of the domain. Many machine learning problems, including influence maximization with k kinds of topics and sensor placement with k kinds of sensors, can be naturally modeled as the problem of maximizing monotone k-submodular functions. In this paper, we give constant-factor approximation algorithms for maximizing monotone k-submodular functions subject to several size constraints. The running time of our algorithms are almost linear in the domain size. We experimentally demonstrate that our algorithms outperform baseline algorithms in terms of the solution quality.	[Ohsaka, Naoto] Univ Tokyo, Tokyo, Japan; [Yoshida, Yuichi] Natl Inst Informat & Preferred Infrastruct Inc, Tokyo, Japan	University of Tokyo	Ohsaka, N (corresponding author), Univ Tokyo, Tokyo, Japan.	ohsaka@is.s.u-tokyo.ac.jp; yyoshida@nii.ac.jp			JSPS [26730009]; MEXT [24106003]; JST, ERATO, Kawarabayashi Large Graph Project	JSPS(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science); MEXT(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)); JST, ERATO, Kawarabayashi Large Graph Project	Y. Y. is supported by JSPS Grant-in-Aid for Young Scientists (B) (No. 26730009), MEXT Grant-in-Aid for Scientific Research on Innovative Areas (24106003), and JST, ERATO, Kawarabayashi Large Graph Project. N. O. is supported by JST, ERATO, Kawarabayashi Large Graph Project.	[Anonymous], 2001, ACAD MARKET SCI REV; Barbieri N, 2012, IEEE DATA MINING, P81, DOI 10.1109/ICDM.2012.122; Buchbinder N, 2012, ANN IEEE SYMP FOUND, P649, DOI 10.1109/FOCS.2012.73; Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991; Domingos P., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P57, DOI 10.1145/502512.502525; Filmus Y, 2014, SIAM J COMPUT, V43, P514, DOI 10.1137/130920277; Goldenberg J, 2001, MARKET LETT, V12, P211, DOI 10.1023/A:1011122126881; Gridchyn I, 2013, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2013.288; Huber Anna, 2012, Combinatorial Optimization. Second International Symposium, ISCO 2012. Revised Selected Papers, P451, DOI 10.1007/978-3-642-32147-4_40; Iwata S., 2016, SODA; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; KO CW, 1995, OPER RES, V43, P684, DOI 10.1287/opre.43.4.684; Krause A, 2008, J MACH LEARN RES, V9, P235; Krause A, 2008, J MACH LEARN RES, V9, P2761; Lin H., 2010, HUMAN LANGUAGE TECHN, P912; Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234, DOI 10.1007/BFb0006528; Mirzasoleiman B, 2015, AAAI CONF ARTIF INTE, P1812; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Richardson M., 2002, P 9 ACM SIGKDD INT C, P61, DOI [DOI 10.1145/775047.775057, 10.1145/775047.775057]; Singh A., 2012, ARTIF INTELL STAT, P1055; Sviridenko M, 2004, OPER RES LETT, V32, P41, DOI 10.1016/S0167-6377(03)00062-2; Ward J., 2014, P 25 ANN ACM SIAM S, P1468	22	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100077
C	Orlitsky, A; Suresh, AT		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Orlitsky, Alon; Suresh, Ananda Theertha			Competitive Distribution Estimation: Why is Good-Turing Good	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Estimating distributions over large alphabets is a fundamental machine-learning tenet. Yet no method is known to estimate all distributions well. For example, add-constant estimators are nearly min-max optimal but often perform poorly in practice, and practical estimators such as absolute discounting, Jelinek-Mercer, and Good-Turing are not known to be near optimal for essentially any distribution. We describe the first universally near-optimal probability estimators. For every discrete distribution, they are provably nearly the best in the following two competitive ways. First they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the distribution up to a permutation. Second, they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the exact distribution, but as all natural estimators, restricted to assign the same probability to all symbols appearing the same number of times. Specifically, for distributions over k symbols and n samples, we show that for both comparisons, a simple variant of Good-Turing estimator is always within KL divergence of (3 + o(n) (1))/n(1/3) from the best estimator, and that a more involved estimator is within (O) over tilde (n) (min(k/n, 1 root n)). Conversely, we show that any estimator must have a KL divergence at least (Omega) over tilde (n) (min(k/n, 1/n(2/3))) over the best estimator for the first comparison, and at least (Omega) over tilde (n) (min(k/n; 1/root n)) for the second.	[Orlitsky, Alon; Suresh, Ananda Theertha] Univ Calif San Diego, San Diego, CA 92093 USA	University of California System; University of California San Diego	Orlitsky, A (corresponding author), Univ Calif San Diego, San Diego, CA 92093 USA.	alon@ucsd.edu; asuresh@ucsd.edu						Abramovich Felix, 2006, ANN STAT; Acharya J., 2012, COLT; Acharya J., 2011, COLT; Acharya Jayadev, 2013, AISTATS; Acharya Jayadev, 2013, COLT; Bontemps D, 2014, IEEE T INFORM THEORY, V60, P808, DOI 10.1109/TIT.2013.2288914; Braess D, 2004, J APPROX THEORY, V128, P187, DOI 10.1016/j.jat.2004.04.010; Chen S., 1996, ACL; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Drukh E., 2004, COLT; Gale William A., 1995, J QUANT LINGUIST, V2, P217, DOI DOI 10.1080/09296179508590051; Gassiat Elisabeth, 2014, CORR; Jelinek Fredrick, 1984, IBM TECH DISCLOSURE; Kamath S., 2015, COLT; Mitzenmacher M., 2005, PROBABILITY COMPUTIN; NEY H, 1994, COMPUT SPEECH LANG, V8, P1, DOI 10.1006/csla.1994.1001; Orlitsky Alon, 2003, FOCS; Paninski L., 2004, NIPS; Ryabko B. Y., 1990, PROBL PEREDACHI INF, V26, P24; Ryabko Boris Yakovlevich, 1984, PROBLEMY PEREDACHI I; Schapire Robert E., 2000, COLT; Thomas M.T.C.A.J., 2006, ELEMENTS INFORM THEO; Tsybakov A. B, 2004, INTRO NONPARAMETRIC; Valiant G., 2014, FOCS; Valiant Gregory, 2015, ABS150405321 CORR, Vabs/1504.05321; Wellner, 1993, EFFICIENT ADAPTIVE E; [No title captured]	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101022
C	Pan, YP; Theodorou, EA; Kontitsis, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Pan, Yunpeng; Theodorou, Evangelos A.; Kontitsis, Michail			Sample Efficient Path Integral Control under Uncertainty	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We present a data-driven optimal control framework that is derived using the path integral (PI) control approach. We find iterative control laws analytically without a priori policy parameterization based on probabilistic representation of the learned dynamics model. The proposed algorithm operates in a forward-backward manner which differentiate it from other PI-related methods that perform forward sampling to find optimal controls. Our method uses significantly less samples to find analytic control laws compared to other approaches within the PI control family that rely on extensive sampling from given dynamics models or trials on physical systems in a model-free fashion. In addition, the learned controllers can be generalized to new tasks without re-sampling based on the compositionality theory for the linearly-solvable optimal control framework. We provide experimental results on three different tasks and comparisons with state-of-the-art model-based methods to demonstrate the efficiency and generalizability of the proposed framework.	[Pan, Yunpeng; Theodorou, Evangelos A.; Kontitsis, Michail] Georgia Inst Technol, Sch Aerosp Engn, Autonomous Control & Decis Syst Lab, Inst Robot & Intelligent Machines, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Pan, YP (corresponding author), Georgia Inst Technol, Sch Aerosp Engn, Autonomous Control & Decis Syst Lab, Inst Robot & Intelligent Machines, Atlanta, GA 30332 USA.	ypan37@gatech.edu; evangelos.theodorou@gatech.edu; kontitsis@gatech.edu			NSF [NRI-1426945]	NSF(National Science Foundation (NSF))	This research is supported by NSF NRI-1426945.	Barto AG, 2004, HDB LEARNING APPROXI; Bertsekas D. P., 1996, ATHENA SCI, V7, P15; Candela J. Quinonero, 2003, IEEE INT C AC SPEECH; Deisenroth M., 2015, IEEE T PATTERN ANAL, V27, P75; Deisenroth M. P., 2014, P 2014 IEEE INT C RO; Dvijotham K., 2012, REINFORCEMENT LEARNI, V17, P119; Fleming W.H., 1971, APPL MATH OPT, V9, P329, DOI DOI 10.1007/BF01442148AM0MBN1432-0606; Gomez Vicenc, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P482, DOI 10.1007/978-3-662-44848-9_31; Hennig P., 2011, ADV NEURAL INFORM PR, P325; Kappen HJ, 2007, AIP CONF PROC, V887, P149; Kappen HJ, 2005, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2005/11/P11011; Kappen HJ, 2005, PHYS REV LETT, V95, DOI 10.1103/PhysRevLett.200201; Levine S, 2014, ADV NEUR IN, V27; Levine S, 2014, PR MACH LEARN RES, V32, P829; Pan Y., 2014, ADV NEURAL INFORM PR, P1907; Pan YP, 2014, 2014 IEEE SYMPOSIUM ON ADAPTIVE DYNAMIC PROGRAMMING AND REINFORCEMENT LEARNING (ADPRL), P63; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rawlik K., 2013, P 23 INT JOINT C ART, P1628; Schulman J., 2015, ARXIV150205477; Stulp F., 2012, ARXIV12064621; Theodorou EA, 2012, IEEE DECIS CONTR P, P1466, DOI 10.1109/CDC.2012.6426381; Theodorou EA, 2010, J MACH LEARN RES, V11, P3137; Thijssen S, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.032104; Todorov E., 2009, ADV NEURAL INFORM PR, V22, P1856; Todorov E, 2009, P NATL ACAD SCI USA, V106, P11478, DOI 10.1073/pnas.0710743106; [No title captured]	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103022
C	Park, G; Raskutti, G		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Park, Gunwoong; Raskutti, Garvesh			Learning Large-Scale Poisson DAG Models based on OverDispersion Scoring	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					In this paper, we address the question of identifiability and learning algorithms for large-scale Poisson Directed Acyclic Graphical (DAG) models. We define general Poisson DAG models as models where each node is a Poisson random variable with rate parameter depending on the values of the parents in the underlying DAG. First, we prove that Poisson DAG models are identifiable from observational data, and present a polynomial-time algorithm that learns the Poisson DAG model under suitable regularity conditions. The main idea behind our algorithm is based on overdispersion, in that variables that are conditionally Poisson are overdispersed relative to variables that are marginally Poisson. Our algorithms exploits overdispersion along with methods for learning sparse Poisson undirected graphical models for faster computation. We provide both theoretical guarantees and simulation results for both small and large-scale DAGs.	[Park, Gunwoong] Univ Wisconsin, Dept Stat, Madison, WI 53706 USA; [Raskutti, Garvesh] Univ Wisconsin, Optimizat Grp, Wisconsin Inst Discovery, Dept Comp Sci,Dept Stat, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison; University of Wisconsin System; University of Wisconsin Madison	Park, G (corresponding author), Univ Wisconsin, Dept Stat, Madison, WI 53706 USA.	parkg@stat.wisc.edu; raskutti@cs.wisc.edu	Park, Gunwoong/AAA-5853-2020					Aliferis C F, 2003, AMIA Annu Symp Proc, P21; [Anonymous], 2012, ARXIV PREPRINT ARXIV; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; COWELL RG, 1999, PROBABILISTIC NETWOR; DEAN CB, 1992, J AM STAT ASSOC, V87, P451, DOI 10.2307/2290276; Friedman J, 2009, R PACKAGE VERSION, P1, DOI DOI 10.18637/JSS.V033.I01; Friedman N, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P206; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Peters J., 2013, BIOMETRIKA; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Spirtes P., 2000, CAUSATION PREDICTION; Tsamardinos I., 2003, P 9 INT WORKSH ART I; Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7; Uhler C, 2013, ANN STAT, V41, P436, DOI 10.1214/12-AOS1080; Verma T., 1991, P 6 UAI C UAI 90, P255; Yang E., 2012, ADV NEURAL INFORM PR, P1358	19	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102045
C	Park, M; Jitkrittum, W; Qamar, A; Szabo, Z; Buesing, L; Sahani, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Park, Mijung; Jitkrittum, Wittawat; Qamar, Ahmad; Szabo, Zoltan; Buesing, Lars; Sahani, Maneesh			Bayesian Manifold Learning: The Locally Linear Latent Variable Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We introduce the Locally Linear Latent Variable Model (LL-LVM), a probabilistic model for non-linear manifold discovery that describes a joint distribution over observations, their manifold coordinates and locally linear maps conditioned on a set of neighbourhood relationships. The model allows straightforward variational optimisation of the posterior distribution on coordinates and locally linear maps from the latent space to the observation space given the data. Thus, the LL-LVM encapsulates the local-geometry preserving intuitions that underlie non-probabilistic methods such as locally linear embedding (LLE). Its probabilistic semantics make it easy to evaluate the quality of hypothesised neighbourhood relationships, select the intrinsic dimensionality of the manifold, construct out-of-sample extensions and to combine the manifold model with additional probabilistic models that capture the structure of coordinates within the manifold.	[Park, Mijung; Jitkrittum, Wittawat; Qamar, Ahmad; Szabo, Zoltan; Buesing, Lars; Sahani, Maneesh] UCL, Gatsby Computat Neurosci Unit, London, England; [Qamar, Ahmad] Thread Genius, New York, NY USA; [Buesing, Lars] Google DeepMind, London, England	University of London; University College London; Google Incorporated	Park, M (corresponding author), UCL, Gatsby Computat Neurosci Unit, London, England.	mijung@gatsby.ucl.ac.uk; wittawat@gatsby.ucl.ac.uk; atqamar@gmail.com; zoltan.szabo@gatsby.ucl.ac.uk; lbuesing@google.com; maneesh@gatsby.ucl.ac.uk	Jitkrittum, Wittawat/U-6881-2019		Gatsby Charitable Foundation	Gatsby Charitable Foundation	The authors were funded by the Gatsby Charitable Foundation.	Balasubramanian M, 2002, SCIENCE, V295; Beal M.J, 2003, THESIS; Belkin M, 2002, ADV NEUR IN, V14, P585; Bishop C.M, 2006, PATTERN RECOGN; Brand  M., 2003, ADV NEURAL INFORM PR, P961, DOI DOI 10.1109/34.682189; Cayton L., 2005, 12 U CAL, P1; Lawrence N, 2011, P INT C ART INT STAT, P51; Lawrence ND, 2004, ADV NEUR IN, V16, P329; Menne MJ, 2009, B AM METEOROL SOC, V90, P993, DOI 10.1175/2008BAMS2613.1; Platt J. C., 2005, P 10 INT WORKSH ART, P261; Roweis S, 2002, ADV NEUR IN, V14, P889; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Titsias MK, 2010, P 13 INT C ARTIFICIA, V9, P844; van der Maaten Laurens, 2009, J MACH LEARN RES, V10, P66; Verbeek J, 2006, IEEE T PATTERN ANAL, V28, P1236, DOI 10.1109/TPAMI.2006.166; ZHAN YB, 2009, NIPS, V5863, P293	17	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103011
C	Perrot, M; Habrard, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Perrot, Michael; Habrard, Amaury			Regressive Virtual Metric Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We are interested in supervised metric learning of Mahalanobis like distances. Existing approaches mainly focus on learning a new distance using similarity and dissimilarity constraints between examples. In this paper, instead of bringing closer examples of the same class and pushing far away examples of different classes we propose to move the examples with respect to virtual points. Hence, each example is brought closer to a a priori defined virtual point reducing the number of constraints to satisfy. We show that our approach admits a closed form solution which can be kernelized. We provide a theoretical analysis showing the consistency of the approach and establishing some links with other classical metric learning methods. Furthermore we propose an efficient solution to the difficult problem of selecting virtual points based in part on recent works in optimal transport. Lastly, we evaluate our approach on several state of the art datasets.	[Perrot, Michael; Habrard, Amaury] Univ Lyon, Univ Jean Monnet St Etienne, CNRS, Lab Hubert Curien,UMR5516, F-42000 St Etienne, France	Centre National de la Recherche Scientifique (CNRS); CNRS - Institute for Engineering & Systems Sciences (INSIS)	Perrot, M (corresponding author), Univ Lyon, Univ Jean Monnet St Etienne, CNRS, Lab Hubert Curien,UMR5516, F-42000 St Etienne, France.	michael.perrot@univ-st-etienne.fr; amaury.habrard@univ-st-etienne.fr						Balcan M.-F., 2008, PROC 21 ANN C LEARN, P287; Bellet A., 2015, SYNTHESIS LECT ARTIF; Bellet Aurelien, 2012, P ICML; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Cortes C., 2005, PROC 22 INT C MACH L, P153, DOI DOI 10.1145/1102351.1102371; Courty Nicolas, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P274, DOI 10.1007/978-3-662-44848-9_18; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Globerson A., 2005, NIPS; Goldberger J., 2004, ADV NEURAL INF PROCE, P1; Jin R., 2009, ADV NEURAL INFORM PR, V22; Kadri Hachem, 2013, INT C MACH LEARN ICM, P471; Kar P., 2011, ADV NEURAL INFORM PR, V24, P1998; Kedem D., 2012, ADV NEURAL INFORM PR, P2582; Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019; Lichman M., 2013, UCI MACHINE LEARNING; Scholkopf B., 1997, Artificial Neural Networks - ICANN '97. 7th International Conference Proceedings, P583, DOI 10.1007/BFb0020217; Shi Y, 2014, AAAI CONF ARTIF INTE, P2078; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Weston J., 2002, NIPS, P873	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100055
C	Plis, S; Danks, D; Freeman, C; Calhoun, V		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Plis, Sergey; Danks, David; Freeman, Cynthia; Calhoun, Vince			Rate-Agnostic (Causal) Structure Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Causal structure learning from time series data is a major scientific challenge. Extant algorithms assume that measurements occur sufficiently quickly; more precisely, they assume approximately equal system and measurement timescales. In many domains, however, measurements occur at a significantly slower rate than the underlying system changes, but the size of the timescale mismatch is often unknown. This paper develops three causal structure learning algorithms, each of which discovers all dynamic causal graphs that explain the observed measurement data, perhaps given undersampling. That is, these algorithms all learn causal structure in a "rate-agnostic" manner: they do not assume any particular relation between the measurement and system timescales. We apply these algorithms to data from simulations to gain insight into the challenge of undersampling.	[Plis, Sergey] Mind Res Network, Albuquerque, NM 87106 USA; [Danks, David] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Freeman, Cynthia] Univ New Mexico, CS Dept, Mind Res Network, Albuquerque, NM 87131 USA; [Calhoun, Vince] Univ New Mexico, ECE Dept, Mind Res Network, Albuquerque, NM 87131 USA	Lovelace Respiratory Research Institute; Carnegie Mellon University; Lovelace Respiratory Research Institute; University of New Mexico; Lovelace Respiratory Research Institute; University of New Mexico	Plis, S (corresponding author), Mind Res Network, Albuquerque, NM 87106 USA.	s.m.plis@gmail.com; ddanks@cmu.edu; cynthiaw2004@gmail.com; vcalhoun@mrn.org	Calhoun, Vince D./ACN-9399-2022; Plis, Sergey/AAA-9928-2022	Calhoun, Vince D./0000-0001-9058-0747; Plis, Sergey/0000-0003-0040-0365	NSF [IIS-1318759, IIS-1318815]; NIH [R01EB005846]; NIH (National Human Genome Research Institute) [U54HG008540]	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NIH (National Human Genome Research Institute)(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Human Genome Research Institute (NHGRI))	SP & DD contributed equally. This work was supported by awards NIH R01EB005846 (SP); NSF IIS-1318759 (SP); NSF IIS-1318815 (DD); & NIH U54HG008540 (DD) (from the National Human Genome Research Institute through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.	Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; Danks D., 2013, JMLR WORKSH C P, V1, P1; GLYMOUR C, 1991, ERKENNTNIS, V35, P151; Gong MM, 2015, PR MACH LEARN RES, V37, P1898; GRANGER CWJ, 1969, ECONOMETRICA, V37, P424, DOI 10.2307/1912791; Johnson D. B., 1975, SIAM Journal on Computing, V4, P77, DOI 10.1137/0204007; Liitkepohl H, 2007, NEW INTRO MULTIPLE T; Moneta A., 2011, P NEURAL INFORM PROC, V12, P95; Murphy KP., 2002, DYNAMIC BAYESIAN NET; Plis S., 2015, P 31 C ANN C UNC ART; Richardson T, 2002, ANN STAT, V30, P962; Seth AK, 2013, NEUROIMAGE, V65, P540, DOI 10.1016/j.neuroimage.2012.09.049; Thiesson B., 2004, PROC 20 C UNCERTAINT, P552; Voortman M., 2010, P 26 C UNC ART INT U, P641; [No title captured]	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100057
C	Qiu, HT; Han, F; Liu, H; Caffo, B		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Qiu, Huitong; Han, Fang; Liu, Han; Caffo, Brian			Robust Portfolio Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				COVARIANCE-MATRIX; QUANTILE REGRESSION	We propose a robust portfolio optimization approach based on quantile statistics. The proposed method is robust to extreme events in asset returns, and accommodates large portfolios under limited historical data. Specifically, we show that the risk of the estimated portfolio converges to the oracle optimal risk with parametric rate under weakly dependent asset returns. The theory does not rely on higher order moment assumptions, thus allowing for heavy-tailed asset returns. Moreover, the rate of convergence quantifies that the size of the portfolio under management is allowed to scale exponentially with the sample size of the historical data. The empirical effectiveness of the proposed method is demonstrated under both synthetic and real stock data. Our work extends existing ones by achieving robustness in high dimensions, and by allowing serial dependence.	[Qiu, Huitong; Han, Fang; Caffo, Brian] Johns Hopkins Univ, Dept Biostat, Baltimore, MD 21205 USA; [Liu, Han] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA	Johns Hopkins University; Princeton University	Qiu, HT (corresponding author), Johns Hopkins Univ, Dept Biostat, Baltimore, MD 21205 USA.	hqiu7@jhu.edu; fhan@jhu.edu; hanliu@princeton.edu; bcaffo@jhsph.edu						Bai JS, 2012, ANN STAT, V40, P436, DOI 10.1214/11-AOS966; Bai JS, 2011, ANN ECON FINANC, V12, P199; Belloni A, 2011, ANN STAT, V39, P82, DOI 10.1214/10-AOS827; BEST MJ, 1991, REV FINANC STUD, V4, P315, DOI 10.1093/rfs/4.2.315; Bickel PJ, 2008, ANN STAT, V36, P2577, DOI 10.1214/08-AOS600; Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Cai TT, 2010, ANN STAT, V38, P2118, DOI 10.1214/09-AOS752; Chen YL, 2011, IEEE T SIGNAL PROCES, V59, P4097, DOI 10.1109/TSP.2011.2138698; CHOPRA VK, 1993, J PORTFOLIO MANAGE, V19, P6, DOI 10.3905/jpm.1993.409440; Couillet R, 2014, J MULTIVARIATE ANAL, V131, P99, DOI 10.1016/j.jmva.2014.06.018; Dowd K., 2007, MEASURING MARKET RIS; Fan JQ, 2013, J R STAT SOC B, V75, P603, DOI 10.1111/rssb.12016; Fan JQ, 2012, J AM STAT ASSOC, V107, P592, DOI 10.1080/01621459.2012.682825; Fan JQ, 2008, J ECONOMETRICS, V147, P186, DOI 10.1016/j.jeconom.2008.09.017; Fang K.-T., 1990, SYMMETRIC MULTIVARIA; GNANADESIKAN R, 1972, BIOMETRICS, V28, P81, DOI 10.2307/2528963; Hall A.R., 2005, GEN METHOD MOMENTS; Huber P., 1981, ROBUST STAT; Jagannathan R, 2003, J FINANC, V58, P1651, DOI 10.1111/1540-6261.00580; Joe H., 1997, MULTIVARIATE MODELS, V1st; Johansen S., 2009, HDB FINANCIAL TIME S, DOI [10.1007/978-3-540-71297-8_29, DOI 10.1007/978-3-540-71297-8_29]; KALLBERG JG, 1984, LECT NOTES ECON MATH, V227, P74; Ledoit O, 2004, J PORTFOLIO MANAGE, V30, P110, DOI 10.3905/jpm.2004.110; Ledoit O, 2004, J MULTIVARIATE ANAL, V88, P365, DOI 10.1016/S0047-259X(03)00096-4; Ledoit O., 2003, J EMPIR FINANC, V10, P603, DOI [10.1016/S0927-5398(03)00007-0, DOI 10.1016/S0927-5398(03)00007-0]; Markowitz H, 1952, J FINANC, V7, P77, DOI 10.1111/j.1540-6261.1952.tb01525.x; Maronna RA, 2002, TECHNOMETRICS, V44, P307, DOI 10.1198/004017002188618509; MERTON RC, 1980, J FINANC ECON, V8, P323, DOI 10.1016/0304-405X(80)90007-0; Rachev S.T., 2003, HDB HEAVY TAILED DIS, V1; Rachev S.T., 2005, FAT TAILED SKEWED AS; ROUSSEEUW PJ, 1993, J AM STAT ASSOC, V88, P1273, DOI 10.2307/2291267; Schmidt R, 2002, MATH METHOD OPER RES, V55, P301, DOI 10.1007/s001860200191; Stock JH, 2002, J AM STAT ASSOC, V97, P1167, DOI 10.1198/016214502388618960; Van de Geer S., 2000, APPL EMPIRICAL PROCE; Wang L, 2012, J AM STAT ASSOC, V107, P214, DOI 10.1080/01621459.2012.656014; Xu M. H., 2012, ADV OPER RES, P1	36	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100082
C	Qu, X; Doshi, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Qu, Xia; Doshi, Prashant			Individual Planning in Infinite-Horizon Multiagent Settings: Inference, Structure and Scalability	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				CONVERGENCE; ALGORITHM	This paper provides the first formalization of self-interested planning in multiagent settings using expectation-maximization (EM). Our formalization in the context of infinite-horizon and finitely-nested interactive POMDPs (I-POMDP) is distinct from EM formulations for POMDPs and cooperative multiagent planning frameworks. We exploit the graphical model structure specific to I-POMDPs, and present a new approach based on block-coordinate descent for further speed up. Forward filtering-backward sampling - a combination of exact filtering with sampling - is explored to exploit problem structure.	[Qu, Xia] Epic Syst, Verona, WI 53593 USA; [Doshi, Prashant] Univ Georgia, THINC Lab, Dept Comp Sci, Athens, GA 30622 USA	University System of Georgia; University of Georgia	Qu, X (corresponding author), Epic Syst, Verona, WI 53593 USA.	quxiapisces@gmail.com; pdoshi@cs.uga.edu			NSF CAREER grant [IIS-0845036]; ONR [N000141310870]	NSF CAREER grant(National Science Foundation (NSF)NSF - Office of the Director (OD)); ONR(Office of Naval Research)	This research is supported in part by a NSF CAREER grant, IIS-0845036, and a grant from ONR, N000141310870. We thank Akshat Kumar for feedback that led to improvements in the paper.	Attias Hagai, 2003, 9 INT WORKSH AI STAT; Cappe O, 2009, J ROY STAT SOC B, V71, P593, DOI 10.1111/j.1467-9868.2009.00698.x; Carter CK, 1996, BIOMETRIKA, V83, P589, DOI 10.1093/biomet/83.3.589; Doshi P., 2009, J ARTIFICIAL INTELLI, V34, P297; Fessler J.A., 2011, PROC INT M FULL 3D I, P262; FESSLER JA, 1994, IEEE T SIGNAL PROCES, V42, P2664, DOI 10.1109/78.324732; Gmytrasiewicz PJ, 2005, J ARTIF INTELL RES, V24, P49, DOI 10.1613/jair.1579; Kumar A, 2010, P C UNC ART INT, P294; Kumar Akshat, 2011, P 22 INT JOINT C ART, P2140; Ng B, 2010, AAAI CONF ARTIF INTE, P1814; Saha A, 2013, SIAM J OPTIMIZ, V23, P576, DOI 10.1137/110840054; Sonu E, 2015, P I C AUTOMAT PLAN S, P202; Sonu E, 2015, AUTON AGENT MULTI-AG, V29, P455, DOI 10.1007/s10458-014-9261-5; Toussaint M., 2008, UAI, V24, P562; Toussaint Marc, 2006, INT C MACH LEARN ICM, P945; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105; WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060; Wu F., 2013, P 23 INT JOINT C ART, P397; Zeng YF, 2012, J ARTIF INTELL RES, V43, P211, DOI 10.1613/jair.3461	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100102
C	Quanrud, K; Khashabi, D		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Quanrud, Kent; Khashabi, Daniel			Online Learning with Adversarial Delays	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				ALGORITHMS	We study the performance of standard online learning algorithms when the feedback is delayed by an adversary. We show that online-gradient-descent [1] and follow-the-perturbed-leader [2] achieve regret O(root D) in the delayed setting, where D is the sum of delays of each round's feedback. This bound collapses to an optimal O(root T) bound in the usual setting of no delays (where D = T). Our main contribution is to show that standard algorithms for online learning already have simple regret bounds in the most general setting of delayed feedback, making adjustments to the analysis and not to the algorithms themselves. Our results help affirm and clarify the success of recent algorithms in optimization and machine learning that operate in a delayed feedback model.	[Quanrud, Kent; Khashabi, Daniel] Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Quanrud, K (corresponding author), Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.	quanrud2@illinois.edu; khashab2@illinois.edu						Amuru S, 2014, IEEE MILIT COMMUN C, P1528, DOI 10.1109/MILCOM.2014.252; Bertsekas D. P., 1997, PARALLEL DISTRIBUTED; Blum A, 2003, ALGORITHMICA, V36, P249, DOI 10.1007/s00453-003-1015-8; Blum A, 1998, LECT NOTES COMPUT SC, V1442, P306, DOI 10.1007/BFb0029575; Cesa-Bianchi N., 1997, J ASSOC COMPUT MACH, V44, P426; Cover T.M., 1991, MATH FINANC, V1, P1, DOI DOI 10.1111/J.1467-9965.1991.TB00002.X; Crammer K, 2003, J MACH LEARN RES, V3, P1025, DOI 10.1162/153244303322533188; Duchi J., 2013, ADV NEURAL INFORM PR, P2832; Duchi J. C., 2015, CORR; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Hazan E., 2015, INTRO ONLINE CONVEX; He X, 2014, DMOA, P1; Helmbold D., 1997, MACH LEARN J, V27, P61; Joulani P., 2013, P 30 INT C MACH LEAR, V28; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Langford J., 2009, PROC 22 INT C NEURAL, P2331; Liu J, 2015, J MACH LEARN RES, V16, P285; Menache I., 2014, 11 INT C AUT COMP IC; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Riabko D., 2005, THESIS; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Streeter M, 2014, ADV NEURAL INFORM PR, P2915; Takimoto E, 2004, J MACH LEARN RES, V4, P773, DOI 10.1162/1532443041424328; Weinberger MJ, 2002, IEEE T INFORM THEORY, V48, P1959, DOI 10.1109/TIT.2002.1013136; Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101092
C	Nguyen, QP; Low, KH; Jaillet, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Quoc Phong Nguyen; Low, Kian Hsiang; Jaillet, Patrick			Inverse Reinforcement Learning with Locally Consistent Reward Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Existing inverse reinforcement learning (IRL) algorithms have assumed each expert's demonstrated trajectory to be produced by only a single reward function. This paper presents a novel generalization of the IRL problem that allows each trajectory to be generated by multiple locally consistent reward functions, hence catering to more realistic and complex experts' behaviors. Solving our generalized IRL problem thus involves not only learning these reward functions but also the stochastic transitions between them at any state (including unvisited states). By representing our IRL problem with a probabilistic graphical model, an expectation-maximization (EM) algorithm can be devised to iteratively learn the different reward functions and the stochastic transitions between them in order to jointly improve the likelihood of the expert's demonstrated trajectories. As a result, the most likely partition of a trajectory into segments that are generated from different locally consistent reward functions selected by EM can be derived. Empirical evaluation on synthetic and real-world datasets shows that our IRL algorithm outperforms the state-of-the-art EM clustering with maximum likelihood IRL, which is, interestingly, a reduced variant of our approach.	[Quoc Phong Nguyen; Low, Kian Hsiang] Natl Univ Singapore, Dept Comp Sci, Singapore, Singapore; [Jaillet, Patrick] MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA	National University of Singapore; Massachusetts Institute of Technology (MIT)	Nguyen, QP (corresponding author), Natl Univ Singapore, Dept Comp Sci, Singapore, Singapore.	qphong@comp.nus.edu.sg; lowkh@comp.nus.edu.sg; jaillet@mit.edu			Singapore-MIT Alliance for Research and Technology [52 R-252-000-550-592]	Singapore-MIT Alliance for Research and Technology(Singapore-MIT Alliance for Research & Technology Centre (SMART))	This work was partially supported by Singapore-MIT Alliance for Research and Technology Subaward Agreement No. 52 R-252-000-550-592.	[Anonymous], 2015, LNCS; BabeVroman M., 2011, P 28 INT C MACHINE L, P897; Bilmes J., 1998, ICSITR9702 U CAL; CHEN JT, 2013, P UAI, V826, P152, DOI DOI 10.4028/WWW.SCIENTIFIC.NET/AMR.826.152; Choi JD, 2011, J MACH LEARN RES, V12, P691; Choi Jaedeug, 2012, P NIPS, P314; Dvijotham Krishnamurthy, 2010, P 27 INT C MACH LEAR, P335, DOI DOI 10.0RG/PAPERS/571.PDF; Hoang T.N., 2013, P IJCAI, P1394; Hoang TN, 2015, PR MACH LEARN RES, V37, P569; Levine S., 2011, C NEURAL INFORM PROC, V24, P19; Low K. H., 2014, P ECML PKDD NECT TRA; Low KH, 2015, AAAI CONF ARTIF INTE, P2821; Neu G., 2007, P 23 C UNC ART INT, P295; Neu G, 2009, MACH LEARN, V77, P303, DOI 10.1007/s10994-009-5110-1; Newson P., 2009, P 17 ACM SIGSPATIAL, P336; Ng A. Y., 2004, P ICML; Ng Andrew Y., 2000, INT C MACH LEARN, V1, P1; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Ramachandran D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2586; Russell S., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P101, DOI 10.1145/279943.279964; Syed U., 2008, P 25 INT C MACH LEAR, P1032, DOI [DOI 10.1145/1390156.1390286, 10.1145/1390156.1390286]; Syed U., 2007, ADV NEURAL INFORM PR; Hoang TN, 2014, PR MACH LEARN RES, V32, P739; Xu N, 2014, AAAI CONF ARTIF INTE, P2585; Yu JB, 2012, 2012 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE AND INTELLIGENT AGENT TECHNOLOGY (WI-IAT 2012), VOL 2, P478, DOI 10.1109/WI-IAT.2012.216; Ziebart B. D., 2008, AAAI, V8, P1433	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102031
C	Rabinovich, M; Angelino, E; Jordan, MI		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Rabinovich, Maxim; Angelino, Elaine; Jordan, Michael I.			Variational Consensus Monte Carlo	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Practitioners of Bayesian statistics have long depended on Markov chain Monte Carlo (MCMC) to obtain samples from intractable posterior distributions. Unfortunately, MCMC algorithms are typically serial, and do not scale to the large datasets typical of modern machine learning. The recently proposed consensus Monte Carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel [22]. A fixed aggregation function then combines these samples, yielding approximate posterior samples. We introduce variational consensus Monte Carlo (VCMC), a variational Bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target. The resulting objective contains an intractable entropy term; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions. We illustrate the advantages of our algorithm on three inference tasks from the literature, demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step. Our algorithm achieves a relative error reduction (measured against serial MCMC) of up to 39% compared to consensus Monte Carlo on the task of estimating 300-dimensional probit regression parameter expectations; similarly, it achieves an error reduction of 92% on the task of estimating cluster comembership probabilities in a Gaussian mixture model with 8 components in 8 dimensions. Furthermore, these gains come at moderate cost compared to the runtime of serial MCMC-achieving near-ideal speedup in some instances.	[Rabinovich, Maxim; Angelino, Elaine; Jordan, Michael I.] Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Rabinovich, M (corresponding author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.	rabinovich@eecs.berkeley.edu; elaine@eecs.berkeley.edu; jordan@eecs.berkeley.edu	Jordan, Michael I/C-5253-2013		Miller Institute for Basic Research in Science, University of California, Berkeley; Hertz Foundation Fellowship; Google; NSF Graduate Research Fellowship; Amazon; ONR under the MURI program [N00014-11-1-0688]	Miller Institute for Basic Research in Science, University of California, Berkeley(University of California System); Hertz Foundation Fellowship; Google(Google Incorporated); NSF Graduate Research Fellowship(National Science Foundation (NSF)); Amazon; ONR under the MURI program	We thank R.P. Adams, N. Altieri, T. Broderick, R. Giordano, M.J. Johnson, and S.L. Scott for helpful discussions. E.A. is supported by the Miller Institute for Basic Research in Science, University of California, Berkeley. M.R. is supported by a Hertz Foundation Fellowship, generously endowed by Google, and an NSF Graduate Research Fellowship. Support for this project was provided by Amazon and by ONR under the MURI program (N00014-11-1-0688).	Asuncion A. U., 2008, ADV NEURAL INFORM PR, P81; Bardenet Remi, 2014, P 31 INT C MACH LEAR; Bertsekas D. P., 1990, NONLINEAR PROGRAMMIN; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; Campbell T., 2014, 30 C UNC ART INT; Cover T.M, 2006, WILEY SERIES TELECOM; Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137; Doshi-Velez F., 2009, ADV NEURAL INFORM PR, P1294; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Gelman A., 2013, TEXTS STAT SCI SERIE, Vthird, DOI 10.1201/b16018; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; JOHNSON M, 2013, ADV NEURAL INFORM PR, P2715; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Korattikara A., 2014, P 31 INT C MACH LEAR; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; MACLAURIN D., 2014, P 30 C UNC ART INT; Mandt S., 2014, ADV NEURAL INFORM PR, P2438; Neiswanger W., 2014, 30 C UNC ART INT; Nishihara R, 2014, J MACH LEARN RES, V15, P2087; Ranganath R., 2013, P 30 INT C MACH LEAR, V28, P298; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Scott S. L., 2013, BAYES, V250; Strathmann H., 2015, ARXIV150103326; Wang X., 2013, ARXIV PREPRINT ARXIV; Welling Max, 2011, P 28 TH INT C MACH L, P681	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102037
C	Rahman, S; Bruce, NDB		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Rahman, Shafin; Bruce, Neil D. B.			Saliency, Scale and Information: Towards a Unifying Theory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MODEL	In this paper we present a definition for visual saliency grounded in information theory. This proposal is shown to relate to a variety of classic research contributions in scale-space theory, interest point detection, bilateral filtering, and to existing models of visual saliency. Based on the proposed definition of visual saliency, we demonstrate results competitive with the state-of-the art for both prediction of human fixations, and segmentation of salient objects. We also characterize different properties of this model including robustness to image transformations, and extension to a wide range of other data types with 3D mesh models serving as an example. Finally, we relate this proposal more generally to the role of saliency computation in visual information processing and draw connections to putative mechanisms for saliency computation in human vision.	[Rahman, Shafin; Bruce, Neil D. B.] Univ Manitoba, Dept Comp Sci, Winnipeg, MB, Canada	University of Manitoba	Rahman, S (corresponding author), Univ Manitoba, Dept Comp Sci, Winnipeg, MB, Canada.	shafin109@gmail.com; bruce@cs.umanitoba.ca	Rahman, Shafin/N-1939-2019	Rahman, Shafin/0000-0001-7169-0318	NSERC Canada Discovery Grants program; University of Manitoba GETS funding; ONR [N00178-14-Q-4583]	NSERC Canada Discovery Grants program; University of Manitoba GETS funding; ONR(Office of Naval Research)	The authors acknowledge financial support from the NSERC Canada Discovery Grants program, University of Manitoba GETS funding, and ONR grant #N00178-14-Q-4583.	Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596; Andreopoulos A, 2012, IEEE T PATTERN ANAL, V34, P110, DOI 10.1109/TPAMI.2011.91; [Anonymous], 1994, GEOMETRY DRIVEN DIFF, DOI DOI 10.1007/978-94-017-1699-4_3; Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49; Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727; Bruce N. D. B., 2005, ADV NEURAL INF PROCE, P155; Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231; Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401; Florack L. M. J., 1994, Journal of Mathematical Imaging and Vision, V4, P171, DOI 10.1007/BF01249895; Gao DS, 2009, NEURAL COMPUT, V21, P239, DOI 10.1162/neco.2009.11-06-391; Garcia-Diaz A., 2012, J VISION, V12; Harel J., 2006, PAPER PRESENTED INT, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073; Hou X., 2009, ADV NEURAL INFORM PR, P681; Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; JAGERSAND M, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P195, DOI 10.1109/ICCV.1995.466786; Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710; Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855; Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244; Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43; Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; Mokhtarian F, 1998, IEEE T PATTERN ANAL, V20, P1376, DOI 10.1109/34.735812; Paris S, 2006, LECT NOTES COMPUT SC, V3954, P568; Park JC, 2008, J VISION, V8, DOI 10.1167/8.10.8; Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743; Toews M., 2010, CVPR WORKSH IEEE, P111	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102094
C	Rai, P; Hu, CW; Henao, R; Carin, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Rai, Piyush; Hu, Changwei; Henao, Ricardo; Carin, Lawrence			Large-Scale Bayesian Multi-Label Learning via Topic-Based Label Embeddings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We present a scalable Bayesian multi-label learning model based on learning low-dimensional label embeddings. Our model assumes that each label vector is generated as a weighted combination of a set of topics (each topic being a distribution over labels), where the combination weights (i.e., the embeddings) for each label vector are conditioned on the observed feature vector. This construction, coupled with a Bernoulli-Poisson link function for each label of the binary label vector, leads to a model with a computational cost that scales in the number of positive labels in the label matrix. This makes the model particularly appealing for real-world multi-label learning problems where the label matrix is usually very massive but highly sparse. Using a data-augmentation strategy leads to full local conjugacy in our model, facilitating simple and very efficient Gibbs sampling, as well as an Expectation Maximization algorithm for inference. Also, predicting the label vector at test time does not require doing an inference for the label embeddings and can be done in closed form. We report results on several benchmark data sets, comparing our model with various state-of-the art methods.	[Rai, Piyush] IIT Kanpur, CSE Dept, Kanpur, Uttar Pradesh, India; [Rai, Piyush; Hu, Changwei; Henao, Ricardo; Carin, Lawrence] Duke Univ, ECE Dept, Durham, NC 27706 USA	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Kanpur; Duke University	Rai, P (corresponding author), IIT Kanpur, CSE Dept, Kanpur, Uttar Pradesh, India.; Rai, P (corresponding author), Duke Univ, ECE Dept, Durham, NC 27706 USA.	piyush@cse.iitk.ac.in; ch237@duke.edu; r.henao@duke.edu; lcarin@duke.edu			ARO; DARPA; ONR; DOE; NGA	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); ONR(Office of Naval Research); DOE(United States Department of Energy (DOE)); NGA	This research was supported in part by ARO, DARPA, DOE, NGA and ONR	Agrawal R., 2013, WWW; [Anonymous], 2011, IJCAI; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Blei D. M., 2003, JMLR; Chen J., 2013, NIPS; Chen Y., 2012, NIPS; Gibaja E., 2014, WILEY INTERDISCIPLIN; Gibaja E, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2716262; Hsu D.J., 2009, NIPS, P772; Hu C., 2015, UAI; Jun Zhu, 2011, KDD; Kapoor A., 2012, NIPS; Karampatziakis N., 2015, ARXIV150202710; Kim Dae I, 2011, NIPS; Kong X., 2014, SDM; Li X., 2015, AISTATS; MIMNO D, 2008, UAI; Mineiro Paul, 2015, ICLR WORKSH; Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001; Rabinovich M., 2014, ICML; Scott J. G., 2013, ARXIV13060040; Tai F., 2012, NEURAL COMPUTATION; Tipping ME, 2004, LECT NOTES ARTIF INT, V3176, P41; Varma M., 2014, KDD; Yan Yan, 2010, KDD; Yi Zhang, 2011, AISTATS; Yu H.-F., 2014, ICML; Zhou M., 2015, AISTATS; Zhou M., 2012, AISTATS	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101030
C	Ramasamy, D; Madhow, U		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Ramasamy, Dinesh; Madhow, Upamanyu			Compressive spectral embedding: sidestepping the SVD	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Spectral embedding based on the Singular Value Decomposition (SVD) is a widely used "preprocessing" step in many learning tasks, typically leading to dimensionality reduction by projecting onto a number of dominant singular vectors and rescaling the coordinate axes (by a predefined function of the singular value). However, the number of such vectors required to capture problem structure grows with problem size, and even partial SVD computation becomes a bottleneck. In this paper, we propose a low-complexity compressive spectral embedding algorithm, which employs random projections and finite order polynomial expansions to compute approximations to SVD-based embedding. For anmxn matrix with T non-zeros, its time complexity is O ((T + m + n) log(m + n)), and the embedding dimension is O (log(m + n)), both of which are independent of the number of singular vectors whose effect we wish to capture. To the best of our knowledge, this is the first work to circumvent this dependence on the number of singular vectors for general SVD-based embeddings. The key to sidestepping the SVD is the observation that, for downstream inference tasks such as clustering and classification, we are only interested in using the resulting embedding to evaluate pairwise similarity metrics derived from the l(2)-norm, rather than capturing the effect of the underlying matrix on arbitrary vectors as a partial SVD tries to do. Our numerical results on network datasets demonstrate the efficacy of the proposed method, and motivate further exploration of its application to large-scale inference tasks.	[Ramasamy, Dinesh; Madhow, Upamanyu] UC Santa Barbara, ECE Dept, Santa Barbara, CA 93106 USA	University of California System; University of California Santa Barbara	Ramasamy, D (corresponding author), UC Santa Barbara, ECE Dept, Santa Barbara, CA 93106 USA.	dineshr@ece.ucsb.edu; madhow@ece.ucsb.edu	, Dinesh/ABD-3649-2020		DARPA GRAPHS [BAA-12-01]; Systems on Nanoscale Information fabriCs (SONIC), one of the six SRC STARnet Centers - MARCO; DARPA	DARPA GRAPHS; Systems on Nanoscale Information fabriCs (SONIC), one of the six SRC STARnet Centers - MARCO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work is supported in part by DARPA GRAPHS (BAA-12-01) and by Systems on Nanoscale Information fabriCs (SONIC), one of the six SRC STARnet Centers, sponsored by MARCO and DARPA. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies.	ACHLIOPTAS D, 2001, P 20 ACM SIGMOD SIGA; [Anonymous], 1999, ADV NEURAL INFORM PR; Bau III D, 1997, NUMERICAL LINEAR ALG; Candes E., 2008, SIGNAL PROCESSING MA; DiNapoli E., 2013, ARXIV13084275CS; Drineas P., 2005, J MACHINE LEARNING R; Fortunato S., 2010, PHYS REPORTS, V486; Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185; Gobel F., 1974, STOCHASTIC PROCESSES; Halko N., 2011, SIAM REV; Kumar S., 2009, ADV NEURAL INFORM PR; Li Mu, 2010, ICML; Lin F., 2010, P 27 INT C MACH LEAR; Lin Frank, 2012, THESIS; Lovasz L, 1993, BOLYAI MATH STUD, V1, P9; MCCORMICK SF, 1977, LINEAR ALGEBRA APPL, V16, P43, DOI 10.1016/0024-3795(77)90018-0; Nadakuditi RR, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.188701; Scholkopf B., 1997, Artificial Neural Networks - ICANN '97. 7th International Conference Proceedings, P583, DOI 10.1007/BFb0020217; Silver RN, 1996, J COMPUT PHYS, V124, P115, DOI 10.1006/jcph.1996.0048; Spielman D., 2014, SIAM J MATRIX ANAL A, V35; Spielman D. A., 2004, STOC 04; Spielman D. A., 2011, SIAM J COMPUTING; White S., 2005, SDM, V5; Yan D, 2009, P 15 ACM SIGKDD INT; Yan W., 2013, J PARALLEL DISTRIBUT; Yang J., 2012, 2012 IEEE 12 INT C D; Zhang K., 2008, P 25 INT C MACH LEAR	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103030
C	Rao, N; Yu, HF; Ravikumar, P; Dhillon, IS		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Rao, Nikhil; Yu, Hsiang-Fu; Ravikumar, Pradeep; Dhillon, Inderjit S.			Collaborative Filtering with Graph Information: Consistency and Scalable Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				LAPLACIAN EIGENMAPS	Low rank matrix completion plays a fundamental role in collaborative filtering applications, the key idea being that the variables lie in a smaller subspace than the ambient space. Often, additional information about the variables is known, and it is reasonable to assume that incorporating this information will lead to better predictions. We tackle the problem of matrix completion when pairwise relationships among variables are known, via a graph. We formulate and derive a highly efficient, conjugate gradient based alternating minimization scheme that solves optimizations with over 55 million observations up to 2 orders of magnitude faster than state-of-the-art (stochastic) gradient-descent based methods. On the theoretical front, we show that such methods generalize weighted nuclear norm formulations, and derive statistical consistency guarantees. We validate our results on both real and synthetic datasets.	[Rao, Nikhil; Yu, Hsiang-Fu; Ravikumar, Pradeep; Dhillon, Inderjit S.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Rao, N (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	nikhilr@cs.utexas.edu; rofuyu@cs.utexas.edu; paradeepr@cs.utexas.edu; inderjit@cs.utexas.edu			NSF [CCF-1320746]; Intel PhD fellowship; ICES fellowship	NSF(National Science Foundation (NSF)); Intel PhD fellowship; ICES fellowship	This research was supported by NSF grant CCF-1320746. H.-F. Yu acknowledges support from an Intel PhD fellowship. NR was supported by an ICES fellowship.	Abernethy J, 2006, CS0611124 ARXIV; [Anonymous], 2006, P ECAI WORKSHOP RECO; Bach Francis, 2008, ABS08121869 CORR; Belkin M, 2002, ADV NEUR IN, V14, P585; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Dhillon Inderjit S., 2011, ADV NEURAL INFORM PR, P882; Dror Gideon, 2012, PROC KDD CUP 2011; Haeffele BD, 2014, PR MACH LEARN RES, V32, P2007; Jain P., 2013, ABS13060626 CORR; Jamali M., 2010, P 4 ACM C RECOMMENDE, P135, DOI [10.1145/1864708.1864736, DOI 10.1145/1864708.1864736]; Kalofolias Vassilis, 2014, EPFLCONF203064; Li Wu-Jun, 2009, 21 INT JOINT C ART I; Ma Hao, 2011, WSDM, P287; Negahban S, 2012, J MACH LEARN RES, V13, P1665; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Smola AJ, 2003, LECT NOTES ARTIF INT, V2777, P144, DOI 10.1007/978-3-540-45167-9_12; Srebro N., 2010, PROC INT C NEURAL IN, P2056; Vershynin Roman, 2009, LECT NOTES; WRIGHT S, 2013, NIPS WORKSH GREED AL; Xu Miao, 2013, ADV NEURAL INFORM PR, P2301, DOI DOI 10.5555/2999792.2999869; Xu YY, 2013, SIAM J IMAGING SCI, V6, P1758, DOI 10.1137/120887795; Zhao Zhou, 2014, KNOWLEDGE DATA ENG I; Zheng FH, 2012, PROCEEDINGS OF 2012 INTERNATIONAL CONFERENCE ON PUBLIC ADMINISTRATION (8TH), VOL III, P403	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102086
C	Razaviyayn, M; Farnia, F; Tse, D		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Razaviyayn, Meisam; Farnia, Farzan; Tse, David			Discrete Renyi Classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MUTUAL INFORMATION	Consider the binary classification problem of predicting a target variable Y from a discrete feature vector X = (X-1, ... ,X-d). When the probability distribution P (X, Y) is known, the optimal classifier, leading to the minimum misclassification rate, is given by the Maximum A-posteriori Probability (MAP) decision rule. However, in practice, estimating the complete joint distribution P (X, Y) is computationally and statistically impossible for large values of d. Therefore, an alternative approach is to first estimate some low order marginals of the joint probability distribution P (X, Y) and then design the classifier based on the estimated low order marginals. This approach is also helpful when the complete training data instances are not available due to privacy concerns. In this work, we consider the problem of finding the optimum classifier based on some estimated low order marginals of (X, Y). We prove that for a given set of marginals, the minimum Hirschfeld-Gebelein-Renyi (HGR) correlation principle introduced in [1] leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier. Then, under a separability condition, it is shown that the proposed algorithm is equivalent to a randomized linear regression approach. In addition, this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case HGR correlation with the target variable. Our theoretical upper-bound is similar to the recent Discrete Chebyshev Classifier (DCC) approach [2], while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem. Finally, we numerically compare our proposed algorithm with the DCC classifier and show that the proposed algorithm results in better misclassification rate over various UCI data repository datasets.	[Razaviyayn, Meisam; Farnia, Farzan; Tse, David] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA	Stanford University	Razaviyayn, M (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.	meisamr@stanford.edu; farnia@stanford.edu; dntse@stanford.edu		Farnia, Farzan/0000-0002-6049-9232	Stanford University; Center for Science of Information (CSoI), an NSF Science and Technology Center [CCF-0939370]	Stanford University(Stanford University); Center for Science of Information (CSoI), an NSF Science and Technology Center	The authors are grateful to Stanford University supporting a Stanford Graduate Fellowship, and the Center for Science of Information (CSoI), an NSF Science and Technology Center under grant agreement CCF-0939370, for the support during this research.	Anantharam V., 2013, ARXIV13046133; BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224; Bertsimas D., 2000, HDB SEMIDEFINITE PRO, DOI 10.1007/978-1-4615-4381-7_16; De Loera J, 2004, SIAM J COMPUT, V33, P819, DOI 10.1137/S0097539702403803; Eban E, 2014, PR MACH LEARN RES, V32, P1233; Eckstein J., 2011, FDN TRENDS MACH LEAR, V3, P1, DOI DOI 10.1561/2200000016; Farnia F., 2015, ARXIV150406010; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Gebelein H, 1941, Z ANGEW MATH MECH, V21, P364, DOI 10.1002/zamm.19410210604; Globerson A., 2004, P 20 C UNC ART INT, P193; Hirschfeld H, 1935, P CAMB PHILOS SOC, V31, P520, DOI 10.1017/S0305004100013517; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Lanckriet G. R. G., 2003, J MACHINE LEARNING R, V3, P555; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159; Renyi A., 1959, ACTA MATH ACAD SCI H, V10, DOI [DOI 10.1007/BF02024507, 10.1007/BF02024507]; Roughgarden T., 2013, ADV NEURAL INFORM PR, P1043; Shapiro A, 2003, HDBK OPER R, V10, P353; Shapiro A., 2014, LECT STOCHASTIC PROG, V16; Sion M., 1958, PAC J MATH, V8, P171, DOI [10.2140/pjm.1958.8.171, DOI 10.2140/PJM.1958.8.171]	21	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100066
C	Reed, S; Zhang, Y; Zhang, YT; Lee, H		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Reed, Scott; Zhang, Yi; Zhang, Yuting; Lee, Honglak			Deep Visual Analogy-Making	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					In addition to identifying the content within a single image, relating images and generating related images are critical tasks for image understanding. Recently, deep convolutional networks have yielded breakthroughs in predicting image labels, annotations and captions, but have only just begun to be used for generating high-quality images. In this paper we develop a novel deep network trained end-to-end to perform visual analogy making, which is the task of transforming a query image according to an example pair of related images. Solving this problem requires both accurately recognizing a visual relationship and generating a transformed query image accordingly. Inspired by recent advances in language modeling, we propose to solve visual analogies by learning to map images to a neural embedding in which analogical reasoning is simple, such as by vector subtraction and addition. In experiments, our model effectively models visual analogies on several datasets: 2D shapes, animated video game sprites, and 3D car models.	[Reed, Scott; Zhang, Yi; Zhang, Yuting; Lee, Honglak] Univ Michigan, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Reed, S (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	reedscot@umich.edu; yeezhang@umich.edu; yutingzh@umich.edu; honglak@umich.edu			NSF GRFP grant [DGE-1256260]; ONR [N00014-13-1-0762]; NSF CAREER grant [IIS-1453651]; NSF [CMMI-1266184]	NSF GRFP grant(National Science Foundation (NSF)NSF - Office of the Director (OD)); ONR(Office of Naval Research); NSF CAREER grant(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF(National Science Foundation (NSF))	This work was supported in part by NSF GRFP grant DGE-1256260, ONR grant N00014-13-1-0762, NSF CAREER grant IIS-1453651, and NSF grant CMMI-1266184. We thank NVIDIA for donating a Tesla K40 GPU.	[Anonymous], 2015, ICLR WORKSH; Ba J., 2017, P 3 INT C LEARN REPR; Bartha P., 2019, STANFORD ENCY PHILOS; Benard P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461929; Cohen T., 2014, ICML; Cohen T., 2015, ICLR; Desjardins G., 2012, ARXIV12105474; Ding W., 2014, ARXIV14063010; Dollar P., 2007, NIPS; Dosovitskiy A., 2015, CVPR; Fidler S, 2012, NIPS, P620; HERTZMANN A., 2001, SIGGRAPH; Hwang S. J., 2013, NIPS; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Kingma D. P., 2014, NIPS; Kulkarni T. D., 2015, NIPS; Levy O., 2014, CONLL 2014; Memisevic R, 2010, NEURAL COMPUT, V22, P1473, DOI 10.1162/neco.2010.01-09-953; Michalski V., 2014, NIPS; Mikolov T., 2013, ADV NEURAL INF PROCE, V26; Pennington Jeffrey, 2014, EMNLP; Reed S., 2014, ICML; Rifai S., 2012, ECCV; Susskind J., 2011, CVPR; Tang Y., 2013, ICML; Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349; Turney PD, 2006, COMPUT LINGUIST, V32, P379, DOI 10.1162/coli.2006.32.3.379; Yang J., 2015, P ADV NEUR INF PROC; Zhu Zhenyao, 2014, NIPS	29	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101104
C	Richard, E; Goetz, G; Chichilnisky, EJ		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Richard, Emile; Goetz, Georges; Chichilnisky, E. J.			Recognizing retinal ganglion cells in the dark	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Many neural circuits are composed of numerous distinct cell types that perform different operations on their inputs, and send their outputs to distinct targets. Therefore, a key step in understanding neural systems is to reliably distinguish cell types. An important example is the retina, for which present-day techniques for identifying cell types are accurate, but very labor-intensive. Here, we develop automated classifiers for functional identification of retinal ganglion cells, the output neurons of the retina, based solely on recorded voltage patterns on a large scale array. We use per-cell classifiers based on features extracted from electrophysiological images (spatiotemporal voltage waveforms) and interspike intervals (autocorrelations). These classifiers achieve high performance in distinguishing between the major ganglion cell classes of the primate retina, but fail in achieving the same accuracy in predicting cell polarities (ON vs. OFF). We then show how to use indicators of functional coupling within populations of ganglion cells (cross-correlation) to infer cell polarities with a matrix completion algorithm. This can result in accurate, fully automated methods for cell type classification.	[Richard, Emile; Goetz, Georges; Chichilnisky, E. J.] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Richard, E (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	emileric@stanford.edu; ggoetz@stanford.edu; ej@stanford.edu		Chichilnisky, E.J./0000-0002-5613-0248	Stanford Data Science Initiative; National Eye Institute [EY017992, EY018003];  [AFOSR/DARPA FA9550-12-1-0411];  [FA9550-13-1-0036]	Stanford Data Science Initiative; National Eye Institute(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Eye Institute (NEI)); ; 	We are grateful to A. Montanari and D. Palanker for inspiring discussions and valuable comments, and C. Rhoades for labeling the data. ER acknowledges support from grants AFOSR/DARPA FA9550-12-1-0411 and FA9550-13-1-0036. We thank the Stanford Data Science Initiative for financial support and NVIDIA Corporation for the donation of the Tesla K40 GPU we used. Data collection was supported by National Eye Institute grants EY017992 and EY018003 (EJC). Please contact EJC (ej@stanford.edu) for access to the data.	Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Chichilnisky EJ, 2002, J NEUROSCI, V22, P2737, DOI 10.1523/JNEUROSCI.22-07-02737.2002; Coates A., 2011, INT C MACH LEARN ICM, V28; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Dacey D, 2004, COGNITIVE NEUROSCIENCES III, THIRD EDITION, P281; Dacey DM, 2003, CURR OPIN NEUROBIOL, V13, P421, DOI 10.1016/S0959-4388(03)00103-X; DACEY DM, 1992, P NATL ACAD SCI USA, V89, P9666, DOI 10.1073/pnas.89.20.9666; DeVries SH, 1997, J NEUROPHYSIOL, V78, P2048, DOI 10.1152/jn.1997.78.4.2048; Greschner M, 2011, J PHYSIOL-LONDON, V589, P75, DOI 10.1113/jphysiol.2010.193888; Jepson LH, 2014, NEURON, V83, P87, DOI 10.1016/j.neuron.2014.04.044; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Li PH, 2015, J NEUROSCI, V35, P4663, DOI 10.1523/JNEUROSCI.3675-14.2015; Litke AM, 2004, IEEE T NUCL SCI, V51, P1434, DOI 10.1109/TNS.2004.832706; MAIRAL J., 2009, P 26 ANN INT C MACH, P689, DOI [10.1145/1553374.1553463, DOI 10.1145/1553374.1553463]; MASTRONARDE DN, 1983, J NEUROPHYSIOL, V49, P303, DOI 10.1152/jn.1983.49.2.303; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; SILVEIRA LCL, 1991, NEUROSCIENCE, V40, P217, DOI 10.1016/0306-4522(91)90186-R	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100020
C	Rosasco, L; Villa, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Rosasco, Lorenzo; Villa, Silvia			Learning with Incremental Iterative Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				STOCHASTIC-APPROXIMATION; GRADIENT; OPERATORS	Within a statistical learning setting, we propose and study an iterative regularization algorithm for least squares defined by an incremental gradient method. In particular, we show that, if all other parameters are fixed a priori, the number of passes over the data (epochs) acts as a regularization parameter, and prove strong universal consistency, i.e. almost sure convergence of the risk, as well as sharp finite sample bounds for the iterates. Our results are a step towards understanding the effect of multiple epochs in stochastic gradient techniques in machine learning and rely on integrating statistical and optimization results.	[Rosasco, Lorenzo] Univ Genoa, DIBRIS, Genoa, Italy; [Rosasco, Lorenzo; Villa, Silvia] IIT, LCSL, Chicago, IL 60616 USA; [Rosasco, Lorenzo; Villa, Silvia] MIT, Cambridge, MA 02139 USA	University of Genoa; Illinois Institute of Technology; Massachusetts Institute of Technology (MIT)	Rosasco, L (corresponding author), Univ Genoa, DIBRIS, Genoa, Italy.	lrosasco@mit.edu; Silvia.Villa@iit.it			CBMM - NSF STC [CCF-1231216]; MIUR FIRB project [RBFR12M3AC]	CBMM - NSF STC; MIUR FIRB project(Ministry of Education, Universities and Research (MIUR)Fund for Investment in Basic Research (FIRB))	This material is based upon work supported by CBMM, funded by NSF STC award CCF-1231216. and by the MIUR FIRB project RBFR12M3AC. S. Villa is member of GNAMPA of the Istituto Nazionale di Alta Matematica (INdAM).	Bach F., 2014, ARXIV14080361; Bartlett PL, 2007, J MACH LEARN RES, V8, P2347; Bertsekas DP, 1997, SIAM J OPTIMIZ, V7, P913, DOI 10.1137/S1052623495287022; Blanchard G., 2010, ADV NEURAL INFORM PR, V23, P226; Bottou L, 2012, OPTIMIZATION FOR MACHINE LEARNING, P351; Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; Caponnetto A., 2006, FDN COMPUT MATH; Caponnetto A, 2010, ANAL APPL, V8, P161, DOI 10.1142/S0219530510001564; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; De Vito E, 2004, J MACH LEARN RES, V5, P1363; Huang P. - S., 2014, IEEE ICASSP; Jiang WX, 2004, ANN STAT, V32, P13; LeCun Y., 1998, NEURAL NETWORKS TRIC; Nedic A, 2001, SIAM J OPTIMIZ, V12, P109, DOI 10.1137/S1052623499362111; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; NEMIROVSKII AS, 1986, USSR COMP MATH MATH+, V26, P7, DOI 10.1016/0041-5553(86)90002-9; Orabona F., 2014, NIPS P; Pinelis I., 1994, ANN PROBAB, V22, P1679, DOI [10.1214/aop/1176988477, DOI 10.1214/AOP/1176988477]; Polyak B. T., 1987, INTRO OPTIMIZATION; Ramsay J, 2005, FUNCTIONAL DATA ANAL, Vsecond, DOI 10.1007/b98888; Raskutti Garvesh, 2011, 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1318; Smale S, 2005, APPL COMPUT HARMON A, V19, P285, DOI 10.1016/j.acha.2005.03.001; Smale S, 2007, CONSTR APPROX, V26, P153, DOI 10.1007/s00365-006-0659-y; Srebro N., 2012, ARXIV10093896; Steinwart I., 2009, P C LEARN THEOR COLT; Tarres P, 2014, IEEE T INFORM THEORY, V60, P5716, DOI 10.1109/TIT.2014.2332531; Vapnik V.N, 1998, STAT LEARNING THEORY; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2; Ying YM, 2008, FOUND COMPUT MATH, V8, P561, DOI 10.1007/s10208-006-0237-y; Zhang T, 2005, ANN STAT, V33, P1538, DOI 10.1214/009053605000000255	36	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103053
C	Rosenbaum, D; Weiss, Y		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Rosenbaum, Dan; Weiss, Yair			The Return of the Gating Network: Combining Generative Models and Discriminative Training in Natural Image Priors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				SPARSE	In recent years, approaches based on machine learning have achieved state-of-the-art performance on image restoration problems. Successful approaches include both generative models of natural images as well as discriminative training of deep neural networks. Discriminative training of feed forward architectures allows explicit control over the computational cost of performing restoration and therefore often leads to better performance at the same cost at run time. In contrast, generative models have the advantage that they can be trained once and then adapted to any image restoration task by a simple use of Bayes' rule. In this paper we show how to combine the strengths of both approaches by training a discriminative, feed-forward architecture to predict the state of latent variables in a generative model of natural images. We apply this idea to the very successful Gaussian Mixture Model (GMM) of natural images. We show that it is possible to achieve comparable performance as the original GMM but with two orders of magnitude improvement in run time while maintaining the advantage of generative models.	[Rosenbaum, Dan; Weiss, Yair] Hebrew Univ Jerusalem, Sch Comp Sci & Engn, Jerusalem, Israel	Hebrew University of Jerusalem	Rosenbaum, D (corresponding author), Hebrew Univ Jerusalem, Sch Comp Sci & Engn, Jerusalem, Israel.				ISF; Intel ICRI-CI; Gatsby Foundation	ISF(Israel Science Foundation); Intel ICRI-CI; Gatsby Foundation	Support by the ISF, Intel ICRI-CI and the Gatsby Foundation is greatfully acknowledged.	Burger H. C., 2012, COMPUT SCI; Burger HC, 2013, LECT NOTES COMPUT SC, V8142, P121, DOI 10.1007/978-3-642-40602-7_13; Chen YC, 2013, LECT NOTES COMPUT SC, V8142, P271, DOI 10.1007/978-3-642-40602-7_30; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Fanello SR, 2014, PROC CVPR IEEE, P1709, DOI 10.1109/CVPR.2014.221; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; Karklin Y, 2009, NATURE, V457, P83, DOI 10.1038/nature07481; Levi Effi, 2009, THESIS; Levin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2833, DOI 10.1109/CVPR.2011.5995309; Lyu S., 2006, P NIPS, P945; Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452; Rassmusen Carl E, 2006, MINIMIZE M; Roth S, 2005, PROC CVPR IEEE, P860; Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349; Schmidt U, 2010, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2010.5539844; Sun LH, 2013, PROCEEDINGS OF THE 2013 INTERNATIONAL WORKSHOP ON COMPUTER SCIENCE IN SPORTS, P1; Uria Benigno, 2013, P 26 INT C NEURAL IN; Yu GS, 2012, IEEE T IMAGE PROCESS, V21, P2481, DOI 10.1109/TIP.2011.2176743; Zoran D., 2012, ADV NEURAL INFORM PR, V3, P1736; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102002
C	Saade, A; Krzakala, F; Zdeborova, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Saade, Alaa; Krzakala, Florent; Zdeborova, Lenka			Matrix Completion from Fewer Entries: Spectral Detectability and Rank Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					The completion of low rank matrices from few entries is a task with many practical applications. We consider here two aspects of this problem: detectability, i.e. the ability to estimate the rank r reliably from the fewest possible random entries, and performance in achieving small reconstruction error. We propose a spectral algorithm for these two tasks called MaCBetH (for Matrix Completion with the Bethe Hessian). The rank is estimated as the number of negative eigenvalues of the Bethe Hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries. We analyze the performance in a random matrix setting using results from the statistical mechanics of the Hopfield neural network, and show in particular that MaCBetH efficiently detects the rank r of a large n x m m matrix from C(r)r root nm entries, where C(r) is a constant close to 1. We also evaluate the corresponding root-mean-square error empirically and show that MaCBetH compares favorably to other existing approaches.	[Saade, Alaa; Krzakala, Florent] CNRS, Lab Phys Stat, Paris, France; [Krzakala, Florent] Univ Pierre & Marie Curie Paris 06, Sorbonne Univ, F-75005 Paris, France; [Zdeborova, Lenka] CEA Saclay, Inst Phys Theor, F-91191 Gif Sur Yvette, France; [Zdeborova, Lenka] CNRS, UMR 3681, F-91191 Gif Sur Yvette, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Cite; UDICE-French Research Universities; Sorbonne Universite; CEA; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay; Centre National de la Recherche Scientifique (CNRS); CNRS - Institute of Physics (INP); UDICE-French Research Universities; Universite Paris Saclay	Saade, A (corresponding author), CNRS, Lab Phys Stat, Paris, France.		Krzakala, Florent/Q-9652-2019	Krzakala, Florent/0000-0003-2313-2578; Schaich Borg, Jana/0000-0002-0066-761X	European Research Council under the European Union's 7th Framework Programme (FP/2007-2013/ERC Grant) [307087-SPARCS]	European Research Council under the European Union's 7th Framework Programme (FP/2007-2013/ERC Grant)	Our research has received funding from the European Research Council under the European Union's 7th Framework Programme (FP/2007-2013/ERC Grant Agreement 307087-SPARCS).	AMIT DJ, 1985, PHYS REV A, V32, P1007, DOI 10.1103/PhysRevA.32.1007; [Anonymous], [No title captured]; Bordenave C, 2010, RANDOM STRUCT ALGOR, V37, P332, DOI 10.1002/rsa.20313; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Castillo IP, 2004, J PHYS A-MATH GEN, V37, P9087, DOI 10.1088/0305-4470/37/39/003; Gross D, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.150401; Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Johnson SG, 2014, NLOPT NONLINEAR OPTI; Kabashima Yoshiyuki, 2014, ARXIV14021298; Keshavan RH, 2009, ANN ALLERTON CONF, P1216, DOI 10.1109/ALLERTON.2009.5394534; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Mezard M., 2009, INFORM PHYS COMPUTAT, pp 584, DOI [10.1093/acprof:oso/9780198570837.001.0001, DOI 10.1093/ACPROF:OSO/9780198570837.001, DOI 10.1093/ACPROF:OSO/9780198570837.001.0001]; Mooij J.M., 2004, ADV NEURAL INFORM PR, P945; Ricci-Tersenghi F, 2012, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2012/08/P08015; Rogers T, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.031116; SAADE A., 2014, ADV NEURAL INFORM PR, V27, P406; Saade A, 2015, IEEE INT SYMP INFO, P1184, DOI 10.1109/ISIT.2015.7282642; Wemmenhove B, 2003, J PHYS A-MATH GEN, V36, P9617, DOI 10.1088/0305-4470/36/37/302; Yedidia J. S., 2001, TR200116 MITS EL RES; Zdeborova L, 2009, ACTA PHYS SLOVACA, V59, P169, DOI 10.2478/v10155-010-0096-6; Zhang P, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.042120	27	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100072
C	Sarkhel, S; Singla, P; Gogate, V		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Sarkhel, Somdeb; Singla, Parag; Gogate, Vibhav			Fast Lifted MAP Inference via Partitioning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Recently, there has been growing interest in lifting MAP inference algorithms for Markov logic networks (MLNs). A key advantage of these lifted algorithms is that they have much smaller computational complexity than propositional algorithms when symmetries are present in the MLN and these symmetries can be detected using lifted inference rules. Unfortunately, lifted inference rules are sound but not complete and can often miss many symmetries. This is problematic because when symmetries cannot be exploited, lifted inference algorithms ground the MLN, and search for solutions in the much larger propositional space. In this paper, we present a novel approach, which cleverly introduces new symmetries at the time of grounding. Our main idea is to partition the ground atoms and force the inference algorithm to treat all atoms in each part as indistinguishable. We show that by systematically and carefully refining (and growing) the partitions, we can build advanced any-time and any-space MAP inference algorithms. Our experiments on several real-world datasets clearly show that our new algorithm is superior to previous approaches and often finds useful symmetries in the search space that existing lifted inference rules are unable to detect.	[Sarkhel, Somdeb; Gogate, Vibhav] Univ Texas Dallas, Richardson, TX 75083 USA; [Singla, Parag] IIT Delhi, New Delhi, India	University of Texas System; University of Texas Dallas; Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Delhi	Sarkhel, S (corresponding author), Univ Texas Dallas, Richardson, TX 75083 USA.				DARPA Probabilistic Programming for Advanced Machine Learning Program under AFRL prime [FA8750-14-C-0005]	DARPA Probabilistic Programming for Advanced Machine Learning Program under AFRL prime	This work was supported in part by the DARPA Probabilistic Programming for Advanced Machine Learning Program under AFRL prime contract number FA8750-14-C-0005.	Apsel U, 2012, P 28 C UNC ART INT C, P74; Apsel U, 2014, AAAI CONF ARTIF INTE, P2403; Bui H., 2013, P 29 C UNC ART INT; de Salvo Braz R., 2007, THESIS; den Broeck G.V., 2011, IJCAI 2011 P 22 INT, P2178, DOI [10.5591/978-1-57735-516-8/IJCAI11-363, DOI 10.5591/978-1-57735-516-8/IJCAI11-363]; Domingos P., 2009, MARKOV LOGIC INTERFA; Domingos P., 2011, UAI, P256; Gurobi Optimization Inc., 2014, GUROBI OPTIMIZER REF; Hadiji F., 2013, P 27 AAAI C ART INT; Jha A., 2010, P 24 ANN C NEUR INF; Kisynski J., 2009, P 25 C UNC ART INT U, P293; Kok S., 2008, TECHNICAL REPORT; Marinescu R, 2009, ARTIF INTELL, V173, P1457, DOI 10.1016/j.artint.2009.07.003; Mittal H., 2014, ADV NEURAL INFORM PR; Mladenov M., 2014, P 17 INT C ART INT S; Niu F., 2011, P VLDB ENDOWMENT; Noessner J., 2013, P 27 AAAI C ART INT; Poole D., 2003, P INT JOINT C ART IN, P985; Sarkhel S., 2014, P 17 INT C ART INT S; Sarkhel S., 2014, ADV NEURAL INFORM PR; Selman B, 1996, CLIQUES COLORING SAT; VandenBroeck G., 2013, ADV NEURAL INFORM PR; Venugopal D., 2014, MACHINE LEARNING KNO	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103025
C	Scanagatta, M; de Campos, CP; Corani, G; Zaffalon, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Scanagatta, Mauro; de Campos, Cassio P.; Corani, Giorgio; Zaffalon, Marco			Learning Bayesian Networks with Thousands of Variables	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We present a method for learning Bayesian networks from data sets containing thousands of variables without the need for structure constraints. Our approach is made of two parts. The first is a novel algorithm that effectively explores the space of possible parent sets of a node. It guides the exploration towards the most promising parent sets on the basis of an approximated score function that is computed in constant time. The second part is an improvement of an existing ordering-based algorithm for structure optimization. The new algorithm provably achieves a higher score compared to its original formulation. Our novel approach consistently outperforms the state of the art on very large data sets.	[Scanagatta, Mauro; Corani, Giorgio] USI, SUPSI, IDSIA, Lugano, Switzerland; [de Campos, Cassio P.] Queens Univ Belfast, Belfast, Antrim, North Ireland; [Zaffalon, Marco] IDSIA, Lugano, Switzerland	Universita della Svizzera Italiana; Queens University Belfast; Universita della Svizzera Italiana	Scanagatta, M (corresponding author), USI, SUPSI, IDSIA, Lugano, Switzerland.	mauro@idsia.ch; c.decampos@qub.ac.uk; giorgio@idsia.ch; zaffalon@idsia.ch	Zaffalon, Marco/M-7035-2017	Zaffalon, Marco/0000-0001-8908-1502	Swiss NSF [200021_146606 / 1]	Swiss NSF(Swiss National Science Foundation (SNSF))	Work partially supported by the Swiss NSF grant n. 200021_146606 / 1.	[Anonymous], 2003, ICML; Bartlett M., 2015, ARTIFICIAL INTELLIGE; CHICKERING DM, 2003, P 19 C UNC ART INT, P124; COOPER GF, 1992, MACH LEARN, V9, P309, DOI 10.1023/A:1022649401552; Cussens J, 2011, UNCERTAINTY ARTIFICI, P153; Cussens J., 2013, IJCAI 2013 TUTORIAL; de Campos C.P., 2009, P 26 ANN INT C MACHI, P113; de Campos CP, 2011, J MACH LEARN RES, V12, P663; Haaren J. V., 2012, P 26 AAAI C ART INT; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; Jaakkola T, 2010, P 13 INT C ART INT S, P358; Koivisto M, 2004, J MACH LEARN RES, V5, P549; Koivisto M, 2006, LECT NOTES ARTIF INT, V4005, P289, DOI 10.1007/11776420_23; Lowd D., 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P334, DOI 10.1109/ICDM.2010.128; McGill WJ., 1954, PSYCHOMETRIKA, V19, P97, DOI 10.1007/BF02289159; Raftery AE, 1995, SOCIOL METHODOL, V25, P111, DOI 10.2307/271063; Silander T., 2006, P 22 C UNC ART INT, P445; Teyssier M., 2005, P 21 C UNC ART INT, P584; Yuan C, 2012, P 28 C UNC ART INT U; Yuan CH, 2013, J ARTIF INTELL RES, V48, P23, DOI 10.1613/jair.4039	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101063
C	Shvartsman, M; Srivastava, V; Cohen, JD		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Shvartsman, Michael; Srivastava, Vaibhav; Cohen, Jonathan D.			A Theory of Decision Making Under Dynamic Context	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				TIME; PROBABILITY; CHOICE; MODELS; CORTEX	The dynamics of simple decisions are well understood and modeled as a class of random walk models [e.g. 1-4]. However, most real-life decisions include a dynamically-changing influence of additional information we call context. In this work, we describe a computational theory of decision making under dynamically shifting context. We show how the model generalizes the dominant existing model of fixed-context decision making [2] and can be built up from a weighted combination of fixed-context decisions evolving simultaneously. We also show how the model generalizes recent work on the control of attention in the Flanker task [5]. Finally, we show how the model recovers qualitative data patterns in another task of longstanding psychological interest, the AX Continuous Performance Test [6], using the same model parameters.	[Shvartsman, Michael; Cohen, Jonathan D.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA; [Srivastava, Vaibhav] Princeton Univ, Dept Mech & Aerosp Engn, Princeton, NJ 08544 USA	Princeton University; Princeton University	Shvartsman, M (corresponding author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.	ms44@princeton.edu; vaibhavs@princeton.edu; jdc@princeton.edu						Bogacz R, 2007, NEURAL COMPUT, V19, P442, DOI 10.1162/neco.2007.19.2.442; Bogacz R, 2006, PSYCHOL REV, V113, P700, DOI 10.1037/0033-295X.113.4.700; Braver TS, 2012, TRENDS COGN SCI, V16, P106, DOI 10.1016/j.tics.2011.12.010; Drugowitsch J, 2012, J NEUROSCI, V32, P3612, DOI 10.1523/JNEUROSCI.4010-11.2012; Frazier P. I., 2008, ADV NEURAL INFORM PR, V2008, P1; GRATTON G, 1988, J EXP PSYCHOL HUMAN, V14, P331, DOI 10.1037/0096-1523.14.3.331; Hanks TD, 2011, J NEUROSCI, V31, P6339, DOI 10.1523/JNEUROSCI.5613-10.2011; Hanks Timothy D., 2015, NATURE; Kira S, 2015, NEURON, V85, P861, DOI 10.1016/j.neuron.2015.01.007; Laming D. R. J., 1968, INFORM THEORY CHOICE; LIU Y, 1992, IEEE T INFORM THEORY, V38, P177, DOI 10.1109/18.108268; Liu YS, 2009, NEURAL COMPUT, V21, P1520, DOI 10.1162/neco.2009.03-07-495; Lositsky O., 2015, MULT C REINF LEARN D, P103; Norris D, 2006, PSYCHOL REV, V113, P327, DOI 10.1037/0033-295X.113.2.327; O'Reilly RC, 2006, NEURAL COMPUT, V18, P283, DOI 10.1162/089976606775093909; RATCLIFF R, 1978, PSYCHOL REV, V85, P59, DOI 10.1037//0033-295X.85.2.59; ServanSchreiber D, 1996, ARCH GEN PSYCHIAT, V53, P1105; Sheppard JP, 2013, J VISION, V13, DOI 10.1167/13.6.4; SIMON JR, 1963, ERGONOMICS, V6, P99, DOI 10.1080/00140136308930679; Srivastava N., 2012, ADV NEURAL INFORM PR, V25, P2312; Stroop JR, 1935, J EXP PSYCHOL, V18, P643, DOI 10.1037/h0054651; Turner BM, 2015, PSYCHOL REV, V122, P312, DOI 10.1037/a0038894; Usher M, 2001, PSYCHOL REV, V108, P550, DOI 10.1037//0033-295X.108.3.550; van Vugt MK, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00106; WALD A, 1948, ANN MATH STAT, V19, P326, DOI 10.1214/aoms/1177730197; Wong KF, 2006, J NEUROSCI, V26, P1314, DOI 10.1523/JNEUROSCI.3733-05.2006; Yu AJ, 2009, J EXP PSYCHOL HUMAN, V35, P700, DOI 10.1037/a0013553	27	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100018
C	Slawski, M; Li, P; Hein, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Slawski, Martin; Li, Ping; Hein, Matthias			Regularization-Free Estimation in Trace Regression with Symmetric Positive Semidefinite Matrices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				LEAST-SQUARES; RECOVERY	Trace regression models have received considerable attention in the context of matrix completion, quantum state tomography, and compressed sensing. Estimation of the underlying matrix from regularization-based approaches promoting low-rankedness, notably nuclear norm regularization, have enjoyed great popularity. In this paper, we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite (spd) and the design satisfies certain conditions. In this situation, simple least squares estimation subject to an spd constraint may perform as well as regularization-based approaches with a proper choice of regularization parameter, which entails knowledge of the noise level and/or tuning. By contrast, constrained least squares estimation comes without any tuning parameter and may hence be preferred due to its simplicity.	[Slawski, Martin; Li, Ping] Rutgers State Univ, Dept Stat & Biostat, Dept Comp Sci, Piscataway, NJ 08854 USA; [Hein, Matthias] Saarland Univ, Dept Math, Dept Comp Sci, Saarbrucken, Germany	Rutgers State University New Brunswick; Saarland University	Slawski, M (corresponding author), Rutgers State Univ, Dept Stat & Biostat, Dept Comp Sci, Piscataway, NJ 08854 USA.	martin.slawski@rutgers.edu; pingli@stat.rutgers.edu; hein@cs.uni-saarland.de			 [NSF-DMS-1444124];  [NSF-III-1360971];  [ONR-N00014-13-1-0764];  [AFOSR-FA9550-13-1-0137]	; ; ; 	The work of Martin Slawski and Ping Li is partially supported by NSF-DMS-1444124, NSF-III-1360971, ONR-N00014-13-1-0764, and AFOSR-FA9550-13-1-0137.	Amelunxen D, 2014, INF INFERENCE, V3, P224, DOI 10.1093/imaiai/iau005; [Anonymous], 2002, LEARNING KERNELS; Cai TT, 2015, ANN STAT, V43, P102, DOI 10.1214/14-AOS1267; Candes E., 2009, FDN COMPUTATIONAL MA, V9, P2053; Candes EJ, 2014, FOUND COMPUT MATH, V14, P1017, DOI 10.1007/s10208-013-9162-z; Demanet L, 2014, J FOURIER ANAL APPL, V20, P199, DOI 10.1007/s00041-013-9305-2; Gross D, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.150401; Horn R. A., 1986, MATRIX ANAL; Kabanva M., 2015, ARXIV150707184; KLIBANOV MV, 1995, INVERSE PROBL, V11, P1, DOI 10.1088/0266-5611/11/1/001; Koltchinskii V, 2011, ANN STAT, V39, P2302, DOI 10.1214/11-AOS894; Meinshausen N, 2013, ELECTRON J STAT, V7, P1607, DOI 10.1214/13-EJS818; Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Rohde A, 2011, ANN STAT, V39, P887, DOI 10.1214/10-AOS860; Slawski M., 2015, ADV NEURAL INF PROCE, V28, P2782; Slawski M, 2013, ELECTRON J STAT, V7, P3004, DOI 10.1214/13-EJS868; Srebro N., 2005, P ADV NEURAL INFORM; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tropp J. A., 2014, USER FRIENDLY TOOLS; Vershynin R., 2012, J THEORETICAL PROBAB, V153, P405; Wang M, 2011, IEEE T SIGNAL PROCES, V59, P1007, DOI 10.1109/TSP.2010.2089624; Williams CKI, 2001, ADV NEUR IN, V13, P682	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100094
C	Slawski, M; Li, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Slawski, Martin; Li, Ping			b-bit Marginal Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				RECOVERY	We consider the problem of sparse signal recovery from m linear measurements quantized to b bits. b-bit Marginal Regression is proposed as recovery algorithm. We study the question of choosing b in the setting of a given budget of bits B = m . b and derive a single easy-to-compute expression characterizing the trade-off between m and b. The choice b = 1 turns out to be optimal for estimating the unit vector corresponding to the signal for any level of additive Gaussian noise before quantization as well as for adversarial noise. For b >= 2, we show that Lloyd-Max quantization constitutes an optimal quantization scheme and that the norm of the signal can be estimated consistently by maximum likelihood by extending [15].	[Slawski, Martin; Li, Ping] Rutgers State Univ, Dept Comp Sci, Dept Stat & Biostat, New Brunswick, NJ 08901 USA	Rutgers State University New Brunswick	Slawski, M (corresponding author), Rutgers State Univ, Dept Comp Sci, Dept Stat & Biostat, New Brunswick, NJ 08901 USA.	martin.slawski@rutgers.edu; pingli@stat.rutgers.edu			 [NSF-Bigdata-1419210];  [NSF-III-1360971];  [ONR-N00014-13-1-0764];  [AFOSR-FA9550-13-1-0137]	; ; ; 	This work is partially supported by NSF-Bigdata-1419210, NSF-III-1360971, ONR-N00014-13-1-0764, and AFOSR-FA9550-13-1-0137.	[Anonymous], 2012, INTRO NONASYMPTOTIC; Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002; Boufounos P., 2008, INFORM SCI SYSTEMS; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Chen Sheng, 2015, AISTATS; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Genovese CR, 2012, J MACH LEARN RES, V13, P2107; Gopi Sivakant, 2013, ICML; Jacques L., 2013, ARXIV13051786; Jacques L, 2013, IEEE T INFORM THEORY, V59, P2082, DOI 10.1109/TIT.2012.2234823; Jacques L, 2011, IEEE T INFORM THEORY, V57, P559, DOI 10.1109/TIT.2010.2093310; KIEFFER JC, 1983, IEEE T INFORM THEORY, V29, P42, DOI 10.1109/TIT.1983.1056622; Laska J., 2011, ARXIV11103450; Laska JN, 2011, APPL COMPUT HARMON A, V31, P429, DOI 10.1016/j.acha.2011.02.002; Li P., 2014, COLT; Li P., 2015, ARXIV150302346; Li P., 2015, ARXIV150306876; Liu J, 2014, APPL COMPUT HARMON A, V37, P325, DOI 10.1016/j.acha.2013.12.006; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; MAX J, 1960, IRE T INFORM THEOR, V6, P7, DOI 10.1109/TIT.1960.1057548; Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002; Plan Y, 2013, COMMUN PUR APPL MATH, V66, P1275, DOI 10.1002/cpa.21442; Plan Y, 2013, IEEE T INFORM THEORY, V59, P482, DOI 10.1109/TIT.2012.2207945; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018; Zhang CH, 2012, STAT SCI, V27, P576, DOI 10.1214/12-STS399; Zhang L., 2014, ICML; Zhang T, 2011, IEEE T INFORM THEORY, V57, P4689, DOI 10.1109/TIT.2011.2146690; Zhu R., 2015, ICML	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100106
C	Smith, D; Gogate, V		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Smith, David; Gogate, Vibhav			Bounding the Cost of Search-Based Lifted Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Recently, there has been growing interest in systematic search-based and importance sampling-based lifted inference algorithms for statistical relational models (SRMs). These lifted algorithms achieve significant complexity reductions over their propositional counterparts by using lifting rules that leverage symmetries in the relational representation. One drawback of these algorithms is that they use an inference-blind representation of the search space, which makes it difficult to efficiently pre-compute tight upper bounds on the exact cost of inference without running the algorithm to completion. In this paper, we present a principled approach to address this problem. We introduce a lifted analogue of the propositional And/Or search space framework, which we call a lifted And/Or schematic. Given a schematic-based representation of an SRM, we show how to efficiently compute a tight upper bound on the time and space cost of exact inference from a current assignment and the remaining schematic. We show how our bounding method can be used within a lifted importance sampling algorithm, in order to perform effective Rao-Blackwellisation, and demonstrate experimentally that the Rao-Blackwellised version of the algorithm yields more accurate estimates on several real-world datasets.	[Smith, David; Gogate, Vibhav] Univ Texas Dallas, 800 W Campbell Rd, Richardson, TX 75080 USA	University of Texas System; University of Texas Dallas	Smith, D (corresponding author), Univ Texas Dallas, 800 W Campbell Rd, Richardson, TX 75080 USA.	dbs014200@utdallas.edu; vibhav.gogate@utdallas.edu			Defense Advanced Research Projects Agency (DARPA) Probabilistic Programming for Advanced Machine Learning Program under Air Force Research Laboratory (AFRL) [FA8750-14-C-0005]	Defense Advanced Research Projects Agency (DARPA) Probabilistic Programming for Advanced Machine Learning Program under Air Force Research Laboratory (AFRL)	We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) Probabilistic Programming for Advanced Machine Learning Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-14-C-0005. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of DARPA, AFRL, or the US government.	Bidyuk B, 2007, J ARTIF INTELL RES, V28, P1, DOI 10.1613/jair.2149; Braz RD, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1319; Casella G, 1996, BIOMETRIKA, V83, P81, DOI 10.1093/biomet/83.1.81; Chavira M, 2008, ARTIF INTELL, V172, P772, DOI 10.1016/j.artint.2007.11.002; Dechter R, 2007, ARTIF INTELL, V171, P73, DOI 10.1016/j.artint.2006.11.003; den Broeck G.V., 2011, IJCAI 2011 P 22 INT, P2178, DOI [10.5591/978-1-57735-516-8/IJCAI11-363, DOI 10.5591/978-1-57735-516-8/IJCAI11-363]; Domingos P., 2011, UAI, P256; Genesereth M. R., 2013, INTRO LOGIC, VSecond; Gogate V., 2012, AAAI; Gogate Vibhav, 2010, STAT RELATIONAL ARTI; Jha A., 2010, P 24 ANN C NEUR INF, P973; Kersting Kristian, 2008, PROBABILISTIC INDUCT; Milch B., 2007, STAT RELATIONAL LEAR; Niepert M., 2013, AAAI, P725; Niepert M., 2012, UAI 2012 WORKSH STAT; Poole D., 2003, P INT JOINT C ART IN, P985; Poole D., 2011, ARXIV11074035; Sang Tian, 2005, AAAI05 P 20 NAT C AR, V1, P475; Suciu Dan, 2010, NIPS; Taghipour N., 2013, ADV NEURAL INFORM PR, V26, P1052; Venugopal D., 2012, NIPS, V3, P1655	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103019
C	Sriperumbudur, BK; Szabo, Z		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Sriperumbudur, Bharath K.; Szabo, Zoltan			Optimal Rates for Random Fourier Features	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				EMPIRICAL CHARACTERISTIC FUNCTION	Kernel methods represent one of the most powerful tools in machine learning to tackle problems expressed in terms of function values and derivatives due to their capability to represent and model complex relations. While these methods show good versatility, they are computationally intensive and have poor scalability to large data as they require operations on Gram matrices. In order to mitigate this serious computational limitation, recently randomized constructions have been proposed in the literature, which allow the application of fast linear algorithms. Random Fourier features (RFF) are among the most popular and widely applied constructions: they provide an easily computable, low-dimensional feature representation for shift-invariant kernels. Despite the popularity of RFFs, very little is understood theoretically about their approximation quality. In this paper, we provide a detailed finite-sample theoretical analysis about the approximation quality of RFFs by (i) establishing optimal (in terms of the RFF dimension, and growing set size) performance guarantees in uniform norm, and (ii) presenting guarantees in L-r (1 <= r < infinity) norms. We also propose an RFF approximation to derivatives of a kernel with a theoretical study on its approximation quality.	[Sriperumbudur, Bharath K.] Penn State Univ, Dept Stat, University Pk, PA 16802 USA; [Szabo, Zoltan] UCL, Sainsbury Wellcome Ctr, Gatsby Unit, CSML, London W1T 4JG, England	Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park; University of London; University College London	Sriperumbudur, BK (corresponding author), Penn State Univ, Dept Stat, University Pk, PA 16802 USA.	bks18@psu.edu; zoltan.szabo@gatsby.ucl.ac.uk			Gatsby Charitable Foundation	Gatsby Charitable Foundation	Z. Szabo wishes to thank the Gatsby Charitable Foundation for its generous support.	Alaoui A., 2015, NIPS; [Anonymous], PROBAB THEORY REL; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Cotter A., 2011, TECHNICAL REPORT; CSORGO S, 1981, Z WAHRSCHEINLICHKEIT, V55, P203, DOI 10.1007/BF00535160; CSORGO S, 1983, ACTA SCI MATH, V45, P141; Drineas P, 2005, J MACH LEARN RES, V6, P2153; FEUERVERGER A, 1977, ANN STAT, V5, P88, DOI 10.1214/aos/1176343742; Folland G.B., 1999, REAL ANAL MODERN TEC, VSecond; Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219; Lopez-Paz D, 2015, PR MACH LEARN RES, V37, P1452; Maji S, 2013, IEEE T PATTERN ANAL, V35, P66, DOI 10.1109/TPAMI.2012.62; Oliva JB, 2015, JMLR WORKSH CONF PRO, V38, P717; Peters J, 2017, ADAPT COMPUT MACH LE; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Rahimi A, 2008, ANN ALLERTON CONF, P555, DOI 10.1109/ALLERTON.2008.4797607; Rosasco L., 2010, J MACHINE LEARNING R, P653; Rosasco L, 2013, J MACH LEARN RES, V14, P1665; Shi L, 2010, J COMPUT APPL MATH, V233, P3046, DOI 10.1016/j.cam.2009.11.059; Shi Q, 2009, INT C ART INT STAT, P496; Sriperumbudur B. K., 2014, TECHNICAL REPORT; Strathmann H, 2015, NIPS; Sutherland DJ, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P862; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Wendland H., 2005, SCATTERED DATA APPRO; Williams CKI, 2001, ADV NEUR IN, V13, P682; Ying YM, 2012, ADV COMPUT MATH, V37, P355, DOI 10.1007/s10444-011-9211-6; Zhou DX, 2008, J COMPUT APPL MATH, V220, P456, DOI 10.1016/j.cam.2007.08.023	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100108
C	Steinhardt, J; Liang, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Steinhardt, Jacob; Liang, Percy			Learning with Relaxed Supervision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					For weakly-supervised problems with deterministic constraints between the latent variables and observed output, learning necessitates performing inference over latent variables conditioned on the output, which can be intractable no matter how simple the model family is. Even finding a single latent variable setting that satisfies the constraints could be difficult; for instance, the observed output may be the result of a latent database query or graphics program which must be inferred. Here, the difficulty lies in not the model but the supervision, and poor approximations at this stage could lead to following the wrong learning signal entirely. In this paper, we develop a rigorous approach to relaxing the supervision, which yields asymptotically consistent parameter estimates despite altering the supervision. Our approach parameterizes a family of increasingly accurate relaxations, and jointly optimizes both the model and relaxation parameters, while formulating constraints between these parameters to ensure efficient inference. These efficiency constraints allow us to learn in otherwise intractable settings, while asymptotic consistency ensures that we always follow a valid learning signal.	[Steinhardt, Jacob; Liang, Percy] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Steinhardt, J (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	jsteinhardt@cs.stanford.edu; pliang@cs.stanford.edu		Steinhardt, Jacob Noah/0000-0002-0257-3860	Fannie & John Hertz Fellowship; NSF Graduate Research Fellowship; Microsoft Research Faculty Fellowship	Fannie & John Hertz Fellowship; NSF Graduate Research Fellowship(National Science Foundation (NSF)); Microsoft Research Faculty Fellowship(Microsoft)	The first author was supported by a Fannie & John Hertz Fellowship and an NSF Graduate Research Fellowship. The second author was supported by a Microsoft Research Faculty Fellowship. We are also grateful to the referees for their valuable comments.	[Anonymous], INT C MACH LEARN ICM; Artzi Y, 2013, T ASSOC COMPUT LING, V1, P49, DOI DOI 10.1162/TACL_A_00209; BESAG J, 1975, J ROY STAT SOC D-STA, V24, P179, DOI 10.2307/2987782; Chang Angel X., 2014, EMPIRICAL METHODS NA; Chang Ming-Wei, 2007, ACL, P280; Clarke James, 2010, P 14 C COMP NAT LANG, P18; Druck G., 2008, SIGIR, P595, DOI [10.1145/1390334.1390436, DOI 10.1145/1390334.1390436]; Fisher M., 2012, ACM SIGGRAPH ASIA, P12; Ganchev K, 2010, J MACH LEARN RES, V11, P2001; Gill PE, 2002, SIAM J OPTIMIZ, V12, P979, DOI 10.1137/S1052623499350013; Gimpel Kevin, 2010, HUMAN LANGUAGE TECHN, P733; Graca J., 2008, NIPS; Gulwani S, 2011, ACM SIGPLAN NOTICES, V46, P317, DOI 10.1145/1925844.1926423; He H., 2013, EMNLP; He H., 2012, ICML INF WORKSH; Jiang J, 2012, ADV NEURAL INFORM PR, V25; Liang P, 2008, P 25 INT C MACH LEAR, P584, DOI [10.1145/1390156.1390230, DOI 10.1145/1390156.1390230]; Liang Percy, 2011, P 49 ANN M ASS COMPU, P590; Mann G., 2008, P ACL, P870; Mansinghka V., 2013, ADV NEURAL INFORM PR; Mintz M., 2009, P ACL, P1003, DOI DOI 10.3115/1690219.1690287; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Ng A. Y., 1999, INT C MACH LEARN ICM; Nielsen F., 2009, ARXIV09114863; Riedel S, 2010, LECT NOTES ARTIF INT, V6323, P148, DOI 10.1007/978-3-642-15939-8_10; Shi T., 2015, AISTATS; Steinhardt J., 2015, ICML; Vaart A. W., 1998, ASYMPTOTIC STAT; Weiss D. J., 2013, ADV NEURAL INFORM PR, P953	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100051
C	Sugiyama, M; Borgwardt, KM		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Sugiyama, Mahito; Borgwardt, Karsten M.			Halting in RandomWalk Kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Random walk kernels measure graph similarity by counting matching walks in two graphs. In their most popular form of geometric random walk kernels, longer walks of length k are downweighted by a factor of lambda(k) (lambda < 1) to ensure convergence of the corresponding geometric series. We know from the fi eld of link prediction that this downweighting often leads to a phenomenon referred to as halting: Longer walks are downweighted so much that the similarity score is completely dominated by the comparison of walks of length 1. This is a naive kernel between edges and vertices. We theoretically show that halting may occur in geometric random walk kernels. We also empirically quantify its impact in simulated datasets and popular graph classi fi cation benchmark datasets. Our fi ndings promise to be instrumental in future graph kernel development and applications of random walk kernels.	[Sugiyama, Mahito] Osaka Univ, ISIR, Suita, Osaka 565, Japan; [Sugiyama, Mahito] JST, PRESTO, Kawaguchi, Saitama, Japan; [Borgwardt, Karsten M.] Swiss Fed Inst Technol, D BSSE, Basel, Switzerland	Osaka University; Japan Science & Technology Agency (JST); Swiss Federal Institutes of Technology Domain; ETH Zurich	Sugiyama, M (corresponding author), Osaka Univ, ISIR, Suita, Osaka 565, Japan.	mahito@ar.sanken.osaka-u.ac.jp; karsten.borgwardt@bsse.ethz.ch	Sugiyama, Mahito/AAD-9213-2019	Sugiyama, Mahito/0000-0001-5907-9831	JSPS KAKENHI [26880013]; Alfried Krupp von Bohlen und Halbach-Stiftung; Marie Curie Initial Training Network MLPM2012 [316861]; SNSF Starting Grant 'Significant Pattern Mining'	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); Alfried Krupp von Bohlen und Halbach-Stiftung; Marie Curie Initial Training Network MLPM2012; SNSF Starting Grant 'Significant Pattern Mining'	This work was supported by JSPS KAKENHI Grant Number 26880013 (MS), the Alfried Krupp von Bohlen und Halbach-Stiftung (KB), the SNSF Starting Grant 'Significant Pattern Mining' (KB), and the Marie Curie Initial Training Network MLPM2012, Grant No. 316861 (KB).	Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007; Borgwardt KM, 2007, THESIS; Brualdi R. A., 2011, MUTUALLY BENEFICIAL; Costa Fabrizio, 2010, INT C MACHINE LEARNI, P255, DOI DOI 10.1016/J.NEUROPHARM.2007.07.003; Gartner T, 2003, LECT NOTES ARTIF INT, V2777, P129, DOI 10.1007/978-3-540-45167-9_11; Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799; Inokuchi A., 2003, INT C MACHINE LEARNI, P321; Katz L., 1953, PSYCHOMETRIKA, V18, P39, DOI [10.1007/BF02289026, DOI 10.1007/BF02289026, DOI 10.1016/j.clinph.2016.12.016]; Kriege N, 2014, IEEE DATA MINING, P881, DOI 10.1109/ICDM.2014.129; Liben-Nowell D, 2007, J AM SOC INF SCI TEC, V58, P1019, DOI 10.1002/asi.20591; MAHE P, 2004, P 21 INT C MACH LEAR; Shervashidze N., 2009, NEURIPS, P1660; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201	14	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100056
C	Sun, K; Wang, J; Kalousis, A; Marchand-Maillet, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Sun, Ke; Wang, Jun; Kalousis, Alexandros; Marchand-Maillet, Stephane			Space-Time Local Embeddings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Space-time is a profound concept in physics. This concept was shown to be useful for dimensionality reduction. We present basic definitions with interesting counter-intuitions. We give theoretical propositions to show that space-time is a more powerful representation than Euclidean space. We apply this concept to manifold learning for preserving local information. Empirical results on non-metric datasets show that more information can be preserved in space-time.	[Sun, Ke; Kalousis, Alexandros; Marchand-Maillet, Stephane] Univ Geneva, Comp Vis & Multimedia Lab, Viper Grp, Geneva, Switzerland; [Wang, Jun] Expedia, Geneva, Switzerland; [Kalousis, Alexandros] Univ Appl Sci, Business Informat Dept, Delemont, Switzerland	University of Geneva	Sun, K (corresponding author), Univ Geneva, Comp Vis & Multimedia Lab, Viper Grp, Geneva, Switzerland.	sunk.edu@gmail.com; jwang1@expedia.com; Alexandros.Kalousis@hesge.ch; Stephane.Marchand-Maillet@unige.ch	SUN, Ke/X-8444-2019	SUN, Ke/0000-0001-6263-7355; Marchand-Maillet, Stephane/0000-0002-4875-6101; Kalousis, Alexandros/0000-0001-6282-0686	Department of Computer Science, University of Geneva; Swiss National Science Foundation Project MAAYA [144238]	Department of Computer Science, University of Geneva; Swiss National Science Foundation Project MAAYA(Swiss National Science Foundation (SNSF))	This work has been supported be the Department of Computer Science, University of Geneva, in collaboration with Swiss National Science Foundation Project MAAYA (Grant number 144238).	[Anonymous], 1998, U S FLORIDA WORD ASS; Cook JA., 2007, P 11 INT C ART INT S, Vvol 2, P67; GOLDFARB L, 1984, PATTERN RECOGN, V17, P575, DOI 10.1016/0031-3203(84)90056-6; Hinton Geoffrey, 2002, ADV NEURAL INFORM PR, V15, P833, DOI DOI 10.1109/TSMCB.2011.2106208; Jost J, 2011, RIEMANNIAN GEOMETRY, DOI DOI 10.1007/978-3-642-21298-7; Laub J, 2004, J MACH LEARN RES, V5, P801; Laub J., 2007, ADV NEURAL INFORM PR, P777; Lawrence N, 2011, P INT C ART INT STAT, P51; Leskovec J, 2007, ACM T WEB, V1, DOI 10.1145/1232722.1232727; Lunga D, 2013, IEEE T GEOSCI REMOTE, V51, P857, DOI 10.1109/TGRS.2012.2205004; O'Neill B., 1983, PURE APPL MATH; Pekalska E, 2005, DISSIMILARITY REPRES; van der Maaten L, 2012, MACH LEARN, V87, P33, DOI 10.1007/s10994-011-5273-4; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Weinberger K. Q., 2004, P 21 INT C MACH LEAR, P839; Wilson RC, 2010, PROC CVPR IEEE, P1903, DOI 10.1109/CVPR.2010.5539863; Zeger K., 1994, Proceedings. 1994 IEEE International Symposium on Information Theory (Cat. No.94CH3467-8), DOI 10.1109/ISIT.1994.394879	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103009
C	Sun, RY; Hong, MY		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Sun, Ruoyu; Hong, Mingyi			Improved Iteration Complexity Bounds of Cyclic Block Coordinate Descent for Convex Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				CONVERGENCE; OPTIMIZATION; MINIMIZATION	The iteration complexity of the block-coordinate descent (BCD) type algorithm has been under extensive investigation. It was recently shown that for convex problems the classical cyclic BCGD (block coordinate gradient descent) achieves an O(1/r) complexity (r is the number of passes of all blocks). However, such bounds are at least linearly depend on K (the number of variable blocks), and are at least K times worse than those of the gradient descent (GD) and proximal gradient (PG) methods. In this paper, we close such theoretical performance gap between cyclic BCD and GD/PG. First we show that for a family of quadratic nonsmooth problems, the complexity bounds for cyclic Block Coordinate Proximal Gradient (BCPG), a popular variant of BCD, can match those of the GD/PG in terms of dependency on K (up to a log(2) (K) factor). Second, we establish an improved complexity bound for Coordinate Gradient Descent (CGD) for general convex problems which can match that of GD in certain scenarios. Our bounds are sharper than the known bounds as they are always at least K times worse than GD. Our analyses do not depend on the update order of block variables inside each cycle, thus our results also apply to BCD methods with random permutation (random sampling without replacement, another popular variant).	[Sun, Ruoyu] Stanford Univ, Dept Management Sci & Engn, Stanford, CA 94305 USA; [Hong, Mingyi] Iowa State Univ, Dept Ind & Mfg Syst Engn, Ames, IA USA; [Hong, Mingyi] Iowa State Univ, Dept Elect & Comp Engn, Ames, IA USA	Stanford University; Iowa State University; Iowa State University	Sun, RY (corresponding author), Stanford Univ, Dept Management Sci & Engn, Stanford, CA 94305 USA.	ruoyu@stanford.edu; mingyi@iastate.edu	Hong, Mingyi/H-6274-2013					ANGELOS JR, 1992, LINEAR ALGEBRA APPL, V170, P117, DOI 10.1016/0024-3795(92)90414-6; Beck A., 2015, CYCLIC BLOCK COORDIN; Beck A, 2013, SIAM J OPTIMIZ, V23, P2037, DOI 10.1137/120887679; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Grippo L, 2000, OPER RES LETT, V26, P127, DOI 10.1016/S0167-6377(99)00074-7; Hong M., 2013, ITERATION COMPLEXITY; Lu Z., 2013, RANDOMIZED BLOCK COO; Lu Z., 2013, MATH PROGRAMMING; LUO ZQ, 1992, J OPTIMIZ THEORY APP, V72, P7, DOI 10.1007/BF00939948; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nutini J., 2015, P 30 INT C MACH LEAR; Powell MJ., 1973, MATH PROGRAM, V4, P193, DOI [DOI 10.1007/BF01584660, 10.1007/BF01584660]; Razaviyayn M., 2014, P NEUR INF PROC NIPS; Razaviyayn M, 2013, SIAM J OPTIMIZ, V23, P1126, DOI 10.1137/120891009; Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z; Saha A, 2013, SIAM J OPTIMIZ, V23, P576, DOI 10.1137/110840054; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105; Xu YY, 2013, SIAM J IMAGING SCI, V6, P1758, DOI 10.1137/120887795	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103066
C	Syrgkanis, V; Agarwal, A; Luo, HP; Schapire, RE		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Syrgkanis, Vasilis; Agarwal, Alekh; Luo, Haipeng; Schapire, Robert E.			Fast Convergence of Regularized Learning in Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We show that natural classes of regularized learning algorithms with a form of recency bias achieve faster convergence rates to approximate efficiency and to coarse correlated equilibria in multiplayer normal form games. When each player in a game uses an algorithm from our class, their individual regret decays at O(T-3/4), while the sum of utilities converges to an approximate optimum at O(T-1)-an improvement upon the worst case O(T-1/2) rates. We show a black-box reduction for any algorithm in the class to achieve (O) over tilde (T-1/2) rates against an adversary, while maintaining the faster rates against algorithms in the class. Our results extend those of Rakhlin and Shridharan [17] and Daskalakis et al. [4], who only analyzed two-player zero-sum games for specific algorithms.	[Syrgkanis, Vasilis; Agarwal, Alekh; Schapire, Robert E.] Microsoft Res, New York, NY 10011 USA; [Luo, Haipeng] Princeton Univ, Princeton, NJ 08544 USA	Microsoft; Princeton University	Syrgkanis, V (corresponding author), Microsoft Res, New York, NY 10011 USA.	vasy@microsoft.com; alekha@microsoft.com; haipengl@cs.princeton.edu; schapire@microsoft.com						Blum A, 2007, ALGORITHMIC GAME THEORY, P79; Blum A, 2008, ACM S THEORY COMPUT, P373; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Daskalakis C, 2015, GAME ECON BEHAV, V92, P327, DOI 10.1016/j.geb.2014.01.003; Foster DP, 1997, GAME ECON BEHAV, V21, P40, DOI 10.1006/game.1997.0595; Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Fudenberg D, 2016, P 15 ACM C EC COMP, P971; Hart S, 2000, ECONOMETRICA, V68, P1127, DOI 10.1111/1468-0262.00153; HOEFFDING W, 1958, ANN MATH STAT, V29, P700, DOI 10.1214/aoms/1177706531; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Rakhlin A., 2013, C LEARN THEOR, V30, P993; Rakhlin S., 2013, ADV NEURAL INFORM PR, P3066; Roughgarden T, 2009, ACM S THEORY COMPUT, P513; Schwarz, 2005, 11765 NAT BUR EC RES; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Syrgkanis V, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P211	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101023
C	Szorenyi, B; Busa-Fekete, R; Paul, A; Hullermeier, E		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Szorenyi, Balazs; Busa-Fekete, Robert; Paul, Adil; Huellermeier, Eyke			Online Rank Elicitation for Plackett-Luce: A Dueling Bandits Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We study the problem of online rank elicitation, assuming that rankings of a set of alternatives obey the Plackett-Luce distribution. Following the setting of the dueling bandits problem, the learner is allowed to query pairwise comparisons between alternatives, i.e., to sample pairwise marginals of the distribution in an online fashion. Using this information, the learner seeks to reliably predict the most probable ranking (or top-alternative). Our approach is based on constructing a surrogate probability distribution over rankings based on a sorting procedure, for which the pairwise marginals provably coincide with the marginals of the Plackett-Luce distribution. In addition to a formal performance and complexity analysis, we present first experimental studies.	[Szorenyi, Balazs] Technion, Haifa, Israel; [Szorenyi, Balazs] MTA SZTE Res Grp Artificial Intelligence, Szeged, Hungary; [Busa-Fekete, Robert; Paul, Adil; Huellermeier, Eyke] Univ Paderborn, Dept Comp Sci, Paderborn, Germany	University of Paderborn	Szorenyi, B (corresponding author), Technion, Haifa, Israel.	szorenyibalazs@gmail.com; busarobi@upb.de; adil.paul@upb.de; eyke@upb.de						Ailon N., 2008, ADV NEURAL INFORM PR, P25; Braverman M., 2009, ABS09101191 CORR; Braverman M, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P268; Bubeck S., 2013, INT C MACHINE LEARNI, P258; Busa-Fekete R., 2013, P 30 ICML JMLR W CP, V28; Busa-Fekete R, 2014, AAAI CONF ARTIF INTE, P1701; Busa-Fekete R, 2014, LECT NOTES ARTIF INT, V8776, P18, DOI 10.1007/978-3-319-11662-4_3; Busa-Fekete Robert, 2014, INT C MACH LEARN, P1071; Clopper CJ, 1934, BIOMETRIKA, V26, P404, DOI 10.2307/2331986; Even-Dar E., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P255; FEIGE U, 1994, SIAM J COMPUT, V23, P1001, DOI 10.1137/S0097539791195877; Gabillon V., 2011, NEURIPS, P2222; Guiver J., 2009, P 26 ANN INT C MACHI, P377; HOARE CAR, 1962, COMPUT J, V5, P10, DOI 10.1093/comjnl/5.1.10; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Hunter DR, 2004, ANN STAT, V32, P384; Luce R, 1959, INDIVIDUAL CHOICE BE; Luce RD, 1965, HDB MATH PSYCHOL, P249; MALLOWS CL, 1957, BIOMETRIKA, V44, P114, DOI 10.2307/2333244; Marden JI, 1995, ANAL MODELING RANK D; McDiarmid CJH, 1996, J ALGORITHM, V21, P476, DOI 10.1006/jagm.1996.0055; Negahban Sahand, 2012, NIPS, P2483; PLACKETT RL, 1975, ROY STAT SOC C-APP, V24, P193; Rajkumar A, 2014, PR MACH LEARN RES, V32; Soufiani Hossein Azari, 2013, P 26 INT C NEUR INF, P2706; Urvoy T., 2013, INT C MACH LEARN, V28, P91; Yue Y., 2011, P 28 INT C MACH LEAR, P241; Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028; Zoghi M, 2014, PR MACH LEARN RES, V32, P10	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102052
C	Takenouchi, T; Kanamori, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Takenouchi, Takashi; Kanamori, Takafumi			Empirical Localization of Homogeneous Divergences on Discrete Sample Spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					In this paper, we propose a novel parameter estimator for probabilistic models on discrete space. The proposed estimator is derived from minimization of homogeneous divergence and can be constructed without calculation of the normalization constant, which is frequently infeasible for models in the discrete space. We investigate statistical properties of the proposed estimator such as consistency and asymptotic normality, and reveal a relationship with the information geometry. Some experiments show that the proposed estimator attains comparable performance to the maximum likelihood estimator with drastically lower computational cost.	[Takenouchi, Takashi] Future Univ Hakodate, Dept Complex & Intelligent Syst, 116-2 Kamedanakano, Hakodate, Hokkaido 0408655, Japan; [Kanamori, Takafumi] Nagoya Univ, Dept Comp Sci & Math Informat, Chikusa Ku, Nagoya, Aichi 4648601, Japan	Future University Hakodate; Nagoya University	Takenouchi, T (corresponding author), Future Univ Hakodate, Dept Complex & Intelligent Syst, 116-2 Kamedanakano, Hakodate, Hokkaido 0408655, Japan.	ttakashi@fun.ac.jp; kanamori@is.nagoya-u.ac.jp						ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; AMARI S, 1992, IEEE T NEURAL NETWOR, V3, P260, DOI 10.1109/72.125867; Amari S., 2000, METHODS INFORM GEOME; Dawid P, 2012, ANN STAT, V40, P593, DOI 10.1214/12-AOS972; Fujisawa H, 2008, J MULTIVARIATE ANAL, V99, P2053, DOI 10.1016/j.jmva.2008.02.004; GOOD IJ, 1971, FDN STAT INFERENCE, P337; Gutmann M., 2012, ARXIV12023727; Hinton G. E., 1986, PARALLEL DISTRIBUTED, V1, DOI DOI 10.1234/12345678; Hinton G. E., 2012, P 26 ANN C NEUR INF, P2447; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hyvarinen A, 2005, J MACH LEARN RES, V6, P695; Hyvarinen A, 2007, COMPUT STAT DATA AN, V51, P2499, DOI 10.1016/j.csda.2006.09.003; Opper M, 2001, ADV MEAN FIELD METHO; R Development Core Team, 2018, R LANG ENV STAT COMP; Sejnowski T. J., 1986, AIP Conference Proceedings, P398, DOI 10.1063/1.36246; Vaart A. W., 1998, ASYMPTOTIC STAT	16	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101096
C	Talwar, K; Thakurta, A; Zhang, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Talwar, Kunal; Thakurta, Abhradeep; Zhang, Li			Nearly-Optimal Private LASSO	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We present a nearly optimal differentially private version of the well known LASSO estimator. Our algorithm provides privacy protection with respect to each training example. The excess risk of our algorithm, compared to the non-private version, is (O) over tilde (1/n(2/3)), assuming all the input data has bounded l(infinity) norm. This is the first differentially private algorithm that achieves such a bound without the polynomial dependence on p under no additional assumptions on the design matrix. In addition, we show that this error bound is nearly optimal amongst all differentially private algorithms.	[Talwar, Kunal; Zhang, Li] Google Res, Mountain View, CA 94043 USA; [Thakurta, Abhradeep] Yahoo Labs, New York, NY USA; [Talwar, Kunal; Thakurta, Abhradeep; Zhang, Li] Microsoft Res, Mountain View, CA USA	Google Incorporated; Microsoft	Talwar, K (corresponding author), Google Res, Mountain View, CA 94043 USA.	kunal@google.com; guhathakurta.abhradeep@gmail.com; liqzhang@google.com						Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bassily R., 2014, FOCS; Bhaskar R., 2010, KDD; Bun Mark, 2014, STOC; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Chaudhuri Kamalika, 2008, NIPS; Duchi J. C., 2013, FOCS; Dwork C., 2013, ARXIV13081385; Dwork  C., 2014, STOC; Dwork C., 2014, FDN TRENDS THEORETIC; Dwork C., 2010, FOCS; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Jaggi Martin., 2013, ICML; Jain P., 2014, INT C MACH LEARN ICM; Jain P., 2012, COLT; Kifer D., 2012, COLT; Koren T, 2012, ICML; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Nikolov Aleksandar, 2013, STOC; Shalev-Shwartz S., 2010, SIAM J OPTIMIZATION; Smith A., 2013, UNPUB; Smith A., 2013, COLT; Smith Adam, 2013, NIPS; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Tibshirani R, 1996, REGRESSION SHRINKAGE, V58, P267; Ullman Jonathan, 2014, ABS14071571 CORR	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100097
C	Tamar, A; Chow, Y; Ghavamzadeh, M; Mannor, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Tamar, Aviv; Chow, Yinlam; Ghavamzadeh, Mohammad; Mannor, Shie			Policy Gradient for Coherent Risk Measures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				OPTIMIZATION	Several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost. These studies have focused on specific risk-measures, such as the variance or conditional value at risk (CVaR). In this work, we extend the policy gradient method to the whole class of coherent risk measures, which is widely accepted in finance and operations research, among other fields. We consider both static and time-consistent dynamic risk measures. For static risk measures, our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming. For dynamic risk measures, our approach is actor-critic style and involves explicit approximation of value function. Most importantly, our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results.	[Tamar, Aviv] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Chow, Yinlam] Stanford Univ, Stanford, CA 94305 USA; [Ghavamzadeh, Mohammad] Adobe Res, San Jose, CA USA; [Ghavamzadeh, Mohammad] INRIA, Rocquencourt, France; [Mannor, Shie] Technion, Haifa, Israel	University of California System; University of California Berkeley; Stanford University; Adobe Systems Inc.; Inria	Tamar, A (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	avivt@berkeley.edu; ychow@stanford.edu; mohammad.ghavamzadeh@inria.fr; shie@ee.technion.ac.il			European Research Council under the European Unions Seventh Framework Program (FP7/2007-2013) / ERC [306638]; Croucher Foundation	European Research Council under the European Unions Seventh Framework Program (FP7/2007-2013) / ERC; Croucher Foundation	The research leading to these results has received funding from the European Research Council under the European Unions Seventh Framework Program (FP7/2007-2013) / ERC Grant Agreement n. 306638. Yinlam Chow is partially supported by Croucher Foundation Doctoral Scholarship.	Artzner P, 1999, MATH FINANC, V9, P203, DOI 10.1111/1467-9965.00068; Baeuerle N, 2011, MATH METHOD OPER RES, V74, P361, DOI 10.1007/s00186-011-0367-0; Bardou O, 2009, MONTE CARLO METHODS, V15, P173, DOI 10.1515/MCMA.2009.011; Bertsekas D.P., 2012, DYNAMIC PROGRAMMING, VII; Boyd S., 2009, CONVEX OPTIMIZATION; Chow Y., 2014, NIPS 27; Chow Y, 2014, AM CONTR C; Delage E., 2010, OPER RES, V58; Fu MC, 2006, HBK OPERAT RES MANAG, V13, P575, DOI 10.1016/S0927-0507(06)13019-4; HADAR J, 1969, AM ECON REV, V59, P25; Iancu D., 2011, ARXIV11066102; Konda V. R., 2000, NIPS; Marbach P, 2001, IEEE T AUTOMAT CONTR, V46, P191, DOI 10.1109/9.905687; Markowitz H, 1959, PORTFOLIO SELECTION; Milgrom P, 2002, ECONOMETRICA, V70, P583, DOI 10.1111/1468-0262.00296; Moody J, 2001, IEEE T NEURAL NETWOR, V12, P875, DOI 10.1109/72.935097; Osogami T., 2012, NIPS; Petrik M., 2012, UAI; Prashanth L., 2013, NIPS 26; Rachev S., 2000, STABLE PARETIAN MODE; Rockafellar R., 2000, J RISK, V2, P21, DOI 10.21314/JOR.2000.038; Ruszczynski A, 2006, MATH OPER RES, V31, P433, DOI 10.1287/moor.1050.0186; Ruszczynski A, 2010, MATH PROGRAM, V125, P235, DOI 10.1007/s10107-010-0393-3; Shapiro A., 2009, LECT STOCHASTIC PROG, P253; Sutton R., 2000, NIPS 13; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tamar A, 2012, INT C MACH LEARN; Tamar A., 2014, INT C MACH LEARN; Tamar A., 2015, AAAI	31	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102071
C	Thrampoulidis, C; Abbasi, E; Hassibi, B		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Thrampoulidis, Christos; Abbasi, Ehsan; Hassibi, Babak			LASSO with Non-linear Measurements is Equivalent to One With Linear Measurements	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				REGRESSION; SENSITIVITY; SELECTION; RISK	Consider estimating an unknown, but structured (e.g. sparse, low-rank, etc.), signal x(0) is an element of R-n from a vector y is an element of R-m of measurements of the form y(i) = g(i)(a(i)(T) x(0)), where the a(i)'s are the rows of a known measurement matrix A, and, g(.) is a (potentially unknown) nonlinear and random link-function. Such measurement functions could arise in applications where the measurement device has nonlinearities and uncertainties. It could also arise by design, e.g., g(i)(x) = sign(x + z(i)), corresponds to noisy 1-bit quantized measurements. Motivated by the classical work of Brillinger, and more recent work of Plan and Vershynin, we estimate x(0) via solving the Generalized-LASSO, i.e., (x) over cap := arg min(x) parallel to y - Ax(0)parallel to(2) + lambda f(x) for some regularization parameter lambda > 0 and some (typically non-smooth) convex regularizer f(.) that promotes the structure of x(0), e.g. l(1)-norm, nuclear-norm, etc. While this approach seems to naively ignore the nonlinear function g(.), both Brillinger (in the non-constrained case) and Plan and Vershynin have shown that, when the entries of A are iid standard normal, this is a good estimator of x(0) up to a constant of proportionality mu, which only depends on g (.). In this work, we considerably strengthen these results by obtaining explicit expressions for parallel to(x) over cap - mu x(0)parallel to(2), for the regularized Generalized-LASSO, that are asymptotically precise when m and n grow large. A main result is that the estimation performance of the Generalized LASSO with non-linear measurements is asymptotically the same as one whose measurements are linear y(i) = mu a(i)(T)x(0) + sigma z(i), with mu = E gamma g(gamma) and sigma(2) = E(g(gamma) - mu gamma)(2), and, gamma standard normal. To the best of our knowledge, the derived expressions on the estimation performance are the first-known precise results in this context. One interesting consequence of our result is that the optimal quantizer of the measurements that minimizes the estimation error of the Generalized LASSO is the celebrated Lloyd-Max quantizer.	[Thrampoulidis, Christos; Abbasi, Ehsan; Hassibi, Babak] CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA	California Institute of Technology	Thrampoulidis, C (corresponding author), CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA.	cthrampo@caltech.edu; eabbasi@caltech.edu; hassibi@caltech.edu			National Science Foundation [CNS-0932428, CCF-1018927, CCF-1423663, CCF-1409204]; Qualcomm Inc.; NASA's Jet Propulsion Laboratory; King Abdulaziz University; King Abdullah University of Science and Technology	National Science Foundation(National Science Foundation (NSF)); Qualcomm Inc.; NASA's Jet Propulsion Laboratory; King Abdulaziz University; King Abdullah University of Science and Technology(King Abdullah University of Science & Technology)	This work was supported in part by the National Science Foundation under grants CNS-0932428, CCF-1018927, CCF-1423663 and CCF-1409204, by a grant from Qualcomm Inc., by NASA's Jet Propulsion Laboratory through the President and Directors Fund, by King Abdulaziz University, and by King Abdullah University of Science and Technology.	[Anonymous], 2013, ARXIV13037291; Bach F., 2010, ADV NEURAL INFORM PR, P118; Bayati M, 2012, IEEE T INFORM THEORY, V58, P1997, DOI 10.1109/TIT.2011.2174612; Belloni A, 2011, BIOMETRIKA, V98, P791, DOI 10.1093/biomet/asr043; BRILLINGER DR, 1977, BIOMETRIKA, V64, P509, DOI 10.2307/2345326; Donoho DL, 2013, IEEE T INFORM THEORY, V59, P3396, DOI 10.1109/TIT.2013.2239356; Donoho DL, 2011, IEEE T INFORM THEORY, V57, P6920, DOI 10.1109/TIT.2011.2165823; DONOHO DL, 1994, PROBAB THEORY REL, V99, P277, DOI 10.1007/BF01199026; El Halabi Marwa, 2014, ARXIV14111990; Garnham AL, 2013, ELECTRON J STAT, V7, P1983, DOI 10.1214/13-EJS831; Gordon Y., 1988, MILMANS INEQUALITY R; ICHIMURA H, 1993, J ECONOMETRICS, V58, P71, DOI 10.1016/0304-4076(93)90114-K; LI KC, 1989, ANN STAT, V17, P1009, DOI 10.1214/aos/1176347254; Oymak S., 2013, ARXIV13110830; Plan Y., 2015, ARXIV150204071; Thrampoulidis C., 2015, PROC C LEARN THEORY, P1683; Thrampoulidis C, 2015, IEEE INT SYMP INFO, P2021, DOI 10.1109/ISIT.2015.7282810; Thrampoulidis C, 2015, INT CONF ACOUST SPEE, P3467, DOI 10.1109/ICASSP.2015.7178615; Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Yi Xinyang, 2015, ARXIV150503257; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x	25	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100107
C	Tripuraneni, N; Gu, SX; Ge, H; Ghahramani, Z		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Tripuraneni, Nilesh; Gu, Shixiang; Ge, Hong; Ghahramani, Zoubin			Particle Gibbs for Infinite Hidden Markov Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Infinite Hidden Markov Models (iHMM's) are an attractive, nonparametric generalization of the classical Hidden Markov Model which can automatically infer the number of hidden states in the system. However, due to the infinite-dimensional nature of the transition dynamics, performing inference in the iHMM is difficult. In this paper, we present an infinite-state Particle Gibbs (PG) algorithm to re-sample state trajectories for the iHMM. The proposed algorithm uses an efficient proposal optimized for iHMMs and leverages ancestor sampling to improve the mixing of the standard PG algorithm. Our algorithm demonstrates significant convergence improvements on synthetic and real world data sets.	[Tripuraneni, Nilesh; Ge, Hong; Ghahramani, Zoubin] Univ Cambridge, Cambridge, England; [Gu, Shixiang] Univ Cambridge, MPI Intelligent Syst, Cambridge, England	University of Cambridge; Max Planck Society; University of Cambridge	Tripuraneni, N (corresponding author), Univ Cambridge, Cambridge, England.	nt357@cam.ac.uk; sg717@cam.ac.uk; hg344@cam.ac.uk; zoubin@eng.cam.ac.uk						Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; Beal MJ, 2001, ADV NEURAL INFORM PR, P577; Bishop C.M, 2006, PATTERN RECOGN; Fox E. B., 2008, 25 INT C MACHINE LEA, P312; Lindsten F, 2014, J MACH LEARN RES, V15, P2145; Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653; Palla K., 2014, ARXIV14034206; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Rosenstein JK, 2013, NANO LETT, V13, P2682, DOI 10.1021/nl400822r; SCOTT S, 2002, J AM STAT ASS, V97; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Van Gael J., 2008, P INT C MACH LEARN, V25	12	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103006
C	Vacher, J; Meso, AI; Perrinet, L; Peyre, G		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Vacher, Jonathan; Meso, Andrew Isaac; Perrinet, Laurent; Peyre, Gabriel			Biologically Inspired Dynamic Textures for Probing Motion Perception	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				PRIOR EXPECTATIONS	Perception is often described as a predictive process based on an optimal inference with respect to a generative model. We study here the principled construction of a generative model specifically crafted to probe motion perception. In that context, we first provide an axiomatic, biologically-driven derivation of the model. This model synthesizes random dynamic textures which are defined by stationary Gaussian distributions obtained by the random aggregation of warped patterns. Importantly, we show that this model can equivalently be described as a stochastic partial differential equation. Using this characterization of motion in images, it allows us to recast motion-energy models into a principled Bayesian inference framework. Finally, we apply these textures in order to psychophysically probe speed perception in humans. In this framework, while the likelihood is derived from the generative model, the prior is estimated from the observed results and accounts for the perceptual bias in a principled fashion.	[Vacher, Jonathan] Univ Paris 09, UNIC, CNRS, F-75775 Paris 16, France; [Vacher, Jonathan; Peyre, Gabriel] Univ Paris 09, CEREMADE, F-75775 Paris 16, France; [Meso, Andrew Isaac; Perrinet, Laurent] Aix Marseille Univ, CNRS, Inst Neurosci Timone, UMR 7289, F-13385 Marseille 05, France; [Peyre, Gabriel] Univ Paris 09, CNRS, F-75775 Paris 16, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Universite Paris-Dauphine; UDICE-French Research Universities; PSL Research University Paris; Universite Paris-Dauphine; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Aix-Marseille Universite; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Universite Paris-Dauphine; Universite Paris Cite	Vacher, J (corresponding author), Univ Paris 09, UNIC, CNRS, F-75775 Paris 16, France.	vacher@ceremade.dauphine.fr; andrew.meso@univ-amu.fr; laurent.perrinet@univ-amu.fr; peyre@ceremade.dauphine.fr	Perrinet, Laurent U./C-4900-2009; Peyré, Gabriel/P-2438-2015; Vacher, Jonathan/AAC-1055-2019	Perrinet, Laurent U./0000-0002-9536-010X; Peyré, Gabriel/0000-0002-4477-0387; Vacher, Jonathan/0000-0002-4321-6309; Meso, Andrew Isaac/0000-0002-1919-7726	EC [FP7-269921]; European Research Council (ERC project SIGMA-Vision); SPEED [ANR-13-SHS2-0006]	EC(European CommissionEuropean Commission Joint Research Centre); European Research Council (ERC project SIGMA-Vision)(European Research Council (ERC)); SPEED	We thank Guillaume Masson for useful discussions during the development of the experiments. We also thank Manon Bouye and Elise Amfreville for proofreading. LUP was supported by EC FP7-269921, "BrainScaleS". The work of JV and GP was supported by the European Research Council (ERC project SIGMA-Vision). AIM and LUP were supported by SPEED ANR-13-SHS2-0006.	ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; Dong DW, 2010, DYNAMICS OF VISUAL MOTION PROCESSING: NEURONAL, BEHAVIORAL, AND COMPUTATIONAL APPROACHES, P261, DOI 10.1007/978-1-4419-0781-3_12; Doretto G, 2003, INT J COMPUT VISION, V51, P91, DOI 10.1023/A:1021669406132; FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379; Galerne B., 2011, IMAGE PROCESSING LIN, V1; Galerne B., 2011, THESIS; GREGORY RL, 1980, PHILOS T ROY SOC B, V290, P181, DOI 10.1098/rstb.1980.0090; Jogan M, 2015, J NEUROSCI, V35, P9381, DOI 10.1523/JNEUROSCI.4801-14.2015; Leon PS, 2012, J NEUROPHYSIOL, V107, P3217, DOI 10.1152/jn.00737.2011; Nestares O, 2000, PROC CVPR IEEE, P523, DOI 10.1109/CVPR.2000.855864; Simoncini C, 2012, NAT NEUROSCI, V15, P1596, DOI 10.1038/nn.3229; Sotiropoulos G, 2014, VISION RES, V97, P16, DOI 10.1016/j.visres.2014.01.012; Stocker AA, 2006, NAT NEUROSCI, V9, P578, DOI 10.1038/nn1669; Unser M, 2014, IEEE T INFORM THEORY, V60, P3036, DOI 10.1109/TIT.2014.2311903; WEI LY, 2009, EUROGRAPHICS 2009 ST; Wei X.-X., 2012, ADV NEURAL INFORM PR, V25, P1313; Weiss Y, 2002, NAT NEUROSCI, V5, P598, DOI 10.1038/nn858; WEISS Y, 2001, PROBABILISTIC MODELS, P81; Xia GS, 2014, SIAM J IMAGING SCI, V7, P476, DOI 10.1137/130918010; Young RA, 2001, SPATIAL VISION, V14, P321, DOI 10.1163/156856801753253591	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101029
C	Vainsencher, D; Liu, H; Zhang, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Vainsencher, Daniel; Liu, Han; Zhang, Tong			Local Smoothness in Variance Reduced Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We propose a family of non-uniform sampling strategies to provably speed up a class of stochastic optimization algorithms with linear convergence including Stochastic Variance Reduced Gradient (SVRG) and Stochastic Dual Coordinate Ascent (SDCA). For a large family of penalized empirical risk minimization problems, our methods exploit data dependent local smoothness of the loss functions near the optimum, while maintaining convergence guarantees. Our bounds are the first to quantify the advantage gained from local smoothness which are significant for some problems significantly better. Empirically, we provide thorough numerical results to back up our theory. Additionally we present algorithms exploiting local smoothness in more aggressive ways, which perform even better in practice.	[Vainsencher, Daniel; Liu, Han] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA; [Zhang, Tong] Rutgers State Univ, Dept Stat, Piscataway, NJ 08854 USA	Princeton University; Rutgers State University New Brunswick	Vainsencher, D (corresponding author), Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.	daniel.vainsencher@princeton.edu; han.liu@princeton.edu; tzhang@stat.rutgers.edu	Zhang, Tong/HGC-1090-2022					Csiba D., 2015, ARXIV150208053; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; LIN Q., 2014, ARXIV14071296; Schmidt Mark, 2013, ARXIV13092388; Shalev-Shwartz S., 2013, MATH PROGRAM, P1; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zhang Yuchen, 2014, ARXIV14093257; Zhao P., 2014, ARXIV14012753; Zhao Peilin, 2015, P 32 INT C MACH LEAR	10	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102062
C	Vincent, P; de Brebisson, A; Bouthillier, X		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Vincent, Pascal; de Brebisson, Alexandre; Bouthillier, Xavier			Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 200 000). Computing the equally large, but typically non-sparse D -dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the D x d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efficient handling of large sparse network inputs is trivial, the case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d(2)) per example instead of O(Dd), remarkably without ever computing the D-dimensional output. The proposed algorithm yields a speedup of D/4d, i.e. two orders of magnitude for typical sizes, for that critical part of the computations that often dominates the training time in this kind of network architecture.	[de Brebisson, Alexandre; Bouthillier, Xavier] Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ, Canada; [Vincent, Pascal] CIFAR, Toronto, ON, Canada	Universite de Montreal; Canadian Institute for Advanced Research (CIFAR)	Vincent, P (corresponding author), CIFAR, Toronto, ON, Canada.				NSERC; Ubisoft	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Ubisoft	We wish to thank Yves Grandvalet for stimulating discussions, Caglar Gulcehre for pointing us to [14], the developers of Theano [17, 18] and Blocks [19] for making these libraries available to build on, and NSERC and Ubisoft for their financial support.	[Anonymous], 2010, PYTH SCI COMP C; [Anonymous], 2014, P INTERSPEECH; Bastien F., 2012, DEEP LEARN UNS FEAT; Bengio Y, 2001, ADV NEUR IN, V13, P932; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Cun YL, 1985, COGNITIVA, V85, P599; Dauphin Y., 2011, P 28 INT C MACH LEAR; Gutmann Michael, 2010, AISTATS; Hill Felix, 2014, ABS14083456 CORR; Jean S., 2015, ACL IJCNLP 2015; LeCun, 1986, DISORDERED SYSTEMS B, DOI DOI 10.1007/978-3-642-82657-3_24; LeCun Y., 1985, DISORDERED SYSTEMS B, P233; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Mnih A., 2013, ADV NEURAL INFORM PR, V26, P2265; Morin F., 2005, PROC INT WORKSHOP AR, P246; Ollivier Yann, 2013, ABS13030818 CORR; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Shrivastava Anshumali, 2014, ADV NEURAL INFORM PR, P2321; van Merrienboer B., 2015, ARXIV E PRINTS; Vijayanarasimhan Sudheendra, 2014, ARXIV14127479	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102014
C	Vinyals, O; Fortunato, M; Jaitly, N		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Vinyals, Oriol; Fortunato, Meire; Jaitly, Navdeep			Pointer Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems - finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem - using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.	[Vinyals, Oriol; Jaitly, Navdeep] Google Brain, Mountain View, CA 94043 USA; [Fortunato, Meire] Univ Calif Berkeley, Dept Math, Berkeley, CA USA	Google Incorporated; University of California System; University of California Berkeley	Vinyals, O (corresponding author), Google Brain, Mountain View, CA 94043 USA.							Bahdanau Dzmitry, 2015, NEURAL MACHINE TRANS; BELLMAN R, 1962, J ACM, V9, P61, DOI 10.1145/321105.321111; Donahue Jeff, 2014, CVPR 2015; Graham R. L., 1972, Information Processing Letters, V1, P132, DOI 10.1016/0020-0190(72)90045-2; Graves A., 2014, NEURAL TURING MACHIN; Graves Alex, 2013, ARXIV13080850 CORR; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Jarvis R. A., 1973, Information Processing Letters, V2, P18, DOI 10.1016/0020-0190(73)90020-3; PREPARATA FP, 1977, COMMUN ACM, V20, P87, DOI 10.1145/359423.359430; REBAY S, 1993, J COMPUT PHYS, V106, P125, DOI 10.1006/jcph.1993.1097; ROBINSON AJ, 1994, IEEE T NEURAL NETWOR, V5, P298, DOI 10.1109/72.279192; Rumelhart D. E., 1985, TECHNICAL REPORT; Srivastava Nitish, 2015, ICML 2015; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; VINYALS O, 2014, GRAMMAR FOREIGN LANG; Vinyals Oriol, 2014, CVPR 2015; Weston Jason, 2014, ICLEAR 2015	18	0	0	15	41	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102015
C	Vinyals, O; Kaiser, L; Koo, T; Petrov, S; Sutskever, I; Hinton, G		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Vinyals, Oriol; Kaiser, Lukasz; Koo, Terry; Petrov, Slav; Sutskever, Ilya; Hinton, Geoffrey			Grammar as a Foreign Language	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.	[Vinyals, Oriol; Kaiser, Lukasz; Koo, Terry; Petrov, Slav; Sutskever, Ilya; Hinton, Geoffrey] Google, Mountain View, CA 94043 USA	Google Incorporated	Vinyals, O (corresponding author), Google, Mountain View, CA 94043 USA.	vinyals@google.com; lukaszkaiser@google.com; terrykoo@google.com; slav@google.com; ilyasu@google.com; geoffhinton@google.com						Bahdanau Dzmitry, 2015, NEURAL MACHINE TRANS; Chorowski J., NIPS 2014 WORKSHOP D; Collins M, 1997, 35TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 8TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P16; Collins Michael, 2004, P 42 ANN M ASS COMPU; Collobert R., 2011, INT C ART INT STAT; Ghahramani Zoubin, 1990, THESIS; Graves Alex, 2013, ARXIV13080850 CORR; Hall David, 2014, ACL; Henderson James, 2003, NAACL; Henderson James, 2004, P 42 ANN M ASS COMP, P95; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hovy Eduard, 2006, NAACL; Huang Zhongqiang, 2009, EMNLP; Jean Sebastien, 2014, ARXIV14122007; Judge J, 2006, COLING/ACL 2006, VOLS 1 AND 2, PROCEEDINGS OF THE CONFERENCE, P497; Kalchbrenner Nal, 2013, P 2013 C EMP METH NA, P1700, DOI DOI 10.1146/ANNUREV.NEURO.26.041002.131047; Klein D, 2003, 41ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P423, DOI 10.3115/1075096.1075150; Li ZH, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P457; Luong T., 2014, ABS14108206 CORR; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; McClosky David, 2006, NAACL; Mikolov T., 2013, EFFICIENT ESTIMATION; Petrov S., 2010, HUMAN LANGUAGE TECHN, P19; Petrov S., 2006, ACL; Petrov Slav, 2012, 1 WORKSH SYNT AN NON; Ratnaparkhi Adwait, 1997, 2 C EMP METH NAT LAN; Socher R., 2011, ICML; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Titov Ivan, 2007, ACL; Toshev Alexander, 2014, ARXIV14114555; Zhu Muhua, 2013, ACL	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100003
C	Vondrick, C; Pirsiavash, H; Oliva, A; Torralba, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Vondrick, Carl; Pirsiavash, Hamed; Oliva, Aude; Torralba, Antonio			Learning visual biases from human imagination	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Although the human visual system can recognize many concepts under challenging conditions, it still has some biases. In this paper, we investigate whether we can extract these biases and transfer them into a machine recognition system. We introduce a novel method that, inspired by well-known tools in human psychophysics, estimates the biases that the human visual system might use for recognition, but in computer vision feature spaces. Our experiments are surprising, and suggest that classifiers from the human visual system can be transferred into a machine with some success. Since these classifiers seem to capture favorable biases in the human visual system, we further present an SVM formulation that constrains the orientation of the SVM hyperplane to agree with the bias from human visual system. Our results suggest that transferring this human bias into machines may help object recognition systems generalize across datasets and perform better when very little training data is available.	[Vondrick, Carl; Oliva, Aude; Torralba, Antonio] MIT, Cambridge, MA 02139 USA; [Pirsiavash, Hamed] Univ Maryland Baltimore Cty, Baltimore, MD 21228 USA	Massachusetts Institute of Technology (MIT); University System of Maryland; University of Maryland Baltimore County	Vondrick, C (corresponding author), MIT, Cambridge, MA 02139 USA.	vondrick@mit.edu; hpirsiav@umbc.edu; oliva@mit.edu; torralba@mit.edu			Google; ONR MURI [N000141010933]	Google(Google Incorporated); ONR MURI(MURIOffice of Naval Research)	We thank Aditya Khosla for important discussions, and Andrew Owens and Zoya Bylinskii for helpful comments. Funding for this research was partially supported by a Google PhD Fellowship to CV, and a Google research award and ONR MURI N000141010933 to AT.	Ahumada Jr A., 1996, PERCEPTUAL CLASSIFIC; Aytar Y., 2011, TABULA RASA MODEL TR; Beard B. L., 1998, TECHNIQUE EXTRACT RE; Blais C, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0003022; Branson S., 2010, VISUAL RECOGNITION H; Chua H. F., 2005, P NATL ACAD SCI US; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, P IEEE COMP SOC C CO; Eckstein M. P., 2002, J VISION; Ellis W., 1999, SOURCE BOOK GESTALT; Epshteyn A., 2005, ECML; Everingham M., 2010, IJCV; Fei-Fei L., 2006, ONE SHOT LEARNING OB; Ferecatu M., 2009, STAT FRAMEWORK IMAGE; Gosselin F., 2003, PSYCHOL SCI; Greene M. R., 2014, VISUAL NOISE NATURAL; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702; Li S., 2011, ASIAN J MATH STAT; Liu R., 2006, SIGCHI HUMAN FACTORS; Lovell J., 1971, J ACOUSTICAL SOC AM; Mahendran A., 2015, CVPR; Mangini M. C., 2004, COGNITIVE SCI; Mezuman E., 2012, NIPS; Murray RF, 2011, J VISION, V11, DOI 10.1167/11.5.2; Palmer S. E., 1981, ATTENTION PERFORMANC, V9; Parikh D., 2011, NIPS WCSSWC; Ponce J., 2006, CATEGORY LEVEL OBJEC; Salakhutdinov R., 2011, CVPR; Sekuler A. B., 2004, CURRENT BIOL; Sorokin A., 2008, CVPR WORKSH; Torralba A., CVPR; Vijayanarasimhan S, 2011, CVPR; Vondrick C., 2013, HOGGLES VISUALIZING; Weinzaepfel Philippe, 2011, CVPR; Yang J., 2007, ICDM WORKSH	36	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101041
C	Voss, J; Belkin, M; Rademacher, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Voss, James; Belkin, Mikhail; Rademacher, Luis			A Pseudo-Euclidean Iteration for Optimal Recovery in Noisy ICA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				BLIND IDENTIFICATION; SEPARATION	Independent Component Analysis (ICA) is a popular model for blind signal separation. The ICA model assumes that a number of independent source signals are linearly mixed to form the observed signals. We propose a new algorithm, PEGI (for pseudo-Euclidean Gradient Iteration), for provable model recovery for ICA with Gaussian noise. The main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-Euclidean (indefinite "inner product") space. The use of this indefinite "inner product" resolves technical issues common to several existing algorithms for noisy ICA. This leads to an algorithm which is conceptually simple, efficient and accurate in testing. Our second contribution is combining PEGI with the analysis of objectives for optimal recovery in the noisy ICA model. It has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural Signal to Interference plus Noise Ratio (SINR) criterion. There have been several partial solutions proposed in the ICA literature. It turns out that any solution to the mixing matrix reconstruction problem can be used to construct an SINR-optimal ICA demixing, despite the fact that SINR itself cannot be computed from data. That allows us to obtain a practical and provably SINR-optimal recovery method for ICA with arbitrary Gaussian noise.	[Voss, James; Belkin, Mikhail; Rademacher, Luis] Ohio State Univ, Columbus, OH 43210 USA	University System of Ohio; Ohio State University	Voss, J (corresponding author), Ohio State Univ, Columbus, OH 43210 USA.	vossj@cse.ohio-state.edu; mbelkin@cse.ohio-state.edu; lrademac@cse.ohio-state.edu		Rademacher, Luis/0000-0001-8708-2755				Albera L, 2004, LINEAR ALGEBRA APPL, V391, P3, DOI 10.1016/j.laa.2004.05.007; Arora S., 2012, NIPS, P2384; Cardoso J.-F., 2005, MATLAB JADE REAL VAL; CARDOSO JF, 1993, IEE PROC-F, V140, P362, DOI 10.1049/ip-f-2.1993.0054; CARDOSO JF, 1991, INT CONF ACOUST SPEE, P3109, DOI 10.1109/ICASSP.1991.150113; Chevalier P, 1999, SIGNAL PROCESS, V73, P27, DOI 10.1016/S0165-1684(98)00183-2; Comon P, 2010, HANDBOOK OF BLIND SOURCE SEPARATION: INDEPENDENT COMPONENT ANALYSIS AND APPLICATIONS, P1; De Lathauwer L, 2007, IEEE T SIGNAL PROCES, V55, P2965, DOI 10.1109/TSP.2007.893943; DeLathauwer L, 1996, 8TH IEEE SIGNAL PROCESSING WORKSHOP ON STATISTICAL SIGNAL AND ARRAY PROCESSING, PROCEEDINGS, P356, DOI 10.1109/SSAP.1996.534890; Gavert H., 2005, MATLAB FASTICA V 2 5; Goyal N, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P584, DOI 10.1145/2591796.2591875; Hyvarinen A, 2001, INDEPENDENT COMPONENT ANALYSIS: PRINCIPLES AND PRACTICE, P71; Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5; Joho M., 2000, Second International Workshop on Independent Component Analysis and Blind Signal Separation. Proceedings, P81; KOLDOVSKY Z, 2006, INT CONF ACOUST SPEE, P873; Koldovsky Z., 2007, TECHNICAL REPORT; Koldovsky Z, 2007, LECT NOTES COMPUT SC, V4666, P730; Makino S, 2007, SIGNALS COMMUN TECHN, P1, DOI 10.1007/978-1-4020-6479-1; Van Veen B. D., 1988, IEEE ASSP Magazine, V5, P4, DOI 10.1109/53.665; VIGARIO R, 2000, SYSTEMS MAN CYBERN A, V47, P589; Voss J. R., 2013, ADV NEURAL INFORM PR, V26, P2544; Yeredor A, 2002, IEEE T SIGNAL PROCES, V50, P1545, DOI 10.1109/TSP.2002.1011195; Yeredor A, 2000, SIGNAL PROCESS, V80, P897, DOI 10.1016/S0165-1684(00)00062-1	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103028
C	Wan, YL; Meila, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Wan, Yali; Meila, Marina			A class of network models recoverable by spectral clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				GRAPHS	Finding communities in networks is a problem that remains difficult, in spite of the amount of attention it has recently received. The Stochastic Block-Model (SBM) is a generative model for graphs with "communities" for which, because of its simplicity, the theoretical understanding has advanced fast in recent years. In particular, there have been various results showing that simple versions of spectral clustering using the Normalized Laplacian of the graph can recover the communities almost perfectly with high probability. Here we show that essentially the same algorithm used for the SBM and for its extension called Degree-Corrected SBM, works on a wider class of Block-Models, which we call Preference Frame Models, with essentially the same guarantees. Moreover, the parametrization we introduce clearly exhibits the free parameters needed to specify this class of models, and results in bounds that expose with more clarity the parameters that control the recovery error in this model class.	[Wan, Yali; Meila, Marina] Univ Washington, Dept Stat, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Wan, YL (corresponding author), Univ Washington, Dept Stat, Seattle, WA 98195 USA.	yaliwan@washington.edu; mmp@stat.washington.edu						Arora Sanjeev, 2012, P 13 ACM C EL COMM, P37, DOI DOI 10.1145/2229012.2229020; Balakrishnan S., 2011, ADV NEURAL INFORM PR, P954; Balcan M.-F., 2012, ARXIV12014899V2; Bollobas Bla, 2001, RANDOM GRAPHS, P215; Chaudhuri K., 2012, JMLR WORKSHOP C P; Chen Y., 2014, ARXIV14021267; Coja-Oghlan A, 2009, SIAM J DISCRETE MATH, V23, P1682, DOI 10.1137/070699354; Jackson MO, 2008, SOCIAL AND ECONOMIC NETWORKS, P1; Le C. M., 2015, CONCENTRATION REGULA; MCKAY BD, 1991, COMBINATORICA, V11, P369, DOI 10.1007/BF01275671; MCKAY BD, 1990, J ALGORITHM, V11, P52, DOI 10.1016/0196-6774(90)90029-E; MCKAY BD, 1985, ARS COMBINATORIA, V19A, P15; Meila M, 2001, ADV NEUR IN, V13, P873; Meila M., 2001, ARTIFICIAL INTELLIGE; Newman M. E. J., 2014, EQUITABLE RANDOM GRA; Ng AY, 2002, ADV NEUR IN, V14, P849; Norris J., 1997, MARKOV CHAINS, DOI DOI 10.1017/CBO9780511810633; Qin T., 2013, ADV NEURAL INFORM PR, P3120; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; Stewart G.W., 1990, MATRIX PERTURBATION, V175; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100076
C	Wang, H; Xing, W; Asif, K; Ziebart, BD		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Wang, Hong; Xing, Wei; Asif, Kaiser; Ziebart, Brian D.			Adversarial Prediction Games for Multivariate Losses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Multivariate loss functions are used to assess performance in many modern prediction tasks, including information retrieval and ranking applications. Convex approximations are typically optimized in their place to avoid NP-hard empirical risk minimization problems. We propose to approximate the training data instead of the loss function by posing multivariate prediction as an adversarial game between a loss-minimizing prediction player and a loss-maximizing evaluation player constrained to match specified properties of training data. This avoids the non-convexity of empirical risk minimization, but game sizes are exponential in the number of predicted variables. We overcome this intractability using the double oracle constraint generation method. We demonstrate the efficiency and predictive performance of our approach on tasks evaluated using the precision at k, the F-score and the discounted cumulative gain.	[Wang, Hong; Xing, Wei; Asif, Kaiser; Ziebart, Brian D.] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA	University of Illinois System; University of Illinois Chicago; University of Illinois Chicago Hospital	Wang, H (corresponding author), Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.	hwang27@uic.edu; wxing3@uic.edu; kasif2@uic.edu; bziebart@uic.edu			National Science Foundation [1526379]; Robust Optimization of Loss Functions with Application to Active Learning	National Science Foundation(National Science Foundation (NSF)); Robust Optimization of Loss Functions with Application to Active Learning	This material is based upon work supported by the National Science Foundation under Grant No. #1526379, Robust Optimization of Loss Functions with Application to Active Learning.	Asif Kaiser, 2015, P C UNC ART INT; Boyd S, 2004, CONVEX OPTIMIZATION; Cao Z., 2007, P 24 INT C MACH LEAR, P129, DOI DOI 10.1145/1273496.1273513; Cortes C, 2004, ADV NEUR IN, V16, P313; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Dembczynski KJ, 2011, ADV NEUR INF PROC SY; Freund Y., 2003, J MACHINE LEARNING R, V4, P933, DOI DOI 10.1162/JMLR.2003.4.6.933; Gilpin Andrew, 2008, AAAI C ART INT, P75; Grunwald PD, 2004, ANN STAT, V32, P1367, DOI 10.1214/009053604000000553; Hazan Tamir, 2010, NEURIPS; HOFFGEN KU, 1995, J COMPUT SYST SCI, V50, P114, DOI 10.1006/jcss.1995.1011; Jansche Martin, 2005, P C HUM LANG TECHN E, P692, DOI DOI 10.3115/1220575.1220662; Joachims T., 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]; Joachims Thorsten, 2005, ICML, DOI DOI 10.1145/1102351.1102399; Jun Xu, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P391; Karp Richard M, 1972, REDUCIBILITY COMBINA; Lewis AS, 2008, NONSMOOTH OPTIMIZATI; Lichman M., 2013, UCI MACHINE LEARNING; Lipton R. J., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P734, DOI 10.1145/195058.195447; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; McMahan H Brendan, 2003, P 20 INT C MACH LEAR, P536; Musicant D., 2003, P INT FLAIRS C, P356; Parambath S., 2014, ADV NEURAL INF PROCE, V27, P2123; Qin T., 2013, ARXIV13062597; Ranjbar M, 2010, LECT NOTES COMPUT SC, V6312, P580, DOI 10.1007/978-3-642-15552-9_42; Taskar B., 2005, P 22 INT C MACH LEAR, P896, DOI DOI 10.1145/1102351.1102464; TOPSOE F, 1979, KYBERNETIKA, V15, P8; Tsochantaridis Ioannis, 2004, P 21 INT C MACH LEAR; VAPNIK V, 1992, ADV NEUR IN, V4, P831; von Neumann J, 1947, THEORY GAMES EC BEHA	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100054
C	Wang, SI; Chaganty, AT; Liang, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Wang, Sida I.; Chaganty, Arun Tejasvi; Liang, Percy			Estimating Mixture Models via Mixtures of Polynomials	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Mixture modeling is a general technique for making any simple model more expressive through weighted combination. This generality and simplicity in part explains the success of the Expectation Maximization (EM) algorithm, in which updates are easy to derive for a wide class of mixture models. However, the likelihood of a mixture model is non-convex, so EM has no known global convergence guarantees. Recently, method of moments approaches offer global guarantees for some mixture models, but they do not extend easily to the range of mixture models that exist. In this work, we present Polymom, an unifying framework based on method of moments in which estimation procedures are easily derivable, just as in EM. Polymom is applicable when the moments of a single mixture component are polynomials of the parameters. Our key observation is that the moments of the mixture model are a mixture of these polynomials, which allows us to cast estimation as a Generalized Moment Problem. We solve its relaxations using semidefinite optimization, and then extract parameters using ideas from computer algebra. This framework allows us to draw insights and apply tools from convex optimization, computer algebra and the theory of moments to study problems in statistical estimation. Simulations show good empirical performance on several models.	[Wang, Sida I.; Chaganty, Arun Tejasvi; Liang, Percy] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Wang, SI (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	sidaw@cs.stanford.edu; chaganty@cs.stanford.edu; pliang@cs.stanford.edu			Microsoft Faculty Research Fellowship; NSERC PGS-D fellowship	Microsoft Faculty Research Fellowship(Microsoft); NSERC PGS-D fellowship(Natural Sciences and Engineering Research Council of Canada (NSERC))	This work was supported by a Microsoft Faculty Research Fellowship to the third author and a NSERC PGS-D fellowship for the first author.	Anandkumar A., 2012, ADV NEURAL INFORM PR; Anandkumar A., 2013, TENSOR DECOMPOSITION; Anandkumar A., 2012, C LEARN THEOR COLT; Anandkumar Anima, 2014, ARXIV14080553; Anandkumar Animashree, 2013, C LEARN THEOR, P867; Balle B, 2014, MACH LEARN, V96, P33, DOI 10.1007/s10994-013-5416-x; Chaganty A., 2013, INT C MACH LEARN ICM; Corless RM, 2009, J SYMB COMPUT, V44, P1536, DOI 10.1016/j.jsc.2008.11.009; Curto R. E., 1996, SOLUTION TRUNCATED C, V568; Ge R., 2015, ARXIV150300424; Hardt M., 2014, ARXIV14044997; Henrion D, 2005, LECT NOTES CONTR INF, V312, P293; Hsu D., 2012, ADV NEURAL INFORM PR; Hsu D., 2013, INNOVATIONS THEORETI; Kalai AT, 2010, ACM S THEORY COMPUT, P553; Lasserre J. B., 2011, MOMENTS POSITIVE POL; Lasserre JB, 2001, SIAM J OPTIMIZ, V11, P796, DOI 10.1137/S1052623400366802; Lasserre JB, 2008, MATH PROGRAM, V112, P65, DOI 10.1007/s10107-006-0085-1; Laurent M, 2009, IMA VOL MATH APPL, V149, P157; McLachlan GJ, 2004, FINITE MIXTURE MODEL, DOI [10.1002/0471721182, DOI 10.1002/0471721182]; MOLLER HM, 1995, NUMER MATH, V70, P311, DOI 10.1007/s002110050122; Ozay N, 2010, PROC CVPR IEEE, P3209, DOI 10.1109/CVPR.2010.5540075; Parrilo P. A., 2003, Algorithmic and Quantitative Real Algebraic Geometry. DIMACS Workshop. Algorithmic and Quantitative Aspects of Real Algebraic Geometry in Mathematics and Computer Science (Discrete Mathematics & Theoretical Comput. Sci. Vol.60), P83; Parrilo PA, 2003, MATH PROGRAM, V96, P293, DOI 10.1007/s10107-003-0387-5; Pearson K., 1894, Philosophical Transactions, V185a, P71, DOI 10.1098/rsta.1894.0003; Stetter H. J., 1993, WORLD SCI SERIES APP, V2, P355; Sturmfels B., 2008, ALGORITHMS INVARIANT; Sturmfels B., 2002, CBMS REG C SER MATH, DOI DOI 10.1090/CBMS/097; Titterington DM, 1985, STAT ANAL FINITE MIX, V7; Viele K, 2002, STAT COMPUT, V12, P315, DOI 10.1023/A:1020779827503	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100070
C	Wang, XY; Leng, CL; Dunson, DB		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Wang, Xiangyu; Leng, Chenlei; Dunson, David B.			On the consistency theory of high dimensional variable screening	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				SELECTION; REGRESSION; LASSO	Variable screening is a fast dimension reduction technique for assisting high dimensional feature selection. As a preselection method, it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model. The performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones. When the data dimension p is substantially larger than the sample size n, variable screening becomes crucial as 1) Faster feature selection algorithms are needed; 2) Conditions guaranteeing selection consistency might fail to hold. This article studies a class of linear screening methods and establishes consistency theory for this special class. In particular, we prove the restricted diagonally dominant (RDD) condition is a necessary and sufficient condition for strong screening consistency. As concrete examples, we show two screening methods SIS and HOLP are both strong screening consistent (subject to additional constraints) with large probability if n > O((rho s+sigma/tau)(2) log p) under random designs. In addition, we relate the RDD condition to the irrepresentable condition, and highlight limitations of SIS.	[Wang, Xiangyu; Dunson, David B.] Duke Univ, Dept Stat Sci, Durham, NC 27708 USA; [Leng, Chenlei] Univ Warwick, Dept Stat, Coventry, W Midlands, England	Duke University; University of Warwick	Wang, XY (corresponding author), Duke Univ, Dept Stat Sci, Durham, NC 27708 USA.	xw56@stat.duke.edu; C.Leng@warwick.ac.uk; dunson@stat.duke.edu			National Institute of Environmental Health Sciences [NIH R01-ES017436]	National Institute of Environmental Health Sciences(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Environmental Health Sciences (NIEHS))	This research was partly support by grant NIH R01-ES017436 from the National Institute of Environmental Health Sciences.	Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P6, DOI 10.1109/MSP.2007.909718; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Cho HR, 2012, J R STAT SOC B, V74, P593, DOI 10.1111/j.1467-9868.2011.01023.x; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Fan JQ, 2008, J R STAT SOC B, V70, P849, DOI 10.1111/j.1467-9868.2008.00674.x; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Jia J., 2012, ARXIV12085584; Lee Jason D, 2013, ADV NEURAL PROCESSIN; Li GR, 2012, ANN STAT, V40, P1846, DOI 10.1214/12-AOS1024; Raskutti G, 2010, J MACH LEARN RES, V11, P2241; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wainwright Martin J, 2009, IEEE T INFORM THEORY; Wang HS, 2009, J AM STAT ASSOC, V104, P1512, DOI 10.1198/jasa.2008.tm08516; Wang Xiangyu, 2015, HIGH DIMENSIONAL ORD; Xue LZ, 2011, BIOMETRIKA, V98, P371, DOI 10.1093/biomet/asr010; Zhang CH, 2008, ANN STAT, V36, P1567, DOI 10.1214/07-AOS520; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; Zhao P, 2006, J MACH LEARN RES, V7, P2541; ZHOU S, 2009, ARXIV09124045	21	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102081
C	Wang, YN; Tung, HY; Smola, A; Anandkumar, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Wang, Yining; Tung, Hsiao-Yu; Smola, Alex; Anandkumar, Anima			Fast and Guaranteed Tensor Decomposition via Sketching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA			Tensor CP decomposition; count sketch; randomized methods; spectral methods; topic modeling		Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized computation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as tensor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iterative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic modeling and obtain competitive results.	[Wang, Yining; Tung, Hsiao-Yu; Smola, Alex] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Anandkumar, Anima] Univ Calif Irvine, Dept EECS, Irvine, CA 92697 USA	Carnegie Mellon University; University of California System; University of California Irvine	Wang, YN (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	yiningwa@cs.cmu.edu; htung@cs.cmu.edu; alex@smola.org; a.anandkumar@uci.edu			Microsoft Faculty Fellowship; Sloan Foundation; Google Faculty Research Grant	Microsoft Faculty Fellowship(Microsoft); Sloan Foundation(Alfred P. Sloan Foundation); Google Faculty Research Grant(Google Incorporated)	Anima Anandkumar is supported in part by the Microsoft Faculty Fellowship and the Sloan Foundation. Alex Smola is supported in part by a Google Faculty Research Grant.	Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; [Anonymous], [No title captured]; [Anonymous], 2010, AAAI; Bhojanapalli S., 2015, ARXIV150205023; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791; Chaganty A., 2014, ICML; Charikar M, 2004, THEOR COMPUT SCI, V312, P3, DOI 10.1016/S0304-3975(03)00400-6; Choi J. H., 2014, NIPS; Dwork Cynthia, 2015, STOC; FIELD A S, 1991, Brain Topography, V3, P407, DOI 10.1007/BF01129000; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Harshman R.A., 1970, MULTIMODAL FACTOR AN; Huang Furong, 2013, ARXIV13090787; Huang Furong, 2014, NIPS OPT WORKSH; Jain A. K., 1989, FUNDAMENTALS DIGITAL; Kang U., 2012, KDD; Klimt B., 2004, CEAS; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kolda Tamara G, 2008, ICDM; Morup M, 2006, NEUROIMAGE, V29, P938, DOI 10.1016/j.neuroimage.2005.08.005; Pagh R., 2012, ITCS; Patrascu M, 2012, J ACM, V59, DOI 10.1145/2220357.2220361; Pham N., 2013, KDD; Phan AH, 2013, IEEE T SIGNAL PROCES, V61, P4834, DOI 10.1109/TSP.2013.2269903; Phan X-H, 2007, GIBBSLDA C C IMPLEME; Tsourakakis CE, 2010, SDM; Tung H.-Y., 2014, NIPS; Wang C., 2014, ECML PKDD; Wang  Y., 2014, NIPS	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102092
C	Wang, ZR; Gu, QQ; Ning, Y; Liu, H		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Wang, Zhaoran; Gu, Quanquan; Ning, Yang; Liu, Han			High Dimensional EM Algorithm: Statistical Optimization and Asymptotic Normality	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				CONFIDENCE-INTERVALS; MIXTURE; LASSO	We provide a general theory of the expectation-maximization (EM) algorithm for inferring high dimensional latent variable models. In particular, we make two contributions: (i) For parameter estimation, we propose a novel high dimensional EM algorithm which naturally incorporates sparsity structure into parameter estimation. With an appropriate initialization, this algorithm converges at a geometric rate and attains an estimator with the (near-) optimal statistical rate of convergence. (ii) Based on the obtained estimator, we propose a new inferential procedure for testing hypotheses for low dimensional components of high dimensional parameters. For a broad family of statistical models, our framework establishes the first computationally feasible approach for optimal estimation and asymptotic inference in high dimensions.	[Wang, Zhaoran; Ning, Yang; Liu, Han] Princeton Univ, Princeton, NJ 08544 USA; [Gu, Quanquan] Univ Virginia, Charlottesville, VA 22903 USA	Princeton University; University of Virginia	Wang, ZR (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.		Wang, Zhaoran/P-7113-2018		NSF [IIS1116730, IIS1332109, IIS1408910, IIS1546482-BIGDATA, DMS1454377-CAREER]; NIH [R01GM083084, R01HG06841, R01MH102339]; FDA [HHSF223201000072C]	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); FDA(United States Department of Health & Human Services)	Research supported by NSF IIS1116730, NSF IIS1332109, NSF IIS1408910, NSF IIS1546482-BIGDATA, NSF DMS1454377-CAREER, NIH R01GM083084, NIH R01HG06841, NIH R01MH102339, and FDA HHSF223201000072C.	Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; [Anonymous], 2014, ARXIV14013889; Balakrishnan Sivaraman, 2014, ARXIV14082156; Bartholomew D. J., 2011, LATENT VARIABLE MODE, V899; Belloni A, 2012, ECONOMETRICA, V80, P2369, DOI 10.3982/ECTA9626; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Cai T, 2011, J AM STAT ASSOC, V106, P594, DOI 10.1198/jasa.2011.tm10155; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Chaganty A. T., 2013, ARXIV13063729; CHAUDHURI K, 2009, ARXIV09120086; Dasgupta S, 2007, J MACH LEARN RES, V8, P203; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Javanmard A, 2014, J MACH LEARN RES, V15, P2869; Khalili A, 2007, J AM STAT ASSOC, V102, P1025, DOI 10.1198/016214507000000590; Knight K, 2000, ANN STAT, V28, P1356; Lee J.D., 2013, ARXIV13116238; Lockhart R, 2014, ANN STAT, V42, P413, DOI 10.1214/13-AOS1175; McLachlan G., 2007, EM ALGORITHM EXTENSI, V382; Meinshausen N, 2010, J R STAT SOC B, V72, P417, DOI 10.1111/j.1467-9868.2010.00740.x; Meinshausen N, 2009, J AM STAT ASSOC, V104, P1671, DOI 10.1198/jasa.2009.tm08647; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Nickl R, 2013, ANN STAT, V41, P2852, DOI 10.1214/13-AOS1170; Stadler N, 2010, TEST-SPAIN, V19, P209, DOI 10.1007/s11749-010-0197-z; Tseng P, 2004, MATH OPER RES, V29, P27, DOI 10.1287/moor.1030.0073; Van de Geer S, 2014, ANN STAT, V42, P1166, DOI 10.1214/14-AOS1221; Van der Vaart A.W, 2000, CAMBRIDGE SERIES STA, V3; Wasserman L, 2009, ANN STAT, V37, P2178, DOI 10.1214/08-AOS646; WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060; Yi X., 2013, ARXIV13103745; Zhang CH, 2014, J R STAT SOC B, V76, P217, DOI 10.1111/rssb.12026	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102063
C	Werling, K; Chaganty, A; Liang, P; Manning, CD		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Werling, Keenon; Chaganty, Arun; Liang, Percy; Manning, Christopher D.			On-the-Job Learning with Bayesian Decision Theory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Our goal is to deploy a high-accuracy system starting with zero training examples. We consider an on-the-job setting, where as inputs arrive, we use real-time crowd-sourcing to resolve uncertainty where needed and output our prediction when confident. As the model improves over time, the reliance on crowdsourcing queries decreases. We cast our setting as a stochastic game based on Bayesian decision theory, which allows us to balance latency, cost, and accuracy objectives in a principled way. Computing the optimal policy is intractable, so we develop an approximation based on Monte Carlo Tree Search. We tested our approach on three datasets-named-entity recognition, sentiment classification, and image classification. On the NER task we obtained more than an order of magnitude reduction in cost compared to full human annotation, while boosting performance relative to the expert provided labels. We also achieve a 8% F-1 improvement over having a single human label the whole set, and a 28% F-1 improvement over online learning.	[Werling, Keenon; Chaganty, Arun; Liang, Percy; Manning, Christopher D.] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Werling, K (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	keenon@cs.stanford.edu; chaganty@cs.stanford.edu; pliang@cs.stanford.edu; manning@cs.stanford.edu	Manning, Christopher/AAM-9535-2020	Manning, Christopher/0000-0001-6155-649X	Sloan Fellowship	Sloan Fellowship(Alfred P. Sloan Foundation)	We are grateful to Kelvin Guu and Volodymyr Kuleshov for useful feedback regarding the calibration of our models and Amy Bearman for providing the image embeddings for the face classification experiments. We would also like to thank our anonymous reviewers for their helpful feedback. Finally, our work was sponsored by a Sloan Fellowship to the third author.	Angeli G., 2014, EMPIRICAL METHODS NA; [Anonymous], INT C MACH LEARN ICM; [Anonymous], P ADV NEUR INF PROC; Bernstein M, 2010, S USER INTERFACE SOF, P313, DOI [10.1145/1866029.1866078, DOI 10.1145/1866029.1866078]; Bernstein M.S., 2011, P 24 ANN ACM S US IN, P33, DOI DOI 10.1145/2047196.2047201; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chai XY, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P51, DOI 10.1109/ICDM.2004.10092; Cheng J, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON COMPUTER-SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING (CSCW'15), P600, DOI 10.1145/2675133.2675214; Chu W., 2011, P 17 ACM SIGKDD INT, P195; Coulom R., 2007, COMP GAM WORKSH; Dai P., 2010, ASS ADV ARTIFICIAL I; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Donmez P, 2008, P 17 ACM C INF KNOWL, P619, DOI DOI 10.1145/1458082.1458165; Finkel Jenny Rose, 2005, P 43 ANN M ASS COMP, P363, DOI DOI 10.3115/1219840.1219885; Gao T., 2011, NEURIPS 2011, P1062; Golovin D, 2010, ARON CULOTTA ADV NEU, P766; Greiner R, 2002, ARTIF INTELL, V139, P137, DOI 10.1016/S0004-3702(02)00209-6; Helmbold D., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P218, DOI 10.1145/267460.267502; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; KOKKALIS N, 2013, P 2013 C COMP SUPP C, P1291; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar N., 2009, ICCV; Lasecki Walter S, 2013, P 2013 C COMP SUPP C; Li CL, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P721, DOI 10.1145/2348283.2348380; Maas A., 2011, P 49 ANN M ASS COMPU, P142; Sculley D., 2007, C EM ANT CEAS; Settles B., 2010, TECHNICAL REPORT; Socher Richard, 2013, EMPIRICAL METHODS NA	28	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102009
C	Wilson, AG; Dann, C; Lucas, CG; Xing, EP		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Wilson, Andrew Gordon; Dann, Christoph; Lucas, Christopher G.; Xing, Eric P.			The Human Kernel	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				EXTRAPOLATION	Bayesian nonparametric models, such as Gaussian processes, provide a compelling framework for automatic statistical modelling: these models have a high degree of flexibility, and automatically calibrated complexity. However, automating human expertise remains elusive; for example, Gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners. In this paper, we create function extrapolation problems and acquire human responses, and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments. We use the learned kernels to gain psychological insights and to extrapolate in human-like ways that go beyond traditional stationary and polynomial kernels. Finally, we investigate Occam's razor in human and Gaussian process based function learning.	[Wilson, Andrew Gordon; Dann, Christoph; Xing, Eric P.] CMU, Mt Pleasant, MI 48859 USA; [Lucas, Christopher G.] Univ Edinburgh, Edinburgh, Midlothian, Scotland	University of Edinburgh	Wilson, AG (corresponding author), CMU, Mt Pleasant, MI 48859 USA.							[Anonymous], ETS RES B SERIES; Bishop C.M, 2006, PATTERN RECOGN; Busemeyer J. R., 1997, CONCEPTS CATEGORIES; DeLosh EL, 1997, J EXP PSYCHOL LEARN, V23, P968, DOI 10.1037/0278-7393.23.4.968; Deneve S, 2008, NEURAL COMPUT, V20, P91, DOI 10.1162/neco.2008.20.1.91; Doya K., 2007, BAYESIAN BRAIN PROBA; Gershman SJ, 2012, NEURAL COMPUT, V24, P1, DOI 10.1162/NECO_a_00226; Ghahramani Z, 2015, NATURE, V521, P452, DOI 10.1038/nature14541; Ghahramani Zoubin, 2001, NEURAL INFORM PROCES; Griffiths TL, 2006, PSYCHOL SCI, V17, P767, DOI 10.1111/j.1467-9280.2006.01780.x; Griffiths TL, 2012, CURR DIR PSYCHOL SCI, V21, P263, DOI 10.1177/0963721412447619; Jaakkola TS, 1999, ADV NEUR IN, V11, P487; Johnson S. G. B., 2014, P 36 ANN C COGN SCI, P701; Kalish ML, 2007, PSYCHON B REV, V14, P288, DOI 10.3758/BF03194066; Knill DC, 1996, PERCEPTION BAYESIAN; KOH KH, 1991, J EXP PSYCHOL LEARN, V17, P811, DOI 10.1037/0278-7393.17.5.811; Little D.R., 2009, P 31 ANN C COGN SCI, P1157; Lucas C. E., 2015, PSYCHONOMIC B REV, P1; Lucas Chris, 2009, NEURAL INFORM PROCES; Lucas Christopher G, 2012, SUPERSPACE EXTRAPOLA; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; McCulloch W., 1943, B MATH BIOPHYS, V5, P115, DOI [10.1007/BF02478259, DOI 10.1007/BF02478259]; McDaniel MA, 2005, PSYCHON B REV, V12, P24, DOI 10.3758/BF03196347; Neal RM., 1996, BAYESIAN LEARNING NE, P29; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Tenenbaum JB, 2011, SCIENCE, V331, P1279, DOI 10.1126/science.1192788; Vul E, 2014, COGNITIVE SCI, V38, P599, DOI 10.1111/cogs.12101; Wilson A. G., 2013, INT C MACH LEARN ICM; Wilson A.G., 2014, ADV NEURAL INFORM PR; Wilson A.G., 2014, COVARIANCE KERNELS F; Wilson A. G., 2012, TECHNICAL REPORT; WOLPERT DM, 1995, SCIENCE, V269, P1880, DOI 10.1126/science.7569931	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101025
C	Wu, AQ; Park, IM; Pillow, JW		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Wu, Anqi; Park, Il Memming; Pillow, Jonathan W.			Convolutional Spike-triggered Covariance Analysis for Neural Subunit Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				RECOGNITION; RESPONSES; MECHANISM; CELLS	Subunit models provide a powerful yet parsimonious description of neural responses to complex stimuli. They are defined by a cascade of two linear-nonlinear (LN) stages, with the first stage defined by a linear convolution with one or more filters and common point nonlinearity, and the second by pooling weights and an output nonlinearity. Recent interest in such models has surged due to their biological plausibility and accuracy for characterizing early sensory responses. However, fitting poses a difficult computational challenge due to the expense of evaluating the log-likelihood and the ubiquity of local optima. Here we address this problem by providing a theoretical connection between spike-triggered covariance analysis and nonlinear subunit models. Specifically, we show that a "convolutional" decomposition of a spike-triggered average (STA) and covariance (STC) matrix provides an asymptotically efficient estimator for class of quadratic subunit models. We establish theoretical conditions for identifiability of the subunit and pooling weights, and show that our estimator performs well even in cases of model mismatch. Finally, we analyze neural data from macaque primary visual cortex and show that our moment-based estimator outperforms a highly regularized generalized quadratic model (GQM), and achieves nearly the same prediction performance as the full maximum-likelihood estimator, yet at substantially lower cost.	[Wu, Anqi; Pillow, Jonathan W.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA; [Park, Il Memming] SUNY Stony Brook, Dept Neurobiol & Behav, Stony Brook, NY 11794 USA	Princeton University; State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook	Wu, AQ (corresponding author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.	anqiw@princeton.edu; memming.park@stonybrook.edu; pillow@princeton.edu						Arcas BAY, 2003, NEURAL COMPUT, V15, P1789, DOI 10.1162/08997660360675044; BARLOW HB, 1965, J PHYSIOL-LONDON, V178, P477, DOI 10.1113/jphysiol.1965.sp007638; Crook JD, 2008, J NEUROSCI, V28, P11277, DOI 10.1523/JNEUROSCI.2982-08.2008; Davis P. J., 1979, CIRCULANT MATRICES; Demb JB, 2001, J NEUROSCI, V21, P7447, DOI 10.1523/JNEUROSCI.21-19-07447.2001; FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251; HOCHSTEIN S, 1976, J PHYSIOL-LONDON, V262, P265, DOI 10.1113/jphysiol.1976.sp011595; Joris PX, 2004, PHYSIOL REV, V84, P541, DOI 10.1152/physrev.00029.2003; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Park I. M., 2011, ADV NEURAL INF PROCE, V24, P1692; Park IM, 2013, ADV NEURAL INFORM PR, V26, P2454; Pillow JW, 2006, J VISION, V6, P414, DOI 10.1167/6.4.9; Rajan K, 2013, NEURAL COMPUT, V25, P1661, DOI 10.1162/NECO_a_00463; Ramirez Alexandro D., 2013, J COMPUTATIONAL NEUR, P1; Rust NC, 2005, NEURON, V46, P945, DOI 10.1016/j.neuron.2005.05.021; Sahani M., 2003, NIPS, P15; Schwartz O, 2006, J VISION, V6, P484, DOI 10.1167/6.4.13; Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56; Sharpee T, 2004, NEURAL COMPUT, V16, P223, DOI 10.1162/089976604322742010; Touryan J, 2002, J NEUROSCI, V22, P10811; VANSTEVENINCK RD, 1988, PROC R SOC SER B-BIO, V234, P379; Vintch B, 2012, ADV NEURAL INFORM PR, V25; Vintch Brett, 2015, J NEURSOCI; Williamson RS, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004141	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102110
C	Wu, YF; Gyorgy, A; Szepesvari, C		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Wu, Yifan; Gyorgy, Andras; Szepesvari, Csaba			Online Learning with Gaussian Payoffs and Side Observations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We consider a sequential learning problem with Gaussian payoffs and side observations: after selecting an action i, the learner receives information about the payoff of every action j in the form of Gaussian observations whose mean is the same as the mean payoff, but the variance depends on the pair (i; j) (and may be infinite). The setup allows a more refined information transfer from one action to another than previous partial monitoring setups, including the recently introduced graph-structured feedback case. For the first time in the literature, we provide non-asymptotic problem-dependent lower bounds on the regret of any algorithm, which recover existing asymptotic problem-dependent lower bounds and finitetime minimax lower bounds available in the literature. We also provide algorithms that achieve the problem-dependent lower bound (up to some universal constant factor) or the minimax lower bounds (up to logarithmic factors).	[Wu, Yifan; Szepesvari, Csaba] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada; [Gyorgy, Andras] Imperial Coll London, Dept Elect & Elect Engn, London, England	University of Alberta; Imperial College London	Wu, YF (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.	ywu12@ualberta.ca; a.gyorgy@imperial.ac.uk; szepesva@ualberta.ca			Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning (AICML); NSERC	Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning (AICML); NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC))	This work was supported by the Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning (AICML) and NSERC. During this work, A. Gyorgy was with the Department of Computing Science, University of Alberta.	Alon N., 2013, ADV NEURAL INFORM PR, P1610; Alon N., 2015, PMLR, V40, P23; Bartok G, 2014, MATH OPER RES, V39, P967, DOI 10.1287/moor.2014.0663; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Buccapatnam Swapna, 2014, ACM SIGMETRICS Performance Evaluation Review, V42, P289, DOI 10.1145/2591971.2591989; Caron S, 2012, P 28 C UNC ART INT A, P142; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Combes R, 2014, PR MACH LEARN RES, V32; Graves TL, 1997, SIAM J CONTROL OPTIM, V35, P715, DOI 10.1137/S0363012994275440; Kaufmann Emilie, 2015, J MACHINE LEARNING R; Kocak Tomas, 2014, ADV NEURAL INFORM PR, P613; Lattimore T, 2014, LECT NOTES ARTIF INT, V8776, P200, DOI 10.1007/978-3-319-11662-4_15; Li LH, 2015, JMLR WORKSH CONF PRO, V38, P608; Magureanu S., 2014, P C LEARN THEOR COLT, P975; Mannor S., 2011, NIPS, P684; Wu Yifan, 2015, ARXIV151008108	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103071
C	Xie, B; Liang, YY; Song, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Xie, Bo; Liang, Yingyu; Song, Le			Scale Up Nonlinear Component Analysis with Doubly Stochastic Gradients	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Nonlinear component analysis such as kernel Principle Component Analysis (KPCA) and kernel Canonical Correlation Analysis (KCCA) are widely used in machine learning, statistics and data analysis, but they cannot scale up to big datasets. Recent attempts have employed random feature approximations to convert the problem to the primal form for linear computational complexity. However, to obtain high quality solutions, the number of random features should be the same order of magnitude as the number of data points, making such approach not directly applicable to the regime with millions of data points. We propose a simple, computationally efficient, and memory friendly algorithm based on the "doubly stochastic gradients" to scale up a range of kernel nonlinear component analysis, such as kernel PCA, CCA and SVD. Despite the non-convex nature of these problems, our method enjoys theoretical guarantees that it converges at the rate (O) over tilde (1/t) to the global optimum, even for the top k eigen subspace. Unlike many alternatives, our algorithm does not require explicit orthogonalization, which is infeasible on big datasets. We demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets.	[Xie, Bo; Song, Le] Georgia Inst Technol, Atlanta, GA 30332 USA; [Liang, Yingyu] Princeton Univ, Princeton, NJ 08544 USA	University System of Georgia; Georgia Institute of Technology; Princeton University	Xie, B (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	bo.xie@gatech.edu; yingyul@cs.princeton.edu; lsong@cc.gatech.edu			NSF/NIH BIGDATA [1R01GM108341]; ONR [N00014-15-1-2340]; NSF [IIS-1218749, CAREER IIS-1350983, CCF-0832797, CCF-1117309, CCF-1302518, DMS-1317308]; Simons Investigator Award; Simons Collaboration Grant	NSF/NIH BIGDATA; ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); Simons Investigator Award; Simons Collaboration Grant	The research was supported in part by NSF/NIH BIGDATA 1R01GM108341, ONR N00014-15-1-2340, NSF IIS-1218749, NSF CAREER IIS-1350983, NSF CCF-0832797, CCF-1117309, CCF-1302518, DMS-1317308, Simons Investigator Award, and Simons Collaboration Grant.	Anandkumar A., 2012, CORR, Vabs/ 1210. 7559; [Anonymous], 2014, ADV NEURAL INFORM PR, DOI DOI 10.1080/01621459.1963; Arora Raman, 2013, ADV NEURAL INFORM PR, P1815; BALSUBRAMANI A., 2013, ADV NEURAL INFORM PR, V26, P3174, DOI 10.1016/j.compbiomed.2021.104502; Cai TT, 2012, ANN STAT, V40, P2389, DOI 10.1214/12-AOS998; Chin TJ, 2007, IEEE T IMAGE PROCESS, V16, P1662, DOI 10.1109/TIP.2007.896668; Dai B., 2014, NIPS; Honeine P, 2012, IEEE T PATTERN ANAL, V34, P1814, DOI 10.1109/TPAMI.2011.270; Kim KI, 2005, IEEE T PATTERN ANAL, V27, P1351, DOI 10.1109/TPAMI.2005.181; Kim M., 2009, P AISTATS, P280; Le Q, 2013, 30 INT C MACH LEARN; Lopez-Paz David, 2014, INT C MACH LEARN ICM; Meier F, 2014, ADV NEURAL INFORM PR, P972; Montavon G., 2012, ADV NEURAL INFORM PR, V25, P449; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; Oja E., 1983, SUBSPACE METHODPAT; Rahimi A., 2008, ADV NEURAL INFORM PR, V20; Rahimi A., 2009, NEURAL INFORM PROCES; SANGER TD, 1989, NEURAL NETWORKS, V2, P459, DOI 10.1016/0893-6080(89)90044-0; SCHRAUDOLPH NN, 2007, ADV NEURAL INFORM PR, V19; Shamir Ohad, 2014, ARXIV14092848; Song L., 2014, INT C MACH LEARN ICM; Vershynin R, 2012, J THEOR PROBAB, V25, P655, DOI 10.1007/s10959-010-0338-z; Williams C., 2000, INT C MACH LEARN	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101073
C	Yanardag, P; Vishwanathan, SVN		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Yanardag, Pinar; Vishwanathan, S. V. N.			A Structural Smoothing Framework For Robust Graph-Comparison	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					In this paper, we propose a general smoothing framework for graph kernels by taking structural similarity into account, and apply it to derive smoothed variants of popular graph kernels. Our framework is inspired by state-of-the-art smoothing techniques used in natural language processing (NLP). However, unlike NLP applications that primarily deal with strings, we show how one can apply smoothing to a richer class of inter-dependent sub-structures that naturally arise in graphs. Moreover, we discuss extensions of the Pitman-Yor process that can be adapted to smooth structured objects, thereby leading to novel graph kernels. Our kernels are able to tackle the diagonal dominance problem while respecting the structural similarity between features. Experimental evaluation shows that not only our kernels achieve statistically significant improvements over the unsmoothed variants, but also outperform several other graph kernels in the literature. Our kernels are competitive in terms of runtime, and offer a viable option for practitioners.	[Yanardag, Pinar] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47906 USA; [Vishwanathan, S. V. N.] Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus; University of California System; University of California Santa Cruz	Yanardag, P (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47906 USA.	ypinar@purdue.edu; vishy@ucsc.edu			National Science Foundation [1219015]	National Science Foundation(National Science Foundation (NSF))	We thank to Hyokun Yun for his tremendous help in implementing Pitman-Yor Processes. We also thank to anonymous NIPS reviewers for their constructive comments, and Jiasen Yang, Joon Hee Choi, Amani Abu Jabal and Parameswaran Raman for reviewing early drafts of the paper. This work is supported by the National Science Foundation under grant No. #1219015.	[Anonymous], 1999, UCSCRL9910; [Anonymous], 2002, LEARNING KERNELS; Borgwardt K. M., 2005, ISMB; Borgwardt K. M., 2005, 5 IEEE INT C DAT MIN, DOI DOI 10.1109/ICDM.2005.132; Chen S.F., 1996, P 34 ANN M ASS COMPU, P310, DOI DOI 10.3115/981863.981904; Croce D., 2011, P C EMPIRICAL METHOD, P1034; DEBNATH AK, 1991, J MED CHEM, V34, P786, DOI 10.1021/jm00106a046; Feragen Aasa, 2013, NIPS, P216; GARTNER T, 2003, COLT, V2777, P129; Goldwater S., 2006, NIPS; Goldwater S, 2011, J MACH LEARN RES, V12, P2335; Kandola J, 2003, LECT NOTES ARTIF INT, V2777, P288, DOI 10.1007/978-3-540-45167-9_22; Kneser R., 1995, ICASSP; McKay B. D., 2007, NAUTY USERS GUIDE VE; Neumann M., 2012, ICML 2012 WORKSH MIN; NEY H, 1994, COMPUT SPEECH LANG, V8, P1, DOI 10.1006/csla.1994.1001; Pitman J, 1997, ANN PROBAB, V25, P855; Przulj N., 2006, ECCB; Ramon J., 2003, 1 INT WORKSH MIN GRA; Severyn A, 2012, DATA MIN KNOWL DISC, V25, P325, DOI 10.1007/s10618-012-0276-8; Shervashidze N., 2010, NIPS; Shervashidze N, 2009, AISTATS; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; SMOLA AJ, 2003, COMPUTATIONAL LEARNI, V2777, P144; Teh Y.-W., 2006, ACL; Toivonen H, 2003, BIOINFORMATICS, V19, P1183, DOI 10.1093/bioinformatics/btg130; Vishwanathan S. V. N., 2010, JMLR; Wale N, 2008, KNOWL INF SYST, V14, P347, DOI 10.1007/s10115-007-0103-5; Yanardag P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1365, DOI 10.1145/2783258.2783417; Zhai CX, 2004, ACM T INFORM SYST, V22, P179, DOI 10.1145/984321.984322	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102029
C	Yang, EH; Lozano, AC; Ravikumar, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Yang, Eunho; Lozano, Aurelie C.; Ravikumar, Pradeep			Closed-form Estimators for High-dimensional Generalized Linear Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				REGULARIZATION; SPARSE	We propose a class of closed-form estimators for GLMs under high-dimensional sampling regimes. Our class of estimators is based on deriving closed-form variants of the vanilla unregularized MLE but which are (a) well-defined even under high-dimensional settings, and (b) available in closed-form. We then perform thresholding operations on this MLE variant to obtain our class of estimators. We derive a unified statistical analysis of our class of estimators, and show that it enjoys strong statistical guarantees in both parameter error as well as variable selection, that surprisingly match those of the more complex regularized GLM MLEs, even while our closed-form estimators are computationally much simpler. We derive instantiations of our class of closed-form estimators, as well as corollaries of our general theorem, for the special cases of logistic, exponential and Poisson regression models. We corroborate the surprising statistical and computational performance of our class of estimators via extensive simulations.	[Yang, Eunho; Lozano, Aurelie C.] IBM TJ Watson Res Ctr, Armonk, NY 10504 USA; [Ravikumar, Pradeep] Univ Texas Austin, Austin, TX 78712 USA	International Business Machines (IBM); University of Texas System; University of Texas Austin	Yang, EH (corresponding author), IBM TJ Watson Res Ctr, Armonk, NY 10504 USA.	eunhyang@us.ibm.com; aclozano@us.ibm.com; pradeepr@cs.utexas.edu	Yang, Eunho/K-8395-2016					Bach F, 2010, ELECTRON J STAT, V4, P384, DOI 10.1214/09-EJS521; Bickel PJ, 2008, ANN STAT, V36, P2577, DOI 10.1214/08-AOS600; Bunea F, 2008, ELECTRON J STAT, V2, P1153, DOI 10.1214/08-EJS287; Cohen MB, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P343, DOI 10.1145/2591796.2591833; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Hoffman G. E., 2013, PLOS COMPUTATIONAL B; Kakade S. M., 2010, INT C AI STAT AISTAT; Kim Y, 2006, STAT SINICA, V16, P375; Koh K, 2007, J MACHINE LEARNING R, V1, P1519; McCullagh P., 1989, GEN LINEAR MODELS; Meier L, 2008, J R STAT SOC B, V70, P53, DOI 10.1111/j.1467-9868.2007.00627.x; Negahban S., 2010, ARXIV10102731V1; Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631; Rothman AJ, 2009, J AM STAT ASSOC, V104, P177, DOI 10.1198/jasa.2009.0101; Spielman DA, 2003, ANN IEEE SYMP FOUND, P416, DOI 10.1109/SFCS.2003.1238215; Spielman DA, 2014, SIAM J MATRIX ANAL A, V35, P835, DOI 10.1137/090771430; van de Geer SA, 2008, ANN STAT, V36, P614, DOI 10.1214/009053607000000929; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018; Witten DM, 2010, STAT METHODS MED RES, V19, P29, DOI 10.1177/0962280209105024; Yang E., 2012, NEUR INFO PROC SYS N, P25; Yang E., 2014, INT C MACH LEARN ICM, P31; Yang E., 2014, NEUR INFO PROC SYS N, P27	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101031
C	Yang, JM; Reed, S; Yang, MH; Lee, H		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Yang, Jimei; Reed, Scott; Yang, Ming-Hsuan; Lee, Honglak			Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					An important problem for both graphics and vision is to synthesize novel views of a 3D object from a single image. This is in particular challenging due to the partial observability inherent in projecting a 3D object onto the image space, and the ill-posedness of inferring object shape and pose. However, we can train a neural network to address the problem if we restrict our attention to specific object classes (in our case faces and chairs) for which we can gather ample training data. In this paper, we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image. The recurrent structure allows our model to capture longterm dependencies along a sequence of transformations, and we demonstrate the quality of its predictions for human faces on the Multi-PIE dataset and for a dataset of 3D chair models, and also show its ability of disentangling latent data factors without using object class labels.	[Yang, Jimei; Yang, Ming-Hsuan] Univ Calif Merced, Merced, CA 95343 USA; [Reed, Scott; Lee, Honglak] Univ Michigan, Ann Arbor, MI 48109 USA	University of California System; University of California Merced; University of Michigan System; University of Michigan	Yang, JM (corresponding author), Univ Calif Merced, Merced, CA 95343 USA.	jyang44@ucmerced.edu; reedscot@umich.edu; mhyang@ucmerced.edu; honglak@umich.edu	Yang, Ming-Hsuan/T-9533-2019; Yang, Ming-Hsuan/AAE-7350-2019	Yang, Ming-Hsuan/0000-0003-4848-2304; 				[Anonymous], 2015, FAST R CNN; [Anonymous], 2009, ICML; [Anonymous], 2011, ICANN; Aubry M., 2014, CVPR; Aubry M., 2015, ICCV; Ba J., 2015, ICLR; Ba J., 2017, P 3 INT C LEARN REPR; Blanz V., 1999, SIGGRAPH; Cheung B., 2015, ICLR; Ding W., 2014, NIPS DEEP LEARN REPR; Dosovitskiy A., 2015, CVPR; Flynn J., 2015, ARXIV150606825; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Jia Y., P ACM MULT, P675; Kholgade N., 2014, SIGGRAPH; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Kulkarni T. D., 2015, NIPS; Long J., 2015, PROC CVPR IEEE, DOI 10.1109/TPAMI.2016.2572683TPAMI.2016.2572683; Michalski V., 2014, NIPS; Mnih V, 2013, PLAYING ATARI DEEP R; Reed S., 2014, ICML; SHEPARD RN, 1971, SCIENCE, V171, P701, DOI 10.1126/science.171.3972.701; Simonyan Karen, 2015, INT C LEARN REPR; Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349; Tieleman T., 2014, THESIS; Vinyals O., 2015, CVPR; Zhu Xiangyu, 2015, CVPR; Zhu Zhenyao, 2014, NIPS	29	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100007
C	Yarkony, J; Fowlkes, CC		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Yarkony, Julian; Fowlkes, Charless C.			Planar Ultrametrics for Image Segmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We study the problem of hierarchical clustering on planar graphs. We formulate this in terms of finding the closest ultrametric to a specified set of distances and solve it using an LP relaxation that leverages minimum cost perfect matching as a subroutine to efficiently explore the space of planar partitions. We apply our algorithm to the problem of hierarchical image segmentation.	[Yarkony, Julian] Experian Data Lab, San Diego, CA 92130 USA; [Fowlkes, Charless C.] Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92717 USA	University of California System; University of California Irvine	Yarkony, J (corresponding author), Experian Data Lab, San Diego, CA 92130 USA.	julian.yarkony@experian.com; fowlkes@ics.uci.edu			NSF [IIS-1253538, DBI-1262547]; Experian	NSF(National Science Foundation (NSF)); Experian	JY acknowledges the support of Experian, CF acknowledges support of NSF grants IIS-1253538 and DBI-1262547	Ailon N, 2005, ANN IEEE SYMP FOUND, P73, DOI 10.1109/SFCS.2005.36; Andres B., 2012, P ECCV; Andres B., 2013, P EMMCVPR; Andres B, 2011, IEEE I CONF COMP VIS, P2611, DOI 10.1109/ICCV.2011.6126550; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Bachrach Yoram, 2013, P AAAI; Bagon S., 2011, CORR; BARAHONA F, 1986, MATH PROGRAM, V36, P157, DOI 10.1007/BF02592023; BARAHONA F, 1982, J PHYS A-MATH GEN, V15, P3241, DOI 10.1088/0305-4470/15/10/028; Barahona F, 1991, MATH PROGRAM, V36, P53; Beier T, 2014, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2014.17; Deza M., 1997, ALGORITHMS COMBINATO, V15; FISHER ME, 1966, J MATH PHYS, V7, P1776, DOI 10.1063/1.1704825; Kim Sungwoong, 2011, ADV NEURAL INFORM PR; Kolmogorov V, 2009, MATH PROGRAM COMPUT, V1, P43, DOI 10.1007/s12532-009-0002-8; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Yarkony Julian, 2014, NEW FRONTIERS MINING; Yarkony Julian, 2014, NIPS 2014 WORKSH; Yarkony Julian, 2012, P ECCV; Zhang C, 2014, LECT NOTES COMPUT SC, V8673, P9, DOI 10.1007/978-3-319-10404-1_2	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101062
C	Yen, IEH; Zhong, K; Hsieh, CJ; Ravikumar, P; Dhillon, IS		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Yen, Ian E. H.; Zhong, Kai; Hsieh, Cho-Jui; Ravikumar, Pradeep; Dhillon, Inderjit S.			Sparse Linear Programming via Primal and Dual Augmented Coordinate Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Over the past decades, Linear Programming (LP) has been widely used in different areas and considered as one of the mature technologies in numerical optimization. However, the complexity offered by state-of-the-art algorithms (i.e. interior-point method and primal, dual simplex methods) is still unsatisfactory for problems in machine learning with huge number of variables and constraints. In this paper, we investigate a general LP algorithm based on the combination of Augmented Lagrangian and Coordinate Descent (AL-CD), giving an iteration complexity of O((log(1/epsilon))(2)) with O(nnz(A)) cost per iteration, where nnz(A) is the number of non-zeros in the m x n constraint matrix A, and in practice, one can further reduce cost per iteration to the order of non-zeros in columns (rows) corresponding to the active primal (dual) variables through an active-set strategy. The algorithm thus yields a tractable alternative to standard LP methods for large-scale problems of sparse solutions and nnz(A) << mn. We conduct experiments on large-scale LP instances from l(1)-regularized multi-class SVM, Sparse Inverse Covariance Estimation, and Nonnegative Matrix Factorization, where the proposed approach finds solutions of 10(-3) precision orders of magnitude faster than state-of-the-art implementations of interior-point and simplex methods.(1)	[Yen, Ian E. H.; Zhong, Kai; Ravikumar, Pradeep; Dhillon, Inderjit S.] Univ Texas Austin, Austin, TX 78712 USA; [Hsieh, Cho-Jui] Univ Calif Davis, Davis, CA 95616 USA	University of Texas System; University of Texas Austin; University of California System; University of California Davis	Yen, IEH (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	ianyen@cs.utexas.edu; zhongkai@ices.utexas.edu; chohsieh@ucdavis.edu; pradeepr@cs.utexas.edu; inderjit@cs.utexas.edu			ARO [W911NF-12-1-0390]; NSF [CCF-1320746, CCF-1117055, IIS-1149803, IIS-1320894, IIS-1447574, DMS-1264033]; NIH as part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences [R01 GM117594-01]	ARO; NSF(National Science Foundation (NSF)); NIH as part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences	We acknowledge the support of ARO via W911NF-12-1-0390, and the support of NSF via grants CCF-1320746, CCF-1117055, IIS-1149803, IIS-1320894, IIS-1447574, DMS-1264033, and NIH via R01 GM117594-01 as part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences.	Bello D, 2006, 2006 IEEE SYSTEMS AND INFORMATION ENGINEERING DESIGN SYMPOSIUM, P90, DOI 10.1109/SIEDS.2006.278719; Chen C., 2014, MATH PROGRAMMING; Delbos F., 2003, GLOBAL LINEAR CONVER; Dhillon I. S., 2011, NIPS; Eleuterio Vania Lucia Dos Santos, 2009, THESIS; Evtushenko YG, 2005, OPTIM METHOD SOFTW, V20, P515, DOI 10.1080/10556780500139690; Gillis N., 2014, JMLR; Gondzio J., 2012, EJOR; Gondzio J., 2012, COMPUTATIONAL OPTIMI; GULER O, 1992, J OPTIMIZ THEORY APP, V75, P445, DOI 10.1007/BF00940486; HOFFMAN AJ, 1952, J RES NAT BUR STAND, V49, P263, DOI 10.6028/jres.049.027; Hong M., 2012, LINEAR CONVERGENCE A; Hsieh C., 2008, ICML, V307; Joachims T., 2006, KDD; Koller D., 2009, PROBABILISTIC GRAPHI; Lin Ting-Wei, 2014, NIPS; Meindl B., 2012, EUROSTAT STAT NETHER; More JJ, 1991, SIAM J OPTIMIZ, V1, P93, DOI 10.1137/0801008; Nellore A., 2013, RECOVERY GUARANTEES; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Rahimi A., 2007, NIPS; Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z; Wang H., 2014, NIPS; Wang PW, 2014, J MACH LEARN RES, V15, P1523; Yen I., 2013, KDD; Yen I., 2015, NIPS; Yen I., 2015, ICML; Yuan G., 2010, JMLR, V11; Yuan M., 2010, JMLR; Zhong K., 2014, NIPS; ZHU J, 2004, NIPS	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102066
C	Yen, IEH; Lin, SW; Lin, SD		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Yen, Ian E. H.; Lin, Shan-Wei; Lin, Shou-De			A Dual-Augmented Block Minimization Framework for Learning with Limited Memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					In past few years, several techniques have been proposed for training of linear Support Vector Machine (SVM) in limited-memory setting, where a dual block-coordinate descent (dual-BCD) method was used to balance cost spent on I/O and computation. In this paper, we consider the more general setting of regularized Empirical Risk Minimization (ERM) when data cannot fit into memory. In particular, we generalize the existing block minimization framework based on strong duality and Augmented Lagrangian technique to achieve global convergence for general convex ERM. The block minimization framework is flexible in the sense that, given a solver working under sufficient memory, one can integrate it with the framework to obtain a solver globally convergent under limited-memory condition. We conduct experiments on L1-regularized classification and regression problems to corroborate our convergence theory and compare the proposed framework to algorithms adopted from online and distributed settings, which shows superiority of the proposed approach on data of size ten times larger than the memory capacity.	[Yen, Ian E. H.] Univ Texas Austin, Austin, TX 78712 USA; [Lin, Shan-Wei; Lin, Shou-De] Natl Taiwan Univ, Taipei, Taiwan	University of Texas System; University of Texas Austin; National Taiwan University	Yen, IEH (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	ianyen@cs.utexas.edu; r03922067@csie.ntu.edu.tw; sdlin@csie.ntu.edu.tw		LIN, SHOU-DE/0000-0001-9970-1250	Telecommunication Lab., Chunghwa Telecom Co., Ltd [TL-103-8201]; AOARD [FA2386-13-1-4045]; Ministry of Science and Technology; National Taiwan University; Intel Co. [MOST102-2911-I-002-001, NTU103R7501, 102-2923-E-002-007-MY2, 102-2221-E-002-170, 103-2221-E-002-104-MY2]	Telecommunication Lab., Chunghwa Telecom Co., Ltd; AOARD; Ministry of Science and Technology(Ministry of Science, ICT & Future Planning, Republic of Korea); National Taiwan University(National Taiwan University); Intel Co.(Intel Corporation)	We thank to the support of Telecommunication Lab., Chunghwa Telecom Co., Ltd via TL-103-8201, AOARD via No. FA2386-13-1-4045, Ministry of Science and Technology, National Taiwan University and Intel Co. via MOST102-2911-I-002-001, NTU103R7501, 102-2923-E-002-007-MY2, 102-2221-E-002-170, 103-2221-E-002-104-MY2.	[Anonymous], 2011, FDN TRENDS MACHINE L; Chang K.-W., 2011, SIGKDD; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Hoffman A.J., 1952, J RES NBS; Hong M., 2012, LINEAR CONVERGENCE A; Hsieh C., 2014, NIPS; Jaggi M., 2014, NIPS; Joachims T., 2005, ICML; Kakade S., 2009, CORR; Lin Ting-Wei, 2014, NIPS; Ma C., 2015, ICML; Obozinski G., 2011, GROUP LASSO OVERLAPS; Rahimi A., 2007, NIPS; Richtarik P., 2014, MATH PROGRAMMING; Shalev-Shwartz S., 2011, JMLR; Shalev-Shwartz S., 2011, MATH PROGRAMMING; Srebro N., 2011, NIPS; Tibshirani R., 1996, J ROYAL STAT SOC; Tomioka R., 2011, JMLR; Trofimov I., 2014, DISTRIBUTED COORDINA; Wang P., 2014, JMLR; Yen I., 2013, SIGKDD; Yen I., 2015, NIPS; Yen I., 2015, ICML; Yu H., 2010, SIGKDD; Yuan Guo-Xun, 2010, JMLR; Zhong K., 2014, NIPS	27	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102072
C	Yi, XY; Wang, ZR; Caramanis, C; Liu, H		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Yi, Xinyang; Wang, Zhaoran; Caramanis, Constantine; Liu, Han			Optimal Linear Estimation under Unknown Nonlinear Transform	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				PRINCIPAL COMPONENT ANALYSIS; PHASE RETRIEVAL; SPARSE	Linear regression studies the problem of estimating a model parameter beta* is an element of R-p, from n observations {(y(i), X-i)}(i=1)(n) from linear model y(i) = < X-i, beta*> + is an element of(i). We consider a significant generalization in which the relationship between < X-i, beta*> and y(i) is noisy, quantized to a single bit, potentially nonlinear, noninvertible, as well as unknown. This model is known as the single-index model in statistics, and, among other things, it represents a significant generalization of one-bit compressed sensing. We propose a novel spectral-based estimation procedure and show that we can recover beta* in settings (i.e., classes of link function f) where previous algorithms fail. In general, our algorithm requires only very mild restrictions on the (unknown) functional relationship between y(i) and < X-i, beta*>. We also consider the high dimensional setting where beta* is sparse, and introduce a two-stage nonconvex framework that addresses estimation challenges in high dimensional regimes where p >> n. For a broad class of link functions between < X-i, beta*> and y(i), we establish minimax lower bounds that demonstrate the optimality of our estimators in both the classical and high dimensional regimes.	[Yi, Xinyang; Caramanis, Constantine] Univ Texas Austin, Austin, TX 78712 USA; [Wang, Zhaoran; Liu, Han] Princeton Univ, Princeton, NJ 08544 USA	University of Texas System; University of Texas Austin; Princeton University	Yi, XY (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	yixy@utexas.edu; zhaoran@princeton.edu; constantine@utexas.edu; hanliu@princeton.edu	Wang, Zhaoran/P-7113-2018		NSF [1056028, 1302435, 1116955, IIS1408910, IIS1332109]; U.S. Department of Transportation through the Data-Supported Transportation Operations and Planning (D-STOP) Tier 1 University Transportation Center; NSF CAREER Award [DMS1454377]; NIH [R01MH102339, R01GM083084, R01HG06841]; MSR PhD fellowship	NSF(National Science Foundation (NSF)); U.S. Department of Transportation through the Data-Supported Transportation Operations and Planning (D-STOP) Tier 1 University Transportation Center; NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); MSR PhD fellowship	XY and CC would like to acknowledge NSF grants 1056028, 1302435 and 1116955. This research was also partially supported by the U.S. Department of Transportation through the Data-Supported Transportation Operations and Planning (D-STOP) Tier 1 University Transportation Center. HL is grateful for the support of NSF CAREER Award DMS1454377, NSF IIS1408910, NSF IIS1332109, NIH R01MH102339, NIH R01GM083084, and NIH R01HG06841. ZW was partially supported by MSR PhD fellowship while this work was done.	Alquier P, 2013, J MACH LEARN RES, V14, P243; Berthet Q., 2013, C LEARN THEOR; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Cai TT, 2013, ANN STAT, V41, P3074, DOI 10.1214/13-AOS1178; d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269; d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506; Delecroix M, 2006, J STAT PLAN INFER, V136, P730, DOI 10.1016/j.jspi.2004.09.006; Delecroix M., 2000, TECH REP; Eldar YC, 2014, APPL COMPUT HARMON A, V36, P473, DOI 10.1016/j.acha.2013.08.003; GOPI S., 2013, INT C MACH LEARN; HARDLE W, 1993, ANN STAT, V21, P157, DOI 10.1214/aos/1176349020; Hristache M, 2001, ANN STAT, V29, P595; Jacques L., 2011, ARXIV11043160; Kakade S. M, 2011, ADV NEURAL INFORM PR; Kalai A. T., 2009, C LEARN THEOR; Ma ZM, 2013, ANN STAT, V41, P772, DOI 10.1214/13-AOS1097; Massart P, 2007, LECT NOTES MATH, V1896, P1, DOI 10.1007/978-3-540-48503-2; Natarajan N., 2013, NEURAL INFORM PROCES, V26; Plan Y., 2014, ARXIV14043749; Plan Y, 2013, COMMUN PUR APPL MATH, V66, P1275, DOI 10.1002/cpa.21442; POWELL JL, 1989, ECONOMETRICA, V57, P1403, DOI 10.2307/1913713; Shen HP, 2008, J MULTIVARIATE ANAL, V99, P1015, DOI 10.1016/j.jmva.2007.06.007; STOKER TM, 1986, ECONOMETRICA, V54, P1461, DOI 10.2307/1914309; Tibshirani Julie, 2013, ARXIV13054987; Vu V. Q., 2013, ADV NEURAL INFORM PR; Witten DM, 2009, BIOSTATISTICS, V10, P515, DOI 10.1093/biostatistics/kxp008; Yi Xinyang, 2015, ARXIV150503257; Yu B., 1997, FESTSCHRIFT L LECAM, P423; Yuan XT, 2013, J MACH LEARN RES, V14, P899; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103051
C	Yoshikawa, Y; Iwata, T; Sawada, H; Yamada, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Yoshikawa, Yuya; Iwata, Tomoharu; Sawada, Hiroshi; Yamada, Takeshi			Cross-Domain Matching for Bag-of-Words Data via Kernel Embeddings of Latent Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				SPACE	We propose a kernel-based method for finding matching between instances across different domains, such as multilingual documents and images with annotations. Each instance is assumed to be represented as a multiset of features, e.g., a bag-of-words representation for documents. The major difficulty in finding cross-domain relationships is that the similarity between instances in different domains cannot be directly measured. To overcome this difficulty, the proposed method embeds all the features of different domains in a shared latent space, and regards each instance as a distribution of its own features in the shared latent space. To represent the distributions efficiently and nonparametrically, we employ the framework of the kernel embeddings of distributions. The embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart. In our experiments, we show that the proposed method can achieve high performance on finding correspondence between multi-lingual Wikipedia articles, between documents and tags, and between images and tags.	[Yoshikawa, Yuya] Nara Inst Sci & Technol, Nara 6300192, Japan; [Iwata, Tomoharu; Yamada, Takeshi] NTT Commun Sci Labs, Kyoto 6190237, Japan; [Sawada, Hiroshi] NTT Serv Evolut Labs, Yokosuka, Kanagawa 2390847, Japan	Nara Institute of Science & Technology; Nippon Telegraph & Telephone Corporation	Yoshikawa, Y (corresponding author), Chiba Inst Technol, Software Technol & Artificial Intelligence Res La, Narashino, Chiba, Japan.	yoshikawa.yuya.yl9@is.naist.jp; iwata.tomoharu@lab.ntt.co.jp; sawada.hiroshi@lab.ntt.co.jp; yamada.tak@lab.ntt.co.jp	Yamada, Takeshi/AAE-5235-2020		JSPS [259867]	JSPS(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science)	This work was supported by JSPS Grant-in-Aid for JSPS Fellows (259867).	Akaho S., 2001, P INT M PSYCH SOC; Andrew G., 2013, INT C MACH LEARN, p1247?1255; Dudik M, 2007, J MACH LEARN RES, V8, P1217; Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4; Gretton A., 2008, ADV NEURAL INFORM PR; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Iwata Tomoharu, 2009, ADV NEURAL INFORM PR; Iwata Tomoharu, 2011, P 22 INT JOINT C ART; Kamencay P, 2014, INT J ADV ROBOT SYST, V11, DOI 10.5772/58251; Li Bin, 2009, P 26 ANN INT C MACH; Li YY, 2006, J INTELL INF SYST, V27, P117, DOI 10.1007/s10844-006-1627-y; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Muandet Krikamol, 2012, ADVANCES IN NEURAL I; Muandet Krikamol, 2013, P 29 C UNC ART INT; Ngiam J, 2011, P 28 INT C MACH LEAR, V28, P689, DOI DOI 10.5555/3104482.3104569; Rasiwasia N., 2010, P INT C MULT; Sejdinovic D., 2013, ADV NEURAL INFORM PR; Smola A., 2007, ALGORITHMIC LEARNING; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Vinokourov A., 2003, ADV NEURAL INFORM PR; Yoshikawa Y, 2015, AAAI CONF ARTIF INTE, P3129; Yoshikawa Yuya, 2014, ADV NEURAL INFORM PR; Zhang T., 2013, P 23 INT JOINT C ART	23	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102107
C	Yun, SY; Lelarge, M; Proutiere, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Yun, Se-Young; Lelarge, Marc; Proutiere, Alexandre			Fast and Memory Optimal Low-Rank Matrix Approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					In this paper, we revisit the problem of constructing a near-optimal rank k approximation of a matrix M is an element of [0, 1](mxn) under the streaming data model where the columns of M are revealed sequentially. We present SLA (Streaming Low-rank Approximation), an algorithm that is asymptotically accurate, when ks(k+1) (M) = o(root mn) where s(k+1)(M) is the (k + 1)-th largest singular value of M. This means that its average mean-square error converges to 0 as m and n grow large (i.e., parallel to(M) over cap ((k)) - M-(k)parallel to(2)(F) = o(mn) with high probability, where (M) over cap ((k)) and M-(k) denote the output of SLA and the optimal rank k approximation of M, respectively). Our algorithm makes one pass on the data if the columns of M are revealed in a random order, and two passes if the columns of M arrive in an arbitrary order. To reduce its memory footprint and complexity, SLA uses random sparsification, and samples each entry of M with a small probability delta. In turn, SLA is memory optimal as its required memory space scales as k (m + n), the dimension of its output. Furthermore, SLA is computationally efficient as it runs in O(delta kmn) time (a constant number of operations is made for each observed entry of M), which can be as small as O (k log(m)(4)n) for an appropriate choice of delta and if n >= m.	[Yun, Se-Young] MSR, Cambridge, England; [Lelarge, Marc] Inria, Rocquencourt, France; [Lelarge, Marc] ENS, Paris, France; [Proutiere, Alexandre] KTH, EE Sch, ACL, Stockholm, Sweden	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Royal Institute of Technology	Yun, SY (corresponding author), MSR, Cambridge, England.	seyoung.yun@inria.fr; marc.lelarge@ens.fr; alepro@kth.se	Yun, Seyoung/M-6903-2017		French Agence Nationale de la Recherche (ANR) [ANR-11-JS02-005-01]; ERC FSA grant; SSF ICT-Psi project	French Agence Nationale de la Recherche (ANR)(French National Research Agency (ANR)); ERC FSA grant; SSF ICT-Psi project	Work performed as part of MSR-INRIA joint research centre. M.L. acknowledges the support of the French Agence Nationale de la Recherche (ANR) under reference ANR-11-JS02-005-01 (GAP project).; A. Proutiere's research is supported by the ERC FSA grant, and the SSF ICT-Psi project.	Achlioptas D, 2007, J ACM, V54, DOI 10.1145/1219092.1219097; Bhojanapalli S., 2015, P 2015 ANN ACM SIAM, P902, DOI DOI 10.1137/1.9781611973730.62; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Clarkson KL, 2009, ACM S THEORY COMPUT, P205; Ghashami M., 2014, P 25 ANN ACM SIAM S, P707, DOI [DOI 10.1137/1.9781611973402.53, 10.1137/1.9781611973402.53]; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Liberty E, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P581, DOI 10.1145/2487575.2487623; Mitliagkas I., 2013, ADV NEURAL INFORM PR; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; Tropp JA, 2011, ADV DATA SCI ADAPT, V3, P115, DOI 10.1142/S1793536911000787; Woodruff David P, 2014, ADV NEURAL INFORM PR, P1781	11	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102077
C	Zhang, CC; Song, JM; Chen, KC; Chaudhuri, K		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Zhang, Chicheng; Song, Jimin; Chen, Kevin C.; Chaudhuri, Kamalika			Spectral Learning of Large Structured HMMs for Comparative Epigenomics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				DISCOVERY	We develop a latent variable model and an efficient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types. A natural model for chromatin data in one cell type is a Hidden Markov Model (HMM); we model the relationship between multiple cell types by connecting their hidden states by a fixed tree of known structure. The main challenge with learning parameters of such models is that iterative methods such as EM are very slow, while naive spectral methods result in time and space complexity exponential in the number of cell types. We exploit properties of the tree structure of the hidden states to provide spectral algorithms that are more computationally efficient for current biological datasets. We provide sample complexity bounds for our algorithm and evaluate it experimentally on biological data from nine human cell types. Finally, we show that beyond our specific model, some of our algorithmic ideas can be applied to other graphical models.	[Zhang, Chicheng; Chaudhuri, Kamalika] Univ Calif San Diego, San Diego, CA 92103 USA; [Song, Jimin; Chen, Kevin C.] Rutgers State Univ, New Brunswick, NJ USA	University of California System; University of California San Diego; Rutgers State University New Brunswick	Zhang, CC (corresponding author), Univ Calif San Diego, San Diego, CA 92103 USA.	chz038@eng.ucsd.edu; song@dls.rutgers.edu; kcchen@dls.rutgers.edu; kamalika@eng.ucsd.edu	Chen, Kevin/GYU-8963-2022		NSF [IIS 1162581]	NSF(National Science Foundation (NSF))	KC and CZ thank NSF under IIS 1162581 for research support.	Anandkumar A., 2012, ABS12107559 CORR; [Anonymous], [No title captured]; Balle B., 2014, MACHINE LEARNING, V96; Balle B, 2014, PR MACH LEARN RES, V32, P1386; Bernstein BE, 2010, NAT BIOTECHNOL, V28, P1045, DOI 10.1038/nbt1010-1045; Biesinger J, 2013, BMC BIOINFORMATICS, V14, DOI 10.1186/1471-2105-14-S5-S4; Chaganty A., 2014, ICML; Creyghton MP, 2010, P NATL ACAD SCI USA, V107, P21931, DOI 10.1073/pnas.1016071107; Djebali S., 2012, NATURE; Dunham I, 2012, NATURE, V489, P57, DOI 10.1038/nature11247; Ernst J, 2011, NATURE, V473, P43, DOI 10.1038/nature09906; Ernst J, 2010, NAT BIOTECHNOL, V28, P817, DOI 10.1038/nbt.1662; Foster D., 2012, CORR; Foti N., 2014, NIPS; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hoffman MM, 2012, NAT METHODS, V9, P473, DOI [10.1038/NMETH.1937, 10.1038/nmeth.1937]; Hsu D. J., 2009, COLT; Melnyk I., 2015, AISTATS; Mossel E., 2006, ANN APPL PROBAB, V16; Parikh A. P., 2011, P 28 INT C MACH LEAR, P1065; Parikh AP, 2012, UAI; Siddiqi S., 2010, AISTATS; Song JM, 2015, GENOME BIOL, V16, DOI 10.1186/s13059-015-0598-0; Song L., 2013, ICML; Zhu Jun, 2010, PLOS COMPUT BIOL, V6; Zou J., 2013, NIPS	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102028
C	Zheng, QQ; Lafferty, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Zheng, Qinqing; Lafferty, John			A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We propose a simple, scalable, and fast gradient descent algorithm to optimize a nonconvex objective for the rank minimization problem and a closely related family of semidefinite programs. With O(r(3)kappa(2)n log n) random measurements of a positive semidefinite nxn matrix of rank r and condition number kappa, our method is guaranteed to converge linearly to the global optimum.	[Zheng, Qinqing; Lafferty, John] Univ Chicago, Chicago, IL 60637 USA	University of Chicago	Zheng, QQ (corresponding author), Univ Chicago, Chicago, IL 60637 USA.	qinqing@cs.uchicago.edu; lafferty@galton.uchicago.edu			NSF [IIS-1116730]; ONR [N00014-12-1-0762]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	Research supported in part by NSF grant IIS-1116730 and ONR grant N00014-12-1-0762.	Amini AA, 2009, ANN STAT, V37, P2877, DOI 10.1214/08-AOS664; Bach F, 2014, J MACH LEARN RES, V15, P595; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes E., 2014, ARXIV14071065; d'Aspremont A., 2004, ADV NEURAL INFORM PR; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hoffman Matt, 2013, J MACHINE LEARNING R, V14; Jain P, 2010, P ADV NEUR INF PROC, P937; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Laurent B, 2000, ANN STAT, V28, P1302; Ledoux M, 2010, ELECTRON J PROBAB, V15, P1319, DOI 10.1214/EJP.v15-798; Meka R., 2008, P 25 INT C MACH LEAR, P656; Moulines E., 2011, ADV NEURAL INF PROCE, V24; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Netrapalli P., 2013, P ADV NEUR INF PROC, P2796; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Tomioka R., 2010, ARXIV10100789; Tropp J. A., 2015, ARXIV150101571; Tu S., 2015, ARXIV150703566	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101089
C	Zheng, QQ; Tomioka, R		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Zheng, Qinqing; Tomioka, Ryota			Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We consider the problem of recovering a low-rank tensor from its noisy observation. Previous work has shown a recovery guarantee with signal to noise ratio O (n(inverted right perpendicularK/2inverted left perpendicular/2)) for recovering a K th order rank one tensor of size n x...x n by recursive unfolding. In this paper, we first improve this bound to O (n(K/4)) by a much simpler approach, but with a more careful analysis. Then we propose a new norm called the subspace norm, which is based on the Kronecker products of factors obtained by the proposed simple estimator. The imposed Kronecker structure allows us to show a nearly ideal O (root n + root HK-1) bound, in which the parameter H controls the blend from the non-convex estimator to mode-wise nuclear norm minimization. Furthermore, we empirically demonstrate that the subspace norm achieves the nearly ideal denoising performance even with H = O (1).	[Zheng, Qinqing] Univ Chicago, Chicago, IL 60637 USA; [Tomioka, Ryota] Toyota Technol Inst, Chicago, IL USA	University of Chicago; Toyota Technological Institute - Chicago	Zheng, QQ (corresponding author), Univ Chicago, Chicago, IL 60637 USA.	qinqing@cs.uchicago.edu; tomioka@ttic.edu							0	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101082
C	Zhong, MJ; Goddard, N; Sutton, C		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Zhong, Mingjun; Goddard, Nigel; Sutton, Charles			Latent Bayesian melding for integrating individual and population models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				INFERENCE	In many statistical problems, a more coarse-grained model may be suitable for population-level behaviour, whereas a more detailed model is appropriate for accurate modelling of individual behaviour. This raises the question of how to integrate both types of models. Methods such as posterior regularization follow the idea of generalized moment matching, in that they allow matching expectations between two models, but sometimes both models are most conveniently expressed as latent variable models. We propose latent Bayesian melding, which is motivated by averaging the distributions over populations statistics of both the individual-level and the population-level models under a logarithmic opinion pool framework. In a case study on electricity disaggregation, which is a type of single-channel blind source separation problem, we show that latent Bayesian melding leads to significantly more accurate predictions than an approach based solely on generalized moment matching.	[Zhong, Mingjun; Goddard, Nigel; Sutton, Charles] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland	University of Edinburgh	Zhong, MJ (corresponding author), Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.	mzhong@inf.ed.ac.uk; nigel.goddard@inf.ed.ac.uk; csutton@inf.ed.ac.uk			Engineering and Physical Sciences Research Council, UK [EP/K002732/1, EP/M008223/1]	Engineering and Physical Sciences Research Council, UK(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work is supported by the Engineering and Physical Sciences Research Council, UK (grant numbers EP/K002732/1 and EP/M008223/1).	Alkema L, 2007, ANN APPL STAT, V1, P229, DOI 10.1214/07-AOAS111; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Batra Nipun, 2014, P E ENERGY 2014 5 AC, P265, DOI DOI 10.1145/2602044.2602051; BORDLEY RF, 1982, MANAGE SCI, V28, P1137, DOI 10.1287/mnsc.28.10.1137; Chiu GS, 2010, ENVIRONMETRICS, V21, P728, DOI 10.1002/env.1035; Elhamifar E, 2015, AAAI CONF ARTIF INTE, P629; Ganchev K, 2010, J MACH LEARN RES, V11, P2001; Ghosh S., 2012, P 26 AAAI C ART INT, V1, P356, DOI 10.5555/2900728.2900780; Giffin A., 2007, 27 INT WORKSH BAYES; HART GW, 1992, P IEEE, V80, P1870, DOI 10.1109/5.192069; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; Kelly J., 2015, UK DALE DATASET DOME, V2; Kim H., 2011, P SIAM C DAT MIN MES, DOI [10.1137/1.9781611972818.64, 10.1137/1, DOI 10.1137/1]; Kolter J.Z., 2012, P ARTIFICIAL INTELLI, P1472; Liang Percy, 2009, P 26 ANN INT C MACH, P641; MacKinnon JG, 1998, J ECONOMETRICS, V85, P205, DOI 10.1016/S0304-4076(97)00099-7; Mann G., 2008, P ACL, P870; Myerscough Keith, 2014, ARXIV14116011; Poole D, 2000, J AM STAT ASSOC, V95, P1244, DOI 10.2307/2669764; Rufo MJ, 2012, BAYESIAN ANAL, V7, P411, DOI 10.1214/12-BA714; Sevcikova H, 2011, TRANSPORT RES A-POL, V45, P540, DOI 10.1016/j.tra.2011.03.009; Villelaa Daniel AM, 2015, CHOICE WEIGHTS LOGAR; WOLPERT RL, 1995, J AM STAT ASSOC, V90, P426, DOI 10.2307/2291052; Wytock M, 2014, AAAI CONF ARTIF INTE, P486; Zhong MJ, 2014, ADV NEUR IN, V27; Zimmermann JP, 2012, HOUSEHOLD ELECT SURV	26	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101016
C	Acerbi, L; Ma, WJ; Vijayakumar, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Acerbi, Luigi; Ma, Wei Ji; Vijayakumar, Sethu			A Framework for Testing Identifiability of Bayesian Models of Perception	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				REPRESENTATIONS	Bayesian observer models are very effective in describing human performance in perceptual tasks, so much so that they are trusted to faithfully recover hidden mental representations of priors, likelihoods, or loss functions from the data. However, the intrinsic degeneracy of the Bayesian framework, as multiple combinations of elements can yield empirically indistinguishable results, prompts the question of model identifiability. We propose a novel framework for a systematic testing of the identifiability of a significant class of Bayesian observer models, with practical applications for improving experimental design. We examine the theoretical identifiability of the inferred internal representations in two case studies. First, we show which experimental designs work better to remove the underlying degeneracy in a time interval estimation task. Second, we find that the reconstructed representations in a speed perception task under a slow-speed prior are fairly robust.	[Acerbi, Luigi; Vijayakumar, Sethu] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland; [Acerbi, Luigi; Ma, Wei Ji] NYU, Ctr Neural Sci, New York, NY 10003 USA; [Acerbi, Luigi; Ma, Wei Ji] NYU, Dept Psychol, New York, NY 10003 USA	University of Edinburgh; New York University; New York University	Acerbi, L (corresponding author), Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.	luigi.acerbi@nyu.edu; weijima@nyu.edu; sethu.vijayakumar@ed.ac.uk						Acerbi L, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003661; Acerbi L, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002771; ANDERSON JR, 1978, PSYCHOL REV, V85, P249, DOI 10.1037/0033-295X.85.4.249; Bowers JS, 2012, PSYCHOL BULL, V138, P389, DOI 10.1037/a0026450; Carreira-Perpinan MA, 2000, IEEE T PATTERN ANAL, V22, P1318, DOI 10.1109/34.888716; Chalk M, 2010, J VISION, V10, DOI 10.1167/10.8.2; Geisler WS, 2011, VISION RES, V51, P771, DOI 10.1016/j.visres.2010.09.027; Gekas N, 2013, J VISION, V13, DOI 10.1167/13.4.8; Girshick AR, 2011, NAT NEUROSCI, V14, P926, DOI 10.1038/nn.2831; Hedges JH, 2011, J VISION, V11, DOI 10.1167/11.6.14; Houlsby NMT, 2013, CURR BIOL, V23, P2169, DOI 10.1016/j.cub.2013.09.012; Jazayeri M, 2010, NAT NEUROSCI, V13, P1020, DOI 10.1038/nn.2590; Jones M, 2011, BEHAV BRAIN SCI, V34, P215, DOI [10.1017/S0140525X11001439, 10.1017/S0140525X10003134]; Knill DC, 2003, VISION RES, V43, P831, DOI 10.1016/S0042-6989(03)00003-8; Knill DC, 1996, PERCEPTION BAYESIAN; Kording KP, 2004, P NATL ACAD SCI USA, V101, P9839, DOI 10.1073/pnas.0308394101; Kording KP, 2004, NATURE, V427, P244, DOI 10.1038/nature02169; Kwon OS, 2013, P NATL ACAD SCI USA, V110, pE1064, DOI 10.1073/pnas.1214869110; Maloney LT, 2009, VISUAL NEUROSCI, V26, P147, DOI 10.1017/S0952523808080905; Mamassian P, 2010, NAT NEUROSCI, V13, P914, DOI 10.1038/nn0810-914; Natarajan R., 2009, ADV NEURAL INFORM PR, V21, P1153; Navarro DJ, 2004, COGNITIVE PSYCHOL, V49, P47, DOI 10.1016/j.cogpsych.2003.11.001; Pouget A, 2013, NAT NEUROSCI, V16, P1170, DOI 10.1038/nn.3495; Sanborn A. N., 2008, ADV NEURAL INFORM PR, V20, P1265; Simoncelli, 2009, COGNITIVE NEUROSCIEN, VIV, P525; Spiegelhalter DJ, 2002, J R STAT SOC B, V64, P583, DOI 10.1111/1467-9868.00353; Stocker AA, 2006, NAT NEUROSCI, V9, P578, DOI 10.1038/nn1669; Tassinari H, 2006, J NEUROSCI, V26, P10154, DOI 10.1523/JNEUROSCI.2779-06.2006; Trommershauser J, 2008, TRENDS COGN SCI, V12, P291, DOI 10.1016/j.tics.2008.04.010; Vilares I, 2012, CURR BIOL, V22, P1641, DOI 10.1016/j.cub.2012.07.010	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103004
C	Acharya, J; Jafarpour, A; Orlitsky, A; Suresh, AT		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Acharya, Jayadev; Jafarpour, Ashkan; Orlitsky, Alon; Suresh, Ananda Theertha			Near-Optimal-Sample Estimators for Spherical Gaussian Mixtures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Many important distributions are high dimensional, and often they can be modeled as Gaussian mixtures. We derive the first sample-efficient polynomial-time estimator for high-dimensional spherical Gaussian mixtures. Based on intuitive spectral reasoning, it approximates mixtures of k spherical Gaussians in d-dimensions to within l(1) distance epsilon using O(dk(9) (log(2) d)/epsilon(4)) samples and O-k,O-epsilon (d(3) log(5) d) computation time. Conversely, we show that any estimator requires Omega dk/epsilon(2) samples, hence the algorithm's sample complexity is nearly optimal in the dimension. The implied time-complexity factor O-k,O-epsilon is exponential in k, but much smaller than previously known. We also construct a simple estimator for one-dimensional Gaussian mixtures that uses (O) over tilde (k/epsilon(2)) samples and (O) over tilde((k/epsilon)(3k+1)) computation time.	[Acharya, Jayadev] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Acharya, Jayadev; Jafarpour, Ashkan; Orlitsky, Alon; Suresh, Ananda Theertha] Univ Calif San Diego, San Diego, CA USA	Massachusetts Institute of Technology (MIT); University of California System; University of California San Diego	Acharya, J (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	jayadev@mit.edu; ashkan@ucsd.edu; alon@ucsd.edu; asuresh@ucsd.edu						Acharya J., 2014, ISIT; Achlioptas Dimitris, 2005, COLT; Anderson J., 2014, COLT; Assouad B. Yu., 1997, FESTSCHRIFT L LECAM; Azizyan M., 2013, NIPS; Belkin M., 2010, FOCS; Chan S., 2013, SODA; Chan S., 2014, STOC; Chaudhuri K., 2009, CORR; Dasgupta S., 2000, UAI; Dasgupta Sanjoy, 1999, FOCS; Daskalakis C., 2012, SODA; Daskalakis C., 2014, COLT; Devroye L., 2001, SPRINGER SERIES STAT; Dhillon I. S., 2002, ICDM; Feldman J., 2006, COLT; Feldman J., 2005, FOCS; Freund Y., 1999, COLT; Hsu Daniel, 2013, ITCS; Kalai A. T., 2010, STOC; Kearns Michael, 1994, STOC; Ma J., 2001, NEURAL COMPUTATION, V12; Moitra A., 2010, FOCS; Orlitsky A., 2004, UAI; Paninski L., 2004, NIPS; REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034; REYNOLDS DA, 1995, IEEE T SPEECH AUDI P, V3, P72, DOI 10.1109/89.365379; Titterington DM, 1985, STAT ANAL FINITE MIX; Valiant G., 2011, STOC; Vempala S., 2002, FOCS; Vershynin Roman, 2010, CORR; Xing E. P., 2001, ICML	32	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100030
C	Adametz, D; Roth, V		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Adametz, David; Roth, Volker			Distance-Based Network Recovery under Feature Correlation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				MARGINAL LIKELIHOOD; REPAIR; MODELS	We present an inference method for Gaussian graphical models when only pair-wise distances of n objects are observed. Formally, this is a problem of estimating an n x n covariance matrix from the Mahalanobis distances d(MH) (x(i), x(j)), where object x(i) lives in a latent feature space. We solve the problem in fully Bayesian fashion by integrating over the Matrix-Normal likelihood and a Matrix-Gamma prior; the resulting Matrix-T posterior enables network recovery even under strongly correlated features. Hereby, we generalize TiWnet [19], which assumes Euclidean distances with strict feature independence. In spite of the greatly increased flexibility, our model neither loses statistical power nor entails more computational cost. We argue that the extension is highly relevant as it yields significantly better results in both synthetic and real-world experiments, which is successfully demonstrated for a network of biological pathways in cancer patients.	[Adametz, David; Roth, Volker] Univ Basel, Dept Math & Comp Sci, Basel, Switzerland	University of Basel	Adametz, D (corresponding author), Univ Basel, Dept Math & Comp Sci, Basel, Switzerland.	david.adametz@unibas.ch; volker.roth@unibas.ch	Roth, Volker/Q-4025-2017	Roth, Volker/0000-0003-0991-0273				Allen GI, 2010, ANN APPL STAT, V4, P764, DOI 10.1214/09-AOAS314; Bhattacharyya A., 1943, BULL CALCUTTA MATH S, V35, P99; Daniels MJ, 2009, J MULTIVARIATE ANAL, V100, P2352, DOI 10.1016/j.jmva.2009.04.015; de Vos A., 2008, TECHNICAL REPORT; Ein-Dor L, 2006, P NATL ACAD SCI USA, V103, P5923, DOI 10.1073/pnas.0601231103; Fortini P, 2003, BIOCHIMIE, V85, P1053, DOI 10.1016/j.biochi.2003.11.003; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; GUPTA A, 1999, PMS SERIES; HARVILLE DA, 1977, J AM STAT ASSOC, V72, P320, DOI 10.2307/2286796; Iranmanesh A, 2010, IRAN J MATH SCI INFO, V5, P33, DOI 10.7508/ijmsi.2010.02.004; JEBARA T, 2003, C LEARN THEOR; KALBFLEI.JD, 1970, J ROY STAT SOC B, V32, P175; McCullagh P, 2009, STAT SINICA, V19, P631; McCullagh P, 2008, BERNOULLI, V14, P593, DOI 10.3150/07-BEJ119; MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129; Murphy SA, 2000, J AM STAT ASSOC, V95, P449, DOI 10.2307/2669386; PATTERSON HD, 1971, BIOMETRIKA, V58, P545, DOI 10.1093/biomet/58.3.545; Peltomaki P, 2001, MUTAT RES-REV MUTAT, V488, P77, DOI 10.1016/S1383-5742(00)00058-2; Prabhakaran S, 2013, MACH LEARN, V92, P251, DOI 10.1007/s10994-013-5370-7; Sheffer M, 2009, P NATL ACAD SCI USA, V106, P7131, DOI 10.1073/pnas.0902232106	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102031
C	Agarwal, A; Beygelzimer, A; Hsu, D; Langford, J; Telgarsky, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Agarwal, Alekh; Beygelzimer, Alma; Hsu, Daniel; Langford, John; Telgarsky, Matus			Scalable Nonlinear Learning with Adaptive Polynomial Expansions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ONLINE	Can we effectively learn a nonlinear representation in time comparable to linear learning ? We describe a new algorithm that explicitly and adaptively expands higher-order interaction features over base linear representations. The algorithm is designed for extreme computational efficiency, and an extensive experimental study shows that its computation/prediction tradeoff ability compares very favorably against strong baselines.	[Agarwal, Alekh; Langford, John] Microsoft Res, Redmond, WA 98052 USA; [Beygelzimer, Alma] Yahoo Labs, Sunnyvale, CA USA; [Hsu, Daniel] Columbia Univ, New York, NY 10027 USA; [Telgarsky, Matus] Rutgers State Univ, New Brunswick, NJ USA; [Telgarsky, Matus] Microsoft Res, New York, NY USA	Microsoft; Columbia University; Rutgers State University New Brunswick; Microsoft	Agarwal, A (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	alekha@microsoft.com; beygel@yahoo-inc.com; djhsu@cs.columbia.edu; jcl@microsoft.com; mtelgars@cs.ucsd.edu		Hsu, Daniel/0000-0002-3495-7113				Agarwal A., 2014, ARXIV14100440CSLG; Agarwal A, 2014, J MACH LEARN RES, V15, P1111; Andoni A., 2014, SODA; Blum A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P203, DOI 10.1145/307400.307439; Bordes A, 2005, J MACH LEARN RES, V6, P1579; Bubeck S., 2014, ARXIV14054980MATHOC; Dimakis A. G., 2014, CORR; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Hall K., 2010, WORKSH LEARN COR CLU; Hamid R., 2014, ICML; IVAKHNENKO AG, 1971, IEEE T SYST MAN CYB, VSMC1, P364, DOI 10.1109/TSMC.1971.4308320; Kalai A. T., 2009, FOCS; Kar P., 2012, ARTIF INTELL, P583; Karampatziakis N, 2011, P 27 C UNC ART INT, P392; Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035; Mukherjee I., 2013, P EUR C MACH LEARN P; Pham N, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P239, DOI 10.1145/2487575.2487591; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Ross S., 2013, UAI; Sanger T. D., 1992, ADV NEURAL INFORM PR, V4; Scholkopf B., 2002, LEARNING KERNELS; Shamir O., 2013, P INT C MACH LEARN A, P71; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108; Williams CKI, 2001, ADV NEUR IN, V13, P682; Zinkevich M., 2003, P 20 INT C MACH LEAR, P928, DOI 10.5555/3041838	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101068
C	Akram, S; Simon, JZ; Shamma, S; Babadi, B		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Akram, Sahar; Simon, Jonathan Z.; Shamma, Shihab; Babadi, Behtash			A State-Space Model for Decoding Auditory Attentional Modulation from MEG in a Competing-Speaker Environment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				SPEECH	Humans are able to segregate auditory objects in a complex acoustic scene, through an interplay of bottom-up feature extraction and top-down selective attention in the brain. The detailed mechanism underlying this process is largely unknown and the ability to mimic this procedure is an important problem in artificial intelligence and computational neuroscience. We consider the problem of decoding the attentional state of a listener in a competing-speaker environment from magnetoencephalographic (MEG) recordings from the human brain. We develop a behaviorally inspired state-space model to account for the modulation of the MEG with respect to attentional state of the listener. We construct a decoder based on the maximum a posteriori (MAP) estimate of the state parameters via the Expectation-Maximization (EM) algorithm. The resulting decoder is able to track the attentional modulation of the listener with multi-second resolution using only the envelopes of the two speech streams as covariates. We present simulation studies as well as application to real MEG data from two human subjects. Our results reveal that the proposed decoder provides substantial gains in terms of temporal resolution, complexity, and decoding accuracy.	[Akram, Sahar; Simon, Jonathan Z.; Shamma, Shihab; Babadi, Behtash] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA; [Akram, Sahar; Simon, Jonathan Z.; Shamma, Shihab; Babadi, Behtash] Univ Maryland, Inst Syst Res, College Pk, MD 20742 USA; [Simon, Jonathan Z.] Univ Maryland, Dept Biol, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park	Akram, S (corresponding author), Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.	sakram@umd.edu; jzsimon@umd.edu; sas@umd.edu; behtash@umd.edu	Simon, Jonathan Z/A-8196-2008	Simon, Jonathan Z/0000-0003-0858-0698				Ba D, 2014, IEEE T SIGNAL PROCES, V62, P183, DOI 10.1109/TSP.2013.2287685; Bregman A. S., 1998, COMPUTATIONAL AUDITO, P1; Bregman A.S., 1994, AUDITORY SCENE ANAL; CHERRY EC, 1953, J ACOUST SOC AM, V25, P975, DOI 10.1121/1.1907229; David SV, 2007, NETWORK-COMP NEURAL, V18, P191, DOI 10.1080/09548980701609235; de Cheveigne A, 2008, J NEUROSCI METH, V171, P331, DOI 10.1016/j.jneumeth.2008.03.015; de Cheveigne A, 2007, J NEUROSCI METH, V165, P297, DOI 10.1016/j.jneumeth.2007.06.003; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Ding N, 2012, P NATL ACAD SCI USA, V109, P11854, DOI 10.1073/pnas.1205381109; Ding N, 2012, J NEUROPHYSIOL, V107, P78, DOI 10.1152/jn.00297.2011; Elhilali M, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000129; Fisher N.I., 1995, STAT ANAL CIRCULAR D, DOI DOI 10.1017/CBO9780511564345; Fisher RA, 1914, BIOMETRIKA, V10, P507; Griffiths TD, 2004, NAT REV NEUROSCI, V5, P887, DOI 10.1038/nrn1538; Mesgarani N, 2012, NATURE, V485, P233, DOI 10.1038/nature11020; Shamma SA, 2011, TRENDS NEUROSCI, V34, P114, DOI 10.1016/j.tins.2010.11.002; Shinn-Cunningham BG, 2008, TRENDS COGN SCI, V12, P182, DOI 10.1016/j.tics.2008.02.003; Shumway R. H., 1982, Journal of Time Series Analysis, V3, P253, DOI 10.1111/j.1467-9892.1982.tb00349.x; Smith AC, 2004, J NEUROSCI, V24, P447, DOI 10.1523/JNEUROSCI.2908-03.2004; Smith AC, 2003, NEURAL COMPUT, V15, P965, DOI 10.1162/089976603765202622	20	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102084
C	Andersen, MR; Winther, O; Hansen, LK		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Andersen, Michael Riis; Winther, Ole; Hansen, Lars Kai			Bayesian Inference for Structured Spike and Slab Priors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				SELECTION	Sparse signal recovery addresses the problem of solving underdetermined linear inverse problems subject to a sparsity constraint. We propose a novel prior formulation, the structured spike and slab prior, which allows to incorporate a priori knowledge of the sparsity pattern by imposing a spatial Gaussian process on the spike and slab probabilities. Thus, prior information on the structure of the sparsity pattern can be encoded using generic covariance functions. Furthermore, we provide a Bayesian inference scheme for the proposed model based on the expectation propagation framework. Using numerical experiments on synthetic data, we demonstrate the benefits of the model.	[Andersen, Michael Riis; Winther, Ole; Hansen, Lars Kai] Tech Univ Denmark, DTU Compute, DK-2800 Lyngby, Denmark	Technical University of Denmark	Andersen, MR (corresponding author), Tech Univ Denmark, DTU Compute, DK-2800 Lyngby, Denmark.	miri@dtu.dk; olwi@dtu.dk; lkh@dtu.dk						Baillet S, 2001, IEEE SIGNAL PROC MAG, V18, P14, DOI 10.1109/79.962275; Baldassarre L., 2012, 2012 2nd International Workshop on Pattern Recognition in NeuroImaging (PRNI), P5, DOI 10.1109/PRNI.2012.31; Bishop C.M, 2006, PATTERN RECOGN; Cevher V., 2008, NIPS; Cotter SF, 2005, IEEE T SIGNAL PROCES, V53, P2477, DOI 10.1109/TSP.2005.849172; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Gerven M.V., 2009, ADV NEURAL INFORM PR, V22, P1901; Hernandez-Lobato D, 2013, J MACH LEARN RES, V14, P1891; Jenatton Rodolphe, 2010, P 13 INT C ART INT S, P366; Minka T.P., 2001, P 17 C UNC ART INT, P362; MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129; Obozinski G., 2009, ACM INT C P SERIES, V382; Opper M, 2000, NEURAL COMPUT, V12, P2655, DOI 10.1162/089976600300014881; Petersen K.B., 2012, MATRIX COOKBOOK; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Schniter P., 2012, 2012 46 ANN C INF SC; Simon N, 2013, J COMPUT GRAPH STAT, V22, P231, DOI 10.1080/10618600.2012.681250; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; Wipf DP, 2007, IEEE T SIGNAL PROCES, V55, P3704, DOI 10.1109/TSP.2007.894265; Yu L, 2012, SIGNAL PROCESS, V92, P259, DOI 10.1016/j.sigpro.2011.07.015; Ziniel J, 2013, IEEE T SIGNAL PROCES, V61, P5270, DOI 10.1109/TSP.2013.2273196	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102025
C	Aslam, O; Zhang, XH; Schuurmans, D		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Aslam, Ozlem; Zhang, Xinhua; Schuurmans, Dale			Convex Deep Learning via Normalized Kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Deep learning has been a long standing pursuit in machine learning, which until recently was hampered by unreliable training methods before the discovery of improved heuristics for embedded layer training. A complementary research strategy is to develop alternative modeling architectures that admit efficient training methods while expanding the range of representable structures toward deep models. In this paper, we develop a new architecture for nested nonlinearities that allows arbitrarily deep compositions to be trained to global optimality. The approach admits both parametric and nonparametric forms through the use of normalized kernels to represent each latent layer. The outcome is a fully convex formulation that is able to capture compositions of trainable nonlinear layers to arbitrary depth.	[Aslam, Ozlem; Schuurmans, Dale] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada; [Zhang, Xinhua] NICTA, Machine Learning Grp, Sydney, NSW, Australia; [Zhang, Xinhua] ANU, Sydney, NSW, Australia	University of Alberta; Australian National University; Australian National University	Aslam, O (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.	ozlem@cs.ualberta.ca; xizhang@nicta.com.au; dale@cs.ualberta.ca						[Anonymous], P ICML; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Arora S., 2014, ICML; Aslan O., 2013, NIPS; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Cheng H., 2013, UAI; Cho Y., 2010, NEURAL COMPUT, V22; Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090; Dinuzzo F, 2011, ICML; Erhan D, 2010, J MACH LEARN RES, V11, P625; Gens R., 2012, NIPS, V25; GORI M, 1992, IEEE T PATTERN ANAL, V14, P76, DOI 10.1109/34.107014; Grippo L, 2000, OPER RES LETT, V26, P127, DOI 10.1016/S0167-6377(99)00074-7; HAJNAL A, 1993, J COMPUT SYST SCI, V46, P129, DOI 10.1016/0022-0000(93)90001-D; Hinton G., 2006, NEUR COMP, V18; Hinton G.E., 2012, COMPUT SCI, V3, P212, DOI DOI 10.9774/GLEAF.978-1-909493-38-42; Hoeffgen K., 1995, JCSS, V52, P114; Jaggi Martin., 2013, ICML; Joachims T, 1999, ICML; Joulin A., 2012, P ICML; Joulin A., 2012, P CVPR; KIMELDORF G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3; Krizhevsky A., 2012, ADV NEURAL INF PROCE; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Livni R., 2014, ARXIV13047045V2; Nair V., 2010, ICML; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; OVERTON ML, 1993, MATH PROGRAM, V62, P321, DOI 10.1007/BF01585173; Peng JM, 2007, SIAM J OPTIMIZ, V18, P186, DOI 10.1137/050641983; Razborov A., 1992, ALGORITHM THEORY SWA; Rifkin RM, 2007, J MACH LEARN RES, V8, P441; Schoelkopf B., 2002, LEARNING KERNELS; Sindhwani Vikas, 2006, SIGIR; Socher R., 2011, ICML; Sutskever I., 2013, P 30 INT C MACH LEAR; Tesauro G., 1995, CACM, V38; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Zhuang J., 2011, AISTATS	40	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102057
C	Awasthi, P; Blum, A; Sheffet, O; Vijayaraghavan, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Awasthi, Pranjal; Blum, Avrim; Sheffet, Or; Vijayaraghavan, Aravindan			Learning Mixtures of Ranking Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					This work concerns learning probabilistic models for ranking data in a heterogeneous population. The specific problem we study is learning the parameters of a Mallows Mixture Model. Despite being widely studied, current heuristics for this problem do not have theoretical guarantees and can get stuck in bad local optima. We present the first polynomial time algorithm which provably learns the parameters of a mixture of two Mallows models. A key component of our algorithm is a novel use of tensor decomposition techniques to learn the top-k prefix in both the rankings. Before this work, even the question of identifiability in the case of a mixture of two Mallows models was unresolved.	[Awasthi, Pranjal] Princeton Univ, Princeton, NJ 08544 USA; [Blum, Avrim] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Sheffet, Or] Harvard Univ, Cambridge, MA 02138 USA; [Vijayaraghavan, Aravindan] NYU, New York, NY 10003 USA	Princeton University; Carnegie Mellon University; Harvard University; New York University	Awasthi, P (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	pawashti@cs.princeton.edu; avrim@cs.cmu.edu; osheffet@seas.harvard.edu; vijayara@cims.nyu.edu	Vijayaraghavan, Aravindan/I-2257-2015		NSF [CCF-1101215, CCF-1116892]; Simons Institute; Simons Foundation	NSF(National Science Foundation (NSF)); Simons Institute; Simons Foundation	This work was supported in part by NSF grants CCF-1101215, CCF-1116892, the Simons Institute, and a Simons Foundation Postdoctoral fellowhsip. Part of this work was performed while the 3rd author was at the Simons Institute for the Theory of Computing at the University of California, Berkeley and the 4th author was at CMU.	Achlioptas Dimitris, 2005, COLT; Anandkumar A., 2012, CORR, Vabs/ 1210. 7559; Anandkumar A., 2012, COLT; ARORA S, 2001, STOC; Bhaskara A., 2013, CORR; Bhaskara A., 2014, S THEOR COMP STOC; Braverman M., 2009, CORR; Busse L., 2007, ICML; Dasgupta Sanjoy, 1999, FOCS; Diaconis P., 1998, LECT NOTES MONOGRAPH, V11; Goyal N., 2014, S THEOR COMP STOC; Hsu Daniel, 2013, ITCS; Kalai A. T., 2010, STOC; Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.2307/2332226; Lebanon Guy, 2002, ICML; Lu Tyler, 2011, ICML; MALLOWS CL, 1957, BIOMETRIKA, V44, P114, DOI 10.2307/2333244; Mandhani Bhushan, 2009, J MACHINE LEARNING R, V5; Marden JI, 1995, ANAL MODELING RANK D; Meila Marina, 2010, UAI; Meila Marina, 2007, TECHNICAL REPORT; Moitra A., 2010, FDN COMP SCI FOCS 20; Murphy Thomas Brendan, 2003, COMPUTATIONAL STAT D, V41; Oren Joel, 2013, JCAI; Stanley R. P., 2002, CAMBRIDGE STUDIES AD; Vempala Santosh, 2004, J COMPUT SYST SCI, V68; Young H Peyton, 1988, AM POLITICAL SCI REV	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101108
C	Balcan, MF; Kanchanapally, V; Liang, YY; Woodruff, D		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Balcan, Maria-Florina; Kanchanapally, Vandana; Liang, Yingyu; Woodruff, David			Improved Distributed Principal Component Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We study the distributed computing setting in which there are multiple servers, each holding a set of points, who wish to compute functions on the union of their point sets. A key task in this setting is Principal Component Analysis (PCA), in which the servers would like to compute a low dimensional subspace capturing as much of the variance of the union of their point sets as possible. Given a procedure for approximate PCA, one can use it to approximately solve problems such as k-means clustering and low rank approximation. The essential properties of an approximate distributed PCA algorithm are its communication cost and computational efficiency for a given desired accuracy in downstream applications. We give new algorithms and analyses for distributed PCA which lead to improved communication and computational costs for k-means clustering and related problems. Our empirical study on real world data shows a speedup of orders of magnitude, preserving communication with only a negligible degradation in solution quality. Some of these techniques we develop, such as a general transformation from a constant success probability subspace embedding to a high success probability subspace embedding with a dimension and sparsity independent of the success probability, may be of independent interest.	[Balcan, Maria-Florina] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA; [Kanchanapally, Vandana] Georgia Inst Technol, Sch Comp Sci, Atlanta, GA 30332 USA; [Liang, Yingyu] Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA; [Woodruff, David] IBM Res, Almaden Res Ctr, Yorktown Hts, NY USA	Carnegie Mellon University; University System of Georgia; Georgia Institute of Technology; Princeton University; International Business Machines (IBM)	Balcan, MF (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.	ninamf@cs.cmu.edu; vvandana@gatech.edu; yingyu1@cs.princeton.edu; dpwoodru@us.ibm.com			NSF [CCF-0953192, CCF-1451177, CCF-1101283, CCF-1422910]; ONR [N00014-09-1-0751]; AFOSR [FA9550-09-1-0538]; XDATA program of the Defense Advanced Research Projects Agency (DARPA) [FA8750-12-C0323]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); XDATA program of the Defense Advanced Research Projects Agency (DARPA)(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work was supported in part by NSF grants CCF-0953192, CCF-1451177, CCF-1101283, and CCF-1422910, ONR grant N00014-09-1-0751, and AFOSR grant FA9550-09-1-0538. David Woodruff would like to acknowledge the XDATA program of the Defense Advanced Research Projects Agency (DARPA), administered through Air Force Research Laboratory contract FA8750-12-C0323, for supporting this work.	Balcan Maria-Florina F, 2013, ADV NEURAL INFORM PR; Blei D. M., 2003, J MACHINE LEARNING R; BOUTSIDIS C., 2011, CORR; Clarkson Kenneth L., 2013, P 45 ANN ACM S THEOR; Cohen Michael, 2014, ARXIV14106801; CORBETT J. C., 2012, P USENIX S OP SYST D; Feldman D., 2011, P ANN ACM S THEOR CO; Feldman Dan, 2013, P ANN ACM SIAM S DIS; Ghashami M., 2014, ACM SIAM S DISCR ALG; Halko N., 2011, SIAM REV; Kannan R., 2014, P C LEARN THEOR; Karampatziakis N., 2013, CORR; Le Borgne Yann-Ael, 2008, SENSORS; Leen T. K., 2001, ADV NEURAL INFORM PR; Lichman M, 2013, UCI MACHINE LEARNING; Meng X., 2013, P ANN ACM S S THEOR; Mitra S, 2011, ACM T WEB, V5, DOI 10.1145/1961659.1961662; Nelson John, 2013, IEEE ANN S FDN COMP; Olston C., 2003, P 2003 ACM SIGMOD IN, P563; Qu Y., 2002, WORK HIGH PERFORM DA; Sarlos T, 2006, IEEE S FDN COMP SCI; Valcarcel Macua S., 2010, P IEEE INT WORKSH SI	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103070
C	Balcan, MF; Berlind, C; Blum, A; Cohen, E; Patnaik, K; Song, L		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Balcan, Maria-Florina; Berlind, Christopher; Blum, Avrim; Cohen, Emma; Patnaik, Kaushik; Song, Le			Active Learning and Best-Response Dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We examine an important setting for engineered systems in which low-power distributed sensors are each making highly noisy measurements of some unknown target function. A center wants to accurately learn this function by querying a small number of sensors, which ordinarily would be impossible due to the high noise rate. The question we address is whether local communication among sensors, together with natural best-response dynamics in an appropriately-defined game, can denoise the system without destroying the true signal and allow the center to succeed from only a small number of active queries. By using techniques from game theory and empirical processes, we prove positive (and negative) results on the denoising power of several natural dynamics. We then show experimentally that when combined with recent agnostic active learning algorithms, this process can achieve low error from very few queries, performing substantially better than active or passive learning without these denoising dynamics as well as passive learning with denoising.	[Balcan, Maria-Florina; Blum, Avrim] Carnegie Mellon, Pittsburgh, PA 15213 USA; [Berlind, Christopher; Cohen, Emma; Patnaik, Kaushik; Song, Le] Georgia Tech, Atlanta, GA USA	Carnegie Mellon University; University System of Georgia; Georgia Institute of Technology	Balcan, MF (corresponding author), Carnegie Mellon, Pittsburgh, PA 15213 USA.	ninamf@cs.cmu.edu; cberlind@gatech.edu; avrim@cs.cmu.edu; ecohen@gatech.edu; kpatnaik3@gatech.edu; lsong@cc.gatech.edu			NSF [CCF-0953192, CCF-1101283, CCF-1116892, IIS-1065251, IIS1116886]; AFOSR [FA9550-09-1-0538]; ONR [N00014-09-1-0751]; Raytheon Faculty Fellowship; NSF CAREER [IIS1350983]; NSF/NIH BIGDATA [1R01GM108341]	NSF(National Science Foundation (NSF)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); ONR(Office of Naval Research); Raytheon Faculty Fellowship; NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF/NIH BIGDATA	This work was supported in part by NSF grants CCF-0953192, CCF-1101283, CCF-1116892, IIS-1065251, IIS1116886, NSF/NIH BIGDATA 1R01GM108341, NSF CAREER IIS1350983, AFOSR grant FA9550-09-1-0538, ONR grant N00014-09-1-0751, and Raytheon Faculty Fellowship.	Awasthi P., 2014, STOC; Balcan M.-F., 2009, EC; Balcan M.-F., 2013, NIPS; Balcan M.-F., 2014, SICOMP; Beygelzimer Alina, 2009, ICML; BLUME LE, 1993, GAME ECON BEHAV, V5, P387, DOI 10.1006/game.1993.1023; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Ellison G, 1993, ECONOMETRICA, V61, P1047, DOI DOI 10.2307/2951493; Golovin D., 2010, NIPS; Hanneke S, 2013, FOUND TRENDS MACH LE, P1; Hanneke S., 2013, COMMUNICATION; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Morris S, 2000, REV ECON STUD, V67, P57, DOI 10.1111/1467-937X.00121; Settles Burr, 2012, ACTIVE LEARNING, DOI 10.2200/s00429ed1v01y201207aim018; Yang L., 2013, THESIS	16	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102035
C	Banerjee, A; Chen, S; Fazayeli, F; Sivakumar, V		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Banerjee, Arindam; Chen, Sheng; Fazayeli, Farideh; Sivakumar, Vidyashankar			Estimation with Norm Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				REGRESSION; SELECTION; RECOVERY; LASSO	Analysis of non-asymptotic estimation error and structured statistical recovery based on norm regularized regression, such as Lasso, needs to consider four aspects: the norm, the loss function, the design matrix, and the noise model. This paper presents generalizations of such estimation error analysis on all four aspects. We characterize the restricted error set, establish relations between error sets for the constrained and regularized problems, and present an estimation error bound applicable to any norm. Precise characterizations of the bound is presented for a variety of noise models, design matrices, including sub-Gaussian, anisotropic, and dependent samples, and loss functions, including least squares and generalized linear models. Gaussian width, a geometric measure of size of sets, and associated tools play a key role in our generalized analysis.	[Banerjee, Arindam; Chen, Sheng; Fazayeli, Farideh; Sivakumar, Vidyashankar] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities	Banerjee, A (corresponding author), Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.	banerjee@cs.umnedu; shengc@cs.umnedu; farideh@cs.umnedu; sivakuma@cs.umnedu			NSF [IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711]; NASA [NNX12AQ39A]	NSF(National Science Foundation (NSF)); NASA(National Aeronautics & Space Administration (NASA))	We thank the anonymous reviewers for helpful comments and suggestions on related work. We thank Sergey Bobkov, Snigdhansu Chatterjee, and Pradeep Ravikumar for discussions related to the paper. The research was supported by NSF grants IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, and by NASA grant NNX12AQ39A.	Amelunxen D, 2014, INF INFERENCE, V3, P224, DOI 10.1093/imaiai/iau005; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; GORDON Y, 1988, LECT NOTES MATH, V1317, P84; Ledoux M., 2013, PROBABILITY BANACH S, P86; Ledoux M., MATH SURVEYS MONOGRA; Meinshausen N, 2009, ANN STAT, V37, P246, DOI 10.1214/07-AOS582; Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Oymak S., 2013, ARXIV13110830V2; Plan Y, 2013, IEEE T INFORM THEORY, V59, P482, DOI 10.1109/TIT.2012.2207945; Raskutti G, 2010, J MACH LEARN RES, V11, P2241; Rudelson M, 2013, IEEE T INFORM THEORY, V59, P3434, DOI 10.1109/TIT.2013.2243201; Talagrand M., 2005, THE GENERIC CHAINING; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tropp J.A., 2014, SAMPLING THEORY RENA; Vizcarra AB, 2008, PROG PROBAB, V59, P363; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018; Zhao P, 2006, J MACH LEARN RES, V7, P2541; Zhou SH, 2014, ANN STAT, V42, P532, DOI 10.1214/13-AOS1187	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102026
C	Benson, AR; Lee, JD; Rajwa, B; Gleich, DF		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Benson, Austin R.; Lee, Jason D.; Rajwa, Bartek; Gleich, David F.			Scalable Methods for Nonnegative Matrix Factorizations of Near-separable Tall-and-skinny Matrices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ALGORITHM	Numerous algorithms are used for nonnegative matrix factorization under the assumption that the matrix is nearly separable. In this paper, we show how to make these algorithms scalable for data matrices that have many more rows than columns, so-called "tall-and-skinny matrices." One key component to these improved methods is an orthogonal matrix transformation that preserves the separability of the NMF problem. Our final methods need to read the data matrix only once and are suitable for streaming, multi-core, and MapReduce architectures. We demonstrate the efficacy of these algorithms on terabyte-sized matrices from scientific computing and bioinformatics.	[Benson, Austin R.; Lee, Jason D.] Stanford Univ, ICME, Stanford, CA 94305 USA; [Rajwa, Bartek] Purdue Univ, Bindley Biosci Ctr, W Lafayette, IN 47907 USA; [Gleich, David F.] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA	Stanford University; Purdue University System; Purdue University; Purdue University West Lafayette Campus; Purdue University System; Purdue University; Purdue University West Lafayette Campus	Benson, AR (corresponding author), Stanford Univ, ICME, Stanford, CA 94305 USA.	arbenson@stanford.edu; jd117@stanford.edu; brajwa@purdue.edu; dgleich@purdue.edu	Rajwa, Bartek/B-3169-2009	Rajwa, Bartek/0000-0001-7540-8236	Office of Technology Licensing Stanford Graduate Fellowship; NSF Graduate Research Fellowship; NSF CAREER award [CCF-1149756]; NIH [1R21EB015707-01]	Office of Technology Licensing Stanford Graduate Fellowship; NSF Graduate Research Fellowship(National Science Foundation (NSF)); NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	ARB and JDL are supported by an Office of Technology Licensing Stanford Graduate Fellowship. JDL is also supported by a NSF Graduate Research Fellowship. DFG is supported by NSF CAREER award CCF-1149756. BR is supported by NIH grant 1R21EB015707-01.	Anderson M., 2011, Proceedings of the 25th IEEE International Parallel & Distributed Processing Symposium (IPDPS 2011), P48, DOI 10.1109/IPDPS.2011.15; [Anonymous], P 19 ACM INT C INF K, DOI DOI 10.1145/1871437.1871729; Araujo MCU, 2001, CHEMOMETR INTELL LAB, V57, P65, DOI 10.1016/S0169-7439(01)00119-8; Arora S, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P145; Benson AR, 2013, IEEE INT CONF BIG DA; Bioucas-Dias JM, 2011, INT GEOSCI REMOTE SE, P1135, DOI 10.1109/IGARSS.2011.6049397; BITTORF V., 2012, P ADV NEURAL INFORM, V25, P1223; Boardman J.W., 1993, AVIRIS WORKSH, V1, P11; CHAN TF, 1982, ACM T MATH SOFTWARE, V8, P72, DOI 10.1145/355984.355990; Chu MT, 2008, SIAM J SCI COMPUT, V30, P1131, DOI 10.1137/070680436; Cichocki A, 2007, LECT NOTES COMPUT SC, V4493, P793; Constantine P. G., 2014, SIAM J SCI COMPUT; Constantine P.G., 2011, P 2 INT WORKSH MAPRE, P43, DOI DOI 10.1145/1996092; Damle A., 2014, ARXIV14054275; Demmel J., 2012, SIAM J SCI COMP, V34; Dongmin Kim, 2008, Statistical Analysis and Data Mining, V1, P38, DOI 10.1002/sam.104; Donoho D., 2003, NIPS; Gillis N., 2013, SYSTEMS MAN CYBERN A, VPP, P1; Guillamet D, 2002, LECT NOTES ARTIF INT, V2504, P336; Jia S, 2009, IEEE T GEOSCI REMOTE, V47, P161, DOI 10.1109/TGRS.2008.2002882; Kim W, 2011, EXPERT SYST APPL, V38, P13198, DOI 10.1016/j.eswa.2011.04.133; Kuang D., 2012, PROC 2012 SIAM INT C, P106, DOI DOI 10.1137/1.9781611972825.10; Kumar A., 2013, ICML; Lee Daniel D., 2000, NIPS; Liu C., 2010, PROC 19 INT C WORLD, P681, DOI 10.1145/1772690.1772760	26	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101006
C	Berend, D; Kontorovich, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Berend, Daniel; Kontorovich, Aryeh			Consistency of weighted majority votes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				THEOREM	We revisit from a statistical learning perspective the classical decision-theoretic problem of weighted expert voting. In particular, we examine the consistency (both asymptotic and finitary) of the optimal Nitzan-Paroush weighted majority and related rules. In the case of known expert competence levels, we give sharp error estimates for the optimal rule. When the competence levels are unknown, they must be empirically estimated. We provide frequentist and Bayesian analyses for this situation. Some of our proof techniques are non-standard and may be of independent interest. The bounds we derive are nearly optimal, and several challenging open problems are posed.	[Berend, Daniel; Kontorovich, Aryeh] Ben Gurion Univ Negev, Comp Sci Dept, Beer Sheva, Israel; [Berend, Daniel] Ben Gurion Univ Negev, Math Dept, Beer Sheva, Israel	Ben Gurion University; Ben Gurion University	Berend, D (corresponding author), Ben Gurion Univ Negev, Comp Sci Dept, Beer Sheva, Israel.	berend@cs.bgu.ac.il; karyeh@cs.bgu.ac.il	Kontorovich, Aryeh/X-9225-2019; Kontorovich, Aryeh/AAB-4744-2020; BEREND, DANIEL/AAJ-4653-2020	Kontorovich, Aryeh/0000-0001-8038-8671; 				Audibert J. - Y., 2007, ALT; Baharad E, 2012, THEOR DECIS, V72, P113, DOI 10.1007/s11238-010-9240-5; Baharad E, 2011, AUTON AGENT MULTI-AG, V22, P31, DOI 10.1007/s10458-009-9120-y; Berend D, 1998, SOC CHOICE WELFARE, V15, P481, DOI 10.1007/s003550050118; Berend D., 2014, PREPRINT; Berend D, 2007, SOC CHOICE WELFARE, V28, P507, DOI 10.1007/s00355-006-0179-y; Berend D, 2013, ELECTRON COMMUN PROB, V18, P1, DOI 10.1214/ECP.v18-2359; BOLAND PJ, 1989, J APPL PROBAB, V26, P81, DOI 10.2307/3214318; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Condorcet J. A. N. de Caritat marquis de, 1785, AMS CHELSEA PUBLISHI; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Eban E., 2014, ICML; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman J., 2009, ELEMENTS STAT LEARNI, DOI 10.1007/978-0-387-84858-7; Gao Chao, 2014, ARXIV13105764; Helmbold DP, 2012, J MACH LEARN RES, V13, P2145; Kearns Michael J., 1998, UAI; Kontorovich A, 2012, MARKOV PROCESS RELAT, V18, P613; Lacasse A., 2006, NIPS; Laviolette F, 2007, J MACH LEARN RES, V8, P1461; Li H., 2013, ABS13072674 CORR; Littlestone N., 1989, FOCS; Mansour Y., 2013, PREPRINT; Maurer A., 2009, COLT; McAllester D. A, 2003, J MACHINE LEARNING R, V4, P895; Mnih Volodymyr, 2008, ICML; Neyman J, 1933, PHILOS T R SOC LOND, V231, P289, DOI 10.1098/rsta.1933.0009; NITZAN S, 1982, INT ECON REV, V23, P289, DOI 10.2307/2526438; Parisi F., 2014, P NAT ACAD SCI; Raginsky M, 2013, FOUND TRENDS COMMUN, V10, P1, DOI 10.1561/0100000064; Roy J., 2011, ICML; Schapire Robert E., 2012, BOOSTING FDN ALGORIT	34	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100032
C	Bishopl, WE; Byron, MY		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Bishopl, William E.; Byron, M. Yu			Deterministic Symmetric Positive Semidefinite Matrix Completion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We consider the problem of recovering a symmetric, positive semidefinite (SPSD) matrix from a subset of its entries, possibly corrupted by noise. In contrast to previous matrix recovery work, we drop the assumption of a random sampling of entries in favor of a deterministic sampling of principal submatrices of the matrix. We develop a set of sufficient conditions for the recovery of a SPSD matrix from a set of its principal submatrices, present necessity results based on this set of conditions and develop an algorithm that can exactly recover a matrix when these conditions are met. The proposed algorithm is naturally generalized to the problem of noisy matrix recovery, and we provide a worst-case bound on reconstruction error for this scenario. Finally, we demonstrate the algorithm's utility on noiseless and noisy simulated datasets.	[Bishopl, William E.] Carnegie Mellon Univ, Machine Learning, Pittsburgh, PA 15213 USA; [Bishopl, William E.; Byron, M. Yu] Carnegie Mellon Univ, Ctr Neural Basis Cognit, Pittsburgh, PA 15213 USA; [Byron, M. Yu] Carnegie Mellon Univ, Biomed Engn, Pittsburgh, PA 15213 USA; [Byron, M. Yu] Carnegie Mellon Univ, Elect & Comp Engn, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; Carnegie Mellon University; Carnegie Mellon University	Bishopl, WE (corresponding author), Carnegie Mellon Univ, Machine Learning, Pittsburgh, PA 15213 USA.	wbishop@cmu.edu; byronyu@cmu.edu			NDSEG fellowship; NIH [R90 DA023426-06, T90 DA022762]; Craig H. Nielsen Foundation	NDSEG fellowship; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Craig H. Nielsen Foundation	This work was supported by an NDSEG fellowship, NIH grant T90 DA022762, NIH grant R90 DA023426-06 and by the Craig H. Nielsen Foundation. We thank Martin Azizyan, Geoff Gordon, Akshay Krishnamurthy and Aarti Singh for their helpful discussions and Rob Kass for his guidance.	Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chen J, 2013, PARALLEL GAUSSIAN PR; Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776; Heiman Eyal, 2013, RANDOM STRUCTURES AL; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Keshri S., 2013, ARXIV13093724; Koltchinskii V, 2011, ANN STAT, V39, P2302, DOI 10.1214/11-AOS894; Kumar S, 2009, P INT C ART INT STAT, P304; Laurent M, 2014, LINEAR ALGEBRA APPL, V452, P292, DOI 10.1016/j.laa.2014.03.015; Laurent M, 2014, MATH PROGRAM, V145, P291, DOI 10.1007/s10107-013-0648-x; Lee JA, 2007, INFORM SCI STAT, P1; Lee T., 2013, ADV NEURAL INFORM PR, P1781; Negahban S, 2012, J MACH LEARN RES, V13, P1665; Pardalos P.M., 2009, ENCY OPTIMIZATION, P1967, DOI DOI 10.1007/978-0-387-74759-0355; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Scholkopf B., 2001, LEARNING KERNELS SUP; SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451; Turaga S., 2013, ADV NEURAL INF PROCE, P539; Williams C., 2001, ADV NEURAL INFORM PR	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102028
C	Boix, X; Roig, G; Diether, S; Van Gool, L		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Boix, Xavier; Roig, Gemma; Diether, Salomon; Van Gool, Luc			Self-Adaptable Templates for Feature Coding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Hierarchical feed-forward networks have been successfully applied in object recognition. At each level of the hierarchy, features are extracted and encoded, followed by a pooling step. Within this processing pipeline, the common trend is to learn the feature coding templates, often referred as codebook entries, filters, or over-complete basis. Recently, an approach that apparently does not use templates has been shown to obtain very promising results. This is the second-order pooling (O2P) [1, 2, 3, 4, 5]. In this paper, we analyze O2P as a coding-pooling scheme. We find that at testing phase, O2P automatically adapts the feature coding templates to the input features, rather than using templates learned during the training phase. From this finding, we are able to bring common concepts of coding-pooling schemes to O2P, such as feature quantization. This allows for significant accuracy improvements of O2P in standard benchmarks of image classification, namely Caltech101 and VOC07.	[Boix, Xavier; Roig, Gemma; Diether, Salomon; Van Gool, Luc] Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland; [Boix, Xavier; Roig, Gemma] MIT, LCSL, Cambridge, MA 02139 USA; [Boix, Xavier; Roig, Gemma] Ist Italiano Tecnol, Genoa, Italy	Swiss Federal Institutes of Technology Domain; ETH Zurich; Massachusetts Institute of Technology (MIT); Istituto Italiano di Tecnologia - IIT	Boix, X (corresponding author), Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland.	xboix@mit.edu; gemmar@mit.edu; sdiether@vision.ee.ethz.ch; vangool@vision.ee.ethz.ch			ERC; AdG VarCity	ERC(European Research Council (ERC)European Commission); AdG VarCity	We thank the ERC for support from AdG VarCity.	Arsigny V., 2007, J MATRIX ANAL APPL; Bergstra J., 2013, ICML; Bhatia R., 2009, POSITIVE DEFINITE MA; Boix X., 2012, ECCV; Boix X., 2013, CVPR; Carreira J., 2012, ECCV; Coates A., 2011, ICML; Csurka G., 2004, ECCV; Duchenne O., 2011, ICCV; Everingham M., 2010, IJCV; Fan R.-E., 2008, JMLR; Fei-Fei L., 2006, TPAMI; Fukushima K., 1980, BIOL CYBERNETICS; Girshick R., 2014, CVPR, DOI 10.1109/CVPR.2014.81; Le Bihan D., 2001, J MAGNETIC RESONANCE; Li P., 2012, ECCV; Lowe D. G., 2004, IJCV; Poggio T., 1999, NATURE NEUROSCIENCE; Sanchez J., 2013, IJCV; Tuzel O., 2006, ECCV; Weickert J., 2006, VISUALIZATION PROCES; Zhou X., 2010, ECCV	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100065
C	Bresler, G; Gamarnik, D; Shah, D		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Bresler, Guy; Gamarnik, David; Shah, Devavrat			Hardness of parameter estimation in graphical models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				COMPLEXITY	We consider the problem of learning the canonical parameters specifying an undirected graphical model (Markov random field) from the mean parameters. For graphical models representing a minimal exponential family, the canonical parameters are uniquely determined by the mean parameters, so the problem is feasible in principle. The goal of this paper is to investigate the computational feasibility of this statistical task. Our main result shows that parameter estimation is in general intractable: no algorithm can learn the canonical parameters of a generic pair-wise binary graphical model from the mean parameters in time bounded by a polynomial in the number of variables (unless RP = NP). Indeed, such a result has been believed to be true (see [1]) but no proof was known. Our proof gives a polynomial time reduction from approximating the partition function of the hard-core model, known to be hard, to learning approximate parameters. Our reduction entails showing that the marginal polytope boundary has an inherent repulsive property, which validates an optimization procedure over the polytope that does not use any knowledge of its structure (as required by the ellipsoid method and others).	[Bresler, Guy; Shah, Devavrat] MIT, Lab Informat & Decis Syst, Dept EECS, Cambridge, MA 02139 USA; [Gamarnik, David] MIT, Sloan Sch Management, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Bresler, G (corresponding author), MIT, Lab Informat & Decis Syst, Dept EECS, Cambridge, MA 02139 USA.	gbresler@mit.edu; gamarnik@mit.edu; devavrat@mit.edu			NSF [CMMI-1335155, CNS-1161964]; Army Research Office MURI Award [W911NF-11-1-0036]	NSF(National Science Foundation (NSF)); Army Research Office MURI Award	GB thanks Sahand Negahban for helpful discussions. Also we thank Andrea Montanari for sharing his unpublished manuscript [9]. This work was supported in part by NSF grants CMMI-1335155 and CNS-1161964, and by Army Research Office MURI Award W911NF-11-1-0036.	Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Bogdanov A, 2008, LECT NOTES COMPUT SC, V5171, P331; Borwein J. M., 2010, CONVEX FUNCTIONS CON, V109; Bubeck S., THEORY CONVEX OPTIMI; Deza M., 1997, GEOMETRY CUTS METRIC; Dyer M, 2002, SIAM J COMPUT, V31, P1527, DOI 10.1137/S0097539701383844; Galanis A., 2012, ARXIV12032226; Istrail S., 2000, STOC, P87; JAEGER F, 1990, MATH PROC CAMBRIDGE, V108, P35, DOI 10.1017/S0305004100068936; JERRUM M, 1993, SIAM J COMPUT, V22, P1087, DOI 10.1137/0222066; Jiang LB, 2010, IEEE T INFORM THEORY, V56, P6182, DOI 10.1109/TIT.2010.2081490; Kakade SM, 2012, J MACH LEARN RES, V13, P1865; Karger D, 2001, SIAM PROC S, P392; Luby M, 1999, RANDOM STRUCT ALGOR, V15, P229, DOI 10.1002/(SICI)1098-2418(199910/12)15:3/4<229::AID-RSA3>3.0.CO;2-X; Montanari A., 2014, COMPUTATIONAL UNPUB; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Papadimitriou C.H, 2003, COMPUT COMPLEX; Rockafellar R. T., 1997, CONVEX ANAL, V28; Roughgarden T., 2013, ADV NEURAL INFORM PR, P1043; Shah D, 2011, IEEE T INFORM THEORY, V57, P7810, DOI 10.1109/TIT.2011.2168897; Sly A, 2012, ANN IEEE SYMP FOUND, P361, DOI 10.1109/FOCS.2012.56; Sly A, 2010, ANN IEEE SYMP FOUND, P287, DOI 10.1109/FOCS.2010.34; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Ziegler GM, 2000, DMV SEMINAR, V29, P1	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103020
C	Bresler, G; Gamarnik, D; Shah, D		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Bresler, Guy; Gamarnik, David; Shah, Devavrat			Structure learning of antiferromagnetic Ising models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In this paper we investigate the computational complexity of learning the graph structure underlying a discrete undirected graphical model from i.i.d. samples. Our first result is an unconditional computational lower bound of Omega (p(d/2)) for learning general graphical models on p nodes of maximum degree d, for the class of so-called statistical algorithms recently introduced by Feldman et al. [1]. The construction is related to the notoriously difficult learning parities with noise problem in computational learning theory. Our lower bound suggests that the (O) over tilde (p(d+2)) runtime required by Bresler, Mossel, and Sly's [2] exhaustive-search algorithm cannot be significantly improved without restricting the class of models. Aside from structural assumptions on the graph such as it being a tree, hypertree, tree-like, etc., many recent papers on structure learning assume that the model has the correlation decay property. Indeed, focusing on ferromagnetic Ising models, Bento and Montanari [3] showed that all known low-complexity algorithms fail to learn simple graphs when the interaction strength exceeds a number related to the correlation decay threshold. Our second set of results gives a class of repelling (antiferromagnetic) models that have the opposite behavior: very strong interaction allows efficient learning in time (O) over tilde (p(2)). We provide an algorithm whose performance interpolates between (O) over tilde (p(2)) and (O) over tilde (p(d+2)) depending on the strength of the repulsion.	[Bresler, Guy; Shah, Devavrat] MIT, Lab Informat & Decis Syst, Dept EECS, Cambridge, MA 02139 USA; [Gamarnik, David] MIT, Lab Informat & Decis Syst, Sloan Sch Management, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Bresler, G (corresponding author), MIT, Lab Informat & Decis Syst, Dept EECS, Cambridge, MA 02139 USA.	gbresler@mit.edu; gamarnik@mit.edu; devavrat@mit.edu			NSF [CMMI-1335155, CNS-1161964]; Army Research Office MURI Award [W911NF-11-1-0036]	NSF(National Science Foundation (NSF)); Army Research Office MURI Award	This work was supported in part by NSF grants CMMI-1335155 and CNS-1161964, and by Army Research Office MURI Award W911NF-11-1-0036.	Abbeel P, 2006, J MACH LEARN RES, V7, P1743; Anandkumar A, 2012, ANN STAT, V40, P1346, DOI 10.1214/12-AOS1009; Bento J., 2009, NIPS; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Csiszar I, 2006, ANN STAT, V34, P123, DOI 10.1214/009053605000000912; Dasgupta S, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P134; Dobrushin R. L., 1985, Statistical Physics and Dynamical Systems: Rigorous Results, P347; DOBRUSHIN RL, 1970, THEOR PROBAB APPL+, V15, P458, DOI 10.1137/1115049; Feldman V, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P655; Gamarnik D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1245; Jalali A., 2011, ADV NEURAL INF PROCE, V24, P1935; Jalali A., 2011, INT C AI STAT AISTAT, V14; Kearns M, 1998, J ACM, V45, P983, DOI 10.1145/293347.293351; Lee S. - I., 2006, ADV NEURAL INFORM PR, P817; Netrapalli P., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1295, DOI 10.1109/ALLERTON.2010.5707063; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; Ray A., 2012, 50 ALL C; RuiWu R Srikant, 2013, STOCHASTIC SYSTEMS, V3, P362; Salas J, 1997, J STAT PHYS, V86, P551, DOI 10.1007/BF02199113; Sinclair A, 2014, J STAT PHYS, V155, P666, DOI 10.1007/s10955-014-0947-5; Srebro N., 2001, P 17 C UNC ART INT U, P504	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100098
C	Bresler, G; Chen, GH; Shah, D		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Bresler, Guy; Chen, George H.; Shah, Devavrat			A Latent Source Model for Online Collaborative Filtering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Despite the prevalence of collaborative filtering in recommendation systems, there has been little theoretical development on why and how well it works, especially in the "online" setting, where items are recommended to users over time. We address this theoretical gap by introducing a model for online recommendation systems, cast item recommendation under the model as a learning problem, and analyze the performance of a cosine-similarity collaborative filtering method. In our model, each of n users either likes or dislikes each of m items. We assume there to be k types of users, and all the users of a given type share a common string of probabilities determining the chance of liking each item. At each time step, we recommend an item to each user, where a key distinction from related bandit literature is that once a user consumes an item (e.g., watches a movie), then that item cannot be recommended to the same user again. The goal is to maximize the number of likable items recommended to users over time. Our main result establishes that after nearly log (km) initial learning time steps, a simple collaborative filtering algorithm achieves essentially optimal performance without knowing k. The algorithm has an exploitation step that uses cosine similarity and two types of exploration steps, one to explore the space of items (standard in the literature) and the other to explore similarity between users (novel to this work).	[Bresler, Guy; Chen, George H.; Shah, Devavrat] MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Bresler, G (corresponding author), MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA.	gbresler@mit.edu; georgehc@mit.edu; devavrat@mit.edu			NSF [CNS-1161964]; Army Research Office MURI Award [W911NF-11-1-0036]; NDSEG fellowship	NSF(National Science Foundation (NSF)); Army Research Office MURI Award; NDSEG fellowship	This work was supported in part by NSF grant CNS-1161964 and by Army Research Office MURI Award W911NF-11-1-0036. GHC was supported by an NDSEG fellowship.	Aiolli F., 2013, P ECAI WORKSHOP PREF, P73; Anandkumar A., 2012, ARXIV12107559; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Barman K, 2012, IEEE T INFORM THEORY, V58, P7110, DOI 10.1109/TIT.2012.2216980; Belkin M, 2010, ANN IEEE SYMP FOUND, P103, DOI 10.1109/FOCS.2010.16; Bell RM., 2009, NETFLIX PRIZE DOCUME; Bertin-Mahieux T., 2011, ISMIR; Biau G, 2010, ANN STAT, V38, P1568, DOI 10.1214/09-AOS759; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Bui Loc, 2012, ARXIV12064169; Chaudhuri K., 2008, COLT, V4, P9; Dabeer O, 2013, IEEE INT SYMP INFO, P1197, DOI 10.1109/ISIT.2013.6620416; Deshpande Yash, 2013, ARXIV13011722; Grosse Roger, 2012, 28 C UNC ART INT CAT, P306; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Kleinberg R, 2010, MACH LEARN, V80, P245, DOI 10.1007/s10994-010-5178-7; Koren Y., 2009, THE NETFLIX PRIZE, V81, P1; Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344; Moitra Ankur, 2010, P 51 ANN IEEE S FDN; Piotte M., 2009, PRAGMATIC THEORY SOL; Resnick P., 1994, Transcending Boundaries, CSCW '94. Proceedings of the Conference on Computer Supported Cooperative Work, P175, DOI 10.1145/192844.192905; Sutskever I., 2009, P 22 INT C NEURAL IN, P1821; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101001
C	Bruer, JJ; Tropp, JA; Cevher, V; Becker, SR		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Bruer, John J.; Tropp, Joel A.; Cevher, Volkan; Becker, Stephen R.			Time-Data Tradeoffs by Aggressive Smoothing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				CONVEX; MINIMIZATION; GRADIENT	This paper proposes a tradeoff between sample complexity and computation time that applies to statistical estimators based on convex optimization. As the amount of data increases, we can smooth optimization problems more and more aggressively to achieve accurate estimates more quickly. This work provides theoretical and experimental evidence of this tradeoff for a class of regularized linear inverse problems.	[Bruer, John J.; Tropp, Joel A.] CALTECH, Dept Comp Math Sci, Pasadena, CA 91125 USA; [Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst, Lausanne, Switzerland; [Becker, Stephen R.] Univ Colorado Boulder, Dept Appl Math, Boulder, CO USA	California Institute of Technology; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; University of Colorado System; University of Colorado Boulder	Bruer, JJ (corresponding author), CALTECH, Dept Comp Math Sci, Pasadena, CA 91125 USA.	jbruer@cms.caltech.edu	Becker, Stephen R/R-7528-2016	Becker, Stephen R/0000-0002-1932-8159	ONR [N00014-11-1002]; AFOSR [FA9550-09-1-0643]; Sloan Research Fellowship; European Commission [MIRG-268398]; ERC Future Proof [SNF 200021-132548, SNF 200021-146750, SNF CRSII2-147633]	ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); Sloan Research Fellowship(Alfred P. Sloan Foundation); European Commission(European CommissionEuropean Commission Joint Research Centre); ERC Future Proof	JJB's and JAT's work was supported under ONR award N00014-11-1002, AFOSR award FA9550-09-1-0643, and a Sloan Research Fellowship. VC's work was supported in part by the European Commission under Grant MIRG-268398, ERC Future Proof, SNF 200021-132548, SNF 200021-146750 and SNF CRSII2-147633. SRB was previously with IBM Research, Yorktown Heights, NY 10598 during the completion of this work.	Agarwal A., 2012, 12080129V1 ARXIV; Agarwal A, 2012, ANN STAT, V40, P2452, DOI 10.1214/12-AOS1032; Amelunxen D., 2014, INFORM INFERENCE; Amini AA, 2009, ANN STAT, V37, P2877, DOI 10.1214/08-AOS664; Auslender A, 2006, SIAM J OPTIMIZ, V16, P697, DOI 10.1137/S1052623403427823; Becker SR, 2011, MATH PROGRAM COMPUT, V3, P165, DOI 10.1007/s12532-011-0029-5; Berthet Q., 2013, 13040828V2 ARXIV; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Boyd S, 2004, CONVEX OPTIMIZATION; Bruer J. J., 2014, IEEE J SELECTED TOPI; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Cai JF, 2009, MATH COMPUT, V78, P1515, DOI 10.1090/S0025-5718-08-02189-3; Chandrasekaran V, 2013, P NATL ACAD SCI USA, V110, pE1181, DOI 10.1073/pnas.1302293110; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; CLAERBOUT JF, 1973, GEOPHYSICS, V38, P826, DOI 10.1190/1.1440378; Daniely A., 2013, ADV NEURAL INFORM PR, P145; Fazel M., 2002, MATRIX RANK MINIMIZA; Jordan MI, 2013, BERNOULLI, V19, P1378, DOI 10.3150/12-BEJSP17; Lai MJ, 2013, SIAM J IMAGING SCI, V6, P1059, DOI 10.1137/120863290; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Osher S, 2010, COMMUN MATH SCI, V8, P93; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; SANTOSA F, 1986, SIAM J SCI STAT COMP, V7, P1307, DOI 10.1137/0907087; Shalev-Shwartz S., 2008, P 25 INT C MACH LEAR, V307, P928, DOI DOI 10.1145/1390156.1390273; Shender D., 2013, P 30 INT C MACH LEAR, V28, P756; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100038
C	Carlson, DE; Borg, JS; Dzirasa, K; Carin, L		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Carlson, David E.; Borg, Jana Schaich; Dzirasa, Kafui; Carin, Lawrence			On the Relationship Between LFP & Spiking Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					One of the goals of neuroscience is to identify neural networks that correlate with important behaviors, environments, or genotypes. This work proposes a strategy for identifying neural networks characterized by time- and frequency-dependent connectivity patterns, using convolutional dictionary learning that links spike-train data to local field potentials (LFPs) across multiple areas of the brain. Analytical contributions are: (i) modeling dynamic relationships between LFPs and spikes; (ii) describing the relationships between spikes and LFPs, by analyzing the ability to predict LFP data from one region based on spiking information from across the brain; and (iii) development of a clustering methodology that allows inference of similarities in neurons from multiple regions. Results are based on data sets in which spike and LFP data are recorded simultaneously from up to 16 brain regions in a mouse.	[Carlson, David E.; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Duham, NC 27701 USA; [Borg, Jana Schaich; Dzirasa, Kafui] Duke Univ, Dept Psychiat & Behav Sci, Duham, NC 27701 USA	Duke University; Duke University	Carlson, DE (corresponding author), Duke Univ, Dept Elect & Comp Engn, Duham, NC 27701 USA.	david.carlson@duke.edu; jana.borg@duke.edu; kafui.dzirasa@duke.edu; lcarin@duke.edu	Dzirasa, Kafui/GQB-1424-2022	Carin, Lawrence/0000-0001-6277-7948; Carlson, David/0000-0003-1005-6385; Schaich Borg, Jana/0000-0002-0066-761X	ARO; DARPA; DOE; NGA; ONR	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NGA; ONR(Office of Naval Research)	The research reported here was funded in part by ARO, DARPA, DOE, NGA and ONR. We thank the reviewers for their helpful comments.	Blei David M, 2006, BAYESIAN ANAL; Calabrese A., 2010, J NEUROSCI METHODS; Carlson D.E., 2013, IEEE TBME; Chen B., 2011, ICML; Dzirasa K., 2006, J NEUROSCI; Dzirasa K, 2010, J NEUROSCI; Ferguson T.S., 1973, ANN STAT         MAR; Heller K., 2005, ICML; Hughes M. C., 2013, NIPS; Ishwaran H., 2001, JASA; Kelly R.C., 2010, J COMP NEUROSCI; Kim D.I., 2012, NIPS; Larson-Prior L.J., 2009, PNAS; Le Van Quyen M., 2007, TRENDS NEUROSCIENCES; Mehring C, 2003, NAT NEUROSCI, V6, P1253, DOI 10.1038/nn1158; Nauhaus I., 2009, NATURE NEURO     JAN; Pesaran B, 2002, NAT NEUROSCI, V5, P805, DOI 10.1038/nn890; Pillow J., 2007, NIPS; Rasch M., 2009, J NEUROSCI; Rasch M.J., 2008, J NEUROPHYSIOLOGY; Roberts S.J., 2002, IEEE T SIGNAL PROCES; Rue H., 2009, J ROYAL STAT SOC; Uhlhaas P.J., 2010, NATURE REV NEURO; Varela F., 2001, NATURE REV NEURO	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101075
C	Chan, SO; Diakonikolas, I; Servedio, RA; Sun, XR		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Chan, Siu-On; Diakonikolas, Ilias; Servedio, Rocco A.; Sun, Xiaorui			Near-Optimal Density Estimation in Near-Linear Time Using Variable-Width Histograms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				RESTRICTIONS	Let p be an unknown and arbitrary probability distribution over [0, 1). We consider the problem of density estimation, in which a learning algorithm is given i.i.d. draws from p and must (with high probability) output a hypothesis distribution that is close to p. The main contribution of this paper is a highly efficient density estimation algorithm for learning using a variable-width histogram, i.e., a hypothesis distribution with a piecewise constant probability density function. In more detail, for any k and epsilon, we give an algorithm that makes (O) over tilde (k/epsilon(2)) draws from p, runs in (O) over tilde (k/epsilon(2)) time, and outputs a hypothesis distribution h that is piecewise constant with O(k log(2)(1/epsilon)) pieces. With high probability the hypothesis h satisfies d(TV)(p, h) <= C . opt(k)(p) + epsilon, where d(TV) denotes the total variation distance (statistical distance), C is a universal constant, and opt(k)(p) is the smallest total variation distance between p and any k-piecewise constant distribution. The sample size and running time of our algorithm are optimal up to logarithmic factors. The "approximation factor" C in our result is inherent in the problem, as we prove that no algorithm with sample size bounded in terms of k and epsilon can achieve C < 2 regardless of what kind of hypothesis distribution it uses.	[Chan, Siu-On] Microsoft Res, Redmond, WA 98052 USA; [Diakonikolas, Ilias] Univ Edinburgh, Edinburgh, Midlothian, Scotland; [Servedio, Rocco A.; Sun, Xiaorui] Columbia Univ, New York, NY 10027 USA	Microsoft; University of Edinburgh; Columbia University	Chan, SO (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	sochan@gmail.com; ilias.d@ed.ac.uk; rocco@cs.columbia.edu; xiaoruisun@cs.columbia.edu	Chan, Siu On/AAY-3621-2020					Acharya J., 2014, TECHNICAL REPORT; Barlow R. E., 1972, STAT INFERENCE ORDER; Birge L, 1997, ANN STAT, V25, P970, DOI 10.1214/aos/1069362733; BIRGE L, 1987, ANN STAT, V15, P995, DOI 10.1214/aos/1176350488; BRUNK HD, 1958, ANN MATH STAT, V29, P437, DOI 10.1214/aoms/1177706621; Chan SO, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P604, DOI 10.1145/2591796.2591848; Chan SO, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1380; Chaudhuri S., 1998, SIGMOD Record, V27, P436, DOI 10.1145/276305.276343; Daskalakis Constantinos, 2014, P 27 C LEARN THEOR C, P1183; De A., 2012, INVERSE PROBLEMS APP; Devroye L., 2001, SPRINGER SERIES STAT; DEVROYE L, 1985, NONPARAMETRIC DENSIT; Gilbert A. C., 2002, P 34 ANN ACM S THEOR, P389; Grenander U, 1956, SKAND AKTUARIETIDSK, V1956, P125; Groeneboom P., 1985, P BERK C HON JERZ NE, VII, P539; Guha S, 2006, ACM T DATABASE SYST, V31, P396, DOI 10.1145/1132863.1132873; HANSON DL, 1976, ANN STAT, V4, P1038, DOI 10.1214/aos/1176343640; Indyk P, 2012, PODS, P15; Jagadish H. V., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P275; Kearns M., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P273, DOI 10.1145/195058.195155; Massart Pascal, 2003, LECT NOTES MATH, V33; Pearson K., 1895, PHILOS T ROYAL SOC A, P343, DOI [https://doi.org/10.1098/rsta.1895.0010, DOI 10.1098/RSTA.1895.0010]; RAO BLSP, 1969, SANKHYA SER A, V31, P23; Reboul L, 2005, ANN STAT, V33, P1330, DOI 10.1214/009053605000000138; Scott D. W., 1992, MULTIVARIATE DENSITY, DOI 10.1002/9780470316849; Silverman B.W, 1986, DENSITY ESTIMATION; Valiant L. G., 1984, Communications of the ACM, V27, P1134, DOI 10.1145/1968.1972; WEGMAN EJ, 1970, ANN MATH STAT, V41, P457, DOI 10.1214/aoms/1177697085	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100005
C	Chandar, APS; Lauly, S; Larochelle, H; Khapra, MM; Ravindran, B; Raykar, V; Saha, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Chandar, Sarath A. P.; Lauly, Stanislas; Larochelle, Hugo; Khapra, Mitesh M.; Ravindran, Balaraman; Raykar, Vikas; Saha, Amrita			An Autoencoder Approach to Learning BilingualWord Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Cross-language learning allows one to use training data from one language to build models for a different language. Many approaches to bilingual learning require that we have word-level alignment of sentences from parallel corpora. In this work we explore the use of autoencoder-based methods for cross-language learning of vectorial word representations that are coherent between two languages, while not relying on word-level alignments. We show that by simply learning to reconstruct the bag-of-words representations of aligned sentences, within and between languages, we can in fact learn high-quality representations and do without word alignments. We empirically investigate the success of our approach on the problem of cross-language text classification, where a classifier trained on a given language (e.g., English) must learn to generalize to a different language (e.g., German). In experiments on 3 language pairs, we show that our approach achieves state-of-the-art performance, outperforming a method exploiting word alignments and a strong machine translation baseline.	[Chandar, Sarath A. P.; Ravindran, Balaraman] Indian Inst Technol Madras, Madras, Tamil Nadu, India; [Lauly, Stanislas; Larochelle, Hugo] Univ Sherbrooke, Sherbrooke, PQ, Canada; [Khapra, Mitesh M.; Raykar, Vikas; Saha, Amrita] IBM Res India, Bengaluru, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Madras; University of Sherbrooke	Chandar, APS (corresponding author), Indian Inst Technol Madras, Madras, Tamil Nadu, India.	apsarathchandar@gmail.com; stanislas.lauly@usherbrooke.ca; hugo.larochelle@usherbrooke.ca; mikhapra@in.ibm.com; viraykar@in.ibm.com; ravi@cse.iitm.ac.in; amrsaha4@in.ibm.com			Google	Google(Google Incorporated)	We would like to thank Alexander Klementiev and Ivan Titov for providing the code for the classifier and data indices. This work was supported in part by Google.	[Anonymous], 2009, P JOINT C 47 ANN M A, P235, DOI DOI 10.3115/1687878.1687913; Collobert R., 2011, J MACHINE LEARNING R, V12; Das D., 2011, P 49 ANN M ASS COMP, V1, P600; Dauphin Y., 2011, P 28 INT C MACH LEAR, P945; Dumais Susan T, 1997, AAAI SPRING S CROSS, V15, P21; Faruqui Manaal, 2014, P EACL, DOI [10.3115/v1/E14-1049, DOI 10.3115/V1/E14-1049]; Gao JF, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P699; Hermann KM, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P58; Hermann KM., 2014, P INT C LEARN REPR I; Klementiev Alexandre, 2012, P INT C COMP LING; KOEHN P, 2005, MT SUMMIT; Larochelle Hugo, 2012, ADV NEURAL INFORM PR; Liu B., 2012, SENTIMENT ANAL OPINI; Mihalcea Rada, 2007, P ANN M ASS COMP LIN, P976; Mikolov T., 2013, EXPLOITING SIMILARIT; Mnih A., 2008, P 21 INT C NEURAL IN, P1081; Morin F., 2005, PROC INT WORKSHOP AR, P246; Pado S, 2009, J ARTIF INTELL RES, V36, P307, DOI 10.1613/jair.2863; Platt John C, 2010, P 2010 C EMP METH NA, P251; Socher R., 2013, LONG PAPERS, V1, P455; Steven E. K. Bird, 2009, NATURAL LANGUAGE PRO; Toutanova K, 2003, HLT-NAACL 2003: HUMAN LANGUAGE TECHNOLOGY CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE MAIN CONFERENCE, P252, DOI 10.3115/1073445.1073478; Turian J, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P384; Yarowsky David, 2001, P 1 INT C HUM LANG T; Yih Wen-tau, 2011, P 15 C COMP NAT LANG, P247; Zou Will Y., 2013, EMPIRICAL METHODS NA	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100049
C	Chaudhuri, K; Dasgupta, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Chaudhuri, Kamalika; Dasgupta, Sanjoy			Rates of convergence for nearest neighbor classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We analyze the behavior of nearest neighbor classification in metric spaces and provide finite-sample, distribution-dependent rates of convergence under minimal assumptions. These are more general than existing bounds, and enable us, as a by-product, to establish the universal consistency of nearest neighbor in a broader range of data spaces than was previously known. We illustrate our upper and lower bounds by introducing a new smoothness class customized for nearest neighbor classification. We find, for instance, that under the Tsybakov margin condition the convergence rate of nearest neighbor matches recently established lower bounds for nonparametric classification.	[Chaudhuri, Kamalika; Dasgupta, Sanjoy] Univ Calif San Diego, Comp Sci & Engn, San Diego, CA 92103 USA	University of California System; University of California San Diego	Chaudhuri, K (corresponding author), Univ Calif San Diego, Comp Sci & Engn, San Diego, CA 92103 USA.	kamalika@cs.ucsd.edu; dasgupta@cs.ucsd.edu			National Science Foundation [IIS-1162581]	National Science Foundation(National Science Foundation (NSF))	The authors are grateful to the National Science Foundation for support under grant IIS-1162581.	Audibert JY, 2007, ANN STAT, V35, P608, DOI 10.1214/009053606000001217; Cover T. M., 1968, P HAW INT C SYST SCI; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Dasgupta S., 2012, 25 C LEARN THEOR; DEVROYE L, 1994, ANN STAT, V22, P1371, DOI 10.1214/aos/1176325633; Federer H., 1969, GEOMETRIC MEASURE TH, V153; Fix E., 1951, 2149004 USAF SCH AV; FRITZ J, 1975, IEEE T INFORM THEORY, V21, P552, DOI 10.1109/TIT.1975.1055443; GYORFI L, 1981, IEEE T INFORM THEORY, V27, P362, DOI 10.1109/TIT.1981.1056344; Kaas R, 1980, STAT NEERL, V34, P13, DOI DOI 10.1111/J.1467-9574.1980.TB00681.X; KULKARNI SR, 1995, IEEE T INFORM THEORY, V41, P1028, DOI 10.1109/18.391248; Mammen E, 1999, ANN STAT, V27, P1808; SLUD EV, 1977, ANN PROBAB, V5, P404, DOI 10.1214/aop/1176995801; Tsybakov AB, 2004, ANN STAT, V32, P135; Urner R., 2011, INT C MACH LEARN; WAGNER TJ, 1971, IEEE T INFORM THEORY, V17, P566, DOI 10.1109/TIT.1971.1054698	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101110
C	Chaudhuri, K; Hsu, D; Song, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Chaudhuri, Kamalika; Hsu, Daniel; Song, Shuang			The Large Margin Mechanism for Differentially Private Maximization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					A basic problem in the design of privacy-preserving algorithms is the private maximization problem: the goal is to pick an item from a universe that (approximately) maximizes a data-dependent function, all under the constraint of differential privacy. This problem has been used as a sub-routine in many privacy-preserving algorithms for statistics and machine learning. Previous algorithms for this problem are either range-dependent-i.e., their utility diminishes with the size of the universe-or only apply to very restricted function classes. This work provides the first general purpose, range-independent algorithm for private maximization that guarantees approximate differential privacy. Its applicability is demonstrated on two fundamental tasks in data mining and machine learning.	[Chaudhuri, Kamalika; Song, Shuang] Univ Calif San Diego, La Jolla, CA 92093 USA; [Hsu, Daniel] Columbia Univ, New York, NY USA	University of California System; University of California San Diego; Columbia University	Chaudhuri, K (corresponding author), Univ Calif San Diego, La Jolla, CA 92093 USA.	kamalika@cs.ucsd.edu; djhsu@cs.columbia.edu; shs037@eng.ucsd.edu	song, song/GWN-2626-2022		NIH [U54 HL108460]; NSF [IIS 1253942]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF))	We thank an anonymous reviewer for suggesting the simpler variant of LMM based on the exponential mechanism. (The original version of LMM used a max of truncated exponentials mechanism, which gives the same guarantees up to constant factors.) This work was supported in part by the NIH under U54 HL108460 and the NSF under IIS 1253942.	Bassily R., 2014, ARXIV14057085; Beimel A, 2010, LECT NOTES COMPUT SC, V5978, P437, DOI 10.1007/978-3-642-11799-2_26; Beimel Amos, 2013, INNOVATIONS THEORETI, P97, DOI [10.1145/2422436.2422450, DOI 10.1145/2422436.2422450]; Beimel Amos, 2013, RANDOM; Bhaskar R., 2010, KDD; Blum A., 2005, PODS; Blum A, 2013, J ACM, V60, DOI 10.1145/2450142.2450148; Bonomi L, 2013, PROC VLDB ENDOW, V6, P1422, DOI 10.14778/2536274.2536329; Bun Mark, 2014, STOC; Chaudhuri K, 2012, ADV NEURAL INFORM PR, P989; Chaudhuri K., 2013, P NEURIPS, P2652; Chaudhuri K., 2012, ICML; Chaudhuri K., 2011, COLT; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Chen R., 2011, VLDB; De A, 2012, LECT NOTES COMPUT SC, V7194, P321, DOI 10.1007/978-3-642-28914-9_18; Dwork C., 2006, THEORY CRYPTOGRAPHY; Dwork C, 2008, LECT NOTES COMPUT SC, V4978, P1, DOI 10.1007/978-3-540-79228-4_1; Dwork C, 2009, ACM S THEORY COMPUT, P371; Dwork Cynthia, 2006, EURO CRYPT; Friedman Arik, 2010, KDD; Hardt M., 2010, FOCS; Hardt M, 2010, ACM S THEORY COMPUT, P705; Jain P., 2012, COLT; Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090; Langford J, 2004, J MACH LEARN RES, V5, P529; Li N., 2012, VLDB; McSherry Frank, 2007, FOCS; Sarwate AD, 2013, IEEE SIGNAL PROC MAG, V30, P86, DOI 10.1109/MSP.2013.2259911; Smith A., 2011, STOC; Smith A., 2013, COLT; Uhler C, 2012, ARXIV12050739; Wasserman L, 2010, J AM STAT ASSOC, V105, P375, DOI 10.1198/jasa.2009.tm08651; Zeng Chen, 2012, VLDB	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101062
C	Chen, C; Liu, H; Metaxas, DN; Zhao, TQ		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Chen, Chao; Liu, Han; Metaxas, Dimitris N.; Zhao, Tianqi			Mode Estimation for High Dimensional Discrete Tree Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					This paper studies the following problem: given samples from a high dimensional discrete distribution, we want to estimate the leading (delta, rho)-modes of the underlying distributions. A point is defined to be a (delta, rho)-mode if it is a local optimum of the density within a delta-neighborhood under metric rho. As we increase the "scale" parameter delta, the neighborhood size increases and the total number of modes monotonically decreases. The sequence of the (delta, rho)-modes reveal intrinsic topographical information of the underlying distributions. Though the mode finding problem is generally intractable in high dimensions, this paper unveils that, if the distribution can be approximated well by a tree graphical model, mode characterization is significantly easier. An efficient algorithm with provable theoretical guarantees is proposed and is applied to applications like data analysis and multiple predictions.	[Chen, Chao; Metaxas, Dimitris N.] Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA; [Liu, Han; Zhao, Tianqi] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA	Rutgers State University New Brunswick; Princeton University	Chen, C (corresponding author), Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA.	chao.chen.cchen@gmail.com; hanliu@princeton.edu; dnm@cs.rutgers.edu; tianqi@princeton.edu			 [NSF IIS 1451292];  [NSF CNS 1229628];  [NSF IIS1408910];  [NSF IIS1332109];  [NIH R01MH102339];  [NIH R01GM083084];  [NIH R01HG06841]	; ; ; ; ; ; 	Chao Chen thanks Vladimir Kolmogorov and Christoph H. Lampert for helpful discussions. The research of Chao Chen and Dimitris N. Metaxas is partially supported by the grants NSF IIS 1451292 and NSF CNS 1229628. The research of Han Liu is partially supported by the grants NSF IIS1408910, NSF IIS1332109, NIH R01MH102339, NIH R01GM083084, and NIH R01HG06841.	Bach F.R., 2003, J MACHINE LEARNING R, V4, P1205; Batra D, 2012, LECT NOTES COMPUT SC, V7576, P1, DOI 10.1007/978-3-642-33715-4_1; Chaudhuri P, 1999, J AM STAT ASSOC, V94, P807, DOI 10.2307/2669996; Chazal F, 2011, COMPUTATIONAL GEOMETRY (SCG 11), P97; Chen C., 2013, INT C ART INT STAT A; Chen C., 2014, TECHNICAL REPORT; Chen C, 2011, IEEE I CONF COMP VIS, P423, DOI 10.1109/ICCV.2011.6126271; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Fromer M, 2009, PROTEINS, V75, P682, DOI 10.1002/prot.22280; Lafferty J, 2001, P 18 INT C MACH LEAR, P282, DOI DOI 10.1038/NPROT.2006.61; Li J, 2007, J MACH LEARN RES, V8, P1687; Lindeberg T., 1993, SCALE SPACE THEORY C; Liu H, 2011, J MACH LEARN RES, V12, P907; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Maitra R, 2009, IEEE ACM T COMPUT BI, V6, P144, DOI 10.1109/TCBB.2007.70244; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; Minnotte M.C., 1993, J COMPUT GRAPH STAT, V2, P51; Nilsson D, 1998, STAT COMPUT, V8, P159, DOI 10.1023/A:1008990218483; Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033; Ray S, 2005, ANN STAT, V33, P2042, DOI 10.1214/009053605000000417; SILVERMAN BW, 1981, J ROY STAT SOC B MET, V43, P97; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wille A, 2004, GENOME BIOL, V5, DOI 10.1186/gb-2004-5-11-r92; Witkin AP, 1987, READINGS COMPUTER VI, P329	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102094
C	Chen, DQ		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Chen, Dongqu			Learning Shuffle Ideals Under Restricted Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				FINITE-STATE TRANSDUCERS; IDENTIFICATION; COMPLEXITY; EFFICIENT	The class of shuffle ideals is a fundamental sub-family of regular languages. The shuffle ideal generated by a string set U is the collection of all strings containing some string u is an element of U as a (not necessarily contiguous) subsequence. In spite of its apparent simplicity, the problem of learning a shuffle ideal from given data is known to be computationally intractable. In this paper, we study the PAC learnability of shuffle ideals and present positive results on this learning problem under element-wise independent and identical distributions and Markovian distributions in the statistical query model. A constrained generalization to learning shuffle ideals under product distributions is also provided. In the empirical direction, we propose a heuristic algorithm for learning shuffle ideals from given labeled strings under general unrestricted distributions. Experiments demonstrate the advantage for both efficiency and accuracy of our algorithm.	[Chen, Dongqu] Yale Univ, Dept Comp Sci, New Haven, CT 06520 USA	Yale University	Chen, DQ (corresponding author), Yale Univ, Dept Comp Sci, New Haven, CT 06520 USA.	dongqu.chen@yale.edu						Angluin D, 2013, J MACH LEARN RES, V14, P1513; [Anonymous], 2007, LARGE SCALE KERNEL M; Bshouty NH, 1997, MACH LEARN, V26, P25, DOI 10.1023/A:1007320031970; de la Higuera C, 2005, PATTERN RECOGN, V38, P1332, DOI 10.1016/j.patcog.2005.01.003; Gnedenko B., 1949, ADDISON WESLEY SERIE; GOLD EM, 1978, INFORM CONTROL, V37, P302, DOI 10.1016/S0019-9958(78)90562-4; Ibragimov I. A., 1956, THEORY PROBABILITY I, V1, P255, DOI DOI 10.1137/1101021; Kearns M, 1998, J ACM, V45, P983, DOI 10.1145/293347.293351; Kontorovich L, 2008, THEOR COMPUT SCI, V405, P223, DOI 10.1016/j.tcs.2008.06.037; Kontorovich L, 2009, J MACH LEARN RES, V10, P1095; Koskenniemi Kimmo, 1983, IJCAI, V83, P683; Lichman M, 2013, UCI MACHINE LEARNING; Mohri M, 1997, COMPUT LINGUIST, V23, P269; Mohri M, 2002, COMPUT SPEECH LANG, V16, P69, DOI 10.1006/csla.2001.0184; Mohri M, 2010, IEEE T AUDIO SPEECH, V18, P197, DOI 10.1109/TASL.2009.2023170; PITT L, 1993, J ACM, V40, P95, DOI 10.1145/138027.138042; Sproat R, 1996, COMPUT LINGUIST, V22, P377; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; [No title captured], DOI DOI 10.1017/S135132499600126X; [No title captured]	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103057
C	Chen, XJ; Yuille, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Chen, Xianjie; Yuille, Alan			Articulated Pose Estimation by a Graphical Model with Image Dependent Pairwise Relations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We present a method for estimating articulated human pose from a single static image based on a graphical model with novel pairwise relations that make adaptive use of local image measurements. More precisely, we specify a graphical model for human pose which exploits the fact the local image measurements can be used both to detect parts (or joints) and also to predict the spatial relationships between them (Image Dependent Pairwise Relations). These spatial relationships are represented by a mixture model. We use Deep Convolutional Neural Networks (DCNNs) to learn conditional probabilities for the presence of parts and their spatial relationships within image patches. Hence our model combines the representational flexibility of graphical models with the efficiency and statistical power of DCNNs. Our method significantly outperforms the state of the art methods on the LSP and FLIC datasets and also performs very well on the Buffy dataset without any training.	[Chen, Xianjie; Yuille, Alan] Univ Calif Los Angeles, Los Angeles, CA 90024 USA	University of California System; University of California Los Angeles	Chen, XJ (corresponding author), Univ Calif Los Angeles, Los Angeles, CA 90024 USA.	cxj@ucla.edu; yuille@stat.ucla.edu			 [ONR MURI N000014-10-1-0933];  [ONR N00014-12-1-0883];  [ARO 62250-CS]	; ; 	This research has been supported by grants ONR MURI N000014-10-1-0933, ONR N00014-12-1-0883 and ARO 62250-CS. The GPUs used in this research were generously donated by the NVIDIA Corporation.	CHEN X., 2014, COMPUTER VISION PATT; Cho N.-G., 2013, PATTERN RECOGNITION; Dalal N.B.T., 2005, COMPUTER VISION PATT; Eichner M., 2012, INT J COMPUTER VISIO; Eichner M., 2012, AS C COMP VIS ACCV; Felzenszwalb P. F., 2005, INT J COMPUTER VISIO; Ferrari V, 2008, PROC CVPR IEEE, DOI 10.1109/CVPR.2008.4587468; Fischler M. A., 1973, IEEE TRANSACTIONS ON; Jia Y., 2013, CAFFE OPEN SOURCE CO; Johnson S., 2010, BRIT MACH VIS C BMVC; Karlinsky L., 2012, EUR C COMP VIS ECCV; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lafferty J., 2001, P 18 INT C MACH LEAR; Ouyang W., 2014, COMPUTER VISION PATT; Pishchulin L., 2013, INT C COMP VIS ICCV; Pishchulin L., 2013, COMPUTER VISION PATT; Ramanan D., 2006, NIPS; Rother C., 2004, ACM T GRAPHICS TOG; Sapp B., 2013, COMPUTER VISION PATT; Sapp B., 2010, EUR C COMP VIS ECCV; Sapp B., 2010, COMPUTER VISION PATT; Sermanet P., 2014, P 2 INT C LEARN REPR; Toshev A., 2014, COMPUTER VISION PATT; Tsochantaridis I., 2004, INT C MACH LEARN ICM; Wang C., 2013, COMPUTER VISION PATT; Yang Y., 2013, IEEE T PATTERN ANAL; Yang Y., 2011, COMPUTER VISION PATT	27	0	0	0	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100070
C	Chen, X; Cheng, XY; Mallat, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Chen, Xu; Cheng, Xiuyuan; Mallat, Stephane			Unsupervised Deep Haar Scattering on Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The classification of high-dimensional data defined on graphs is particularly difficult when the graph geometry is unknown. We introduce a Haar scattering transform on graphs, which computes invariant signal descriptors. It is implemented with a deep cascade of additions, subtractions and absolute values, which iteratively compute orthogonal Haar wavelet transforms. Multiscale neighborhoods of unknown graphs are estimated by minimizing an average total variation, with a pair matching algorithm of polynomial complexity. Supervised classification with dimension reduction is tested on data bases of scrambled images, and for signals sampled on unknown irregular grids on a sphere.	[Chen, Xu] Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA; [Chen, Xu; Cheng, Xiuyuan; Mallat, Stephane] Ecole Normale Super, Dept Informat, Paris, France	Princeton University; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Chen, X (corresponding author), Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA.				ERC [InvariantClass 320959]	ERC(European Research Council (ERC)European Commission)	This work was supported by the ERC grant InvariantClass 320959.	Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bruna J, 2013, PROC INT C LEARN REP; Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; Edmonds J., 1965, CANADIAN J MATH; Gavish M., 2010, P 27 INT C MACH LEAR, P367; Goodfellow I. J., 2013, ARXIV13024389; Hein M, 2011, NIPS; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Le Q., 2013, ICML; LeCun Y., 2010, P IEEE INT SUMP CIRC; Li D., 2011, 12 ANN C INT SPEECH, P2285; Mallat S, 2010, P EUSICO C DENM; Mehmood T, 2012, CHEMOMETR INTELL LAB, V118, P62, DOI 10.1016/j.chemolab.2012.07.010; Rothberg E., JACM, V23; Roux N. L., 2008, ADV NEURAL INF PROCE, P841; Rustamov R., 2013, NIPS; Schwartz W. R., 2009, COMPUTER VISION ICCV; Shuman D., 2013, IEEE SIGNAL P MA MAY; Zhang H., 2014, SIGN PROC C EUSIPCO	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102106
C	Chow, Y; Ghavamzadeh, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Chow, Yinlam; Ghavamzadeh, Mohammad			Algorithms for CVaR Optimization in MDPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				VALUE-AT-RISK; STOCHASTIC-APPROXIMATION; DECISION; COST	In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion. Conditional value-at-risk (CVaR) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures, and because of its computational efficiencies has gained popularity in finance and operations research. In this paper, we consider the mean-CVaR optimization problem in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem.	[Chow, Yinlam] Stanford Univ, Inst Computat & Math Engn, Stanford, CA 94305 USA; [Chow, Yinlam; Ghavamzadeh, Mohammad] Adobe Res, San Jose, CA USA; [Ghavamzadeh, Mohammad] INRIA Lille, Team SequeL, Lille, France	Stanford University; Adobe Systems Inc.	Chow, Y (corresponding author), Stanford Univ, Inst Computat & Math Engn, Stanford, CA 94305 USA.							Artzner P, 1999, MATH FINANC, V9, P203, DOI 10.1111/1467-9965.00068; Baeuerle N, 2011, MATH METHOD OPER RES, V74, P361, DOI 10.1007/s00186-011-0367-0; Bardou O, 2009, MONTE CARLO METHODS, V15, P173, DOI 10.1515/MCMA.2009.011; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Bhatnagar S., 2008, ADV NEURAL INFORM PR, P105; Boda K, 2006, MATH METHOD OPER RES, V63, P169, DOI 10.1007/s00186-005-0045-1; Borkar V., 2014, IEEE T AUTOMATIC CON; Borkar VS, 2002, MATH OPER RES, V27, P294, DOI 10.1287/moor.27.2.294.324; Chow Y., 2014, ARXIV14063339; FILAR JA, 1989, MATH OPER RES, V14, P147, DOI 10.1287/moor.14.1.147; HOWARD RA, 1972, MANAGE SCI, V18, P356, DOI 10.1287/mnsc.18.7.356; Markowitz H, 1959, PORTFOLIO SELECTION; Morimura T., 2010, PROC 27 INT C MACH L, P799; Ott J., 2010, THESIS; Peters J, 2005, LECT NOTES ARTIF INT, V3720, P280, DOI 10.1007/11564096_29; Petrik M., 2012, P 28 INT C UNC ART I; Rockafellar RT, 2002, J BANK FINANC, V26, P1443, DOI 10.1016/S0378-4266(02)00271-6; SOBEL MJ, 1982, J APPL PROBAB, V19, P794, DOI 10.2307/3213832; SPALL JC, 1992, IEEE T AUTOMAT CONTR, V37, P332, DOI 10.1109/9.119632; Tamar A., 2012, P 29 INT C MACH LEAR, P1651; Tamar A., 2014, ARXIV14043862V1	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100025
C	Conejo, B; Komodakis, N; Leprince, S; Avouac, JP		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Conejo, Bruno; Komodakis, Nikos; Leprince, Sebastien; Avouac, Jean Philippe			Inference by Learning: Speeding-up Graphical Model Optimization via a Coarse-to-Fine Cascade of Pruning Classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We propose a general and versatile framework that significantly speeds-up graphical model optimization while maintaining an excellent solution accuracy. The proposed approach, refereed as Inference by Learning or in short as IbyL, relies on a multi-scale pruning scheme that progressively reduces the solution space by use of a coarse-to-fine cascade of learnt classifiers. We thoroughly experiment with classic computer vision related MRF problems, where our novel framework constantly yields a significant time speed-up (with respect to the most efficient inference methods) and obtains a more accurate solution than directly optimizing the MRF. We make our code available on-line [4].	[Conejo, Bruno; Leprince, Sebastien; Avouac, Jean Philippe] CALTECH, GPS Div, Pasadena, CA 91125 USA; [Conejo, Bruno; Komodakis, Nikos] Univ Paris Est, Ecole Ponts ParisTech, Marne La Vallee, France	California Institute of Technology; Ecole des Ponts ParisTech; Universite Gustave-Eiffel	Conejo, B (corresponding author), CALTECH, GPS Div, Pasadena, CA 91125 USA.	bconejo@caltech.edu; nikos.komodakis@enpc.fr; leprincs@caltech.edu; avouac@gps.caltech.edu	Avouac, Jean-Philippe/B-5699-2015	Avouac, Jean-Philippe/0000-0002-3060-8442	USGS; LiDAR project (USGS Award) [G13AP00037]; Terrestrial Hazard Observation and Reporting Center of Caltech; Moore foundation through the Advanced Earth Surface Observation Project (AESOP) [2808]	USGS(United States Geological Survey); LiDAR project (USGS Award); Terrestrial Hazard Observation and Reporting Center of Caltech; Moore foundation through the Advanced Earth Surface Observation Project (AESOP)	This work was supported by USGS through the Measurements of surface ruptures produced by continental earthquakes from optical imagery and LiDAR project (USGS Award G13AP00037), the Terrestrial Hazard Observation and Reporting Center of Caltech, and the Moore foundation through the Advanced Earth Surface Observation Project (AESOP Grant 2808).	Baker S., 2007, ICCV 2007; Bergtholdt M., 2010, IJCV; Black M. J, 2005, CVPR; Boykov Y., 2001, PAMI; Conejo B., 2014, ISPRS ANN PHOTOGRAMM; Felzenszwalb P., 2004, IJCV; Felzenszwalb Pedro F., 2004, CVPR; Felzenszwalb Pedro F., 2005, IJCV; Freeman W.T., 1999, ICCV; Hu X., 2012, PAMI; Kappes J. H., 2013, CVPR; Kim J., 2003, ICCV; Kim S., 2014, IMAGE SEGMENTATION U; Kim Taesup, 2011, CVPR; Kohlberger T., 2005, IEEE T INFORM THEORY; Kohli Pushmeet, 2010, DAGM S; Kolmogorov V., 2006, PAMI; Kolmogorov V., 2004, PAMI; Komodakis N., 2007, P 2007 IEEE C COMP V, P1, DOI [10.1109/CVPR.2007.383095, DOI 10.1109/CVPR.2007.383095]; Kumar M.P., 2005, CVPR; KUMAR MP, 2009, UAI; Lombaert H., 2005, ICCV 2005; MELTZER T, 2005, ICCV; Perez P., 1996, IEEE T INFORM THEORY, P1914; Rother C., 2007, CVPR; Shekhovtsov Alexander, 2014, CVPR; Shi J., 2000, PAMI; SZELISKI R, 2008, PAMI; Wainwright M. J., 2005, IEEE T INFORM THEORY	29	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101028
C	Dekel, O; Hazan, E; Koren, T		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Dekel, Ofer; Hazan, Elad; Koren, Tomer			The Blinded Bandit: Learning with Adaptive Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				REGRET	We study an online learning setting where the player is temporarily deprived of feedback each time it switches to a different action. Such model of adaptive feedback naturally occurs in scenarios where the environment reacts to the player's actions and requires some time to recover and stabilize after the algorithm switches actions. This motivates a variant of the multi-armed bandit problem, which we call the blinded multi-armed bandit, in which no feedback is given to the algorithm whenever it switches arms. We develop efficient online learning algorithms for this problem and prove that they guarantee the same asymptotic regret as the optimal algorithms for the standard multi-armed bandit problem. This result stands in stark contrast to another recent result, which states that adding a switching cost to the standard multi-armed bandit makes it substantially harder to learn, and provides a direct comparison of how feedback and loss contribute to the difficulty of an online learning problem. We also extend our results to the general prediction framework of bandit linear optimization, again attaining near-optimal regret bounds.	[Dekel, Ofer] Microsoft Res, Redmond, WA 98052 USA; [Hazan, Elad; Koren, Tomer] Technion, Haifa, Israel	Microsoft	Dekel, O (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	oferd@microsoft.com; ehazan@ie.technion.ac.il; tomerk@technion.ac.il		Hazan, Elad/0000-0002-1566-3216	Microsoft-Technion EC center; European Union's Seventh Framework Programme (FP7/2007-2013]) [336078 ERC-SUBLRN]	Microsoft-Technion EC center; European Union's Seventh Framework Programme (FP7/2007-2013])	The research leading to these results has received funding from the Microsoft-Technion EC center, and the European Union's Seventh Framework Programme (FP7/2007-2013]) under grant agreement no 336078 ERC-SUBLRN.	Abernethy J, 2008, 21 ANN C LEARNING TH, P263; Antos A., 2012, THEORETICAL COMPUTER; Arora R, 2012, P 29 INT C MACH LEAR; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Awerbuch B, 2004, P 36 ANN ACM S THEOR, P45; Bubeck S., 2012, P 25 ANN C LEARN THE, V23; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cesa-Bianchi N, 2005, IEEE T INFORM THEORY, V51, P2152, DOI 10.1109/TIT.2005.847729; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; DANI V, 2006, P 17 ANN ACM SIAM S; Dani V, 2007, ADV NEURAL INFORM PR, V20, P345; Dekel O., 2013, ARXIV13102997; Hazan E., 2013, ARXIV13126214; Kohavi R., 2012, P 18 ACM SIGKDD INT, P786; Kohavi R, 2009, DATA MIN KNOWL DISC, V18, P140, DOI 10.1007/s10618-008-0114-1; Mesterharm C., 2005, P 16 INT C ALG LEARN; NEU G, 2010, ADV NEURAL INFORM PR, P1804	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102088
C	Derezinski, M; Warmuth, MK		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Derezinski, Michal; Warmuth, Manfred K.			The limits of squared Euclidean distance regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				BOUNDS	Some of the simplest loss functions considered in Machine Learning are the square loss, the logistic loss and the hinge loss. The most common family of algorithms, including Gradient Descent (GD) with and without Weight Decay, always predict with a linear combination of the past instances. We give a random construction for sets of examples where the target linear weight vector is trivial to learn but any algorithm from the above family is drastically sub-optimal. Our lower bound on the latter algorithms holds even if the algorithms are enhanced with an arbitrary kernel function. This type of result was known for the square loss. However, we develop new techniques that let us prove such hardness results for any loss function satisfying some minimal requirements on the loss function (including the three listed above). We also show that algorithms that regularize with the squared Euclidean distance are easily confused by random features. Finally, we conclude by discussing related open problems regarding feed forward neural networks. We conjecture that our hardness results hold for any training algorithm that is based on the squared Euclidean distance regularization (i.e. Back-propagation with the Weight Decay heuristic).	[Derezinski, Michal; Warmuth, Manfred K.] Univ Calif Santa Cruz, Comp Sci Dept, Santa Cruz, CA 95064 USA	University of California System; University of California Santa Cruz	Derezinski, M (corresponding author), Univ Calif Santa Cruz, Comp Sci Dept, Santa Cruz, CA 95064 USA.	mderezin@soe.ucsc.edu; manfred@cse.ucsc.edu			NSF [IIS-1118028]	NSF(National Science Foundation (NSF))	This research was supported by the NSF grant IIS-1118028.	ALON N, 1985, P 26 S FDN COMP SCI, P277; [Anonymous], 2012, ABS12070580 CORR; Balcan M.-F., 2008, PROC 21 ANN C LEARN, P287; Ben-David S, 2002, J MACHINE LEARNING R, V3, P441; Forster J, 2006, THEOR COMPUT SCI, V350, P40, DOI 10.1016/j.tcs.2005.10.015; Forster J, 2002, LECT NOTES ARTIF INT, V2533, P128; Helmbold DP, 1999, IEEE T NEURAL NETWOR, V10, P1291, DOI 10.1109/72.809075; KIMELDORF G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3; Kivinen J, 1997, ARTIF INTELL, V97, P325, DOI 10.1016/S0004-3702(97)00039-8; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Ng A.Y., 2004, P 21 INT C MACH LEAR, P615; Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27; Srebro N., 2004, THESIS; Warmuth M. K., 2014, J THEORETICAL COMPUT; WARMUTH MK, 2005, P 18 ANN C LEARN THE	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102001
C	Deshpande, Y; Montanari, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Deshpande, Yash; Montanari, Andrea			Sparse PCA via Covariance Thresholding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				LARGEST EIGENVALUE; DEFORMATION	In sparse principal component analysis we are given noisy observations of a low-rank matrix of dimension n x p and seek to reconstruct it under additional sparsity assumptions. In particular, we assume here that the principal components v(1), ... ,v(r) have at most k(1), ... ,k(g) non-zero entries respectively, and study the high-dimensional regime in which p is of the same order as n. In an influential paper, Johnstone and Lu [JL04] introduced a simple algorithm that estimates the support of the principal vectors v(1),..., v(r) by the largest entries in the diagonal of the empirical covariance. This method can be shown to succeed with high probability if k(q )<= C-1 root n/log p, and to fail with high probability if k(q) >= C-2 root n/log p for two constants 0 < C-1,C-2 < infinity. Despite a considerable amount of work over the last ten years, no practical algorithm exists with provably better support recovery guarantees. Here we analyze a covariance thresholding algorithm that was recently proposed by Krauthgamer, Nadler and Vilenchik [KNV13]. We confirm empirical evidence presented by these authors and rigorously prove that the algorithm succeeds with high probability for k of order root n. Recent conditional lower bounds [BR13] suggest that it might be impossible to do significantly better. The key technical component of our analysis develops new bounds on the norm of kernel random matrices, in regimes that were not considered before.	[Deshpande, Yash] Stanford Univ, Elect Engn, Stanford, CA 94305 USA; [Montanari, Andrea] Stanford Univ, Elect Engn & Stat, Stanford, CA 94305 USA	Stanford University; Stanford University	Deshpande, Y (corresponding author), Stanford Univ, Elect Engn, Stanford, CA 94305 USA.	yashd@stanford.edu; montanari@stanford.edu						Amini AA, 2009, ANN STAT, V37, P2877, DOI 10.1214/08-AOS664; [Anonymous], 2005, ADV NEURAL INF PROCE; Bickel PJ, 2008, ANN STAT, V36, P199, DOI 10.1214/009053607000000758; Cai TT, 2013, ANN STAT, V41, P3074, DOI 10.1214/13-AOS1178; Cheng Xiuyuan, 2012, ARXIV12023155; d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269; d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506; DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001; DONOHO DL, 1994, PROBAB THEORY REL, V99, P277, DOI 10.1007/BF01199026; El Karoui N, 2010, ANN STAT, V38, P3191, DOI 10.1214/10-AOS801; El Karoui N, 2010, ANN STAT, V38, P1, DOI 10.1214/08-AOS648; Feral D, 2007, COMMUN MATH PHYS, V272, P185, DOI 10.1007/s00220-007-0209-3; Johnstone Iain M, 2009, J AM STAT ASS, V104; Johnstone Iain M, 2004, SPARSE PRINCIP UNPUB; Johnstone IM, 2002, FUNCTION ESTIM UNPUB, V2; Knowles A., 2013, COMMUNICATIONS PURE; Krauthgamer R., 2013, CORR; Ma Z., 2013, ARXIV13095914; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Rigollet P., 2013, ARXIV13040828; Vincent Q Vu, 2012, P 15 INT C ART INT S; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018; Witten DM, 2009, BIOSTATISTICS, V10, P515, DOI 10.1093/biostatistics/kxp008; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101077
C	Deshpande, Y; Montanari, A; Richard, E		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Deshpande, Yash; Montanari, Andrea; Richard, Emile			Cone-constrained Principal Component Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				SIGNAL RECOVERY; ALGORITHMS	Estimating a vector from noisy quadratic observations is a task that arises naturally in many contexts, from dimensionality reduction, to synchronization and phase retrieval problems. It is often the case that additional information is available about the unknown vector (for instance, sparsity, sign or magnitude of its entries). Many authors propose non-convex quadratic optimization problems that aim at exploiting optimally this information. However, solving these problems is typically NP-hard. We consider a simple model for noisy quadratic observation of an unknown vector v(0). The unknown vector is constrained to belong to a cone C (sic) v(0). While optimal estimation appears to be intractable for the general problems in this class, we provide evidence that it is tractable when C is a convex cone with an efficient projection. This is surprising, since the corresponding optimization problem is non-convex and -from a worst case perspective- often NP hard. We characterize the resulting minimax risk in terms of the statistical dimension of the cone delta(C). This quantity is already known to control the risk of estimation from gaussian observations and random linear measurements. It is rather surprising that the same quantity plays a role in the estimation risk from quadratic measurements.	[Deshpande, Yash; Richard, Emile] Stanford Univ, Elect Engn, Stanford, CA 94305 USA; [Montanari, Andrea] Stanford Univ, Elect Engn & Stat, Stanford, CA 94305 USA	Stanford University; Stanford University	Deshpande, Y (corresponding author), Stanford Univ, Elect Engn, Stanford, CA 94305 USA.							Amelunxen D., 2013, UNPUB; Amini AA, 2008, IEEE INT SYMP INFO, P2454, DOI 10.1109/ISIT.2008.4595432; Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817; Birnbaum A, 2013, ANN STAT, V41, P1055, DOI 10.1214/12-AOS1014; Biswas P, 2004, IPSN '04: THIRD INTERNATIONAL SYMPOSIUM ON INFORMATION PROCESSING IN SENSOR NETWORKS, P46; Deshpande Yash, 2013, ARXIV13115179; Donoho DL, 2013, IEEE T INFORM THEORY, V59, P3396, DOI 10.1109/TIT.2013.2239356; Feral D, 2007, COMMUN MATH PHYS, V272, P185, DOI 10.1007/s00220-007-0209-3; FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758; Gartner Bernd, 2012, APPROXIMATION ALGORI; Jaganathan Kishore, 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1473, DOI 10.1109/ISIT.2012.6283508; Johnstone Iain M, 2004, UNPUB; Jolliffe I., 2011, INT ENCY STAT SCI; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Li XD, 2013, SIAM J MATH ANAL, V45, P3019, DOI 10.1137/120893707; Luss R, 2013, SIAM REV, V55, P65, DOI 10.1137/110839072; Ma Z., 2013, ARXIV13095914; Montanari A., 2014, ARXIV14064775; PAATERO P, 1994, ENVIRONMETRICS, V5, P111, DOI 10.1002/env.3170050203; Paatero P, 1997, CHEMOMETR INTELL LAB, V37, P23, DOI 10.1016/S0169-7439(96)00044-5; Singer A, 2008, P NATL ACAD SCI USA, V105, P9507, DOI 10.1073/pnas.0709842104; Vilenchik, 2013, ARXIV13063690; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103069
C	Diego, F; Hamprecht, FA		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Diego, Ferran; Hamprecht, Fred A.			Sparse space-time deconvolution for Calcium image analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We describe a unified formulation and algorithm to find an extremely sparse representation for Calcium image sequences in terms of cell locations, cell shapes, spike timings and impulse responses. Solution of a single optimization problem yields cell segmentations and activity estimates that are on par with the state of the art, without the need for heuristic pre- or postprocessing. Experiments on real and synthetic data demonstrate the viability of the proposed method.	[Diego, Ferran; Hamprecht, Fred A.] Heidelberg Univ, Interdisciplinary Ctr Sci Comp IWR, Heidelberg Collaboratory Image Proc HCI, D-69115 Heidelberg, Germany	Ruprecht Karls University Heidelberg	Diego, F (corresponding author), Heidelberg Univ, Interdisciplinary Ctr Sci Comp IWR, Heidelberg Collaboratory Image Proc HCI, D-69115 Heidelberg, Germany.	ferran.diego@iwr.uni-heidelberg.de; fred.hamprecht@iwr.uni-heidelberg.de						Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Chambolle A., 2004, ALGORITHM TOTAL VARI; Diego F., 2013, NIPS; Diego F, 2013, I S BIOMED IMAGING, P1058; Grewe BF, 2010, NAT METHODS, V7, P399, DOI 10.1038/nmeth.1453; Grienberger C, 2012, NEURON, V73, P862, DOI 10.1016/j.neuron.2012.02.011; Huber D, 2012, NATURE, V484, P473, DOI 10.1038/nature11039; Kavukcuoglu K., 2010, NIPS; Kreuz T., 2012, J NEUROPHYSIOLOGY; Lutcke H, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00201; Mairal Julien, 2010, J MACHINE LEARNING R; Maruyama R, 2014, NEURAL NETWORKS, V55, P11, DOI 10.1016/j.neunet.2014.03.007; Mukamel E. A., 2009, NEURON; Pachitariu M., 2013, NIPS; Pnevmatikakis E. A., 2013, NIPS; Reichinnek S, 2012, NEUROIMAGE, V60, P139, DOI 10.1016/j.neuroimage.2011.12.018; Rigamonti Roberto, 2013, C COMP VIS PATT REC; Schmidt M. N., 2006, ICA; Smaragdis P, 2004, LECT NOTES COMPUT SC, V3195, P494; Szlam A., 2010, COMPUTER RES REPOSIT; Taylor G. W., 2010, CONVOLUTIONAL LEARNI; Tomek J, 2013, J NEUROPHYSIOL, V110, P243, DOI 10.1152/jn.00087.2013; Valmianski I., 2010, J NEUROPHYSIOLOGY; Vogelstein J. T., 2010, J NEUROPHYSIOLOGY; Zeiler Matthew D., 2010, CVPR	25	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101013
C	Djolonga, J; Krause, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Djolonga, Josip; Krause, Andreas			From MAP to Marginals: Variational Inference in Bayesian Submodular Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				OPTIMIZATION	Submodular optimization has found many applications in machine learning and beyond. We carry out the first systematic investigation of inference in probabilistic models defined through submodular functions, generalizing regular pairwise MRFs and Determinantal Point Processes. In particular, we present L-FIELD, a variational approach to general log-submodular and log-supermodular distributions based on sub- and supergradients. We obtain both lower and upper bounds on the log-partition function, which enables us to compute probability intervals for marginals, conditionals and marginal likelihoods. We also obtain fully factorized approximate posteriors, at the same computational cost as ordinary submodular optimization. Our framework results in convex problems for optimizing over differentials of submodular functions, which we show how to optimally solve. We provide theoretical guarantees of the approximation quality with respect to the curvature of the function. We further establish natural relations between our variational approach and the classical mean-field method. Lastly, we empirically demonstrate the accuracy of our inference scheme on several submodular models.	[Djolonga, Josip; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	ETH Zurich	Djolonga, J (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	josipd@inf.ethz.ch; krausea@ethz.ch			SNSF [200021_137528, ERC StG 307036]; Microsoft Research Faculty Fellowship	SNSF(Swiss National Science Foundation (SNSF)); Microsoft Research Faculty Fellowship(Microsoft)	This research was supported in part by SNSF grant 200021_137528, ERC StG 307036 and a Microsoft Research Faculty Fellowship.	Bach F., 2010, NIPS; Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y., 2004, PATTERN ANAL MACHINE, V26; Buchbinder N., 2012, FDN COMPUTER SCI FOC; Calinescu G., 2007, INTEGER PROGRAMMING; Cevher V, 2011, IEEE J-STSP, V5, P979, DOI 10.1109/JSTSP.2011.2161862; CONFORTI M, 1984, DISCRETE APPL MATH, V7, P251, DOI 10.1016/0166-218X(84)90003-9; CUNNINGHAM WH, 1983, COMBINATORICA, V3, P53, DOI 10.1007/BF02579341; Edmonds Jack, 1970, COMBINATORIAL STRUCT, P69; El-Arini K., 2009, P ACM SIGKDD INT C K; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Gillenwater J., 2012, P NEUR INF PROC SYST; Goldberg LA, 2007, COMB PROBAB COMPUT, V16, P43, DOI 10.1017/S096354830600767X; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; Gomes R., 2010, ICML; Iyer R., 2013, P 30 INT C MACH LEAR, P855; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Jegelka S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1897, DOI 10.1109/CVPR.2011.5995589; Jegelka S., 2011, NIPS; Jegelka S., 2013, ADV NEURAL INFO PROC, P1313; JERRUM M, 1993, SIAM J COMPUT, V22, P1087, DOI 10.1137/0222066; Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Krause A., 2005, C UNC ART INT UAI; Krause A, 2008, J WATER RES PL-ASCE, V134, P516, DOI 10.1061/(ASCE)0733-9496(2008)134:6(516); Krause A, 2014, TRACTABILITY, P71; Kulesza A., 2012, FDN TRENDS MACHINE L, V5; Lin Hui, 2011, P 49 ANN M ASS COMP, P510; Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10; Stobbe P., 2010, P NEUR INF PROC SYST; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Yue Y., 2011, NEURAL INFORM PROCES	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102053
C	Drugowitsch, J; Moreno-Bote, R; Pongee, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Drugowitsch, Jan; Moreno-Bote, Ruben; Pongee, Alexandre			7 Optimal decision-making with time-varying evidence reliability	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Previous theoretical and experimental work on optimal decision-making was restricted to the artificial setting of a reliability of the momentary sensory evidence that remained constant within single trials. The work presented here describes the computation and characterization of optimal decision-making in the more realistic case of an evidence reliability that varies across time even within a trial. It shows that, in this case, the optimal behavior is determined by a bound in the decision maker's belief that depends only on the current, but not the past, reliability. We furthermore demonstrate that simpler heuristics fail to match the optimal performance for certain characteristics of the process that determines the time-course of this reliability, causing a drop in reward rate by more than 50%.	[Drugowitsch, Jan; Pongee, Alexandre] Univ Geneva, Dept Neurosci Fondamentales, CH-1211 Geneva 4, Switzerland; [Moreno-Bote, Ruben] Parc Sanitari St Joan de Deu, Res Unit, Barcelona 08950, Spain; [Moreno-Bote, Ruben] Univ Barcelona, Barcelona 08950, Spain	University of Geneva; University of Barcelona	Drugowitsch, J (corresponding author), Univ Geneva, Dept Neurosci Fondamentales, CH-1211 Geneva 4, Switzerland.	jdrugo@gmail.com; alexandre.pouget@unige.ch; rmoreno@fsjd.org						Bellman RE, 1957, DYNAMIC PROGRAMMING; Bogacz R, 2006, PSYCHOL REV, V113, P700, DOI 10.1037/0033-295X.113.4.700; Bogacz R, 2010, Q J EXP PSYCHOL, V63, P863, DOI 10.1080/17470210903091643; COX JC, 1985, ECONOMETRICA, V53, P385, DOI 10.2307/1911242; Deneve S, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00075; Drugowitsch J, 2014, ELIFE, V3, DOI 10.7554/eLife.03005; Drugowitsch J, 2012, J NEUROSCI, V32, P3612, DOI 10.1523/JNEUROSCI.4010-11.2012; Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a; Fetsch CR, 2009, J NEUROSCI, V29, P15601, DOI 10.1523/JNEUROSCI.2574-09.2009; Fleming W.H., 2012, STOCHASTIC MODELLING; Johnson S. G., NLOPT NONLINEAR OPTI; Kiani R, 2008, J NEUROSCI, V28, P3017, DOI 10.1523/JNEUROSCI.4761-07.2008; Mahadevan S, 1996, MACH LEARN, V22, P159, DOI 10.1007/BF00114727; Mazurek ME, 2003, CEREB CORTEX, V13, P1257, DOI 10.1093/cercor/bhg097; Palmer J, 2005, J VISION, V5, P376, DOI 10.1167/5.5.1; Puterman M.L., 2014, WILEY SERIES PROBABI; RATCLIFF R, 1978, PSYCHOL REV, V85, P59, DOI 10.1037//0033-295X.85.2.59; Roitman JD, 2002, J NEUROSCI, V22, P9475; Rowan TH, 1990, DISSERTATION	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102101
C	Du, N; Liang, YY; Balcan, MF; Song, L		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Du, Nan; Liang, Yingyu; Balcan, Maria-Florina; Song, Le			Learning Time-Varying Coverage Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				RATES	Coverage functions are an important class of discrete functions that capture the law of diminishing returns arising naturally from applications in social network analysis, machine learning, and algorithmic game theory. In this paper, we propose a new problem of learning time-varying coverage functions, and develop a novel parametrization of these functions using random features. Based on the connection between time-varying coverage functions and counting processes, we also propose an efficient parameter learning algorithm based on likelihood maximization, and provide a sample complexity analysis. We applied our algorithm to the influence function estimation problem in information diffusion in social networks, and show that with few assumptions about the diffusion processes, our algorithm is able to estimate influence significantly more accurately than existing approaches on both synthetic and real world data.	[Du, Nan; Song, Le] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA; [Liang, Yingyu] Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA; [Balcan, Maria-Florina] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	University System of Georgia; Georgia Institute of Technology; Princeton University; Carnegie Mellon University	Du, N (corresponding author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.	dunan@gatech.edu; yingyul@cs.princeton.edu; ninamf@cs.cmu.edu; lsong@cc.gatech.edu			NSF [CCF-0953192, CCF-1451177, CCF-1101283, CCF-1422910, IIS1116886]; ONR [N00014-09-1-0751]; AFOSR [FA9550-09-1-0538]; Raytheon Faculty Fellowship; NSF/NIH BIGDATA [1R01GM108341]; NSF CAREER [IIS1350983]; Facebook Graduate Fellowship	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); Raytheon Faculty Fellowship; NSF/NIH BIGDATA; NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); Facebook Graduate Fellowship(Facebook Inc)	This work was supported in part by NSF grants CCF-0953192, CCF-1451177, CCF-1101283, and CCF-1422910, ONR grant N00014-09-1-0751, AFOSR grant FA9550-09-1-0538, Raytheon Faculty Fellowship, NSF IIS1116886, NSF/NIH BIGDATA 1R01GM108341, NSF CAREER IIS1350983 and Facebook Graduate Fellowship 2014-2015.	Aalen OO, 2008, STAT BIOL HEALTH, P1; [Anonymous], 2011, ARXIV11050697; Badanidiyuru A., 2012, ANN ACM SIAM S DISCR; Balcan MF, 2011, ACM S THEORY COMPUT, P793; Birge L, 1998, BERNOULLI, V4, P329, DOI 10.2307/3318720; Du Nan, 2013, NIPS; Du Nan, 2014, ICML; Feldman Vitaly, 2013, ARXIV13042079; Feldman Vitaly, 2013, FOCS; Friedlander M. P., 2009, AISTATS; GUESTRIN C, 2005, INT C MACH LEARN ICM; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Kolar M, 2010, ANN APPL STAT, V4, P94, DOI 10.1214/09-AOAS308; Lehmann B., 2001, EC'01. Proceedings of the 3rd ACM Conference on Electronic Commerce, P18, DOI 10.1145/501158.501161; Leskovec J, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P497; Leskovec J, 2010, J MACH LEARN RES, V11, P985; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Netrapalli Praneeth, 2012, Performance Evaluation Review, V40, P211, DOI 10.1145/2318857.2254783; Rahimi A., 2009, NEURAL INFORM PROCES; Shorack G.R., 1986, EMPIRICAL PROCESSES; Song L., 2009, ADV NEURAL INFORM PR, P1732; VandeGeer S, 1995, ANN STAT, V23, P1779, DOI 10.1214/aos/1176324323; VANDEGEER S, 1993, ANN STAT, V21, P14, DOI 10.1214/aos/1176349013; WONG WH, 1995, ANN STAT, V23, P339, DOI 10.1214/aos/1176324524	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR	25960624				2022-12-19	WOS:000452647101037
C	Felzenszwalb, PF; Oberlin, JG		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Felzenszwalb, Pedro F.; Oberlin, John G.			Multiscale Fields of Patterns	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ENERGY MINIMIZATION; COMPLETION; MODELS	We describe a framework for defining high-order image models that can be used in a variety of applications. The approach involves modeling local patterns in a multiscale representation of an image. Local properties of a coarsened image reflect non-local properties of the original image. In the case of binary images local properties are defined by the binary patterns observed over small neighborhoods around each pixel. With the multiscale representation we capture the frequency of patterns observed at different scales of resolution. This framework leads to expressive priors that depend on a relatively small number of parameters. For inference and learning we use an MCMC method for block sampling with very large blocks. We evaluate the approach with two example applications. One involves contour detection. The other involves binary segmentation.	[Felzenszwalb, Pedro F.; Oberlin, John G.] Brown Univ, Providence, RI 02906 USA	Brown University	Felzenszwalb, PF (corresponding author), Brown Univ, Providence, RI 02906 USA.	pff@brown.edu; john_oberlin@brown.edu			National Science Foundation [1161282]	National Science Foundation(National Science Foundation (NSF))	We would like to thank Alexandra Shapiro for helpful discussions and initial experiments related to this project. This material is based upon work supported by the National Science Foundation under Grant No. 1161282.	Alpert S., 2010, ECCV; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; August J, 2003, IEEE T PATTERN ANAL, V25, P387, DOI 10.1109/TPAMI.2003.1190567; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Descombes X., 1995, SCIA; Eslami S. M. A., 2012, CVPR; Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4; FELZENSZWALB PF, 2007, CVPR; Leung T., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P544, DOI 10.1007/BFb0055689; Mahamud S, 2003, IEEE T PATTERN ANAL, V25, P433, DOI 10.1109/TPAMI.2003.1190570; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Neal R. M., 1993, PROBABILISTIC INFERE; Ren XF, 2008, INT J COMPUT VISION, V77, P47, DOI 10.1007/s11263-007-0092-6; Rizve M., ABS06329 ARXIV, V2021; Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6; Shekhovtsov A., 2012, DAGM; Takhanov Rustem, 2013, ICML; Tieleman T., 2008, ICML; Ullman S., 1988, P 2 INT C COMP VIS, P321, DOI DOI 10.1109/CCV.1988.590008; Williams LR, 1997, NEURAL COMPUT, V9, P859, DOI 10.1162/neco.1997.9.4.859; Williams LR, 1997, NEURAL COMPUT, V9, P837, DOI 10.1162/neco.1997.9.4.837; Willsky AS, 2002, P IEEE, V90, P1396, DOI 10.1109/JPROC.2002.800717; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100062
C	Feng, JS; Xu, H; Mannor, S; Yan, SC		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Feng, Jiashi; Xu, Huan; Mannor, Shie; Yan, Shuicheng			Robust Logistic Regression and Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				MODELS	We consider logistic regression with arbitrary outliers in the covariate matrix. We propose a new robust logistic regression algorithm, called RoLR, that estimates the parameter through a simple linear programming procedure. We prove that RoLR is robust to a constant fraction of adversarial outliers. To the best of our knowledge, this is the first result on estimating logistic regression model when the covariate matrix is corrupted with any performance guarantees. Besides regression, we apply RoLR to solving binary classification problems where a fraction of training samples are corrupted.	[Feng, Jiashi] Univ Calif Berkeley, EECS Dept, Berkeley, CA 94720 USA; [Feng, Jiashi] Univ Calif Berkeley, ICSI, Berkeley, CA 94720 USA; [Xu, Huan] Natl Univ Singapore, ME Dept, Singapore, Singapore; [Mannor, Shie] Technion, EE Dept, Haifa, Israel; [Yan, Shuicheng] Natl Univ Singapore, ECE Dept, Singapore, Singapore	University of California System; University of California Berkeley; University of California System; University of California Berkeley; National University of Singapore; Technion Israel Institute of Technology; National University of Singapore	Feng, JS (corresponding author), Univ Calif Berkeley, EECS Dept, Berkeley, CA 94720 USA.	jshfeng@berkeley.edu; mpexuh@nus.edu.sg; shie@ee.technion.ac.il; eleyans@nus.edu.sg	Feng, Jiashi/AGX-6209-2022; Yan, Shuicheng/HCI-1431-2022		Ministry of Education of Singapore through AcRF Tier Two grant [R-265-000-443-112]; Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI); Israel Science Foundation (ISF) [920/12]	Ministry of Education of Singapore through AcRF Tier Two grant(Ministry of Education, Singapore); Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI); Israel Science Foundation (ISF)(Israel Science Foundation)	The work of H. Xu was partially supported by the Ministry of Education of Singapore through AcRF Tier Two grant R-265-000-443-112. The work of S. Mannor was partially funded by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) and by the Israel Science Foundation (ISF under contract 920/12).	Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bianco AM., 1996, ROBUST ESTIMATION LO; Chen Y., 2013, ICML; Cook RD., 1982, RESIDUALS INFLUENCE; COPAS JB, 1988, J R STAT SOC B, V50, P225; Ding N., 2013, J MACHINE LEARNING R, V5, P1; HAMPEL FR, 1974, J AM STAT ASSOC, V69, P383, DOI 10.2307/2285666; Huber P. J., 2011, ROBUST STAT; JOHNSON W, 1985, BIOMETRIKA, V72, P59, DOI 10.2307/2336335; KUNSCH HR, 1989, J AM STAT ASSOC, V84, P460, DOI 10.2307/2289930; Lee Su- In, 2006, AAAI; Loh PL, 2012, ANN STAT, V40, P1637, DOI 10.1214/12-AOS1018; Plan Y, 2013, IEEE T INFORM THEORY, V59, P482, DOI 10.1109/TIT.2012.2207945; PREGIBON D, 1981, ANN STAT, V9, P705, DOI 10.1214/aos/1176345513; PREGIBON D, 1982, BIOMETRICS, V38, P485, DOI 10.2307/2530463; STEFANSKI LA, 1986, BIOMETRIKA, V73, P413; Tibshirani Julie, 2013, ARXIV13054987; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025	18	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102076
C	Fevotte, C; Kowalski, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Fevotte, Cedric; Kowalski, Matthieu			Low-Rank Time-Frequency Synthesis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				NONNEGATIVE MATRIX FACTORIZATION	Many single-channel signal decomposition techniques rely on a low-rank factorization of a time-frequency transform. In particular, nonnegative matrix factorization (NMF) of the spectrogram - the (power) magnitude of the short-time Fourier transform (STFT) - has been considered in many audio applications. In this setting, NMF with the Itakura-Saito divergence was shown to underly a generative Gaussian composite model (GCM) of the STFT, a step forward from more empirical approaches based on ad-hoc transform and divergence specifications. Still, the GCM is not yet a generative model of the raw signal itself, but only of its STFT. The work presented in this paper fills in this ultimate gap by proposing a novel signal synthesis model with low-rank time-frequency structure. In particular, our new approach opens doors to multi-resolution representations, that were not possible in the traditional NMF setting. We describe two expectation-maximization algorithms for estimation in the new model and report audio signal processing results with music decomposition and speech enhancement.	[Fevotte, Cedric] OCA, CNRS, Lab Lagrange, Nice, France; [Fevotte, Cedric] Univ Nice, Nice, France; [Kowalski, Matthieu] Supelec, CNRS, Lab Signaux & Syst, Gif Sur Yvette, France; [Kowalski, Matthieu] Univ Paris Sud, Gif Sur Yvette, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Cote d'Azur; Observatoire de la Cote d'Azur; UDICE-French Research Universities; Universite Cote d'Azur; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay; UDICE-French Research Universities; Universite Paris Saclay	Fevotte, C (corresponding author), OCA, CNRS, Lab Lagrange, Nice, France.	cfevotte@unice.fr; kowalski@lss.supelec.fr						Cohen I, 2003, IEEE T SPEECH AUDI P, V11, P466, DOI 10.1109/TSA.2003.811544; DAPRA, 2021, DAPRA TIMIT AC PHON; Daudet L, 2002, SIGNAL PROCESS, V82, P1595, DOI 10.1016/S0165-1684(02)00304-3; Elad M, 2005, APPL COMPUT HARMON A, V19, P340, DOI 10.1016/j.acha.2005.03.005; Elad M, 2006, IEEE T INFORM THEORY, V52, P5559, DOI 10.1109/TIT.2006.885522; Fevotte C, 2009, NEURAL COMPUT, V21, P793, DOI 10.1162/neco.2008.04-08-771; Figueiredo MAT, 2003, IEEE T IMAGE PROCESS, V12, P906, DOI 10.1109/TIP.2003.814255; Hale ET, 2008, SIAM J OPTIMIZ, V19, P1107, DOI 10.1137/070698920; Kowalski M, 2009, SIGNAL IMAGE VIDEO P, V3, P251, DOI 10.1007/s11760-008-0076-1; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Ozerov A, 2010, IEEE T AUDIO SPEECH, V18, P550, DOI 10.1109/TASL.2009.2031510; Prusa Z., 2013, P 10 INT S COMPUTER, P299; Seeger MW, 2008, J MACH LEARN RES, V9, P759; Smaragdis P., 2007, P 7 INT C IND COMP A; Smaragdis P, 2014, IEEE SIGNAL PROC MAG, V31, P66, DOI 10.1109/MSP.2013.2297715; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Wipf DP, 2004, IEEE T SIGNAL PROCES, V52, P2153, DOI 10.1109/TSP.2004.831016	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102083
C	Fletcher, AK; Rangan, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Fletcher, Alyson K.; Rangan, Sundeep			Scalable Inference for Neuronal Connectivity from Calcium Imaging	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Fluorescent calcium imaging provides a potentially powerful tool for inferring connectivity in neural circuits with up to thousands of neurons. However, a key challenge in using calcium imaging for connectivity detection is that current systems often have a temporal response and frame rate that can be orders of magnitude slower than the underlying neural spiking process. Bayesian inference methods based on expectation-maximization (EM) have been proposed to overcome these limitations, but are often computationally demanding since the E-step in the EM procedure typically involves state estimation for a high-dimensional nonlinear dynamical system. In this work, we propose a computationally fast method for the state estimation based on a hybrid of loopy belief propagation and approximate message passing (AMP). The key insight is that a neural system as viewed through calcium imaging can be factorized into simple scalar dynamical systems for each neuron with linear interconnections between the neurons. Using the structure, the updates in the proposed hybrid AMP methodology can be computed by a set of one-dimensional state estimation procedures and linear transforms with the connectivity matrix. This yields a computationally scalable method for inferring connectivity of large neural circuits. Simulations of the method on realistic neural networks demonstrate good accuracy with computation times that are potentially significantly faster than current approaches based on Markov Chain Monte Carlo methods.					Fletcher, Alyson K/C-3226-2015; Rangan, Sundeep/AAH-2526-2020	Fletcher, Alyson K/0000-0002-3756-6580; 				Bishop C.M., 2014, ANTIMICROB AGENTS CH, V58, P7250; Dayan P, 2001, THEORETICAL NEUROSCI; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038; Doucet A., 2009, HDB NONLINEAR FILTER, V12, P656; Fletcher A. K., 2014, ARXIV14090289; Fletcher A. K., 2011, P NEUR INF PROC SYST; Kamilov U. S., 2012, P NIPS; Minka T., 2001, THESIS MIT CAMBRIDGE; Mishchenko Y, 2011, ANN APPL STAT, V5, P1229, DOI 10.1214/09-AOAS303; Ohki K, 2005, NATURE, V433, P597, DOI 10.1038/nature03274; Rangan S., 2012, P IEEE INT S INF THE, P1241; SAYER RJ, 1990, J NEUROSCI, V10, P826; Soriano J, 2008, P NATL ACAD SCI USA, V105, P13758, DOI 10.1073/pnas.0707492105; Stetter O, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002653; Stosiek C, 2003, P NATL ACAD SCI USA, V100, P7319, DOI 10.1073/pnas.1232232100; Svoboda K, 2006, NEURON, V50, P823, DOI 10.1016/j.neuron.2006.05.019; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; TSIEN RY, 1989, ANNU REV NEUROSCI, V12, P227, DOI 10.1146/annurev.ne.12.030189.001303; Vogelstein JT, 2009, BIOPHYS J, V97, P636, DOI 10.1016/j.bpj.2008.08.005; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Yizhar O, 2011, NEURON, V71, P9, DOI 10.1016/j.neuron.2011.06.004	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101012
C	Garreau, D; Lajugie, R; Arlot, S; Bach, F		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Garreau, Damien; Lajugie, Remi; Arlot, Sylvain; Bach, Francis			Metric Learning for Temporal Sequence Alignment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				TIME-SERIES; ALGORITHM	In this paper, we propose to learn a Mahalanobis distance to perform alignment of multivariate time series. The learning examples for this task are time series for which the true alignment is known. We cast the alignment problem as a structured prediction task, and propose realistic losses between alignments for which the optimization is tractable. We provide experiments on real data in the audio-to-audio context, where we show that the learning of a similarity measure leads to improvements in the performance of the alignment task. We also propose to use this metric learning framework to perform feature selection and, from basic audio features, build a combination of these with better alignment performance.	[Garreau, Damien] ENS, Paris, France; [Lajugie, Remi; Bach, Francis] INRIA, Rocquencourt, France; [Arlot, Sylvain] CNRS, Paris, France; [Garreau, Damien; Lajugie, Remi; Arlot, Sylvain; Bach, Francis] Ecole Normale Super, CNRS INRIA ENS, Dept Informat, SIERRA Project Team, Paris, France	UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Inria; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Cite; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Garreau, D (corresponding author), ENS, Paris, France.	damien.garreau@ens.fr; remi.lajugie@inria.fr; sylvain.arlot@ens.fr; francis.bach@inria.fr			European Research Council (SIERRA project) [239993]; GARGANTUA project - Mastodons program of CNRS; Airbus foundation	European Research Council (SIERRA project); GARGANTUA project - Mastodons program of CNRS; Airbus foundation	The authors acknowledge the support of the European Research Council (SIERRA project 239993), the GARGANTUA project funded by the Mastodons program of CNRS and the Airbus foundation through a PhD fellowship. Thanks to Piotr Bojanowski, for helpful discussions. Warm thanks go to Arshia Cont and Philippe Cuvillier for sharing their knowledge about audio processing, and to Holger Kirchhoff and Alexander Lerch for their dataset.	Aach J, 2001, BIOINFORMATICS, V17, P495, DOI 10.1093/bioinformatics/17.6.495; Banderier C, 2005, J STAT PLAN INFER, V135, P40, DOI 10.1016/j.jspi.2005.02.004; Caetano TS, 2009, IEEE T PATTERN ANAL, V31, P1048, DOI 10.1109/TPAMI.2009.28; Cont A., 2007, P ISMIR; Cuturi M, 2007, INT CONF ACOUST SPEE, P413; DIXON S, 2005, P INT C MUS INF RETR, P492; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Gold B., 2011, SPEECH AUDIO SIGNAL, DOI 10.1002/9781118142882; Hamming R. W., 1950, BELL SYSTEM TECHNICA, V29; Hu N., 2003, POLYPHONIC AUDIO MAT, P521; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Joder C, 2013, IEEE T AUDIO SPEECH, V21, P2118, DOI 10.1109/TASL.2013.2266794; Keshet J, 2007, IEEE T AUDIO SPEECH, V15, P2373, DOI 10.1109/TASL.2007.903928; Kirchhoff H, 2011, J NEW MUSIC RES, V40, P27, DOI 10.1080/09298215.2010.529917; Lacoste-Julien S., 2013, P ICML; Lajugie R., 2014, P ICML; McFee B., 2010, P 27 INT C MACHINE L, P775; Muller M., 2007, INFORM RETRIEVAL MUS, V6, P9; SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Szummer M., 2008, P CVPR; Taskar B., 2003, ADV NIPS; Thompson JD, 1999, BIOINFORMATICS, V15, P87, DOI 10.1093/bioinformatics/15.1.87; Torres A, 2003, DNA SEQUENCE, V14, P427, DOI 10.1080/10425170310001617894; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101054
C	Ghoshdastidar, D; Dukkipati, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Ghoshdastidar, Debarghya; Dukkipati, Ambedkar			Consistency of Spectral Partitioning of Uniform Hypergraphs under Planted Partition Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Spectral graph partitioning methods have received significant attention from both practitioners and theorists in computer science. Some notable studies have been carried out regarding the behavior of these methods for infinitely large sample size (von Luxburg et al., 2008; Rohe et al., 2011), which provide sufficient confidence to practitioners about the effectiveness of these methods. On the other hand, recent developments in computer vision have led to a plethora of applications, where the model deals with multi-way affinity relations and can be posed as uniform hyper-graphs. In this paper, we view these models as random m-uniform hyper-graphs and establish the consistency of spectral algorithm in this general setting. We develop a planted partition model or stochastic blockmodel for such problems using higher order tensors, present a spectral technique suited for the purpose and study its large sample behavior. The analysis reveals that the algorithm is consistent for m-uniform hypergraphs for larger values of m, and also the rate of convergence improves for increasing m. Our result provides the first theoretical evidence that establishes the importance of m-way affinities.	[Ghoshdastidar, Debarghya; Dukkipati, Ambedkar] Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, Karnataka, India	Indian Institute of Science (IISC) - Bangalore	Ghoshdastidar, D (corresponding author), Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, Karnataka, India.	debarghya.g@csa.iisc.ernet.in; ad@csa.iisc.ernet.in			Google Ph.D. Fellowship in Statistical Learning Theory	Google Ph.D. Fellowship in Statistical Learning Theory	D. Ghoshdastidar is supported by Google Ph.D. Fellowship in Statistical Learning Theory.	Agarwal S., 2006, ICML, P17, DOI DOI 10.1145/1143844.1143847; Anandkumar A., 2013, C LEARN THEOR; [Anonymous], 2002, LEARNING KERNELS; BOLLA M, 1993, DISCRETE MATH, V117, P19, DOI 10.1016/0012-365X(93)90322-K; Bulo SR, 2013, IEEE T PATTERN ANAL, V35, P1312, DOI 10.1109/TPAMI.2012.226; Chen GL, 2009, FOUND COMPUT MATH, V9, P517, DOI 10.1007/s10208-009-9043-7; Chertok M, 2010, IEEE T PATTERN ANAL, V32, P2205, DOI 10.1109/TPAMI.2010.51; Chung FR, 1997, SPECTRAL GRAPH THEOR, V92; Cooper J, 2012, LINEAR ALGEBRA APPL, V436, P3268, DOI 10.1016/j.laa.2011.11.018; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696; Frieze A., 2008, IARCS ANN C FDN SOFT, V2, P187; Govindu VM, 2005, PROC CVPR IEEE, P1150; Jain SD, 2013, IEEE I CONF COMP VIS, P1313, DOI 10.1109/ICCV.2013.166; Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850; Liu H., 2010, ADV NEURAL INFORM PR, P1414; McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929; Ng AY, 2002, ADV NEUR IN, V14, P849; Rodriguez JA, 2009, APPL MATH LETT, V22, P916, DOI 10.1016/j.aml.2008.07.020; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; Schweikert G., 1979, P 9 DES AUT WORKSH, P57; Selvakkumaran N, 2006, IEEE T COMPUT AID D, V25, P504, DOI 10.1109/TCAD.2005.854637; Stewart G., 1990, MATRIX PERTURBATION; Tron Roberto, 2007, IEEE C COMP VIS PATT; von Luxburg U, 2008, ANN STAT, V36, P555, DOI 10.1214/009053607000000640; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100093
C	Glazer, A; Weissbrod, O; Lindenbaum, M; Markovitch, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Glazer, Assaf; Weissbrod, Omer; Lindenbaum, Michael; Markovitch, Shaul			Approximating Hierarchical MV-sets for Hierarchical Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				DENSITY; SUPPORT	The goal of hierarchical clustering is to construct a cluster tree, which can be viewed as the modal structure of a density. For this purpose, we use a convex optimization program that can efficiently estimate a family of hierarchical dense sets in high-dimensional distributions. We further extend existing graph-based methods to approximate the cluster tree of a distribution. By avoiding direct density estimation, our method is able to handle high-dimensional data more efficiently than existing density-based approaches. We present empirical results that demonstrate the superiority of our method over existing ones.	[Glazer, Assaf; Weissbrod, Omer; Lindenbaum, Michael; Markovitch, Shaul] Technion Israel Inst Technol, Dept Comp Sci, Haifa, Israel	Technion Israel Institute of Technology	Glazer, A (corresponding author), Technion Israel Inst Technol, Dept Comp Sci, Haifa, Israel.	assafgr@cs.technion.ac.il; omerw@cs.technion.ac.il; mic@cs.technion.ac.il; shaulm@cs.technion.ac.il						Altshuler DM, 2012, NATURE, V491, P56, DOI 10.1038/nature11632; Ankerst M, 1999, SIGMOD RECORD, VOL 28, NO 2 - JUNE 1999, P49; Ben-Hur A, 2002, J MACH LEARN RES, V2, P125, DOI 10.1162/15324430260185565; Biau G., 2007, ESAIM-PROBAB STAT, V11, P272; Carlsson G, 2010, STUD CLASS DATA ANAL, P63, DOI 10.1007/978-3-642-10745-0_6; Carlsson G, 2010, J MACH LEARN RES, V11, P1425; Cole A. J., 1969, NUMERICAL TAXONOMY, P282; Cuevas A, 2001, COMPUT STAT DATA AN, V36, P441, DOI 10.1016/S0167-9473(00)00052-9; Dasgupta S., 2008, P 25 INT C MACH LEAR, P208, DOI DOI 10.1145/1390156.1390183; Dasgupta S, 2011, THEOR COMPUT SCI, V412, P1767, DOI 10.1016/j.tcs.2010.12.054; Ester M., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P226; Forina M., 1983, FOOD RES DATA ANAL, V1983, P189; Glazer A., 2013, ADV NEURAL INFORM PR, P503; Gronau I, 2011, NAT GENET, V43, P1031, DOI 10.1038/ng.937; Hartigan J.A., 1975, CLUSTERING ALGORITHM; Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011; Koller D., 1997, ICML, V97, P170; Larsen, 1999, P 5 ACM SIGKDD INT C, P16, DOI [DOI 10.1145/312129.312186, 10.1145/312129.312186]; Martinez-Perez Alvaro, 2012, ARXIV12106292; Rigollet P, 2009, BERNOULLI, V15, P1154, DOI 10.3150/09-BEJ184; Rinaldo A, 2012, J MACH LEARN RES, V13, P905; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; Steinbach M., 2000, COMP DOCUMENT CLUSTE; Stuetzle W, 2003, J CLASSIF, V20, P25, DOI 10.1007/s00357-003-0004-6; Stuetzle Werner, 2010, J COMPUTATIONAL GRAP, V19; Urner Ruth, 2013, COLT, P1; Walther G, 1997, ANN STAT, V25, P2273; Xu R, 2005, IEEE T NEURAL NETWOR, V16, P645, DOI 10.1109/TNN.2005.845141	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103033
C	Gottlieb, LA; Kontorovich, A; Nisnevitch, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Gottlieb, Lee-Ad; Kontorovich, Aryeh; Nisnevitch, Pinhas			Near-optimal sample compression for nearest neighbors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				LEARNING ALGORITHMS; CONSISTENCY	We present the first sample compression algorithm for nearest neighbors with nontrivial performance guarantees. We complement these guarantees by demonstrating almost matching hardness lower bounds, which show that our bound is nearly optimal. Our result yields new insight into margin-based nearest neighbor classification in metric spaces and allows us to significantly sharpen and simplify existing bounds. Some encouraging empirical results are also presented.	[Gottlieb, Lee-Ad; Nisnevitch, Pinhas] Ariel Univ, Dept Comp Sci & Math, Ariel, Israel; [Kontorovich, Aryeh] Ben Gurion Univ Negev, Comp Sci Dept, Beer Sheva, Israel	Ariel University; Ben Gurion University	Gottlieb, LA (corresponding author), Ariel Univ, Dept Comp Sci & Math, Ariel, Israel.	leead@ariel.ac.il; karyeh@cs.bgu.ac.il; pinhasn@gmail.com	Kontorovich, Aryeh/AAB-4744-2020; Kontorovich, Aryeh/X-9225-2019	Kontorovich, Aryeh/0000-0001-8038-8671; 				Angiulli F., 2005, ICML; Arora S., 1993, FOCS; Bartlett P, 1999, ADVANCES IN KERNEL METHODS, P43; Beygelzimer A., 2006, ICML; Bshouty NH, 2009, J COMPUT SYST SCI, V75, P323, DOI 10.1016/j.jcss.2009.01.003; Chaudhuri Kamalika, 2014, NIPS; Clarkson K. L., 1994, SCG; Cole R., 2006, STOC; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; DEVROYE L, 1994, ANN STAT, V22, P1371, DOI 10.1214/aos/1176325633; Dinur I., 2004, INFO PROC LETT; FIX E, 1989, INT STAT REV, V57, P238, DOI 10.2307/1403797; GATES GW, 1972, IEEE T INFORM THEORY, V18, P431, DOI 10.1109/TIT.1972.1054809; Gottlieb L., 2010, COLT; Gottlieb L, 2013, ALT; Gottlieb L, 2013, SIMBAD; Gottlieb LA, 2013, SIAM J DISCRETE MATH, V27, P1759, DOI 10.1137/120874242; Graepel T, 2005, MACH LEARN, V59, P55, DOI 10.1007/s10994-005-0462-7; GUPTA A, 2003, FOCS; Har-Peled S, 2006, SIAM J COMPUT, V35, P1148, DOI 10.1137/S0097539704446281; HART PE, 1968, IEEE T INFORM THEORY, V14, P515, DOI 10.1109/TIT.1968.1054155; HAUSSLER D, 1988, ARTIF INTELL, V36, P177, DOI 10.1016/0004-3702(88)90002-1; Kontorovich A., 2014, ARXIV14070208; Krauthgamer R, 2004, SODA; Laviolette F, 2010, MACH LEARN, V78, P175, DOI 10.1007/s10994-009-5137-3; Li Y., 2006, NIPS; Littlestone N., 1986, RELATING DATA UNPUB; Marchand M, 2003, J MACH LEARN RES, V3, P723, DOI 10.1162/jmlr.2003.3.4-5.723; Mohri M., 2018, FDN MACHINE LEARNING; RITTER GL, 1975, IEEE T INFORM THEORY, V21, P665, DOI 10.1109/TIT.1975.1055464; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Snapp RR, 1998, ANN STAT, V26, P850; Toussaint G, 2002, LECT NOTES COMPUT SC, V2866, P273; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; von Luxburg U., 2004, JMLR; Wilfong G., 1991, SCG; Wilson DR, 2000, MACH LEARN, V38, P257, DOI 10.1023/A:1007626913721; Zukhba A. V., 2010, Pattern Recognition and Image Analysis, V20, P484, DOI 10.1134/S1054661810040097	40	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102089
C	Grabska-Barwinska, A; Pillow, JW		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Grabska-Barwinska, Agnieszka; Pillow, Jonathan W.			Optimal prior-dependent neural population codes under shared input noise	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				BAYESIAN-INFERENCE; FISHER INFORMATION; ORIENTATION	The brain uses population codes to form distributed, noise-tolerant representations of sensory and motor variables. Recent work has examined the theoretical optimality of such codes in order to gain insight into the principles governing population codes found in the brain. However, the majority of the population coding literature considers either conditionally independent neurons or neurons with noise governed by a stimulus-independent covariance matrix. Here we analyze population coding under a simple alternative model in which latent "input noise" corrupts the stimulus before it is encoded by the population. This provides a convenient and tractable description for irreducible uncertainty that cannot be overcome by adding neurons, and induces stimulus-dependent correlations that mimic certain aspects of the correlations observed in real populations. We examine prior-dependent, Bayesian optimal coding in such populations using exact analyses of cases in which the posterior is approximately Gaussian. These analyses extend previous results on independent Poisson population codes and yield an analytic expression for squared loss and a tight upper bound for mutual information. We show that, for homogeneous populations that tile the input domain, optimal tuning curve width depends on the prior, the loss function, the resource constraint, and the amount of input noise. This framework provides a practical testbed for examining issues of optimality, noise, correlation, and coding fidelity in realistic neural populations.	[Grabska-Barwinska, Agnieszka] UCL, Gatsby Computat Neurosci Unit, London, England; [Pillow, Jonathan W.] Princeton Univ, Dept Psychol, Princeton Neurosci Inst, Princeton, NJ 08544 USA	University of London; University College London; Princeton University	Grabska-Barwinska, A (corresponding author), UCL, Gatsby Computat Neurosci Unit, London, England.	agnieszka@gatsby.ucl.ac.uk; pillow@princeton.edu			McKnight Foundation; NSF CAREER Award [IIS-1150186]; NIMH [MH099611]; Gatsby Charitable Foundation	McKnight Foundation; NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NIMH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH)); Gatsby Charitable Foundation	This work was supported by the McKnight Foundation (JP), NSF CAREER Award IIS-1150186 (JP), NIMH grant MH099611 (JP) and the Gatsby Charitable Foundation (AGB).	Beck JM, 2011, J NEUROSCI, V31, P15310, DOI 10.1523/JNEUROSCI.1706-11.2011; Berens P, 2011, P NATL ACAD SCI USA, V108, P4423, DOI 10.1073/pnas.1015904108; Bethge M, 2002, NEURAL COMPUT, V14, P2317, DOI 10.1162/08997660260293247; Brunel N, 1998, NEURAL COMPUT, V10, P1731, DOI 10.1162/089976698300017115; BURR DC, 1991, VISION RES, V31, P1449, DOI 10.1016/0042-6989(91)90064-C; Dehaene G., 2013, COSYNE; DEVALOIS RL, 1982, VISION RES, V22, P531, DOI 10.1016/0042-6989(82)90112-2; Ganguli D., 2010, ADV NEURAL INFORM PR, V23; Ganguli D, 2014, NEURAL COMPUT, V26, P2103, DOI 10.1162/NECO_a_00638; Haefner R., 2010, NEURAL INFORM PROCES; Josic K, 2009, NEURAL COMPUT, V21, P2774, DOI 10.1162/neco.2009.10-08-879; Ma WJ, 2006, NAT NEUROSCI, V9, P1432, DOI 10.1038/nn1790; Macke JH, 2011, PHYS REV LETT, V106, DOI 10.1103/PhysRevLett.106.208102; Miura K, 2012, NEURON, V74, P1087, DOI 10.1016/j.neuron.2012.04.021; Montemurro MA, 2006, NEURAL COMPUT, V18, P1555, DOI 10.1162/neco.2006.18.7.1555; Moreno-Bote R, 2014, NAT NEUROSCI, V17, P1410, DOI 10.1038/nn.3807; Pouget A, 2001, VISUAL ATTENTION AND CORTICAL CIRCUITS, P265; Pouget A, 1999, NEURAL COMPUT, V11, P85, DOI 10.1162/089976699300016818; Series P, 2009, NEURAL COMPUT, V21, P3271, DOI 10.1162/neco.2009.09-08-869; SEUNG HS, 1993, P NATL ACAD SCI USA, V90, P10749, DOI 10.1073/pnas.90.22.10749; Wang Z., 2012, ADV NEURAL INF PROCE, V25, P2177; Wei X.-X., 2012, ADV NEURAL INFORM PR, V25, P1313; Yaeli S, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00130; Yarrow S, 2012, NEURAL COMPUT, V24, P1740, DOI 10.1162/NECO_a_00292; Zemel RS, 1998, NEURAL COMPUT, V10, P403, DOI 10.1162/089976698300017818; Zhang KC, 1999, NEURAL COMPUT, V11, P75, DOI 10.1162/089976699300016809; ZOHARY E, 1994, NATURE, V370, P140, DOI 10.1038/370140a0	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101060
C	Grinberg, Y; Precup, D; Gendreau, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Grinberg, Yuri; Precup, Doina; Gendreau, Michel			Optimizing Energy Production Using Policy Search and Predictive State Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We consider the challenging practical problem of optimizing the power production of a complex of hydroelectric power plants, which involves control over three continuous action variables, uncertainty in the amount of water inflows and a variety of constraints that need to be satisfied. We propose a policy-search-based approach coupled with predictive modelling to address this problem. This approach has some key advantages compared to other alternatives, such as dynamic programming: the policy representation and search algorithm can conveniently incorporate domain knowledge; the resulting policies are easy to interpret, and the algorithm is naturally parallelizable. Our algorithm obtains a policy which outperforms the solution found by dynamic programming both quantitatively and qualitatively.	[Grinberg, Yuri; Precup, Doina] McGill Univ, Sch Comp Sci, Montreal, PQ, Canada; [Gendreau, Michel] Ecole Polytech Montreal, NSERC Hydro Quebec Ind Res Chair Stochast Optimiz, CIRRELT, Montreal, PQ, Canada; [Gendreau, Michel] Ecole Polytech Montreal, Dept Math & Genie Ind, Montreal, PQ, Canada	McGill University; Universite de Montreal; Polytechnique Montreal; Universite de Montreal; Polytechnique Montreal	Grinberg, Y (corresponding author), McGill Univ, Sch Comp Sci, Montreal, PQ, Canada.	ygrinb@cs.mcgill.ca; dprecup@cs.mcgill.ca; michel.gendreau@cirrelt.ca	Gendreau, Michel/HDO-6155-2022; Gendreau, Michel/N-7950-2019	Gendreau, Michel/0000-0002-9262-3648	NSERC/Hydro-Quebec Industrial Research Chair on the Stochastic Optimization of Electricity Generation; NSERC Discovery Program	NSERC/Hydro-Quebec Industrial Research Chair on the Stochastic Optimization of Electricity Generation; NSERC Discovery Program(Natural Sciences and Engineering Research Council of Canada (NSERC))	We thank Gregory Emiel and Laura Fagherazzi of Hydro-Quebec for many helpful discussions and for providing access to the simulator and their DP results, and Kamran Nagiyev for porting an initial version of the simulator to Java. This research was supported by the NSERC/Hydro-Quebec Industrial Research Chair on the Stochastic Optimization of Electricity Generation, and by the NSERC Discovery Program.	Banos R, 2011, RENEW SUST ENERG REV, V15, P1753, DOI 10.1016/j.rser.2010.12.008; Bellman R. E., 1954, DYNAMIC PROGRAMMING; Boots B., 2010, P ROB SCI SYST 6; Breton M, 2002, AUTOMATICA, V38, P477, DOI 10.1016/S0005-1098(01)00225-4; Carpentier PL, 2013, WATER RESOUR RES, V49, P2812, DOI 10.1002/wrcr.20254; Deisenroth M. P., 2013, FDN TRENDS ROBOTICS, P388; Fortin P, 2008, IEEE POWER ENERGY M, V6, P40, DOI 10.1109/MPE.2008.924811; Gosavi A., 2003, OPERAT RES COMP SCI, V25; Labadie JW, 2004, J WATER RES PLAN MAN, V130, P93, DOI 10.1061/(ASCE)0733-9496(2004)130:2(93); Littman M., 2002, ADV NEURAL INFORM PR; Loucks D.P., 1981, WATER RESOURCES SYST; Marco J. B., 1993, STOCHASTIC HYDROLOGY, V237; Ong S., 2013, P 27 AAAI C ART INT; Rani D, 2010, WATER RESOUR MANAG, V24, P1107, DOI 10.1007/s11269-009-9488-0; Salas J. D., 1980, APPL MODELING HYDROL; Singh S., 2004, P 20 C UNC ART INT; Sveinsson O. G. B., 2007, STOCHASTIC ANALISYS; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102007
C	Gu, QQ; Gui, H; Han, JW		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Gu, Quanquan; Gui, Huan; Han, Jiawei			Robust Tensor Decomposition with Gross Corruption	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				MATRIX DECOMPOSITION; COMPLETION	In this paper, we study the statistical performance of robust tensor decomposition with gross corruption. The observations are noisy realization of the superposition of a low-rank tensor W* and an entrywise sparse corruption tensor V*. Unlike conventional noise with bounded variance in previous convex tensor decomposition analysis, the magnitude of the gross corruption can be arbitrary large. We show that under certain conditions, the true low-rank tensor as well as the sparse corruption tensor can be recovered simultaneously. Our theory yields nonasymptotic Frobenius-norm estimation error bounds for each tensor separately. We show through numerical experiments that our theory can precisely predict the scaling behavior in practice.	[Gu, Quanquan] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA; [Gui, Huan; Han, Jiawei] Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA	Princeton University; University of Illinois System; University of Illinois Urbana-Champaign	Gu, QQ (corresponding author), Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.	qgu@princeton.edu; huangui2@illinois.edu; hanj@illinois.edu			Army Research Lab [W911NF-09-2-0053]; Army Research Office [W911NF-13-1-0193]; National Science Foundation [IIS-1017362, IIS-1320617, IIS-1354329, HDTRA1-10-1-0120]; MIAS; DHS-IDS Center for Multimodal Information Access and Synthesis at UIUC	Army Research Lab(United States Department of DefenseUS Army Research Laboratory (ARL)); Army Research Office; National Science Foundation(National Science Foundation (NSF)); MIAS; DHS-IDS Center for Multimodal Information Access and Synthesis at UIUC	We would like to thank the anonymous reviewers for their helpful comments. Research was sponsored in part by the Army Research Lab, under Cooperative Agreement No. W911NF-09-2-0053 (NSCTA), the Army Research Office under Cooperative Agreement No. W911NF-13-1-0193, National Science Foundation IIS-1017362, IIS-1320617, and IIS-1354329, HDTRA1-10-1-0120, and MIAS, a DHS-IDS Center for Multimodal Information Access and Synthesis at UIUC.	Alex M, 2002, LECT NOTES COMPUT SC, V2350, P447; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1324, DOI 10.1137/S0895479898346995; Gong Pinghua, 2012, KDD, V2012, P895; Hale ET, 2008, SIAM J OPTIMIZ, V19, P1107, DOI 10.1137/070698920; Hsu D, 2011, IEEE T INFORM THEORY, V57, P7221, DOI 10.1109/TIT.2011.2158250; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39; Mu C., 2013, CORR; Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; SREBRO N, 2005, COLT, V3559, P545; Tomioka R., 2010, ESTIMATION LOW RANK; Yang E., 2013, ADV NEURAL INF PROCE, V26, P611	25	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101080
C	Gu, Q; Wang, Z; Liu, H		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Gu, Quanquan; Wang, Zhaoran; Liu, Han			Sparse PCA with Oracle Property	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				PRINCIPAL COMPONENT ANALYSIS; VARIABLE SELECTION; POWER METHOD; CONSISTENCY; RATES	In this paper, we study the estimation of the k-dimensional sparse principal sub-space of covariance matrix Sigma in the high-dimensional setting. We aim to recover the oracle principal subspace solution, i.e., the principal subspace estimator obtained assuming the true support is known a priori. To this end, we propose a family of estimators based on the semidefinite relaxation of sparse PCA with novel regularizations. In particular, under a weak assumption on the magnitude of the population projection matrix, one estimator within this family exactly recovers the true support with high probability, has exact rank-k, and attains a root s/n statistical rate of convergence with s being the subspace sparsity level and n the sample size. Compared to existing support recovery results for sparse PCA, our approach does not hinge on the spiked covariance model or the limited correlation condition. As a complement to the first estimator that enjoys the oracle property, we prove that, another estimator within the family achieves a sharper statistical rate of convergence than the standard semidefinite relaxation of sparse PCA, even when the previous assumption on the magnitude of the projection matrix is violated. We validate the theoretical results by numerical experiments on synthetic datasets.	[Gu, Quanquan; Wang, Zhaoran; Liu, Han] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA	Princeton University	Gu, QQ (corresponding author), Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.	qgu@princeton.edu; zhaoran@princeton.edu; hanliu@princeton.edu	Wang, Zhaoran/P-7113-2018		NSF [IIS1408910, IIS1332109]; NIH [R01HG06841, R01MH102339, R01GM083084]	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	We would like to thank the anonymous reviewers for their helpful comments. This research is partially supported by the grants NSF IIS1408910, NSF IIS1332109, NIH R01MH102339, NIH R01GM083084, and NIH R01HG06841.	Amini AA, 2009, ANN STAT, V37, P2877, DOI 10.1214/08-AOS664; Berthet Q, 2013, ANN STAT, V41, P1780, DOI 10.1214/13-AOS1127; Birnbaum A, 2013, ANN STAT, V41, P1055, DOI 10.1214/12-AOS1014; Breheny P, 2011, ANN APPL STAT, V5, P232, DOI 10.1214/10-AOAS388; Cai TT, 2013, ANN STAT, V41, P3074, DOI 10.1214/13-AOS1178; d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269; d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506; Dattorro J., 2011, CONVEX OPTIMIZATION; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; He BS, 2014, SIAM J OPTIMIZ, V24, P1011, DOI 10.1137/13090849X; Johnstone IM, 2009, J AM STAT ASSOC, V104, P682, DOI 10.1198/jasa.2009.0121; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; Journee M, 2010, J MACH LEARN RES, V11, P517; Lei J., 2014, ARXIV14016978; Loh Po-Ling, 2013, ARXIV13052436; Ma ZM, 2013, ANN STAT, V41, P772, DOI 10.1214/13-AOS1097; Paul D., 2012, ARXIV12021242; Rigollet P., 2013, ARXIV13040828; Shen D, 2013, J MULTIVARIATE ANAL, V115, P317, DOI 10.1016/j.jmva.2012.10.007; Shen HP, 2008, J MULTIVARIATE ANAL, V99, P1015, DOI 10.1016/j.jmva.2007.06.007; Vilenchik, 2013, ARXIV13063690; Vu V., 2012, INT C ARTIFICIAL INT, P1278; Vu VQ, 2013, ANN STAT, V41, P2905, DOI 10.1214/13-AOS1151; Vu VQ, 2013, ADV NEURAL INFORM PR, V26; Wang Z., 2014, ARXIV14085352; Wang ZR, 2014, ANN STAT, V42, P2164, DOI 10.1214/14-AOS1238; Yuan XT, 2013, J MACH LEARN RES, V14, P899; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103066
C	Guez, A; Heess, N; Silver, D; Dayan, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Guez, Arthur; Heess, Nicolas; Silver, David; Dayan, Peter			Bayes-Adaptive Simulation-based Search with Value Function Approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Bayes-adaptive planning offers a principled solution to the exploration-exploitation trade-off under model uncertainty. It finds the optimal policy in belief space, which explicitly accounts for the expected effect on future rewards of reductions in uncertainty. However, the Bayes-adaptive solution is typically intractable in domains with large or continuous state spaces. We present a tractable method for approximating the Bayes-adaptive solution by combining simulation-based search with a novel value function approximation technique that generalises appropriately over belief space. Our method outperforms prior approaches in both discrete bandit tasks and simple continuous navigation and control tasks.	[Guez, Arthur; Dayan, Peter] UCL, Gatsby Unit, London, England; [Guez, Arthur; Heess, Nicolas; Silver, David] Google DeepMind, London, England	University of London; University College London; Google Incorporated	Guez, A (corresponding author), UCL, Gatsby Unit, London, England.	aguez@google.com						Asmuth J., 2011, P 27 C UNC ART INT, P19; Branavan SRK, 2012, J ARTIF INTELL RES, V43, P661, DOI 10.1613/jair.3484; Dallaire P, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P2604, DOI 10.1109/IROS.2009.5354013; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Deisenroth MP, 2009, NEUROCOMPUTING, V72, P1508, DOI 10.1016/j.neucom.2008.12.019; Duff M, 2002, THESIS; Duff M., 2003, P 20 INT C MACH LEAR, P131; Fonteneau R., 2013, IEEE INT S AD DYN PR; Gittins J., 1989, MULTIARMED BANDIT AL; Guez A., 2012, ADV NEURAL INFORM PR, P1034; Kurniawati H., 2008, ROBOTICS SCI SYSTEMS, P65, DOI DOI 10.15607/RSS.2008.IV.009.; Maei H.R., 2010, 27 ICML, P719; Moldovan Teodor Mihai, 2013, REINF LEARN DEC MAK; Ross Stephane, 2008, Uncertain Artif Intell, V2008, P476; Silver D., 2010, NIPS, P2164; Silver D, 2012, MACH LEARN, V87, P183, DOI 10.1007/s10994-012-5280-0; Sutton RS, 2009, ACM INT C P SERIES, V382, P125; Thrun S., 1999, P NIPS, V12, P1064; Thrun S., 2003, IJCAI, P1025, DOI DOI 10.5555/1630659.1630806; Wang T., 2005, P 22 INT C MACH LEAR, P956, DOI DOI 10.1145/1102351.1102472; Wang Y., 2012, P 29 INT C MACH LEAR	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102062
C	Han, SB; Du, L; Salazar, E; Carin, L		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Han, Shaobo; Du, Lin; Salazar, Esther; Carin, Lawrence			Dynamic Rank Factor Model for Text Streams	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				LIKELIHOOD; INFERENCE	We propose a semi-parametric and dynamic rank factor model for topic modeling, capable of (i) discovering topic prevalence over time, and (ii) learning contemporary multi-scale dependence structures, providing topic and word correlations as a byproduct. The high-dimensional and time-evolving ordinal/rank observations (such as word counts), after an arbitrary monotone transformation, are well accommodated through an underlying dynamic sparse factor model. The framework naturally admits heavy-tailed innovations, capable of inferring abrupt temporal jumps in the importance of topics. Posterior inference is performed through straightforward Gibbs sampling, based on the forward-filtering backward-sampling algorithm. Moreover, an efficient data subsampling scheme is leveraged to speed up inference on massive datasets. The modeling framework is illustrated on two real datasets: the US State of the Union Address and the JSTOR collection from Science.	[Han, Shaobo; Du, Lin; Salazar, Esther; Carin, Lawrence] Duke Univ, Durham, NC 27708 USA	Duke University	Han, SB (corresponding author), Duke Univ, Durham, NC 27708 USA.	shaobo.han@duke.edu; lin.du@duke.edu; esther.salazar@duke.edu; lcarin@duke.edu		Carin, Lawrence/0000-0001-6277-7948; Han, Shaobo/0000-0003-2545-7114	ARO; DARPA; DOE; NGA; ONR	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NGA; ONR(Office of Naval Research)	The research reported here was funded in part by ARO, DARPA, DOE, NGA and ONR. The authors are grateful to Jonas Wallin, Lund University, Sweden, for providing efficient package on simulation of the GIG distribution.	Ahmed A., 2010, TIMELINE DYNAMIC HIE; AITCHISON J, 1982, J ROY STAT SOC B MET, V44, P139; AITCHISON J, 1989, BIOMETRIKA, V76, P643; Armagan A., 2011, ADV NEURAL INFORM PR; Bhattacharya A, 2011, BIOMETRIKA, V98, P291, DOI 10.1093/biomet/asr013; Blei D. M., 2006, INT C MACHINE LEARNI; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; BLEI DM, 2006, ADV NEURAL INFORM PR; Cargnoni C, 1997, J AM STAT ASSOC, V92, P640, DOI 10.2307/2965711; CARTER CK, 1994, BIOMETRIKA, V81, P541; Chib S, 2001, J BUS ECON STAT, V19, P428, DOI 10.1198/07350010152596673; Christian A., 2014, KIEL WORKING PAPERS; Doucet A., 2001, SEQUENTIAL MONTE CAR; Gao C., 2012, SPARSE MODELING; Geweke J, 1996, REV FINANC STUD, V9, P557, DOI 10.1093/rfs/9.2.557; Ghosh J, 2009, J COMPUT GRAPH STAT, V18, P306, DOI 10.1198/jcgs.2009.07145; Hoff PD, 2007, ANN APPL STAT, V1, P265, DOI 10.1214/07-AOAS107; Hoff PD, 2009, SPRINGER TEXTS STAT, P1, DOI 10.1007/978-0-387-92407-6_1; Inouye D., 2014, INT C MACHINE LEARNI; Kalaitzis A., 2013, ADV NEURAL INFORM PR; Korattikara A, 2014, PR MACH LEARN RES, V32; Lawrence E, 2008, TECHNOMETRICS, V50, P182, DOI 10.1198/004017008000000064; Lin LZ, 2014, BIOMETRIKA, V101, P303, DOI 10.1093/biomet/ast063; Lopes HF, 2004, STAT SINICA, V14, P41; Murray JS, 2013, J AM STAT ASSOC, V108, P656, DOI 10.1080/01621459.2012.762328; PETTITT AN, 1982, J ROY STAT SOC B MET, V44, P234; Polson NG, 2012, BAYESIAN ANAL, V7, P887, DOI 10.1214/12-BA730; Quiroz M., 2014, ARXIV14044178; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Reis EA, 2006, INT STAT REV, V74, P203, DOI 10.1111/j.1751-5823.2006.tb00170.x; Reisinger J., 2010, INT C MACHINE LEARNI	33	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100080
C	Hazan, E; Levy, KY		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Hazan, Elad; Levy, Kfir Y.			Bandit Convex Optimization: Towards Tight Bounds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Bandit Convex Optimization (BCO) is a fundamental framework for decision making under uncertainty, which generalizes many problems from the realm of online and statistical learning. While the special case of linear cost functions is well understood, a gap on the attainable regret for BCO with nonlinear losses remains an important open question. In this paper we take a step towards understanding the best attainable regret bounds for BCO: we give an efficient and near-optimal regret algorithm for BCO with strongly-convex and smooth loss functions. In contrast to previous works on BCO that use time invariant exploration schemes, our method employs an exploration scheme that shrinks with time.	[Hazan, Elad; Levy, Kfir Y.] Technion Israel Inst Technol, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Hazan, E (corresponding author), Technion Israel Inst Technol, IL-32000 Haifa, Israel.	ehazan@ie.technion.ac.il; kfiryl@tx.technion.ac.il			European Union's Seventh Framework Programme (FP7/2007-2013) [336078 - ERC-SUBLRN]	European Union's Seventh Framework Programme (FP7/2007-2013)	The research leading to these results has received funding from the European Union's Seventh Framework Programme (FP7/2007-2013) under grant agreement no 336078 - ERC-SUBLRN.	Abernethy J, 2008, 21 ANN C LEARNING TH, P263; Agarwal A., 2010, P COLT, P28; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Dani Varsha, 2007, NIPS; Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385; Hazan E, 2012, OPTIMIZATION FOR MACHINE LEARNING, P287; Kleinberg R., 2004, ADV NEURAL INFORM PR, V17, P697; Nemirovskii A., 2004, LECT NOTES; Saha A, 2011, P 14 INT C ART INT S, P636; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Shamir O., 2013, P C LEARN THEOR, P3	11	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101048
C	Houlsby, NMT; Blei, DM		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Houlsby, Neil M. T.; Blei, David M.			A Filtering Approach to Stochastic Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Stochastic variational inference (SVI) uses stochastic optimization to scale up Bayesian computation to massive data. We present an alternative perspective on SVI as approximate parallel coordinate ascent. SVI trades-off bias and variance to step close to the unknown true coordinate optimum given by batch variational Bayes (VB). We define a model to automate this process. The model infers the location of the next VB optimum from a sequence of noisy realizations. As a consequence of this construction, we update the variational parameters using Bayes rule, rather than a hand-crafted optimization schedule. When our model is a Kalman filter this procedure can recover the original SVI algorithm and SVI with adaptive steps. We may also encode additional assumptions in the model, such as heavy-tailed noise. By doing so, our algorithm outperforms the original SVI schedule and a state-of-the-art adaptive SVI algorithm in two diverse domains.	[Houlsby, Neil M. T.] Google Res, Zurich, Switzerland; [Blei, David M.] Columbia Univ, Dept Comp Sci, Dept Stat, New York, NY 10027 USA	Google Incorporated; Columbia University	Houlsby, NMT (corresponding author), Google Res, Zurich, Switzerland.	neilhoulsby@google.com; david.blei@colombia.edu			Google European Doctoral Fellowship scheme; NSF CAREER NSF [IIS-0745520]; NSF BIGDATA NSF [IIS-1247664]; NSF NEURO NSF [IIS-1009542]; ONR [N00014-11-1-0651]; DARPA [FA8750-14-2-0009]	Google European Doctoral Fellowship scheme(Google Incorporated); NSF CAREER NSF; NSF BIGDATA NSF; NSF NEURO NSF; ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	NMTH is grateful to the Google European Doctoral Fellowship scheme for funding this research. DMB is supported by NSF CAREER NSF IIS-0745520, NSF BIGDATA NSF IIS-1247664, NSF NEURO NSF IIS-1009542, ONR N00014-11-1-0651 and DARPA FA8750-14-2-0009. We thank James McInerney, Alp Kucukelbir, Stephan Mandt, Rajesh Ranganath, Maxim Rabinovich, David Duvenaud, Thang Bui and the anonymous reviews for insightful feedback.	Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; [Anonymous], ICML; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; Capobianco E., 2001, EURASIP Journal on Applied Signal Processing, V2001, P121, DOI 10.1155/S1110865701000178; CHIEN YT, 1967, IEEE T SYST SCI CYB, VSSC3, P28, DOI 10.1109/TSSC.1967.300105; de Freitas JFG, 2000, NEURAL COMPUT, V12, P933, DOI 10.1162/089976600300015655; George AP, 2006, MACH LEARN, V65, P167, DOI 10.1007/S10994-006-8365-9; Ghahramani Z., 2000, NIPS WORKSH ONL LEAR; Gopalan PK, 2013, P NATL ACAD SCI USA, V110, P14534, DOI 10.1073/pnas.1221839110; Gunawardana A, 2009, J MACH LEARN RES, V10, P2935; Haykin S., 2004, KALMAN FILTERING NEU, V47; Hennig P, 2013, J MACH LEARN RES, V14, P843; Hensman James, 2013, ABS13096835 CORR; Hernandez-Lobato J. M., 2014, ICML; Hoffman M., 2010, ONLINE LEARNING LATE, P856; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Kohavi R., 2000, ACM SIGKDD EXPLOR NE, P86, DOI DOI 10.1145/380995.381033; Lakshminarayanan B., 2011, P 14 INT C ART INT S, P425; Nakajima Shinichi, 2010, NIPS, V23, P1759; Ranganath R., 2013, P 30 INT C MACH LEAR, V28, P298; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roth M, 2013, INT CONF ACOUST SPEE, P5770, DOI 10.1109/ICASSP.2013.6638770; Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674; Snoek J., 2012, P NIPS, V12, P2960; Yin Junming, 2013, Adv Neural Inf Process Syst, V2013, P422	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103007
C	Hsieh, CJ; Dhillon, IS; Ravikumar, P; Becker, S; Olsen, PA		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Hsieh, Cho-Jui; Dhillon, Inderjit S.; Ravikumar, Pradeep; Becker, Stephen; Olsen, Peder A.			QUIC & DIRTY: A Quadratic Approximation Approach for Dirty Statistical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				MATRIX DECOMPOSITION	In this paper, we develop a family of algorithms for optimizing "superpositionstructured" or "dirty" statistical estimators for high-dimensional problems involving the minimization of the sum of a smooth loss function with a hybrid regularization. Most of the current approaches are first-order methods, including proximal gradient or Alternating Direction Method of Multipliers (ADMM). We propose a new family of second-order methods where we approximate the loss function using quadratic approximation. The superposition structured regularizer then leads to a subproblem that can be efficiently solved by alternating minimization. We propose a general active subspace selection approach to speed up the solver by utilizing the low-dimensional structure given by the regularizers, and provide convergence guarantees for our algorithm. Empirically, we show that our approach is more than 10 times faster than state-of-the-art first-order approaches for the latent variable graphical model selection problems and multi-task learning problems when there is more than one regularizer. For these problems, our approach appears to be the first algorithm that can extend active subspace ideas to multiple regularizers.	[Hsieh, Cho-Jui; Dhillon, Inderjit S.; Ravikumar, Pradeep] Univ Texas Austin, Austin, TX 78712 USA; [Becker, Stephen] Univ Colorado, Boulder, CO 80309 USA; [Olsen, Peder A.] IBM Corp, TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA	University of Texas System; University of Texas Austin; University of Colorado System; University of Colorado Boulder; International Business Machines (IBM)	Hsieh, CJ (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	cjhsieh@cs.utexas.edu; inderjit@cs.utexas.edu; pradeepr@cs.utexas.edu	Becker, Stephen R/R-7528-2016	Becker, Stephen R/0000-0002-1932-8159	NSF [IIS-1149803, IIS-1447574, DMS-1264033, CCF-1320746, CCF-1117055]; IBM PhD fellowship; ARO [W911NF-12-1-0390]; IBM Research Goldstine Postdoctoral Fellowship	NSF(National Science Foundation (NSF)); IBM PhD fellowship(International Business Machines (IBM)); ARO; IBM Research Goldstine Postdoctoral Fellowship(International Business Machines (IBM))	This research was supported by NSF grants CCF-1320746 and CCF-1117055. C.-J.H also acknowledges support from an IBM PhD fellowship. P.R. acknowledges the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1447574, and DMS-1264033. S.R.B. was supported by an IBM Research Goldstine Postdoctoral Fellowship while the work was performed.	[Anonymous], 2010, NIPS; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Candes E., 2012, MATH PROGRAMMING; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chandrasekaran V., 2012, ANN STAT; Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793; Collins Michael, 2012, NIPS; Dinh Q. T., 2013, ARXIV13111756; Hsieh C.- J., 2011, NIPS; Hsieh C. - J., 2013, NIPS; Hsieh C.-J., 2012, NIPS; Hsieh Cho-Jui, 2014, ICML; Hsu D, 2011, IEEE T INFORM THEORY, V57, P7221, DOI 10.1109/TIT.2011.2158250; Lee J. D., 2012, NIPS; Ma SQ, 2013, NEURAL COMPUT, V25, P2172, DOI 10.1162/NECO_a_00379; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Olsen P., 2012, NIPS; Qin Z., 2013, MATH PROGRAMMING COM; Scheinberg K., 2014, ARXIV13116547; Tewari A., 2011, NIPS; Tseng P, 2007, MATH PROGRAM, V117, P387; van Breukelen M, 1998, KYBERNETIKA, V34, P381; Wang CJ, 2010, SIAM J OPTIMIZ, V20, P2994, DOI 10.1137/090772514; Yang E., 2013, NIPS; Yen E.- H., 2014, NIPS; Yuan GX, 2012, J MACH LEARN RES, V13, P1999; Yun S, 2011, MATH PROGRAM, V129, P331, DOI 10.1007/s10107-011-0471-1	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103029
C	Hsieh, CJ; Si, S; Dhillon, IS		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Hsieh, Cho-Jui; Si, Si; Dhillon, Inderjit S.			Fast Prediction for Large-Scale Kernel Machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				VECTOR MACHINES; NYSTROM METHOD; MATRIX	Kernel machines such as kernel SVM and kernel ridge regression usually construct high quality models; however, their use in real-world applications remains limited due to the high prediction cost. In this paper, we present two novel insights for improving the prediction efficiency of kernel machines. First, we show that by adding "pseudo landmark points" to the classical Nystrom kernel approximation in an elegant way, we can significantly reduce the prediction error without much additional prediction cost. Second, we provide a new theoretical analysis on bounding the error of the solution computed by using Nystrom kernel approximation method, and show that the error is related to the weighted kmeans objective function where the weights are given by the model computed from the original kernel. This theoretical insight suggests a new landmark point selection technique for the situation where we have knowledge of the original model. Based on these two insights, we provide a divide-and-conquer framework for improving the prediction speed. First, we divide the whole problem into smaller local subproblems to reduce the problem size. In the second phase, we develop a kernel approximation based fast prediction approach within each subproblem. We apply our algorithm to real world large-scale classification and regression datasets, and show that the proposed algorithm is consistently and significantly better than other competitors. For example, on the Covertype classification problem, in terms of prediction time, our algorithm achieves more than 10000 times speedup over the full kernel SVM, and a two-fold speedup over the state-of-the-art LDKL approach, while obtaining much higher prediction accuracy than LDKL (95.2% vs. 89.53%).	[Hsieh, Cho-Jui; Si, Si; Dhillon, Inderjit S.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Hsieh, CJ (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	cjhsieh@cs.utexas.edu; ssi@cs.utexas.edu; inderjit@cs.utexas.edu			NSF [CCF-1320746, CCF-1117055]; IBM PhD fellowship	NSF(National Science Foundation (NSF)); IBM PhD fellowship(International Business Machines (IBM))	This research was supported by NSF grants CCF-1320746 and CCF-1117055. C.-J. H also acknowledges support from an IBM PhD fellowship.	Chang Y.W., 2010, J MACH LEARN RES, V11, P1471, DOI DOI 10.5555/1756006.1859899; Cossalter M., 2011, ICML; Cotter Andrew, 2013, ICML; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Drineas P, 2006, SIAM J COMPUT, V36, P184, DOI 10.1137/S0097539704442702; Hsieh C.-J., 2012, NIPS; Hsieh Cho-Jui, 2014, ICML; Joachims T, 2009, MACH LEARN, V76, P179, DOI 10.1007/s10994-009-5126-6; Jose C., 2013, ICML; Jung H. G., 2014, IEEE T INTELLIGENT T; Kar P., 2012, AISTATS; Keerthi SS, 2006, J MACH LEARN RES, V7, P1493; Kumar Sanjiv, 2009, NIPS; Ladicky Lubor, 2011, ICML; Le Q., 2013, ICML; Lee Y.-J., 2001, SDM; Maji S., 2013, IEEE PAMI, V35; Nandan M, 2014, J MACH LEARN RES, V15, P59; Pavlov D., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P295, DOI 10.1145/347090.347155; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Scholkopf B., 1998, DAGM S, P124; Si S., 2014, ICML; Tsang IW, 2005, J MACH LEARN RES, V6, P363; Wang PW, 2014, J MACH LEARN RES, V15, P1523; Wang SS, 2013, J MACH LEARN RES, V14, P2729; Williams C. K. I., 2001, NIPS; Zhang K, 2010, IEEE T NEURAL NETWOR, V21, P1576, DOI 10.1109/TNN.2010.2064786; Zhang Kai, 2008, ICML	29	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102042
C	Irfan, MT; Ortiz, LE		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Irfan, Mohammad T.; Ortiz, Luis E.			Causal Strategic Inference in Networked Microfinance Economies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ENTRY; SUBSTITUTES; MODEL	Performing interventions is a major challenge in economic policy-making. We propose causal strategic inference as a framework for conducting interventions and apply it to large, networked microfinance economies. The basic solution platform consists of modeling a microfinance market as a networked economy, learning the parameters of the model from the real-world microfinance data, and designing algorithms for various causal questions. For a special case of our model, we show that an equilibrium point always exists and that the equilibrium interest rates are unique. For the general case, we give a constructive proof of the existence of an equilibrium point. Our empirical study is based on the microfinance data from Bangladesh and Bolivia, which we use to first learn our models. We show that causal strategic inference can assist policy-makers by evaluating the outcomes of various types of interventions, such as removing a loss-making bank from the market, imposing an interest rate cap, and subsidizing banks.	[Irfan, Mohammad T.] Bowdoin Coll, Dept Comp Sci, Brunswick, ME 04011 USA; [Ortiz, Luis E.] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA	Bowdoin College; State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook	Irfan, MT (corresponding author), Bowdoin Coll, Dept Comp Sci, Brunswick, ME 04011 USA.	mirfan@bowdoin.edu; leortiz@cs.stonybrook.edu			NSF CAREER Award [IIS-1054541]	NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD))	We thank the reviewers. Luis E. Ortiz was supported in part by NSF CAREER Award IIS-1054541.	[Anonymous], 2013, THESIS; Armendariz B., 2005, EC MICROFINANCE; Arrow KJ, 1954, ECONOMETRICA, V22, P265, DOI 10.2307/1907353; Augereau A, 2006, RAND J ECON, V37, P887, DOI 10.1111/j.1756-2171.2006.tb00062.x; Bajari P., 2010, WORKING PAPER; BERRY ST, 1992, ECONOMETRICA, V60, P889, DOI 10.2307/2951571; Bjorn P. A., 1984, 527 CAL I TECHN; BRESNAHAN TF, 1990, REV ECON STUD, V57, P531, DOI 10.2307/2298085; BRESNAHAN TF, 1991, J ECONOMETRICS, V48, P57, DOI 10.1016/0304-4076(91)90032-9; BULOW JI, 1985, J POLIT ECON, V93, P488, DOI 10.1086/261312; Dubey P, 2006, GAME ECON BEHAV, V54, P77, DOI 10.1016/j.geb.2004.10.007; EISENBERG E, 1959, ANN MATH STAT, V30, P165, DOI 10.1214/aoms/1177706369; Fisher I., 1892, MATH INVESTIGATIONS; Ghatak M, 1999, J DEV ECON, V60, P195, DOI 10.1016/S0304-3878(99)00041-3; Gine D. K. X., 2006, 936 YAL U EC GROWTH; I. of Microfinance (InM), 2009, BANGLADESH MICROFINA; Kakade S. M., 2005, ADV NEURAL INFORM PR, P633; Kakade SM, 2004, LECT NOTES COMPUT SC, V3120, P17, DOI 10.1007/978-3-540-27819-1_2; Luce R, 1959, INDIVIDUAL CHOICE BE; MCKELVEY RD, 1995, GAME ECON BEHAV, V10, P6, DOI 10.1006/game.1995.1023; Morduch J, 1999, J DEV ECON, V60, P229, DOI 10.1016/S0304-3878(99)00042-5; Pardo B., 2002, Computer Music Journal, V26, P27, DOI 10.1162/014892602760137167; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Porteous D., 2006, FOCUS NOTE, V33; Seim K, 2006, RAND J ECON, V37, P619, DOI 10.1111/j.1756-2171.2006.tb00034.x; Vazirani VV, 2007, ALGORITHMIC GAME THEORY, P103; Wright D., 2004, DONORS LOCAL CONSULT	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102099
C	Jagabathula, S; Subramanian, L; Venkataraman, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Jagabathula, Srikanth; Subramanian, Lakshminarayanan; Venkataraman, Ashwin			Reputation-based Worker Filtering in Crowdsourcing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In this paper, we study the problem of aggregating noisy labels from crowd workers to infer the underlying true labels of binary tasks. Unlike most prior work which has examined this problem under the random worker paradigm, we consider a much broader class of adversarial workers with no specific assumptions on their labeling strategy. Our key contribution is the design of a computationally efficient reputation algorithm to identify and filter out these adversarial workers in crowd-sourcing systems. Our algorithm uses the concept of optimal semi-matchings in conjunction with worker penalties based on label disagreements, to assign a reputation score for every worker. We provide strong theoretical guarantees for deterministic adversarial strategies as well as the extreme case of sophisticated adversaries where we analyze the worst-case behavior of our algorithm. Finally, we show that our reputation algorithm can significantly improve the accuracy of existing label aggregation algorithms in real-world crowdsourcing datasets.	[Jagabathula, Srikanth] NYU, Stern Sch Business, Dept IOMS, New York, NY 10003 USA; [Subramanian, Lakshminarayanan; Venkataraman, Ashwin] NYU, Dept Comp Sci, New York, NY 10003 USA; [Subramanian, Lakshminarayanan; Venkataraman, Ashwin] New York Univ Abu Dhabi, CTED, Abu Dhabi, U Arab Emirates	New York University; New York University	Jagabathula, S (corresponding author), NYU, Stern Sch Business, Dept IOMS, New York, NY 10003 USA.	sjagabat@stern.nyu.edu; lakshmi@cs.nyu.edu; ashwin@cs.nyu.edu			Center for Technology and Economic Development (CTED)	Center for Technology and Economic Development (CTED)	We thank the anonymous reviewers for their valuable feedback. Ashwin Venkataraman was supported by the Center for Technology and Economic Development (CTED).	Biggio B., 2012, INT C MACH LEARN; Dalvi N, 2013, P 22 INT C WORLD WID, P285, DOI DOI 10.1145/2488388.2488414; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Demartini G., 2012, P 21 INT C WORLD WID, P469; Downs JS, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2399; Eickhoff C., 2011, P ACM SIGIR WORKSH C, P21; Ghosh A., 2011, P 12 ACM C EL COMM, P167, DOI DOI 10.1145/1993574.1993599; Harvey NJA, 2003, LECT NOTES COMPUT SC, V2748, P294; Ipeirotis Panagiotis G., 2010, P ACM SIGKDD WORKSH, DOI [10.1145/1837885.1837906, DOI 10.1145/1837885.1837906]; Karger D. R., 2011, NEURAL INFORM PROCES; Kittur A, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P453; LE J., ENSURING QUALITY CRO; Lee K., 2013, 7 INT AAAI C WEBL SO; PeterWelinder Steve Branson, 2010, P NIPS, V23, P1; Raykar VC, 2012, J MACH LEARN RES, V13, P491; Raykar VC, 2010, J MACH LEARN RES, V11, P1297; Smyth P., 1995, Advances in Neural Information Processing Systems 7, P1085; Snow Rion, 2008, P 2008 C EMP METH NA, P254, DOI DOI 10.3115/1613715.1613751; Tran Nguyen, 2009, NSDI; Vuurens JBP, 2012, IEEE INTERNET COMPUT, V16, P20, DOI 10.1109/MIC.2012.71; Wang G., 2012, P 21 INT C WORLD WID, P679, DOI [10.1145/2187836.2187928, DOI 10.1145/2187836.2187928]; Wang G., 2014, 23 USENIX SEC S USEN; Whitehill J., 2009, ADV NEURAL INFORM PR, V22, P7	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101064
C	Jain, P; Tewari, A; Kar, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Jain, Prateek; Tewari, Ambuj; Kar, Purushottam			On Iterative Hard Thresholding Methods for High-dimensional M-Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				REGRESSION; ALGORITHM; SPARSITY	The use of M-estimators in generalized linear regression models in high dimensional settings requires risk minimization with hard L-0 constraints. Of the known methods, the class of projected gradient descent (also known as iterative hard thresholding (IHT)) methods is known to offer the fastest and most scalable solutions. However, the current state-of-the-art is only able to analyze these methods in extremely restrictive settings which do not hold in high dimensional statistical models. In this work we bridge this gap by providing the first analysis for IHT-style methods in the high dimensional statistical setting. Our bounds are tight and match known minimax lower bounds. Our results rely on a general analysis framework that enables us to analyze several popular hard thresholding style algorithms (such as HTP, CoSaMP, SP) in the high dimensional regression setting. Finally, we extend our analysis to the problem of low-rank matrix recovery.	[Jain, Prateek; Kar, Purushottam] Microsoft Res, Bangalore, Karnataka, India; [Tewari, Ambuj] Univ Michigan, Ann Arbor, MI 48109 USA	Microsoft; University of Michigan System; University of Michigan	Jain, P (corresponding author), Microsoft Res, Bangalore, Karnataka, India.	prajain@microsoft.com; tewaria@umich.edu; t-purkar@microsoft.com	Kar, Purushottam/W-8113-2019	Kar, Purushottam/0000-0003-2096-5267				Agarwal A, 2012, ANN STAT, V40, P2452, DOI 10.1214/12-AOS1032; [Anonymous], 2010, THESIS U BRIT COLUMB; Bahmani S, 2013, J MACH LEARN RES, V14, P807; Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002; Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Dai Wei, 2009, IEEE T INFORM THEORY, V55; Foucart S, 2011, SIAM J NUMER ANAL, V49, P2543, DOI 10.1137/100806278; GARG R., 2009, ICML; Jain Prateek, 2011, ANN C NEUR INF PROC; Jalali A., 2011, ADV NEURAL INF PROCE, V24, P1935; Liu J, 2014, PR MACH LEARN RES, V32; Loh P., 2013, ARXIV13052436MATHST; Loh PL, 2012, ANN STAT, V40, P1637, DOI 10.1214/12-AOS1018; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002; Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Raskutti G, 2011, IEEE T INFORM THEORY, V57, P6976, DOI 10.1109/TIT.2011.2165799; Rohde A, 2011, ANN STAT, V39, P887, DOI 10.1214/10-AOS860; Shalev-Shwartz S, 2010, SIAM J OPTIMIZ, V20, P2807, DOI 10.1137/090759574; Yuan X. -T., 2014, P 31 INT C MACH LEAR; Zhang T, 2011, IEEE T INFORM THEORY, V57, P4689, DOI 10.1109/TIT.2011.2146690; Zhang  Yuchen, 2014, ARXIV14021918	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100072
C	Jain, P; Oh, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Jain, Prateek; Oh, Sewoong			Provable Tensor Factorization with Missing Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				MATRIX COMPLETION; APPROXIMATION; RANK	We study the problem of low-rank tensor factorization in the presence of missing data. We ask the following question: how many sampled entries do we need, to efficiently and exactly reconstruct a tensor with a low-rank orthogonal decomposition? We propose a novel alternating minimization based method which iteratively refines estimates of the singular vectors. We show that under certain standard assumptions, our method can recover a three-mode n x n x n dimensional rank-r tensor exactly from O(n(3/2)r(5) log(4) n) randomly sampled entries. In the process of proving this result, we solve two challenging sub-problems for tensors with missing data. First, in analyzing the initialization step, we prove a generalization of a celebrated result by Szemer ' edie et al. on the spectrum of random graphs. We show that this initialization step alone is sufficient to achieve the root mean squared error on the parameters bounded by C(r(2)n(3/2)(log n)(4)/vertical bar Omega vertical bar) from vertical bar Omega vertical bar observed entries for some constant C independent of n and r. Next, we prove global convergence of alternating minimization with this good initialization. Simulations suggest that the dependence of the sample size on the dimensionality n is indeed tight.	[Jain, Prateek] Microsoft Res, Bangalore, Karnataka, India; [Oh, Sewoong] Univ Illinois, Dept Ind & Enterprise Syst Engn, Urbana, IL 61801 USA	Microsoft; University of Illinois System; University of Illinois Urbana-Champaign	Jain, P (corresponding author), Microsoft Res, Bangalore, Karnataka, India.	prajain@microsoft.com; swoh@illinois.edu						Acar E, 2011, CHEMOMETR INTELL LAB, V106, P41, DOI 10.1016/j.chemolab.2010.08.004; Anandkumar Animashree, 2014, ARXIV14025180; Anima Anandkumar, 2012, ABS12107559 CORR; Azar Y., 2001, P 33 ANN ACM S THEOR, P619; Berke R, 2009, LECT NOTES COMPUT SC, V5792, P117, DOI 10.1007/978-3-642-04944-6_10; Bro R, 1998, THESIS; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791; de Silva V, 2008, SIAM J MATRIX ANAL A, V30, P1084, DOI 10.1137/06066518X; Feige U, 2005, RANDOM STRUCT ALGOR, V27, P251, DOI 10.1002/rsa.20089; Friedman J., 1989, Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, P587, DOI 10.1145/73007.73063; Hardt M., 2013, ARXIV13120925; Harshman Richard A., 1970, FDN PARAFAC PROCEDUR; Hitchcock FL, 1927, EXPRESSION TENSOR PO; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39; Mu C., 2013, ARXIV13075870; Negahban S., 2012, J MACHINE LEARNING R; Tomasi G, 2005, CHEMOMETR INTELL LAB, V75, P163, DOI 10.1016/j.chemolab.2004.07.003; Walczak B, 2001, CHEMOMETR INTELL LAB, V58, P15, DOI 10.1016/S0169-7439(01)00131-9; Zhang T, 2001, SIAM J MATRIX ANAL A, V23, P534, DOI 10.1137/S0895479899352045	26	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102036
C	Janoos, F; Denli, H; Subrahmanya, N		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Janoos, Firdaus; Denli, Huseyin; Subrahmanya, Niranjan			Multi-scale Graphical Models for Spatio-Temporal Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Learning the dependency structure between spatially distributed observations of a spatio-temporal process is an important problem in many fields such as geology, geophysics, atmospheric sciences, oceanography, etc.. However, estimation of such systems is complicated by the fact that they exhibit dynamics at multiple scales of space and time arising due to a combination of diffusion and convection/advection [17]. As we show, time-series graphical models based on vector auto-regressive processes[18] are inefficient in capturing such multi-scale structure. In this paper, we present a hierarchical graphical model with physically derived priors that better represents the multi-scale character of these dynamical systems. We also propose algorithms to efficiently estimate the interaction structure from data. We demonstrate results on a general class of problems arising in exploration geophysics by discovering graphical structure that is physically meaningful and provide evidence of its advantages over alternative approaches.	[Janoos, Firdaus; Denli, Huseyin; Subrahmanya, Niranjan] ExxonMobil Corp Strateg Res, Annandale, NJ 08801 USA	Exxon Mobil Corporation	Janoos, F (corresponding author), ExxonMobil Corp Strateg Res, Annandale, NJ 08801 USA.	firdaus@ieee.org						Akcelik V, 2006, LECT NOTES COMPUT SC, V3993, P481; Anderson BDO, 2012, IEEE DECIS CONTR P, P184, DOI 10.1109/CDC.2012.6426713; Aw A, 2000, SIAM J APPL MATH, V60, P916, DOI 10.1137/S0036139997332099; Bach FR, 2004, IEEE T SIGNAL PROCES, V52, P2189, DOI 10.1109/TSP.2004.831032; Beck A, 2009, IEEE T IMAGE PROCESS, V18, P2419, DOI 10.1109/TIP.2009.2028250; Bertsekas D. P., 1999, ATHENA SCI, P5; Christmas J, 2010, IEEE IJCNN; Crank J., 1975, MATH DIFFUSION, DOI DOI 10.1016/0306-4549(77)90072-X; Cressie N, 2011, STAT SPATIO TEMPORAL; Eichler M., 2013, PHILOS T ROYAL SOC A, V371; Haufe S., 2008, ADV NEURAL INFORM PR, V1, P1; Huang T., 2012, EUR C MACH LERN; Hughes T., 1995, COMPUT METHODS APPL, V127; Hyvarinen A, 2010, J MACH LEARN RES, V11, P1709; Janoos F., 2012, ADV NEUR INF P SYS N; Kearey P., 2011, BLACK; Lloyd CD., 2014, EXPLORING SPATIAL SC; Moneta A., 2009, NIPS MIN S CAUS TIME; Panagakis Y, 2014, PATTERN RECOGN LETT, V38, P46, DOI 10.1016/j.patrec.2013.10.021; Szabo Z, 2009, ACTA CYBERN, V19, P177, DOI 10.14232/actacyb.19.1.2009.12; Tarantola A., 2005, INVERSE PROBLEM THEO, DOI DOI 10.1137/1.9780898717921; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105; Wang H., 2013, ABS13100505 CORR; Wightman W., 2003, FHWAIF04021; Willsky AS, 2002, P IEEE, V90, P1396, DOI 10.1109/JPROC.2002.800717	25	0	0	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102034
C	Kairouz, P; Oh, S; Viswanath, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Kairouz, Peter; Oh, Sewoong; Viswanath, Pramod			Extremal Mechanisms for Local Differential Privacy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Local differential privacy has recently surfaced as a strong measure of privacy in contexts where personal information remains private even from data analysts. Working in a setting where the data providers and data analysts want to maximize the utility of statistical inferences performed on the released data, we study the fundamental tradeoff between local differential privacy and information theoretic utility functions. We introduce a family of extremal privatization mechanisms, which we call staircase mechanisms, and prove that it contains the optimal privatization mechanism that maximizes utility. We further show that for all information theoretic utility functions studied in this paper, maximizing utility is equivalent to solving a linear program, the outcome of which is the optimal staircase mechanism. However, solving this linear program can be computationally expensive since it has a number of variables that is exponential in the data size. To account for this, we show that two simple staircase mechanisms, the binary and randomized response mechanisms, are universally optimal in the high and low privacy regimes, respectively, and well approximate the intermediate regime.	[Kairouz, Peter; Viswanath, Pramod] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA; [Oh, Sewoong] Univ Illinois, Dept Ind & Enterprise Syst Engn, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign	Kairouz, P (corresponding author), Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.	kairouz2@illinois.edu; swoh@illinois.edu; pramodv@illinois.edu						Acquisti A., 2004, P 5 ACM C EL COMM NE, P21, DOI DOI 10.1145/988772.988777; Acquisti Alessandro, 2007, DIGITAL PRIVACY, P329; Beimel A, 2008, LECT NOTES COMPUT SC, V5157, P451, DOI 10.1007/978-3-540-85174-5_25; BLACKWELL D, 1953, ANN MATH STAT, V24, P265, DOI 10.1214/aoms/1177729032; Blocki J, 2012, ANN IEEE SYMP FOUND, P410, DOI 10.1109/FOCS.2012.67; Chaudhuri K, 2012, ADV NEURAL INFORM PR, P989; Chaudhuri K., 2012, ARXIV12066395; Chaudhuri K., 2008, PROC 22 ANN C NEURAL, P289; De A, 2012, LECT NOTES COMPUT SC, V7194, P321, DOI 10.1007/978-3-642-28914-9_18; Duchi JC, 2013, ANN IEEE SYMP FOUND, P429, DOI 10.1109/FOCS.2013.53; Dwork C, 2006, LECT NOTES COMPUT SC, V4052, P1; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2009, ACM S THEORY COMPUT, P371; Geng Q., 2012, ARXIV12121186; Geng Q., 2013, ARXIV13120655; Geng Q., 2013, ARXIV13051330; Ghosh A, 2012, SIAM J COMPUT, V41, P1673, DOI 10.1137/09076828X; Hardt M, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P1255; Hardt M, 2010, ACM S THEORY COMPUT, P705; Hardt M, 2010, ANN IEEE SYMP FOUND, P61, DOI 10.1109/FOCS.2010.85; Kairouz P., 2014, ARXIV14071338; Kairouz  P., 2014, ARXIV14071546; Kapralov M., 2013, SODA, V5, P1; Lei J, 2011, ADV NEURAL INFORM PR, P361; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Oh S., 2013, ARXIV13110776; WARNER SL, 1965, J AM STAT ASSOC, V60, P63, DOI 10.2307/2283137	29	0	0	2	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101063
C	Kangas, K; Niinimaki, T; Koivisto, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Kangas, Kustaa; Niinimaki, Teppo; Koivisto, Mikko			Learning Chordal Markov Networks by Dynamic Programming	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				BAYESIAN NETWORKS; STRUCTURE DISCOVERY	We present an algorithm for finding a chordal Markov network that maximizes any given decomposable scoring function. The algorithm is based on a recursive characterization of clique trees, and it runs in O(4(n)) time for n vertices. On an eight-vertex benchmark instance, our implementation turns out to be about ten million times faster than a recently proposed, constraint satisfaction based algorithm (Corander et al., NIPS 2013). Within a few hours, it is able to solve instances up to 18 vertices, and beyond if we restrict the maximum clique size. We also study the performance of a recent integer linear programming algorithm (Bartlett and Cussens, UAI 2013). Our results suggest that, unless we bound the clique sizes, currently only the dynamic programming algorithm is guaranteed to solve instances with around 15 or more vertices.	[Kangas, Kustaa; Niinimaki, Teppo; Koivisto, Mikko] Univ Helsinki, HIIT, Dept Comp Sci, Helsinki, Finland	Aalto University; University of Helsinki	Kangas, K (corresponding author), Univ Helsinki, HIIT, Dept Comp Sci, Helsinki, Finland.	jwkangas@cs.helsinki.fi; tzniinim@cs.helsinki.fi; mkhkoivil@cs.helsinki.fi			Academy of Finland [276864]	Academy of Finland(Academy of Finland)	This work was supported by the Academy of Finland, grant 276864. The authors thank Matti Jarvisalo for useful discussions on constraint programming approaches to learning Markov networks.	Abbeel P, 2006, J MACH LEARN RES, V7, P1743; Bartlett M., 2013, UNCERTAINTY ARTIFICI, P182; Berg J, 2014, JMLR WORKSH CONF PRO, V33, P86; Bromberg F, 2009, J ARTIF INTELL RES, V35, P449, DOI 10.1613/jair.2773; Chechetka A., 2007, NIPS; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Corander J., 2013, ADV NEURAL INFORM PR, P1349; Corander J, 2008, DATA MIN KNOWL DISC, V17, P431, DOI 10.1007/s10618-008-0099-9; Davis Jesse, 2010, P 27 INT C MACHINE L; DAWID AP, 1993, ANN STAT, V21, P1272, DOI 10.1214/aos/1176349260; de Campos CP, 2011, J MACH LEARN RES, V12, P663; DellaPietra S, 1997, IEEE T PATTERN ANAL, V19, P380, DOI 10.1109/34.588021; Elidan G, 2008, J MACH LEARN RES, V9, P2699; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; Koivisto M, 2004, J MACH LEARN RES, V5, P549; Korhonen J.H., 2013, JMLR W CP, V31, P370; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; LAURITZEN SL, 1988, J ROY STAT SOC B MET, V50, P157; Lichman M, 2013, UCI MACHINE LEARNING; Narasimhan M., 2004, UAI 04, P410; Ott S, 2003, PACIFIC SYMPOSIUM ON BIOCOMPUTING 2004, P557; Parviainen P, 2014, JMLR WORKSH CONF PRO, V33, P751; Silander T., 2006, P 22 C UNC ART INT, P445; Srebro N, 2003, ARTIF INTELL, V143, P123, DOI 10.1016/S0004-3702(02)00360-0; Van Haaren J, 2012, P 26 AAAI C ART INT, P1148; Yuan CH, 2013, J ARTIF INTELL RES, V48, P23, DOI 10.1613/jair.4039	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103028
C	Kervrann, C		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Kervrann, Charles			PEWA: Patch-based Exponentially Weighted Aggregation for image denoising	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				PAC-BAYESIAN BOUNDS; SPARSE	Patch-based methods have been widely used for noise reduction in recent years. In this paper, we propose a general statistical aggregation method which combines image patches denoised with several commonly-used algorithms. We show that weakly denoised versions of the input image obtained with standard methods, can serve to compute an efficient patch-based aggregated estimator. In our approach, we evaluate the Stein's Unbiased Risk Estimator (SURE) of each denoised candidate image patch and use this information to compute the exponential weighted aggregation (EWA) estimator. The aggregation method is flexible enough to combine any standard denoising algorithm and has an interpretation with Gibbs distribution. The denoising algorithm (PEWA) is based on a MCMC sampling and is able to produce results that are comparable to the current state-of-the-art.	[Kervrann, Charles] Inria Rennes Bretagne Atlantique, Serpico Project Team, Campus Univ Beaulieu, F-35042 Rennes, France		Kervrann, C (corresponding author), Inria Rennes Bretagne Atlantique, Serpico Project Team, Campus Univ Beaulieu, F-35042 Rennes, France.	charles.kervrann@inria.fr						Alquier P, 2011, ELECTRON J STAT, V5, P127, DOI 10.1214/11-EJS601; Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024; Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952; Chatterjee P, 2012, IEEE T IMAGE PROCESS, V21, P1635, DOI 10.1109/TIP.2011.2172799; Chatterjee P, 2010, IEEE T IMAGE PROCESS, V19, P895, DOI 10.1109/TIP.2009.2037087; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Dalalyan A, 2008, MACH LEARN, V72, P39, DOI 10.1007/s10994-008-5051-0; Dalayan A. S., 2009, SPARSE REGRESSION LE; Deledalle CA, 2012, J MATH IMAGING VIS, V43, P103, DOI 10.1007/s10851-011-0294-y; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Kervrann C, 2006, IEEE T IMAGE PROCESS, V15, P2866, DOI 10.1109/TIP.2006.877529; Lebrun M, 2013, IMAGE PROCESS ON LIN, V3, P1, DOI 10.5201/ipol.2013.16; Leung G, 2006, IEEE T INFORM THEORY, V52, P3396, DOI 10.1109/TIT.2006.878172; Levin A, 2012, LECT NOTES COMPUT SC, V7576, P73, DOI 10.1007/978-3-642-33715-4_6; Louchet C, 2011, SIAM J IMAGING SCI, V4, P651, DOI 10.1137/100785855; Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452; Milanfar P, 2013, IEEE SIGNAL PROC MAG, V30, P106, DOI 10.1109/MSP.2011.2179329; Ram I, 2013, IEEE T IMAGE PROCESS, V22, P2764, DOI 10.1109/TIP.2013.2257813; Roth S, 2005, PROC CVPR IEEE, P860; Salmon J, 2009, IEEE IMAGE PROC, P2977, DOI 10.1109/ICIP.2009.5414512; Talebi H, 2013, IEEE T IMAGE PROCESS, V22, P1468, DOI 10.1109/TIP.2012.2231691; Van De Ville D, 2009, IEEE SIGNAL PROC LET, V16, P973, DOI 10.1109/LSP.2009.2027669; Wang YQ, 2013, SIAM J IMAGING SCI, V6, P999, DOI 10.1137/120901131; Xue F, 2013, IEEE T IMAGE PROCESS, V22, P1954, DOI 10.1109/TIP.2013.2240004; Yu GS, 2011, IMAGE PROCESS ON LIN, V1, P292, DOI 10.5201/ipol.2011.ys-dct; Zontak M, 2011, PROC CVPR IEEE, P977, DOI 10.1109/CVPR.2011.5995401; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101081
C	Khan, ME		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Khan, Mohammad Emtiyaz			Decoupled Variational Gaussian Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				BAYESIAN-INFERENCE; DESIGN	Variational Gaussian (VG) inference methods that optimize a lower bound to the marginal likelihood are a popular approach for Bayesian inference. A difficulty remains in computation of the lower bound when the latent dimensionality L is large. Even though the lower bound is concave for many models, its computation requires optimization over O(L-2) variational parameters. Efficient reparameterization schemes can reduce the number of parameters, but give inaccurate solutions or destroy concavity leading to slow convergence. We propose decoupled variational inference that brings the best of both worlds together. First, it maximizes a Lagrangian of the lower bound reducing the number of parameters to O(N), where N is the number of data examples. The reparameterization obtained is unique and recovers maxima of the lower-bound even when it is not concave. Second, our method maximizes the lower bound using a sequence of convex problems, each of which is parallellizable over data examples. Each gradient computation reduces to prediction in a pseudo linear regression model, thereby avoiding all direct computations of the covariance and only requiring its linear projections. Theoretically, our method converges at the same rate as existing methods in the case of concave lower bounds, while remaining convergent at a reasonable rate for the non-concave case.	[Khan, Mohammad Emtiyaz] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Khan, ME (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	emtiyaz@gmail.com			School of Computer Science and Communication at EPFL	School of Computer Science and Communication at EPFL	This work was supported by School of Computer Science and Communication at EPFL. I would specifically like to thank Matthias Grossglauser, Rudiger Urbanke, and Jame Larus for providing me support and funding during this work. I would like to personally thank Volkan Cevher, Quoc Tran-Dinh, and Matthias Seeger from EPFL for early discussions of this work and Marc Desgroseilliers from EPFL for checkin some proofs.	Bertsekas D. P., 1998, NONLINEAR PROGRANMMI; Challis E., 2011, INT C ART INT STAT; Friedlander MP, 2005, SIAM J OPTIMIZ, V15, P863, DOI 10.1137/S1052623402419789; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Honkela A., 2011, J MACHINE LEARNING R, V11, P3235; Jaakkola T., 1996, INT C ART INT STAT; Khan Emtiyaz, 2013, P 30 INT C MACH LEAR, P951; Khan M, 2012, THESIS; Khan M, 2012, INT C ART INT STAT; Khan M. E., 2012, ADV NEURAL INFORM PR; Lazaro-Gredilla M., 2011, INT C MACH LEARN; Marlin B., 2011, INT C MACH LEARN; MINKA T, 2001, P C UNC ART INT; Nickisch Hannes, 2008, J MACHINE LEARNING R, V9; Opper M, 2009, NEURAL COMPUT, V21, P786, DOI 10.1162/neco.2008.08-07-592; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Robinson SM., 1972, MATH PROGRAM, V3, P145, DOI DOI 10.1007/BF01584986; Schmidt MN, 2009, LECT NOTES COMPUT SC, V5441, P540, DOI 10.1007/978-3-642-00599-2_68; Seeger MW, 2008, J MACH LEARN RES, V9, P759; Seeger MW, 2011, SIAM J IMAGING SCI, V4, P166, DOI 10.1137/090758775; Seeger MW, 2009, J PHYS CONF SER, V197, DOI 10.1088/1742-6596/197/1/012001; Wright S., 1999, NUMERICAL OPTIMIZATI, V2	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103010
C	Kiros, R; Zemel, RS; Salakhutdinov, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Kiros, Ryan; Zemel, Richard S.; Salakhutdinov, Ruslan			A Multiplicative Model for Learning Distributed Text-Based Attribute Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. Attributes can correspond to a wide variety of concepts, such as document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation.	[Kiros, Ryan; Zemel, Richard S.; Salakhutdinov, Ruslan] Univ Toronto, Canadian Inst Adv Res, Toronto, ON, Canada	Canadian Institute for Advanced Research (CIFAR); University of Toronto	Kiros, R (corresponding author), Univ Toronto, Canadian Inst Adv Res, Toronto, ON, Canada.	rkiros@cs.toronto.edu; zemel@cs.toronto.edu; rsalakhu@cs.toronto.edu			NSERC; Google; Samsung; ONR [N00014-14-1-0232]	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Google(Google Incorporated); Samsung(Samsung); ONR(Office of Naval Research)	We would also like to thank the anonymous reviewers for their valuable comments and suggestions. This work was supported by NSERC, Google, Samsung, and ONR Grant N00014-14-1-0232.	Blunsom Phil, 2014, ACL; Bordes A., 2013, ADV NEURAL INFORM PR; Charlin Laurent, 2011, UAI; Collobert R., 2008, AUNIFIED ARCHITECTUR, P160; Dauphin Y., 2014, ICLR; Frome A., NIPS2013; Hermann Karl Moritz, 2014, ICLR; Kiros R., 2014, ICML; Klementiev Alexandre, 2012, P COLING 2012, P1459; Koehn P., 2005, P MT SUMM PHUK THAIL, VVolume 5, P79; Le Q., 2014, ICML; Memisevic Roland, 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383036; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Mnih A., 2007, INT C MACHINE LEARNI, P641, DOI DOI 10.1145/1273496.1273577; Pang B., 2005, P 43 ANN M ASS COMP, V43, P115, DOI DOI 10.3115/1219840.1219855; PEROZZI B, 2014, KDD; Ranzato M., 2010, P 13 INT C ART INT S, P621; Sarath Chandar A P, 2014, NIPS; Schler J., 2006, FRONT INFORM TECH EL, V274, P199; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Socher R., 2013, ADV NEURAL INFORM PR, P926, DOI DOI 10.1109/ICICIP.2013.6568119; Socher Richard, 2012, P 2012 JOINT C EMP M, P1201, DOI DOI 10.1162/153244303322533223; Socher Richard, 2011, P C EMP METH NAT LAN, P151; Sutskever Ilya, 2011, P 28 INT C MACH LEAR; Taylor Graham, 2009, P 26 ANN INT C MACH, DOI DOI 10.1145/1553374.1553505; Turian J, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P384	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101096
C	Kocaoglu, M; Shanmugam, K; Dimakis, AG; Klivans, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Kocaoglu, Murat; Shanmugam, Karthikeyan; Dimakis, Alexandras G.; Klivans, Adam			Sparse Polynomial Learning and Graph Sketching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Let f : {-1, 1}(n) -> R be a polynomial with at most s non-zero real coefficients. We give an algorithm for exactly reconstructing f given random examples from the uniform distribution on {-1, 1}(n) that runs in time polynomial in n and 2(s) and succeeds if the function satisfies the unique sign property: there is one output value which corresponds to a unique set of values of the participating parities. This sufficient condition is satisfied when every coefficient of f is perturbed by a small random noise, or satisfied with high probability when s parity functions are chosen randomly or when all the coefficients are positive. Learning sparse polynomials over the Boolean domain in time polynomial in n and 2(s) is considered notoriously hard in the worst-case. Our result shows that the problem is tractable for almost all sparse polynomials. Then, we show an application of this result to hypergraph sketching which is the problem of learning a sparse (both in the number of hyperedges and the size of the hyperedges) hypergraph from uniformly drawn random cuts. We also provide experimental results on a real world dataset.	[Kocaoglu, Murat; Shanmugam, Karthikeyan; Dimakis, Alexandras G.] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA; [Klivans, Adam] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin; University of Texas System; University of Texas Austin	Kocaoglu, M (corresponding author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.	mkocaoglu@utexas.edu; karthiksh@utexas.edu; dimakis@austin.utexas.edu; klivans@cs.utexas.edu	Dimakis, Alexandros G/A-5496-2011	Dimakis, Alexandros G/0000-0002-4244-7033; Kocaoglu, Murat/0000-0003-2447-2689	NSF [CCF 1422549, 1344364, 1344179]; DARPA STTR; ARO YIP award	NSF(National Science Foundation (NSF)); DARPA STTR; ARO YIP award	M.K, K.S. and A.D. acknowledge the support of NSF via CCF 1422549, 1344364, 1344179 and DARPA STTR and a ARO YIP award.	Akavia A, 2010, COLT, P381; Andoni A., 2014, P SODA; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Gilbert A.C., 2002, P ANN ACM S THEORY C, P152, DOI [10.1145/509907.509933, DOI 10.1145/509907.509933]; Gopalan P, 2008, ACM S THEORY COMPUT, P527; Kalai AT, 2009, ANN IEEE SYMP FOUND, P395, DOI 10.1109/FOCS.2009.60; KUSHILEVITZ E, 1993, SIAM J COMPUT, V22, P1331, DOI 10.1137/0222080; Li P., 2013, THESIS; MANSOUR Y, 1995, SIAM J COMPUT, V24, P357, DOI 10.1137/S0097539792239291; Negahban S, 2012, ANN ALLERTON CONF, P2032, DOI 10.1109/Allerton.2012.6483472; O'Donnell R, 2014, ANAL BOOLEAN FUNCTIO; Schapire R., 1996, JCSS J COMPUTER SYST, V52; Spielman D., 2004, JACM J ACM, V51; Stobbe Peter, 2012, P 15 INT C ART INT S, P1125	16	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101097
C	Kong, DG; Fujimaki, R; Liu, J; Nie, FP; Ding, C		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Kong, Deguang; Fujimaki, Ryohei; Liu, Ji; Nie, Feiping; Ding, Chris			Exclusive Feature Learning on Arbitrary Structures via l(1,2)-norm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				VARIABLE SELECTION; REGRESSION; SHRINKAGE; SPARSITY	Group LASSO is widely used to enforce the structural sparsity, which achieves the sparsity at the inter-group level. In this paper, we propose a new formulation called "exclusive group LASSO", which brings out sparsity at intra-group level in the context of feature selection. The proposed exclusive group LASSO is applicable on any feature structures, regardless of their overlapping or non-overlapping structures. We provide analysis on the properties of exclusive group LASSO, and propose an effective iteratively re-weighted algorithm to solve the corresponding optimization problem with rigorous convergence analysis. We show applications of exclusive group LASSO for uncorrelated feature selection. Extensive experiments on both synthetic and real-world datasets validate the proposed method.	[Kong, Deguang; Nie, Feiping; Ding, Chris] Univ Texas Arlington, Dept Comp Sci, Arlington, TX 76019 USA; [Fujimaki, Ryohei] NEC Labs Amer, Cupertino, CA 95014 USA; [Liu, Ji] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA	University of Texas System; University of Texas Arlington; NEC Corporation; University of Rochester	Kong, DG (corresponding author), Univ Texas Arlington, Dept Comp Sci, Arlington, TX 76019 USA.	doogkong@gmail.com; rfujimaki@nec-labs.com; jliu@cs.rochester.edu; feipingnie@gmail.com; chqding@uta.edu	Nie, Feiping/B-3039-2012					[Anonymous], P ICML 2009; Bach F., 2012, ICPRAM; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Habbema J. D. F., 1977, TECHNOMETRICS; Jenatton R, 2011, J MACH LEARN RES, V12, P2777; Ji SW, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P407; Kong D, 2011, P 20 ACM INT C INF K, P673; Kong D., 2014, AAAI, P1918; Kong D, 2013, IEEE DATA MINING, P379, DOI 10.1109/ICDM.2013.168; Liu J., 2014, ICML; Liu J., 2010, 23TH ANN C ADV NEURA, P1459; Nesterov Y, 2007, GRADIENT METHODS MIN; Nie F., 2010, ADV NEURAL INFORM PR, V1, P1813, DOI DOI 10.1007/978-3-319-10690-8_12; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Quattoni A., 2009, ICML, P108; Robnik-Sikonja M, 2003, MACH LEARN, V53, P23, DOI 10.1023/A:1025667309714; Roth V, 2004, IEEE T NEURAL NETWOR, V15, P16, DOI 10.1109/TNN.2003.809398; Steele J. M., 2004, MAA PROBLEM BOOK SER; Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Weston J., 2003, Journal of Machine Learning Research, V3, P1439, DOI 10.1162/153244303322753751; Yang T., 2012, ABS12015283 CORR; Yuan L., 2011, ADV NEURAL INFORM PR, P352; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Zhou JY, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P135, DOI 10.1145/2623330.2623711; Zhou Y., 2010, PROC 13 INT C ARTIF, P988; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	27	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103082
C	Koolen, WM; van Erven, T; Grunwald, PD		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Koolen, Wouter M.; van Erven, Tim; Grunwald, Peter D.			Learning the Learning Rate for Prediction with Expert Advice	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Most standard algorithms for prediction with expert advice depend on a parameter called the learning rate. This learning rate needs to be large enough to fit the data well, but small enough to prevent overfitting. For the exponential weights algorithm, a sequence of prior work has established theoretical guarantees for higher and higher data-dependent tunings of the learning rate, which allow for increasingly aggressive learning. But in practice such theoretical tunings often still perform worse (as measured by their regret) than ad hoc tuning with an even higher learning rate. To close the gap between theory and practice we introduce an approach to learn the learning rate. Up to a factor that is at most (poly) logarithmic in the number of experts and the inverse of the learning rate, our method performs as well as if we would know the empirically best learning rate from a large range that includes both conservative small values and values that are much higher than those for which formal guarantees were previously available. Our method employs a grid of learning rates, yet runs in linear time regardless of the size of the grid.	[Koolen, Wouter M.] Queensland Univ Technol, Brisbane, Qld, Australia; [Koolen, Wouter M.] Univ Calif Berkeley, Berkeley, CA USA; [van Erven, Tim; Grunwald, Peter D.] Leiden Univ, Leiden, Netherlands; [Grunwald, Peter D.] Ctr Wiskunde & Informat, Amsterdam, Netherlands	Queensland University of Technology (QUT); University of California System; University of California Berkeley; Leiden University; Leiden University - Excl LUMC	Koolen, WM (corresponding author), Queensland Univ Technol, Brisbane, Qld, Australia.	wouter.koolen@qut.edu.au; tim@timvanerven.nl; pdg@cwi.nl						Auer P, 2002, J COMPUT SYST SCI, V64, P48, DOI 10.1006/jcss.2001.1795; Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; de Rooij S, 2014, J MACH LEARN RES, V15, P1281; Devaine M, 2013, MACH LEARN, V90, P231, DOI 10.1007/s10994-012-5314-7; Erven T., 2011, ADV NEURAL INFORM PR, V24, P1656; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Grunwald P. D., 2012, P 23 INT C ALG LEARN; Vovk V, 2001, INT STAT REV, V69, P213; Vovk V, 1998, J COMPUT SYST SCI, V56, P153, DOI 10.1006/jcss.1997.1556	12	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100020
C	Koolen, WM; Malek, A; Bartlett, PL		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Koolen, Wouter M.; Malek, Alan; Bartlett, Peter L.			Efficient Minimax Strategies for Square Loss Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We consider online prediction problems where the loss between the prediction and the outcome is measured by the squared Euclidean distance and its generalization, the squared Mahalanobis distance. We derive the minimax solutions for the case where the prediction and action spaces are the simplex (this setup is sometimes called the Brier game) and the l(2) ball (this setup is related to Gaussian density estimation). We show that in both cases the value of each sub-game is a quadratic function of a simple statistic of the state, with coefficients that can be efficiently computed using an explicit recurrence relation. The resulting deterministic minimax strategy and randomized maximin strategy are linear functions of the statistic.	[Koolen, Wouter M.; Bartlett, Peter L.] Queensland Univ Technol, Brisbane, Qld, Australia; [Koolen, Wouter M.; Malek, Alan; Bartlett, Peter L.] Univ Calif Berkeley, Berkeley, CA 94720 USA	Queensland University of Technology (QUT); University of California System; University of California Berkeley	Koolen, WM (corresponding author), Queensland Univ Technol, Brisbane, Qld, Australia.	wouter.koolen@qut.edu.au; malek@eecs.berkeley.edu; peter@berkeley.edu						Abernethy Jacob, SERVEDIO AND ZHANG, P415; Bartlett Peter L., 2013, CORR; Brier G. W., 1950, MON WEATHER REV, V78, P1, DOI [10.1175/1520-0493(1950)0782.0.co;2, DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2]; Cesa-Bianchi N., 2011, P ADV NEURAL INFORM, P343; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Hedayati Fares, 2012, JMLR, P504; Kontkanen P, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1613; Koolen Wouter M., 2013, P 26 ANN C LEARN THE; Servedio Rocco A., 2008, 21 ANN C LEARN THEOR; Shtarkov YM, 1987, PROBL INFORM TRANSM, V23, P3; Sion M., 1958, PAC J MATH, V8, P171, DOI [10.2140/pjm.1958.8.171, DOI 10.2140/PJM.1958.8.171]; TAKIMOTO EIJI, 2000, 13 COLT, P100	12	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100022
C	Koyejo, O; Natarajan, N; Ravikumar, P; Dhillon, IS		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Koyejo, Oluwasanmi; Natarajan, Nagarajan; Ravikumar, Pradeep; Dhillon, Inderjit S.			Consistent Binary Classification with Generalized Performance Metrics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				RISK	Performance metrics for binary classification are designed to capture tradeoffs between four fundamental population quantities: true positives, false positives, true negatives and false negatives. Despite significant interest from theoretical and applied communities, little is known about either optimal classifiers or consistent algorithms for optimizing binary classification performance metrics beyond a few special cases. We consider a fairly large family of performance metrics given by ratios of linear combinations of the four fundamental population quantities. This family includes many well known binary classification metrics such as classification accuracy, AM measure, F-measure and the Jaccard similarity coefficient as special cases. Our analysis identifies the optimal classifiers as the sign of the thresholded conditional probability of the positive class, with a performance metric-dependent threshold. The optimal threshold can be constructed using simple plug-in estimators when the performance metric is a linear combination of the population quantities, but alternative techniques are required for the general case. We propose two algorithms for estimating the optimal classifiers, and prove their statistical consistency. Both algorithms are straightforward modifications of standard approaches to address the key challenge of optimal threshold selection, thus are simple to implement in practice. The first algorithm combines a plug-in estimate of the conditional probability of the positive class with optimal threshold selection. The second algorithm leverages recent work on calibrated asymmetric surrogate losses to construct candidate classifiers. We present empirical comparisons between these algorithms on benchmark datasets.	[Koyejo, Oluwasanmi] Stanford Univ, Dept Psychol, Stanford, CA 94305 USA; [Natarajan, Nagarajan; Ravikumar, Pradeep; Dhillon, Inderjit S.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	Stanford University; University of Texas System; University of Texas Austin	Koyejo, O (corresponding author), Stanford Univ, Dept Psychol, Stanford, CA 94305 USA.	sanmi@stanford.edu; naga86@cs.utexas.edu; pradeepr@cs.utexas.edu; inderjit@cs.utexas.edu			NSF [IIS-1149803, IIS-1320894, CCF-1117055, CCF-1320746]; ARO [W911NF-12-1-0390]	NSF(National Science Foundation (NSF)); ARO	This research was supported by NSF grant CCF-1117055 and NSF grant CCF-1320746. P.R. acknowledges the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1320894.	Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Boyd S, 2004, CONVEX OPTIMIZATION; Cai D., 2009, P 26 ANN INT C MACH, P105, DOI DOI 10.1145/1553374.1553388; Choi SS., 2010, J SYSTEMICS CYBERNET, V8910, P43; Devroye Luc P., 1996, PROBABILISTIC THEORY, V31; Drummond C, 2005, LECT NOTES ARTIF INT, V3720, P539, DOI 10.1007/11564096_52; Elkan C., 2001, INT JOINT C ART INT, P973; Gu Q, 2009, COMM COM INF SC, V51, P461, DOI 10.1007/978-3-642-04962-0_53; He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239; Lewis D. D., 1994, SIGIR '94. Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, P3; Lipton Zachary Chase, 1892, ABS14021892 ARXIV; Menon A, 2013, P 30 INT C MACH LEAR, V28, P603; Mika S., 1999, Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468), P41, DOI 10.1109/NNSP.1999.788121; Platt J., 1999, FAST TRAINING SUPPOR; Reid MD, 2010, J MACH LEARN RES, V11, P2387; Reid MD, 2009, P 26 ANN INT C MACH, P897; Scott C, 2012, ELECTRON J STAT, V6, P958, DOI 10.1214/12-EJS699; Sergeyev YD, 1998, MATH PROGRAM, V81, P127, DOI 10.1007/BF01584848; Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002; Tewari A, 2007, J MACH LEARN RES, V8, P1007; Webb S., 2006, CEAS; Ye N, 2012, P INT C MACH LEARN; Zhang T, 2004, J MACH LEARN RES, V5, P1225; Zhao MJ, 2013, J MACH LEARN RES, V14, P1033	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102015
C	Kumar, MP		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Kumar, M. Pawan			Rounding-based Moves for Metric Labeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Metric labeling is a special case of energy minimization for pairwise Markov random fields. The energy function consists of arbitrary unary potentials, and pair-wise potentials that are proportional to a given metric distance function over the label set. Popular methods for solving metric labeling include (i) move-making algorithms, which iteratively solve a minimum st-cut problem; and (ii) the linear programming (LP) relaxation based approach. In order to convert the fractional solution of the LP relaxation to an integer solution, several randomized rounding procedures have been developed in the literature. We consider a large class of parallel rounding procedures, and design move-making algorithms that closely mimic them. We prove that the multiplicative bound of a move-making algorithm exactly matches the approximation factor of the corresponding rounding procedure for any arbitrary distance function. Our analysis includes all known results for move-making algorithms as special cases.	[Kumar, M. Pawan] Ecole Cent Paris, Chatenay Malabry, France; [Kumar, M. Pawan] INRIA Saclay, Palaiseau, France	UDICE-French Research Universities; Universite Paris Saclay	Kumar, MP (corresponding author), Ecole Cent Paris, Chatenay Malabry, France.	pawan.kumar@ecp.fr			European Research Council under the European Community's Seventh Framework Programme (FP7/2007-2013)/ERC [259112]	European Research Council under the European Community's Seventh Framework Programme (FP7/2007-2013)/ERC	This work is funded by the European Research Council under the European Community's Seventh Framework Programme (FP7/2007-2013)/ERC Grant agreement number 259112.	Archer A., 2004, SODA; Boykov Y., 2004, PAMI; Boykov Y., 1999, ICCV; BOYKOV Y, 1998, CVPR; Chekuri C., 2001, SODA; Flach B., 2006, TECHNICAL REPORT; Globerson Amir, 2008, ADV NEURAL INFORM PR, P553; GUPTA A, 2000, STOC; Kleinberg J., 1999, STOC; Kolmogorov V., 2006, PAMI; KOMODAKIS N, 2007, ICCV; Koster A., 1998, OPERATIONS RES LETT; KUMAR MP, 2009, UAI; KUMAR MP, 2008, NIPS; RAVIKUMAR P, 2008, ICML; Schlesinger M. I., 1976, KIBERNETIKA; SZELISKI R, 2008, PAMI; Tarlow D., 2011, ICML; VEKSLER O, 2007, CVPR; Veksler O., 1999, THESIS; Wainwright M. J., 2005, T INFORM THEORY; Weiss Y., 2007, UAI; Werner T., 2010, PAMI; Werner T., 2007, PAMI; [No title captured]	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101025
C	Kuznetsov, V; Mohri, M; Syed, U		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Kuznetsov, Vitaly; Mohri, Mehryar; Syed, Umar			Multi-Class Deep Boosting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				CONSISTENCY; REGRESSION	We present new ensemble learning algorithms for multi-class classification. Our algorithms can use as a base classifier set a family of deep decision trees or other rich or complex families and yet benefit from strong generalization guarantees. We give new data-dependent learning bounds for convex ensembles in the multi-class classification setting expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set, and the mixture weight assigned to each sub-family. These bounds are finer than existing ones both thanks to an improved dependency on the number of classes and, more crucially, by virtue of a more favorable complexity term expressed as an average of the Rademacher complexities based on the ensemble's mixture weights. We introduce and discuss several new multi-class ensemble algorithms benefiting from these guarantees, prove positive results for the H-consistency of several of them, and report the results of experiments showing that their performance compares favorably with that of multi-class versions of AdaBoost and Logistic Regression and their L-1-regularized counterparts.	[Kuznetsov, Vitaly; Mohri, Mehryar] Courant Inst, 251 Mercer St, New York, NY 10012 USA; [Mohri, Mehryar] Google Res, New York, NY 10012 USA; [Syed, Umar] Google Res, New York, NY 10011 USA	Google Incorporated; Google Incorporated	Kuznetsov, V (corresponding author), Courant Inst, 251 Mercer St, New York, NY 10012 USA.	vitaly@cims.nyu.edu; mohri@cims.nyu.edu; usyed@google.com			NSF [IIS-1117591]; NSERC PGS grant	NSF(National Science Foundation (NSF)); NSERC PGS grant	We thank Andres Munoz Medina and Scott Yang for discussions and help with the experiments. This work was partly funded by the NSF award IIS-1117591 and supported by a NSERC PGS grant.	Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; Collins M, 2002, MACH LEARN, V48, P253, DOI 10.1023/A:1013912006537; Cortes C, 2014, PR MACH LEARN RES, V32, P1179; Dietterich TG, 2000, MACH LEARN, V40, P139, DOI 10.1023/A:1007607513941; Duchi J., 2009, ICML, P38; Duffy N., 1999, ADV NEURAL INFORM PR, P258; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman J., 1998, ANN STAT, V28, P2000; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Grove AJ, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P692; Kivinen J., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P134, DOI 10.1145/307400.307424; Koltchinskii V, 2002, ANN STAT, V30, P1; Ledoux M., 1991, PROBABILITY BANACH S, DOI [10.1007/978-3-642-20212-4, DOI 10.1007/978-3-642-20212-4]; Li P., 2009, INT C MACH LEARN ICM, P79; Li P., 2009, TECHNICAL REPORT; Long P., 2013, P INT C MACHINE LEAR, P801; LUO ZQ, 1992, J OPTIMIZ THEORY APP, V72, P7, DOI 10.1007/BF00939948; Mason L., 1999, NIPS; Mohri M., 2018, FDN MACHINE LEARNING; Mukherjee I, 2013, J MACH LEARN RES, V14, P437; Ratsch G, 2005, J MACH LEARN RES, V6, P2131; Ratsch G., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P334; Ratsch G, 2001, MACH LEARN, V42, P287, DOI 10.1023/A:1007618119488; Ratsch Gunnar, 2001, NIPS, P487; Schapire R.E., 1997, P 14 INT C MACHINE L, P322; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901; Schapire RE, 1999, LECT NOTES ARTIF INT, V1720, P13; Schapire Robert E., 2012, BOOSTING FDN ALGORIT; Sun P., 2012, ICML; Tewari A, 2007, J MACH LEARN RES, V8, P1007; Warmuth MK, 2006, P 23 INT C MACHINE L, P1001, DOI DOI 10.1145/1143844.1143970; Zhang T, 2004, ANN STAT, V32, P56; Zhang T, 2004, J MACH LEARN RES, V5, P1225; Zhu J., 2009, STAT ITS INTERFACE; Zou H, 2008, ANN APPL STAT, V2, P1290, DOI 10.1214/08-AOAS198	35	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102075
C	Lafond, J; Klopp, O; Moulines, E; Salmon, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Lafond, Jean; Klopp, Olga; Moulines, Eric; Salmon, Joseph			Probabilistic low-rank matrix completion on finite alphabets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ALGORITHMS	The task of reconstructing a matrix given a sample of observed entries is known as the matrix completion problem. It arises in a wide range of problems, including recommender systems, collaborative filtering, dimensionality reduction, image processing, quantum physics or multi-class classification to name a few. Most works have focused on recovering an unknown real-valued low-rank matrix from randomly sub-sampling its entries. Here, we investigate the case where the observations take a finite number of values, corresponding for examples to ratings in recommender systems or labels in multi-class classification. We also consider a general sampling scheme (not necessarily uniform) over the matrix entries. The performance of a nuclear-norm penalized estimator is analyzed theoretically. More precisely, we derive bounds for the Kullback-Leibler divergence between the true and estimated distributions. In practice, we have also proposed an efficient algorithm based on lifted coordinate gradient descent in order to tackle potentially high dimensional settings.	[Lafond, Jean; Moulines, Eric; Salmon, Joseph] Telecom ParisTech, CNRS LTCI, Inst Mines Telecom, Paris, France; [Klopp, Olga] Univ Paris Quest, CREST, Paris, France; [Klopp, Olga] Univ Paris Quest, MODALX, Paris, France	Centre National de la Recherche Scientifique (CNRS); IMT - Institut Mines-Telecom; IMT Atlantique; Institut Polytechnique de Paris	Lafond, J (corresponding author), Telecom ParisTech, CNRS LTCI, Inst Mines Telecom, Paris, France.	jean.lafond@telecom-paristech.fr; Olga.KLOPP@math.cnrs.fr; moulines@telecom-paristech.fr; joseph.salmon@telecom-paristech.fr			Direction Generale de l'Armement (DGA); labex LMH in the framework of the "Programme des Investissements d'Avenir" [ANR-11-LABX-0056-LMH]; Chair Machine Learning for Big Data	Direction Generale de l'Armement (DGA); labex LMH in the framework of the "Programme des Investissements d'Avenir"(French National Research Agency (ANR)); Chair Machine Learning for Big Data	Jean Lafond is grateful for fundings from the Direction Generale de l'Armement (DGA) and to the labex LMH through the grant no ANR-11-LABX-0056-LMH in the framework of the "Programme des Investissements d'Avenir". Joseph Salmon acknowledges Chair Machine Learning for Big Data for partial financial support. The authors would also like to thank Alexandre Gramfort for helpful discussions.	Bhatia R., 1997, MATRIX ANAL, V169, DOI DOI 10.1007/978-1-4612-0653-8; Bobadilla J, 2013, KNOWL-BASED SYST, V46, P109, DOI 10.1016/j.knosys.2013.03.012; Boyd S, 2004, CONVEX OPTIMIZATION; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Cai T. T., 2013, CORR; Cai T, 2013, J MACH LEARN RES, V14, P3619; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Davenport M. A., 2012, CORR; Dudik M., 2012, AISTATS; Fazel M., 2002, MATRIX RANK MINIMIZA; Foygel R., 2011, ADV NEURAL INFORM PR, V24, P2133; Golub G., 2013, MATRIX COMPUTATIONS; Gross D, 2011, IEEE T INFORM THEORY, V57, P1548, DOI 10.1109/TIT.2011.2104999; Harchaoui Z., 2014, MATH PROGRAM, V13, P1; Klopp O, 2014, BERNOULLI, V20, P282, DOI 10.3150/12-BEJ486; Koltchinskii V, 2011, ANN STAT, V39, P2302, DOI 10.1214/11-AOS894; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Mazumder R, 2010, J MACH LEARN RES, V11, P2287; Negahban S, 2012, J MACH LEARN RES, V13, P1665; Todeschini A., 2013, ADV NEURAL INFORM PR, V26, P845	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101029
C	Lakshminarayanan, B; Roy, DM; Teh, YW		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Lakshminarayanan, Balaji; Roy, Daniel M.; Teh, Yee Whye			Mondrian Forests: Efficient Online Random Forests	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Ensembles of randomized decision trees, usually referred to as random forests, are widely used for classification and regression tasks in machine learning and statistics. Random forests achieve competitive predictive performance and are computationally efficient to train and test, making them excellent candidates for real-world prediction tasks. The most popular random forest variants (such as Breiman's random forest and extremely randomized trees) operate on batches of training data. Online methods are now in greater demand. Existing online random forests, however, require more training data than their batch counterpart to achieve comparable predictive performance. In this work, we use Mondrian processes (Roy and Teh, 2009) to construct ensembles of random decision trees we call Mondrian forests. Mondrian forests can be grown in an incremental/online fashion and remarkably, the distribution of online Mondrian forests is the same as that of batch Mondrian forests. Mondrian forests achieve competitive predictive performance comparable with existing online random forests and periodically retrained batch random forests, while being more than an order of magnitude faster, thus representing a better computation vs accuracy tradeoff.	[Lakshminarayanan, Balaji] UCL, Gatsby Unit, London, England; [Roy, Daniel M.] Univ Cambridge, Dept Engn, Cambridge, England; [Teh, Yee Whye] Univ Oxford, Dept Stat, Oxford, England	University of London; University College London; University of Cambridge; University of Oxford	Lakshminarayanan, B (corresponding author), UCL, Gatsby Unit, London, England.				Gatsby Charitable Foundation; Research Fellowship at Emmanuel College, Cambridge; Newton International Fellowship through the Royal Society; European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC [617411]	Gatsby Charitable Foundation; Research Fellowship at Emmanuel College, Cambridge; Newton International Fellowship through the Royal Society; European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC(European Research Council (ERC))	We would like to thank Charles Blundell, Gintare Dziugaite, Creighton Heaukulani, Jose Miguel Hernandez-Lobato, Maria Lomeli, Alex Smola, Heiko Strathmann and Srini Turaga for helpful discussions and feedback on drafts. BL gratefully acknowledges generous funding from the Gatsby Charitable Foundation. This research was carried out in part while DMR held a Research Fellowship at Emmanuel College, Cambridge, with funding also from a Newton International Fellowship through the Royal Society. YWT's research leading to these results was funded in part by the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617411.	Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Caruana R., 2006, P INT C MACH LEARN I; Chipman HA, 1998, J AM STAT ASSOC, V93, P935, DOI 10.2307/2669832; CRAWFORD SL, 1989, INT J MAN MACH STUD, V31, P197, DOI 10.1016/0020-7373(89)90027-8; Criminisil A, 2011, FOUND TRENDS COMPUT, V7, P81, DOI [10.1561/0600000035, 10.1501/0000000035]; Cutler A., 2001, COMPUTING SCI STAT, V33, P490; Denil M., 2013, P INT C MACH LEARN I; Denison DGT, 1998, BIOMETRIKA, V85, P363; Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1; Domingos P., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P71, DOI 10.1145/347090.347107; Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1; Goodman JT, 2001, COMPUT SPEECH LANG, V15, P403, DOI 10.1006/csla.2001.0174; Lakshminarayanan B., 2013, P INT C MACH LEARN I; Minka T., 2000, BAYESIAN MODEL AVERA; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Pitman J., 2006, COMBINATORIAL STOCHA, V32; ROY D, 2009, ADV NEURAL INF PROCE, V21, P27; Roy D. M., 2011, THESIS; Saffari A., 2009, COMP VIS WORKSH ICCV; Taddy MA, 2011, J AM STAT ASSOC, V106, P109, DOI 10.1198/jasa.2011.ap09769; Teh YW, 2006, COLING/ACL 2006, VOLS 1 AND 2, PROCEEDINGS OF THE CONFERENCE, P985; Utgoff P. E., 1989, Machine Learning, V4, P161, DOI 10.1023/A:1022699900025; Wood F., 2009, P INT C MACH LEARN I	23	0	0	0	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100013
C	Lawlor, M; Zucker, SW		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Lawlor, Matthew; Zucker, Steven W.			Feedforward Learning of Mixture Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				TIMING-DEPENDENT PLASTICITY; SPIKE	We develop a biologically-plausible learning rule that provably converges to the class means of general mixture models. This rule generalizes the classical BCM neural rule within a tensor framework, substantially increasing the generality of the learning problem it solves. It achieves this by incorporating triplets of samples from the mixtures, which provides a novel information processing interpretation to spike-timing-dependent plasticity. We provide both proofs of convergence, and a close fit to experimental data on STDP.	[Lawlor, Matthew] Yale Univ, Appl Math, New Haven, CT 06520 USA; [Zucker, Steven W.] Yale Univ, Comp Sci, New Haven, CT 06520 USA	Yale University; Yale University	Lawlor, M (corresponding author), Yale Univ, Appl Math, New Haven, CT 06520 USA.	mflawlor@gmail.com; zucker@cs.yale.edu			NSF; NIH; Paul Allen Foundation; Simons Foundation	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Paul Allen Foundation; Simons Foundation	Research supported by NSF, NIH, The Paul Allen Foundation, and The Simons Foundation.	Anandkumar A., 2012, ARXIV12107559; Anandkumar A., 2012, ABS12046703 CORR; Bi GQ, 2002, PHYSIOL BEHAV, V77, P551, DOI 10.1016/S0031-9384(02)00933-2; Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998; BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982; Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639; Delyon B, 1996, IEEE T AUTOMAT CONTR, V41, P1245, DOI 10.1109/9.536495; Doob J. L., 1953, STOCHASTIC PROCESSES, V101; Froemke RC, 2002, NATURE, V416, P433, DOI 10.1038/416433a; Gjorgjieva J, 2011, P NATL ACAD SCI USA, V108, P19383, DOI 10.1073/pnas.1105933108; Hebb D.O., 1949, ORG BEHAV NEUROPSYCH; INTRATOR N, 1992, NEURAL NETWORKS, V5, P3, DOI 10.1016/S0893-6080(05)80003-6; Lawlor M., 2013, NIPS, P1763; Lawlor Matthew, 2014, ONLINE ALGORITHM LEA; LEVY WB, 1983, NEUROSCIENCE, V8, P791, DOI 10.1016/0306-4522(83)90010-6; Nessler B., 2009, ADV NEURAL INFORM PR, V22, P1357; Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006; Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829; Wang HX, 2005, NAT NEUROSCI, V8, P187, DOI 10.1038/nn1387	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101008
C	Lee, JD; Taylor, JE		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Lee, Jason D.; Taylor, Jonathan E.			Exact Post Model Selection Inference for Marginal Screening	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ESTIMATORS	We develop a framework for post model selection inference, via marginal screening, in linear regression. At the core of this framework is a result that characterizes the exact distribution of linear functions of the response y, conditional on the model being selected ("condition on selection" framework). This allows us to construct valid confidence intervals and hypothesis tests for regression coefficients that account for the selection procedure. In contrast to recent work in high-dimensional statistics, our results are exact (non-asymptotic) and require no eigenvalue-like assumptions on the design matrix X. Furthermore, the computational cost of marginal regression, constructing confidence intervals and hypothesis testing is negligible compared to the cost of linear regression, thus making our methods particularly suitable for extremely large datasets. Although we focus on marginal screening to illustrate the applicability of the condition on selection framework, this framework is much more broadly applicable. We show how to apply the proposed framework to several other selection procedures including orthogonal matching pursuit and marginal screening+Lasso.	[Lee, Jason D.] Stanford Univ, Computat & Math Engn, Stanford, CA 94305 USA; [Taylor, Jonathan E.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Stanford University; Stanford University	Lee, JD (corresponding author), Stanford Univ, Computat & Math Engn, Stanford, CA 94305 USA.	jdl17@stanford.edu; jonathan.taylor@stanford.edu						Benjamini Y, 2005, J AM STAT ASSOC, V100, P71, DOI 10.1198/016214504000001907; Berk R, 2013, ANN STAT, V41, P802, DOI 10.1214/12-AOS1077; Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Buhlmann Peter, 2014, STATISTICS, V1; Casella G., 1990, STAT INFERENCE, V70; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Elisseeff A., 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616; Fan JQ, 2008, J R STAT SOC B, V70, P849, DOI 10.1111/j.1467-9868.2008.00674.x; Genovese CR, 2012, J MACH LEARN RES, V13, P2107; Javanmard A, 2013, ARXIV13063171; Lee J. D., 2013, ADV NEURAL INFO PROC, P342; Lee J.D., 2013, ARXIV13116238; Leeb H, 2005, ECONOMET THEOR, V21, P21, DOI 10.1017/S0266466605050036; Leek Jeff, PREDICTION LASSO VS; Lehmann E., 2005, TESTING STAT HYPOTHE, V3rd; Meinshausen Nicolai, 2009, J AM STAT ASS, V104; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Reid S., 2013, ARXIV13115274; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; vande Geer S, 2013, ARXIV13030518; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018; Wasserman L, 2009, ANN STAT, V37, P2178, DOI 10.1214/08-AOS646; Zhang C-H, 2011, ARXIV11102563; Zhao P, 2006, J MACH LEARN RES, V7, P2541	26	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100071
C	Liang, JW; Fadili, JM; Peyre, G		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Liang, Jingwei; Fadili, Jalal M.; Peyre, Gabriel			Local Linear Convergence of Forward-Backward under Partial Smoothness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				SIGNAL RECOVERY; REGRESSION; SELECTION	In this paper, we consider the Forward-Backward proximal splitting algorithm to minimize the sum of two proper closed convex functions, one of which having a Lipschitz continuous gradient and the other being partly smooth relative to an active manifold M. We propose a generic framework under which we show that the Forward-Backward (i) correctly identifies the active manifold M in a finite number of iterations, and then (ii) enters a local linear convergence regime that we characterize precisely. This gives a grounded and unified explanation to the typical behaviour that has been observed numerically for many problems encompassed in our framework, including the Lasso, the group Lasso, the fused Lasso and the nuclear norm regularization to name a few. These results may have numerous applications including in signal/image processing processing, sparse recovery and machine learning.	[Liang, Jingwei; Fadili, Jalal M.] Univ Caen, ENSICAEN, CNRS, GREYC, Caen, France; [Peyre, Gabriel] Univ Paris 09, CNRS, CEREMADE, Paris, France	Centre National de la Recherche Scientifique (CNRS); Universite de Caen Normandie; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Universite Paris-Dauphine	Liang, JW (corresponding author), Univ Caen, ENSICAEN, CNRS, GREYC, Caen, France.	Jingwei.Liang@greyc.ensicaen.fr; Jalal.Fadili@greyc.ensicaen.fr; Gabriel.Peyre@ceremade.dauphine.fr	Peyré, Gabriel/P-2438-2015	Peyré, Gabriel/0000-0002-4477-0387				Agarwal A, 2012, ANN STAT, V40, P2452, DOI 10.1214/12-AOS1032; [Anonymous], 2007, ALGORITHMIC OPER RES; Bach FR, 2008, J MACH LEARN RES, V9, P1019; Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; Bredies K, 2008, J FOURIER ANAL APPL, V14, P813, DOI 10.1007/s00041-008-9041-1; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chambolle A., 2012, IMAGE PROCESSING ANA; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Combettes PL, 2005, MULTISCALE MODEL SIM, V4, P1168, DOI 10.1137/050626090; Daniilidis A., 2014, SIAM J MATRIX ANAL A; Davies PL, 2001, ANN STAT, V29, P1, DOI 10.1214/aos/996986501; Grave E., 1990, ARXIV11091990; Hale ET, 2008, SIAM J OPTIMIZ, V19, P1107, DOI 10.1137/070698920; Hare WL, 2011, SPRINGER SER OPTIM A, V49, P261, DOI 10.1007/978-1-4419-9569-8_13; Hare WL, 2004, J CONVEX ANAL, V11, P251; Lewis AS, 2008, MATH OPER RES, V33, P216, DOI 10.1287/moor.1070.0291; Lewis AS, 2003, SIAM J OPTIMIZ, V13, P702, DOI 10.1137/S1052623401387623; Miller SA, 2005, MATH PROGRAM, V104, P609, DOI 10.1007/s10107-005-0631-2; Poliquin RA, 2000, T AM MATH SOC, V352, P5231, DOI 10.1090/S0002-9947-00-02550-2; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tseng P., 2009, MATH PROG B, V117; Vaiter S., 2013, MODEL SELECTION LOW; Vaiter S., 2014, MODEL CONSISTENCY PA; WRIGHT SJ, 1993, SIAM J CONTROL OPTIM, V31, P1063, DOI 10.1137/0331048; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x	29	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100039
C	Lim, CH; Wright, SJ		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Lim, Cong Han; Wright, Stephen J.			Beyond the Birkhoff Polytope: Convex Relaxations for Vector Permutation Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The Birkhoff polytope (the convex hull of the set of permutation matrices), which is represented using Theta(n(2)) variables and constraints, is frequently invoked in formulating relaxations of optimization problems over permutations. Using a recent construction of Goemans [1], we show that when optimizing over the convex hull of the permutation vectors (the permutahedron), we can reduce the number of variables and constraints to Theta(n log n) in theory and Theta(n log(2) n) in practice. We modify the recent convex formulation of the 2-SUM problem introduced by Fogel et al. [2] to use this polytope, and demonstrate how we can attain results of similar quality in significantly less computational time for large n. To our knowledge, this is the first usage of Goemans' compact formulation of the permutahedron in a convex optimization problem. We also introduce a simpler regularization scheme for this convex formulation of the 2-SUM problem that yields good empirical results.	[Lim, Cong Han; Wright, Stephen J.] Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison	Lim, CH (corresponding author), Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.	conghan@cs.wisc.edu; swright@cs.wisc.edu			NSF [DMS-1216318, DMS-0914524]; ExxonMobil; ONR [N00014-13-1-0129]; DOE [DE-SC0002283]; AFOSR [FA9550-13-1-0138]; Argonne National Laboratory [3F-30222]	NSF(National Science Foundation (NSF)); ExxonMobil(Exxon Mobil Corporation); ONR(Office of Naval Research); DOE(United States Department of Energy (DOE)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); Argonne National Laboratory(United States Department of Energy (DOE))	We thank Okan Akalin and Taedong Kim for helpful comments and suggestions for the experiments. We thank the anonymous referees for feedback that improved the paper's presentation. We also thank the authors of [2] for sharing their experimental code, and Fajwal Fogel for helpful discussions. Lim's work on this project was supported in part by NSF Awards DMS-0914524 and DMS-1216318, and a grant from ExxonMobil. Wright's work was supported in part by NSF Award DMS-1216318, ONR Award N00014-13-1-0129, DOE Award DE-SC0002283, AFOSR Award FA9550-13-1-0138, and Subcontract 3F-30222 from Argonne National Laboratory.	Ajtai M., 1983, P 15 ANN ACM S THEOR, P1; [Anonymous], 2014, GUROBI OPTIMIZER REF; Atkins JE, 1998, SIAM J COMPUT, V28, P297, DOI 10.1137/S0097539795285771; Batcher K.E., 1968, P APRIL 30 MAY2 1968, P307, DOI [DOI 10.1145/1468075.1468121, 10.1145/1468075.1468121]; Cormen T. H., 2009, INTRO ALGORITHMS, V3rd; Ding C, 2004, P 21 INT C MACHINE L, P30, DOI [10.1145/1015330.1015407, DOI 10.1145/1015330.1015407]; Fiori M., 2013, ADV NEURAL INFORM PR, V26, P127; Fogel F., 2013, ADV NEURAL INFORM PR, P1016; George A, 1997, SIAM J MATRIX ANAL A, V18, P706, DOI 10.1137/S089547989427470X; Goemans M., 2010, WORKING PAPER; Grant MC, 2008, LECT NOTES CONTR INF, V371, P95, DOI 10.1007/978-1-84800-155-8_7; Liiv Innar, 2010, Statistical Analysis and Data Mining, V3, P70, DOI 10.1002/sam.10071; Lim C. H., 2014, ARXIV14076609; Robinson WS, 1951, AM ANTIQUITY, V16, P293, DOI 10.2307/276978; Sokal R, 1963, PRINCIPLES NUMERICAL	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103062
C	Lim, SH; Chen, YD; Xu, H		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Lim, Shiau Hong; Chen, Yudong; Xu, Huan			Clustering from Labels and Time-Varying Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ALGORITHMS	We present a general framework for graph clustering where a label is observed to each pair of nodes. This allows a very rich encoding of various types of pairwise interactions between nodes. We propose a new tractable approach to this problem based on maximum likelihood estimator and convex optimization. We analyze our algorithm under a general generative model, and provide both necessary and sufficient conditions for successful recovery of the underlying clusters. Our theoretical results cover and subsume a wide range of existing graph clustering results including planted partition, weighted clustering and partially observed graphs. Furthermore, the result is applicable to novel settings including time-varying graphs such that new insights can be gained on solving these problems. Our theoretical findings are further supported by empirical results on both synthetic and real data.	[Lim, Shiau Hong; Xu, Huan] Natl Univ Singapore, Singapore, Singapore; [Chen, Yudong] Univ Calif Berkeley, EECS, Berkeley, CA 94720 USA	National University of Singapore; University of California System; University of California Berkeley	Lim, SH (corresponding author), Natl Univ Singapore, Singapore, Singapore.	mpelsh@nus.edu.sg; yudong.chen@eecs.berkeley.edu; mpexuh@nus.edu.sg			Ministry of Education of Singapore through AcRF Tier Two grant [R-265-000-443-112]; NSF [CIF-31712-23800]; ONR MURI grant [N00014-11-1-0688]	Ministry of Education of Singapore through AcRF Tier Two grant(Ministry of Education, Singapore); NSF(National Science Foundation (NSF)); ONR MURI grant	S.H. Lim and H. Xu were supported by the Ministry of Education of Singapore through AcRF Tier Two grant R-265-000-443-112. Y. Chen was supported by NSF grant CIF-31712-23800 and ONR MURI grant N00014-11-1-0688.	Ames BPW, 2011, MATH PROGRAM, V129, P69, DOI 10.1007/s10107-011-0459-x; Ames Brendan P. W., 2013, MATH PROGRAM, P1; Anandkumar A., 2013, ARXIV13022684; Balakrishnan S., 2011, NEURIPS 2011 WORKSH, V4; Bansal Nikhil, 2004, MACHINE LEARNING, V56; Chakrabarti D., 2006, P 12 ACM SIGKDD INT, P554, DOI [10.1145/1150402.1150467, DOI 10.1145/1150402.1150467]; Chaudhuri K., 2012, COLT; Chen Y., 2014, ICML; Chen Y., 2012, NIPS 2012; Chen YD, 2014, J MACH LEARN RES, V15, P2213; Condon A, 2001, RANDOM STRUCT ALGOR, V18, P116, DOI 10.1002/1098-2418(200103)18:2<116::AID-RSA1001>3.0.CO;2-2; Eagle N, 2006, PERS UBIQUIT COMPUT, V10, P255, DOI 10.1007/s00779-005-0046-3; Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002; Heimlicher S., 2012, NIPS WORKSH ALG STAT; Jalali A., 2011, ICML; Kawadia V, 2012, SCI REP-UK, V2, DOI 10.1038/srep00794; Kolar M., 2011, ADV NEURAL INFORM PR, V24, P909; Lelarge Marc, 2013, IEEE INF THEOR WORKS IEEE INF THEOR WORKS; Lin Z, 2009, UILUENG092215 UIUC; Mathieu C, 2010, PROC APPL MATH, V135, P712; McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929; Nguyen NP, 2011, IEEE INFOCOM SER, P2282, DOI 10.1109/INFCOM.2011.5935045; Oymak Samet, 2011, ARXIV11045186V1; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; Sun Jimeng, 2007, ACM KDD; Topsoe F, 2000, IEEE T INFORM THEORY, V46, P1602, DOI 10.1109/18.850703; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101002
C	Lindsey, RV; Khajah, M; Mozer, MC		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Lindsey, Robert, V; Khajah, Mohammad; Mozer, Michael C.			Automatic Discovery of Cognitive Skills to Improve the Prediction of Student Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					To master a discipline such as algebra or physics, students must acquire a set of cognitive skills. Traditionally, educators and domain experts use intuition to determine what these skills are and then select practice exercises to hone a particular skill. We propose a technique that uses student performance data to automatically discover the skills needed in a discipline. The technique assigns a latent skill to each exercise such that a student's expected accuracy on a sequence of same-skill exercises improves monotonically with practice. Rather than discarding the skills identified by experts, our technique incorporates a nonparametric prior over the exercise-skill assignments that is based on the expert-provided skills and a weighted Chinese restaurant process. We test our technique on datasets from five different intelligent tutoring systems designed for students ranging in age from middle school through college. We obtain two surprising results. First, in three of the five datasets, the skills inferred by our technique support significantly improved predictions of student performance over the expert-provided skills. Second, the expert-provided skills have little value: our technique predicts student performance nearly as well when it ignores the domain expertise as when it attempts to leverage it. We discuss explanations for these surprising results and also the relationship of our skill-discovery technique to alternative approaches.	[Lindsey, Robert, V] Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA; Univ Colorado, Inst Cognit Sci, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder; University of Colorado System; University of Colorado Boulder	Lindsey, RV (corresponding author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.				NSF [BCS-0339103, BCS-720375]; NSF Graduate Research Fellowship	NSF(National Science Foundation (NSF)); NSF Graduate Research Fellowship(National Science Foundation (NSF))	This research was supported by NSF grants BCS-0339103 and BCS-720375 and by an NSF Graduate Research Fellowship to R. L.	Aldous D.J., 1985, LECT NOTES MATH, V1117, P1, DOI DOI 10.1007/BFB0099421; ATKINSON RC, 1972, J EXP PSYCHOL, V96, P124, DOI 10.1037/h0033475; Barnes T., 2005, P 2005 AAAI ED DAT M; Cen H, 2006, LECT NOTES COMPUT SC, V4053, P164; CORBETT AT, 1994, USER MODEL USER-ADAP, V4, P253, DOI 10.1007/BF01099821; Corbett AT, 1997, HDB HUMAN COMPUTER I, P849, DOI DOI 10.1016/B978-044481862-1.50103-5; Gonzalez-Brenes J., 2012, P 5 INT C ED DAT MIN; Gonzalez-Brenes J., 2013, P 6 INT C ED DAT MIN; Hannah LA, 2011, J MACH LEARN RES, V12, P1923; Ishwaran H, 2003, STAT SINICA, V13, P1211; Khajah M., 2014, EDM 2014; Koedinger K., 2010, HDB ED DATA MINING; Koedinger KR, 2012, COGNITIVE SCI, V36, P757, DOI 10.1111/j.1551-6709.2012.01245.x; Lan A. S., 2014, ACM SIGKDD C KNOWL D; Lindsey RV, 2014, PSYCHOL SCI, V25, P639, DOI 10.1177/0956797613504302; Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653; Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461; Pardos Zachary A., 2012, Intelligent Tutoring Systems. Proceedings 11th International Conference (ITS 2012), P405, DOI 10.1007/978-3-642-30950-2_52; Pardos ZA, 2011, LECT NOTES COMPUT SC, V6787, P243, DOI 10.1007/978-3-642-22362-4_21; Rafferty A., 2011, P 15 INT C AI ED; Smith AC, 2004, J NEUROSCI, V24, P447, DOI 10.1523/JNEUROSCI.2908-03.2004; Sohl-Dickstein J., 2013, NIPS WORKSH DAT DRIV; Teh Y. W., 2011, ADV NEURAL INFORM PR; Thai-Nghe N., 2011, ED RECOMMENDER SYSTE, P129; Whitehill J.R, 2012, THESIS	25	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103005
C	Liu, H; Wang, L; Zhao, T		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Liu, Han; Wang, Lie; Zhao, Tuo			Multivariate Regression with Calibration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				LASSO; RECOVERY; FRAMEWORK; SELECTION; MODEL	We propose a new method named calibrated multivariate regression (CMR) for fitting high dimensional multivariate regression models. Compared to existing methods, CMR calibrates the regularization for each regression task with respect to its noise level so that it is simultaneously tuning insensitive and achieves an improved finite-sample performance. Computationally, we develop an efficient smoothed proximal gradient algorithm which has a worst-case iteration complexity O(1/epsilon), where. is a pre-specified numerical accuracy. Theoretically, we prove that CMR achieves the optimal rate of convergence in parameter estimation. We illustrate the usefulness of CMR by thorough numerical simulations and show that CMR consistently outperforms other high dimensional multivariate regression methods. We also apply CMR on a brain activity prediction problem and find that CMR is as competitive as the handcrafted model created by human experts.	[Liu, Han; Zhao, Tuo] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA; [Wang, Lie] MIT, Dept Math, Cambridge, MA 02139 USA; [Zhao, Tuo] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA	Princeton University; Massachusetts Institute of Technology (MIT); Johns Hopkins University	Liu, H (corresponding author), Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.				NSF [IIS1408910, IIS1332109, DMS-1005539]; NIH [R01HG06841, R01MH102339, R01GM083084]	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	The authors are listed in alphabetical order. This work is partially supported by the grants NSF IIS1408910, NSF IIS1332109, NSF Grant DMS-1005539, NIH R01MH102339, NIH R01GM083084, and NIH R01HG06841.	Anderson T.W, 1958, INTRO MULTIVARIATE S; Ando RK, 2005, J MACH LEARN RES, V6, P1817; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Beck A, 2009, IEEE T IMAGE PROCESS, V18, P2419, DOI 10.1109/TIP.2009.2028250; Belloni A, 2011, BIOMETRIKA, V98, P791, DOI 10.1093/biomet/asr043; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Breiman L, 1997, J ROY STAT SOC B MET, V59, P3, DOI 10.1111/1467-9868.00054; Bunea F, 2014, IEEE T INFORM THEORY, V60, P1313, DOI 10.1109/TIT.2013.2290040; Johnstone IM, 2001, INST MATH S, V36, P399, DOI 10.1214/lnms/1215090080; Ledoux M, 2011, CLASS MATH, P1; Liu H., 2009, P 26 ANN INT C MACHI, P649, DOI DOI 10.1145/1553374.1553458; Liu J., 2010, TECHNICAL REPORT; Lounici K, 2011, ANN STAT, V39, P2164, DOI 10.1214/11-AOS896; Meinshausen N, 2010, J R STAT SOC B, V72, P417, DOI 10.1111/j.1467-9868.2010.00740.x; Meinshausen N, 2009, ANN STAT, V37, P246, DOI 10.1214/07-AOS582; Mitchell TM, 2008, SCIENCE, V320, P1191, DOI 10.1126/science.1152876; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Obozinski G, 2011, ANN STAT, V39, P1, DOI 10.1214/09-AOS776; Ouyang Hua, 2013, P 30 INT C MACH LEAR, P80; Raskutti G, 2010, J MACH LEARN RES, V11, P2241; Rothman AJ, 2010, J COMPUT GRAPH STAT, V19, P947, DOI 10.1198/jcgs.2010.09188; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Turlach BA, 2005, TECHNOMETRICS, V47, P349, DOI 10.1198/004017005000000139; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Zhang J, 2006, THESIS	26	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103081
C	Liu, LQ; Shen, CH; Wang, L; van den Hengel, A; Wang, C		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Liu, Lingqiao; Shen, Chunhua; Wang, Lei; van den Hengel, Anton; Wang, Chao			Encoding High Dimensional Local Features by Sparse Coding Based Fisher Vectors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Deriving from the gradient vector of a generative model of local features, Fisher vector coding (FVC) has been identified as an effective coding method for image classification. Most, if not all, FVC implementations employ the Gaussian mixture model (GMM) to characterize the generation process of local features. This choice has shown to be sufficient for traditional low dimensional local features, e.g., SIFT; and typically, good performance can be achieved with only a few hundred Gaussian distributions. However, the same number of Gaussians is insufficient to model the feature space spanned by higher dimensional local features, which have become popular recently. In order to improve the modeling capacity for high dimensional features, it turns out to be inefficient and computationally impractical to simply increase the number of Gaussians. In this paper, we propose a model in which each local feature is drawn from a Gaussian distribution whose mean vector is sampled from a subspace. With certain approximation, this model can be converted to a sparse coding procedure and the learning/inference problems can be readily solved by standard sparse coding methods. By calculating the gradient vector of the proposed model, we derive a new fisher vector encoding strategy, termed Sparse Coding based Fisher Vector Coding (SCFVC). Moreover, we adopt the recently developed Deep Convolutional Neural Network (CNN) descriptor as a high dimensional local feature and implement image classification with the proposed SCFVC. Our experimental evaluations demonstrate that our method not only significantly outperforms the traditional GMM based Fisher vector encoding but also achieves the state-of-the-art performance in generic object recognition, indoor scene, and fine-grained image classification problems.	[Liu, Lingqiao; Shen, Chunhua; van den Hengel, Anton] Univ Adelaide, Sch Comp Sci, Adelaide, SA, Australia; [Shen, Chunhua; van den Hengel, Anton] ARC Ctr Excellence Robot Vis, Brisbane, Qld, Australia; [Wang, Lei; Wang, Chao] Univ Wollongong, Sch Comp Sci & Software Engn, Wollongong, NSW, Australia	University of Adelaide; Australian Centre for Robotic Vision; University of Wollongong	Liu, LQ (corresponding author), Univ Adelaide, Sch Comp Sci, Adelaide, SA, Australia.		Wang, Lei/AAL-9684-2020; Wang, Lei/D-9079-2013	Wang, Lei/0000-0002-0961-0441	Australian Research Council [FT120100969, LP120200485]; Data to Decisions Cooperative Research Centre	Australian Research Council(Australian Research Council); Data to Decisions Cooperative Research Centre	This work was in part supported by Australian Research Council grants FT120100969, LP120200485, and the Data to Decisions Cooperative Research Centre. Correspondence should be addressed to C. Shen (email: chhshen@gmail.com).	[Anonymous], 2010, P INT C MACH LEARN; [Anonymous], 2010, P IEEE C COMP VIS PA; Arandjelovic R., 2013, P IEEE INT C COMP VI; Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Bo L., P ADV NEUR INF PROC, P2115; Chapelle O, 2002, MACH LEARN, V46, P131, DOI 10.1023/A:1012450327387; Chen Q, 2012, PROC CVPR IEEE, P3426, DOI 10.1109/CVPR.2012.6248083; Doersch C., 2013, ADV NEUR INF PROC SY; Donahue, 2014, CAFFE; Donahue J., 2013, P INT C MACH LEARN; Dong J, 2013, PROC CVPR IEEE, P827, DOI 10.1109/CVPR.2013.112; Gong Y., 2014, P EUR C COMP VIS; Jaakkola TS, 1999, ADV NEUR IN, V11, P487; Krapac J, 2011, IEEE I CONF COMP VIS, P1487, DOI 10.1109/ICCV.2011.6126406; Lee H., 2007, ADV NEURAL INF PROCE, P801; Pandey M, 2011, IEEE I CONF COMP VIS, P1307, DOI 10.1109/ICCV.2011.6126383; Perronnin F., 2010, P EUR C COMP VIS; Perronnin F, 2007, P IEEE C COMP VIS PA; Sharif Razavian A, 2014, CNN FEATURES OFF THE; Simonyan Karen, 2013, ADV NEURAL INFORM PR, V26; SYDOROV V, 2014, P IEEE C COMP VIS PA; YAN S., 2011, P IEEE C COMP VIS PA; Yan SY, 2012, LECT NOTES COMPUT SC, V7575, P473, DOI 10.1007/978-3-642-33765-9_34; Zhang N., 2013, P IEEE INT C COMP VI	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100064
C	Liu, Q; Ihler, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Liu, Qiang; Ihler, Alexander			Distributed Estimation, Information Loss and Exponential Families	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Distributed learning of probabilistic models from multiple data repositories with minimum communication is increasingly important. We study a simple communication-efficient learning framework that first calculates the local maximum likelihood estimates (MLE) based on the data subsets, and then combines the local MLEs to achieve the best possible approximation to the global MLE given the whole dataset. We study this framework's statistical properties, showing that the efficiency loss compared to the global setting relates to how much the underlying distribution families deviate from full exponential families, drawing connection to the theory of information loss by Fisher, Rao and Efron. We show that the "full-exponential-family-ness" represents the lower bound of the error rate of arbitrary combinations of local MLEs, and is achieved by a KL-divergence-based combination method but not by a more common linear combination method. We also study the empirical properties of both methods, showing that the KL method significantly outperforms linear combination in practical settings with issues such as model misspecification, non-convexity, and heterogeneous data partitions.	[Liu, Qiang; Ihler, Alexander] Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA	University of California System; University of California Irvine	Liu, Q (corresponding author), Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA.	qliu1@uci.edu; ihler@ics.uci.edu			NSF [IIS-1065618, IIS-1254071]; US Air Force under DARPA's PPAML program [FA8750-14-C-0011]	NSF(National Science Foundation (NSF)); US Air Force under DARPA's PPAML program	This work sponsored in part by NSF grants IIS-1065618 and IIS-1254071, and the US Air Force under Contract No. FA8750-14-C-0011 under DARPA's PPAML program.	Balcan M.-F., 2012, ARXIV12043514; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; EFRON B, 1975, ANN STAT, V3, P1189, DOI 10.1214/aos/1176343282; Fisher RA, 1925, P CAMB PHILOS SOC, V22, P700, DOI 10.1017/S0305004100009580; Forero PA, 2011, IEEE J-STSP, V5, P707, DOI 10.1109/JSTSP.2011.2114324; Kass Robert E., 2011, GEOMETRICAL FDN ASYM, V908; Liang Yingyu, 2013, BIG LEARN WORKSH NIP; Liu Qiang, 2012, INT C MACH LEARN ICM, P1487; Meng Z., 2013, INT C ART INT STAT A; Merugu S, 2003, THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P211; Merugu Srujana, 2006, THESIS; Neiswanger W., 2013, ARXIV13114780; Predd J. B, 2007, DISTRIBUTED LEARNING; Rao C.R, 1963, SANKHYA A, VA25, P189; Scott Steven L, 2013, EFAB BAY 250 C, V16; Shamir Ohad, 2013, ARXIV13113494; Van der Vaart A.W, 2000, CAMBRIDGE SERIES STA, V3; Wang X., 2013, ARXIV PREPRINT ARXIV; Zhang Y., 2013, NEURAL INFORM PROCES, P2328; Zhang YC, 2013, J MACH LEARN RES, V14, P3321	20	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100008
C	Liu, W; Mu, C; Kumar, S; Chang, SF		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Liu, Wei; Mu, Cun; Kumar, Sanjiv; Chang, Shih-Fu			Discrete Graph Hashing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				SCENE	Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic databases. In particular, learning based hashing has received considerable attention due to its appealing storage and search efficiency. However, the performance of most unsupervised learning based hashing methods deteriorates rapidly as the hash code length increases. We argue that the degraded performance is due to inferior optimization procedures used to achieve discrete binary codes. This paper presents a graph-based unsupervised hashing model to preserve the neighborhood structure of massive data in a discrete code space. We cast the graph hashing problem into a discrete optimization framework which directly learns the binary codes. A tractable alternating maximization algorithm is then proposed to explicitly deal with the discrete constraints, yielding high-quality codes to well capture the local neighborhoods. Extensive experiments performed on four large datasets with up to one million samples show that our discrete optimization based graph hashing method obtains superior search accuracy over state-of-the-art unsupervised hashing methods, especially for longer codes.	[Liu, Wei] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA; [Mu, Cun; Chang, Shih-Fu] Columbia Univ, New York, NY 10027 USA; [Kumar, Sanjiv] Google Res, Mountain View, CA USA	International Business Machines (IBM); Columbia University; Google Incorporated	Liu, W (corresponding author), IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.	weillu@us.ibm.com; cm3052@columbia.edu; sfchang@ee.columbia.edu; sanjivk@google.com						Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244; Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494; [Anonymous], P ICML; Broder A. Z., 1998, P STOC; Charikar M., 2002, P STOC; De Leeuw J, 1977, RECENT DEV STAT, P133; Dean T. L., 2013, P CVPR; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Foo C.-S., 2009, P ICML; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Gong Yunchao, 2014, P ECCV, P2; Heiser WJ, 1995, RECENT ADV DESCRIPTI, P157; Jebara T., 2012, NIPS, V25; Kong W., 2012, NIPS, V25; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Kulis B., 2009, NIPS; Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219; Li P., 2011, NIPS, V24; Li P, 2011, COMMUN ACM, V54, P101, DOI 10.1145/1978542.1978566; Li X., 2013, P ICML; Liu W., 2012, P ICML; Liu W., 2012, P CVPR; Liu W., 2010, P ICML; Liu W., 2011, P ICML; Mu Y., 2010, P CVPR; Neyshabur B., 2013, NIPS, V26; Norouzi M., 2012, NIPS, V25; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006; SHAKHNAROVICH G, 2003, P ICCV; Shen F., 2013, P CVPR; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Shi QF, 2009, J MACH LEARN RES, V10, P2615; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; Weinberger K., 2009, P ICML; Weiss Y., 2012, P ECCV; Weiss Y., 2008, NIPS; Wolf Lior, 2011, P CVPR; Xiao J., 2010, P CVPR	40	0	0	0	11	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101003
C	Liu, XHJ; Domke, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Liu, Xianghang; Domke, Justin			ProjectingMarkov Random Field Parameters for Fast Mixing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				DYNAMICS	Markov chain Monte Carlo (MCMC) algorithms are simple and extremely powerful techniques to sample from almost arbitrary distributions. The flaw in practice is that it can take a large and/or unknown amount of time to converge to the stationary distribution. This paper gives sufficient conditions to guarantee that univariate Gibbs sampling on Markov Random Fields (MRFs) will be fast mixing, in a precise sense. Further, an algorithm is given to project onto this set of fast-mixing parameters in the Euclidean norm. Following recent work, we give an example use of this to project in various divergence measures, comparing univariate marginals obtained by sampling after projection to common variational methods and Gibbs sampling on the original parameters.	[Liu, Xianghang] Univ New South Wales, NICTA, Kensington, NSW, Australia; [Domke, Justin] Australian Natl Univ, NICTA, Canberra, ACT, Australia	Australian National University; University of New South Wales Sydney; Australian National University	Liu, XHJ (corresponding author), Univ New South Wales, NICTA, Kensington, NSW, Australia.	xianghang.liu@nicta.com.au; justin.domke@nicta.com.au			Australian Government through the Department of Communications; Australian Research Council through the ICT Centre of Excellence Program	Australian Government through the Department of Communications(Australian Government); Australian Research Council through the ICT Centre of Excellence Program(Australian Research Council)	NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program.	Bertsekas Dimitri, 2004, NONLINEAR PROGRAMMIN; BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069; Domke J., 2013, NIPS; Duchi J., 2008, ICML; Dyer M, 2009, ANN APPL PROBAB, V19, P71, DOI 10.1214/08-AAP532; Globerson Amir, 2007, UAI; Hayes TP, 2006, ANN IEEE SYMP FOUND, P39; Hazan T., 2008, UAI, P264; Koller D., 2009, PROBABILISTIC GRAPHI; Minka T. P., 2001, UAI; Minka Tom, 2005, TECHNICAL REPORT; SWENDSEN RH, 1987, PHYS REV LETT, V58, P86, DOI 10.1103/PhysRevLett.58.86; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085	14	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100094
C	Liu, YY; Shang, FH; Fan, W; Cheng, J; Cheng, H		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Liu, Yuanyuan; Shang, Fanhua; Fan, Wei; Cheng, James; Cheng, Hong			Generalized Higher-Order Orthogonal Iteration for Tensor Decomposition and Completion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				MATRIX COMPLETION	Low-rank tensor estimation has been frequently applied in many real-world problems. Despite successful applications, existing Schatten 1-norm minimization (SNM) methods may become very slow or even not applicable for large-scale problems. To address this difficulty, we therefore propose an efficient and scalable core tensor Schatten 1-norm minimization method for simultaneous tensor decomposition and completion, with a much lower computational complexity. We first induce the equivalence relation of Schatten 1-norm of a low-rank tensor and its core tensor. Then the Schatten 1-norm of the core tensor is used to replace that of the whole tensor, which leads to a much smaller-scale matrix SNM problem. Finally, an efficient algorithm with a rank-increasing scheme is developed to solve the proposed problem with a convergence guarantee. Extensive experimental results show that our method is usually more accurate than the state-of-the-art methods, and is orders of magnitude faster.	[Liu, Yuanyuan; Cheng, Hong] Chinese Univ Hong Kong, Dept Syst Engn & Engn Management, Hong Kong, Peoples R China; [Shang, Fanhua; Cheng, James] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China; [Fan, Wei] Huawei Noahs Ark Lab, Hong Kong, Peoples R China	Chinese University of Hong Kong; Chinese University of Hong Kong; Huawei Technologies	Shang, FH (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.	yyliu@se.cuhk.edu.hk; fhshang@cse.cuhk.edu.hk; david.fanwei@huawei.com; jcheng@cse.cuhk.edu.hk; hcheng@se.cuhk.edu.hk	liu, yuanyuan/GWZ-5838-2022		SHIAE [8115048]; MSRA [6903555]; GRF [411211]; CUHK [4055015, 4055017]; China 973 Fundamental RD Program [2014CB340304]; Huawei [7010255]	SHIAE; MSRA; GRF; CUHK(Chinese University of Hong Kong); China 973 Fundamental RD Program(National Basic Research Program of China); Huawei(Huawei Technologies)	This research is supported in part by SHIAE Grant No. 8115048, MSRA Grant No. 6903555, GRF No. 411211, CUHK direct grant Nos. 4055015 and 4055017, China 973 Fundamental R&D Program, No. 2014CB340304, and Huawei Grant No. 7010255.	Anandkumar A., 2013, ADV NEURAL INFORM PR, P1986; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1324, DOI 10.1137/S0895479898346995; Fazel M., 2002, MATRIX RANK MINIMIZA; Filipovic M., 2014, MULTIDIM SYST SIGN P; Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010; Goldfarb D, 2014, SIAM J MATRIX ANAL A, V35, P225, DOI 10.1137/130905010; Huang B., 2014, OPTIMIZATION ONLINE; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Lin Z., 2011, PROC INT 25 C NEURAL, P612, DOI DOI 10.1007/S11263-013-0611-6; Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39; Liu J, 2009, IEEE I CONF COMP VIS, P2114; Morup Morten, 2010, ARXIV10052197V1MATHN, DOI [10.1137/1.9781611972801.61, DOI 10.1137/1.9781611972801.61]; Mu C, 2014, PR MACH LEARN RES, V32, P73; Nick H., 1995, MATRIX PROCRUSTES PR; Romera-Paredes B., 2013, ADV NEURAL INFORM PR, V2, P2967; Shang FH, 2014, AAAI CONF ARTIF INTE, P1279; Signoretto M., 2010, 10186 ESATSISTA; Signoretto M, 2014, MACH LEARN, V94, P303, DOI 10.1007/s10994-013-5366-3; Wang Y, 2012, ICML; Wen ZW, 2012, MATH PROGRAM COMPUT, V4, P333, DOI 10.1007/s12532-012-0044-1; Xu Y., 2013, ARXIV13121254; Yilmaz KY, 2011, ADV NEURAL INFORM PR, P2151	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102037
C	Liu, Z; Lafferty, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Liu, Zhe; Lafferty, John			Blossom Tree Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We combine the ideas behind trees and Gaussian graphical models to form a new nonparametric family of graphical models. Our approach is to attach nonpara-normal "blossoms", with arbitrary graphs, to a collection of nonparametric trees. The tree edges are chosen to connect variables that most violate joint Gaussianity. The non-tree edges are partitioned into disjoint groups, and assigned to tree nodes using a nonparametric partial correlation statistic. A nonparanormal blossom is then "grown" for each group using established methods based on the graphical lasso. The result is a factorization with respect to the union of the tree branches and blossoms, defining a high-dimensional joint density that can be efficiently estimated and evaluated on test points. Theoretical properties and experiments with simulated and real data demonstrate the effectiveness of blossom trees.	[Liu, Zhe] Univ Chicago, Dept Stat, Chicago, IL 60637 USA; [Lafferty, John] Univ Chicago, Dept Stat, Dept Comp Sci, Chicago, IL 60637 USA	University of Chicago; University of Chicago	Liu, Z (corresponding author), Univ Chicago, Dept Stat, Chicago, IL 60637 USA.				NSF [IIS-1116730]; AFOSR [FA9550-09-1-0373]; ONR [N000141210762]; Amazon AWS in Education Machine Learning Research grant	NSF(National Science Foundation (NSF)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); ONR(Office of Naval Research); Amazon AWS in Education Machine Learning Research grant	Research supported in part by NSF grant IIS-1116730, AFOSR grant FA9550-09-1-0373, ONR grant N000141210762, and an Amazon AWS in Education Machine Learning Research grant.	Bergsma Wicher, 2011, ARXIV11014616; Cai T. Tony, 2013, ARXIV13090482; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; HAN F., 2012, ADV NEURAL INFORM PR; Kruskal J. B., 1956, P AM MATH SOC, V7, P48, DOI [DOI 10.1090/S0002-9939-1956-0078686-7, 10.2307/2033241]; Liu H, 2011, J MACH LEARN RES, V12, P907; Liu H, 2009, J MACH LEARN RES, V10, P2295; MEINSHAUSEN N, 2006, ANN STAT, V34; Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809; Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018	10	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100095
C	Loh, PL; Wibisono, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Loh, Po-Ling; Wibisono, Andre			Concavity of reweighted Kikuchi approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				LOOPY BELIEF PROPAGATION; CONVERGENCE	We analyze a reweighted version of the Kikuchi approximation for estimating the log partition function of a product distribution defined over a region graph. We establish sufficient conditions for the concavity of our reweighted objective function in terms of weight assignments in the Kikuchi expansion, and show that a reweighted version of the sum product algorithm applied to the Kikuchi region graph will produce global optima of the Kikuchi approximation whenever the algorithm converges. When the region graph has two layers, corresponding to a Bethe approximation, we show that our sufficient conditions for concavity are also necessary. Finally, we provide an explicit characterization of the polytope of concavity in terms of the cycle structure of the region graph. We conclude with simulations that demonstrate the advantages of the reweighted Kikuchi approach.	[Loh, Po-Ling] Univ Penn, Wharton Sch, Dept Stat, Philadelphia, PA 19104 USA; [Wibisono, Andre] Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA	University of Pennsylvania; University of California System; University of California Berkeley	Loh, PL (corresponding author), Univ Penn, Wharton Sch, Dept Stat, Philadelphia, PA 19104 USA.	loh@wharton.upenn.edu; wibisono@berkeley.edu			Hertz Foundation Fellowship; NSF Graduate Research Fellowship while at Berkeley	Hertz Foundation Fellowship; NSF Graduate Research Fellowship while at Berkeley	The authors thank Martin Wainwright for introducing the problem to them and providing helpful guidance. The authors also thank Varun Jog for discussions regarding the generalization of Hall's lemma. The authors thank the anonymous reviewers for feedback that improved the clarity of the paper. PL was partly supported from a Hertz Foundation Fellowship and an NSF Graduate Research Fellowship while at Berkeley.	Aji S. M., 2001, P 39 ALL C; [Anonymous], 1935, P R SOC LONDON A, DOI DOI 10.1098/RSPA.1935.0122; BARAHONA F, 1982, J PHYS A-MATH GEN, V15, P3241, DOI 10.1088/0305-4470/15/10/028; Heskes T, 2004, NEURAL COMPUT, V16, P2379, DOI 10.1162/0899766041941943; Heskes T., 2002, ADV NEURAL INFORM PR, V15; Heskes T, 2006, J ARTIF INTELL RES, V26, P153, DOI 10.1613/jair.1933; Ihler AT, 2005, J MACH LEARN RES, V6, P905; KIKUCHI R, 1951, PHYS REV, V81, P988, DOI 10.1103/PhysRev.81.988; Korte B., 2007, COMBINATORIAL OPTIMI; McEliece R. J., 2002, MATH SYST THEORY, P275; Meltzer T., 2009, P 25 C UNC ART INT U; Mooij JM, 2007, IEEE T INFORM THEORY, V53, P4422, DOI 10.1109/TIT.2007.909166; Pakzad P, 2005, NEURAL COMPUT, V17, P1836, DOI 10.1162/0899766054026693; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Roosta TG, 2008, IEEE T SIGNAL PROCES, V56, P4293, DOI 10.1109/TSP.2008.924136; Roth D, 1996, ARTIF INTELL, V82, P273, DOI 10.1016/0004-3702(94)00092-1; Ruozzi N., 2012, ADV NEURAL INFORM PR, V25; Sudderth E. B., 2007, ADV NEURAL INFORM PR, V20; Tatikonda S. C., 2002, P 18 C UNC ART INT U; Vontobel PO, 2013, IEEE T INFORM THEORY, V59, P1866, DOI 10.1109/TIT.2012.2227109; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P2313, DOI 10.1109/TIT.2005.850091; Watanabe Y., 2009, ADV NEURAL INFORM PR, V22; Watanabe Y., 2011, ARXIV11030605; Weiss Y, 2000, NEURAL COMPUT, V12, P1, DOI 10.1162/089976600300015880; Werner Tomas, 2010, UAI 2010, P651; Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085; YEDIDIA JS, 2000, ADV NEURAL INFORM PR, V13	29	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103022
C	Lopes, ME		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Lopes, Miles E.			A Residual Bootstrap for High-Dimensional Regression with Near Low-Rank Designs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We study the residual bootstrap (RB) method in the context of high-dimensional linear regression. Specifically, we analyze the distributional approximation of linear contrasts C-T ((beta) over cap (rho)-beta), where (beta) over cap (rho) is a ridge-regression estimator. When regression coefficients are estimated via least squares, classical results show that RB consistently approximates the laws of contrasts, provided that p << n, where the design matrix is of size n x p. Up to now, relatively little work has considered how additional structure in the linear model may extend the validity of RB to the setting where p/n asymptotic to 1. In this setting, we propose a version of RB that resamples residuals obtained from ridge regression. Our main structural assumption on the design matrix is that it is nearly low rank - in the sense that its singular values decay according to a power-law profile. Under a few extra technical assumptions, we derive a simple criterion for ensuring that RB consistently approximates the law of a given contrast. We then specialize this result to study confidence intervals for mean response values X-i(T) beta, where X-i(T) is the ith row of the design. More precisely, we show that conditionally on a Gaussian design with near low-rank structure, RB simultaneously approximates all of the laws X-i(T) ((beta) over cap (rho)-beta), i = 1, ... , n. This result is also notable as it imposes no sparsity assumptions on beta. Furthermore, since our consistency results are formulated in terms of the Mallows (Kantorovich) metric, the existence of a limiting distribution is not required.	[Lopes, Miles E.] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Lopes, ME (corresponding author), Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA.	mlopes@stat.berkeley.edu			DOE CSGF [DE-FG02-97ER25308]; NSF-GRFP	DOE CSGF(United States Department of Energy (DOE)); NSF-GRFP(National Science Foundation (NSF)NSF - Office of the Director (OD))	MEL thanks Prof. Peter J. Bickel for many helpful discussions, and gratefully acknowledges the DOE CSGF under grant DE-FG02-97ER25308, as well as the NSF-GRFP.	Bobkov S., 2014, PREPRINT; Buhlmann P, 2013, BERNOULLI, V19, P1212, DOI 10.3150/12-BEJSP11; Chatterjee A, 2013, ANN STAT, V41, P1232, DOI 10.1214/13-AOS1106; Draper N.R., 1998, APPL REGRESSION ANAL, V326; FREEDMAN DA, 1981, ANN STAT, V9, P1218, DOI 10.1214/aos/1176345638; Javanmard A., 2013, ARXIV13014240; Javanmard A, 2013, ARXIV13063171; Lee J.D., 2013, ARXIV13116238; Lehmann E., 2005, TESTING STAT HYPOTHE, V3rd; Liu HZ, 2013, ELECTRON J STAT, V7, P3124, DOI 10.1214/14-EJS875; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; vande Geer S, 2013, ARXIV13030518; Zhang CH, 2014, J R STAT SOC B, V76, P217, DOI 10.1111/rssb.12026	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101098
C	Lu, YC; Foster, DP		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Lu, Yichao; Foster, Dean P.			Large Scale Canonical Correlation Analysis with Iterative Least Squares	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Canonical Correlation Analysis (CCA) is a widely used statistical tool with both well established theory and favorable performance for a wide range of machine learning problems. However, computing CCA for huge datasets can be very slow since it involves implementing QR decomposition or singular value decomposition of huge matrices. In this paper we introduce L-CCA, a iterative algorithm which can compute CCA fast on huge sparse datasets. Theory on both the asymptotic convergence and finite time accuracy of L-CCA are established. The experiments also show that L-CCA outperform other fast CCA approximation schemes on two real datasets.	[Lu, Yichao] Univ Penn, Philadelphia, PA 19104 USA; [Foster, Dean P.] Yahoo Labs, New York, NY USA	University of Pennsylvania	Lu, YC (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.	yichaolu@wharton.upenn.edu; dean@foster.net						Avron H, 2013, P 30 INT C MACH LEAR, P347; Bach Francis R, 2005, TECHNICAL REPORT; Bau III D, 1997, NUMERICAL LINEAR ALG; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Dhillon P S, 2013, ADV NEURAL INFORM PR, P360; Dhillon P. S., 2012, P 29 INT C MACH LEAR; Dhillon P S., 2011, ADV NEURAL INFORM PR, V24; DRINEAS P., 2007, ABS07101435 CORR; Epelman Marina A., 2007, RATE CONVERGENCE STE; Foster D. P., 2008, TECHNICAL REPORT; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Golub Gene. H, 1992, TECHNICAL REPORT; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Halko N, 2011, SIAM J SCI COMPUT, V33, P2580, DOI 10.1137/100804139; Hardoon David R, 2007, TECHNICAL REPORT; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Johoson R., 2013, ADV NEURAL INFORM PR; Kakade Sham M., 2007, P C LEARN THEOR; Lamar M., 2010, P ACL 2010 C, P215; Ma J., 2009, P INT C MACH LEARN I; Sun L., 2008, ICML, P1024	21	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103068
C	Maillard, OA; Mann, TA; Mannor, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Maillard, Odalric-Ambrym; Mann, Timothy A.; Mannor, Shie			"How hard is my MDP?" The distribution-norm to the rescue	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In Reinforcement Learning (RL), state-of-the-art algorithms require a large number of samples per state-action pair to estimate the transition kernel p. In many problems, a good approximation of p is not needed. For instance, if from one state-action pair (s, a), one can only transit to states with the same value, learning p(.vertical bar s, a) accurately is irrelevant (only its support matters). This paper aims at capturing such behavior by defining a novel hardness measure for Markov Decision Processes (MDPs) based on what we call the distribution-norm. The distributionnorm w.r.t. a measur nu is defined on zero nu-mean functions f by the standard variation of f with respect to nu. We first provide a concentration inequality for the dual of the distribution-norm. This allows us to replace the problem-free, loose parallel to center dot parallel to(1) concentration inequalities used in most previous analysis of RL algorithms, with a tighter problem-dependent hardness measure. We then show that several common RL benchmarks have low hardness when measured using the new norm. The distribution-norm captures finer properties than the number of states or the diameter and can be used to assess the difficulty of MDPs.	[Maillard, Odalric-Ambrym; Mann, Timothy A.; Mannor, Shie] Technion, Haifa, Israel		Maillard, OA (corresponding author), Technion, Haifa, Israel.	odalric-ambrym.maillard@ens-cachan.org; mann.timothy@gmail.com; shie@ee.technion.ac.il			European Community [306638]; Technion	European Community(European Commission); Technion	This work was supported by the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement 306638 (SUPREL) and the Technion.	[Anonymous], 2009, ADV NEURAL INFORM PR; [Anonymous], 2001, P 18 INT C MACHINE L; BARTLETT PL, 2009, P 25 C UNC ART INT, P35; Dietterich T. G., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P118; Farahmand AM, 2011, ADV NEURAL INFORM PR, P172; Filippi S., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P115, DOI 10.1109/ALLERTON.2010.5706896; Hester T., 2009, 8 INT C AUT AG MULT; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Kakade Sham M., 2003, THESIS; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; Mankowitz D. J., 2014, P 31 INT C MACH LEAR; Maurer Andreas, 2009, C LEARN THEOR COLT; Ortner R., 2014, TECHNICAL REPORT; Ortner Ronald, 2012, ADV NEURAL INFORM PR, P1772; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Szita I., 2010, ICML; Tamar A., 2013, P 30 INT C MACH LEAR; Weissman T., 2003, TECHNICAL REPORT	21	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102002
C	Makin, JG; Sabes, PN		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Makin, Joseph G.; Sabes, Philip N.			Sensory Integration and Density Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				INFORMATION; IDENTIFIABILITY; MODELS	The integration of partially redundant information from multiple sensors is a standard computational problem for agents interacting with the world. In man and other primates, integration has been shown psychophysically to be nearly optimal in the sense of error minimization. An influential generalization of this notion of optimality is that populations of multisensory neurons should retain all the information from their unisensory afferents about the underlying, common stimulus [1]. More recently, it was shown empirically that a neural network trained to perform latent-variable density estimation, with the activities of the unisensory neurons as observed data, satisfies the information-preservation criterion, even though the model architecture was not designed to match the true generative process for the data [2]. We prove here an analytical connection between these seemingly different tasks, density estimation and sensory integration; that the former implies the latter for the model used in [2]; but that this does not appear to be true for all models.	[Makin, Joseph G.; Sabes, Philip N.] Univ Calif San Francisco, Ctr Integrat Neurosci, Dept Physiol, San Francisco, CA 94143 USA	University of California System; University of California San Francisco	Makin, JG (corresponding author), Univ Calif San Francisco, Ctr Integrat Neurosci, Dept Physiol, San Francisco, CA 94143 USA.	makin@phy.ucsf.edu; sabes@phy.ucsf.edu	Makin, Joseph Gerard/HGB-4156-2022	Makin, Joseph/0000-0002-0053-7006				Alais D, 2004, CURR BIOL, V14, P257, DOI 10.1016/j.cub.2004.01.029; Allman ES, 2009, ANN STAT, V37, P3099, DOI 10.1214/09-AOS689; Beck J, 2011, NEURAL COMPUT, V23, P1484, DOI 10.1162/NECO_a_00125; Beck Jeffrey M., 2013, ADV NEURAL INFORM PR, V25, P1; Bell AJ, 2007, AIP CONF PROC, V954, P56, DOI 10.1063/1.2821301; Cover TM, 2006, ELEMENTS INFORM THEO; Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a; Hinton G., 2010, TECHNICAL REPORT; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Knill David C., 2004, TRENDS NEUROSCIENCES, V27; Ma WJ, 2006, NAT NEUROSCI, V9, P1423; Makin JG, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003035; McCullagh P., 1989, GEN LINEAR MODELS, V2nd; Olshausen BA, 2002, NEU INF PRO, P257; Sabes PN, 2011, PROG BRAIN RES, V191, P195, DOI 10.1016/B978-0-444-53752-2.00004-7; Saunders JA, 2001, VISION RES, V41, P3163, DOI 10.1016/S0042-6989(01)00187-0; TEICHER H, 1967, ANN MATH STAT, V38, P1300, DOI 10.1214/aoms/1177698805; Titterington DM, 1985, STAT ANAL FINITE MIX; van Beers RJ, 1999, J NEUROPHYSIOL, V81, P1355, DOI 10.1152/jn.1999.81.3.1355; Welling M., 2004, ADV NEURAL INFORM PR, V17, P1481; Yildirim I, 2012, COGNITIVE SCI, V36, P305, DOI 10.1111/j.1551-6709.2011.01216.x	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100009
C	Marchand, M; Su, HY; Morvant, E; Rousu, J; Shawe-Taylor, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Marchand, Mario; Su, Hongyu; Morvant, Emilie; Rousu, Juho; Shawe-Taylor, John			Multilabel Structured Output Learning with Random Spanning Trees of Max-Margin Markov Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We show that the usual score function for conditional Markov networks can be written as the expectation over the scores of their spanning trees. We also show that a small random sample of these output trees can attain a significant fraction of the margin obtained by the complete graph and we provide conditions under which we can perform tractable inference. The experimental results confirm that practical learning is scalable to realistic datasets using this approach.	[Marchand, Mario] Univ Laval, Dept Informat & Genie Logiciel, Quebec City, PQ, Canada; [Su, Hongyu; Rousu, Juho] Aalto Univ, Dept Informat & Comp Sci, Helsinki Inst Informat Technol, Helsinki, Finland; [Morvant, Emilie] Univ St Etienne, UMR CNRS 5516, LaHC, St Etienne, France; [Shawe-Taylor, John] UCL, Dept Comp Sci, London, England; [Morvant, Emilie] IST Austria, Klosterneurburg, Austria	Laval University; Aalto University; University of Helsinki; Centre National de la Recherche Scientifique (CNRS); CNRS - Institute for Engineering & Systems Sciences (INSIS); Universite Jean Monnet; University of London; University College London; Institute of Science & Technology - Austria	Marchand, M (corresponding author), Univ Laval, Dept Informat & Genie Logiciel, Quebec City, PQ, Canada.	mario.marchand@ift.ulaval.ca; hongyu.su@aalto.fi; emilie.morvant@univ-st-etienne.fr; juho.rousu@aalto.fi; j.shawe-taylor@ucl.ac.uk	Rousu, Juho/E-8195-2012	Rousu, Juho/0000-0002-0705-4314; Shawe-Taylor, John/0000-0002-2030-0073				Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Bennett KP, 1999, ADVANCES IN KERNEL METHODS, P307; COWELL RG, 1999, PROBABILISTIC NETWOR; Cristianini N., 2000, INTRO SUPPORT VECTOR; Gartner T, 2009, MACH LEARN, V76, P227, DOI 10.1007/s10994-009-5129-3; Jaakkola A., 2007, ADV NEURAL INFORM PR, V19, P473; Maurer A., 2004, CORR; McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064; Rousu J., 2007, PREDICTING STRUCTURE, P105; Rousu J, 2006, J MACH LEARN RES, V7, P1601; Seldin Y, 2012, IEEE T INFORM THEORY, V58, P7086, DOI 10.1109/TIT.2012.2211334; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Su HY, 2015, MACH LEARN, V99, P231, DOI 10.1007/s10994-014-5465-9; Taskar B, 2004, ADV NEUR IN, V16, P25; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938; Wainwright MJ, 2004, ADV NEUR IN, V16, P369	16	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101053
C	McCarter, C; Kim, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		McCarter, Calvin; Kim, Seyoung			On Sparse Gaussian Chain Graph Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In this paper, we address the problem of learning the structure of Gaussian chain graph models in a high-dimensional space. Chain graph models are generalizations of undirected and directed graphical models that contain a mixed set of directed and undirected edges. While the problem of sparse structure learning has been studied extensively for Gaussian graphical models and more recently for conditional Gaussian graphical models (CGGMs), there has been little previous work on the structure recovery of Gaussian chain graph models. We consider linear regression models and a re-parameterization of the linear regression models using CGGMs as building blocks of chain graph models. We argue that when the goal is to recover model structures, there are many advantages of using CGGMs as chain component models over linear regression models, including convexity of the optimization problem, computational efficiency, recovery of structured sparsity, and ability to leverage the model structure for semi-supervised learning. We demonstrate our approach on simulated and genomic datasets.	[McCarter, Calvin] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Kim, Seyoung] Carnegie Mellon Univ, Lane Ctr Computat Biol, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	McCarter, C (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	calvinm@cmu.edu; sssykim@cs.cmu.edu	Kim, SeYoung/GSE-5296-2022; McCarter, Calvin/AAE-4434-2020	Kim, SeYoung/0000-0001-9188-868X; McCarter, Calvin/0000-0002-7257-1350	NSF CAREER Award [MCB-1149885]; Sloan Research Fellowship; Okawa Foundation Research Grant	NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Sloan Research Fellowship(Alfred P. Sloan Foundation); Okawa Foundation Research Grant	This material is based upon work supported by an NSF CAREER Award No. MCB-1149885, Sloan Research Fellowship, and Okawa Foundation Research Grant.	Abegaz F, 2013, BIOSTATISTICS, V14, P586, DOI 10.1093/biostatistics/kxt005; Andersson SA, 1996, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P40; Andersson SA, 2001, SCAND J STAT, V28, P33, DOI 10.1111/1467-9469.00224; Buntine W. L., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P46; Chen X., 2012, P 15 INT C ART INT S, V16; Chen YQ, 2008, NATURE, V452, P429, DOI 10.1038/nature06757; Drton M, 2006, SCAND J STAT, V33, P247, DOI 10.1111/j.1467-9469.2006.00482.x; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; FRYDENBERG M, 1990, SCAND J STAT, V17, P333; Hsieh C. J., 2011, ADV NEURAL INFORM PR; Jacob L., 2009, ICML, DOI [10.1145/1553374.1553431, DOI 10.1145/1553374.1553431]; Koller D., 2009, PROBABILISTIC GRAPHI; Lafferty John, 2001, CONDITIONAL RANDOM F, P282; LAURITZEN SL, 1989, ANN STAT, V17, P31, DOI 10.1214/aos/1176347003; Obozinski G, 2008, ANN ALLERTON CONF, P21, DOI 10.1109/ALLERTON.2008.4797530; Rothman AJ, 2010, J COMPUT GRAPH STAT, V19, P947, DOI 10.1198/jcgs.2010.09188; Sohn K., 2012, P 15 INT C ART INT S, V16; Tu ZD, 2012, PLOS GENET, V8, DOI 10.1371/journal.pgen.1003107; Wu JJ, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.066111; Wytock M., 2013, P 30 INT C MACH LEAR, V28; Yin JX, 2011, ANN APPL STAT, V5, P2630, DOI 10.1214/11-AOAS494	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100099
C	Meek, C; Meila, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Meek, Christopher; Meila, Marina			Recursive Inversion Models for Permutations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				RIFFLED INDEPENDENCE; INFERENCE	We develop a new exponential family probabilistic model for permutations that can capture hierarchical structure and that has the Mallows and generalized Mallows models as subclasses. We describe how to do parameter estimation and propose an approach to structure search for this class of models. We provide experimental evidence that this added flexibility both improves predictive performance and enables a deeper understanding of collections of permutations.	[Meek, Christopher] Microsoft Res, Redmond, WA 98052 USA; [Meila, Marina] Univ Washington, Seattle, WA 98195 USA	Microsoft; University of Washington; University of Washington Seattle	Meek, C (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	meek@microsoft.com; mmp@stat.washington.edu						Ali Alnur, 2011, MATH SOCIAL SCI COMP; Andrews G. E., 1985, THEORY PARTITIONS; BARTHOLDI J, 1989, SOC CHOICE WELFARE, V6, P157, DOI 10.1007/BF00303169; EARLEY J, 1970, COMMUN ACM, V13, P94, DOI 10.1145/362007.362035; FLIGNER MA, 1986, J R STAT SOC B, V48, P359; Gormley I. C., 2007, P 24 ANN INT C MACH, P90; Huang J, 2012, ELECTRON J STAT, V6, P199, DOI 10.1214/12-EJS670; Huang J, 2012, J ARTIF INTELL RES, V44, P491, DOI 10.1613/jair.3543; Huang J, 2009, J MACH LEARN RES, V10, P997; Kamishima T., 2003, P 9 ACM SIGKDD INT C, P583; Klementiev A., 2008, P 25 INT C MACH LEAR, P472; Lebanon G., 2002, P 19 INT C MACH LEAR, P363; MALLOWS CL, 1957, BIOMETRIKA, V44, P114, DOI 10.2307/2333244; Mandhani B, 2009, ARTIFICIAL INTELLIGE; Mannila H., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P161, DOI 10.1145/347090.347122; Schalekamp F., 2009, P 11 WORKSH ALG ENG, P38, DOI DOI 10.1137/1.9781611972894.4	16	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103030
C	Mehta, NA; Williamson, RC		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Mehta, Nishant A.; Williamson, Robert C.			From Stochastic Mixability to Fast Rates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				BOUNDS	Empirical risk minimization (ERM) is a fundamental learning rule for statistical learning problems where the data is generated according to some unknown distribution P and returns a hypothesis f chosen from a fixed class F with small loss l. In the parametric setting, depending upon (l, F, P) ERM can have slow (1/root n) or fast (1/n) rates of convergence of the excess risk as a function of the sample size n. There exist several results that give sufficient conditions for fast rates in terms of joint properties of l, F, and P, such as the margin condition and the Bernstein condition. In the non-statistical prediction with expert advice setting, there is an analogous slow and fast rate phenomenon, and it is entirely characterized in terms of the mixability of the loss l (there being no role there for F or P). The notion of stochastic mixability builds a bridge between these two models of learning, reducing to classical mixability in a special case. The present paper presents a direct proof of fast rates for ERM in terms of stochastic mixability of (l, F, P), and in so doing provides new insight into the fast-rates phenomenon. The proof exploits an old result of Kemperman on the solution to the general moment problem. We also show a partial converse that suggests a characterization of fast rates for ERM in terms of stochastic mixability is possible.	[Mehta, Nishant A.; Williamson, Robert C.] Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT, Australia	Australian National University	Mehta, NA (corresponding author), Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT, Australia.	nishant.mehta@anu.edu.au; bob.williamson@anu.edu.au			Australian Research Council; NICTA; Australian Government through the Department of Communications; Australian Research Council through the ICT Centre of Excellence program	Australian Research Council(Australian Research Council); NICTA(NICTA); Australian Government through the Department of Communications(Australian Government); Australian Research Council through the ICT Centre of Excellence program(Australian Research Council)	RCW thanks Tim van Erven for the initial discussions around the Cramer-Chernoff method during his visit to Canberra in 2013 and for his gracious permission to proceed with the present paper without him as an author, and both authors thank him for the further enormously helpful spotting of a serious error in our original proof for fast rates for VC-type classes. This work was supported by the Australian Research Council (NAM and RCW) and NICTA (RCW). NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence program.	Abernethy J., 2009, P 22 ANN C LEARN THE; Audibert JY, 2009, ANN STAT, V37, P1591, DOI 10.1214/08-AOS623; Bartlett PL, 2006, PROBAB THEORY REL, V135, P311, DOI 10.1007/s00440-005-0462-3; Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Gruanwald P., 2012, INT C ALG LEARN THEO, P169; Gruanwald P., 2011, P 24 ANN C LEARN THE, P397; Kalnishkan Y, 2005, LECT NOTES COMPUT SC, V3559, P188, DOI 10.1007/11503415_13; KEMPERMAN JH, 1968, ANN MATH STAT, V39, P93, DOI 10.1214/aoms/1177698508; Koltchinskii V, 2006, ANN STAT, V34, P2593, DOI 10.1214/009053606000001019; Lecue Guillaume, 2011, INTERPLAY CONCENTRAT; Lee WS, 1998, IEEE T INFORM THEORY, V44, P1974, DOI 10.1109/18.705577; Li J. Q., 1999, THESIS; Mammen E, 1999, ANN STAT, V27, P1808; Massart P, 2006, ANN STAT, V34, P2326, DOI 10.1214/009053606000000786; Mendelson S., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P1; Mendelson S, 2008, IEEE T INFORM THEORY, V54, P3797, DOI 10.1109/TIT.2008.926323; Mendelson S, 2008, J COMPLEXITY, V24, P380, DOI 10.1016/j.jco.2007.09.001; Tsybakov AB, 2004, ANN STAT, V32, P135; van Erven Tim, 2012, ADV NEURAL INFORM PR, V25, P1700; Vidyasagar M., 2002, LEARNING GEN APPL NE; Vovk V, 2001, INT STAT REV, V69, P213; Vovk V, 1998, J COMPUT SYST SCI, V56, P153, DOI 10.1006/jcss.1997.1556	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101105
C	Mi, YY; Li, LZ; Wang, DH; Wu, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Mi, Yuanyuan; Li, Luozheng; Wang, Dahui; Wu, Si			A Synaptical Story of Persistent Activity with Graded Lifetime in a Neural System	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				FACILITATION; DEPRESSION; SYNAPSES; NETWORKS	Persistent activity refers to the phenomenon that cortical neurons keep firing even after the stimulus triggering the initial neuronal responses is moved. Persistent activity is widely believed to be the substrate for a neural system retaining a memory trace of the stimulus information. In a conventional view, persistent activity is regarded as an attractor of the network dynamics, but it faces a challenge of how to be closed properly. Here, in contrast to the view of attractor, we consider that the stimulus information is encoded in a marginally unstable state of the network which decays very slowly and exhibits persistent firing for a prolonged duration. We propose a simple yet effective mechanism to achieve this goal, which utilizes the property of short-term plasticity (STP) of neuronal synapses. STP has two forms, short-term depression (STD) and short-term facilitation (STF), which have opposite effects on retaining neuronal responses. We find that by properly combining STF and STD, a neural system can hold persistent activity of graded lifetime, and that persistent activity fades away naturally without relying on an external drive. The implications of these results on neural information representation are discussed.	[Mi, Yuanyuan; Li, Luozheng] Beijing Normal Univ, State Key Lab Cognit Neurosci & Learning, Beijing 100875, Peoples R China; [Wang, Dahui] Beijing Normal Univ, Sch Syst Sci, State Key Lab Cognit Neurosci & Learning, Beijing 100875, Peoples R China; [Wu, Si] Beijing Normal Univ, IDG McGovern Inst Brain Res, State Key Lab Cognit Neurosci & Learning, Beijing 100875, Peoples R China	Beijing Normal University; Beijing Normal University; Beijing Normal University	Mi, YY (corresponding author), Beijing Normal Univ, State Key Lab Cognit Neurosci & Learning, Beijing 100875, Peoples R China.	miyuanyuan0102@163.com; liluozheng@mail.bnu.edu.cn; wangdh@bnu.edu.cn; wusi@bnu.edu.cn			National Key Basic Research Program of China [2014CB846101]; National Foundation of Natural Science of China [11305112, 31261160495, 31271169]; Fundamental Research Funds for the central Universities [31221003]; SRFDP [20130003110022]; Natural Science Foundation of Jiangsu Province [BK20130282]	National Key Basic Research Program of China(National Basic Research Program of China); National Foundation of Natural Science of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the central Universities(Fundamental Research Funds for the Central Universities); SRFDP(Specialized Research Fund for the Doctoral Program of Higher Education (SRFDP)); Natural Science Foundation of Jiangsu Province(Natural Science Foundation of Jiangsu Province)	This work is supported by grants from National Key Basic Research Program of China (NO.2014CB846101), and National Foundation of Natural Science of China (No.11305112, Y.Y.M.; No.31261160495, S.W.; No.31271169,D.H.W.), and the Fundamental Research Funds for the central Universities (No.31221003, S.W.), and SRFDP (No.20130003110022, S.W), and Natural Science Foundation of Jiangsu Province BK20130282.	Abbott LF, 2004, NATURE, V431, P796, DOI 10.1038/nature03010; AMARI SI, 1977, BIOL CYBERN, V27, P77, DOI 10.1007/BF00337259; Amit D. J, 1989, MODELLING BRAIN FUNC; Barak O, 2007, PLOS COMPUT BIOL, V3, P323, DOI 10.1371/journal.pcbi.0030035; Dittman JS, 2000, J NEUROSCI, V20, P1374; FUNAHASHI S, 1989, J NEUROPHYSIOL, V61, P331, DOI 10.1152/jn.1989.61.2.331; Fung CCA, 2012, NEURAL COMPUT, V24, P1147, DOI 10.1162/NECO_a_00269; FUSTER JM, 1971, SCIENCE, V173, P652, DOI 10.1126/science.173.3997.652; Gutkin BS, 2001, J COMPUT NEUROSCI, V11, P121, DOI 10.1023/A:1012837415096; Markram H, 1998, P NATL ACAD SCI USA, V95, P5323, DOI 10.1073/pnas.95.9.5323; Markram H, 1996, NATURE, V382, P807, DOI 10.1038/382807a0; Mongillo G, 2008, SCIENCE, V319, P1543, DOI 10.1126/science.1150769; Pfister JP, 2010, NAT NEUROSCI, V13, P1271, DOI 10.1038/nn.2640; Romo R, 1999, NATURE, V399, P470, DOI 10.1038/20939; Torres JJ, 2007, NEURAL COMPUT, V19, P2739, DOI 10.1162/neco.2007.19.10.2739; Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502; Tsodyks M., 2013, SCHOLARPEDIA, V8, P3153, DOI [10.4249/scholarpedia.3153, DOI 10.4249/SCHOLARPEDIA.3153, DOI 10.4249/SCH0LARPEDIA.3153]; Wang XJ, 1999, J NEUROSCI, V19, P9587; Wang Y, 2006, NAT NEUROSCI, V9, P534, DOI 10.1038/nn1670	19	0	0	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101076
C	Mi, YY; Fung, CCA; Wong, KYM; Wu, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Mi, Yuanyuan; Fung, C. C. Alan; Wong, K. Y. Michael; Wu, Si			Spike Frequency Adaptation Implements Anticipative Tracking in Continuous Attractor Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				HEAD-DIRECTION; ANTERIOR THALAMUS; PATH-INTEGRATION; DYNAMICS; REPRESENTATION; ORIENTATION; SIGNALS; MOTION; CORTEX; BUMP	To extract motion information, the brain needs to compensate for time delays that are ubiquitous in neural signal transmission and processing. Here we propose a simple yet effective mechanism to implement anticipative tracking in neural systems. The proposed mechanism utilizes the property of spike-frequency adaptation (SFA), a feature widely observed in neuronal responses. We employ continuous attractor neural networks (CANNs) as the model to describe the tracking behaviors in neural systems. Incorporating SFA, a CANN exhibits intrinsic mobility, manifested by the ability of the CANN to support self-sustained travelling waves. In tracking a moving stimulus, the interplay between the external drive and the intrinsic mobility of the network determines the tracking performance. Interestingly, we find that the regime of anticipation effectively coincides with the regime where the intrinsic speed of the travelling wave exceeds that of the external drive. Depending on the SFA amplitudes, the network can achieve either perfect tracking, with zero-lag to the input, or perfect anticipative tracking, with a constant leading time to the input. Our model successfully reproduces experimentally observed anticipative tracking behaviors, and sheds light on our understanding of how the brain processes motion information in a timely manner.	[Mi, Yuanyuan] Beijing Normal Univ, State Key Lab Cognit Neurosci & Learning, Beijing 100875, Peoples R China; [Fung, C. C. Alan; Wong, K. Y. Michael] Hong Kong Univ Sci & Technol, Dept Phys, Hong Kong, Peoples R China; [Wu, Si] Beijing Normal Univ, IDG McGovern Inst Brain Res, State Key Lab Cognit Neurosci & Learning, Beijing 100875, Peoples R China	Beijing Normal University; Hong Kong University of Science & Technology; Beijing Normal University	Mi, YY (corresponding author), Beijing Normal Univ, State Key Lab Cognit Neurosci & Learning, Beijing 100875, Peoples R China.	miyuanyuan0102@bnu.edu.cn; phccfung@ust.hk; phkywong@ust.hk; wusi@bnu.edu.cn			National Key Basic Research Program of China [2014CB846101]; National Foundation of Natural Science of China [11305112, 31261160495]; Fundamental Research Funds for the central Universities [31221003]; SRFDP [20130003110022]; Research Grants Council of Hong Kong [605813, 604512, N_HKUST606/12]; Natural Science Foundation of Jiangsu Province [BK20130282]	National Key Basic Research Program of China(National Basic Research Program of China); National Foundation of Natural Science of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the central Universities(Fundamental Research Funds for the Central Universities); SRFDP(Specialized Research Fund for the Doctoral Program of Higher Education (SRFDP)); Research Grants Council of Hong Kong(Hong Kong Research Grants Council); Natural Science Foundation of Jiangsu Province(Natural Science Foundation of Jiangsu Province)	This work is supported by grants from National Key Basic Research Program of China (NO.2014CB846101, S.W.), and National Foundation of Natural Science of China (No.11305112, Y.Y.M.; No. 31261160495, S.W.), and the Fundamental Research Funds for the central Universities (No. 31221003, S. W.), and SRFDP (No.20130003110022, S.W), and Research Grants Council of Hong Kong (Nos. 605813, 604512 and N_HKUST606/12, C.C.A.F. and K.Y.W), and Natural Science Foundation of Jiangsu Province BK20130282.	Bassett JP, 2005, J NEUROPHYSIOL, V93, P1304, DOI 10.1152/jn.00490.2004; Benda J, 2003, NEURAL COMPUT, V15, P2523, DOI 10.1162/089976603322385063; BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844; BLAIR HT, 1995, J NEUROSCI, V15, P6260; Bressloff PC, 2012, J PHYS A-MATH THEOR, V45, DOI 10.1088/1751-8113/45/3/033001; DUHAMEL JR, 1992, SCIENCE, V255, P90, DOI 10.1126/science.1553535; Fung C. C. A., 2012, ADV NEURAL INFORM PR, V25, P1097; Fung CCA, 2012, NEURAL COMPUT, V24, P1147, DOI 10.1162/NECO_a_00269; Fung CCA, 2010, NEURAL COMPUT, V22, P752, DOI 10.1162/neco.2009.07-08-824; GEORGOPOULOS AP, 1993, SCIENCE, V260, P47, DOI 10.1126/science.8465199; Goodridge JP, 2000, J NEUROPHYSIOL, V83, P3402, DOI 10.1152/jn.2000.83.6.3402; Gutkin B., 2014, SCHOLARPEDIA, V9, P30643, DOI [10.4249/scholarpedia.30643, DOI 10.4249/SCHOLARPEDIA.30643]; Koch C, 1996, CEREB CORTEX, V6, P93, DOI 10.1093/cercor/6.2.93; NIJHAWAN R, 1994, NATURE, V370, P256, DOI 10.1038/370256b0; Nijhawan R, 2009, PHILOS T R SOC A, V367, P1063, DOI 10.1098/rsta.2008.0270; NOWAK LG, 1995, VISUAL NEUROSCI, V12, P371, DOI 10.1017/S095252380000804X; Samsonovich A, 1997, J NEUROSCI, V17, P5900; Sharp PE, 2001, BEHAV NEUROSCI, V115, P571, DOI 10.1037/0735-7044.115.3.571; Taube JS, 1998, HIPPOCAMPUS, V8, P87, DOI 10.1002/(SICI)1098-1063(1998)8:2<87::AID-HIPO1>3.0.CO;2-4; Wimmer K, 2014, NAT NEUROSCI, V17, P431, DOI 10.1038/nn.3645; Zhang K, 1996, J NEUROSCI, V16, P2112; Zhang WH, 2012, NEURAL COMPUT, V24, P1695, DOI 10.1162/NECO_a_00296	22	0	0	0	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103074
C	Mittal, H; Goyal, P; Gogate, V; Singla, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Mittal, Happy; Goyal, Prasoon; Gogate, Vibhav; Singla, Parag			New Rules for Domain Independent Lifted MAP Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Lifted inference algorithms for probabilistic first-order logic frameworks such as Markov logic networks (MLNs) have received significant attention in recent years. These algorithms use so called lifting rules to identify symmetries in the first-order representation and reduce the inference problem over a large probabilistic model to an inference problem over a much smaller model. In this paper, we present two new lifting rules, which enable fast MAP inference in a large class of MLNs. Our first rule uses the concept of single occurrence equivalence class of logical variables, which we define in the paper. The rule states that the MAP assignment over an MLN can be recovered from a much smaller MLN, in which each logical variable in each single occurrence equivalence class is replaced by a constant (i.e., an object in the domain of the variable). Our second rule states that we can safely remove a subset of formulas from the MLN if all equivalence classes of variables in the remaining MLN are single occurrence and all formulas in the subset are tautology (i.e., evaluate to true) at extremes (i.e., assignments with identical truth value for groundings of a predicate). We prove that our two new rules are sound and demonstrate via a detailed experimental evaluation that our approach is superior in terms of scalability and MAP solution quality to the state of the art approaches.	[Mittal, Happy; Goyal, Prasoon; Singla, Parag] IIT Delhi, Dept Comp Sci & Engn, New Delhi 110016, India; [Gogate, Vibhav] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75080 USA	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Delhi; University of Texas System; University of Texas Dallas	Mittal, H (corresponding author), IIT Delhi, Dept Comp Sci & Engn, New Delhi 110016, India.	happy.mittal@cse.lltd.ac.in; prasoongoyall3@gmail.com; vgogate@hlt.utdallas.edu; Parags@cse.lltd.ac.in			TCS Research Scholar Program; DARPA Probabilistic Programming for Advanced Machine Learning Program under AFRL prime [FA8750-14-C-0005]	TCS Research Scholar Program; DARPA Probabilistic Programming for Advanced Machine Learning Program under AFRL prime	Happy Mittal was supported by TCS Research Scholar Program. Vibhav Gogate was partially supported by the DARPA Probabilistic Programming for Advanced Machine Learning Program under AFRL prime contract number FA8750-14-C-0005. We are grateful to Somdeb Sarkhel and Deepak Venugopal for sharing their code and also for helpful discussions.	[Anonymous], 2013, GUROBI OPTIMIZER REF; Braz RD, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1319; Bui Hung Hai, 2013, UAI, P132; de Salvo Braz R., 2006, P 21 C ART INT AAAI; Domingos P., 2009, SYNTHESIS LECT ARTIF; Gogate V., 2011, UAI 2011 P 27 C UNC, P256; Gogate V., 2012, P 26 ANN C NEUR INF, P1664; Gogate Vibhav, 2012, P AAAI 12, P1910; Jha A., 2010, P 24 ANN C NEUR INF, P973; Kautz H, 1997, COMMUN ACM, V40, P63, DOI 10.1145/245108.245123; Kersting K., 2009, P 25 C UNC ART INT, P277; Kersting K, 2012, FRONT ARTIF INTEL AP, V242, P33, DOI 10.3233/978-1-61499-098-7-33; Kok S., 2008, TECHNICAL REPORT; Mladenov M, 2014, JMLR WORKSH CONF PRO, V33, P623; Niepert M, 2014, AAAI CONF ARTIF INTE, P2467; Noessner Jan, 2013, P 27 AAAI C ART INT, P739, DOI DOI 10.1007/BFB0027523; Poole D., 2003, P INT JOINT C ART IN, P985; Russell SP, 2010, ARTIFICIAL INTELLIGE; Sarkhel S., 2014, P AISTATS 14, P895; Singla P., 2008, P 23 AAAI C ART INT, V8, P1094; VandenBroeck G, 2011, ADV NEURAL INFORM PR, V24, P1386	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102059
C	Mizrahi, YD; Denil, M; de Freitas, N		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Mizrahi, Yariv D.; Denil, Misha; de Freitas, Nando			Distributed Parameter Estimation in Probabilistic Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				LIKELIHOOD	This paper presents foundational theoretical results on distributed parameter estimation for undirected probabilistic graphical models. It introduces a general condition on composite likelihood decompositions of these models which guarantees the global consistency of distributed estimators, provided the local estimators are consistent.	[Mizrahi, Yariv D.] Univ British Columbia, Vancouver, BC, Canada; [Denil, Misha; de Freitas, Nando] Univ Oxford, Oxford, England; [de Freitas, Nando] Canadian Inst Adv Res, Toronto, ON, Canada; [de Freitas, Nando] Google DeepMind, London, England	University of British Columbia; University of Oxford; Canadian Institute for Advanced Research (CIFAR); Google Incorporated	Mizrahi, YD (corresponding author), Univ British Columbia, Vancouver, BC, Canada.	yariv@math.ubc.ca; misha.denil@cs.ox.ac.uk; nando@cs.ox.ac.uk						ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; Asuncion Arthur, 2010, P 13 INT C ART INT S, P33; BESAG J, 1974, J ROY STAT SOC B MET, V36, P192; Bremaud P, 2001, MARKOV CHAINS GIBBS; Dillon JV, 2010, J MACH LEARN RES, V11, P2597; Fienberg SE, 2012, ANN STAT, V40, P996, DOI 10.1214/12-AOS986; Griffeath D, 1976, DENUMERABLE MARKOV C, V40, P425; Guestrin C, 2012, INT C ART INT STAT, P136; Hammersley J.M., 1971, MARKOV FIELDS FINITE; Koller D., 2009, PROBABILISTIC GRAPHI; Liang P, 2008, P 25 INT C MACH LEAR, P584, DOI [10.1145/1390156.1390230, DOI 10.1145/1390156.1390230]; Lindsay BG, 1988, CONT MATH, V80, P221, DOI DOI 10.1090/CONM/080/999014; Liu Q., 2012, INT C MACH LEARN; Mardia KV, 2009, BIOMETRIKA, V96, P975, DOI 10.1093/biomet/asp056; Marlin B., 2011, UAI, P497; Marlin BM., 2010, P 13 INT C ART INT S, V9, P509; Meng Z., 2013, P ART INT STAT, P39; Meng Z., 2014, TECHNICAL REPORT; Mizrahi Y., 2014, INT C MACH LEARN; Nowozin S., 2013, ICML WORKSH INF INT; Okabayashi S, 2011, STAT SINICA, V21, P331; Smolensky P., 1986, PARALLEL DISTRIBUTED, V1, P194; STRAUSS D, 1990, J AM STAT ASSOC, V85, P204, DOI 10.2307/2289546; Vaart A. W., 1998, ASYMPTOTIC STAT; Varin C, 2011, STAT SINICA, V21, P5; Wiesel A, 2012, IEEE T SIGNAL PROCES, V60, P211, DOI 10.1109/TSP.2011.2172430	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100096
C	Mobin, SA; Arnemann, JA; Sommer, FT		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Mobin, Shariq A.; Arnemann, James A.; Sommer, Friedrich T.			Information-based learning by agents in unbounded state spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The idea that animals might use information-driven planning to explore an unknown environment and build an internal model of it has been proposed for quite some time. Recent work has demonstrated that agents using this principle can efficiently learn models of probabilistic environments with discrete, bounded state spaces. However, animals and robots are commonly confronted with unbounded environments. To address this more challenging situation, we study information-based learning strategies of agents in unbounded state spaces using non-parametric Bayesian models. Specifically, we demonstrate that the Chinese Restaurant Process (CRP) model is able to solve this problem and that an Empirical Bayes version is able to efficiently explore bounded and unbounded worlds by relying on little prior information.	[Mobin, Shariq A.; Arnemann, James A.; Sommer, Friedrich T.] Univ Calif Berkeley, Redwood Ctr Theoret Neurosci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Mobin, SA (corresponding author), Univ Calif Berkeley, Redwood Ctr Theoret Neurosci, Berkeley, CA 94720 USA.	shariqmobin@berkeley.edu; arnemann@berkeley.edu; fsommer@berkeley.edu			NSF [IIS-1111765]; Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program of the U.S. Department of Energy [DE-AC02-05CH11231]	NSF(National Science Foundation (NSF)); Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program of the U.S. Department of Energy(United States Department of Energy (DOE))	JAA was funded by NSF grant IIS-1111765. FTS was supported by the Director, Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. The authors thank Bruno Olshausen, Tamara Broderick, and the members of the Redwood Center for Theoretical Neuroscience for their valuable input.	Aldous D.J., 1985, LECT NOTES MATH, V1117, P1, DOI DOI 10.1007/BFB0099421; Asmuth J., 2009, P 25 C UNC ART INT, P19; Ay N, 2008, EUR PHYS J B, V63, P329, DOI 10.1140/epjb/e2008-00175-0; Bellman E., 1957, DYNAMIC PROGRAMMING; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Gimbert H, 2007, LECT NOTES COMPUT SC, V4393, P200; Gottlieb J, 2013, TRENDS COGN SCI, V17, P585, DOI 10.1016/j.tics.2013.09.001; Ishwaran H, 2003, STAT SINICA, V13, P1211; ITTI L, 2009, VISION RES, V49, P1295, DOI DOI 10.1016/J.VISRES.2008.09.007; Lee J, 2013, STAT SCI, V28, P209, DOI 10.1214/12-STS407; Little D. Y., 2011, ARXIV11121125; Little DY, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00037; MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590; PITMAN J, 1995, PROBAB THEORY REL, V102, P145, DOI 10.1007/BF01213386; Poupart P., 2006, ICML, P697; SATO M, 1988, IEEE T SYST MAN CYB, V18, P677, DOI 10.1109/21.21595; Somani A., 2013, ADV NEURAL INFORM PR, P1772; Storck J., 1995, ICANN '95. International Conference on Artificial Neural Networks. Neuronimes '95 Scientific Conference, P159; Strens, 2000, P 17 INT C MACH LEAR, P943; Thrun S.B., 1992, EFFICIENT EXPLORATIO; Yi Sun, 2011, Artificial General Intelligence. Proceedings 4th International Conference, AGI 2011, P41, DOI 10.1007/978-3-642-22887-2_5; Zhang J., ADV NEURAL INFORM PR, P1617	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100045
C	Mohan, K; Pearl, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Mohan, Karthika; Pearl, Judea			Graphical Models for Recovering Probabilistic and Causal Queries from Missing Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We address the problem of deciding whether a causal or probabilistic query is estimable from data corrupted by missing entries, given a model of missingness process. We extend the results of Mohan et al. [2013] by presenting more general conditions for recovering probabilistic queries of the form P(y vertical bar x) and P(y,x) as well as causal queries of the form P(y vertical bar do(x)). We show that causal queries may be recoverable even when the factors in their identifying estimands are not recoverable. Specifically, we derive graphical conditions for recovering causal effects of the form P(y vertical bar do(x)) when Y and its missingness mechanism are not d-separable. Finally, we apply our results to problems of attrition and characterize the recovery of causal effects from data corrupted by attrition.	[Mohan, Karthika; Pearl, Judea] Univ Calif Los Angeles, Comp Sci Dept, Cognit Syst Lab, Los Angeles, CA 90024 USA	University of California System; University of California Los Angeles	Mohan, K (corresponding author), Univ Calif Los Angeles, Comp Sci Dept, Cognit Syst Lab, Los Angeles, CA 90024 USA.	karthika@cs.ucla.edu; judea@cs.ucla.edu			NSF [IIS1249822, IIS1302448]; ONR [N00014-13-1-0153, N00014-10-1-0933]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	This paper has benefited from discussions with Ilya Shpitser. This research was supported in parts by grants from NSF #IIS1249822 and #IIS1302448, and ONR #N00014-13-1-0153 and #N00014-10-1-0933.	Allison P. D, 2002, MISSING DATA SERIES; Bonissone P., 1991, P 6 C UNC ART INT, P255; Daniel RM, 2012, STAT METHODS MED RES, V21, P243, DOI 10.1177/0962280210394469; Darwiche A, 2009, MODELING AND REASONING WITH BAYESIAN NETWORKS, P1; Garcia F. M., 2013, WORKING PAPER; HEITJAN DF, 1991, ANN STAT, V19, P2244, DOI 10.1214/aos/1176348396; Koller D., 2009, PROBABILISTIC GRAPHI; LAURITZEN SL, 1995, COMPUT STAT DATA AN, V19, P191, DOI 10.1016/0167-9473(93)E0056-A; Little R. J., 2019, STAT ANAL MISSING DA, V793; Marlin B., 2011, IJCAI; Marlin B. M., 2007, UAI; Marlin Benjamin M, 2009, RECSYS, P5, DOI DOI 10.1145/1639714.1639717; Mohan K., 2014, P AISTAT; Mohan K, 2013, ADV NEUTRAL INFORM P, V26, P1277; Pearl J., 2008, P 23 AAAI C ART INT, V2, P1081; Pearl J, 2013, R417 UCLA; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Richardson T, 2003, SCAND J STAT, V30, P145, DOI 10.1111/1467-9469.00323; ROBINS JM, 1994, J AM STAT ASSOC, V89, P846, DOI 10.2307/2290910; Robins JM., 1992, AIDS EPIDEMIOLOGY ME, P297; Rose, 2013, R002 CORN U; Rothman KJ., 2008, MODERN EPIDEMIOLOGY, V3rd ed; RUBIN DB, 1976, BIOMETRIKA, V63, P581, DOI 10.2307/2335739; Shadish WR, 2002, PSYCHOL METHODS, V7, P3, DOI 10.1037//1082-989X.7.1.3; Shpitser I, 2006, P 22 C UNCERTAINTY A, P437, DOI [10.48550/ARXIV.1206.6876, DOI 10.48550/ARXIV.1206.6876]; Twisk J, 2002, J CLIN EPIDEMIOL, V55, P329, DOI 10.1016/S0895-4356(01)00476-0; Van der Laan M., 2003, SPR S STAT; van der Laan MJ, 1998, J AM STAT ASSOC, V93, P693, DOI 10.2307/2670119	28	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103026
C	Mohapatra, P; Jawahar, CV; Kumar, MP		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Mohapatra, Pritish; Jawahar, C., V; Kumar, M. Pawan			Efficient Optimization for Average Precision SVM	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The accuracy of information retrieval systems is often measured using average precision (AP). Given a set of positive (relevant) and negative (non-relevant) samples, the parameters of a retrieval system can be estimated using the AP-SVM framework, which minimizes a regularized convex upper bound on the empirical AP loss. However, the high computational complexity of loss-augmented inference, which is required for learning an AP-SVM, prohibits its use with large training datasets. To alleviate this deficiency, we propose three complementary approaches. The first approach guarantees an asymptotic decrease in the computational complexity of loss-augmented inference by exploiting the problem structure. The second approach takes advantage of the fact that we do not require a full ranking during loss-augmented inference. This helps us to avoid the expensive step of sorting the negative samples according to their individual scores. The third approach approximates the AP loss over all samples by the AP loss over difficult samples (for example, those that are incorrectly classified by a binary SVM), while ensuring the correct classification of the remaining samples. Using the PASCAL VOC action classification and object detection datasets, we show that our approaches provide significant speed-ups during training without degrading the test accuracy of AP-SVM.	[Mohapatra, Pritish; Jawahar, C., V] IIIT Hyderabad, Hyderabad, Telangana, India; [Kumar, M. Pawan] Ecole Cent Paris, Paris, France; [Kumar, M. Pawan] INRIA Saclay, Palaiseau, France	International Institute of Information Technology Hyderabad; UDICE-French Research Universities; Universite Paris Saclay	Mohapatra, P (corresponding author), IIIT Hyderabad, Hyderabad, Telangana, India.	pritish.mohapatra@research.iiit.ac.in; jawahar@iiit.ac.in; pawan.kumar@ecp.fr		Jawahar, C. V./0000-0001-6767-7057	European Research Council under the European Community's Seventh Framework Programme (FP7/2007-2013)/ERC [259112]; TCS Research Scholar Program	European Research Council under the European Community's Seventh Framework Programme (FP7/2007-2013)/ERC; TCS Research Scholar Program	This work is partially funded by the European Research Council under the European Community's Seventh Framework Programme (FP7/2007-2013)/ERC Grant agreement number 259112. Pritish is supported by the TCS Research Scholar Program.	[Anonymous], BMVC; Behl A., 2014, CVPR; Blaschko M., 2014, GCPR; Boix X., 2012, ECCV; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dokania P., 2014, ECCV; Everingham M., PASCAL VISUAL OBJECT; Everingham M., 2010, IJCV; Girshick R., 2014, CVPR, DOI 10.1109/CVPR.2014.81; Hofmann T., 2004, P 21 INT C MACH LEAR, P104, DOI 10.1145/1015330.1015341; Jaderberg M., 2014, BMVC; Kim D., 2006, MULTIOBJECTIVE MACHI; Maji S., 2011, CVPR; Shen C., 2010, ARXIV10085188; Szegedy C., 2013, NIPS; Taskar Ben, 2003, NIPS; Uijlings J., 2013, IJCV; Yang J., 2010, ECCV; Yue Yisong, 2007, SIGIR	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101033
C	Mohler, GO		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Mohler, George O.			Learning convolution filters for inverse covariance estimation of neural network connectivity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We consider the problem of inferring direct neural network connections from Calcium imaging time series. Inverse covariance estimation has proven to be a fast and accurate method for learning macro- and micro-scale network connectivity in the brain and in a recent Kaggle Connectomics competition inverse covariance was the main component of several top ten solutions, including our own and the winning team's algorithm. However, the accuracy of inverse covariance estimation is highly sensitive to signal preprocessing of the Calcium fluorescence time series. Furthermore, brute force optimization methods such as grid search and coordinate ascent over signal processing parameters is a time intensive process, where learning may take several days and parameters that optimize one network may not generalize to networks with different size and parameters. In this paper we show how inverse covariance estimation can be dramatically improved using a simple convolution filter prior to applying sample covariance. Furthermore, these signal processing parameters can be learned quickly using a supervised optimization algorithm. In particular, we maximize a binomial log-likelihood loss function with respect to a convolution filter of the time series and the inverse covariance regularization parameter. Our proposed algorithm is relatively fast on networks the size of those in the competition (1000 neurons), producing AUC scores with similar accuracy to the winning solution in training time under 2 hours on a cpu. Prediction on new networks of the same size is carried out in less than 15 minutes, the time it takes to read in the data and write out the solution.	[Mohler, George O.] Santa Clara Univ, Dept Math & Comp Sci, Santa Clara, CA 95053 USA	Santa Clara University	Mohler, GO (corresponding author), Santa Clara Univ, Dept Math & Comp Sci, Santa Clara, CA 95053 USA.	gmohler@scu.edu						Banerjee O, 2008, J MACH LEARN RES, V9, P485; Bradley AP, 1997, PATTERN RECOGN, V30, P1145, DOI 10.1016/S0031-3203(96)00142-2; Bressler SL, 2011, NEUROIMAGE, V58, P323, DOI 10.1016/j.neuroimage.2010.02.059; Burges C., 2011, P LEARN RANK CHALL, P25; Dahlhaus R, 1997, J NEUROSCI METH, V77, P93, DOI 10.1016/S0165-0270(97)00100-3; Eldawlatly S, 2010, NEURAL COMPUT, V22, P158, DOI 10.1162/neco.2009.11-08-900; Errais E, 2010, SIAM J FINANC MATH, V1, P642, DOI 10.1137/090771272; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Friedman Jerome, 2013, PACKAGE RGLASSO; Hsieh C.-J., 2011, ADV NEURAL INFORM PR, P2330; Huang S., 2009, ADV NEURAL INFORM PR, V22, P808; Ledoit O, 2004, J MULTIVARIATE ANAL, V88, P365, DOI 10.1016/S0047-259X(03)00096-4; Minet O, 2008, MED LASER APP, V23, P216; Mishchenko Y, 2011, ANN APPL STAT, V5, P1229, DOI 10.1214/09-AOAS303; Ng B, 2012, LECT NOTES COMPUT SC, V7510, P707, DOI 10.1007/978-3-642-33415-3_87; Roudi Y, 2009, PHYS REV E, V79, DOI 10.1103/PhysRevE.79.051915; Shandilya SG, 2011, NEW J PHYS, V13, DOI 10.1088/1367-2630/13/1/013004; Stetter O, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002653; Stomakhin A, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/11/115013; Van Bussel F, 2011, FRONT COMPUT NEUROSC, V5, DOI 10.3389/fncom.2011.00003; Varoquaux G., 2010, ADV NEURAL INFORM PR, P2334	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103077
C	Mohri, M; Medina, AM		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Mohri, Mehryar; Medina, Andres Munoz			Optimal Regret Minimization in Posted-Price Auctions with Strategic Buyers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We study revenue optimization learning algorithms for posted-price auctions with strategic buyers. We analyze a very broad family of monotone regret minimization algorithms for this problem, which includes the previously best known algorithm, and show that no algorithm in that family admits a strategic regret more favorable than Omega(root T). We then introduce a new algorithm that achieves a strategic regret differing from the lower bound only by a factor in O(log T), an exponential improvement upon the previous best algorithm. Our new algorithm admits a natural analysis and simpler proofs, and the ideas behind its design are general. We also report the results of empirical evaluations comparing our algorithm with the previous state of the art and show a consistent exponential improvement in several different scenarios.	[Mohri, Mehryar] Courant Inst & Google Res, 251 Mercer St, New York, NY 10012 USA; [Medina, Andres Munoz] Courant Inst, New York, NY 10012 USA	Google Incorporated	Mohri, M (corresponding author), Courant Inst & Google Res, 251 Mercer St, New York, NY 10012 USA.	mohri@cims.nyu.edu; munoz@cims.nyu.edu			NSF [IIS-1117591]	NSF(National Science Foundation (NSF))	We thank Kareem Amin, Afshin Rostamizadeh and Umar Syed for several discussions about the topic of this paper. This work was partly funded by the NSF award IIS-1117591.	AGRAWAL R, 1995, SIAM J CONTROL OPTIM, V33, P1926, DOI 10.1137/S0363012992237273; Amin K., 2013, ADV NEURAL INFORM PR, V1, P1169; Arora R., 2012, P ICML; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Cesa-Bianchi N, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1190; Edelman B., 2007, DECISION SUPPORT SYS, V43; He D., 2013, IJCAI 2013, P206; Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232; Kuleshov V., 2010, J MACHINE LEARNING; Michael Ostrovsky, 2011, P 12 ACM C ELECT COM, P59, DOI DOI 10.1145/1993574.1993585; MILGROM PR, 1982, ECONOMETRICA, V50, P1089, DOI 10.2307/1911865; Mohri M., 2014, P ICML; Morris P., 1994, INTRO GAME THEORY, P115; Nachbar JH, 1997, ECONOMETRICA, V65, P275, DOI 10.2307/2171894; Nachbar JH, 2001, SOC CHOICE WELFARE, V18, P303, DOI 10.1007/PL00007181; Robbins Herbert, 1985, H ROBBINS SELECTED P, P169; VICKREY W, 1961, J FINANC, V16, P8, DOI 10.2307/2977633	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101109
C	Mohri, M; Yang, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Mohri, Mehryar; Yang, Scott			Conditional Swap Regret and Conditional Correlated Equilibrium	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				GAMES	We introduce a natural extension of the notion of swap regret, conditional swap regret, that allows for action modifications conditioned on the player's action history. We prove a series of new results for conditional swap regret minimization. We present algorithms for minimizing conditional swap regret with bounded conditioning history. We further extend these results to the case where conditional swaps are considered only for a subset of actions. We also define a new notion of equilibrium, conditional correlated equilibrium, that is tightly connected to the notion of conditional swap regret: when all players follow conditional swap regret minimization strategies, then the empirical distribution approaches this equilibrium. Finally, we extend our results to the multi-armed bandit scenario.	[Mohri, Mehryar; Yang, Scott] Courant Inst, 251 Mercer St, New York, NY 10012 USA; [Mohri, Mehryar] Google, New York, NY 10012 USA	Google Incorporated	Mohri, M (corresponding author), Courant Inst, 251 Mercer St, New York, NY 10012 USA.	mchri@cims.nyu.edu; yangs@cims.nyu.edu			NSF [IIS-1117591]; National Science Foundation Graduate Research Fellowship [DGE 1342536]	NSF(National Science Foundation (NSF)); National Science Foundation Graduate Research Fellowship(National Science Foundation (NSF))	We thank the reviewers for their comments, many of which were very insightful. We are particularly grateful to the reviewer who found an issue in our discussion on conditional correlated equilibrium and proposed a helpful resolution. This work was partly funded by the NSF award IIS-1117591. The material is also based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE 1342536.	Arora R., 2012, ICML; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Aumann Robert, 1974, J MATH ECON, V1, P67, DOI 10.1016/0304-4068(74)90037-8; Bau III D, 1997, NUMERICAL LINEAR ALG; Blum A, 2007, J MACH LEARN RES, V8, P1307; Bubeck Sebastien, 2012, CORR; Cesa-Bianchi N., 2013, P 26 INT C NEUR INF, P1160; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; FORGES F, 1986, ECONOMETRICA, V54, P1375, DOI 10.2307/1914304; Foster DP, 1997, GAME ECON BEHAV, V21, P40, DOI 10.1006/game.1997.0595; Greenwald A.R., 2008, COLT, P239; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; LEHRER E, 1992, MATH OPER RES, V17, P175, DOI 10.1287/moor.17.1.175; Nisan N, 2007, ALGORITHMIC GAME THEORY, P1, DOI 10.1017/CBO9780511800481; Stoltz G, 2007, GAME ECON BEHAV, V59, P187, DOI 10.1016/j.geb.2006.04.007	16	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101040
C	Montanari, A; Richard, E		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Montanari, Andrea; Richard, Emile			A statistical model for tensor PCA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We consider the Principal Component Analysis problem for large tensors of arbitrary order k under a single-spike (or rank-one plus noise) model. On the one hand, we use information theory, and recent results in probability theory, to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources. It turns out that this is possible as soon as the signal-to-noise ratio beta becomes larger than C root k log k (and in particular beta can remain bounded as the problem dimensions increase). On the other hand, we analyze several polynomial-time estimation algorithms, based on tensor unfolding, power iteration and message passing ideas from graphical models. We show that, unless the signal-to-noise ratio diverges in the system dimensions, none of these approaches succeeds. This is possibly related to a fundamental limitation of computationally tractable estimators for this problem. We discuss various initializations for tensor power iteration, and show that a tractable initialization based on the spectrum of the unfolded tensor outperforms significantly baseline methods, statistically and computationally. Finally, we consider the case in which additional side information is available about the unknown signal. We characterize the amount of side information that allows the iterative algorithms to converge to a good estimate.	[Montanari, Andrea] Stanford Univ, Stat & Elect Engn, Stanford, CA 94305 USA; [Richard, Emile] Stanford Univ, Elect Engn, Stanford, CA 94305 USA	Stanford University; Stanford University	Montanari, A (corresponding author), Stanford Univ, Stat & Elect Engn, Stanford, CA 94305 USA.				NSF [CCF-1319979]; AFOSR/DARPA [FA9550-12-1-0411, FA9550-13-1-0036]	NSF(National Science Foundation (NSF)); AFOSR/DARPA(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)Defense Advanced Research Projects Agency (DARPA))	This work was partially supported by the NSF grant CCF-1319979 and the grants AFOSR/DARPA FA9550-12-1-0411 and FA9550-13-1-0036.	Anandkumar A., 2012, ARXIV12107559; Auffinger A, 2013, COMMUN PUR APPL MATH, V66, P165, DOI 10.1002/cpa.21422; Bai Z, 2010, SPRINGER SER STAT, P1, DOI 10.1007/978-1-4419-0661-8; Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817; Feral D, 2009, J MATH PHYS, V50, DOI 10.1063/1.3155785; GEMAN S, 1980, ANN PROBAB, V8, P252, DOI 10.1214/aop/1176994775; Hillar C., 2009, J ACM, V6; Johnstone Iain M, 2009, J AM STAT ASS, V104; Kamilov U. S., 2012, P NEUR INF PROC SYST, P2447; Kolda TG, 2011, SIAM J MATRIX ANAL A, V32, P1095, DOI 10.1137/100801482; Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39; Montanari A., 2014, ARXIV14064775; Mu C., 2013, INT C MACH LEARN ICM; Romera-Paredes B., 2013, NEURAL INFORM PROCES; Schniter P., 2011, P WORKSH SIGN PROC A, P68; Schniter P, 2012, ANN ALLERTON CONF, P815, DOI 10.1109/Allerton.2012.6483302; Shawe-Taylor J., 2011, P NEUR INF PROC SYST, P2555; Tomioka R., 2011, NEURAL INFORM PROCES; WATERHOUSE WC, 1990, LINEAR ALGEBRA APPL, V128, P97, DOI 10.1016/0024-3795(90)90284-J; Wedin P.-A., 1972, BIT (Nordisk Tidskrift for Informationsbehandling), V12, P99, DOI 10.1007/BF01932678	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103067
C	Naghibi, T; Pfister, B		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Naghibi, Tofigh; Pfister, Beat			A Boosting Framework on Grounds of Online Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					By exploiting the duality between boosting and online learning, we present a boosting framework which proves to be extremely powerful thanks to employing the vast knowledge available in the online learning area. Using this framework, we develop various algorithms to address multiple practically and theoretically interesting questions including sparse boosting, smooth-distribution boosting, agnostic learning and, as a by-product, some generalization to double-projection online learning algorithms.	[Naghibi, Tofigh; Pfister, Beat] Swiss Fed Inst Technol, Comp Engn & Networks Lab, Zurich, Switzerland	ETH Zurich	Naghibi, T (corresponding author), Swiss Fed Inst Technol, Comp Engn & Networks Lab, Zurich, Switzerland.	naghibi@tik.ee.ethz.ch; pfister@tik.ee.ethz.ch			SNSF	SNSF(Swiss National Science Foundation (SNSF))	This work was partially supported by SNSF. We would like to thank Professor Rocco Servedio for an inspiring email conversation and our colleague Hui Liang for his helpful comments.	Ben-David S., 2001, COLT; Bradley J. K., 2008, NIPS; Breiman L., 1999, NEURAL COMPUTATION; Breiman Leo, 1997, TECHNICAL REPORT; Bshouty N. H., 2002, J MACHINE LEARNING R; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chiang Chao-Kai, 2012, COLT; Dai Wenyuan, 2007, ICML; Domingo C., 2000, COLT; Duchi J., 2010, COLT; FREUND Y, 1996, COLT; Freund Y., 1995, J INFORM COMPUTATION; Freund Y., 1997, J COMPUTER SYSTEM SC; Friedman J., 1998, ANN STAT; Gavinsky D., 2003, J MACHINE LEARNING R; Hatano K., 2006, ALGORITHMIC LEARNING; Hazan E., 2009, SURVEY CONVEX OPTIMI; Kalai A., 2009, NIPS; Kearns M. J., 1992, COLT; Mason L., 1999, NIPS; Rakhlin Alexander, 2013, COLT; Schapire R. E., 1990, J MACHINE LEARNING R; Servedio R. A., 2003, J MACHINE LEARNING R; Shalev-Shwartz Shai, 2008, COLT; Wang W, 2013, PROJECTION ONTO PROB; Warmuth M. K., 2006, ICML; Zinkevich M, 2003, ICML	27	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102073
C	Narasimhan, H; Vaish, R; Agarwal, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Narasimhan, Harikrishna; Vaish, Rohit; Agarwal, Shivani			On the Statistical Consistency of Plug-in Classifiers for Non-decomposable Performance Measures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				CLASSIFICATION; RISK	We study consistency properties of algorithms for non-decomposable performance measures that cannot be expressed as a sum of losses on individual data points, such as the F-measure used in text retrieval and several other performance measures used in class imbalanced settings. While there has been much work on designing algorithms for such performance measures, there is limited understanding of the theoretical properties of these algorithms. Recently, Ye et al. (2012) showed consistency results for two algorithms that optimize the F-measure, but their results apply only to an idealized setting, where precise knowledge of the underlying probability distribution (in the form of the 'true' posterior class probability) is available to a learning algorithm. In this work, we consider plug-in algorithms that learn a classifier by applying an empirically determined threshold to a suitable 'estimate' of the class probability, and provide a general methodology to show consistency of these methods for any non-decomposable measure that can be expressed as a continuous function of true positive rate (TPR) and true negative rate (TNR), and for which the Bayes optimal classifier is the class probability function thresholded suitably. We use this template to derive consistency results for plug-in algorithms for the F-measure and for the geometric mean of TPR and precision; to our knowledge, these are the first such results for these measures. In addition, for continuous distributions, we show consistency of plug-in algorithms for any performance measure that is a continuous and monotonically increasing function of TPR and TNR. Experimental results confirm our theoretical findings.	[Narasimhan, Harikrishna; Vaish, Rohit; Agarwal, Shivani] Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, Karnataka, India	Indian Institute of Science (IISC) - Bangalore	Narasimhan, H (corresponding author), Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, Karnataka, India.	harikrishna@csa.iisc.ernet.in; rohit.vaish@csa.iisc.ernet.in; shivani@csa.iisc.ernet.in			Google India PhD Fellowship; DST; Indo-US Science and Technology Forum	Google India PhD Fellowship(Google Incorporated); DST(Department of Science & Technology (India)); Indo-US Science and Technology Forum	HN thanks support from a Google India PhD Fellowship. SA gratefully acknowledges support from DST, Indo-US Science and Technology Forum, and an unrestricted gift from Yahoo.	Agarwal S., 2013, COLT; [Anonymous], 2013, ICML; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Chai K. M. A., 2005, SIGIR; Cheng Jie, 2002, ACM SIGKDD EXPLORATI, V3, P47; Chinta P.M., 2013, IJCNN; Daskalaki S, 2006, APPL ARTIF INTELL, V20, P381, DOI 10.1080/08839510500313653; Dembczynski K., ICML, V13; Dembczynski K. J., 2011, NIPS; Frank A., 2010, UCI MACHINE LEARNING; Gao S., 2003, SIGIR; Gu Q, 2009, COMM COM INF SC, V51, P461, DOI 10.1007/978-3-642-04962-0_53; Jansche M., 2005, HLT; Joachims T., 2005, ICML; Jorissen RN, 2005, J CHEM INF MODEL, V45, P549, DOI 10.1021/ci049641u; Kennedy K., 2009, ICAICS; KUBAT M, 1997, ICML; Lawrence S, 1998, LECT NOTES COMPUT SC, V1524, P299; Lewis D. D., 1995, SIGIR; Lipton Z. C., 2014, ECML PKDD; Liu Z., 2009, BIOMED RES INT, V2009; Musicant D. R., 2003, FLAIRS; Petterson J., 2010, NIPS; Powers R., 2005, KDD; Schutze H., 2008, INTRO INFORM RETRIEV, V39; Scott C, 2012, ELECTRON J STAT, V6, P958, DOI 10.1214/12-EJS699; Yang Y., 2001, SIGIR; Ye N., 2012, ICML; Zhang T, 2004, ANN STAT, V32, P56; Zhao MJ, 2013, J MACH LEARN RES, V14, P1033	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102065
C	Negrinho, R; Martins, AFT		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Negrinho, Renato; Martins, Andre F. T.			Orbit Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				REGRESSION SHRINKAGE; VARIABLE SELECTION; G-MAJORIZATION	We propose a general framework for regularization based on group-induced majorization. In this framework, a group is defined to act on the parameter space and an orbit is fixed; to control complexity, the model parameters are confined to the convex hull of this orbit (the orbitope). We recover several well-known regularizers as particular cases, and reveal a connection between the hyperoctahedral group and the recently proposed sorted l(1)-norm. We derive the properties a group must satisfy for being amenable to optimization with conditional and projected gradient algorithms. Finally, we suggest a continuation strategy for orbit exploration, presenting simulation results for the symmetric and hyperoctahedral groups.	[Negrinho, Renato; Martins, Andre F. T.] Inst Super Tecn, Inst Telecomunicacoes, P-1049001 Lisbon, Portugal; [Martins, Andre F. T.] Priberam Labs, P-1000123 Lisbon, Portugal	Instituto de Telecomunicacoes; Universidade de Coimbra; Universidade de Lisboa; Instituto Superior Tecnico	Negrinho, R (corresponding author), Inst Super Tecn, Inst Telecomunicacoes, P-1049001 Lisbon, Portugal.	renato.negrinho@gmail.com; atm@priberam.pt			FCT [PTDC/EEI-SII/2312/2012, PEst-OE/EEI/LA0008/2011]; EU/FEDER programme; QREN/POR Lisboa (Portugal) [2012/24803]	FCT(Portuguese Foundation for Science and TechnologyEuropean Commission); EU/FEDER programme; QREN/POR Lisboa (Portugal)	We thank all reviewers for their valuable comments. This work was partially supported by FCT grants PTDC/EEI-SII/2312/2012 and PEst-OE/EEI/LA0008/2011, and by the EU/FEDER programme, QREN/POR Lisboa (Portugal), under the Intelligo project (contract 2012/24803).	Bach F, 2012, OPTIMIZATION FOR MACHINE LEARNING, P19; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bertsekas D., 2003, CONVEX ANAL OPTIMIZA; Bogdan M., 2013, ARXIV13101969, P1; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Eaton Morris L., 1984, IMS LECT NOTES MONOG, V5, P13; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Figueiredo MAT, 2007, IEEE J-STSP, V1, P586, DOI 10.1109/JSTSP.2007.910281; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; GIOVAGNOLI A, 1985, LINEAR ALGEBRA APPL, V67, P111, DOI 10.1016/0024-3795(85)90190-9; Hale ET, 2008, SIAM J OPTIMIZ, V19, P1107, DOI 10.1137/070698920; Hardy G. H., 1952, INEQUALITIES; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Luss R., 2010, P NEUR INF PROC SYST, P1513; Marshall AW, 2011, SPRINGER SER STAT, P3, DOI 10.1007/978-0-387-68276-1; MIRSKY L, 1975, MONATSH MATH, V79, P303, DOI 10.1007/BF01647331; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Pardalos PM, 1999, ALGORITHMICA, V23, P211, DOI 10.1007/PL00009258; Rockafellar R. T., 1970, CONVEX ANAL; Sanyal R., 2009, ARXIV09115436; Serre J.-P., 1977, LINEAR REPRESENTATIO, V42; STEERNEMAN AGM, 1990, LINEAR ALGEBRA APPL, V127, P107, DOI 10.1016/0024-3795(90)90338-D; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tong Y., 1980, PROBABILITY INEQUALI, V5; Wright SJ, 2009, IEEE T SIGNAL PROCES, V57, P2479, DOI 10.1109/TSP.2009.2016892; Yu B., 2009, ADV NEURAL INFORM PR, P1348; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Zeng X., 2014, ARXIV14043184; Ziegler G, 1995, LECT POLYTOPES, V152	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101069
C	Neu, G; Valko, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Neu, Gergely; Valko, Michal			Online combinatorial optimization with stochastic decision sets and adversarial losses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Most work on sequential learning assumes a fixed set of actions that are available all the time. However, in practice, actions can consist of picking subsets of readings from sensors that may break from time to time, road segments that can be blocked or goods that are out of stock. In this paper we study learning algorithms that are able to deal with stochastic availability of such unreliable composite actions. We propose and analyze algorithms based on the Follow-The-Perturbed-Leader prediction method for several learning settings differing in the feedback provided to the learner. Our algorithms rely on a novel loss estimation technique that we call Counting Asleep Times. We deliver regret bounds for our algorithms for the previously studied full information and (semi-)bandit settings, as well as a natural middle point between the two that we call the restricted information setting. A special consequence of our results is a significant improvement of the best known performance guarantees achieved by an efficient algorithm for the sleeping bandit problem with stochastic availability. Finally, we evaluate our algorithms empirically and show their improvement over the known approaches.	[Neu, Gergely; Valko, Michal] INRIA Lille Nord Europe, SequeL Team, Lille, France		Neu, G (corresponding author), INRIA Lille Nord Europe, SequeL Team, Lille, France.	gergely.neu@inria.fr; michal.valko@inria.fr			French Ministry of Higher Education and Research; European Community's Seventh Framework Programme (FP7/2007-2013) [270327]; FUI project Hermes	French Ministry of Higher Education and Research; European Community's Seventh Framework Programme (FP7/2007-2013); FUI project Hermes	The research presented in this paper was supported by French Ministry of Higher Education and Research, by European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement no 270327 (CompLACS), and by FUI project Hermes.	Audibert J. Y., 2014, MATH OPERAT IN PRESS; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Bubeck S, 2012, PROC C LEARN THEORY, P41; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Dani V, 2008, ADV NEURAL INFORM PR, V20, P345; Felfernig A, 2011, RECOMMENDER SYSTEMS HANDBOOK, P187, DOI 10.1007/978-0-387-85820-3_6; Freund Y., 1997, P 20 9 ANN ACM S THE, P334, DOI [10.1145/258533.258616, DOI 10.1145/258533.258616]; Gyorgy A, 2007, J MACH LEARN RES, V8, P2369; Hutter M, 2004, LECT NOTES ARTIF INT, V3244, P279; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Kanade V., 2012, P 3 INN THEOR COMP S, P11; Kanade V., 2009, ARTIFICIAL INTELLIGE, P272; Kleinberg Robert, 2008, P 21 ANN C LEARN THE, P425; Koshevoy GA, 1999, MATH SOC SCI, V38, P35, DOI 10.1016/S0165-4896(98)00044-4; McMahan HB, 2004, LECT NOTES COMPUT SC, V3120, P109, DOI 10.1007/978-3-540-27819-1_8; Neu G, 2013, LECT NOTES ARTIF INT, V8139, P234	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101052
C	Nguyen, TV; Bonilla, EV		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Nguyen, Trung, V; Bonilla, Edwin, V			Automated Variational Inference for Gaussian Process Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We develop an automated variational method for approximate inference in Gaussian process (GP) models whose posteriors are often intractable. Using a mixture of Gaussians as the variational distribution, we show that (i) the variational objective and its gradients can be approximated efficiently via sampling from univariate Gaussian distributions and (ii) the gradients wrt the GP hyperparameters can be obtained analytically regardless of the model likelihood. We further propose two instances of the variational distribution whose covariance matrices can be parametrized linearly in the number of observations. These results allow gradient-based optimization to be done efficiently in a black-box manner. Our approach is thoroughly verified on five models using six benchmark datasets, performing as well as the exact or hard-coded implementations while running orders of magnitude faster than the alternative MCMC sampling approaches. Our method can be a valuable tool for practitioners and researchers to investigate new models with minimal effort in deriving model-specific inference algorithms.	[Nguyen, Trung, V] Australian Natl Univ, Canberra, ACT, Australia; [Nguyen, Trung, V] NICTA, Canberra, ACT, Australia; [Bonilla, Edwin, V] Univ New South Wales, Kensington, NSW, Australia	Australian National University; Australian National University; University of New South Wales Sydney	Nguyen, TV (corresponding author), Australian Natl Univ, Canberra, ACT, Australia.	VanTrung.Nguyen@nicta.com.au; e.bonilla@unsw.edu.au	Bonilla, Edwin V/T-1682-2018	Bonilla, Edwin V/0000-0002-9904-2408	Australian Government through the Department of Communications; Australian Research Council through the ICT Centre of Excellence Program	Australian Government through the Department of Communications(Australian Government); Australian Research Council through the ICT Centre of Excellence Program(Australian Research Council)	NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program.	[Anonymous], 2012, ARTIF INTELL; Casella George, 1996, BIOMETRIKA; Cole D, 2000, SCI TECHNOL WELD JOI, V5, P81, DOI 10.1179/136217100101538065; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Gelman A, 1992, STAT SCI, V7, P136, DOI 10.1214/ss/1177011136; Gershman S., 2012, ICML; Girolami M, 2006, NEURAL COMPUT, V18, P1790, DOI 10.1162/neco.2006.18.8.1790; Huber M. F., 2008, IEEE INT C MULT FUS; JARRETT RG, 1979, BIOMETRIKA, V66, P191, DOI 10.2307/2335266; Jordan M.I., 1998, INTRO VARIATIONAL ME; Khan Mohammad E., 2012, ADV NEURAL INFORM PR, V25, P3149; Lazaro-Gredilla  M., 2012, NIPS, P1628; Lazaro-Gredilla Miguel, 2011, ICML; Lichman M, 2013, UCI MACHINE LEARNING; Moller J, 1998, SCAND J STAT, V25, P451, DOI 10.1111/1467-9469.00115; Murray Iain, 2010, AISTATS; Neal R., 1993, TECHNICAL REPORT; Nguyen T.V., 2013, 16 C ART INT STAT AI, P472; Nickisch Hannes, 2008, J MACHINE LEARNING R, V9; Opper M, 2009, NEURAL COMPUT, V21, P786, DOI 10.1162/neco.2008.08-07-592; Ranganath R., 2014, AISTATS; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rue H, 2009, J R STAT SOC B, V71, P319, DOI 10.1111/j.1467-9868.2008.00700.x; Snelson E., 2003, NIPS; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807; Wilson A. G., 2012, ICML	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101045
C	Nickell, M; Jiang, XY; Tresp, V		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Nickell, Maximilian; Jiang, Xueyan; Tresp, Volker			Reducing the Rank of Relational Factorization Models by Including Observable Patterns	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Tensor factorization has become a popular method for learning from multirelational data. In this context, the rank of the factorization is an important parameter that determines runtime as well as generalization ability. To identify conditions under which factorization is an efficient approach for learning from relational data, we derive upper and lower bounds on the rank required to recover adjacency tensors. Based on our findings, we propose a novel additive tensor factorization model to learn from latent and observable patterns on multi-relational data and present a scalable algorithm for computing the factorization. We show experimentally both that the proposed additive model does improve the predictive performance over pure latent variable methods and that it also reduces the required rank - and therefore runtime and memory complexity - significantly.	[Nickell, Maximilian] MIT, Poggio Lab, LCSL, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Nickell, Maximilian] Ist Italiano Tecnol, Genoa, Italy; [Jiang, Xueyan; Tresp, Volker] Ludwig Maximilians Univ Munchen, Munich, Germany; [Jiang, Xueyan; Tresp, Volker] Siemens AG, Corp Technol, Munich, Germany	Massachusetts Institute of Technology (MIT); Istituto Italiano di Tecnologia - IIT; University of Munich; Siemens AG; Siemens Germany	Nickell, M (corresponding author), MIT, Poggio Lab, LCSL, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	mnick@mit.edu; xueyan.jiang.ext@siemens.com; volker.tresp@siemens.com			Center for Brains, Minds and Machines (CBMM) - NSF STC award [CCF-1231216]	Center for Brains, Minds and Machines (CBMM) - NSF STC award	Maximilian Nickel acknowledges support by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. We thank Youssef Mroueh and Lorenzo Rosasco for clarifying discussions on the theoretical part of this paper.	Airoldi EM, 2008, J MACH LEARN RES, V9, P1981; Bordes A., 2011, P 25 C ART INT; Brualdi R.A., 1991, COMBINATORIAL MATRIX; Carlson A, 2010, AAAI CONF ARTIF INTE, P1306; Dong X. L., 2014, P 20 ACM SIGKDD C KN; Getoor L., 2007, INTRO STAT RELATIONA, P129; Hoff P., 2008, ADV NEURAL INFORM PR, P657; Jenatton R., 2012, INT C ADV NEUR INF P, P3176; Jiang X., 2012, P INT WORKSH SEM TEC, V919, P1; Kemp C., 2006, AAAI, V3, P5; Kok S., 2007, P 24 INT C MACH LEAR, P433; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kolda TG, 2005, FIFTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P242, DOI 10.1109/ICDM.2005.77; Koren Y., 2008, P 14 ACM SIGKDD INT, P426, DOI DOI 10.1145/1401890.1401944; Lao N, 2010, MACH LEARN, V81, P53, DOI 10.1007/s10994-010-5205-8; Liben-Nowell D, 2007, J AM SOC INF SCI TEC, V58, P1019, DOI 10.1002/asi.20591; Losch Uta, 2012, The Semantic Web: Research and Applications. Proceedings 9th Extended Semantic Web Conference (ESWC 2012), P134, DOI 10.1007/978-3-642-30284-8_16; Monson S.D., 1995, B ICA, V14, P17; Nickel M., 2012, P 21 INT C WORLD WID, P271; Nickel M., 2011, INT C INT C MACH LEA, P809, DOI DOI 10.5555/3104482.3104584; Nickel M., 2013, TENSOR FACTORIZATION; QUINLAN JR, 1990, MACH LEARN, V5, P239, DOI 10.1007/BF00117105; Serre D, 2010, GRAD TEXTS MATH, V216, P1, DOI 10.1007/978-1-4419-7683-3; Singh A.P., 2008, PROC PROC 14 ACM SIG, P650; Suchanek F.M., 2007, P 16 INT C WORLD WID, DOI 10.1145/1242572.1242667; Sutskever I., 2009, P 22 INT C NEURAL IN, P1821; XU Z., 2006, P 22 C ANN C UNC ART, P544	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102009
C	Nie, S; Maua, DD; de Campos, CP; Ji, Q		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Nie, Siqi; Maua, Denis D.; de Campos, Cassio P.; Ji, Qiang			Advances in Learning Bayesian Networks of Bounded Treewidth	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					This work presents novel algorithms for learning Bayesian networks of bounded treewidth. Both exact and approximate methods are developed. The exact method combines mixed integer linear programming formulations for structure learning and treewidth computation. The approximate method consists in sampling k-trees (maximal graphs of treewidth k), and subsequently selecting, exactly or approximately, the best structure whose moral graph is a subgraph of that k-tree. The approaches are empirically compared to each other and to state-of-the-art methods on a collection of public data sets with up to 100 variables.	[Nie, Siqi; Ji, Qiang] Rensselaer Polytech Inst, Troy, NY 12180 USA; [Maua, Denis D.] Univ Sao Paulo, Sao Paulo, Brazil; [de Campos, Cassio P.] Queens Univ Belfast, Belfast, Antrim, North Ireland	Rensselaer Polytechnic Institute; Universidade de Sao Paulo; Queens University Belfast	Nie, S (corresponding author), Rensselaer Polytech Inst, Troy, NY 12180 USA.	nies@rpi.edu; denis.maua@usp.br; c.decampos@qub.ac.uk; qji@ecse.rpi.edu	Mauá, Denis Deratani/N-1842-2019	Mauá, Denis Deratani/0000-0003-2297-6349	Swiss NSF [200021_146606/1]; Sao Paulo Research Foundation (FAPESP) [2013/23197-4]; US Office of Navy Research [N00014-12-1-0868]	Swiss NSF(Swiss National Science Foundation (SNSF)); Sao Paulo Research Foundation (FAPESP)(Fundacao de Amparo a Pesquisa do Estado de Sao Paulo (FAPESP)); US Office of Navy Research	We thank the authors of [19, 21] for making their software publicly available and the anonymous reviewers for their useful suggestions. Most of this work has been performed while C. P. de Campos was with the Dalle Molle Institute for Artificial Intelligence. This work has been partially supported by the Swiss NSF grant 200021_146606/1, by the Sao Paulo Research Foundation (FAPESP) grant 2013/23197-4, and by the grant N00014-12-1-0868 from the US Office of Navy Research.	ARNBORG S, 1987, SIAM J ALGEBRA DISCR, V8, P277, DOI 10.1137/0608024; Bach FR, 2002, ADV NEUR IN, V14, P569; Bartlett M., 2013, UNCERTAINTY ARTIFICI, P182; Berg J, 2014, JMLR WORKSH CONF PRO, V33, P86; Beygelzimer A., 1998, P 8 INT C PRINC KNOW, P558; Caminiti S, 2010, THEOR COMPUT SYST, V46, P284, DOI 10.1007/s00224-008-9131-0; Chandrasekaran V., 2008, P 24 C UNC ART INT, P70; Chechetka A., 2007, ADV NEURAL INFORM PR, P273; Cussens J, 2011, UNCERTAINTY ARTIFICI, P153; Cussens J, 2013, GENET EPIDEMIOL, V37, P69, DOI 10.1002/gepi.21686; Dasgupta S, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P134; de Campos C.P., 2009, P 26 ANN INT C MACHI, P113; de Campos CP, 2011, J MACH LEARN RES, V12, P663; Elidan G, 2008, J MACH LEARN RES, V9, P2699; Friedman N., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P129; Grigoriev A., 2011, TECHNICAL REPORT; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; Korhonen J.H., 2013, JMLR W CP, V31, P370; Kwisthout JHP, 2010, FRONT ARTIF INTEL AP, V215, P237, DOI 10.3233/978-1-60750-606-5-237; Parviainen P, 2014, JMLR WORKSH CONF PRO, V33, P751; Perrier E, 2008, J MACH LEARN RES, V9, P2251; Roth D, 1996, ARTIF INTELL, V82, P273, DOI 10.1016/0004-3702(94)00092-1; Silander T., 2006, P 22 C UNC ART INT, P445; Srebro N, 2003, ARTIF INTELL, V143, P123, DOI 10.1016/S0004-3702(02)00360-0; Teyssier M., 2005, P 21 C UNC ART INT, P584	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102091
C	Oh, S; Shah, D		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Oh, Sewoong; Shah, Devavrat			Learning Mixed Multinomial Logit Model from Ordinal Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				MATRIX COMPLETION	Motivated by generating personalized recommendations using ordinal (or preference) data, we study the question of learning a mixture of MultiNomial Logit (MNL) model, a parameterized class of distributions over permutations, from partial ordinal or preference data (e.g. pair-wise comparisons). Despite its long standing importance across disciplines including social choice, operations research and revenue management, little is known about this question. In case of single MNL models (no mixture), computationally and statistically tractable learning from pair-wise comparisons is feasible. However, even learning mixture with two MNL components is infeasible in general. Given this state of affairs, we seek conditions under which it is feasible to learn the mixture model in both computationally and statistically efficient manner. We present a sufficient condition as well as an efficient algorithm for learning mixed MNL models from partial preferences/comparisons data. In particular, a mixture of r MNL components over n objects can be learnt using samples whose size scales polynomially in n and r (concretely, r(3.5)n(3)(log n)(4), with r << n(2/7) when the model parameters are sufficiently incoherent). The algorithm has two phases: first, learn the pair- wise marginals for each component using tensor decomposition; second, learn the model parameters for each component using RANKCENTRALITY introduced by Negahban et al. In the process of proving these results, we obtain a generalization of existing analysis for tensor decomposition to a more realistic regime where only partial information about each sample is available.	[Oh, Sewoong] Univ Illinois, Dept Ind & Enterprise Syst Engn, Urbana, IL 61801 USA; [Shah, Devavrat] MIT, Dept Elect Engn, Cambridge, MA 02139 USA	University of Illinois System; University of Illinois Urbana-Champaign; Massachusetts Institute of Technology (MIT)	Oh, S (corresponding author), Univ Illinois, Dept Ind & Enterprise Syst Engn, Urbana, IL 61801 USA.	swoh@illinois.edu; devavrat@mit.edu						Ammar A., 2014, P ACM SIGMETRICS INT; Anandkumar A., 2012, CORR, Vabs/ 1210. 7559; Azari H, 2012, NIPS 12, P126; Blanchet J., 2013, EC, P103; Bollobas Bla, 2001, RANDOM GRAPHS, P215; BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001; Duchi J. C., 2010, P ICML C HAIF ISR JU; Farias V. F., 2009, NIPS; Ford L.R., 1957, AM MATH MON, V64, P28, DOI DOI 10.2307/2308513; Jain P., 2014, ARXIV13112972; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Luce D., 1959, INDIVIDUAL CHOICE BE; Mitliagkas Ioannis., 2011, COMM CONTR COMP 2011, P1143; Negahban S., 2012, J MACHINE LEARNING R; Negahban Sahand, 2012, NIPS, P2483; Pardo B., 2002, Computer Music Journal, V26, P27, DOI 10.1162/014892602760137167; Samuelson PA, 1938, ECONOMICA-NEW SER, V5, P61, DOI 10.2307/2548836; Soufiani HA, 2014, PR MACH LEARN RES, V32; Soufiani Hossein Azari, 2013, P 26 INT C NEUR INF, P2706; Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288; Tropp J. A., 2011, FDN COMPUTATIONAL MA; Zermelo E, 1929, MATH Z, V29, P436, DOI 10.1007/BF01180541	25	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100004
C	Oiwa, H; Fujimaki, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Oiwa, Hidekazu; Fujimaki, Ryohei			Partition-wise Linear Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ALGORITHM	Region-specific linear models are widely used in practical applications because of their non-linear but highly interpretable model representations. One of the key challenges in their use is non-convexity in simultaneous optimization of regions and region-specific models. This paper proposes novel convex region-specific linear models, which we refer to as partition-wise linear models. Our key ideas are 1) assigning linear models not to regions but to partitions (region-specifiers) and representing region-specific linear models by linear combinations of partition-specific models, and 2) optimizing regions via partition selection from a large number of given partition candidates by means of convex structured regularizations. In addition to providing initialization-free globally-optimal solutions, our convex formulation makes it possible to derive a generalization bound and to use such advanced optimization techniques as proximal methods and decomposition of the proximal maps for sparsity-inducing regularizations. Experimental results demonstrate that our partition-wise linear models perform better than or are at least competitive with state-of-the-art region-specific or locally linear models.	[Oiwa, Hidekazu] Univ Tokyo, Grad Sch Informat Sci & Technol, Tokyo, Japan; [Fujimaki, Ryohei] NEC Labs Amer, Princeton, NJ USA	University of Tokyo; NEC Corporation	Oiwa, H (corresponding author), Univ Tokyo, Grad Sch Informat Sci & Technol, Tokyo, Japan.	hidekazu.oiwa@gmail.com; rfujimaki@nec-labs.com						Bach F., 2010, ADV NEURAL INFORM PR, P118; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Duchi J, 2009, J MACH LEARN RES, V10, P2899; GALLO G, 1989, SIAM J COMPUT, V18, P30, DOI 10.1137/0218003; Gu Q., 2013, ARTIF INTELL, P307; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; Jose C., 2013, INT C MACH LEARN, P486, DOI DOI 10.1145/2623330.2623759; Ladicky L, 2011, P 28 INT C INT C MAC, P985; Maurer A, 2012, J MACH LEARN RES, V13, P671; Nagano Kiyohito, 2013, UAI; Nesterov Y., 2007, CORE DISCUSSION PAPE; Oiwa Hidekazu, 2014, CORR; Olshen R., 1984, CLASSIFICATION REGRE; Segata N, 2010, J MACH LEARN RES, V11, P1883; Tseng P, 2010, MATH PROGRAM, V125, P263, DOI 10.1007/s10107-010-0394-2; Wang J., 2012, ADV NEURAL INF PROCE, V25, P91; Xu Z.E., 2013, ICML 2013, P133; Yao-Liang Y., 2013, ADV NEURAL INFORM PR, V26, P91; Yu Kai, 2009, ADV NEURAL INFORM PR, P2223; Zhang Z., 2011, ADV NEURAL INFORM PR, V24, P1611	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103056
C	Osband, I; Van Roy, B		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Osband, Ian; Van Roy, Benjamin			Model-based Reinforcement Learning and the Eluder Dimension	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We consider the problem of learning to optimize an unknown Markov decision process (MDP). We show that, if the MDP can be parameterized within some known function class, we can obtain regret bounds that scale with the dimensionality, rather than cardinality, of the system. We characterize this dependence explicitly as (O) over tilde(root d(K)d(E)T) where T is time elapsed, d(K) is the Kolmogorov dimension and d(E) is the eluder dimension. These represent the first unified regret bounds for model-based reinforcement learning and provide state of the art guarantees in several important settings. More-over, we present a simple and computationally efficient algorithm posterior sampling for reinforcement learning (PSRL) that satisfies these bounds.	[Osband, Ian; Van Roy, Benjamin] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Osband, I (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	iosband@stanford.edu; bvr@stanford.edu			Stanford Graduate Fellowships courtesy of PACCAR inc [CMMI-0968707]; National Science Foundation	Stanford Graduate Fellowships courtesy of PACCAR inc; National Science Foundation(National Science Foundation (NSF))	Osband is supported by Stanford Graduate Fellowships courtesy of PACCAR inc. This work was supported in part by Award CMMI-0968707 from the National Science Foundation.	Abbasi-yadkori Yasin, 2011, ADV NEURAL INFORM PR, V24, P2312; Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663; Bertsekas D. P., 1995, DYNAMIC PROGRAMMING, V1; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Bubeck Sebastien, 2011, J MACHINE LEARNING R, V12; Burnetas AN, 1997, MATH OPER RES, V22, P222, DOI 10.1287/moor.22.1.222; Ibrahimi M., 2012, ADV NEURAL INFORM PR, P2645; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Kaelbling L. P., 1996, CS9605103 ARXIV; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Li LH, 2011, MACH LEARN, V82, P399, DOI 10.1007/s10994-010-5225-4; Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914; Ortner Ronald, 2012, ADV NEURAL INFORM PR, P1772; Osband Ian, 2014, ARXIV14033741; Osband Ian, 2013, ADV NEURAL INFORM PR; Russo D., 2013, ABS13012609 CORR; Russo Daniel, 2013, ADV NEURAL INFORM PR, P2256; Strehl A.L., 2006, ICML, P881, DOI [10.1145/1143844.1143955, DOI 10.1145/1143844.1143955]; Strens, 2000, P 17 INT C MACH LEAR, P943; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Van Roy Benjamin, 2014, ARXIV14020635	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100024
C	Osband, I; Van Roy, B		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Osband, Ian; Van Roy, Benjamin			Near-optimal Reinforcement Learning in Factored MDPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Any reinforcement learning algorithm that applies to all Markov decision processes (MDPs) will suffer Omega(root SAT) regret on some MDP, where T is the elapsed time and S and A are the cardinalities of the state and action spaces. This implies T = Omega(SA) time to guarantee a near-optimal policy. In many settings of practical interest, due to the curse of dimensionality, S and A can be so enormous that this learning time is unacceptable. We establish that, if the system is known to be a factored MDP, it is possible to achieve regret that scales polynomially in the number of parameters encoding the factored MDP, which may be exponentially smaller than S or A. We provide two algorithms that satisfy near-optimal regret bounds in this context: posterior sampling reinforcement learning (PSRL) and an upper confidence bound algorithm (UCRL-Factored).	[Osband, Ian; Van Roy, Benjamin] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Osband, I (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	iosband@stanford.edu; bvr@stanford.edu			Stanford Graduate Fellowships courtesy of PACCAR inc [CMMI-0968707]; National Science Foundation	Stanford Graduate Fellowships courtesy of PACCAR inc; National Science Foundation(National Science Foundation (NSF))	Osband is supported by Stanford Graduate Fellowships courtesy of PACCAR inc. This work was supported in part by Award CMMI-0968707 from the National Science Foundation.	BARTLETT PL, 2009, P 25 C UNC ART INT, P35; Boutilier C, 2000, ARTIF INTELL, V121, P49, DOI 10.1016/S0004-3702(00)00033-3; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Burnetas AN, 1997, MATH OPER RES, V22, P222, DOI 10.1287/moor.22.1.222; Delgado KV, 2011, ARTIF INTELL, V175, P1498, DOI 10.1016/j.artint.2011.01.001; Diuk C., 2009, P 26 ANN INT C MACH, P249; Diuk C., 2007, P 22 AAAI C ART INT, P645; Ghahramani Z, 1998, LECT NOTES ARTIF INT, V1387, P168, DOI 10.1007/BFb0053999; Guestrin C, 2003, J ARTIF INTELL RES, V19, P399, DOI 10.1613/jair.1000; GUESTRIN C, 2001, IJCAI 01, V1, P673; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; Kearns M, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P740; Koller D, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P324; Osband Ian, 2013, ADV NEURAL INFORM PR; Sanner Scott, 2012, ARXIV12071415; Strehl AL, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON APPROXIMATE DYNAMIC PROGRAMMING AND REINFORCEMENT LEARNING, P103, DOI 10.1109/ADPRL.2007.368176; Strens, 2000, P 17 INT C MACH LEAR, P943; Szita I., 2009, P 26 INT C MACH LEAR, P1001; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Weissman T., 2003, INEQUALITIES L1 DEVI	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102006
C	Osogami, T; Otsuka, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Osogami, Takayuki; Otsuka, Makoto			Restricted Boltzmann machines modeling human choice	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				DECISION FIELD-THEORY	We extend the multinomial logit model to represent some of the empirical phenomena that are frequently observed in the choices made by humans. These phenomena include the similarity effect, the attraction effect, and the compromise effect. We formally quantify the strength of these phenomena that can be represented by our choice model, which illuminates the flexibility of our choice model. We then show that our choice model can be represented as a restricted Boltzmann machine and that its parameters can be learned effectively from data. Our numerical experiments with real data of human choices suggest that we can train our choice model in such a way that it represents the typical phenomena of choice.	[Osogami, Takayuki; Otsuka, Makoto] IBM Res Tokyo, Tokyo, Japan	International Business Machines (IBM)	Osogami, T (corresponding author), IBM Res Tokyo, Tokyo, Japan.	osogami@jp.ibm.com; motsuka@ucla.edu			JST, CREST	JST, CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST))	A part of this research is supported by JST, CREST.	Ariely D, 2010, PAYOFF HIDDEN LOGIC; Bierlaire M., 2001, SWISS TRANSP RES C, P1, DOI 10.3929/ethz-a-004238511; Brochu E., 2008, ADV NEURAL INFORM PR, P409; BUSEMEYER JR, 1993, PSYCHOL REV, V100, P432, DOI 10.1037/0033-295X.100.3.432; Chapelle O., 2005, ADV NEURAL INFORM PR, V17, P257; Farias VF, 2013, MANAGE SCI, V59, P305, DOI 10.1287/mnsc.1120.1610; Freund Y., 1994, UNSUPERVISED LEARNIN; Hruschka H., 2012, OR SPECTRUM, P1; Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510; Luce R, 1959, INDIVIDUAL CHOICE BE; Osogami T, 2014, INT C PATT RECOG, P3618, DOI 10.1109/ICPR.2014.622; Otter T, 2008, MARKET LETT, V19, P255, DOI 10.1007/s11002-008-9039-0; Rieskamp J, 2006, J ECON LIT, V44, P631, DOI 10.1257/jel.44.3.631; Roe RM, 2001, PSYCHOL REV, V108, P370, DOI 10.1037//0033-295X.108.2.370; Train KE, 2009, DISCRETE CHOICE METHODS WITH SIMULATION, 2ND EDITION, P1, DOI 10.1017/CBO9780511805271; TVERSKY A, 1993, MANAGE SCI, V39, P1179, DOI 10.1287/mnsc.39.10.1179; Usher M, 2004, PSYCHOL REV, V111, P757, DOI 10.1037/0033-295X.111.3.757; Yu A. J., 2013, P 35 ANN M COGN SCI, P1300	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100059
C	Ozdemir, B; Davis, LS		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Ozdemir, Bahadir; Davis, Larry S.			A Probabilistic Framework for Multimodal Retrieval using Integrative Indian Buffet Process	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				IMAGE RETRIEVAL	We propose a multimodal retrieval procedure based on latent feature models. The procedure consists of a Bayesian nonparametric framework for learning underlying semantically meaningful abstract features in a multimodal dataset, a probabilistic retrieval model that allows cross-modal queries and an extension model for relevance feedback. Experiments on two multimodal datasets, PASCAL-Sentence and SUN-Attribute, demonstrate the effectiveness of the proposed retrieval procedure in comparison to the state-of-the-art algorithms for learning binary codes.	[Ozdemir, Bahadir] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA; [Davis, Larry S.] Univ Maryland, Inst Adv Comp Studies, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park	Ozdemir, B (corresponding author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.	ozdemir@cs.umd.edu; lsd@umiacs.umd.edu			NSF [12621215 EAGER]	NSF(National Science Foundation (NSF))	This work was supported by the NSF Grant 12621215 EAGER: Video Analytics in Large Heterogeneous Repositories.	Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928; Doshi-Velez F., 2009, P 26 ANN INT C MACH, P273; Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432; Griffiths T.L., 2006, ADV NEURAL INFORM PR, P475; Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466; Kumar S, 2011, P TWENTYSECOND INT J, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230; Lin D., 1998, P INT C MACH LEARN; Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998; Raginsky M., 2009, ADV NEURAL INFORM PR, P1509, DOI [10.5555/2984093.2984263, DOI 10.5555/2984093.2984263]; Rastegari M., 2013, ICML; Sharma A., 2012, CVPR, DOI DOI 10.1109/CVPR.2012.6247923; Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972; Srivastava Nitish, 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49; Torralba A, 2008, PROC CVPR IEEE, P2269; Weiss Y, 2009, ADV NEURAL INFORM PR, P1753; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170; Zhen Y., 2012, ADV NEURAL INFORM PR, P1376, DOI [10.5555/2999134.2999288, DOI 10.5555/2999134.2999288]; Zhen Y., 2012, PA CM SIGKDD INT C K, P940, DOI DOI 10.1145/2339530.2339678; Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102050
C	Pachauri, D; Kondor, R; Sargur, G; Singh, V		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Pachauri, Deepti; Kondor, Risi; Sargur, Gautam; Singh, Vikas			Permutation Diffusion Maps (PDM) with Application to the Image Association Problem in Computer	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Consistently matching keypoints across images, and the related problem of finding clusters of nearby images, are critical components of various tasks in Computer Vision, including Structure from Motion (SfM). Unfortunately, occlusion and large repetitive structures tend to mislead most currently used matching algorithms, leading to characteristic pathologies in the final output. In this paper we introduce a new method, Permutations Diffusion Maps (PDM), to solve the matching problem, as well as a related new affinity measure, derived using ideas from harmonic analysis on the symmetric group. We show that just by using it as a preprocessing step to existing SfM pipelines, PDM can greatly improve reconstruction quality on difficult datasets.	[Pachauri, Deepti; Sargur, Gautam; Singh, Vikas] Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA; [Singh, Vikas] Univ Wisconsin, Dept Biostat & Med Informat, Madison, WI USA; [Kondor, Risi] Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA; [Kondor, Risi] Univ Chicago, Dept Stat, Chicago, IL 60637 USA	University of Wisconsin System; University of Wisconsin Madison; University of Wisconsin System; University of Wisconsin Madison; University of Chicago; University of Chicago	Pachauri, D (corresponding author), Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.	pachauri@cs.wisc.edu; risi@uchicago.edu; gautam@cs.wisc.edu; vsingh@biostat.wisc.edu			University of Wisconsin Graduate School [NSF-1320344, NSF-1320755]	University of Wisconsin Graduate School	This work was supported in part by NSF-1320344, NSF-1320755, and funds from the University of Wisconsin Graduate School. We thank Charles Dyer and Li Zhang for useful discussions and suggestions.	Crandall D., 2011, CVPR; Enqvist O., 2011, ICCV WORKSH; Fan R. K. Chung, 1996, SPECTRAL GRAPH THEOR, V92; Govindu VM, 2006, LECT NOTES COMPUT SC, V3852, P457; Havlena M., 2009, CVPR; Huang J., 2009, JMLR; Huang Qi-Xing, 2013, COMPUTER GRAPHICS FO; Jiang N., 2012, CVPR; Kondor R., 2010, SODA; Kuhn H, 1955, NAVAL RES LOGISTICS, V2; Li R., 2010, GENOME RES, V20; LOWE DG, 2004, IJCV, V60; Martinec D., 2007, CVPR; Mikolajczyk K., 2004, IJCV, V60; Nguyen A., 2011, COMPUTER GRAPHICS FO, V30; Ozyesil O., 2013, CORR; Pachauri D., 2013, NIPS; Pop M., 2002, IEEE COMPUTER, V35; Roberts R., 2011, CVPR; Rockmore D., 2002, APPL COMP HARMONIC A; SCHAFFALITZKY F, 2002, ECCV; Singer A., 2011, COMMUNICATIONS PURE; Singer A, 2011, APPL COMPUTATIONAL H, V30; Singer A., 2011, SIAM J IMAGING SCI, V4; Sinha S. N., 2012, TRENDS TOPICS COMPUT; Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964; Wilson K., 2013, ICCV; WU C, 2011, CVPR; Wu C., 2013, 3DTV C INT C; Zach C., 2008, CVPR; Zach Christopher, 2010, CVPR; Zhu S., 2010, CVPR	33	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102082
C	Pan, XH; Jegelka, S; Gonzalez, J; Bradley, J; Jordan, MI		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Pan, Xinghao; Jegelka, Stefanie; Gonzalez, Joseph; Bradley, Joseph; Jordan, Michael, I			Parallel Double Greedy Submodular Maximization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Many machine learning problems can be reduced to the maximization of submodular functions. Although well understood in the serial setting, the parallel maximization of submodular functions remains an open area of research with recent results [1] only addressing monotone functions. The optimal algorithm for maximizing the more general class of non-monotone submodular functions was introduced by Buchbinder et al. [2] and follows a strongly serial double-greedy logic and program analysis. In this work, we propose two methods to parallelize the double-greedy algorithm. The first, coordination-free approach emphasizes speed at the cost of a weaker approximation guarantee. The second, concurrency control approach guarantees a tight 1/2-approximation, at the quantifiable cost of additional coordination and reduced parallelism. As a consequence we explore the tradeoff space between guaranteed performance and objective optimality. We implement and evaluate both algorithms on multi-core hardware and billion edge graphs, demonstrating both the scalability and tradeoffs of each approach.	[Pan, Xinghao; Jegelka, Stefanie; Gonzalez, Joseph; Bradley, Joseph; Jordan, Michael, I] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA; [Jordan, Michael, I] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA	University of California System; University of California Berkeley; University of California System; University of California Berkeley	Pan, XH (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	xinghao@eecs.berkeley.edu; stefje@eecs.berkeley.edu; jegonzal@eecs.berkeley.edu; josephkb@eecs.berkeley.edu; jordan@eecs.berkeley.edu	Jordan, Michael I/C-5253-2013		NSF CISE Expeditions Award [CCF-1139158]; LBNL Award [7076018]; DARPA XData Award [FA8750-12-2-0331]; Office of Naval Research [N00014-11-1-0688]; DSO National Laboratories Postgraduate Scholarship	NSF CISE Expeditions Award; LBNL Award; DARPA XData Award; Office of Naval Research(Office of Naval Research); DSO National Laboratories Postgraduate Scholarship(DSO National Laboratories)	This research is supported in part by NSF CISE Expeditions Award CCF-1139158, LBNL Award 7076018, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, SAP, The Thomas and Stacey Siebel Foundation, Adobe, Apple, Inc., Bosch, C3Energy, Cisco, Cloudera, EMC, Ericsson, Facebook, GameOnTalis, Guavus, HP, Huawei, Intel, Microsoft, NetApp, Pivotal, Splunk, Virdata, VMware, and Yahoo!. This research was in part funded by the Office of Naval Research under contract/grant number N00014-11-1-0688. X. Pan's work is also supported by a DSO National Laboratories Postgraduate Scholarship.	AHMED A., 2012, P 5 ACM INT C WEB SE; Badanidiyuru A., 2014, SODA; Bilmes J., 2013, NIPS TUTORIAL; Boldi P, 2004, SOFTWARE PRACT EXPER, V34, P711, DOI 10.1002/spe.587; Boldi P., 2004, WWW; Buchbinder N., 2012, FOCS; FRANK A, 1993, DISCRETE MATH, V111, P231, DOI 10.1016/0012-365X(93)90158-P; Gillenwater J., 2012, ADV NEURAL INFORM PR; Ho Q., 2013, NIPS; Kempe D., 2003, ACM SIGKDD C KNOWL D; Kim G, 2011, INT C COMP VIS ICCV; Krause A., 2013, ICML TUTORIAL; Krause A., 2010, JAIR, V39; Krause A, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1989734.1989736; Kumar R., 2013, SPAA; Kung H., 1981, TODS, V6; Leskovec J., 2011, STANFORD NETWORK ANA; Li M., 2013, BIG LEARN WORKSH NIP; Lin H., 2011, 49 ANN M ASS COMP LI; Mirzasoleiman Baharan, 2013, ADV NEURAL INFORM PR, V26; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Ozsu MT., 2007, PRINCIPLES DISTRIBUT, V3; Pan X., 2013, ADV NEURAL INFORM PR, V26; Recht B., 2011, ADV NEURAL INFORM PR, V24; Reed C., 2013, INT C MACH LEARN ICM; Santini M., 2011, WWW; SCHRIJVER A, 2002, COMBINATORIAL OPTIMI; Shapley L.S., 1971, INT J GAME THEORY, V1, P11, DOI [DOI 10.1007/BF01753431, 10.1007/BF01753431]; Wei K., 2014, INT C MACH LEARN ICM	29	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102052
C	Parambath, SAP; Usunier, N; Grandvalet, Y		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Parambath, Shameem A. Puthiya; Usunier, Nicolas; Grandvalet, Yves			Optimizing F-Measures by Cost-Sensitive Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We present a theoretical analysis of F-measures for binary, multiclass and multilabel classification. These performance measures are non-linear, but in many scenarios they are pseudo-linear functions of the per-class false negative/false positive rate. Based on this observation, we present a general reduction of F-measure maximization to cost-sensitive classification with unknown costs. We then propose an algorithm with provable guarantees to obtain an approximately optimal classifier for the F-measure by solving a series of cost-sensitive classification problems. The strength of our analysis is to be valid on any dataset and any class of classifiers, extending the existing theoretical results on F-measures, which are asymptotic in nature. We present numerical experiments to illustrate the relative importance of cost asymmetry and thresholding when learning linear classifiers on various F-measure optimization tasks.	[Parambath, Shameem A. Puthiya; Usunier, Nicolas; Grandvalet, Yves] Univ Technol Compiegne, CNRS, Heudiasyc UMR 7253, Compiegne, France	Centre National de la Recherche Scientifique (CNRS); Picardie Universites; Universite de Technologie de Compiegne	Parambath, SAP (corresponding author), Univ Technol Compiegne, CNRS, Heudiasyc UMR 7253, Compiegne, France.	sputhiya@utc.fr; nusunier@utc.fr; grandval@utc.fr			Labex MS2T; Picardy Region; French Government [ANR-11-IDEX-0004-02]	Labex MS2T; Picardy Region(Region Hauts-de-France); French Government	This work was carried out and funded in the framework of the Labex MS2T. It was supported by the Picardy Region and the French Government, through the program "Investments for the future" managed by the National Agency for Research (Reference ANR-11-IDEX-0004-02).	[Anonymous], 2010, ADV NEURAL INFORM PR; Bach FR, 2006, J MACH LEARN RES, V7, P1713; Cambini A, 2009, LECT NOTES ECON MATH, V616, P1; Dembczynski K., 2013, P 30 INT C MACH LEAR, P1130; Dembczynski KJ, 2011, ADV NEUR INF PROC SY; Elkan C., 2001, INT JOINT C ART INT, P973; Fan R.-E., 2007, TECHNICAL REPORT; Grandvalet Y., 2005, NIPS; Joachims Thorsten, 2005, ICML, DOI DOI 10.1145/1102351.1102399; Kim JD, 2013, P BIONLP SHAR TASK 2, P8; Lipton Zachary C, 2014, Mach Learn Knowl Discov Databases, V8725, P225, DOI 10.1007/978-3-662-44851-9_15; Musicant D., 2003, P INT FLAIRS C, P356; Nan Y., 2012, ICML; Petterson J., 2011, ADV NEURAL INFORM PR, P1512; Pillai I, 2013, PATTERN RECOGN, V46, P2055, DOI 10.1016/j.patcog.2013.01.012; Pillai I, 2012, INT C PATT RECOG, P2424; Scott C, 2012, ELECTRON J STAT, V6, P958, DOI 10.1214/12-EJS699; Tsoumakas G., 2007, INT J DATA WAREHOUS, V3, P1; van Rijsbergen C., 1979, INFORM RETRIEVAL, V2nd; Weiwei Cheng, 2012, Rough Sets and Current Trends in Computing. Proceedings 8th International Conference, RSCTC 2012, P439, DOI 10.1007/978-3-642-32115-3_52; Zhou ZH, 2010, COMPUT INTELL-US, V26, P232, DOI 10.1111/j.1467-8640.2010.00358.x	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102069
C	Pareek, H; Ravikumar, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Pareek, Harsh; Ravikumar, Pradeep			A Representation Theory for Ranking Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					This paper presents a representation theory for permutation-valued functions, which in their general form can also be called listwise ranking functions. Pointwise ranking functions assign a score to each object independently, without taking into account the other objects under consideration; whereas listwise loss functions evaluate the set of scores assigned to all objects as a whole. In many supervised learning to rank tasks, it might be of interest to use listwise ranking functions instead; in particular, the Bayes Optimal ranking functions might themselves be listwise, especially if the loss function is listwise. A key caveat to using listwise ranking functions has been the lack of an appropriate representation theory for such functions. We show that a natural symmetricity assumption that we call exchangeability allows us to explicitly characterize the set of such exchangeable listwise ranking functions. Our analysis draws from the theories of tensor analysis, functional analysis and De Finetti theorems. We also present experiments using a novel reranking method motivated by our representation theory.	[Pareek, Harsh; Ravikumar, Pradeep] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Pareek, H (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	harshp@cs.utexas.edu; pradeepr@cs.utexas.edu			ARO [W911NF-12-1-0390]; NSF [IIS-1149803, IIS-1320894, IIS-1447574, DMS-1264033]	ARO; NSF(National Science Foundation (NSF))	We acknowledge the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1320894, IIS-1447574, and DMS-1264033.	Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463; Bernardo J.M., 2009, BAYESIAN THEORY, V405; Burges C., 2010, LEARNING, V11; Cao Z., 2007, P 24 INT C MACH LEAR, P129, DOI DOI 10.1145/1273496.1273513; Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025; Chapelle O., 2011, P MACHINE LEARNING R, V14, P91; Chapelle O., 2009, C INF KNOWL MAN CIKM; Chapelle O., 2011, P LEARN RANK CHALL, P1; Comon P, 2008, SIAM J MATRIX ANAL A, V30, P1254, DOI 10.1137/060661569; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696; Dembczynski Krzysztof, 2012, ARXIV12066401; DIACONIS P, 1980, ANN PROBAB, V8, P745, DOI 10.1214/aop/1176994663; DIACONIS P, 1977, SYNTHESE, V36, P271, DOI 10.1007/BF00486116; Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010; HEATH D, 1976, AM STAT, V30, P188, DOI 10.2307/2683760; Hewitt E., 1955, T AM MATH SOC, V80, P470, DOI DOI 10.1090/S0002-9947-1955-0076206-8; Jarvelin K., 2000, SIGIR Forum, V34, P41; Jaynes E. T., 1986, BAYESIAN INFERENCE D, V31, P42; KINGMAN JFC, 1978, ANN PROBAB, V6, P183, DOI 10.1214/aop/1176995566; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; QI L., 2012, ARXIV12013424; Qin T., 2008, P 22 ANN C NEUR INF; Qin T, 2010, INFORM RETRIEVAL, V13, P346, DOI 10.1007/s10791-009-9123-y; Ravikumar P, 2011, NDCG CONSISTENCY LIS; Reed M., 1980, METHODS MODERN MATH, V1; Weston J., 2012, ARXIV12104914	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100029
C	Pasa, L; Sperduti, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Pasa, Luca; Sperduti, Alessandro			Pre-training of Recurrent Neural Networks via Linear Autoencoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				PRINCIPAL COMPONENT ANALYSIS; DEEP ARCHITECTURES	We propose a pre-training technique for recurrent neural networks based on linear autoencoder networks for sequences, i.e. linear dynamical systems modelling the target sequences. We start by giving a closed form solution for the definition of the optimal weights of a linear autoencoder given a training set of sequences. This solution, however, is computationally very demanding, so we suggest a procedure to get an approximate solution for a given number of hidden units. The weights obtained for the linear autoencoder are then used as initial weights for the input-to-hidden connections of a recurrent neural network, which is then trained on the desired task. Using four well known datasets of sequences of polyphonic music, we show that the proposed pre-training approach is highly effective, since it allows to largely improve the state of the art results on all the considered datasets.	[Pasa, Luca; Sperduti, Alessandro] Univ Padua, Dept Math, Padua, Italy	University of Padua	Pasa, L (corresponding author), Univ Padua, Dept Math, Padua, Italy.	pasa@math.unipd.it; sperduti@math.unipd.it						BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; Bay M., 2009, ISMIR, P315; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Boulanger-Lewandowski N., 2012, ICML; BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918; Di Lena P, 2012, BIOINFORMATICS, V28, P2449, DOI 10.1093/bioinformatics/bts475; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Hochreiter S., 1996, NIPS, P473; Jolliffe IT, 2002, ENCY STATIST BEHAV S, DOI [10.1007/0-387-22440-8_13, 10.1007/b98835]; Kremer SC., 2001, FIELD GUIDE DYNAMICA; Martinsson Gunnar, 2010, WORKS ALG MOD MASS D; Micheli A, 2007, LECT NOTES COMPUT SC, V4669, P826; Rabani E., 2001, PPSC; Saxe A., 2014, INT C LEARNING REPRE; Sperduti A., 2013, NESY 13 9 INT WORKSH; SPERDUTI A, 2007, ECML, V4701, P335; Sperduti A, 2006, LECT NOTES COMPUT SC, V4131, P349; Sutskever I., 2011, P 28 INT C MACH LEAR, P1033, DOI DOI 10.1145/346152.346166; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Voegtlin T, 2005, NEURAL NETWORKS, V18, P1051, DOI 10.1016/j.neunet.2005.07.005; Zhang ZY, 2001, SIAM J MATRIX ANAL A, V22, P1245, DOI 10.1137/S0895479899357875	23	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100050
C	Piot, B; Geist, M; Pietquin, O		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Piot, Bilal; Geist, Matthieu; Pietquin, Olivier			Difference of Convex Functions Programming for Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Large Markov Decision Processes are usually solved using Approximate Dynamic Programming methods such as Approximate Value Iteration or Approximate Policy Iteration. The main contribution of this paper is to show that, alternatively, the optimal state-action value function can be estimated using Difference of Convex functions (DC) Programming. To do so, we study the minimization of a norm of the Optimal Bellman Residual (OBR) T*Q - Q, where T* is the so-called optimal Bellman operator. Controlling this residual allows controlling the distance to the optimal action-value function, and we show that minimizing an empirical norm of the OBR is consistant in the Vapnik sense. Finally, we frame this optimization problem as a DC program. That allows envisioning using the large related literature on DC Programming to address the Reinforcement Leaning problem.	[Piot, Bilal; Geist, Matthieu] GeorgiaTech CNRS, UMI 2958, MaLIS Res Grp SUPELEC, Metz, France; [Piot, Bilal; Pietquin, Olivier] UMR 8022 CNRS Lille 1, LIFL, SequeL Team, Lille, France; [Pietquin, Olivier] Univ Lille 1, IUF, Villeneuve Dascq, France	Centre National de la Recherche Scientifique (CNRS); CNRS - Institute for Engineering & Systems Sciences (INSIS); Universite de Lille - ISITE; Universite de Lille; Universite de Lille - ISITE; Universite de Lille	Piot, B (corresponding author), GeorgiaTech CNRS, UMI 2958, MaLIS Res Grp SUPELEC, Metz, France.	bilal.piot@lifl.fr; matthieu.geist@supelec.fr; olivier.pietquin@univ-lille1.fr			European Union Seventh Framework Program (FP7/2007-2013) [270780]; ANR ContInt program (MaRDi project) [ANR- 12-CORD-021 01]	European Union Seventh Framework Program (FP7/2007-2013); ANR ContInt program (MaRDi project)(French National Research Agency (ANR))	The research leading to these results has received partial funding from the European Union Seventh Framework Program (FP7/2007-2013) under grant agreement number 270780 and the ANR ContInt program (MaRDi project, number ANR- 12-CORD-021 01). We also would like to thank professors Le Thi Hoai An and Pham Dinh Tao for helpful discussions about DC programming.	An LTH, 2005, ANN OPER RES, V133, P23, DOI 10.1007/s10479-004-5022-1; Antos A., 2008, MACHINE LEARNING; Antos A., 2007, P NIPS; Archibald T., 1995, J OPERATIONAL RES SO; Azar M. G., 2012, J MACHINE LEARNING R, V13; Baird L., 1995, P ICML; Bertsekas D. P., 1995, DYNAMIC PROGRAMMING, V1; de Farias Daniela P., 2003, OPERATIONS RES, V51; Desai V.V., 2009, ADV NEURAL INFORM PR, P459; Farahmand A., 2010, P NIPS; Grubb A., 2011, P ICML; Hiriart-Urruty J. B, 1985, CONVEXITY DUALITY OP; Klein E., 2012, P NIPS; Lever G., 2012, P ICML; Maillard O.-A., 2010, P ACML; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Scherrer Bruno, 2014, P ICML; SHOR NZ, 1985, MINIMIZATION METHODS; Tao P. D., 1997, ACTA MATH VIETNAM, V22, P289; Taylor G., 2012, P UAI; Vapnik V.N, 1998, STAT LEARNING THEORY	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102004
C	Qin, DF; Chen, XL; Guillaumin, M; Van Gool, L		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Qin, Danfeng; Chen, Xuanli; Guillaumin, Matthieu; Van Gool, Luc			Quantized Kernel Learning for Feature Matching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Matching local visual features is a crucial problem in computer vision and its accuracy greatly depends on the choice of similarity measure. As it is generally very difficult to design by hand a similarity or a kernel perfectly adapted to the data of interest, learning it automatically with as few assumptions as possible is preferable. However, available techniques for kernel learning suffer from several limitations, such as restrictive parametrization or scalability. In this paper, we introduce a simple and flexible family of non-linear kernels which we refer to as Quantized Kernels (QK). QKs are arbitrary kernels in the index space of a data quantizer, i.e., piecewise constant similarities in the original feature space. Quantization allows to compress features and keep the learning tractable. As a result, we obtain state-of-the-art matching performance on a standard benchmark dataset with just a few bits to represent each feature dimension. QKs also have explicit non-linear, low-dimensional feature mappings that grant access to Euclidean geometry for uncompressed features.	[Qin, Danfeng; Guillaumin, Matthieu; Van Gool, Luc] Swiss Fed Inst Technol, Zurich, Switzerland; [Chen, Xuanli] Tech Univ Munich, Munich, Germany	Swiss Federal Institutes of Technology Domain; ETH Zurich; Technical University of Munich	Qin, DF (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	qind@vision.ee.ethz.ch; xuanli.chen@tum.de; guillaumin@vision.ee.ethz.ch; vangool@vision.ee.ethz.ch						Achlioptas D., 2001, ADV NEURAL INFORM PR, P335; Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293; [Anonymous], 2012, IEEE C COMP VIS PATT; Bach F.R., 2004, P INT C MACH LEARN; Boix Xavier, 2013, IEEE C COMP VIS PATT; Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3; Brown M, 2011, IEEE T PATTERN ANAL, V33, P43, DOI 10.1109/TPAMI.2010.54; Dean T. L, 2013, CVPR; Fazel M., 2002, MATRIX RANK MINIMIZA; Gong Y., 2012, ADV NEURAL INFORM PR, P1196; Hoi Steven CH, 2007, P INT C MACH LEARN; Hua Gang, 2007, ICCV 2007; Jegou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Lowe D. G., 2004, IJCV; Maji S, 2013, IEEE T PATTERN ANAL, V35, P66, DOI 10.1109/TPAMI.2012.62; Orabona Francesco, 2011, P 28 INT C MACH LEAR, P249; Roig Gemma, 2013, ARXIV13075161; Simonyan K., 2014, IEEE T PATTERN ANAL; Trzcinski T., 2012, NIPS; Trzcinski Tomasz, 2013, IEEE C COMP VIS PATT; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Weinberger Kilian Q, 2006, ADV NEURAL INFORM PR, P1473, DOI DOI 10.1007/978-3-319-13168-9_; Winder S., 2007, CVPR; Xiao L, 2010, J MACH LEARN RES, V11, P2543; Yi Z, 2008, ELECTRON LETT, V44, P107, DOI 10.1049/el:20082477; [No title captured]	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101083
C	Que, QC; Belkin, M; Wang, YS		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Que, Qichao; Belkin, Mikhail; Wang, Yusu			Learning with Fredholm Kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In this paper we propose a framework for supervised and semi-supervised learning based on reformulating the learning problem as a regularized Fredholm integral equation. Our approach fits naturally into the kernel framework and can be interpreted as constructing new data-dependent kernels, which we call Fredholm kernels. We proceed to discuss the "noise assumption" for semi-supervised learning and provide both theoretical and experimental evidence that Fredholm kernels can effectively utilize unlabeled data under the noise assumption. We demonstrate that methods based on Fredholm learning show very competitive performance in the standard semi-supervised learning setting.	[Que, Qichao; Belkin, Mikhail; Wang, Yusu] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA	University System of Ohio; Ohio State University	Que, QC (corresponding author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.	que@cse.ohio-state.edu; mbelkin@cse.ohio-state.edu; yusu@cse.ohio-state.edu			NSF [CCF-1319406, RI 1117707]	NSF(National Science Foundation (NSF))	The work was partially supported by NSF Grants CCF-1319406 and RI 1117707. We thank the anonymous NIPS reviewers for insightful comments.	Belkin M, 2006, J MACH LEARN RES, V7, P2399; Chapelle O, 2005, P INT WORKSH ART INT, V2005, P57; Chapelle O., 2006, IEEE T NEURAL NETW, V20, P542; Chapelle O, 2003, ADV NEURAL INFORM PR, P585; Gretton A, 2009, NEURAL INF PROCESS S, P131; Grunewalder S., 2012, P 29 INT C MACH LEAR, P1823; Hazewinkel Michiel, 1989, ENCY MATH, V4; Muandet Krikamol, 2014, ARXIV14055505; Que Q., 2013, ADV NEURAL INFORM PR, P1484; Scholkopf B., 2001, LEARNING KERNELS SUP; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Sindhwani V., 2005, P 22 INT C MACH LEAR, P824, DOI DOI 10.1145/1102351.1102455; Vishwanathan SVN, 2007, INT J COMPUT VISION, V73, P95, DOI 10.1007/s11263-006-9352-0; Zhu X., 2005, TECHNICAL REPORT; [No title captured]	15	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100016
C	Quoc, TD; Cevher, V		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Quoc Tran-Dinh; Cevher, Volkan			Constrained convex minimization via model-based excessive gap	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We introduce a model-based excessive gap technique to analyze first-order primal-dual methods for constrained convex minimization. As a result, we construct first-order primal-dual methods with optimal convergence rates on the primal objective residual and the primal feasibility gap of their iterates separately. Through a dual smoothing and prox-center selection strategy, our framework subsumes the augmented Lagrangian, alternating direction, and dual fast-gradient methods as special cases, where our rates apply.	[Quoc Tran-Dinh; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, CH-1015 Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Quoc, TD (corresponding author), Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, CH-1015 Lausanne, Switzerland.	quoc.trandinh@epfl.ch; volkan.cevher@epfl.ch	Tran-Dinh, Quoc/AAX-8950-2020	Tran-Dinh, Quoc/0000-0002-1077-2579	European Commission [MIRG-268398]; ERC Future Proof; Swiss Science Foundation [SNF 200021-132548, SNF 200021-146750, SNF CRSII2-147633]	European Commission(European CommissionEuropean Commission Joint Research Centre); ERC Future Proof; Swiss Science Foundation(Swiss National Science Foundation (SNSF))	This work is supported in part by the European Commission under the grants MIRG-268398 and ERC Future Proof, and by the Swiss Science Foundation under the grants SNF 200021-132548, SNF 200021-146750 and SNF CRSII2-147633.	Auslender A., 1976, OPTIMISATION METHODE; Beck A, 2014, OPER RES LETT, V42, P1, DOI 10.1016/j.orl.2013.10.007; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bertsekas D. P., 1997, PARALLEL DISTRIBUTED; Bertsekas DP., 1996, CONSTRAINED OPTIMIZA, V1st edn; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chandrasekaran V., 2012, CONVEX GEOMETRY LINE; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; CHEN G, 1994, MATH PROGRAM, V64, P81, DOI 10.1007/BF01582566; Combettes PL, 2005, MULTISCALE MODEL SIM, V4, P1168, DOI 10.1137/050626090; Deng W., 2012, TECH REP; Facchinei F., 2002, FINITE DIMENSIONAL V; Goldstein T., 2013, ADAPTIVE PRIMAL DUAL, P1; Goldstein T, 2014, SIAM J IMAGING SCI, V7, P1588, DOI 10.1137/120896219; He B., 2012, NONERGODIC CON UNPUB; He BS, 2012, SIAM J NUMER ANAL, V50, P700, DOI 10.1137/110836936; Lan G., 2013, ITERATION COMPLEXITY; McCoy MB, 2014, IEEE SIGNAL PROC MAG, V31, P87, DOI 10.1109/MSP.2013.2296605; Nedelcu V, 2014, SIAM J CONTROL OPTIM, V52, P3109, DOI 10.1137/120897547; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y., 2018, APPL OPTIMIZATION; Ouyang Hua, 2013, P 30 INT C MACH LEAR, P80; Ouyang Y., 2014, ACCELERATED LINEARIZ; Parikh N., 2014, FDN TRENDS OPTIM, V1, P127, DOI DOI 10.1561/2400000003; Rockafellar R. T., 1976, Mathematics of Operations Research, V1, P97, DOI 10.1287/moor.1.2.97; Shefi R, 2014, SIAM J OPTIMIZ, V24, P269, DOI 10.1137/130910774; Toh K. - C., 2010, IMPLEMENTATION USAGE; Tran-Dinh Q., 2014, TECHNICAL REPORT, P1; Wainwright MJ, 2014, ANNU REV STAT APPL, V1, P233, DOI 10.1146/annurev-statistics-022513-115643; Wang H, 2013, TRANSPORT RES REC, P1, DOI 10.3141/2352-01	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102055
C	Raiko, T; Yao, L; Cho, K; Bengio, Y		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Raiko, Tapani; Yao, Li; Cho, KyungHyun; Bengio, Yoshua			Iterative Neural Autoregressive Distribution Estimator (NADE-k)	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Training of the neural autoregressive density estimator (NADE) can be viewed as doing one step of probabilistic inference on missing values in data. We propose a new model that extends this inference scheme to multiple steps, arguing that it is easier to learn to improve a reconstruction in k steps rather than to learn to reconstruct in a single inference step. The proposed model is an unsupervised building block for deep learning that combines the desirable properties of NADE and multi-prediction training: (1) Its test likelihood can be computed analytically, (2) it is easy to generate independent samples from it, and (3) it uses an inference engine that is a superset of variational inference for Boltzmann machines. The proposed NADE-k is competitive with the state-of-the-art in density estimation on the two datasets tested.	[Raiko, Tapani] Aalto Univ, Espoo, Finland; [Yao, Li; Cho, KyungHyun; Bengio, Yoshua] Univ Montreal, Montreal, PQ, Canada	Aalto University; Universite de Montreal	Raiko, T (corresponding author), Aalto Univ, Espoo, Finland.				NSERC; Calcul Quebec; Compute Canada; Canada Research Chair; CIFAR	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Calcul Quebec; Compute Canada; Canada Research Chair(Natural Resources CanadaCanadian Forest ServiceCanada Research Chairs); CIFAR(Canadian Institute for Advanced Research (CIFAR))	The authors would like to acknowledge the support of NSERC, Calcul Quebec, Compute Canada, the Canada Research Chair and CIFAR, and developers of Theano (Bergstra et al., 2010; Bastien et al., 2012).	[Anonymous], 2010, PYTH SCI COMP C; Bastien F., 2012, DEEP LEARN UNS FEAT; Bengio Y, 2000, ADV NEUR IN, V12, P400; Bengio Y., 2013, P 30 INT C MACH LEAR; Cho K, 2013, NEURAL COMPUT, V25, P805, DOI 10.1162/NECO_a_00397; Domke J., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2937, DOI 10.1109/CVPR.2011.5995320; Gregor K., 2014, INT C MACH LEARN ICM; Heckerman D, 2001, J MACH LEARN RES, V1, P49, DOI 10.1162/153244301753344614; Hinton G. E., 2000, 2000004 GCNU TR U CO; Huang F, 2002, ANN I STAT MATH, V54, P1, DOI 10.1023/A:1016170102988; Larochelle H., 2011, INT C ART INT STAT; Marlin BM., 2010, P 13 INT C ART INT S, V9, P509; Pascanu R., 2014, 2 INT C LEARN REPR I; Peterson C., 1987, Complex Systems, V1, P995; Stoyanov Veselin, 2011, P AISTATS; Uria B., 2014, P 30 INT C MACH LEAR; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Yao L., 2014, EUR C MACH LEARN ECM; Zeiler M.D, 2012, CORR ABS12125701	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100056
C	Rajkumar, A; Agarwal, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Rajkumar, Arun; Agarwal, Shivani			Online Decision-Making in General Combinatorial Spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ALGORITHMS	We study online combinatorial decision problems, where one must make sequential decisions in some combinatorial space without knowing in advance the cost of decisions on each trial; the goal is to minimize the total regret over some sequence of trials relative to the best fixed decision in hindsight. Such problems have been studied mostly in settings where decisions are represented by Boolean vectors and costs are linear in this representation. Here we study a general setting where costs may be linear in any suitable low-dimensional vector representation of elements of the decision space. We give a general algorithm for such problems that we call low-dimensional online mirror descent (LDOMD); the algorithm generalizes both the Component Hedge algorithm of Koolen et al. (2010), and a recent algorithm of Suehiro et al. (2012). Our study offers a unification and generalization of previous work, and emphasizes the role of the convex polytope arising from the vector representation of the decision space; while Boolean representations lead to 0-1 polytopes, more general vector representations lead to more general polytopes. We study several examples of both types of polytopes. Finally, we demonstrate the benefit of having a general framework for such problems via an application to an online transportation problem; the associated transportation polytopes generalize the Birkhoff polytope of doubly stochastic matrices, and the resulting algorithm generalizes the PermELearn algorithm of Helmbold and Warmuth (2009).	[Rajkumar, Arun; Agarwal, Shivani] Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, Karnataka, India	Indian Institute of Science (IISC) - Bangalore	Rajkumar, A (corresponding author), Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, Karnataka, India.	arun_r@csa.iisc.ernet.in; shivani@csa.iisc.ernet.in			Microsoft Research India PhD Fellowship; DST; Indo-US Science & Technology Forum	Microsoft Research India PhD Fellowship(Microsoft); DST(Department of Science & Technology (India)); Indo-US Science & Technology Forum	Thanks to the anonymous reviewers for helpful comments and Chandrashekar Lakshminarayanan for helpful discussions. AR is supported by a Microsoft Research India PhD Fellowship. SA thanks DST and the Indo-US Science & Technology Forum for their support.	Ailon Nir, 2013, ABS13086797 CORR; Ailon Nir, 2013, ABS13121530 CORR; Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598; Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039; BOWMAN VJ, 1972, SIAM J APPL MATH, V22, P580, DOI 10.1137/0122054; Brualdi Richard A., 2006, COMBINATORIAL MATRIX; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Cossock D, 2008, IEEE T INFORM THEORY, V54, P5140, DOI 10.1109/TIT.2008.929939; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; GROTSCHEL M, 1985, MATH PROGRAM, V33, P43, DOI 10.1007/BF01582010; Helmbold DP, 2009, J MACH LEARN RES, V10, P1705; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Kivinen J., 2010, COLT; Suehiro Daiki, 2012, ALT; Takimoto E, 2004, J MACH LEARN RES, V4, P773, DOI 10.1162/1532443041424328; Warmuth MK, 2008, J MACH LEARN RES, V9, P2287; Yasutake S, 2011, LECT NOTES COMPUT SC, V7074, P534, DOI 10.1007/978-3-642-25591-5_55; Yasutake Shota, 2012, ACML; Zhang J, 2004, J MATH PSYCHOL, V48, P107, DOI 10.1016/j.jmp.2003.12.002; Ziegler G. M., 1995, LECT POLYTOPES	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100023
C	Rakesh, S; Bhattacharyya, C		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Rakesh, S.; Bhattacharyya, Chiranjib			Learning on graphs using Orthonormal Representation is Statistically Consistent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				CLASSIFICATION	Existing research [4] suggests that embedding graphs on a unit sphere can be beneficial in learning labels on the vertices of a graph. However the choice of optimal embedding remains an open issue. Orthonormal representation of graphs, a class of embeddings over the unit sphere, was introduced by Loyasz [2]. In this paper, we show that there exists orthonormal representations which are statistically consistent over a large class of graphs, including power law and random graphs. This result is achieved by extending the notion of consistency designed in the inductive setting to graph transduction. As part of the analysis, we explicitly derive relationships between the Rademacher complexity measure and structural properties of graphs, such as the chromatic number. We further show the fraction of vertices of a graph G, on n nodes, that need to be labelled for the learning algorithm to be I consistent, also known as labelled sample complexity, is Omega (v(G)/n)(& frac14;) where v(G) is the famous Loyasz v function of the graph. This, for the first time, relates labelled sample complexity to graph connectivity properties, such as the density of graphs. In the multiview setting, whenever individual views are expressed by a graph, it is a well known heuristic that a convex combination of Laplacians [7] tend to improve accuracy. The analysis presented here easily extends to Multiple graph transduction, and helps develop a sound statistical understanding of the heuristic, previously unavailable.	[Rakesh, S.] Indian Inst Sci, Dept Elect Engn, Bangalore 560012, Karnataka, India; [Bhattacharyya, Chiranjib] Indian Inst Sci, Dept CSA, Bangalore 560012, Karnataka, India	Indian Institute of Science (IISC) - Bangalore; Indian Institute of Science (IISC) - Bangalore	Rakesh, S (corresponding author), Indian Inst Sci, Dept Elect Engn, Bangalore 560012, Karnataka, India.	rakeshsmysore@gmail.com; chiru@csa.iisc.ernet.in						Ando R., 2007, NEURAL INFORM PROCES; Argyriou A., 2005, NEURAL INFORM PROCES; Asuncion A., 2000, UCI MACHINE LEARNING; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Blum A., 2001, P INT C MACH LEARN I, P19, DOI DOI 10.1184/R1/6606860.V1; Chen YF, 2009, IEEE ASIAN SOLID STA, P145, DOI 10.1109/ASSCC.2009.5357199; Coja-Oghlan A, 2005, COMB PROBAB COMPUT, V14, P439, DOI 10.1017/S0963548305006826; Cormen T. H., 2009, INTRO ALGORITHMS, V3rd; Cortes C., 2009, ARXIV09040814; El-Yaniv R., 2007, LEARNING THEORY, P151; El-Yaniv R, 2009, J ARTIF INTELL RES, V35, P193, DOI 10.1613/jair.2587; Jethava V., 2012, NEURAL INFORM PROCES, P1169; Johnson R., 2007, J MACHINE LEARNING R, V8; LOVASZ L, 1979, IEEE T INFORM THEORY, V25, P1, DOI 10.1109/TIT.1979.1055985; Luz CJ, 2005, SIAM J DISCRETE MATH, V19, P382, DOI 10.1137/S0895480104429181; Madani O, 2013, MACH LEARN, V92, P457, DOI 10.1007/s10994-013-5377-0; Szummer M, 2002, ADV NEUR IN, V14, P945; Tang W, 2009, IEEE DATA MINING, P1016, DOI 10.1109/ICDM.2009.125; Tsuda K, 2005, BIOINFORMATICS, V21, P59, DOI 10.1093/bioinformatics/bti1110; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; WELSH DJA, 1967, COMPUT J, V10, P85, DOI 10.1093/comjnl/10.1.85; ZHANG T, 2006, ADV NEURAL INFORM PR, V18, P1601; Zhou D., 2008, NEURAL INFORM PROCES, V16, P321	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101059
C	Ravanbakhsh, S; Rabbany, R; Greiner, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Ravanbakhsh, Siamak; Rabbany, Reihaneh; Greiner, Russell			Augmentative Message Passing for Traveling Salesman Problem and Graph Partitioning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The cutting plane method is an augmentative constrained optimization procedure that is often used with continuous-domain optimization techniques such as linear and convex programs. We investigate the viability of a similar idea within message passing - for integral solutions in the context of two combinatorial problems: 1) For Traveling Salesman Problem (TSP), we propose a factor-graph based on Held-Karp formulation, with an exponential number of constraint factors, each of which has an exponential but sparse tabular form. 2) For graph-partitioning (a.k.a. community mining) using modularity optimization, we introduce a binary variable model with a large number of constraints that enforce formation of cliques. In both cases we are able to derive simple message updates that lead to competitive solutions on benchmark instances. In particular for TSP we are able to find near-optimal solutions in the time that empirically grows with N-3, demonstrating that augmentation is practical and efficient.	[Ravanbakhsh, Siamak; Rabbany, Reihaneh; Greiner, Russell] Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada	University of Alberta	Ravanbakhsh, S (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada.	mravanba@ualberta.ca; rabbanyk@ualberta.ca; rgreiner@ualberta.ca	Greiner, Russell/AAQ-4502-2020	Greiner, Russell/0000-0001-8327-934X				Applegate D., 2006, CONCORDE TSP SOLVER; Applegate D. L., 2006, TRAVELING SALESMAN P; Bayati M., 2005, ISIT; Ben-Dor A., 1997, J COMPUTATIONAL BIOL; Blondel VD, 2008, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2008/10/P10008; Brandes U., 2008, IEEE KDE; Clauset A, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.026132; Climer S., 2004, ICML; Dantzig G., 1954, J OPERATIONS RES SOC; Frey B., 2007, MULTI DATABASE RETRI, Vvol. 315, ppp, DOI [DOI 10.1126/SCIENCE.1136800, 10.1126/science.1136800]; Gomory R.E., 1958, B AM MATH SOC, V64, P275, DOI DOI 10.1090/S0002-9904-1958-10224-4; Gupta R., 2007, ICML; HELD M, 1962, J SOC IND APPL MATH, V10, P196, DOI 10.1137/0110015; Held M., 1970, OPERATIONS RES; Helsgaun K., 2009, MATH PROGRAMMING COM; Huang B., 1769, ARXIV09081769; Johnson D., 2004, VLDB; Johnson DS, 1997, LOCAL SEARCH COMBINA, P215, DOI DOI 10.1108/01445150910987763; Kschischang F., 2001, INFORM THEORY IEEE; Leskovec J., 2010, P 19 INT C WORLD WID, P631; MEZARD M, 2002, SCIENCE; Newman MEJ, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.036104; Newman MEJ, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066133; PADBERG M, 1991, SIAM REV, V33, P60, DOI 10.1137/1033004; Papadimitriou C. H., 1977, Theoretical Computer Science, V4, P237, DOI 10.1016/0304-3975(77)90012-3; Potetz B, 2008, COMPUT VIS IMAGE UND, V112, P39, DOI 10.1016/j.cviu.2008.05.007; Ravanbakhsh S., 2014, ICML; Ravanbakhsh S., 2014, ARXIV14016686; Reichardt J, 2004, PHYS REV LETT, V93, DOI 10.1103/PhysRevLett.93.218701; Reinelt G., 1991, ORSA Journal on Computing, V3, P376, DOI 10.1287/ijoc.3.4.376; Ronhovde P, 2010, PHYS REV E, V81, DOI 10.1103/PhysRevE.81.046114; Sontag D, 2007, ADV NEURAL INFORM PR, P1393; Tarlow Daniel, 2010, INT C ART INT STAT, P812; Wang C., MESSAGE PASSING TRAV; Yanover C., 2006, JMLR	35	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103052
C	Refaat, KS; Choi, A; Darwiche, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Refaat, Khaled S.; Choi, Arthur; Darwiche, Adnan			Decomposing Parameter Estimation Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We propose a technique for decomposing the parameter learning problem in Bayesian networks into independent learning problems. Our technique applies to incomplete datasets and exploits variables that are either hidden or observed in the given dataset. We show empirically that the proposed technique can lead to orders-of-magnitude savings in learning time. We explain, analytically and empirically, the reasons behind our reported savings, and compare the proposed technique to related ones that are sometimes used by inference algorithms.	[Refaat, Khaled S.; Choi, Arthur; Darwiche, Adnan] Univ Calif Los Angeles, Comp Sci Dept, Los Angeles, CA 90024 USA	University of California System; University of California Los Angeles	Refaat, KS (corresponding author), Univ Calif Los Angeles, Comp Sci Dept, Los Angeles, CA 90024 USA.	krefaat@cs.ucla.edu; aychoi@cs.ucla.edu; darwiche@cs.ucla.edu			ONR [N00014-12-1-0423]; NSF [IIS-1118122]	ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	This work has been partially supported by ONR grant #N00014-12-1-0423 and NSF grant #IIS-1118122.	Bache K., 2013, TECHNICAL REPORT; Choi Arthur, 2011, P C UNC ART INT; Darwiche A, 2009, MODELING AND REASONING WITH BAYESIAN NETWORKS, P1, DOI 10.1017/CBO9780511811357; Darwiche Adnan, 2008, RESULTS PROBABILISTI; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Koller D., 2009, PROBABILISTIC GRAPHI; LAURITZEN SL, 1995, COMPUT STAT DATA AN, V19, P191, DOI 10.1016/0167-9473(93)E0056-A; Lin Yan, 1997, P 30 C UNC ART INT; Meng Z., 2013, P INT C ART INT STAT; Mizrahi Y. D., 2014, INT C MACH LEARN ICM; Refaat Khaled S., 2013, NEURAL INFORM PROCES; Refaat Khaled S., 2012, P C UNC ART INT, P705; Russel S., 1995, P 14 INT JOINT C ART; Shachter R., 1986, OPERATIONS RES; Shachter R., 1989, P 5 C UNC ART INT	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102032
C	Richard, E; Obozinski, G; Vert, JP		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Richard, Emile; Obozinski, Guillaume; Vert, Jean-Philippe			Tight convex relaxations for sparse matrix factorization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				NUCLEAR-NORM	Based on a new atomic norm, we propose a new convex formulation for sparse matrix factorization problems in which the number of non-zero elements of the factors is assumed fixed and known. The formulation counts sparse PCA with multiple factors, subspace clustering and low-rank sparse bilinear regression as potential applications. We compute slow rates and an upper bound on the statistical dimension [1] of the suggested norm for rank 1 matrices, showing that its statistical dimension is an order of magnitude smaller than the usual l(1)-norm, trace norm and their combinations. Even though our convex formulation is in theory hard and does not lead to provably polynomial time algorithmic schemes, we propose an active set algorithm leveraging the structure of the convex problem to solve it and show promising numerical results.	[Richard, Emile] Stanford Univ, Elect Engn, Stanford, CA 94305 USA; [Obozinski, Guillaume] Univ Paris Est, Ecole Ponts ParisTech, Paris, France; [Vert, Jean-Philippe] Inst Curie, MINES ParisTech, Paris, France	Stanford University; Ecole des Ponts ParisTech; UDICE-French Research Universities; PSL Research University Paris; MINES ParisTech; UNICANCER; Institut Curie	Richard, E (corresponding author), Stanford Univ, Elect Engn, Stanford, CA 94305 USA.				Agence Nationale de la Recherche (CHORUS project) [ANR-13-MONU-005-10]; ERC grant [SMAC-ERC-280032]	Agence Nationale de la Recherche (CHORUS project)(French National Research Agency (ANR)); ERC grant	This project was partially funded by Agence Nationale de la Recherche grant ANR-13-MONU-005-10 (CHORUS project) and by ERC grant SMAC-ERC-280032.	Amelunxen D, 2014, INF INFERENCE, V3, P224, DOI 10.1093/imaiai/iau005; Argyriou A., 2012, ADV NEURAL INFORM PR, P1466; Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015; Candes E. J., 2011, COMM PURE APPL MATH; Candes EJ, 2013, APPL COMPUT HARMON A, V34, P317, DOI 10.1016/j.acha.2012.08.010; Chandrasekaran V., 2013, P NATL ACAD SCI US; Chandrasekaran V., 2012, FDN COMPUTATIONAL MA, V12; d'Aspremont A., 2007, SIAM REV; Koltchinskii V, 2011, ANN STAT, V39, P2302, DOI 10.1214/11-AOS894; Mackey L. W., 2008, NIPS P, P1017; Mairal J, 2010, J MACH LEARN RES, V11, P19; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Oymak S., 2012, 12123753 ARXIV; Oymak S., 2013, 1302714 ARXIV; Richard E., 2012, INT C MACH LEARN ICM; Richard E., 2012, J MACHINE LEARNING R, V15, P565; Witten DM, 2009, BIOSTATISTICS, V10, P515, DOI 10.1093/biostatistics/kxp008; Doan XV, 2013, SIAM J OPTIMIZ, V23, P2502, DOI 10.1137/100814251; Yuan XT, 2013, J MACH LEARN RES, V14, P899; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101079
C	Ridgway, J; Alquier, P; Chopin, N; Liang, F		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Ridgway, James; Alquier, Pierre; Chopin, Nicolas; Liang, Feng			PAC-Bayesian AUC classification and scoring	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				VARIABLE SELECTION; BOUNDS	We develop a scoring and classification procedure based on the PAC-Bayesian approach and the AUC (Area Under Curve) criterion. We focus initially on the class of linear score functions. We derive PAC-Bayesian non-asymptotic bounds for two types of prior for the score parameters: a Gaussian prior, and a spike-and-slab prior; the latter makes it possible to perform feature selection. One important advantage of our approach is that it is amenable to powerful Bayesian computational tools. We derive in particular a Sequential Monte Carlo algorithm, as an efficient method which may be used as a gold standard, and an Expectation-Propagation algorithm, as a much faster but approximate method. We also extend our method to a class of non-linear score functions, essentially leading to a nonparametric procedure, by considering a Gaussian process prior.	[Ridgway, James] Univ Dauphine, CREST, Paris, France; [Ridgway, James] Univ Dauphine, CEREMADE, Paris, France; [Alquier, Pierre; Chopin, Nicolas] ENSAE, CREST, Paris, France; [Chopin, Nicolas] HEC Paris, Paris, France; [Liang, Feng] Univ Illinois, Champaign, IL USA	Institut Polytechnique de Paris; Hautes Etudes Commerciales (HEC) Paris; University of Illinois System; University of Illinois Urbana-Champaign	Ridgway, J (corresponding author), Univ Dauphine, CREST, Paris, France.	james.ridgway@ensae.fr; pierre.alquier@ucd.ie; nicolas.chopin@ensae.fr; liangf@illinois.edu	Liang, Fenghua/HHM-3798-2022; Liang, Feng/GZK-4305-2022					Alquier P, 2008, MATH METHODS STAT, V17, P279, DOI 10.3103/S1066530708040017; Alquier P, 2013, J MACH LEARN RES, V14, P243; Buhlmann P., 2011, STAT HIGH DIMENSIONN; Catoni Olivier, 2007, IMS LECT NOTES MONOG, V56; Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910; Clemencon Stephan, 2008, Journal of Biological Dynamics, V2, P392, DOI 10.1080/17513750801993266; Cortes C., 2003, NIPS, V9; Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x; Freund Y., 2003, J MACHINE LEARNING R, V4, P933, DOI DOI 10.1162/JMLR.2003.4.6.933; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; Hernandez-Lobato D, 2013, J MACH LEARN RES, V14, P1891; Jasra A, 2007, STAT COMPUT, V17, P263, DOI 10.1007/s11222-007-9028-9; Lecue G., 2007, THESIS, V6; Mammen E, 1999, ANN STAT, V27, P1808; McAllester D. A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P230, DOI 10.1145/279943.279989; Minka T.P., 2001, P 17 C UNC ART INT, P362; MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129; Opper M, 2000, NEURAL COMPUT, V12, P2655, DOI 10.1162/089976600300014881; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Robbiano S, 2013, ELECTRON J STAT, V7, P1249, DOI 10.1214/13-EJS805; Rockova V., 2013, J AM STAT ASS; Seeger M. W., 2005, EXPECTATION PROPAGAT; Shawe-Taylor J., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P2, DOI 10.1145/267460.267466; van der Vaart AW, 2009, ANN STAT, V37, P2655, DOI 10.1214/08-AOS678; van Gerven MAJ, 2010, NEUROIMAGE, V50, P150, DOI 10.1016/j.neuroimage.2009.11.064; Yan L., 2003, P 20 INT C MACH LEAR, P848	26	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103055
C	Rosman, G; Volkov, M; Feldman, D; Fisher, JW; Rus, D		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Rosman, Guy; Volkov, Mikhail; Feldman, Danny; Fisher, John W., III; Rus, Daniela			Coresets for k-Segmentation of Streaming Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				APPROXIMATION	Life-logging video streams, financial time series, and Twitter tweets are a few examples of high-dimensional signals over practically unbounded time. We consider the problem of computing optimal segmentation of such signals by a k-piecewise linear function, using only one pass over the data by maintaining a coreset for the signal. The coreset enables fast further analysis such as automatic summarization and analysis of such signals. A coreset (core-set) is a compact representation of the data seen so far, which approximates the data well for a specific task - in our case, segmentation of the stream. We show that, perhaps surprisingly, the segmentation problem admits coresets of cardinality only linear in the number of segments k, independently of both the dimension d of the signal, and its number n of points. More precisely, we construct a representation of size O(k log n/epsilon(2)) that provides a (1+epsilon)-approximation for the sum of squared distances to any given k-piecewise linear function. Moreover, such coresets can be constructed in a parallel streaming approach. Our results rely on a novel reduction of statistical estimations to problems in computational geometry. We empirically evaluate our algorithms on very large synthetic and real data sets from GPS, video and financial domains, using 255 machines in Amazon cloud.	[Rosman, Guy; Volkov, Mikhail; Feldman, Danny; Fisher, John W., III; Rus, Daniela] MIT, CSAIL, 32 Vassar St, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Rosman, G (corresponding author), MIT, CSAIL, 32 Vassar St, Cambridge, MA 02139 USA.	rosman@csail.mit.edu; mikhail@csail.mit.edu; dannyf@csail.mit.edu; fisher@csail.mit.edu; rus@csail.mit.edu			MIT-Technion fellowship; MIT Lincoln Laboratory; Hon Hai/Foxconn Technology Group	MIT-Technion fellowship; MIT Lincoln Laboratory; Hon Hai/Foxconn Technology Group	Guy Rosman was partially supported by MIT-Technion fellowship; Support for this research has been provided by Hon Hai/Foxconn Technology Group and MIT Lincoln Laboratory. The authors are grateful for this support.	Agarwal P, 2005, COMBINATORIAL COMPUT, P1; Bandla S., 2013, ICCV; BBC, 2013, BITC PAN SELL HALV I; BELLMAN R, 1961, COMMUN ACM, V4, P284, DOI 10.1145/366573.366611; Churchill W., 2012, P IEEE INT TRANSP SY; CNBC, 2013, BITC CRASH SPURS RAC; Deng J., 2009, P CVPR; Douglas DH, 1973, CARTOGR INT J GEOGR, V10, P112, DOI [10.3138/fm57-6770-u75u-7727, DOI 10.3138/FM57-6770-U75U-7727]; Feldman D., 2010, STOC; Feldman D., 2012, P 20 INT C ADV GEOGR, P23, DOI DOI 10.1145/2424321.2424325; Feldman D, 2012, IPSN'12: PROCEEDINGS OF THE 11TH INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS, P257, DOI 10.1109/IPSN.2012.6920941; Feldman Dan, 2013, SODA; Gilbert A. C., 2002, P 34 ANN ACM S THEOR, P389; Girdhar Y, 2012, IEEE INT CONF ROBOT, P3490, DOI 10.1109/ICRA.2012.6224657; Guha S, 2006, ACM T DATABASE SYST, V31, P396, DOI 10.1145/1132863.1132873; Li YP, 2009, IEEE I CONF COMP VIS, P1957, DOI 10.1109/ICCV.2009.5459432; Lu Z, 2013, PROC CVPR IEEE, P2714, DOI 10.1109/CVPR.2013.350; Paul Rohan, 2014, ICRA, P2; Ramer U, 1972, COMPUT GRAPH IMAGE P, V1, P244, DOI [DOI 10.1016/S0146-664X(72)80017-0, 10.1016/S0146-664X(72)80017-0]; RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Trajcevski G., 2006, MobiDE 2006. Proceedings of the Fifth ACM International Workshop on Data Engineering for Wireless and Mobile Access, P19, DOI 10.1145/1140104.1140110	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103032
C	Ruozzi, N; Jebara, T		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Ruozzi, Nicholas; Jebara, Tony			Making Pairwise Binary Graphical Models Attractive	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				BELIEF PROPAGATION; CONVERGENCE	Computing the partition function (i.e., the normalizing constant) of a given pairwise binary graphical model is NP-hard in general. As a result, the partition function is typically estimated by approximate inference algorithms such as belief propagation (BP) and tree-reweighted belief propagation (TRBP). The former provides reasonable estimates in practice but has convergence issues. The later has better convergence properties but typically provides poorer estimates. In this work, we propose a novel scheme that has better convergence properties than BP and provably provides better partition function estimates in many instances than TRBP. In particular, given an arbitrary pairwise binary graphical model, we construct a specific "attractive" 2-cover. We explore the properties of this special cover and show that it can be used to construct an algorithm with the desired properties.	[Ruozzi, Nicholas] Columbia Univ, Inst Data Sci & Engn, New York, NY 10027 USA; [Jebara, Tony] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA	Columbia University; Columbia University	Ruozzi, N (corresponding author), Columbia Univ, Inst Data Sci & Engn, New York, NY 10027 USA.	nr2493@columbia.edu; jebara@cs.columbia.edu			NSF [IIS-1117631, CCF-1302269, IIS-1451500]	NSF(National Science Foundation (NSF))	This work was supported in part by NSF grants IIS-1117631, CCF-1302269 and IIS-1451500.	Bayati M, 2011, SIAM J DISCRETE MATH, V25, P989, DOI 10.1137/090753115; Crama Y., 2011, BOOLEAN FUNCTIONS TH, V142; Globerson A., 2007, P 23 UNC ART INT UAI; Globerson A., 2007, P 21 NEUR INF PROC S; GREIG DM, 1989, J ROY STAT SOC B MET, V51, P271, DOI 10.1111/j.2517-6161.1989.tb01764.x; Harary F, 1953, MICH MATH J, V2, P143; Kolmogorov V, 2002, LECT NOTES COMPUT SC, V2352, P65; Kolmogorov V., 2005, P 21 C UNC ART INT, P316, DOI DOI 10.5555/3020336.3020376; Kolmogorov V, 2007, IEEE T PATTERN ANAL, V29, P1274, DOI 10.1109/TPAMI.2007.1031; Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10; Marshall A.W, 1979, INEQUALITIES THEORY; Meltzer T., 2009, P 25 UNC ART INT UAI; Mooij JM, 2007, IEEE T INFORM THEORY, V53, P4422, DOI 10.1109/TIT.2007.909166; Richardson M, 2003, LECT NOTES COMPUT SC, V2870, P351; Ruozzi N, 2013, J MACH LEARN RES, V14, P2287; Ruozzi Nicholas, 2012, NEURAL INFORM PROCES; Schlesinger D, 2007, LECT NOTES COMPUT SC, V4679, P28; Taga N, 2006, IEICE T FUND ELECTR, VE89A, P575, DOI 10.1093/ietfec/e89-a.2.575; Vontobel P. O., 2005, ABSCS0512078 CORR; Vontobel P. O., 2013, SYSTEMS MAN CYBERN A; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938; Watanabe Y., 2011, ADV NEURAL INFORM PR, P1521; Weller A., 2014, UNCERTAINTY ARTIFICA; Weller A., 2013, 16 INT C ART INT STA; Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036; Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085; [No title captured]	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102093
C	Russo, D; Van Roy, B		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Russo, Daniel; Van Roy, Benjamin			Learning to Optimize via Information-Directed Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				BOUNDS	We propose information-directed sampling - a new algorithm for online optimization problems in which a decision-maker must balance between exploration and exploitation while learning from partial feedback. Each action is sampled in a manner that minimizes the ratio between the square of expected single-period regret and a measure of information gain: the mutual information between the optimal action and the next observation. We establish an expected regret bound for information-directed sampling that applies across a very general class of models and scales with the entropy of the optimal action distribution. For the widely studied Bernoulli and linear bandit models, we demonstrate simulation performance surpassing popular approaches, including upper confidence bound algorithms, Thompson sampling, and knowledge gradient. Further, we present simple analytic examples illustrating that information-directed sampling can dramatically outperform upper confidence bound algorithms and Thompson sampling due to the way it measures information gain.	[Russo, Daniel; Van Roy, Benjamin] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Russo, D (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	djrusso@stanford.edu; bvr@stanford.edu						Audibert J. - Y., 2013, MATH OPERATIONS RES; Audibert Jean-Yves, 2009, COLT; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Boyd S, 2004, CONVEX OPTIMIZATION; Brochu E, 2010, ARXIV PREPRINT ARXIV; Bubeck S, 2011, J MACH LEARN RES, V12, P1655; Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119; Cesa-Bianchi N., 2012, ARXIV12045721; Chaloner K, 1995, STAT SCI, V10, P273, DOI 10.1214/ss/1177009939; Chapelle O., 2011, NIPS; Dani V, 2008, P C LEARN THEOR COLT, P355; Dani V, 2007, ADV NEURAL INFORM PR, V20, P345; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; Gopalan A., 2014, ICML; Goyal N., 2013, ICML; Gray RM, 2011, ENTROPY AND INFORMATION THEORY , SECOND EDITION, P395, DOI 10.1007/978-1-4419-7970-4; Hennig P, 2012, J MACH LEARN RES, V13, P1809; Jedynak B, 2012, J APPL PROBAB, V49, P114, DOI 10.1239/jap/1331216837; Kauffmann E., 2012, ALT; Kaufmann E., 2012, AISTATS; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Powell W.B, 2012, OPTIMAL LEARNING, V841; Rusmevichientong P, 2010, MATH OPER RES, V35, P395, DOI 10.1287/moor.1100.0446; Russo D., 2014, ARXIV14035341; Russo D., 2013, CORR; Russo D., 2014, ARXIV14035556; Ryzhov IO, 2012, OPER RES, V60, P180, DOI 10.1287/opre.1110.0999; Scott SL, 2010, APPL STOCH MODEL BUS, V26, P639, DOI 10.1002/asmb.874; Srinivas N, 2012, IEEE T INFORM THEORY, V58, P3250, DOI 10.1109/TIT.2011.2182033; Villemonteix J, 2009, J GLOBAL OPTIM, V44, P509, DOI 10.1007/s10898-008-9354-2	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102024
C	Saberian, M; Vasconcelos, N		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Saberian, Mohammad; Vasconcelos, Nuno			Multi-Resolution Cascades for Multiclass Object Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					An algorithm for learning fast multiclass object detection cascades is introduced. It produces multi-resolution (MRes) cascades, whose early stages are binary target vs. non-target detectors that eliminate false positives, late stages multiclass classifiers that finely discriminate target classes, and middle stages have intermediate numbers of classes, determined in a data-driven manner. This MRes structure is achieved with a new structurally biased boosting algorithm (SBBoost). SBBost extends previous multiclass boosting approaches, whose boosting mechanisms are shown to implement two complementary data-driven biases: 1) the standard bias towards examples difficult to classify, and 2) a bias towards difficult classes. It is shown that structural biases can be implemented by generalizing this class-based bias, so as to encourage the desired MRes structure. This is accomplished through a generalized definition of multiclass margin, which includes a set of bias parameters. SBBoost is a boosting algorithm for maximization of this margin. It can also be interpreted as standard multiclass boosting algorithm augmented with margin thresholds or a cost-sensitive boosting algorithm with costs defined by the bias parameters. A stage adaptive bias policy is then introduced to determine bias parameters in a data driven manner. This is shown to produce MRes cascades that have high detection rate and are computationally efficient. Experiments on multiclass object detection show improved performance over previous solutions.	[Saberian, Mohammad] Yahoo Labs, La Jolla, CA 92093 USA; [Vasconcelos, Nuno] Univ Calif San Diego, Stat Visual Comp Lab, San Diego, CA 92103 USA	University of California System; University of California San Diego	Saberian, M (corresponding author), Yahoo Labs, La Jolla, CA 92093 USA.	saberian@yahoo-inc.com; nuno@ucsd.edu						Bourdev L, 2005, PROC CVPR IEEE, P236, DOI 10.1109/cvpr.2005.310; Dollar P., 2009, BMVC; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Huang C, 2007, IEEE T PATTERN ANAL, V29, P671, DOI 10.1109/TPAMI.2007.1011; Jones M., 2003, P COMP VIS PATT REC; KUO C, 2009, P IEEE WORKSH APPL C, P1, DOI DOI 10.1109/WACV.2009.5403033; Larsson F, 2011, IET COMPUT VIS, V5, P244, DOI 10.1049/iet-cvi.2010.0040; Masnadi-Shirazi H, 2011, IEEE T PATTERN ANAL, V33, P294, DOI 10.1109/TPAMI.2010.71; MASON L, 2000, NIPS; Mease D, 2008, J MACH LEARN RES, V9, P131; Perrotton X, 2010, PROC CVPR IEEE, P958, DOI 10.1109/CVPR.2010.5540115; Pham M., 2008, P IEEE C COMP VIS PA, P1; Saberian M. J., 2011, NIPS; Saberian MJ, 2012, IEEE T PATTERN ANAL, V34, P2005, DOI 10.1109/TPAMI.2011.281; Sochman J, 2005, PROC CVPR IEEE, P150; Torralba A, 2007, IEEE T PATTERN ANAL, V29, P854, DOI 10.1109/TPAMI.2007.1055; Viola P, 2002, ADV NEUR IN, V14, P1311; Viola P., 2001, WORKSH STAT COMP THE; Wu B, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P79; Zhu Qiang, 2006, CVPR, DOI DOI 10.1109/CVPR.2006.119	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102074
C	Sani, A; Neu, G; Lazaric, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Sani, Amir; Neu, Gergely; Lazaric, Alessandro			Exploiting easy data in online optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ALGORITHMS; TRACKING; REGRET	We consider the problem of online optimization, where a learner chooses a decision from a given decision set and suffers some loss associated with the decision and the state of the environment. The learner's objective is to minimize its cumulative regret against the best fixed decision in hindsight. Over the past few decades numerous variants have been considered, with many algorithms designed to achieve sub-linear regret in the worst case. However, this level of robustness comes at a cost. Proposed algorithms are often over-conservative, failing to adapt to the actual complexity of the loss sequence which is often far from the worst case. In this paper we introduce a general algorithm that, provided with a "safe" learning algorithm and an opportunistic "benchmark", can effectively combine good worst-case guarantees with much improved performance on "easy" data. We derive general theoretical bounds on the regret of the proposed algorithm and discuss its implementation in a wide range of applications, notably in the problem of learning with shifting experts (a recent COLT open problem). Finally, we provide numerical simulations in the setting of prediction with expert advice with comparisons to the state of the art.	[Sani, Amir; Neu, Gergely; Lazaric, Alessandro] INRIA Lille, SequeL Team, Lille, France		Sani, A (corresponding author), INRIA Lille, SequeL Team, Lille, France.	amir.sani@inria.fr; gergely.neu@inria.fr; alessandro.lazaric@inria.fr			French Ministry of Higher Education and Research; European Community [270327]; FUI project Hermes	French Ministry of Higher Education and Research; European Community(European Commission); FUI project Hermes	This work was supported by the French Ministry of Higher Education and Research and by the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement 270327 (project CompLACS), and by FUI project Hermes.	Agarwal A., 2010, P COLT, P28; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bubeck S., 2012, COLT; Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; de Rooij S., 2014, J MACHINE LEARNING R; Even-Dar E, 2008, MACH LEARN, V72, P21, DOI 10.1007/s10994-008-5060-z; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Gaillard P., 2014, JMLR WORKSHOP C P, P176; Geulen S., 2010, P 23 ANN C LEARN THE, P132; Grunwald P., 2013, NIPS WORKSH LEARN FA; Gyorgy A, 2012, IEEE T INFORM THEORY, V58, P6709, DOI 10.1109/TIT.2012.2209627; Gyorgy A., 2013, IEEE T INFORM UNPUB; Hazan E., 2008, ADV NEURAL INFORM PR, P65; Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Seldin Y, 2014, PR MACH LEARN RES, V32; Seldin Y, 2014, PR MACH LEARN RES, V32, P1287; Vovk V. G., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory, P371; Warmuth M., 2014, COLT 2014 OPEN PROBL; Zinkevich M., 2003, INT C MACH LEARN ICM	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101107
C	Sarkhel, S; Venugopal, D; Singla, P; Gogate, V		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Sarkhel, Somdeb; Venugopal, Deepak; Singla, Parag; Gogate, Vibhav			An Integer Polynomial Programming Based Framework for Lifted MAP Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In this paper, we present a new approach for lifted MAP inference in Markov logic networks (MLNs). The key idea in our approach is to compactly encode the MAP inference problem as an Integer Polynomial Program (IPP) by schematically applying three lifted inference steps to the MLN: lifted decomposition, lifted conditioning, and partial grounding. Our IPP encoding is lifted in the sense that an integer assignment to a variable in the IPP may represent a truth-assignment to multiple indistinguishable ground atoms in the MLN. We show how to solve the IPP by first converting it to an Integer Linear Program (ILP) and then solving the latter using state-of-the-art ILP techniques. Experiments on several benchmark MLNs show that our new algorithm is substantially superior to ground inference and existing methods in terms of computational efficiency and solution quality.	[Sarkhel, Somdeb; Venugopal, Deepak; Gogate, Vibhav] Univ Texas Dallas, Comp Sci Dept, Richardson, TX 75083 USA; [Singla, Parag] IIT Delhi, Dept CSE, Delhi, India	University of Texas System; University of Texas Dallas; Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Delhi	Sarkhel, S (corresponding author), Univ Texas Dallas, Comp Sci Dept, Richardson, TX 75083 USA.	sxs104721@utdallas.edu; dxv021000@utdallas.edu; parags@cse.iitd.ac.in; vgogate@hlt.utdallas.edu			AFRL [FA8750-14-C-0021]; ARO MURI [W911NF-08-1-0242]; DARPA Probabilistic Programming for Advanced Machine Learning Program under AFRL prime [FA8750-14-C-0005]	AFRL(United States Department of DefenseUS Air Force Research Laboratory); ARO MURI(MURI); DARPA Probabilistic Programming for Advanced Machine Learning Program under AFRL prime	This work was supported in part by the AFRL under contract number FA8750-14-C-0021, by the ARO MURI grant W911NF-08-1-0242, and by the DARPA Probabilistic Programming for Advanced Machine Learning Program under AFRL prime contract number FA8750-14-C-0005. Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views or official policies, either expressed or implied, of DARPA, AFRL, ARO or the US government.	Apsel U, 2012, P 28 C UNC ART INT C, P74; Bui H., 2013, UAI; de Salvo Braz R., 2007, THESIS; den Broeck G.V., 2011, IJCAI 2011 P 22 INT, P2178, DOI [10.5591/978-1-57735-516-8/IJCAI11-363, DOI 10.5591/978-1-57735-516-8/IJCAI11-363]; Domingos P., 2009, MARKOV LOGIC INTERFA; Domingos P., 2011, UAI, P256; Getoor Lise, 2007, INTRO STAT RELATIONA; Gogate V., 2012, AAAI; Gurobi Optimization Inc., 2014, GUROBI OPTIMIZER REF; Hadiji Fabian, 2013, AAAI, P394; Jha A., 2010, P 24 ANN C NEUR INF, P973; Kok S., 2008, TECHNICAL REPORT; Mladenov Martin, 2014, AISTATS 2014; Niu F, 2011, PROC VLDB ENDOW, V4, P373, DOI 10.14778/1978665.1978669; Noessner J, 2013, AAAI; Pipatsrisawat K, 2007, LECT NOTES COMPUT SC, V4830, P223; Poole D., 2003, P INT JOINT C ART IN, P985; Sarkhel Somdeb, 2014, AISTATS 2014; Selman B, 1996, DIMACS SERIES DISCRE, P521, DOI [DOI 10.1090/DIMACS/026, 10.1090/dimacs/026/25, DOI 10.1090/DIMACS/026/25]; SHAVLIK J, 2009, IJCAI, P1951; Singla P., 2008, P 23 AAAI C ART INT, V8, P1094; Van den Broeck G., 2012, UAI, P131; Venugopal D., 2012, NIPS, V3, P1655; WATTERS LJ, 1967, OPER RES, V15, P1171, DOI 10.1287/opre.15.6.1171; Zhang Nevin Lianwen, 2008, MILCH BRIAN LUKE S Z, V2, P1062	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102060
C	Savin, C; Deneve, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Savin, Cristina; Deneve, Sophie			Spatio-temporal Representations of Uncertainty in Spiking Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				NEURONAL CORRELATION	It has been long argued that, because of inherent ambiguity and noise, the brain needs to represent uncertainty in the form of probability distributions. The neural encoding of such distributions remains however highly controversial. Here we present a novel circuit model for representing multidimensional real-valued distributions using a spike based spatio-temporal code. Our model combines the computational advantages of the currently competing models for probabilistic codes and exhibits realistic neural responses along a variety of classic measures. Furthermore, the model highlights the challenges associated with interpreting neural activity in relation to behavioral uncertainty and points to alternative population-level approaches for the experimental validation of distributed representations.	[Savin, Cristina] IST Austria, A-3400 Klosterneuburg, Austria; [Deneve, Sophie] ENS Paris, Grp Neural Theory, Paris, France	Institute of Science & Technology - Austria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Savin, C (corresponding author), IST Austria, A-3400 Klosterneuburg, Austria.	csavin@ist.ac.at; sophie.deneve@ens.fr	Savin, Cristina/ABI-4570-2020	Savin, Cristina/0000-0002-3414-8244				Boerlin M., 2013, PLOS COMPUTATIONAL B; Boerlin M, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001080; Bourdoukan R, 2012, ADV NEURAL INFORM PR, V25, P2294; Buesing L, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002211; Churchland MM, 2010, NAT NEUROSCI, V13, P369, DOI 10.1038/nn.2501; Ecker AS, 2010, SCIENCE, V327, P584, DOI 10.1126/science.1179867; Fiser J, 2010, TRENDS COGN SCI, V14, P119, DOI 10.1016/j.tics.2010.01.003; Hennequin G, 2014, ARXIV14043521; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hoyer PO, 2003, ADV NEURAL INFORM PR, V15, P293; Kohn A, 2005, J NEUROSCI, V25, P3661, DOI 10.1523/JNEUROSCI.5106-04.2005; Macke Jakob H, 2012, ADV NEURAL INFORM PR, V24, P1350; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Ohiorhenuan IE, 2010, NATURE, V466, P617, DOI 10.1038/nature09178; Pouget A, 1998, NEURAL COMPUT, V10, P373, DOI 10.1162/089976698300017809; Pouget A, 2013, NAT NEUROSCI, V16, P1170, DOI 10.1038/nn.3495; Renart A, 2010, SCIENCE, V327, P587, DOI 10.1126/science.1179850; Savin C, 2013, ADV NEURAL INFORM PR, V26, P288; Savin C, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003489; Smith MA, 2008, J NEUROSCI, V28, P12591, DOI 10.1523/JNEUROSCI.2929-08.2008; Yan KL, 2005, NEURAL COMPUT, V17, P397, DOI 10.1162/0899766053011474; Yu JN, 2013, J NEUROSCI, V33, P18855, DOI 10.1523/JNEUROSCI.2665-13.2013; Zemel RS, 1998, NEURAL COMPUT, V10, P403, DOI 10.1162/089976698300017818	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101014
C	Sedghi, H; Anandkumar, A; Jonckheere, E		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Sedghi, Hanie; Anandkumar, Anima; Jonckheere, Edmond			Multi-Step Stochastic ADMM in High Dimensions: Applications to Sparse Optimization and Matrix Decomposition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				RATES	In this paper, we consider a multi-step version of the stochastic ADMM method with efficient guarantees for high-dimensional problems. We first analyze the simple setting, where the optimization problem consists of a loss function and a single regularizer (e.g. sparse optimization), and then extend to the multi-block setting with multiple regularizers and multiple variables (e.g. matrix decomposition into sparse and low rank components). For the sparse optimization problem, our method achieves the minimax rate of O(s log d/T) for s-sparse problems in d dimensions in T steps, and is thus, unimprovable by any method up to constant factors. For the matrix decomposition problem with a general loss function, we analyze the multi-step ADMM with multiple blocks. We establish O(1/T) rate and efficient scaling as the size of matrix grows. For natural noise models (e.g. independent noise), our convergence rate is minimax-optimal. Thus, we establish tight convergence guarantees for multi-block ADMM in high dimensions. Experiments show that for both sparse optimization and matrix decomposition problems, our algorithm outperforms the state-of-the-art methods.	[Sedghi, Hanie; Jonckheere, Edmond] Univ Southern Calif, Los Angeles, CA 90089 USA; [Anandkumar, Anima] Univ Calif Irvine, Irvine, CA 92697 USA	University of Southern California; University of California System; University of California Irvine	Sedghi, H (corresponding author), Univ Southern Calif, Los Angeles, CA 90089 USA.	hsedghi@usc.edu; a.anandkumar@uci.edu; jonckhee@usc.edu			Microsoft Faculty Fellowship; NSF [CCF-1219234, CCF-1254106]; ARO YIP Award [W911NF-13-1-0084]	Microsoft Faculty Fellowship(Microsoft); NSF(National Science Foundation (NSF)); ARO YIP Award	We acknowledge detailed discussions with Majid Janzamin and thank him for valuable comments on sparse and low rank recovery. The authors thank Alekh Agarwal for detailed discussions of his work and the minimax bounds. A. Anandkumar is supported in part by Microsoft Faculty Fellowship, NSF Career award CCF-1254106, NSF Award CCF-1219234, and ARO YIP Award W911NF-13-1-0084.	Agarwal A., 2012, NIPS, P1547; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793; Deng W., 2012, GLOBAL LINEAR CONVER, ptR12; Duchi J., 2008, PROC 25 INT C MACH L, P272; Goldstein T., 2012, FAST ALTERNATING DIR, P12; Hsu D, 2011, IEEE T INFORM THEORY, V57, P7221, DOI 10.1109/TIT.2011.2158250; Lin Z., 2010, ARXIV10095055, DOI DOI 10.1016/J.JSB.2012.10.010; Luo Z.-Q, 2013, ARXIV13085294; Luo Z. - Q., 2012, ARXIV12083922; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Ouyang Hua, 2013, P 30 INT C MACH LEAR, P80; Raskutti G, 2011, IEEE T INFORM THEORY, V57, P6976, DOI 10.1109/TIT.2011.2165799; Sedghi Hanie, 2014, ARXIV14025131; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; Vu V. H., 2005, P 37 ANN ACM S THEOR, P423; Wang H., 2013, ARXIV13063203	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103064
C	Seiler, C; Rubinstein-Salzedo, S; Holmes, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Seiler, Christof; Rubinstein-Salzedo, Simon; Holmes, Susan			Positive Curvature and Hamiltonian Monte Carlo	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The Jacobi metric introduced in mathematical physics can be used to analyze Hamiltonian Monte Carlo (HMC). In a geometrical setting, each step of HMC corresponds to a geodesic on a Riemannian manifold with a Jacobi metric. Our calculation of the sectional curvature of this HMC manifold allows us to see that it is positive in cases such as sampling from a high dimensional multivariate Gaussian. We show that positive curvature can be used to prove theoretical concentration results for HMC Markov chains.	[Seiler, Christof; Rubinstein-Salzedo, Simon; Holmes, Susan] Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Stanford University	Seiler, C (corresponding author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	cseiler@stanford.edu; simonr@stanford.edu; susan@stat.stanford.edu		Holmes, Susan/0000-0002-2208-8168	Swiss National Science Foundation; NIH [R01-GM086884]	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	The authors would like to thank Sourav Chatterjee, Otis Chodosh, Persi Diaconis, Emanuel Milman, Veniamin Morgenshtern, Richard Montgomery, Yann Ollivier, Xavier Pennec, Mehrdad Shahshahani, and Aaron Smith for their insight and helpful discussions. This work was supported by a postdoctoral fellowship from the Swiss National Science Foundation and NIH grant R01-GM086884.	Allassonniere S, 2013, ANN MATH BLAISE PASC, V20, P1; Allassonniere S, 2010, BERNOULLI, V16, P641, DOI 10.3150/09-BEJ229; Beskos A, 2013, BERNOULLI, V19, P1501, DOI 10.3150/12-BEJ414; Cotter CJ, 2013, INVERSE PROBL, V29, DOI 10.1088/0266-5611/29/4/045011; Do Carmo M.P., 1992, RIEMANNIAN GEOMETRY; Genz A., 2009, COMPUTATION MULTIVAR, V195; Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x; Grenander U, 1998, Q APPL MATH, V56, P617, DOI 10.1090/qam/1668732; Holmes Susan, 2014, ARXIV14071114; Jacobi C. G. J., 2009, TEXTS READINGS MATH, V51; Joulin A, 2007, BERNOULLI, V13, P782, DOI 10.3150/07-BEJ6039; Joulin A, 2010, ANN PROBAB, V38, P2418, DOI 10.1214/10-AOP541; Kuo F.Y., 2005, NOTICES AMS, V52, P9; Levy P., 1922, LECONS ANAL FONCTION; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Ollivier Y, 2009, J FUNCT ANAL, V256, P810, DOI 10.1016/j.jfa.2008.11.001; ONG CP, 1975, ADV MATH, V15, P269; Pettini M., 2007, INTERDISCIPLINARY AP, V33; Stuart AM, 2010, ACTA NUMER, V19, P451, DOI 10.1017/S0962492910000061; Sturm KT, 2006, ACTA MATH-DJURSHOLM, V196, P65, DOI 10.1007/s11511-006-0002-8; Van Leemput K, 2009, IEEE T MED IMAGING, V28, P822, DOI 10.1109/TMI.2008.2010434; Zhang Miaomiao, 2013, Inf Process Med Imaging, V23, P37, DOI 10.1007/978-3-642-38868-2_4	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102061
C	Shamir, O		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Shamir, Ohad			Fundamental Limits of Online and Distributed Algorithms for Statistical Learning and Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				INFORMATION	Many machine learning approaches are characterized by information constraints on how they interact with the training data. These include memory and sequential access constraints (e.g. fast first-order methods to solve stochastic optimization problems); communication constraints (e.g. distributed learning); partial access to the underlying data (e.g. missing features and multi-armed bandits) and more. However, currently we have little understanding how such information constraints fundamentally affect our performance, independent of the learning problem semantics. For example, are there learning problems where any algorithm which has small memory footprint (or can use any bounded number of bits from each example, or has certain communication constraints) will perform worse than what is possible without such constraints ? In this paper, we describe how a single set of results implies positive answers to the above, for several different settings.	[Shamir, Ohad] Weizmann Inst Sci, Rehovot, Israel	Weizmann Institute of Science	Shamir, O (corresponding author), Weizmann Inst Sci, Rehovot, Israel.	ohad.shamir@weizmann.ac.il			Intel ICRI-CI Institute; Israel Science Foundation [425/13]; FP7 Marie Curie CIG grant	Intel ICRI-CI Institute; Israel Science Foundation(Israel Science Foundation); FP7 Marie Curie CIG grant	This research is supported by the Intel ICRI-CI Institute, Israel Science Foundation grant 425/13, and an FP7 Marie Curie CIG grant. We thank John Duchi, Yevgeny Seldin and Yuchen Zhang for helpful comments.	Agarwal A., 2010, COLT; Alon Noga, 1996, STOC; Audibert J.-Y., 2011, COLT; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Balcan M.-F., 2012, COLT; Balsubramani A., 2013, NIPS; Berthet A., 2013, COLT; Bien J, 2011, BIOMETRIKA, V98, P807, DOI 10.1093/biomet/asr054; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cesa-Bianchi N, 2011, J MACH LEARN RES, V12, P2857; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chien S., 2010, ICS; Cover TM, 2006, ELEMENTS INFORM THEO; Crouch M., 2013, MASSIVE; Dragomir S. S., 2000, SELECTED TOPICS HERM; Guha S., 2007, AISTATS; Kontorovich L, 2012, STAT COMPUT, V22, P1155, DOI 10.1007/s11222-011-9293-5; Mannor S., 2011, NIPS; Mitliagkas I., 2013, NIPS; Muthukrishnan S., 2005, DATA STREAMS ALGORIT; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Rahimi A., 2007, NIPS; Seldin Y., 2014, ICML; Taneja IJ, 2004, INFORM SCIENCES, V166, P105, DOI 10.1016/j.ins.2003.11.002; Williams C. K. I., 2001, NIPS; Woodruff D., 2009, ICDT; Zhang Y, 2013, NIPS; Zhang Y., 2012, NIPS; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	31	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101057
C	Shanmugam, K; Tandon, R; Dimakis, AG; Ravikumar, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Shanmugam, Karthikeyan; Tandon, Rashish; Dimakis, Alexandros G.; Ravikumar, Pradeep			On the Information Theoretic Limits of Learning Ising Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				MINIMAX RATES	We provide a general framework for computing lower-bounds on the sample complexity of recovering the underlying graphs of Ising models, given i:i:d: samples. While there have been recent results for specific graph classes, these involve fairly extensive technical arguments that are specialized to each specific graph class. In contrast, we isolate two key graph-structural ingredients that can then be used to specify sample complexity lower-bounds. Presence of these structural properties makes the graph class hard to learn. We derive corollaries of our main result that not only recover existing recent results, but also provide lower bounds for novel graph classes not considered previously. We also extend our framework to the random graph setting and derive corollaries for Erdos-Renyi graphs in a certain dense setting.	[Shanmugam, Karthikeyan; Dimakis, Alexandros G.] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA; [Tandon, Rashish; Ravikumar, Pradeep] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin; University of Texas System; University of Texas Austin	Shanmugam, K (corresponding author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.	karthiksh@utexas.edu; rashish@cs.utexas.edu; dimakis@austin.utexas.edu; pradeepr@cs.utexas.edu	Dimakis, Alexandros G/P-6034-2019; Dimakis, Alexandros G/A-5496-2011	Dimakis, Alexandros G/0000-0002-4244-7033; Dimakis, Alexandros G/0000-0002-4244-7033	ARO [W911NF-12-1-0390]; NSF [IIS-1149803, IIS-1320894, IIS-1447574, DMS-1264033, CCF 1422549, 1344364, 1344179]; DARPA STTR; ARO YIP award	ARO; NSF(National Science Foundation (NSF)); DARPA STTR; ARO YIP award	R.T. and P.R. acknowledge the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1320894, IIS-1447574, and DMS-1264033. K.S. and A.D. acknowledge the support of NSF via CCF 1422549, 1344364, 1344179 and DARPA STTR and a ARO YIP award.	Anandkumar A, 2012, ANN STAT, V40, P1346, DOI 10.1214/12-AOS1009; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Cover T.M, 2006, WILEY SERIES TELECOM; CROSS GR, 1983, IEEE T PATTERN ANAL, V5, P25, DOI 10.1109/TPAMI.1983.4767341; Dembo A, 2010, ANN APPL PROBAB, V20, P565, DOI 10.1214/09-AAP627; Fan Chung, 2006, COMPLEX GRAPHS NETWO; Gamal A.E., 2011, NETWORK INFORM THEOR; Goel A, 2013, SIAM J COMPUT, V42, P1392, DOI 10.1137/100812513; Hassner M., 1978, Proceedings of the 4th International Joint Conference on Pattern Recognition, P538; Ising E, 1925, Z PHYS, V31, P253, DOI 10.1007/BF02980577; Jukna S., 2001, EXTREMAL COMBINATORI, V2; Manning CD, 1999, FDN STAT NATURAL LAN; Raskutti G, 2011, IEEE T INFORM THEORY, V57, P6976, DOI 10.1109/TIT.2011.2165799; Ripley B, 1981, U PENN LAW REV, P252, DOI DOI 10.2307/3313062; Tandon R., 2013, IEEE INT S INF THEOR; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918; WOODS JW, 1978, IEEE T AUTOMAT CONTR, V23, P846, DOI 10.1109/TAC.1978.1101866; Yang YH, 1999, ANN STAT, V27, P1564; Zhang Y., 2013, NEURAL INFORM PROCES, P2328	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102049
C	Shen, J; Xu, H; Li, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Shen, Jie; Xu, Huan; Li, Ping			Online Optimization for Max-Norm Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ROBUST PCA; RANK	Max-norm regularizer has been extensively studied in the last decade as it promotes an effective low rank estimation of the underlying data. However, max-norm regularized problems are typically formulated and solved in a batch manner, which prevents it from processing big data due to possible memory bottleneck. In this paper, we propose an online algorithm for solving max-norm regularized problems that is scalable to large problems. Particularly, we consider the matrix decomposition problem as an example, although our analysis can also be applied in other problems such as matrix completion. The key technique in our algorithm is to reformulate the max-norm into a matrix factorization form, consisting of a basis component and a coefficients one. In this way, we can solve the optimal basis and coefficients alternatively. We prove that the basis produced by our algorithm converges to a stationary point asymptotically. Experiments demonstrate encouraging results for the effectiveness and robustness of our algorithm.	[Shen, Jie] Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA; [Xu, Huan] Natl Univ Singapore, Dept Mech Engn, Singapore 117575, Singapore; [Li, Ping] Rutgers State Univ, Dept Comp Sci, Dept Stat, Piscataway, NJ 08854 USA	Rutgers State University New Brunswick; National University of Singapore; Rutgers State University New Brunswick	Shen, J (corresponding author), Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA.	js2007@rutgers.edu; mpexuh@nus.edu.sg; pingli@stat.rutgers.edu			NSF [DMS-1444124, III-1360971, Bigdata-1419210]; ONR [N00014-13-1-0764]; AFOSR [FA9550-13-1-0137]; Ministry of Education of Singapore through AcRF Tier Two grant [R-265-000-443-112]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); Ministry of Education of Singapore through AcRF Tier Two grant(Ministry of Education, Singapore)	The research of Jie Shen and Ping Li is partially supported by NSF-DMS-1444124, NSF-III-1360971, NSF-Bigdata-1419210, ONR-N00014-13-1-0764, and AFOSR-FA9550-13-1-0137. Part of the work of Jie Shen was conducted at Shanghai Jiao Tong University. The work of Huan Xu is partially supported by the Ministry of Education of Singapore through AcRF Tier Two grant R-265-000-443-112.	Artac M, 2002, INT C PATT RECOG, P781, DOI 10.1109/ICPR.2002.1048133; Bonnans JF, 1998, SIAM REV, V40, P228, DOI 10.1137/S0036144596302644; Bottou Leon, 1998, ON LINE LEARNING NEU, V17; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Cai T, 2013, J MACH LEARN RES, V14, P3619; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Foygel Rina, 2012, P 26 ANN C NEUR INF, P944; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Jalali A., 2012, ICML; Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88; Mairal J, 2010, J MACH LEARN RES, V11, P19; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Rennie J. D., 2005, P 22 INT C MACHINE L, P713; Salakhutdinov R., 2010, TC X, V10, P2; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Srebro Nathan, 2004, ADV NEURAL INFORM PR, V17, P1329; Van der Vaart A.W, 2000, CAMBRIDGE SERIES STA, V3; Xu H, 2010, COLT, P490; Xu H, 2013, IEEE T INFORM THEORY, V59, P546, DOI 10.1109/TIT.2012.2212415; Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156; Zhang ZD, 2012, INT J COMPUT VISION, V99, P1, DOI 10.1007/s11263-012-0515-x	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101072
C	Si, S; Shin, D; Dhillon, IS; Parlett, BN		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Si, Si; Shin, Donghyuk; Dhillon, Inderjit S.; Parlett, Beresford N.			Multi-Scale Spectral Decomposition of Massive Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				LANCZOS; EIGENVECTORS	Computing the k dominant eigenvalues and eigenvectors of massive graphs is a key operation in numerous machine learning applications; however, popular solvers suffer from slow convergence, especially when k is reasonably large. In this paper, we propose and analyze a novel multi-scale spectral decomposition method (MSEIGS), which first clusters the graph into smaller clusters whose spectral decomposition can be computed efficiently and independently. We show theoretically as well as empirically that the union of all cluster's subspaces has significant overlap with the dominant subspace of the original graph, provided that the graph is clustered appropriately. Thus, eigenvectors of the clusters serve as good initializations to a block Lanczos algorithm that is used to compute spectral decomposition of the original graph. We further use hierarchical clustering to speed up the computation and adopt a fast early termination strategy to compute quality approximations. Our method outperforms widely used solvers in terms of convergence speed and approximation quality. Furthermore, our method is naturally parallelizable and exhibits significant speedups in shared-memory parallel settings. For example, on a graph with more than 82 million nodes and 3.6 billion edges, MSEIGS takes less than 3 hours on a single-core machine while Randomized SVD takes more than 6 hours, to obtain a similar approximation of the top-50 eigenvectors. Using 16 cores, we can reduce this time to less than 40 minutes.	[Si, Si; Shin, Donghyuk; Dhillon, Inderjit S.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA; [Parlett, Beresford N.] Univ Calif Berkeley, Dept Math, Berkeley, CA 94720 USA	University of Texas System; University of Texas Austin; University of California System; University of California Berkeley	Si, S (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	ssi@cs.utexas.edu; dshin@cs.utexas.edu; inderjit@cs.utexas.edu; parlett@math.berkeley.edu		Shin, Donghyuk/0000-0001-8687-0258	NSF [CCF-1320746, CCF-1117055]	NSF(National Science Foundation (NSF))	This research was supported by NSF grant CCF-1117055 and NSF grant CCF-1320746.	Baglama J, 2003, SIAM J SCI COMPUT, V24, P1650, DOI 10.1137/S1064827501397949; Cremonesi P., 2010, P 2010 ACM C RECOMME, P39, DOI [10.1145/1864708.1864721, DOI 10.1145/1864708.1864721]; CUPPEN JJM, 1981, NUMER MATH, V36, P177, DOI 10.1007/BF01396757; DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001; Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI [10.1109/TPAMI.2007.1115, 10.1109/TP'AMI.2007.1115]; GRIMES RG, 1994, SIAM J MATRIX ANAL A, V15, P228, DOI 10.1137/S0895479888151111; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Jain P., 2013, ABS13060626 CORR; Jamali M., 2010, P 4 ACM C RECOMMENDE, P135, DOI [10.1145/1864708.1864736, DOI 10.1145/1864708.1864736]; Karasuyama M., 2013, ADV NEURAL INFORM PR, P1547; Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997; Larsen R. M., 1998, PB357 DAIMI AARH U; LaSalle D., 2014, 14010 U MINN; Lehoucq R., 1998, ARPACK USERS GUIDE; Li RC, 1998, SIAM J MATRIX ANAL A, V20, P471, DOI 10.1137/S0895479896298506; Liu W., 2010, P 27 INT C MACH LEAR, P679; Meusel R, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P427, DOI 10.1145/2567948.2576928; Natarajan N, 2014, BIOINFORMATICS, V30, P60, DOI 10.1093/bioinformatics/btu269; Ng AY, 2002, ADV NEUR IN, V14, P849; Papailiopoulos DS, 2014, PR MACH LEARN RES, V32, P1890; Parlett B. N., 1998, SYMMETRIC EIGENVALUE, V20, DOI DOI 10.1137/1.9781611971163; Patro R, 2012, BIOINFORMATICS, V28, P3105, DOI 10.1093/bioinformatics/bts592; SAAD Y, 1980, SIAM J NUMER ANAL, V17, P687, DOI 10.1137/0717059; Shin D., 2012, P 21 ACM INT C INF K, P215; Vasuki V, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2036264.2036267; Wang B, 2013, IEEE I CONF COMP VIS, P425, DOI 10.1109/ICCV.2013.60; Whang JJ, 2012, IEEE DATA MINING, P705, DOI 10.1109/ICDM.2012.148; Yang JW, 2012, IEEE DATA MINING, P745, DOI 10.1109/ICDM.2012.138; Zhou DY, 2004, ADV NEUR IN, V16, P321	29	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102080
C	Silva, R; Evans, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Silva, Ricardo; Evans, Robin			Causal Inference through a Witness Protection Program	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					One of the most fundamental problems in causal inference is the estimation of a causal effect when variables are confounded. This is difficult in an observational study because one has no direct evidence that all confounders have been adjusted for. We introduce a novel approach for estimating causal effects that exploits observational conditional independencies to suggest "weak" paths in a unknown causal graph. The widely used faithfulness condition of Spirtes et al. is relaxed to allow for varying degrees of "path cancellations" that will imply conditional independencies but do not rule out the existence of confounding causal paths. The outcome is a posterior distribution over bounds on the average causal effect via a linear programming approach and Bayesian inference. We claim this approach should be used in regular practice to complement other default tools in observational studies.	[Silva, Ricardo] UCL, Dept Stat Sci, London, England; [Silva, Ricardo] UCL, CSML, London, England; [Evans, Robin] Univ Oxford, Dept Stat, Oxford, England	University of London; University College London; University of London; University College London; University of Oxford	Silva, R (corresponding author), UCL, Dept Stat Sci, London, England.	ricardo@stats.ucl.ac.uk; evans@stats.ox.ac.uk						[Anonymous], 2011, BAYESIAN STAT; Balke A, 1997, J AM STAT ASSOC, V92, P1171, DOI 10.2307/2965583; Buntine W., 1991, P 7 C UNC ART INT, P52, DOI DOI 10.1016/B978-1-55860-203-8.50010-3; Buntine W., 2004, P 20 C UNC ART INT, P59; Chen LS, 2007, GENOME BIOL, V8, DOI 10.1186/gb-2007-8-10-r219; Dawid A. P., 2003, HIGHLY STRUCTURED ST, P45; Entner D., 2013, P 16 INT C ART INT S, V31, P256; Evans R., 2012, P 22 WORKSH MACH LEA; Geiger P, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P240; Halpern, 2010, HEURISTICS PROBABILI, P415; Hirano K, 2000, Biostatistics, V1, P69, DOI 10.1093/biostatistics/1.1.69; Mani S., 2006, P UAI, P314; Manski C. F., 2007, IDENTIFICATION PREDI; MCDONALD CJ, 1992, M D COMPUT, V9, P304; Meek C., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P411; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Pearl J., 2009, R348 UCLA COGN SYST; Ramsahai RR, 2011, BIOMETRIKA, V98, P987, DOI 10.1093/biomet/asr040; Ramsahai RR, 2012, J MACH LEARN RES, V13, P829; Robins JM, 2003, BIOMETRIKA, V90, P491, DOI 10.1093/biomet/90.3.491; Rosenbaum P. R., 2002, OBSERVATIONAL STUDIE; Spirtes P., 2000, CAUSATION PREDICTION	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103053
C	Singh, S; Poczos, B		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Singh, Shashank; Poczos, Barnabas			Exponential Concentration of a Density Functional Estimator	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				INFORMATION	We analyze a plug-in estimator for a large class of integral functionals of one or more continuous probability densities. This class includes important families of entropy, divergence, mutual information, and their conditional versions. For densities on the d-dimensional unit cube [0,1](d) that lie in a beta-Holder smoothness class, we prove our estimator converges at the rate O (n(-beta/beta+d)). Furthermore, we prove the estimator is exponentially concentrated about its mean, whereas most previous related results have proven only expected error bounds on estimators.	[Singh, Shashank] Carnegie Mellon Univ, Stat Dept, Pittsburgh, PA 15213 USA; [Singh, Shashank; Poczos, Barnabas] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	Singh, S (corresponding author), Carnegie Mellon Univ, Stat Dept, Pittsburgh, PA 15213 USA.	sss1@andrew.cmu.edu; bapoczos@cs.cmu.edu	Singh, Shashank/V-8230-2019	Singh, Shashank/0000-0002-7305-673X				Aghagolzadeh M, 2007, IEEE IMAGE PROC, P277; Fukumizu K., 2008, NEURAL INFORM PROCES; Goria MN, 2005, J NONPARAMETR STAT, V17, P277, DOI 10.1080/104852504200026815; Hero AO, 2002, IEEE SIGNAL PROC MAG, V19, P85, DOI 10.1109/MSP.2002.1028355; Koller D., 2009, PROBABILISTIC GRAPHI; Krishnamurthy Akshay, 2014, INT C MACH LEARN; KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694; Leonenko N., 2008, TATRA MT MATH PUBLIC, V39; Leonenko N, 2008, ANN STAT, V36, P2153, DOI 10.1214/07-AOS539; Lewi J., 2007, ADV NEURAL INFORM PR, V19; LIU H, 2012, NEURAL INFORM PROCES; McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148, DOI DOI 10.1017/CBO9781107359949.008; Miller E G L, 2003, J MACHINE LEARNING R, V4, P1271; Montgomery D. C., 2005, DESIGN ANAL EXPT, V6th; Nguyen X., 2010, IEEE T INFORM THEORY; Oliva J., 2013, INT C MACH LEARN ICM; Pearl J., 1998, ESCHOLARSHIP; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159; Poczos B., 2012, 25 IEEE C COMP VIS P; Poczos B., 2012, JMLR WORKSHOP C P, V20; Reddi S. J., 2013, INT C MACH LEARN ICM; Renyi A., 1970, PROBABILITY THEORY; Shan Caifeng, 2005, BRIT MACH VIS C BMVC; Singh S., 2014, INT C MACH LEARN ICM; Sricharan K., 2013, ENSEMBLE ESTIMATORS; Su LJ, 2008, ECONOMET THEOR, V24, P829, DOI 10.1017/S0266466608080341; Szabo Z, 2007, J MACH LEARN RES, V8, P1063; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; Villmann T., 2010, MATH ASPECTS DIVERGE; Wang Q, 2009, IEEE T INFORM THEORY, V55, P2392, DOI 10.1109/TIT.2009.2016060; Wolsztynski E, 2005, SIGNAL PROCESS, V85, P937, DOI 10.1016/j.sigpro.2004.11.028; Zhang K., 2011, UNCERTAINTY ARTIFICI; [No title captured]	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102066
C	Soh, DW; Tatikonda, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Soh, De Wen; Tatikonda, Sekhar			Testing Unfaithful Gaussian Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				SELECTION	The global Markov property for Gaussian graphical models ensures graph separation implies conditional independence. Specifically if a node set S graph separates nodes u and v then X-u is conditionally independent of X-v given X-S. The opposite direction need not be true, that is, X-u perpendicular to X-v vertical bar X-S need not imply S is a node separator of u and v. When it does, the relation X-u perpendicular to X-v vertical bar X-S is called faithful. In this paper we provide a characterization of faithful relations and then provide an algorithm to test faithfulness based only on knowledge of other conditional relations of the form X-i perpendicular to X-j vertical bar X-S.	[Soh, De Wen; Tatikonda, Sekhar] Yale Univ, Dept Elect Engn, 17 Hillhouse Ave, New Haven, CT 06511 USA	Yale University	Soh, DW (corresponding author), Yale Univ, Dept Elect Engn, 17 Hillhouse Ave, New Haven, CT 06511 USA.	dewen.soh@yale.edu; sekhar.tatikonda@yale.edu	Soh, De Wen/AFR-7227-2022	Soh, De Wen/0000-0001-6490-6467				Anandkumar A, 2012, J MACH LEARN RES, V13, P2293; BECKER A., 2005, PROBABILITY MATH STA, V25, P231; FRYDENBERG M, 1990, ANN STAT, V18, P790, DOI 10.1214/aos/1176347626; Kauermann G, 1996, SCAND J STAT, V23, P105; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Lin S., PREPRINT; Malouche D., 2009, TECHNICAL REPORT; MEEK C, 1995, P 11 INT C UNC ART I; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631; SPIRITES P, 1993, CAUSATION PREDICTION; Uhler C, 2013, ANN STAT, V41, P436, DOI 10.1214/12-AOS1080; Whittaker J., 1990, GRAPHICAL MODELS APP; Wu R., 2013, STOCHASTIC SYSTEMS, V3	15	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102043
C	Song, HO; Lee, YJ; Jegelka, S; Darrell, T		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Song, Hyun Oh; Lee, Yong Jae; Jegelka, Stefanie; Darrell, Trevor			Weakly-supervised Discovery of Visual Pattern Configurations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				APPROXIMATIONS	The prominence of weakly labeled data gives rise to a growing demand for object detection methods that can cope with minimal supervision. We propose an approach that automatically identifies discriminative configurations of visual patterns that are characteristic of a given object class. We formulate the problem as a constrained submodular optimization problem and demonstrate the benefits of the discovered configurations in remedying mislocalizations and finding informative positive and negative training examples. Together, these lead to state-of-the-art weakly-supervised detection results on the challenging PASCAL VOC dataset.	[Song, Hyun Oh; Jegelka, Stefanie; Darrell, Trevor] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Lee, Yong Jae] Univ Calif Davis, Davis, CA 95616 USA	University of California System; University of California Berkeley; University of California System; University of California Davis	Song, HO (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.				DARPA's MSEE program; NSF [IIS-1427425, IIS-1212798, IIS-1116411]; Toyota; DARPA's SMISC program	DARPA's MSEE program; NSF(National Science Foundation (NSF)); Toyota; DARPA's SMISC program	This work was supported in part by DARPA's MSEE and SMISC programs, by NSF awards IIS-1427425, IIS-1212798, IIS-1116411, and by support from Toyota.	[Anonymous], 2012, ECCV; [Anonymous], 2014, ARXIV PREPRINT ARXIV; Barinova O., 2012, IEEE TPAMI; Chen Y., 2014, ICML; Darrell T., 2013, ARXIV E PRINTS; Deselaers T., 2010, ECCV; Deselaers T., 2012, IJCV; Doersch C., 2012, SIGGRAPH; Donahue J., 2013, ARXIV E PRINTS; Faktor A., 2012, ECCV; Farhadi A., 2011, CVPR; Felzenszwalb P., 2008, CVPR; Felzenszwalb Pedro F, 2010, TPAMI, V32; Fergus R., 2003, CVPR; FISHER ML, 1978, MATH PROGRAM STUD, V8, P73, DOI 10.1007/BFb0121195; GRAUMAN K, 2006, CVPR; Jenkyns T. A., 1976, P S E C COMB GRAPH T, P341; Juneja M., 2013, CVPR; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Lee Y. J., 2009, IJCV, V85; Li C., 2012, CVPR; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Pandey M., 2011, ICCV; Parikh D., 2008, CVPR; Quack T., 2007, ICCV; Singh S., 2012, ECCV; Siva P., 2011, ICCV; SIVIC J, 2004, CVPR; Sivic J., 2005, ICCV; Uijlings J., 2013, IJCV; Vizing V.G., 1964, DISKRETN ANAL, V3, P25, DOI DOI 10.2307/2371086; WEBER M, 2000, ECCV; Zhang Y., 2009, CVPR; Zou J., 2013, NIPS	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100063
C	Srivastava, N; Vul, E; Schrater, PR		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Srivastava, Nisheeth; Vul, Edward; Schrater, Paul R.			Magnitude-sensitive preference formation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				DECISION	Our understanding of the neural computations that underlie the ability of animals to choose among options has advanced through a synthesis of computational modeling, brain imaging and behavioral choice experiments. Yet, there remains a gulf between theories of preference learning and accounts of the real, economic choices that humans face in daily life, choices that are usually between some amount of money and an item. In this paper, we develop a theory of magnitude-sensitive preference learning that permits an agent to rationally infer its preferences for items compared with money options of different magnitudes. We show how this theory yields classical and anomalous supply-demand curves and predicts choices for a large panel of risky lotteries. Accurate replications of such phenomena without recourse to utility functions suggest that the theory proposed is both psychologically realistic and econometrically viable.	[Srivastava, Nisheeth; Vul, Edward] Univ San Diego, Dept Psychol, La Jolla, CA 92093 USA; [Schrater, Paul R.] Univ Minnesota, Dept Psychol, Minneapolis, MN 55455 USA	University of San Diego; University of Minnesota System; University of Minnesota Twin Cities	Srivastava, N (corresponding author), Univ San Diego, Dept Psychol, La Jolla, CA 92093 USA.	nisheeths@gmail.com; edwardvul@gmail.com; schrater@umn.edu			Institute for New Economic Thinking; NSF CPS [1239323]	Institute for New Economic Thinking; NSF CPS(National Science Foundation (NSF)NSF - Directorate for Engineering (ENG))	NS and PRS acknowledge funding from the Institute for New Economic Thinking. EV acknowledges funding from NSF CPS Grant #1239323.	Anderson J. R, 1990, ADAPTIVE CHARACTER T, DOI DOI 10.4324/9780203771730; Ariely D, 2009, PREDICTABLY IRRATION; Bordalo P, 2012, Q J ECON, V127, P1243, DOI 10.1093/qje/qjs018; Chen MK, 2006, J POLIT ECON, V114, P517, DOI 10.1086/503550; De Jaegher Kris, 2012, NEW SIGHTS THEORY GI, P53; Dragulescu A, 2000, EUR PHYS J B, V17, P723, DOI 10.1007/s100510070114; Elliott R, 2008, EUR J NEUROSCI, V27, P2213, DOI 10.1111/j.1460-9568.2008.06202.x; Gul F., 2010, FDN POSITIVE NORMATI, P3, DOI DOI 10.1093/ACPROF:OSO/9780195328318.003.0001; Jern A., 2011, ADV NEURAL INFORM PR, V24, P2276; KAHNEMAN D, 1979, ECONOMETRICA, V47, P263, DOI 10.2307/1914185; Kleiber C., 2003, STAT SIZE DISTRIBUTI, DOI 10.1002/0471457175; Paravisini Daniel, 2010, TECHNICAL REPORT; Plassmann H, 2008, P NATL ACAD SCI USA, V105, P1050, DOI 10.1073/pnas.0706929105; Rabin M, 1998, J ECON LIT, V36, P11; Srivastava N, 2012, P ADV NEUR INF PROC, V25; Stewart N, 2006, COGNITIVE PSYCHOL, V53, P1, DOI 10.1016/j.cogpsych.2005.10.003; Sun JZ, 2012, J MATH PSYCHOL, V56, P495, DOI 10.1016/j.jmp.2012.08.002; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tremblay L, 1999, NATURE, V398, P704, DOI 10.1038/19525; Vlaev I, 2011, TRENDS COGN SCI, V15, P546, DOI 10.1016/j.tics.2011.09.008	20	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100003
C	Subakan, YC; Traa, J; Smaragdis, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Subakan, Y. Cem; Traa, Johannes; Smaragdis, Paris			Spectral Learning of Mixture of Hidden Markov Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In this paper, we propose a learning approach for the Mixture of Hidden Markov Models (MHMM) based on the Method of Moments (MoM). Computational advantages of MoM make MHMM learning amenable for large data sets. It is not possible to directly learn an MHMM with existing learning approaches, mainly due to a permutation ambiguity in the estimation process. We show that it is possible to resolve this ambiguity using the spectral properties of a global transition matrix even in the presence of estimation noise. We demonstrate the validity of our approach on synthetic and real data.	[Subakan, Y. Cem; Smaragdis, Paris] Univ Illinois, Dept Comp Sci, Champaign, IL 61801 USA; [Traa, Johannes; Smaragdis, Paris] Univ Illinois, Dept Elect & Comp Engn, Champaign, IL USA; [Smaragdis, Paris] Adobe Syst Inc, San Jose, CA USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign; Adobe Systems Inc.	Subakan, YC (corresponding author), Univ Illinois, Dept Comp Sci, Champaign, IL 61801 USA.	subakan2@illinois.edu; traa2@illinois.edu; paris@illinois.edu			National Science Foundation [1319708]	National Science Foundation(National Science Foundation (NSF))	We would like to thank Taylan Cemgil, David Forsyth and John Hershey for valuable discussions. This material is based upon work supported by the National Science Foundation under Grant No. 1319708.	Anandkumar A., 2012, COLT; Anandkumar A., 2012, ARXIV12107559V2; Chaganty A., 2013, INT C MACH LEARN ICM; Hsu D., 2009, J COMPUTER SYSTEM SC; Jonathan A., 2003, CVPR; Lichman M, 2013, UCI MACHINE LEARNING; Oates T., 1999, P IJCAI 99 WORKSH NE, P17; QI YT, 2007, SYSTEMS MAN CYBERN A, V55, P5209, DOI DOI 10.1109/TSP.2007.898782; Smyth P., 1997, ADV NEURAL INFORM PR	9	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102079
C	Susemihl, A; Opper, M; Meir, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Susemihl, Alex; Opper, Manfred; Meir, Ron			Optimal Neural Codes for Control and Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				MUTUAL INFORMATION; MOTOR; TIME	Agents acting in the natural world aim at selecting appropriate actions based on noisy and partial sensory observations. Many behaviors leading to decision making and action selection in a closed loop setting are naturally phrased within a control theoretic framework. Within the framework of optimal Control Theory, one is usually given a cost function which is minimized by selecting a control law based on the observations. While in standard control settings the sensors are assumed fixed, biological systems often gain from the extra flexibility of optimizing the sensors themselves. However, this sensory adaptation is geared towards control rather than perception, as is often assumed. In this work we show that sensory adaptation for control differs from sensory adaptation for perception, even for simple control setups. This implies, consistently with recent experimental results, that when studying sensory adaptation, it is essential to account for the task being performed.	[Susemihl, Alex; Opper, Manfred] Tech Univ Berlin, Methods Artificial Intelligence, Berlin, Germany; [Susemihl, Alex] Google, Mountain View, CA USA; [Meir, Ron] Technion Haifa, Dept Elect Engn, Haifa, Israel	Technical University of Berlin; Google Incorporated; Technion Israel Institute of Technology	Susemihl, A (corresponding author), Tech Univ Berlin, Methods Artificial Intelligence, Berlin, Germany.							Andrievsky BR, 2010, AUTOMAT REM CONTR+, V71, P572, DOI 10.1134/S000511791004003X; Astrom Karl J., 2006, INTR STOCHASTIC CONT; Attwell D, 2001, J CEREBR BLOOD F MET, V21, P1133, DOI 10.1097/00004647-200110000-00001; Bar-Shalom Yaakov, 1974, IEEE T AUTOMATIC CON; Battaglia PW, 2007, J NEUROSCI, V27, P6984, DOI 10.1523/JNEUROSCI.1309-07.2007; BUCY RS, 1965, IEEE T AUTOMAT CONTR, VAC10, P198, DOI 10.1109/TAC.1965.1098109; Guo DN, 2005, IEEE T INFORM THEORY, V51, P1261, DOI 10.1109/TIT.2005.844072; Huber D, 2012, NATURE, V484, P473, DOI 10.1038/nature11039; Izawa J, 2008, J NEUROSCI, V28, P11360, DOI 10.1523/JNEUROSCI.3063-08.2008; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; KANAYA F, 1991, IEEE T INFORM THEORY, V37, P1151, DOI 10.1109/18.87006; Mattar AAG, 2013, J NEUROPHYSIOL, V109, P782, DOI 10.1152/jn.00734.2011; Merhav N, 2011, IEEE T INFORM THEORY, V57, P3887, DOI 10.1109/TIT.2011.2132590; Susemihl A, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03009; Todorov E, 2002, NAT NEUROSCI, V5, P1226, DOI 10.1038/nn963; Todorov E, 2008, IEEE DECIS CONTR P, P4286, DOI 10.1109/CDC.2008.4739438; TSE E, 1973, IEEE T AUTOMAT CONTR, VAC18, P109, DOI 10.1109/TAC.1973.1100242; Vahdat S, 2011, J NEUROSCI, V31, P16907, DOI 10.1523/JNEUROSCI.2737-11.2011; WITSENHAUSEN HS, 1968, SIAM J CONTROL, V6, P131, DOI 10.1137/0306011; Yaeli Steve, 2010, FRONTIERS COMPUTATIO, V4, P16	20	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101061
C	Tekin, C; van der Schaar, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Tekin, Cem; van der Schaar, Mihaela			Discovering, Learning and Exploiting Relevance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In this paper we consider the problem of learning online what is the information to consider when making sequential decisions. We formalize this as a contextual multi-armed bandit problem where a high dimensional (D-dimensional) context vector arrives to a learner which needs to select an action to maximize its expected reward at each time step. Each dimension of the context vector is called a type. We assume that there exists an unknown relation between actions and types, called the relevance relation, such that the reward of an action only depends on the contexts of the relevant types. When the relation is a function, i.e., the reward of an action only depends on the context of a single type, and the expected reward of an action is Lipschitz continuous in the context of its relevant type, we propose an algorithm that achieves (O) over tilde (T-gamma) regret with a high probability, where gamma = 2/(1 + root 2). Our algorithm achieves this by learning the unknown relevance relation, whereas prior contextual bandit algorithms that do not exploit the existence of a relevance relation will have (O) over tilde (T(D+1)/(D+2)) regret. Our algorithm alternates between exploring and exploiting, it does not require reward observations in exploitations, and it guarantees with a high probability that actions with suboptimality greater than is an element of are never selected in exploitations. Our proposed method can be applied to a variety of learning applications including medical diagnosis, recommender systems, popularity prediction from social networks, network security etc., where at each instance of time vast amounts of different types of information are available to the decision maker, but the effect of an action depends only on a single type.	[Tekin, Cem; van der Schaar, Mihaela] Univ Calif Los Angeles, Elect Engn Dept, Los Angeles, CA 90024 USA	University of California System; University of California Los Angeles	Tekin, C (corresponding author), Univ Calif Los Angeles, Elect Engn Dept, Los Angeles, CA 90024 USA.	cmtkn@ucla.edu; mihaela@ee.ucla.edu						Abernethy J., 2013, P ICML, P588; Agarwal A., 2014, ARXIV14020555; Amin K., 2011, C UNC ART INT UAI; [Anonymous], 2007, P ADV NEURAL INF PRO; Cesa-Bianchi N., 2009, P 26 ANN INT C MACHI, P121; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Dudik M., 2011, ARXIV11062369; Hazan E., 2011, NIPS, V11, P891; Hazan E, 2007, LECT NOTES COMPUT SC, V4539, P499, DOI 10.1007/978-3-540-72927-3_36; Kakade S. M., 2008, ICML 2008, P440; Li LH, 2011, MACH LEARN, V82, P399, DOI 10.1007/s10994-010-5225-4; LU T., 2010, PROC 30 INT C ARTIF, P485; Slivkins A., 2011, C LEARN THEOR COLT; Tyagi H, 2014, LECT NOTES COMPUT SC, V8447, P108, DOI 10.1007/978-3-319-08001-7_10	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101051
C	Tristan, JB; Huang, Dg; Tassarotti, J; Pocock, LA; Green, SSJ; Steele, GL		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Tristan, Jean-Baptiste; Huang, Daniel; Tassarotti, Joseph; Pocock, Adam L.; Green, Stephen J. S.; Steele, Guy L., Jr.			Augur: Data-Parallel Probabilistic Modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Implementing inference procedures for each new probabilistic model is time-consuming and error-prone. Probabilistic programming addresses this problem by allowing a user to specify the model and then automatically generating the inference procedure. To make this practical it is important to generate high performance inference code. In turn, on modern architectures, high performance requires parallel execution. In this paper we present Augur, a probabilistic modeling language and compiler for Bayesian networks designed to make effective use of data-parallel architectures such as GPUs. We show that the compiler can generate data-parallel inference code scalable to thousands of GPU cores by making use of the conditional independence relationships in the Bayesian network.	[Tristan, Jean-Baptiste; Pocock, Adam L.; Green, Stephen J. S.; Steele, Guy L., Jr.] Oracle Labs, Belmont, CA 94002 USA; [Huang, Daniel] Harvard Univ, Cambridge, MA 02138 USA; [Tassarotti, Joseph] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Oracle; Harvard University; Carnegie Mellon University	Tristan, JB (corresponding author), Oracle Labs, Belmont, CA 94002 USA.	jean.baptiste.tristan@oracle.com; dehuang@fas.harvard.edu; jtassaro@cs.cmu.edu; adam.pocock@oracle.com; stephan.x.green@oracle.com; guy.steele@oracle.com						Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Blelloch GE, 1996, COMMUN ACM, V39, P85, DOI 10.1145/227234.227246; Goodman N. D., 2008, P 24 C UNCERTAINTY A, P220; Goodman ND, 2013, ACM SIGPLAN NOTICES, V48, P399, DOI 10.1145/2480359.2429117; Griffiths T. L., 2004, P NATL ACAD SCI, V101; Hershey S., 2012, ABS12122991 CPRR; HILLIS WD, 1986, COMMUN ACM, V29, P1170, DOI 10.1145/7902.7903; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Lichman M, 2013, UCI MACHINE LEARNING; LUNN D, 2009, STAT MED; Mansinghka V. K., 2014, ABS14040099 CORR; Marsaglia G, 2000, ACM T MATH SOFTWARE, V26, P363, DOI 10.1145/358407.358414; McCallum Andrew, 2009, ADV NEURAL INFORM PR, P1249; Minka T., 2012, INFER NET 2 5; Neal R., 2013, LECTURE, V3; Pfeffer A., 2009, TECHNICAL REPORT; Plummer M., 2003, P 3 INT WORKSH DISTR, V124, P1; Ragan-Kelley J, 2013, ACM SIGPLAN NOTICES, V48, P519, DOI 10.1145/2499370.2462176; Smola A, 2010, PROC VLDB ENDOW, V3, P703, DOI 10.14778/1920841.1920931; Stan Dev. Team, 2014, STAN MOD LANGUS GUDD; Thomas A, 1992, BAYESIAN STATISTICS, P837; Venugopal D., 2013, 29 C UNC ART INT	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102092
C	Tung, HYF; Smola, AJ		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Tung, Hsiao-Yu Fish; Smola, Alexander J.			Spectral Methods for Indian Buffet Process Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The Indian Buffet Process is a versatile statistical tool for modeling distributions over binary matrices. We provide an efficient spectral algorithm as an alternative to costly Variational Bayes and sampling-based algorithms. We derive a novel tensorial characterization of the moments of the Indian Buffet Process proper and for two of its applications. We give a computationally efficient iterative inference algorithm, concentration of measure bounds, and reconstruction guarantees. Our algorithm provides superior accuracy and cheaper computation than comparable Variational Bayesian approach on a number of reference problems.	[Tung, Hsiao-Yu Fish; Smola, Alexander J.] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Smola, Alexander J.] Google, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Google Incorporated	Tung, HYF (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.							Altun Y, 2006, LECT NOTES ARTIF INT, V4005, P139, DOI 10.1007/11776420_13; Aly M., 2012, P 21 INT C COMP WORL, P3, DOI DOI 10.1145/2187980.2187982; Anandkumar A., 2012, ARXIV12107559; Anandkumar A., 2012, ABS12046703 CORR; Anandkumar A., 2011, NEURAL INFORM PROCES; Anandkumar Anima, 2013, P ANN C COMP LEARN T; Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Boots Byron, 2013, C UNC ART INT; Cardoso JF, 1998, P IEEE, V86, P2009, DOI 10.1109/5.720250; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Fox E. B., 2010, NIPS, V22; Ghahramani Z, 2010, ADV NEURAL INF PROCE, V2010, P19; Gretton A, 2012, J MACH LEARN RES, V13, P723; Griffiths T., 2011, J MACHINE LEARNING R, V12; Griffiths T.L., 2006, ADV NEURAL INFORM PR, P475; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Halko N., 2009, ARXIV09094061; Hsu D., 2009, P ANN C COMP LEARN T; Hsu D., 2012, ELECTRON COMMUN PROB, V17, P13; Hsu D., 2012, LEARNING MIXTURES SP; Knowles D., 2007, INT C IND COMP AN SI; McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148, DOI DOI 10.1017/CBO9781107359949.008; Miller K. T., 2009, SNOWBIRD, P2; Pearson K., 1894, Philosophical Transactions, V185a, P71, DOI 10.1098/rsta.1894.0003; Pisier G., 1989, CAMBRIDGE TRACTS MAT, V94; Song L., 2010, INT C MACH LEARN; Wood F., 2006, UAI	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102077
C	Turner, R; Bottone, S; Avasarala, B		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Turner, Ryan; Bottone, Steven; Avasarala, Bhargav			A Complete Variational Tracker	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				INFERENCE	We introduce a novel probabilistic tracking algorithm that incorporates combinatorial data association constraints and model-based track management using variational Bayes. We use a Bethe entropy approximation to incorporate data association constraints that are often ignored in previous probabilistic tracking algorithms. Noteworthy aspects of our method include a model-based mechanism to replace heuristic logic typically used to initiate and destroy tracks, and an assignment posterior with linear computation cost in window length as opposed to the exponential scaling of previous MAP-based approaches. We demonstrate the applicability of our method on radar tracking and computer vision problems.	[Turner, Ryan; Bottone, Steven; Avasarala, Bhargav] Northrop Grumman Corp, Falls Church, VA 22042 USA	Northrop Grumman Corporation	Turner, R (corresponding author), Northrop Grumman Corp, Falls Church, VA 22042 USA.	ryan.turner@ngc.com; steven.bottone@ngc.com; bhargay.avasarala@ngc.com						Bar-Shalom Y., 2011, TRACKING DATA FUSION; Beal MJ, 2003, BAYESIAN STATISTICS 7, P453; Benfold B., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3457, DOI 10.1109/CVPR.2011.5995667; Bishop C. M., 2006, J ELECT IMAG, V16, P140; Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104; Bromiley P. A., 2013, 2003003 U MANCH; Byrd E., 2003, 2003029 DTIC; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Davey S. J., 2003, THESIS; Eaton F, 2013, NEURAL COMPUT, V25, P1213, DOI 10.1162/NECO_a_00441; Hartikainen Jouni, 2010, Proceedings of the 2010 IEEE International Workshop on Machine Learning for Signal Processing (MLSP), P379, DOI 10.1109/MLSP.2010.5589113; Heskes T., 2003, ADV NEURAL INFORM PR, V15, P359; HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075; Jensfelt P, 2001, IEEE T ROBOTIC AUTOM, V17, P748, DOI 10.1109/70.964673; JONKER R, 1987, COMPUTING, V38, P325, DOI 10.1007/BF02278710; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Lau R. A., 2011, Proceedings of the 2011 Seventh International Conference on Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP), P437, DOI 10.1109/ISSNIP.2011.6146551; Lazaro-Gredilla M, 2012, PATTERN RECOGN, V45, P1386, DOI 10.1016/j.patcog.2011.10.004; Mahler RPS, 2003, IEEE T AERO ELEC SYS, V39, P1152, DOI 10.1109/TAES.2003.1261119; POORE AB, 1993, P SOC PHOTO-OPT INS, V1955, P172, DOI 10.1117/12.154972; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; RAUCH HE, 1965, AIAA J, V3, P1445, DOI 10.2514/3.3166; Scholkopf B., 2001, LEARNING KERNELS SUP; Valiant L. G., 1979, Theoretical Computer Science, V8, P189, DOI 10.1016/0304-3975(79)90044-6; Watanabe Y, 2010, J PHYS A-MATH THEOR, V43, DOI 10.1088/1751-8113/43/24/242002; Yedidia J. S., 2001, TR200116 MITS EL RES	26	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103023
C	Tyagi, H; Krause, A; Gartner, B		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Tyagi, Hemant; Krause, Andreas; Gartner, Bernd			Efficient Sampling for Learning Sparse Additive Models in High Dimensions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				RECOVERY	We consider the problem of learning sparse additive models, i.e., functions of the form: f(x) = Sigma(l is an element of S) phi(l) (x(l)), x is an element of R-d from point queries of f. Here S is an unknown subset of coordinate variables with vertical bar S vertical bar = k << d. Assuming phi(l) 's to be smooth, we propose a set of points at which to sample f and an efficient randomized algorithm that recovers a uniform approximation to each unknown phi(l). We provide a rigorous theoretical analysis of our scheme along with sample complexity bounds. Our algorithm utilizes recent results from compressive sensing theory along with a novel convex quadratic program for recovering robust uniform approximations to univariate functions, from point queries corrupted with arbitrary bounded noise. Lastly we theoretically analyze the impact of noise - either arbitrary but bounded, or stochastic - on the performance of our algorithm.	[Tyagi, Hemant; Krause, Andreas; Gartner, Bernd] Swiss Fed Inst Technol, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Tyagi, H (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	htyagi@inf.ethz.ch; krausea@ethz.ch; gaertner@inf.ethz.ch		Krause, Andreas/0000-0001-7260-9673	SNSF [200021_137528]; Microsoft Research Faculty Fellowship	SNSF(Swiss National Science Foundation (SNSF)); Microsoft Research Faculty Fellowship(Microsoft)	This research was supported in part by SNSF grant 200021_137528 and a Microsoft Research Faculty Fellowship.	Ahlberg J.H., 1967, THEORY SPLINES THEIR; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Cohen A., 2011, CONSTR APPR, P1; Craven P., 1979, Numerische Mathematik, V31, P377, DOI 10.1007/BF01404567; De Boor C., 1978, PRACTICAL GUIDE SPLI, V27; DeVore R, 2011, CONSTR APPROX, V33, P125, DOI 10.1007/s00365-010-9105-8; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Fornasier M, 2012, FOUND COMPUT MATH, V12, P229, DOI 10.1007/s10208-012-9115-y; HALL CA, 1976, J APPROX THEORY, V16, P105, DOI 10.1016/0021-9045(76)90040-X; Koltchinskii V., 2008, 21 ANN C LEARN THEOR, P229; Koltchinskii V, 2010, ANN STAT, V38, P3660, DOI 10.1214/10-AOS825; Lin Y, 2006, ANN STAT, V34, P2272, DOI 10.1214/009053606000000722; Maathuis MH, 2009, ANN STAT, V37, P3133, DOI 10.1214/09-AOS685; Meier L, 2009, ANN STAT, V37, P3779, DOI 10.1214/09-AOS692; Muller-Gronbach T, 2008, MONTE CARLO AND QUASI-MONTE CARLO METHODS 2006, P53, DOI 10.1007/978-3-540-74496-2_4; Raskutti G, 2012, J MACH LEARN RES, V13, P389; Ravikumar P, 2009, J R STAT SOC B, V71, P1009, DOI 10.1111/j.1467-9868.2009.00718.x; REINSCH CH, 1967, NUMER MATH, V10, P177, DOI 10.1007/BF02162161; SCHOENBERG IJ, 1964, P NATL ACAD SCI USA, V52, P947, DOI 10.1073/pnas.52.4.947; Tyagi H., 2012, ADV NEURAL INFORM PR, V25, P1475; WAHBA G, 1975, NUMER MATH, V24, P383, DOI 10.1007/BF01437407; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P5728, DOI 10.1109/TIT.2009.2032816; Wojtaszczyk P, 2012, SIAM J NUMER ANAL, V50, P458, DOI 10.1137/110833130; Yuan M., 2007, P 11 INT C ART INT S, V2, P660	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102027
C	Ulrich, K; Carlson, DE; Lian, WZ; Borg, JS; Dzirasa, K; Carin, L		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Ulrich, Kyle; Carlson, David E.; Lian, Wenzhao; Borg, Jana Schaich; Dzirasa, Kafui; Carin, Lawrence			Analysis of Brain States from Multi-Region LFP Time-Series	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The local field potential (LFP) is a source of information about the broad patterns of brain activity, and the frequencies present in these time-series measurements are often highly correlated between regions. It is believed that these regions may jointly constitute a "brain state," relating to cognition and behavior. An infinite hidden Markov model (iHMM) is proposed to model the evolution of brain states, based on electrophysiological LFP data measured at multiple brain regions. A brain state influences the spectral content of each region in the measured LFP. A new state-dependent tensor factorization is employed across brain regions, and the spectral properties of the LFPs are characterized in terms of Gaussian processes (GPs). The LFPs are modeled as a mixture of GPs, with state- and region-dependent mixture weights, and with the spectral content of the data encoded in GP spectral mixture covariance kernels. The model is able to estimate the number of brain states and the number of mixture components in the mixture of GPs. A new variational Bayesian split-merge algorithm is employed for inference. The model infers state changes as a function of external covariates in two novel electrophysiological datasets, using LFP data recorded simultaneously from multiple brain regions in mice; the results are validated and interpreted by subject-matter experts.	[Ulrich, Kyle; Carlson, David E.; Lian, Wenzhao; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA; [Borg, Jana Schaich; Dzirasa, Kafui] Duke Univ, Dept Psychiat & Behav Sci, Durham, NC 27708 USA	Duke University; Duke University	Ulrich, K (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.	kyle.ulrich@duke.edu; david.carlson@duke.edu; wenzhao.lian@duke.edu; jana.borg@duke.edu; kafui.dzirasa@duke.edu; lcarin@duke.edu	Dzirasa, Kafui/GQB-1424-2022	Carlson, David/0000-0003-1005-6385	ARO; ONR; DARPA; DOE; NGA	ARO; ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NGA	The research reported here was funded in part by ARO, DARPA, DOE, NGA and ONR.	[Anonymous], 2003, THESIS U LONDON; Beal M.J., 2002, NIPS; Blei D M, 2004, BAYESIAN ANAL; Bryant M, 2012, NIPS, P1; Dzirasa K, 2006, J NEUROSCI, V26, P10577, DOI 10.1523/JNEUROSCI.1767-06.2006; Dzirasa K, 2011, J NEUROSCI METH, V195, P36, DOI 10.1016/j.jneumeth.2010.11.014; Friston KJ, 2003, NEUROIMAGE, V19, P1273, DOI 10.1016/S1053-8119(03)00202-7; Gervasoni D, 2004, J NEUROSCI, V24, P11137, DOI 10.1523/JNEUROSCI.3524-04.2004; Gilbert CD, 2007, NEURON, V54, P677, DOI 10.1016/j.neuron.2007.05.019; GWilson A, 2013, ICML; Harshman R A, 1970, WORK PAP PHONETICS; Jain S, 2007, BAYESIAN ANAL; Jordan M. I., 2005, NIPS; Kohn A, 2009, CURR OPIN NEUROBIOL, V19, P434, DOI 10.1016/j.conb.2009.06.007; Kurihara K., 2007, NIPS; Lang PJ, 2010, BIOL PSYCHOL, V84, P437, DOI 10.1016/j.biopsycho.2009.10.007; Liang P., 2007, P 2007 JOINT C EMP M, P688; Murray J, 2013, AISTATS; Paisley J, 2009, IEEE T SIGNAL PROCES, V57, P3905, DOI 10.1109/TSP.2009.2024987; Pfaff D, 2008, ANYAS            JAN; Rai P., 2014, ICML; Rasmussen C. E., 2006, GAUSSIAN PROCESSES M, V14; Rolls A, 2011, P NATL ACAD SCI USA, V108, P13305, DOI 10.1073/pnas.1015633108; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Teh Y. W., 2007, NIPS; Tucker MA, 2006, NEUROBIOL LEARN MEM, V86, P241, DOI 10.1016/j.nlm.2006.03.005	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103075
C	Vanchinathan, HP; Bartok, G; Krause, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Vanchinathan, Hastagiri P.; Bartok, Gabor; Krause, Andreas			Efficient Partial Monitoring with Prior Information	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Partial monitoring is a general model for online learning with limited feedback: a learner chooses actions in a sequential manner while an opponent chooses outcomes. In every round, the learner suffers some loss and receives some feedback based on the action and the outcome. The goal of the learner is to minimize her cumulative loss. Applications range from dynamic pricing to label-efficient prediction to dueling bandits. In this paper, we assume that we are given some prior information about the distribution based on which the opponent generates the outcomes. We propose BPM, a family of new efficient algorithms whose core is to track the outcome distribution with an ellipsoid centered around the estimated distribution. We show that our algorithm provably enjoys near-optimal regret rate for locally observable partial-monitoring problems against stochastic opponents. As demonstrated with experiments on synthetic as well as real-world data, the algorithm outperforms previous approaches, even for very uninformed priors, with an order of magnitude smaller regret and lower running time.	[Vanchinathan, Hastagiri P.; Bartok, Gabor; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	ETH Zurich	Vanchinathan, HP (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	hastagiri@inf.ethz.ch; bartok@inf.ethz.ch; krausea@ethz.ch			SNSF [200021_137971, ERC StG 307036]; Microsoft Research Faculty Fellowship	SNSF(Swiss National Science Foundation (SNSF)); Microsoft Research Faculty Fellowship(Microsoft)	This research was supported in part by SNSF grant 200021_137971, ERC StG 307036 and a Microsoft Research Faculty Fellowship.	Ailon Nir, 2014, ARXIV14053396; Audibert Jean-Yves, 2009, COLT; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Bartok G., 2013, COLT, P696; Bartok Gabor, 2012, P 29 INT C MACH LEAR; Bartok Gabor, 2011, P INT C COMP LEARN T, P133; Cesa-Bianchi N, 2006, MATH OPER RES, V31, P562, DOI 10.1287/moor.1060.0206; Piccolboni A, 2001, LECT NOTES ARTIF INT, V2111, P208; Singla Adish, 2013, INT WORLD WID WEB C; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; Vovk V. G., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory, P371; Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028	13	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101041
C	Vandermeulen, RA; Scott, CD		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Vandermeulen, Robert A.; Scott, Clayton D.			Robust Kernel Density Estimation by Scaling and Projection in Hilbert Space	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ALGORITHM	While robust parameter estimation has been well studied in parametric density estimation, there has been little investigation into robust density estimation in the nonparametric setting. We present a robust version of the popular kernel density estimator (KDE). As with other estimators, a robust version of the KDE is useful since sample contamination is a common issue with datasets. What "robustness" means for a nonparametric density estimate is not straightforward and is a topic we explore in this paper. To construct a robust KDE we scale the traditional KDE and project it to its nearest weighted KDE in the L-2 norm. This yields a scaled and projected KDE (SPKDE). Because the squared L-2 norm penalizes point-wise errors superlinearly this causes the weighted KDE to allocate more weight to high density regions. We demonstrate the robustness of the SPKDE with numerical experiments and a consistency result which shows that asymptotically the SPKDE recovers the uncontaminated density under sufficient conditions on the contamination.	[Vandermeulen, Robert A.; Scott, Clayton D.] Univ Michigan, Dept EECS, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Vandermeulen, RA (corresponding author), Univ Michigan, Dept EECS, Ann Arbor, MI 48109 USA.	rvdm@umich.edu; clayscot@umich.edu			NSF [0953135, 1047871, 1217880, 1422157]	NSF(National Science Foundation (NSF))	This work support in part by NSF Awards 0953135, 1047871, 1217880, 1422157. We would also like to thank Samuel Brodkey for his assistance with the simulation code.	Bartlett P. L., 2011, ADV NEURAL INFORM PR, P478; Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; BERRY DA, 1996, BAYESIAN ANAL STAT E; BRUCKER P, 1984, OPER RES LETT, V3, P163, DOI 10.1016/0167-6377(84)90010-5; Duchi J., 2008, PROC 25 INT C MACH L, P272; El-Yaniv R., 2007, ADV NEURAL INFORM P, V19; Kim J, 2012, J MACH LEARN RES, V13, P2529; Lanckriet G. R., 2003, ADV NEURAL INFORM PR, P905; PARDALOS PM, 1990, MATH PROGRAM, V46, P321, DOI 10.1007/BF01585748; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; Silverman B.W., 1986, DENSITY ESTIMATION S, V26; Steinwart I, 2005, J MACH LEARN RES, V6, P211; TERRELL GR, 1992, ANN STAT, V20, P1236, DOI 10.1214/aos/1176348768; Theiler J, 2003, P SOC PHOTO-OPT INS, V5093, P230, DOI 10.1117/12.487069; Vandermeulen R., 2013, COLT, V30; Vert R, 2006, J MACH LEARN RES, V7, P817; WILCOXON F, 1946, J ECON ENTOMOL, V39, P269, DOI 10.1093/jee/39.2.269	17	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100007
C	Vega-Brown, W; Doniec, M; Roy, N		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Vega-Brown, William; Doniec, Marek; Roy, Nicholas			Nonparametric Bayesian inference on multivariate exponential families	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				REGRESSION	We develop a model by choosing the maximum entropy distribution from the set of models satisfying certain smoothness and independence criteria; we show that inference on this model generalizes local kernel estimation to the context of Bayesian inference on stochastic processes. Our model enables Bayesian inference in contexts when standard techniques like Gaussian process inference are too expensive to apply. Exact inference on our model is possible for any likelihood function from the exponential family. Inference is then highly efficient, requiring only O (log N) time and O (N) space at run time. We demonstrate our algorithm on several problems and show quantifiable improvement in both speed and performance relative to models based on the Gaussian process.	[Vega-Brown, William; Doniec, Marek; Roy, Nicholas] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Vega-Brown, W (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	wrvb@csail.mit.edu; donie@csail.mit.edu; nickroy@csail.mit.edu			Office of Naval Research [N00014-09-1-1052, N00014-10-1-0936]	Office of Naval Research(Office of Naval Research)	This research was funded by the Office of Naval Research under contracts N00014-09-1-1052 and N00014-10-1-0936. The support of Behzad Kamgar-Parsi and Tom McKenna is gratefully acknowledged.	CLEVELAND WS, 1979, J AM STAT ASSOC, V74, P829, DOI 10.2307/2286407; Duvenaud D.K., 2011, ADV NEURAL INFORM PR, P226; Gray A.G., 2000, ADV NEURAL INF PROCE, V4, P521; Gyorfi L., 2002, DISTRIBUTION FREE TH; Kersting K., 2007, P 24 INT C MACH LEAR, P393, DOI DOI 10.1145/1273496.1273546; Lazaro-Gredilla M., 2011, P ICML; Le Q.V., 2005, P 22 INT C MACHINE L, P489, DOI [10.1145/1102351.1102413, DOI 10.1145/1102351.1102413]; Murphy K.P., 2007, CONJUGATE BAYESIAN A; Nadaraya E.A., 1964, THEOR PROBAB APPL+, V9, P141, DOI [DOI 10.1137/1109020, 10.1137/1109020]; Quadrianto N., 2009, P ICDM MIAM FL US DE; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rasmussen C. E., 2006, GAUSSIAN PROCESSES M, V14; Shang Lifeng, 2013, ARXIV13116371; SILVERMAN BW, 1985, J R STAT SOC B, V47, P1; Snelson E.L., 2007, FLEXIBLE EFFICIENT G; TIBSHIRANI R, 1987, J AM STAT ASSOC, V82, P559, DOI 10.2307/2289465; Vega-Brown W., 2013, P IROS TOK JAP; Vega-Brown W., 2013, THESIS; Wang S. X., 2001, THESIS; Watson G.S., 1964, SANKHYA SER A, V26, P359, DOI DOI 10.2307/25049340; Wilson A. G., 2011, P 27 C UNC ART INT U, P736; Yang CJ, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P464; Yuan M, 2004, STAT PROBABIL LETT, V69, P11, DOI 10.1016/j.spl.2004.03.009	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101042
C	Venugopal, D; Gogate, V		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Venugopal, Deepak; Gogate, Vibhav			Scaling-up Importance Sampling for Markov Logic Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				INFERENCE	Markov Logic Networks (MLNs) are weighted first-order logic templates for generating large (ground) Markov networks. Lifted inference algorithms for them bring the power of logical inference to probabilistic inference. These algorithms operate as much as possible at the compact first-order level, grounding or propositionalizing the MLN only as necessary. As a result, lifted inference algorithms can be much more scalable than propositional algorithms that operate directly on the much larger ground network. Unfortunately, existing lifted inference algorithms suffer from two interrelated problems, which severely affects their scalability in practice. First, for most real-world MLNs having complex structure, they are unable to exploit symmetries and end up grounding most atoms (the grounding problem). Second, they suffer from the evidence problem, which arises because evidence breaks symmetries, severely diminishing the power of lifted inference. In this paper, we address both problems by presenting a scalable, lifted importance sampling-based approach that never grounds the full MLN. Specifically, we show how to scale up the two main steps in importance sampling: sampling from the proposal distribution and weight computation. Scalable sampling is achieved by using an informed, easy-to-sample proposal distribution derived from a compressed MLN-representation. Fast weight computation is achieved by only visiting a small subset of the sampled groundings of each formula instead of all of its possible groundings. We show that our new algorithm yields an asymptotically unbiased estimate. Our experiments on several MLNs clearly demonstrate the promise of our approach.	[Venugopal, Deepak; Gogate, Vibhav] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75083 USA	University of Texas System; University of Texas Dallas	Venugopal, D (corresponding author), Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75083 USA.	dxv021000@utdallas.edu; vgogate@hlt.utdallas.edu			AFRL [FA8750-14-C-0021]; ARO MURI [W911NF-08-1-0242]; DARPA Probabilistic Programming for Advanced Machine Learning Program under AFRL prime [FA8750-14-C-0005]	AFRL(United States Department of DefenseUS Air Force Research Laboratory); ARO MURI(MURI); DARPA Probabilistic Programming for Advanced Machine Learning Program under AFRL prime	This work was supported in part by the AFRL under contract number FA8750-14-C-0021, by the ARO MURI grant W911NF-08-1-0242, and by the DARPA Probabilistic Programming for Advanced Machine Learning Program under AFRL prime contract number FA8750-14-C-0005. Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views or official policies, either expressed or implied, of DARPA, AFRL, ARO or the US government.	Ahmadi B, 2013, MACH LEARN, V92, P91, DOI 10.1007/s10994-013-5385-0; Bui H. B, 2012, AAAI; Darwiche A., 2013, ADV NEURAL INFORM PR, V26, P2868; de Salvo Braz R., 2007, THESIS; den Broeck G.V., 2011, IJCAI 2011 P 22 INT, P2178, DOI [10.5591/978-1-57735-516-8/IJCAI11-363, DOI 10.5591/978-1-57735-516-8/IJCAI11-363]; Domingos P., 2009, MARKOV LOGIC INTERFA; Domingos P., 2011, UAI, P256; GEWEKE J, 1989, ECONOMETRICA, V57, P1317, DOI 10.2307/1913710; Gogate V., 2012, P 26 ANN C NEUR INF, P1664; Gogate V, 2012, P 26 AAAI C ART INT; Jha A., 2010, P 24 ANN C NEUR INF, P973; Kok S., 2008, TECHNICAL REPORT; Liu JS., 2001, MONTE CARLO STRATEGI, DOI DOI 10.1007/978-0-387-76371-2; Poole D., 2003, P INT JOINT C ART IN, P985; Sarkhel S, 2014, JMLR WORKSH CONF PRO, V33, P859; VandenBroeck G, 2011, ADV NEURAL INFORM PR, V24, P1386; Vardi M.Y., 1982, PROC 14 ACM SIGACT S, P137, DOI [/10.1145 /800070.802186, 10.1145/800070.802186, DOI 10.1145/800070.802186]; Venugopal Deepak, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8726, P258, DOI 10.1007/978-3-662-44845-8_17; Zhang Nevin Lianwen, 2008, MILCH BRIAN LUKE S Z, V2, P1062	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102039
C	Nguyen, VA; Boyd-Graber, J; Resnik, P; Chang, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Viet-An Nguyen; Boyd-Graber, Jordan; Resnik, Philip; Chang, Jonathan			Learning a Concept Hierarchy from Multi-labeled Documents	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					While topic models can discover patterns of word usage in large corpora, it is difficult to meld this unsupervised structure with noisy, human-provided labels, especially when the label space is large. In this paper, we present a model-Label to Hierarchy (L2H)-that can induce a hierarchy of user-generated labels and the topics associated with those labels from a set of multi-labeled documents. The model is robust enough to account for missing labels from untrained, disparate annotators and provide an interpretable summary of an otherwise unwieldy label set. We show empirically the effectiveness of L2H in predicting held-out words and labels for unseen documents.	[Viet-An Nguyen; Resnik, Philip] Univ Maryland, Comp Sci, College Pk, MD 20742 USA; [Resnik, Philip] Univ Maryland, Linguist, College Pk, MD 20742 USA; [Resnik, Philip] Univ Maryland, UMIACS, College Pk, MD 20742 USA; [Boyd-Graber, Jordan] Univ Colorado, Comp Sci, Boulder, CO 80309 USA; [Chang, Jonathan] Facebook, Menlo Pk, CA USA	University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park; University of Colorado System; University of Colorado Boulder; Facebook Inc	Nguyen, VA (corresponding author), Univ Maryland, Comp Sci, College Pk, MD 20742 USA.	vietan@cs.umd.edu; Jordan.Boyd.Graber@colorado.edu; resnik@umd.edu; jonchang@fb.com			NSF [1211153, 1018625]	NSF(National Science Foundation (NSF))	We thank Kristina Miler, Ke Zhai, Leo Claudino, and He He for helpful discussions, and thank the anonymous reviewers for insightful comments. This research was supported in part by NSF under grant #1211153 (Resnik) and #1018625 (Boyd-Graber and Resnik). Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.	Adams R. P., 2010, NIPS; Ahmed A., 2013, ICML; [Anonymous], 2009, CVPR; [Anonymous], 2008, THESIS U CAMBRIDGE; Bakalov A., 2012, JCDL; Blei D. M., 2003, JMLR, V3; Blei D. M., 2007, NIPS; Blei DM, 2012, COMMUN ACM, V55, P77, DOI 10.1145/2133806.2133826; Blei David M., 2003, NIPS; Chang J, 2010, ANN APPL STAT, V4, P124, DOI 10.1214/09-AOAS309; Chilton L. B., 2013, CHI; Cowans PJ, 2006, THESIS; Deng J., 2014, CHI; Gerrish S., 2012, NIPS; Gerrish Sean, 2011, ICML; Hariharan B, 2012, MACH LEARN, V88, P127, DOI 10.1007/s10994-012-5291-x; HEYMANN P, 2006, 200610 STANF INFOLAB; Lacoste-Julien S., 2008, P 21 INT C NEURAL IN, P897; Li W., 2007, UAI; Li W., 2006, ICML; Liu X., 2012, KDD; MacKay D.J.C., 1995, NAT LANG ENG, V1, P289; Madjarov G, 2012, PATTERN RECOGN, V45, P3084, DOI 10.1016/j.patcog.2012.03.004; MIMNO D, 2008, UAI; Mimno D., 2007, ICML; Nguyen V. - A., 2014, EMNLP; Nguyen V. - A., 2012, ACL; Nguyen Viet-An, 2013, NIPS; Nikolova S. S., 2011, STUDIES COMPUTATIONA; Paisley J. W., 2012, ABS12106738 CORR; Perotte A.J., 2011, ADV NEURAL INFORM PR, P2609; Petinot Y., 2011, HLT; Plangprasopchok A., 2009, WWW; Quinn KM, 2010, AM J POLIT SCI, V54, P209, DOI 10.1111/j.1540-5907.2009.00427.x; Ramage D., 2009, EMNLP; Ramage D., 2010, ICWSM; Rosen-Zvi M., 2004, UAI; Rubin TN, 2012, MACH LEARN, V88, P157, DOI 10.1007/s10994-011-5272-5; Schmitz P., 2006, WWW 2006; Slutsky A, 2013, IEEE INT CONF BIG DA; Teh Y.-W., 2006, ACL; Tibely G, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0084133; Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34; Wallach H., 2009, ICML; Weld Daniel S., 2013, HCOMP; Zhang M. - L., 2014, IEEE TKDE, V26; Zhu J., 2009, ICML	47	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100082
C	Wager, S; Chamandy, N; Muralidharan, O; Najmi, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wager, Stefan; Chamandy, Nick; Muralidharan, Omkar; Najmi, Amir			Feedback Detection for Live Predictors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				OPTIMAL-DESIGN; IDENTIFICATION; NETWORKS	A predictor that is deployed in a live production system may perturb the features it uses to make predictions. Such a feedback loop can occur, for example, when a model that predicts a certain type of behavior ends up causing the behavior it predicts, thus creating a self-fulfilling prophecy. In this paper we analyze predictor feedback detection as a causal inference problem, and introduce a local randomization scheme that can be used to detect non-linear feedback in real-world problems. We conduct a pilot study for our proposed methodology using a predictive system currently deployed as a part of a search engine.	[Wager, Stefan] Stanford Univ, Stanford, CA 94305 USA; Google Inc, Mountain View, CA USA	Stanford University; Google Incorporated	Wager, S (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	swager@stanford.edu; chamandy@google.com; omuralidharan@google.com; amir@google.com			B. C. and E. J. Eaves Stanford Graduate Fellowship	B. C. and E. J. Eaves Stanford Graduate Fellowship	The authors are grateful to Alex Blocker, Randall Lewis, and Brad Efron for helpful suggestions and interesting conversations. S. W. is supported by a B. C. and E. J. Eaves Stanford Graduate Fellowship.	AKAIKE H, 1968, ANN I STAT MATH, V20, P425, DOI 10.1007/BF02911655; Angrist JD, 1996, J AM STAT ASSOC, V91, P444, DOI 10.2307/2291629; Biedermann S, 2011, BIOMETRIKA, V98, P449, DOI 10.1093/biomet/asr001; Bottou L, 2013, J MACH LEARN RES, V14, P3207; Casella G., 1998, THEORY POINT ESTIMAT; Chan D, 2010, KDD, P7, DOI DOI 10.1145/1835804.1835809; Danielsson J, 2002, J BANK FINANC, V26, P1273, DOI 10.1016/S0378-4266(02)00263-7; EFRON B, 1991, J AM STAT ASSOC, V86, P9, DOI 10.2307/2289707; Efron B., 1994, MONOGR STAT APPL PRO, DOI DOI 10.1007/978-1-4899-4541-9; Evgeniou T, 2000, ADV COMPUT MATH, V13, P1, DOI 10.1023/A:1018946025316; Ferraro F, 2005, ACAD MANAGE REV, V30, P8, DOI 10.5465/AMR.2005.15281412; Friedman J., 2009, ELEMENTS STAT LEARNI, DOI 10.1007/978-0-387-84858-7; Green P., 1994, NONPARAMETRIC REGRES; Hastie T.J., 1990, GEN ADDITIVE MODELS, V43; HOLLAND PW, 1986, J AM STAT ASSOC, V81, P945, DOI 10.2307/2289064; Huber PJ, 1967, P 5 BERKELEY S MATH, P221; IMBENS GW, 1994, ECONOMETRICA, V62, P467, DOI 10.2307/2951620; Merton R., 1948, ANTIOCH REV, V8, P193, DOI DOI 10.2307/4609267; Muller WG, 1996, J STAT PLAN INFER, V55, P389, DOI 10.1016/S0378-3758(95)00197-2; Rubin DB, 2005, J AM STAT ASSOC, V100, P322, DOI 10.1198/016214504000001880; STUDDEN WJ, 1969, ANN MATH STAT, V40, P1557, DOI 10.1214/aoms/1177697373; Wahba G., 1990, SPLINE MODELS OBSERV; WHITE H, 1980, ECONOMETRICA, V48, P817, DOI 10.2307/1912934	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101065
C	Wager, S; Fithian, W; Wang, S; Liang, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wager, Stefan; Fithian, William; Wang, Sida; Liang, Percy			Altitude Training: Strong Bounds for Single-Layer Dropout	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				CLASSIFICATION	Dropout training, originally designed for deep neural networks, has been successful on high-dimensional single-layer natural language tasks. This paper proposes a theoretical explanation for this phenomenon: we show that, under a generative Poisson topic model with long documents, dropout training improves the exponent in the generalization bound for empirical risk minimization. Dropout achieves this gain much like a marathon runner who practices at altitude: once a classifier learns to perform reasonably well on training examples that have been artificially corrupted by dropout, it will do very well on the uncorrupted test set. We also show that, under similar conditions, dropout preserves the Bayes decision boundary and should therefore induce minimal bias in high dimensions.	[Wager, Stefan; Fithian, William; Liang, Percy] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Wang, Sida; Liang, Percy] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University; Stanford University	Wager, S (corresponding author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	swager@stanford.edu; wfithian@stanford.edu; sidaw@cs.stanford.edu; pliang@cs.stanford.edu			B.C. and E.J. Eaves Stanford Graduate Fellowship; NSF VIGRE [DMS-0502385]	B.C. and E.J. Eaves Stanford Graduate Fellowship; NSF VIGRE(National Science Foundation (NSF))	S. Wager and W. Fithian are supported by a B.C. and E.J. Eaves Stanford Graduate Fellowship and NSF VIGRE grant DMS-0502385 respectively.	Andrew Ng, 2001, ADV NEURAL INFORM PR, V14; Baldi P, 2014, ARTIF INTELL, V210, P78, DOI 10.1016/j.artint.2014.02.004; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bouchard G., 2004, INT C COMP STAT; Bouchard Guillaume, 2007, INT C MACH LEARN APP; Bousquet O, 2004, LECT NOTES ARTIF INT, V3176, P169; FELLER W., 1966, INTRO PROBABILITY TH, VII; Globerson Amir, 2006, P INT C MACH LEARN; Goodfellow Ian J, 2013, P INT C MACH LEARN; Hinton G.E., 2012, COMPUT SCI, V3, P212, DOI DOI 10.9774/GLEAF.978-1-909493-38-42; Jimmy Ba L, 2013, ADV NEURAL INFORM PR, V26; Krizhevsky A, 2012, ADV NEURAL INFORM PR; Lasserre J. A., 2006, COMPUTER VISION PATT; LIANG P, 2008, P INT C MACH LEARN; Maas Andrew L., 2011, P ASS COMP LING; McAllester D., 2013, ARXIV PREPRINT ARXIV; McCallum A., 2006, ASS ADV ARTIFICIAL I; Pang B., 2004, P ASS COMP LING; Raina Rajat, 2004, ADV NEURAL INFORM PR, V1, P6; van der Maaten L., 2013, INT C MACH LEARN; WAGER S., 2013, P 27 ANN C NEUR INF; Wan L., 2013, P INT C MACH LEARN; Wang S., 2013, P INT C MACH LEARN; Wang Sida I., 2012, P ASS COMP LING; Wang Sida I, 2013, EMPIRICAL METHODS NA; Zhang T, 2004, ANN STAT, V32, P56	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102063
C	Wang, HH; Banerjee, A; Luo, ZQ		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wang, Huahua; Banerjee, Arindam; Luo, Zhi-Quan			Parallel Direction Method of Multipliers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We consider the problem of minimizing block-separable (non-smooth) convex functions subject to linear constraints. While the Alternating Direction Method of Multipliers (ADMM) for two-block linear constraints has been intensively studied both theoretically and empirically, in spite of some preliminary work, effective generalizations of ADMM to multiple blocks is still unclear. In this paper, we propose a parallel randomized block coordinate method named Parallel Direction Method of Multipliers (PDMM) to solve optimization problems with multi-block linear constraints. At each iteration, PDMM randomly updates some blocks in parallel, behaving like parallel randomized block coordinate descent. We establish the global convergence and the iteration complexity for PDMM with constant step size. We also show that PDMM can do randomized block coordinate descent on overlapping blocks. Experimental results show that PDMM performs better than state-of-the-arts methods in two applications, robust principal component analysis and overlapping group lasso.	[Wang, Huahua; Banerjee, Arindam; Luo, Zhi-Quan] Univ Minnesota, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities	Wang, HH (corresponding author), Univ Minnesota, Minneapolis, MN 55455 USA.	huwang@cs.umn.edu; banerjee@cs.umn.edu; luozq@umn.edu			NSF [IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, IIS-0916750]; NASA [NNX12AQ39A]; DDF from the University of Minnesota; IBM; Yahoo; US AFOSR [FA9550-12-1-0340]; National Science Foundation [DMS-1015346]	NSF(National Science Foundation (NSF)); NASA(National Aeronautics & Space Administration (NASA)); DDF from the University of Minnesota; IBM(International Business Machines (IBM)); Yahoo; US AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); National Science Foundation(National Science Foundation (NSF))	H.W. and A.B. acknowledge the support of NSF via IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, IIS-0916750, and NASA grant NNX12AQ39A. H. W. acknowledges the support of DDF (2013-2014) from the University of Minnesota. A.B. acknowledges support from IBM and Yahoo. Z.Q. Luo is supported in part by the US AFOSR via grant number FA9550-12-1-0340 and the National Science Foundation via grant number DMS-1015346.	Bach Francis, 2011, CONVEX OPTIMIZATION; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Boyd S, 2004, CONVEX OPTIMIZATION; Cai T, 2011, J AM STAT ASSOC, V106, P594, DOI 10.1198/jasa.2011.tm10155; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chandrasekaran V, 2012, ANN STAT, V40, P1935, DOI 10.1214/11-AOS949; Chen C., 2013, PREPRINT; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S1064827596304010; Chen X, 2012, ANN APPL STAT, V6, P719, DOI 10.1214/11-AOAS514; Deng W., 2014, PARALLEL MULTIBLOCK; Fu Q., 2013, UAI; Gabay D., 1976, Computers & Mathematics with Applications, V2, P17, DOI 10.1016/0898-1221(76)90003-1; He BS, 2012, SIAM J OPTIMIZ, V22, P313, DOI 10.1137/110822347; Hong M., 2013, PREPRINT; Hong M., 2012, LINEAR CONVERGENCE A; Nesterov Y., 2012, SIAM J OPTIMIZ, V22; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Richtarik P., 2012, MATH PROGRAMMING; SHOR NZ, 1985, MINIMIZATION METHODS; Tappenden R., 2013, PREPRINT; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wang H., 2014, NIPS; Wang H., 2014, PARALLEL DIRECTION M; Wang H., 2013, NIPS; Wang H., 2012, ICML; Wang X., 2013, PREPRINT; Yu Y., 2012, NIPS; Zhao P, 2009, ANN STAT, V37, P3468, DOI 10.1214/07-AOS584; [No title captured]	29	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100035
C	Wang, HH; Banerjee, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wang, Huahua; Banerjee, Arindam			Bregman Alternating Direction Method of Multipliers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				MIRROR DESCENT	The mirror descent algorithm (MDA) generalizes gradient descent by using a Bregman divergence to replace squared Euclidean distance. In this paper, we similarly generalize the alternating direction method of multipliers (ADMM) to Bregman ADMM (BADMM), which allows the choice of different Bregman divergences to exploit the structure of problems. BADMM provides a unified framework for ADMM and its variants, including generalized ADMM, inexact ADMM and Bethe ADMM. We establish the global convergence and the O(1/T) iteration complexity for BADMM. In some cases, BADMM can be faster than ADMM by a factor of O(n/ln n) where n is the dimensionality. In solving the linear program of mass transportation problem, BADMM leads to massive parallelism and can easily run on GPU. BADMM is several times faster than highly optimized commercial software Gurobi.	[Wang, Huahua; Banerjee, Arindam] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities	Wang, HH (corresponding author), Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.	huwang@cs.umn.edu; banerjee@cs.umn.edu			NSF [IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, IIS-0916750]; NASA [NNX12AQ39A]; DDF from the University of Minnesota; IBM; Yahoo	NSF(National Science Foundation (NSF)); NASA(National Aeronautics & Space Administration (NASA)); DDF from the University of Minnesota; IBM(International Business Machines (IBM)); Yahoo	The research was supported by NSF grants IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, IIS-0916750, and by NASA grant NNX12AQ39A. H.W. and A.B. acknowledge the technical support from the University of Minnesota Supercomputing Institute. H.W. acknowledges the support of DDF (2013-2014) from the University of Minnesota. A.B. acknowledges support from IBM and Yahoo.	[Anonymous], 1983, AUGMENTED LAGRANGIAN; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Censor Y, 1998, PARALLEL OPTIMIZATIO; Chen G, 1993, SIAM J OPTIMIZ, V3, P538, DOI 10.1137/0803026; Combettes PL, 2011, SPRINGER SER OPTIM A, V49, P185, DOI 10.1007/978-1-4419-9569-8_10; Deng W., 2012, GLOBAL LINEAR CONVER, ptR12; Duchi J., 2008, PROC 25 INT C MACH L, P272; Duchi J., 2010, COLT; Figueiredo MAT, 2010, IEEE T IMAGE PROCESS, V19, P3133, DOI 10.1109/TIP.2010.2053941; Friedman J., 2009, ELEMENTS STAT LEARNI, DOI 10.1007/978-0-387-84858-7; Fu Q., 2013, UAI; Goldstein T., 2012, 1235 CAM UCLA; Goldstein T, 2010, J SCI COMPUT, V45, P272, DOI 10.1007/s10915-009-9331-z; He BS, 2012, SIAM J NUMER ANAL, V50, P700, DOI 10.1137/110836936; Hitchcock F.L., 1941, J MATH PHYS, V20, P224, DOI DOI 10.1002/SAPM1941201224; Hong M., 2012, LINEAR CONVERGENCE A; Kiwiel KC, 1997, SIAM J CONTROL OPTIM, V35, P1142, DOI 10.1137/S0363012995281742; Nesterov Y., 2007, 76 UCL CORE; Telgarsky M., 2012, ICML; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wang H., 2012, ICML; Yang J., 2009, ALTERNATING DIRECTIO	26	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103063
C	Wang, Q; Zhang, JX; Song, S; Zhang, Z		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wang, Qian; Zhang, Jiaxing; Song, Sen; Zhang, Zheng			Attentional Neural Network: Feature Selection Using Cognitive Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				MODEL	Attentional Neural Network is a new framework that integrates top-down cognitive bias and bottom-up feature extraction in one coherent architecture. The top-down influence is especially effective when dealing with high noise or difficult segmentation problems. Our system is modular and extensible. It is also easy to train and cheap to run, and yet can accommodate complex behaviors. We obtain classification accuracy better than or competitive with state of art results on the MNIST variation dataset, and successfully disentangle overlaid digits with high success rates. We view such a general purpose framework as an essential foundation for a larger system emulating the cognitive abilities of the whole brain.	[Wang, Qian; Song, Sen] Tsinghua Univ, Dept Biomed Engn, Beijing 100084, Peoples R China; [Zhang, Jiaxing] Microsoft Res Asia, Beijing 100080, Peoples R China; [Zhang, Zheng] NYU Shanghai, Dept Comp Sci, Shanghai 200122, Peoples R China	Tsinghua University; Microsoft; Microsoft Research Asia; NYU Shanghai	Song, S (corresponding author), Tsinghua Univ, Dept Biomed Engn, Beijing 100084, Peoples R China.	qianwang.thu@gmail.com; jiaxz@microsoft.com; sen.song@gmail.com; zz@nyu.edu						Baluchi F, 2011, TRENDS NEUROSCI, V34, P210, DOI 10.1016/j.tins.2011.02.003; Cukur T, 2013, NAT NEUROSCI, V16, P763, DOI 10.1038/nn.3381; Ehinger KA, 2009, VIS COGN, V17, P945, DOI 10.1080/13506280902834720; Henderson JM, 2009, J VISION, V9, DOI 10.1167/9.1.32; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Jain V, 2008, P ADV NEUR INF PROC, P769; Krizhevsky A., 2012, ADV NEURAL INFORM PR, V1, P4; Nair V., 2008, SYST VANC P C ADV NE, P1145; Reichert DP, 2011, LECT NOTES COMPUT SC, V6791, P18, DOI 10.1007/978-3-642-21735-7_3; Rifai S., 2011, PROC INT C MACH LEAR; Rothenstein AL, 2008, IMAGE VISION COMPUT, V26, P114, DOI 10.1016/j.imavis.2005.08.011; Salakhutdinov R., 2009, P 12 INT C ART INT S, P448; Schwenker F, 1996, NEURAL NETWORKS, V9, P445, DOI 10.1016/0893-6080(95)00112-3; Sohn K, 2013, P 30 INT C MACHINE L, P217; Tang Y., 2010, P 27 INT C MACH LEAR, P1055; Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0; ULLMAN S, 1995, CEREB CORTEX, V5, P1, DOI 10.1093/cercor/5.1.1; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Zeiler M. D., 2014, EUR C COMP VIS, P818; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278; [No title captured]	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100047
C	Wang, S; Schwing, AG; Urtasun, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wang, Shenlong; Schwing, Alexander G.; Urtasun, Raquel			Efficient Inference of Continuous Markov Random Fields with Polynomial Potentials	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In this paper, we prove that every multivariate polynomial with even degree can be decomposed into a sum of convex and concave polynomials. Motivated by this property, we exploit the concave-convex procedure to perform inference on continuous Markov random fields with polynomial potentials. In particular, we show that the concave-convex decomposition of polynomials can be expressed as a sum-of-squares optimization, which can be efficiently solved via semidefinite programing. We demonstrate the effectiveness of our approach in the context of 3D reconstruction, shape from shading and image denoising, and show that our method significantly outperforms existing techniques in terms of efficiency as well as quality of the retrieved solution.	[Wang, Shenlong; Schwing, Alexander G.; Urtasun, Raquel] Univ Toronto, Toronto, ON, Canada	University of Toronto	Wang, S (corresponding author), Univ Toronto, Toronto, ON, Canada.	slwang@cs.toronto.edu; aschwing@cs.toronto.edu; urtasun@cs.toronto.edu						Ahmadi A. A., 2013, MATH PROGRAMMING; Ahmadi A. A., 2013, SIAM J OPTIMIZATION; [Anonymous], 2012, ICML; Batselier K., 2013, SIAM J MATRIX ANAL A; Boykov Y., 2001, PAMI; Ecker A., 2010, CVPR; Ihler A., 2009, AISTATS; Komodakis N., 2011, PAMI; Koren Y., 2009, COMPUTER; Lasserre J. B., 2006, SIAM J OPTIMIZATION; Lasserre J. B., 2001, SIAM J OPTIMIZATION; Lempitsky V., 2010, PAMI; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Noorshams N., 2013, JMLR; Papachristodoulou A., 2013, ARXIV13104716; Parrilo PA., 2000, THESIS CALTECH; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Peng J., 2011, ICML; Roth Stefan, 2009, IJCV; Salakhutdinov R., 2002, UAI; Salzmann M., 2013, CVPR; Schwing A., 2011, CVPR; Song L., 2011, AISTATS; Sriperumbudur B., NIPS; Sriperumbudur B., ICML; Sudderth E., 2010, COMMUNICATIONS ACM; Taskar B., 2003, ICML; Uherka D. J., 1977, AM MATH MONTHLY; Varol A., 2012, CVPR; Vicente S., 2012, ECCV; Vishwanathan S, 2005, AISTATS; Weiss Y., 2001, NEURAL COMPUTATION; Yuille A. L., 2003, NEURAL COMPUTATION	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103013
C	Wang, XY; Peng, PC; Dunson, DB		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wang, Xiangyu; Peng, Peichao; Dunson, David B.			Median Selection Subset Aggregation for Parallel Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				INFORMATION CRITERIA; MODEL SELECTION	For massive data sets, efficient computation commonly relies on distributed algorithms that store and process subsets of the data on different machines, minimizing communication costs. Our focus is on regression and classification problems involving many features. A variety of distributed algorithms have been proposed in this context, but challenges arise in defining an algorithm with low communication, theoretical guarantees and excellent practical performance in general settings. We propose a MEdian Selection Subset AGgregation Estimator (message) algorithm, which attempts to solve these problems. The algorithm applies feature selection in parallel for each subset using Lasso or another method, calculates the 'median' feature inclusion index, estimates coefficients for the selected features in parallel for each subset, and then averages these estimates. The algorithm is simple, involves very minimal communication, scales efficiently in both sample and feature size, and has theoretical guarantees. In particular, we show model selection consistency and coefficient estimation efficiency. Extensive experiments show excellent performance in variable selection, estimation, prediction, and computation time relative to usual competitors.	[Wang, Xiangyu; Dunson, David B.] Duke Univ, Dept Stat Sci, Durham, NC 27706 USA; [Peng, Peichao] Univ Penn, Stat Dept, Philadelphia, PA 19104 USA	Duke University; University of Pennsylvania	Wang, XY (corresponding author), Duke Univ, Dept Stat Sci, Durham, NC 27706 USA.	xw56@stat.duke.edu; ppeichao@yahoo.com; dunson@stat.duke.edu						Bach F.R., 2008, P 25 INT C MACH LEAR, P33, DOI [10.1145/1390156.1390161, DOI 10.1145/1390156.1390161]; Baldi Pierre, 2014, ARXIV14024735; Barbieri MM, 2004, ANN STAT, V32, P870, DOI 10.1214/009053604000000238; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Bradley Joseph K, 2011, ARXIV11055379; Chen JH, 2008, BIOMETRIKA, V95, P759, DOI 10.1093/biomet/asn034; Dekel O, 2012, J MACH LEARN RES, V13, P165; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Kim Y, 2012, J MACH LEARN RES, V13, P1037; Konishi S, 1996, BIOMETRIKA, V83, P875, DOI 10.1093/biomet/83.4.875; Li Xingguo, 2013, R PACKAGE FLARE HIGH; Lichman M, 2013, UCI MACHINE LEARNING; Mateos G, 2010, IEEE T SIGNAL PROCES, V58, P5262, DOI 10.1109/TSP.2010.2055862; Minsker S., 2014, ARXIV14032660; Minsker S., 2013, ARXIV13081334; Neiswanger W., 2013, ARXIV13114780; Peng Z., 2013, PREPRINT; Scherrer C, 2012, ADV NEURAL INFORM PR, P28; Scott Steven L, 2013, EFABBAYES 250 C, V16; Smola A, 2010, PROC VLDB ENDOW, V3, P703, DOI 10.14778/1920841.1920931; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wang X., 2013, ARXIV PREPRINT ARXIV; Wood AM, 2008, STAT MED, V27, P3227, DOI 10.1002/sim.3177; Yan F., 2009, ADV NEURAL INFORM PR, V22, P2134; Zhang Yuchen, 2012, NIPS, V4, P5; Zhao P, 2006, J MACH LEARN RES, V7, P2541	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100107
C	Wang, X; Bi, JB; Yu, SP; Sun, JW		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wang, Xin; Bi, Jinbo; Yu, Shipeng; Sun, Jiangwen			On Multiplicative Multitask Feature Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We investigate a general framework of multiplicative multitask feature learning which decomposes each task's model parameters into a multiplication of two components. One of the components is used across all tasks and the other component is task-specific. Several previous methods have been proposed as special cases of our framework. We study the theoretical properties of this framework when different regularization conditions are applied to the two decomposed components. We prove that this framework is mathematically equivalent to the widely used multitask feature learning methods that are based on a joint regularization of all model parameters, but with a more general form of regularizers. Further, an analytical formula is derived for the across-task component as related to the task-specific component for all these regularizers, leading to a better understanding of the shrinkage effect. Study of this framework motivates new multitask learning algorithms. We propose two new learning formulations by varying the parameters in the proposed framework. Empirical studies have revealed the relative advantages of the two new formulations by comparing with the state of the art, which provides instructive insights into the feature learning problem with multiple tasks.	[Wang, Xin; Bi, Jinbo; Sun, Jiangwen] Univ Connecticut, Dept Comp Sci & Engn, Storrs, CT 06269 USA; [Yu, Shipeng] Siemens Healthcare, Hlth Serv Innovat Ctr, Malvern, PA 19355 USA	University of Connecticut; Siemens AG	Wang, X (corresponding author), Univ Connecticut, Dept Comp Sci & Engn, Storrs, CT 06269 USA.	wangxin@engr.uconn.edu; jinbo@engr.uconn.edu; shipeng.yu@siemens.com; javon@engr.uconn.edu			NSF [IIS-1320586, DBI-1356655, IIS-1407205, IIS-1447711]	NSF(National Science Foundation (NSF))	Jinbo Bi and her students Xin Wang and Jiangwen Sun were supported by NSF grants IIS-1320586, DBI-1356655, IIS-1407205, and IIS-1447711.	Argyriou A., 2007, NIPS, V19, P41, DOI DOI 10.1007/S10994-007-5040-8; Bi JB, 2008, LECT NOTES ARTIF INT, V5211, P117; Chen J., 2011, PROC 17 ACM SIGKDD I, P42; Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109; Gong P, 2012, NIPS, V25, P1997; Gong Pinghua, 2012, KDD, V2012, P895; Goodman T.R., 1973, J MULTIVAR ANAL, V3, P204, DOI [10.1016/0047-259X(73)90023-7, DOI 10.1016/0047-259X(73)90023-7]; Jacob L., 2008, CLUSTERED MULTITASK; Jalali A., 2010, ADV NEURAL INF PROCE, V23, P964; Kang Z., 2011, P INT C MACH LEARN, V2, P4; Kumar A., 2012, P ICML 12; Lee S., 2010, PROC INT C NEURAL IN, P1306; Liu J., 2009, P 25 C UNCERTAINTY A, P339, DOI DOI 10.5555/1795114.1795154; Lozano A. C., 2012, PROC 29 INT C MACH L, P595; Obozinski Guillaume, 2006, MULTITASK FEATURE SE; Passos A., 2012, P INT C MACH LEARN, P1103; Quattoni A, 2009, P ICML 09, P108; Rai P., 2010, INT C ART INT STAT, P613; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105; Turlach BA, 2005, TECHNOMETRICS, V47, P349, DOI 10.1198/004017005000000139; Ye JP, 2006, SIAM PROC S, P24; Zhang Y., 2010, PROC INT C NEURAL IN, P2559; Zhou Jiayu, 2011, Adv Neural Inf Process Syst, V2011, P702; Zhou Y., 2010, PROC 13 INT C ARTIF, P988	24	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103078
C	Wang, YN; Zhu, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wang, Yining; Zhu, Jun			Spectral Methods for Supervised Topic Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Supervised topic models simultaneously model the latent topic structure of large collections of documents and a response variable associated with each document. Existing inference methods are based on either variational approximation or Monte Carlo sampling. This paper presents a novel spectral decomposition algorithm to recover the parameters of supervised latent Dirichlet allocation (sLDA) models. The Spectral-sLDA algorithm is provably correct and computationally efficient. We prove a sample complexity bound and subsequently derive a sufficient condition for the identifiability of sLDA. Thorough experiments on a diverse range of synthetic and real-world datasets verify the theory and demonstrate the practical effectiveness of the algorithm.	[Wang, Yining] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Zhu, Jun] Tsinghua Univ, Dept Comp Sci & Tech, Tsinghua Natl TNList Lab, State Key Lab Intell Tech & Sys, Beijing, Peoples R China	Carnegie Mellon University; Tsinghua University	Wang, YN (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	yiningwa@cs.cmu.edu; dcszj@mail.tsinghua.edu.cn			National Basic Research Program of China [2013CB329403]; National NSF of China [61322308, 61332007]; Tsinghua University Initiative Scientific Research Program [20121088071]	National Basic Research Program of China(National Basic Research Program of China); National NSF of China(National Natural Science Foundation of China (NSFC)); Tsinghua University Initiative Scientific Research Program	The work was done when Y.W. was at Tsinghua. The work is supported by the National Basic Research Program of China (No. 2013CB329403), National NSF of China (Nos. 61322308, 61332007), and Tsinghua University Initiative Scientific Research Program (No. 20121088071).	Anandkumar A., 2012, ARXIV12107559; Anandkumar A., 2012, 25 ANN C LEARN THEOR, V23; Anandkumar Animashree, 2012, ARXIV12046703; [Anonymous], 2009, CVPR; Arora S., 2012, FOCS; Arora S., 2013, ICML; Arora S., 2012, STOC; Bittorf V., 2012, NIPS; Blei D. M., 2007, NIPS; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Chaganty A., 2013, ICML; Cohen S. B., 2012, NIPS; Griffiths T., 2007, HDB LATENT SEMANT AN; Hoffman M. D., 2010, NEURAL; KRUSKAL JB, 1977, LINEAR ALGEBRA APPL, V18, P95, DOI 10.1016/0024-3795(77)90069-6; Lacoste-julien S., 2008, NIPS, P2282; LEURGANS SE, 1993, SIAM J MATRIX ANAL A, V14, P1064, DOI 10.1137/0614071; McAuley J.J., 2013, P 22 INT C WORLD WID; Moitra Ankur, 2014, ALGORITHMIC ASPECTS; Porteous I., 2008, SIGKDD; REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034; Zhu J., 2011, UAI; Zhu J, 2014, J MACH LEARN RES, V15, P1073; Zhu J, 2012, J MACH LEARN RES, V13, P2237	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102078
C	Weller, A; Jebara, T		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Weller, Adrian; Jebara, Tony			Clamping Variables and Approximate Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				PROPAGATION; RESOLUTION; ALGORITHM	It was recently proved using graph covers (Ruozzi, 2012) that the Bethe partition function is upper bounded by the true partition function for a binary pairwise model that is attractive. Here we provide a new, arguably simpler proof from first principles. We make use of the idea of clamping a variable to a particular value. For an attractive model, we show that summing over the Bethe partition functions for each sub-model obtained after clamping any variable can only raise (and hence improve) the approximation. In fact, we derive a stronger result that may have other useful implications. Repeatedly clamping until we obtain a model with no cycles, where the Bethe approximation is exact, yields the result. We also provide a related lower bound on a broad class of approximate partition functions of general pairwise multi-label models that depends only on the topology. We demonstrate that clamping a few wisely chosen variables can be of practical value by dramatically reducing approximation error.	[Weller, Adrian; Jebara, Tony] Columbia Univ, New York, NY 10027 USA	Columbia University	Weller, A (corresponding author), Columbia Univ, New York, NY 10027 USA.	adrian@cs.columbia.edu; jebara@cs.columbia.edu			NSF [IIS-1117631, CCF-1302269]	NSF(National Science Foundation (NSF))	We thank Nicholas Ruozzi for careful reading, and Nicholas, David Sontag, Aryeh Kontorovich and Tomaz Slivnik for helpful discussion and comments. This work was supported in part by NSF grants IIS-1117631 and CCF-1302269.	Bafna V, 1999, SIAM J DISCRETE MATH, V12, P289, DOI 10.1137/S0895480196305124; Belanger D., 2013, NIPS WORKSH GREED OP; BERTSEKAS D, 1995, NONLINEAR PROGRAMMIN; Choi A., 2008, UNCERTAINTY ARTIFICI; Darwiche A, 2009, MODELING AND REASONING WITH BAYESIAN NETWORKS, P1, DOI 10.1017/CBO9780511811357; Eaton F., 2009, P MACHINE LEARNING R, V5, P145; Fan K., 1958, MONATSH MATH, V62, P219, DOI 95856; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Karp RM., 1972, COMPLEXITY COMPUTER, P85; Koller D., 2009, PROBABILISTIC GRAPHI; LAURITZEN SL, 1988, J ROY STAT SOC B MET, V50, P157; Liu Y, 2012, IEEE T SIGNAL PROCES, V60, P4135, DOI 10.1109/TSP.2012.2195656; McEliece RJ, 1998, IEEE J SEL AREA COMM, V16, P140, DOI 10.1109/49.661103; Meshi O., 2009, UAI; Milgrom P., 1999, ENVELOPE THEOR UNPUB; Mitchell J.E., 2002, HDB APPL OPTIMIZATIO, V1, P65, DOI DOI 10.1007/S10288-005-0052-3; Murphy K. P., 1999, UNCERTAINTY ARTIFICI; PADBERG M, 1991, SIAM REV, V33, P60, DOI 10.1137/1033004; Pakzad P., 2002, BELIEF PROPAGATION S; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; PEOT MA, 1991, ARTIF INTELL, V48, P299, DOI 10.1016/0004-3702(91)90030-N; Rish I, 2000, J AUTOM REASONING, V24, P225, DOI 10.1023/A:1006303512524; Ruozzi Nicholas, 2012, NEURAL INFORM PROCES; SCHLESINGER D., 2006, TECHNICAL REPORT; Sudderth E., 2007, NIPS; TOPKIS DM, 1978, OPER RES, V26, P305, DOI 10.1287/opre.26.2.305; VONTOBEL PO, 2013, SYSTEMS MAN CYBERN A, V59, P6018, DOI DOI 10.1109/TIT.2013.2264715; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P2313, DOI 10.1109/TIT.2005.850091; Weller A., 2014, UNCERTAINTY ARTIFICI; Weller A., 2014, UAI; Weller A., 2013, AISTATS; Welling M., 2013, P 17 C UNC ART INT; Zivny S, 2009, DISCRETE APPL MATH, V157, P3347, DOI 10.1016/j.dam.2009.07.001	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102090
C	Woodruff, DP		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Woodruff, David P.			Low Rank Approximation Lower Bounds in Row-Update Streams	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ALGORITHMS	We study low-rank approximation in the streaming model in which the rows of an n x d matrix A are presented one at a time in an arbitrary order. At the end of the stream, the streaming algorithm should output a k x d matrix R so that parallel to A - AR(dagger)R parallel to(2)(F) <= (1 + epsilon)parallel to A - A(k)parallel to(2)(F), where A(k) is the best rank-k approximation to A. A deterministic streaming algorithm of Liberty (KDD, 2013), with an improved analysis of Ghashami and Phillips (SODA, 2014), provides such a streaming algorithm using O(dk/epsilon) words of space. A natural question is if smaller space is possible. We give an almost matching lower bound of ohm (dk/epsilon) bits of space, even for randomized algorithms which succeed only with constant probability. Our lower bound matches the upper bound of Ghashami and Phillips up to the word size, improving on a simple ohm (dk) space lower bound.	[Woodruff, David P.] IBM Res Almaden, San Jose, CA 95120 USA	International Business Machines (IBM)	Woodruff, DP (corresponding author), IBM Res Almaden, San Jose, CA 95120 USA.	dpwoodru@us.ibm.com			XDATA program of the Defense Advanced Research Projects Agency (DARPA) [FA8750-12-C0323]	XDATA program of the Defense Advanced Research Projects Agency (DARPA)(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	I would like to thank Edo Liberty and Jeff Phillips for many useful discusions and detailed comments on this work (thanks to Jeff for the figure !). I would also like to thank the XDATA program of the Defense Advanced Research Projects Agency (DARPA), administered through Air Force Research Laboratory contract FA8750-12-C0323 for supporting this work.	Alon N, 2002, J COMPUT SYST SCI, V64, P719, DOI 10.1006/jcss.2001.1813; Andoni A, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1729; Brand M, 2002, LECT NOTES COMPUT SC, V2350, P707; Charikar M, 2004, THEOR COMPUT SCI, V312, P3, DOI 10.1016/S0304-3975(03)00400-6; Clarkson KL, 2009, ACM S THEORY COMPUT, P205; Cormode G, 2005, J ALGORITHMS, V55, P58, DOI 10.1016/j.jalgor.2003.12.001; Feldman D, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1434; Frieze A, 2004, J ACM, V51, P1025, DOI 10.1145/1039488.1039494; Ghashami M., 2014, P 25 ANN ACM SIAM S, P707, DOI [DOI 10.1137/1.9781611973402.53, 10.1137/1.9781611973402.53]; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Kannan R., 2013, CORR; Kremer I, 1999, COMPUT COMPLEX, V8, P21, DOI 10.1007/s000370050018; Kushilevitz E., 2006, COMMUNICATION COMPLE; Levy A, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P739; Liberty E, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P581, DOI 10.1145/2487575.2487623; Martin R., 1998, P BRIT MACH VIS C, P1; MISRA J, 1982, SCI COMPUT PROGRAM, V2, P143, DOI 10.1016/0167-6423(82)90012-0; Muthukrishnan S, 2005, FOUND TRENDS THEOR C, V1, P1, DOI 10.1561/0400000002; Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7; Rudelson M., 2010, CORR; Sarlos T, 2006, ANN IEEE SYMP FOUND, P143	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101078
C	Wu, AQ; Park, M; Koyejo, O; Pillow, JW		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wu, Anqi; Park, Mijung; Koyejo, Oluwasanmi; Pillow, Jonathan W.			Sparse Bayesian structure learning with dependent relevance determination prior	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				VARIABLE SELECTION; REGRESSION; SHRINKAGE	In many problem settings, parameter vectors are not merely sparse, but dependent in such a way that non-zero coefficients tend to cluster together. We refer to this form of dependency as "region sparsity". Classical sparse regression methods, such as the lasso and automatic relevance determination (ARD), model parameters as independent a priori, and therefore do not exploit such dependencies. Here we introduce a hierarchical model for smooth, region-sparse weight vectors and tensors in a linear regression setting. Our approach represents a hierarchical extension of the relevance determination framework, where we add a transformed Gaussian process to model the dependencies between the prior variances of regression weights. We combine this with a structured model of the prior variances of Fourier coefficients, which eliminates unnecessary high frequencies. The resulting prior encourages weights to be region-sparse in two different bases simultaneously. We develop efficient approximate inference methods and show substantial improvements over comparable methods (e.g., group lasso and smooth RVM) for both simulated and real datasets from brain imaging.	[Wu, Anqi; Pillow, Jonathan W.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA; [Park, Mijung] UCL, Gatsby Unit, London, England; [Koyejo, Oluwasanmi] Stanford Univ, Dept Psychol, Stanford, CA 94305 USA	Princeton University; University of London; University College London; Stanford University	Wu, AQ (corresponding author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.	anqiw@princeton.edu; mijung@gatsby.ucl.ac.uk; sanmi@stanford.edu; pillow@princeton.edu			McKnight Foundation; NSF CAREER Award [IIS-1150186]; NIMH [MH099611]; Gatsby Charitable Foundation	McKnight Foundation; NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NIMH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH)); Gatsby Charitable Foundation	This work was supported by the McKnight Foundation (JP), NSF CAREER Award IIS-1150186 (JP), NIMH grant MH099611 (JP) and the Gatsby Charitable Foundation (MP).	Alpert NM, 1996, NEUROIMAGE, V3, P10, DOI 10.1006/nimg.1996.0002; Anirban B., 2012, BAYESIAN SHRINKAGE; Bach F, 2012, OPTIMIZATION FOR MACHINE LEARNING, P19; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Carvalho C. M., 2009, J MACHINE LEARNING R, V5, P73; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Engelhardt Barbara E., 2014, ARXIV14072235; Faul AC, 2002, ADV NEUR IN, V14, P383; Friedman J., 2010, ARXIV; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; Glasser M.F., 2013, NEUROIMAGE; Hans C, 2009, BIOMETRIKA, V96, P835, DOI 10.1093/biomet/asp047; Huang JZ, 2011, J MACH LEARN RES, V12, P3371; Jacob L., 2009, P 26 INT C MACH LEAR, P433, DOI DOI 10.1145/1553374.1553431; Jenatton R, 2011, J MACH LEARN RES, V12, P2777; Kim S, 2009, PLOS GENET, V5, DOI 10.1371/journal.pgen.1000587; Lee H, 2016, ADV NEURAL INFORM PR, V19; Liu H., 2009, ADV NEURAL INFORM PR, V21, P969; MacKay DJC, 1996, FUND THEOR, V62, P221; MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129; Neal R. M., 2012, BAYESIAN LEARNING NE; Park M., 2013, P 16 INT C ART INT S, P489; Park M, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002219; Sahani M, 2002, ADV NEURAL INFORM PR, V15, P317; Schmolck A., 2008, THESIS; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; van Gerven MAJ, 2010, NEUROIMAGE, V50, P150, DOI 10.1016/j.neuroimage.2009.11.064; Wipf D., 2007, NIPS; Yuan GX, 2010, J MACH LEARN RES, V11, P3183; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100012
C	Wu, XJ; Sheldon, D; Zilberstein, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wu, Xiaojian; Sheldon, Daniel; Zilberstein, Shlomo			Stochastic Network Design in Bidirected Trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				LANDSCAPE; FRAMEWORK	We investigate the problem of stochastic network design in bidirected trees. In this problem, an underlying phenomenon (e.g., a behavior, rumor, or disease) starts at multiple sources in a tree and spreads in both directions along its edges. Actions can be taken to increase the probability of propagation on edges, and the goal is to maximize the total amount of spread away from all sources. Our main result is a rounded dynamic programming approach that leads to a fully polynomial-time approximation scheme (FPTAS), that is, an algorithm that can find (1 - epsilon)-optimal solutions for any problem instance in time polynomial in the input size and 1/epsilon. Our algorithm outperforms competing approaches on a motivating problem from computational sustainability to remove barriers in river networks to restore the health of aquatic ecosystems.	[Wu, Xiaojian; Sheldon, Daniel; Zilberstein, Shlomo] Univ Massachusetts, Sch Comp Sci, Amherst, MA 01003 USA; [Sheldon, Daniel] Mt Holyoke Coll, Dept Comp Sci, S Hadley, MA 01075 USA	University of Massachusetts System; University of Massachusetts Amherst; Mount Holyoke College	Wu, XJ (corresponding author), Univ Massachusetts, Sch Comp Sci, Amherst, MA 01003 USA.				NSF [IIS-1116917]	NSF(National Science Foundation (NSF))	This work has been partially supported by NSF grant IIS-1116917.	Bowden Alison A., 2013, ICES J MARINE SCI; Chen W., 2010, KDD, P1029, DOI [10.1145/1835804.1835934, DOI 10.1145/1835804.1835934]; COLBOURN CJ, 1987, SIAM J ALGEBRA DISCR, V8, P404, DOI 10.1137/0608033; Foltete JC, 2014, LANDSCAPE URBAN PLAN, V124, P140, DOI 10.1016/j.landurbplan.2013.12.012; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Kumar A., 2012, P 26 AAAI C ART INT, P309; Letcher BH, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0001139; Martin E.H., 2011, NATURE CONSERVANCY E, V102; McGarigal K., 2011, CONSERVATION ASSESSM; O'Hanley JR, 2005, ENVIRON MODEL ASSESS, V10, P85, DOI 10.1007/s10666-004-4268-y; Peeta S, 2010, COMPUT OPER RES, V37, P1708, DOI 10.1016/j.cor.2009.12.006; Saura S, 2009, ENVIRON MODELL SOFTW, V24, P135, DOI 10.1016/j.envsoft.2008.05.005; Saura S, 2007, LANDSCAPE URBAN PLAN, V83, P91, DOI 10.1016/j.landurbplan.2007.03.005; Tambosi LR, 2014, RESTOR ECOL, V22, P169, DOI 10.1111/rec.12049; VALIANT LG, 1979, SIAM J COMPUT, V8, P410, DOI 10.1137/0208032; Vaughan B., 2010, P 26 C UNC ART INT, P517; Wu X., 2013, NIPS WORKSH MACH LEA; Wu Xiaojian, 2014, P 28 C ART INT AAAI; Xue S., 2012, P 26 C ART INT AAAI, P391; Xue Shan, 2014, P C ART INT STAT AIS	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102054
C	Xie, C; Yan, L; Li, WJ; Zhang, ZH		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Xie, Cong; Yan, Ling; Li, Wu-Jun; Zhang, Zhihua			Distributed Power-law Graph Computing: Theoretical and Empirical Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					With the emergence of big graphs in a variety of real applications like social networks, machine learning based on distributed graph-computing (DGC) frame-works has attracted much attention from big data machine learning community. In DGC frameworks, the graph partitioning (GP) strategy plays a key role to affect the performance, including the workload balance and communication cost. Typically, the degree distributions of natural graphs from real applications follow skewed power laws, which makes GP a challenging task. Recently, many methods have been proposed to solve the GP problem. However, the existing GP methods cannot achieve satisfactory performance for applications with power-law graphs. In this paper, we propose a novel vertex-cut method, called degree-based hashing (DBH), for GP. DBH makes effective use of the skewed degree distributions for GP. We theoretically prove that DBH can achieve lower communication cost than existing methods and can simultaneously guarantee good workload balance. Furthermore, empirical results on several large power-law graphs also show that DBH can outperform the state of the art.	[Xie, Cong; Yan, Ling; Zhang, Zhihua] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, 800 Dongchuan Rd, Shanghai 200240, Peoples R China; [Li, Wu-Jun] Nanjing Univ, Natl Key Lab Novel Software Tech, Dept Comp Sci & Tech, Nanjing 210023, Jiangsu, Peoples R China	Shanghai Jiao Tong University; Nanjing University	Xie, C (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.	xcgoner1108@gmail.com; yling0718@sjtu.edu.cn; liwujun@nju.edu.cn; zhang-zh@cs.sjtu.edu.cn	zhang, zh/GWV-4677-2022		NSFC [61100125, 61472182]; 863 Program of China [2012AA011003]; Fundamental Research Funds for the Central Universities	NSFC(National Natural Science Foundation of China (NSFC)); 863 Program of China(National High Technology Research and Development Program of China); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	This work is supported by the NSFC (No. 61100125, No. 61472182), the 863 Program of China (No. 2012AA011003), and the Fundamental Research Funds for the Central Universities.	Adamic L.A., 2002, GLOTTOMETRICS, V3, P143; Boldi Paolo, 2004, P 13 INT C WORLD WOR; Broder A, 2000, COMPUT NETW, V33, P309, DOI 10.1016/S1389-1286(00)00083-9; Chen Rong, 2013, IPADSTR2013001 SHANG; Davis TA, 2011, ACM T MATH SOFTWARE, V38, DOI [10.1145/2049662.2049670, 10.1145/2049662.2049663]; Gonzalez Joseph E., 2014, P 11 USENIX S OP SYS; Gonzalez Joseph E., 2012, P USENIX S OP SYST D; Jain Nilesh, 2013, P 1 INT WORKSH GRAPH; Karypis G., 1995, P INT C PAR PROC ICP; Kwak H., 2010, P 19 INT C WORLD WID; Kyrola A., 2012, P 10 USENIX S OP SYS; Low Yucheng, 2012, P INT C VERY LARGE D; Low Yucheng, 2010, C UNC ART INT UAI; Malewicz G., 2010, P ACM SIGMOD INT C M; Mislove A, 2007, P 7 ACM SIGCOMM C IN; Raab M, 1998, LECT NOTES COMPUT SC, V1518, P159; Stanton Isabelle, 2012, P 18 ACM SIGKDD INT; Tsourakakis Charalampos, 2014, P 7 ACM INT C WEB SE; Wang Lu, 2014, P INT C DAT ENG ICDE	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101067
C	Xin, Y; Jaakkola, T		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Xin, Yu; Jaakkola, Tommi			Controlling privacy in recommender systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Recommender systems involve an inherent trade-off between accuracy of recommendations and the extent to which users are willing to release information about their preferences. In this paper, we explore a two-tiered notion of privacy where there is a small set of "public" users who are willing to share their preferences openly, and a large set of "private" users who require privacy guarantees. We show theoretically and demonstrate empirically that a moderate number of public users with no access to private user information already suffices for reasonable accuracy. Moreover, we introduce a new privacy concept for gleaning relational information from private users while maintaining a first order deniability. We demonstrate gains from controlled access to private user preferences.	[Xin, Yu; Jaakkola, Tommi] MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Xin, Y (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	yuxin@mit.edu; tommi@csail.mit.edu			Google Research Award; Qualcomm Inc.	Google Research Award(Google Incorporated); Qualcomm Inc.	The work was partially supported by Google Research Award and funding from Qualcomm Inc.	Alvim Mario S., 2012, Formal Aspects of Security and Trust. 8th International Workshop, FAST 2011. Revised Selected Papers, P39, DOI 10.1007/978-3-642-29420-4_3; Candes E. J., 2010, P IEEE; Canny J, 2002, SIGIR; Duchi J., 2013, ADV NEURAL INFORM PR, V26, P1529; Duchi J., 2012, NIPS, V25, P1439; Dwork C., 2008, THEORY APPL MODELS C; Jaggi M., 2010, ICML; Keshavan R., 2010, JMLR; Mathias R., 1997, BIT NUMERICAL MATH; McSherry F., 2009, SIGKDD; Miller B. N., 2004, ACM T INF SYST; Negahban S., 2012, JMLR; Salakhutdinov R., 2010, NIPS; Srebro N., 2004, NIPS; Tropp J. A., 2012, FDN COMPUT MATH; Vershynin R., ARXIV10113027; Xin Y., 2012, AISTATS	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101030
C	Xiong, YJ; Liu, W; Zhao, DL; Tang, XO		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Xiong, Yuanjun; Liu, Wei; Zhao, Deli; Tang, Xiaoou			Zeta Hull Pursuits: Learning Nonconvex Data Hulls	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				NYSTROM METHOD; MATRIX	Selecting a small informative subset from a given dataset, also called column sampling, has drawn much attention in machine learning. For incorporating structured data information into column sampling, research efforts were devoted to the cases where data points are fitted with clusters, simplices, or general convex hulls. This paper aims to study nonconvex hull learning which has rarely been investigated in the literature. In order to learn data-adaptive nonconvex hulls, a novel approach is proposed based on a graph-theoretic measure that leverages graph cycles to characterize the structural complexities of input data points. Employing this measure, we present a greedy algorithmic framework, dubbed Zeta Hulls, to perform structured column sampling. The process of pursuing a Zeta hull involves the computation of matrix inverse. To accelerate the matrix inversion computation and reduce its space complexity as well, we exploit a low-rank approximation to the graph adjacency matrix by using an efficient anchor graph technique. Extensive experimental results show that data representation learned by Zeta Hulls can achieve state-of-the-art accuracy in text and image classification tasks.	[Xiong, Yuanjun; Tang, Xiaoou] Chinese Univ Hong Kong, Informat Engn Dept, Hong Kong, Peoples R China; [Liu, Wei] IBM TJ Watson Res Ctr, Yorktown Hts, NY USA; [Zhao, Deli] HTC, Adv Algorithm Res Grp, Beijing, Peoples R China	Chinese University of Hong Kong; International Business Machines (IBM)	Xiong, YJ (corresponding author), Chinese Univ Hong Kong, Informat Engn Dept, Hong Kong, Peoples R China.	yjxiong@ie.cuhk.edu.hk; weiliu@us.ibm.com; deli_zhao@htc.com; xtang@ie.cuhk.edu.hk		Liu, Wei/0000-0002-3865-8145	Shun Hing Institute of Advanced Engineering, The Chinese University of Hong Kong [MMT-8115038]	Shun Hing Institute of Advanced Engineering, The Chinese University of Hong Kong(Chinese University of Hong Kong)	This research is partially supported by project #MMT-8115038 of the Shun Hing Institute of Advanced Engineering, The Chinese University of Hong Kong.	Belabbas MA, 2009, P NATL ACAD SCI USA, V106, P369, DOI 10.1073/pnas.0810600105; Cai D., 2009, P ICML; Chu MT, 2008, SIAM J SCI COMPUT, V30, P1131, DOI 10.1137/070680436; Das A., 2011, P ICML; Davis T., 2011, SPARSEINV MATLAB TOO; Ding C, 2010, IEEE T PATTERN ANAL, V32, P45, DOI 10.1109/TPAMI.2008.277; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185; Gross R., 2008, P IEEE INT C AUT FAC, P1; GU M, 1995, SIAM J MATRIX ANAL A, V16, P793, DOI 10.1137/S0895479893251472; Kaufman L., 2009, FINDING GROUPS DATA; Kumar S., 2009, NIPS, V23; Kumar S, 2012, J MACH LEARN RES, V13, P981; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012; Liu W., 2010, P ICML; Liu W, 2014, PROC CVPR IEEE, P3826, DOI 10.1109/CVPR.2014.483; Liu W, 2011, SER INF MANAGE SCI, V10, P1; Mahoney MW, 2009, P NATL ACAD SCI USA, V106, P697, DOI [10.1073/pnas.0803205105, 10.1073/pnas.0803205106]; Mairal J, 2010, J MACH LEARN RES, V11, P19; Ruppert David., 2010, ELEMENTS STAT LEARNI, V99, P567, DOI 10.1007/978-0-387-84858-7; SAVCHENKO SV, 1993, RUSS MATH SURV+, V48, P189, DOI 10.1070/RM1993v048n01ABEH001001; Talwalkar A., 2008, P CVPR; Talwalkar A, 2013, J MACH LEARN RES, V14, P3129; Thurau C., 2010, P CIKM; Wang FY, 2010, IEEE T PATTERN ANAL, V32, P875, DOI 10.1109/TPAMI.2009.72; Wang J., 2010, P CVPR; Wang S., 2012, NIPS, V26; Williams C., 2000, NIPS, V14; Winter M. E., 1999, SPIES INT S OPT SCI; Xiong Y., 2013, P ICCV; Zhang K, 2009, NEURAL COMPUT, V21, P121, DOI [10.1162/neco.2009.11-07-651, 10.1162/neco.2008.11-07-651]; Zhao D., 2008, NIPS, V22	34	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100091
C	Xu, MJ; Lakshminarayanan, B; Teh, YW; Zhu, J; Zhang, B		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Xu, Minjie; Lakshminarayanan, Balaji; Teh, Yee Whye; Zhu, Jun; Zhang, Bo			Distributed Bayesian Posterior Sampling via Moment Sharing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We propose a distributed Markov chain Monte Carlo (MCMC) inference algorithm for large scale Bayesian posterior simulation. We assume that the dataset is partitioned and stored across nodes of a cluster. Our procedure involves an independent MCMC posterior sampler at each node based on its local partition of the data. Moment statistics of the local posteriors are collected from each sampler and propagated across the cluster using expectation propagation message passing with low communication costs. The moment sharing scheme improves posterior estimation quality by enforcing agreement among the samplers. We demonstrate the speed and inference quality of our method with empirical studies on Bayesian logistic regression and sparse linear regression with a spike-and-slab prior.	[Xu, Minjie; Zhu, Jun; Zhang, Bo] Tsinghua Univ, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China; [Xu, Minjie; Zhu, Jun; Zhang, Bo] Tsinghua Univ, Tsinghua Natl TNList Lab, Beijing 100084, Peoples R China; [Xu, Minjie; Zhu, Jun; Zhang, Bo] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China; [Lakshminarayanan, Balaji] UCL, Gatsby Unit, London WC1N 3AR, England; [Teh, Yee Whye] Univ Oxford, Dept Stat, Oxford OX1 3TG, England	Tsinghua University; Tsinghua University; Tsinghua University; University of London; University College London; University of Oxford	Xu, MJ (corresponding author), Tsinghua Univ, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.				National Basic Research Program of China [2013CB329403]; National NSF of China [61322308, 61332007]; Gatsby charitable foundation; EPSRC [EP/K009362/1]	National Basic Research Program of China(National Basic Research Program of China); National NSF of China(National Natural Science Foundation of China (NSFC)); Gatsby charitable foundation; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We thank Willie Neiswanger for sharing his implementation of NEIS(n), and Michalis K Titsias for sharing the code used in [7]. MX, JZ and BZ gratefully acknowledge funding from the National Basic Research Program of China (No. 2013CB329403) and National NSF of China (Nos. 61322308, 61332007). BL gratefully acknowledges generous funding from the Gatsby charitable foundation. YWT gratefully acknowledges EPSRC for research funding through grant EP/K009362/1.	Ahn S., 2012, INT C MACH LEARN ICM; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137; Hoffman M., 2010, ONLINE LEARNING LATE, P856; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Minka T., 2001, THESIS MIT CAMBRIDGE; Minsker S, 2014, PR MACH LEARN RES, V32, P1656; MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129; Muirhead R. J., 2009, ASPECTS MULTIVARIATE, V197; Neiswanger W, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P623; Newman D, 2009, J MACH LEARN RES, V10, P1801; Patterson S., 2013, P 26 INT C NEUR INF, P3102; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Scott Steven L, 2013, EFABBAYES 250 C, P16; Seeger MW, 2008, J MACH LEARN RES, V9, P759; Titsias M. K., 2011, PROC 24 INT C NEURAL; Tsukuma H, 2006, J MULTIVARIATE ANAL, V97, P1477, DOI 10.1016/j.jmva.2005.11.006; Wang X., 2013, ARXIV PREPRINT ARXIV; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3	20	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103047
C	Yang, TB; Jin, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Yang, Tianbao; Jin, Rong			Extracting Certainty from Uncertainty: Transductive Pairwise Classification from Pairwise Similarities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In this work, we study the problem of transductive pairwise classification from pairwise similarities(1). The goal of transductive pairwise classification from pairwise similarities is to infer the pairwise class relationships, to which we refer as pairwise labels, between all examples given a subset of class relationships for a small set of examples, to which we refer as labeled examples. We propose a very simple yet effective algorithm that consists of two simple steps: the first step is to complete the sub-matrix corresponding to the labeled examples and the second step is to reconstruct the label matrix from the completed sub-matrix and the provided similarity matrix. Our analysis exhibits that under several mild preconditions we can recover the label matrix with a small error, if the top eigen-space that corresponds to the largest eigenvalues of the similarity matrix covers well the column space of label matrix and is subject to a low coherence, and the number of observed pairwise labels is sufficiently enough. We demonstrate the effectiveness of the proposed algorithm by several experiments.	[Yang, Tianbao] Univ Iowa, Iowa City, IA 52242 USA; [Jin, Rong] Michigan State Univ, E Lansing, MI 48824 USA; [Jin, Rong] Alibaba Grp, Hangzhou 311121, Zhejiang, Peoples R China	University of Iowa; Michigan State University; Alibaba Group	Yang, TB (corresponding author), Univ Iowa, Iowa City, IA 52242 USA.	tianbao-yang@uiowa.edu; rongjin@msu.edu			National Science Foundation [IIS-1251031]; Office of Naval Research [N000141210431]	National Science Foundation(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research)	The work of Rong Jin was supported in part by National Science Foundation (IIS-1251031) and Office of Naval Research (N000141210431).	Ailon N, 2012, J MACH LEARN RES, V13, P137; Basu Sugato, 2004, KDD, P59, DOI DOI 10.1145/1014052.1014062; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Drineas P., 2012, P ICML; Fred A., 2003, P IEEE CVPR, V2; Gittens A., 2011, SPECTRAL NORM ERROR; Gittens Alex, 2013, CORR; Guyon Isabelle, 2004, NIPS; Hoi S., 2006, P 12 ACM SIGKDD INT, P187, DOI DOI 10.1145/1150402.1150426; Hullermeier E, 2011, LECT NOTES ARTIF INT, V6925, P38, DOI 10.1007/978-3-642-24412-4_5; Jin R, 2013, IEEE T INFORM THEORY, V59, P6939, DOI 10.1109/TIT.2013.2271378; Kelil A, 2007, BMC BIOINFORMATICS, V8, DOI 10.1186/1471-2105-8-286; Kulis B., 2004, P 10 ACM SIGKDD INT, P551, DOI DOI 10.1145/1014052.1014118; Liben-Nowell D, 2007, J AM SOC INF SCI TEC, V58, P1019, DOI 10.1002/asi.20591; Ng AY, 2002, ADV NEUR IN, V14, P849; Russell S. J, 2002, ADV NEURAL INFORM PR, P12, DOI DOI 10.5555/2968618.2968683; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Stewart G., 1990, MATRIX PERTURBATION; Talwalkar A., 2010, P 26 C UNC ART INT, P572; Towell G., 1991, P 4 INT C NEUR INF P, P977; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Xu Miao, 2013, ADV NEURAL INFORM PR, P2301, DOI DOI 10.5555/2999792.2999869; Yang TB, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P927; Yang Tianbao, 2012, NIPS, P485; Yi J., 2013, P 30 INT C MACH LEAR, P1400	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103058
C	Yang, YZ; Liang, F; Yan, SC; Wang, ZY; Huang, TS		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Yang, Yingzhen; Liang, Feng; Yan, Shuicheng; Wang, Zhangyang; Huang, Thomas S.			On a Theory of Nonparametric Pairwise Similarity for Clustering: Connecting Clustering to Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				RATES; CONSISTENCY; UNIFORM	Pairwise clustering methods partition the data space into clusters by the pairwise similarity between data points. The success of pairwise clustering largely depends on the pairwise similarity function defined over the data points, where kernel similarity is broadly used. In this paper, we present a novel pairwise clustering framework by bridging the gap between clustering and multi-class classification. This pairwise clustering framework learns an unsupervised nonparametric classifier from each data partition, and search for the optimal partition of the data by minimizing the generalization error of the learned classifiers associated with the data partitions. We consider two nonparametric classifiers in this framework, i.e. the nearest neighbor classifier and the plug-in classifier. Modeling the underlying data distribution by nonparametric kernel density estimation, the generalization error bounds for both unsupervised nonparametric classifiers are the sum of nonparametric pairwise similarity terms between the data points for the purpose of clustering. Under uniform distribution, the nonparametric similarity terms induced by both unsupervised classifiers exhibit a well known form of kernel similarity. We also prove that the generalization error bound for the unsupervised plug-in classifier is asymptotically equal to the weighted volume of cluster boundary [1] for Low Density Separation, a widely used criteria for semi-supervised learning and clustering. Based on the derived nonparametric pairwise similarity using the plug-in classifier, we propose a new nonparametric exemplar-based clustering method with enhanced discriminative capability, whose superiority is evidenced by the experimental results.	[Yang, Yingzhen; Liang, Feng; Wang, Zhangyang; Huang, Thomas S.] Univ Illinois, Urbana, IL 61801 USA; [Yan, Shuicheng] Natl Univ Singapore, Singapore 117576, Singapore	University of Illinois System; University of Illinois Urbana-Champaign; National University of Singapore	Yang, YZ (corresponding author), Univ Illinois, Urbana, IL 61801 USA.	yyang58@illinois.edu; liangf@illinois.edu; eleyans@nus.edu.sg; zwang119@illinois.edu; t-huang1@illinois.edu	Liang, Fenghua/HHM-3798-2022; Liang, Feng/GZK-4305-2022; Yang, Yingzhen/AAU-6048-2020; Yan, Shuicheng/HCI-1431-2022		National Science Foundation [1318971]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported by the National Science Foundation under Grant No. 1318971.	Audibert JY, 2007, ANN STAT, V35, P608, DOI 10.1214/009053606000001217; Chapelle O., 2005, P AISTATS, P1; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Devroye Luc P., 1996, PROBABILISTIC THEORY, V31; Dudley R., 1999, UNIFORM CENTRAL LIMI; Einmahl U, 2005, ANN STAT, V33, P1380, DOI 10.1214/009053605000000129; Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800; Gine E, 2002, ANN I H POINCARE-PR, V38, P907, DOI 10.1016/S0246-0203(02)01128-7; Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830; JENSSEN R, 2004, NIPS; Karnin Zohar, 2012, J MACHINE LEARNING R, V23; Krause Andreas, 2010, ADV NEURAL INFORM PR, V23, P5; Lashkari D., 2007, NIPS; Maier M, 2008, NIPS, P1025; Narayanan Hariharan, 2006, ADV NEURAL INFORM PR, P1025; Ng AY, 2002, ADV NEUR IN, V14, P849; NOLAN D, 1987, ANN STAT, V15, P780, DOI 10.1214/aos/1176350374; Shental N., 2003, NIPS; Sugiyama M., 2011, P 28 INT C MACH LEAR, P65; Van Der VaartJon A., 1996, WEAK CONVERGENCE EMP; Xu L, 2004, NIPS; Yang YH, 1999, IEEE T INFORM THEORY, V45, P2271; Yang Z., 2009, ADV NEURAL INFORM PR, P2125; Zhu X.J., 2005, CARNEGIE MELLON U PR	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100083
C	Yao, HS; Szepesvari, C; Sutton, R; Modayil, J; Bhatnagar, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Yao, Hengshuai; Szepesvari, Csaba; Sutton, Rich; Modayil, Joseph; Bhatnagar, Shalabh			Universal Option Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We consider the problem of learning models of options for real-time abstract planning, in the setting where reward functions can be specified at any time and their expected returns must be efficiently computed. We introduce a new model for an option that is independent of any reward function, called the universal option model (UOM). We prove that the UOM of an option can construct a traditional option model given a reward function, and also supports efficient computation of the option-conditional return. We extend the UOM to linear function approximation, and we show the UOM gives the TD solution of option returns and the value function of a policy over options. We provide a stochastic approximation algorithm for incrementally learning UOMs from data and prove its consistency. We demonstrate our method in two domains. The first domain is a real-time strategy game, where the controller must select the best game unit to accomplish a dynamically-specified task. The second domain is article recommendation, where each user query defines a new reward function and an article's relevance is the expected return from following a policy that follows the citations between articles. Our experiments show that UOMs are substantially more efficient than previously known methods for evaluating option returns and policies over options.	[Yao, Hengshuai; Szepesvari, Csaba; Sutton, Rich; Modayil, Joseph] Univ Alberta, Dept Comp Sci, Edmonton, AB T6H 4M5, Canada; [Bhatnagar, Shalabh] Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, Karnataka, India	University of Alberta; Indian Institute of Science (IISC) - Bangalore	Yao, HS (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB T6H 4M5, Canada.	hengshua@cs.ualberta.ca; szepesva@cs.ualberta.ca; sutton@cs.ualberta.ca; jmodayil@cs.ualberta.ca; shalabh@csa.iisc.ernet.in			Alberta Innovates Technology Futures; NSERC; Department of Science and Technology, Government of India	Alberta Innovates Technology Futures; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Department of Science and Technology, Government of India(Department of Science & Technology (India))	Thank the reviewers for their comments. This work was supported by grants from Alberta Innovates Technology Futures, NSERC, and Department of Science and Technology, Government of India.	Abbeel P, 2010, INT J ROBOT RES, V29, P1608, DOI 10.1177/0278364910371999; [Anonymous], 1998, PAGERANK CITATION RA; [Anonymous], 2000, THESIS; Barto A. G., 1994, ADV NEURAL INFORM PR, V687; Richardson M., 2002, NIPS; Sorg J., 2010, P 9 INT C AUT AG MUL, P31; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Syed U, 2010, THESIS; Tang Jie, 2008, P 14 ACM SIGKDD INT, P990, DOI DOI 10.1145/1401890.1402008	13	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103041
C	Yen, IEH; Lin, TW; Lin, SD; Ravikumar, P; Dhillon, IS		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Yen, Ian E. H.; Lin, Ting-Wei; Lin, Shou-De; Ravikumar, Pradeep; Dhillon, Inderjit S.			Sparse Random Features Algorithm as Coordinate Descent in Hilbert Space	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In this paper, we propose a Sparse Random Features algorithm, which learns a sparse non-linear predictor by minimizing an l(1)-regularized objective function over the Hilbert Space induced from a kernel function. By interpreting the algorithm as Randomized Coordinate Descent in an in finite-dimensional space, we show the proposed approach converges to a solution within epsilon-precision of that using an exact kernel method, by drawing O(1/epsilon) random features, in contrast to the O(1/epsilon(2)) convergence achieved by current Monte-Carlo analyses of Random Features. In our experiments, the Sparse Random Feature algorithm obtains a sparse solution that requires less memory and prediction time, while maintaining comparable performance on regression and classification tasks. Moreover, as an approximate solver for the infinite-dimensional l(1)-regularized problem, the randomized approach also enjoys better convergence guarantees than a Boosting approach in the setting where the greedy Boosting step cannot be performed exactly.	[Yen, Ian E. H.; Ravikumar, Pradeep; Dhillon, Inderjit S.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA; [Lin, Ting-Wei; Lin, Shou-De] Natl Taiwan Univ, Dept Comp Sci, Taipei, Taiwan	University of Texas System; University of Texas Austin; National Taiwan University	Yen, IEH (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	ianyen@cs.utexas.edu; b97083@csie.ntu.edu.tw; sdlin@csie.ntu.edu.tw; pradeepr@cs.utexas.edu; inderjit@cs.utexas.edu			Telecommunication Lab.; Chunghwa Telecom Co., Ltd. [TL-103-8201]; AOARD [FA2386-13-1-4045]; Ministry of Science and Technology; National Taiwan University; Intel Co. [MOST102-2911-I-002-001, NTU103R7501, 102-2923-E-002-007-MY2, 102-2221-E-002-170, 103-2221-E-002-104-MY2]; ARO [W911NF-12-1-0390]; NSF [IIS-1149803, IIS-1320894, IIS-1447574, DMS-1264033, CCF-1320746, CCF-1117055]	Telecommunication Lab.; Chunghwa Telecom Co., Ltd.; AOARD; Ministry of Science and Technology(Ministry of Science, ICT & Future Planning, Republic of Korea); National Taiwan University(National Taiwan University); Intel Co.(Intel Corporation); ARO; NSF(National Science Foundation (NSF))	S.-D.Lin acknowledges the support of Telecommunication Lab., Chunghwa Telecom Co., Ltd via TL-103-8201, AOARD via No. FA2386-13-1-4045, Ministry of Science and Technology, National Taiwan University and Intel Co. via MOST102-2911-I-002-001, NTU103R7501, 102-2923-E-002-007-MY2, 102-2221-E-002-170, 103-2221-E-002-104-MY2. P.R. acknowledges the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1320894, IIS-1447574, and DMS-1264033. This research was also supported by NSF grants CCF-1320746 and CCF-1117055.	[Anonymous], 2002, LEARNING KERNELS; Chang C.-C., 2011, ACM T INTELLIGENT SY; Chen S.-T., 2012, ICML; Kar P., 2012, ARTIF INTELL, P583; KIMELDOR.GS, 1970, ANN MATH STAT, V41, P495, DOI 10.1214/aoms/1177697089; Le Q., 2013, 30 INT C MACH LEARN; Lin Husan-Tien, 2008, JMLR; Rahimi A., 2007, NIPS 20, V20; Rahimi A., 2008, NIPS 21, V21; Ratsch Gunnar, 2001, NIPS; Ricktarik P., 2011, TECH REP; Rosset S., 2004, JMLR; Rosset Saharon, 2007, LEARNING THEORY; Song G., 2011, J APPL COMPUTATIONAL; Steinwart I., 2008, SUPPORT VECTOR MACHI; Taskar B., 2004, NIPS 16, V16; Telgarsky Matus, 2011, NIPS; Vedaldi A., 2010, CVPR; Yang T., 2012, ADV NIPS	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102040
C	Yen, IEH; Hsieh, CJ; Ravikumar, P; Dhillon, I		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Yen, Ian E. H.; Hsieh, Cho-Jui; Ravikumar, Pradeep; Dhillon, Inderjit			Constant Nullspace Strong Convexity and Fast Convergence of Proximal Methods under High-Dimensional Settings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					State of the art statistical estimators for high-dimensional problems take the form of regularized, and hence non-smooth, convex programs. A key facet of these statistical estimation problems is that these are typically not strongly convex under a high-dimensional sampling regime when the Hessian matrix becomes rank-deficient. Under vanilla convexity however, proximal optimization methods attain only a sublinear rate. In this paper, we investigate a novel variant of strong convexity, which we call Constant Nullspace Strong Convexity (CNSC), where we require that the objective function be strongly convex only over a constant subspace. As we show, the CNSC condition is naturally satisfied by high-dimensional statistical estimators. We then analyze the behavior of proximal methods under this CNSC condition: we show global linear convergence of Proximal Gradient and local quadratic convergence of Proximal Newton Method, when the regularization function comprising the statistical estimator is decomposable. We corroborate our theory via numerical experiments, and show a qualitative difference in the convergence rates of the proximal algorithms when the loss function does satisfy the CNSC condition.	[Yen, Ian E. H.; Hsieh, Cho-Jui; Ravikumar, Pradeep; Dhillon, Inderjit] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Yen, IEH (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	ianyen@cs.utexas.edu; cjhsieh@cs.utexas.edu; pradeepr@cs.utexas.edu; inderjit@cs.utexas.edu			NSF [IIS-1149803, IIS-1320894, IIS-1447574, DMS-1264033, CCF-1320746, CCF-1117055]; IBM PhD fellowship; ARO [W911NF-12-1-0390]	NSF(National Science Foundation (NSF)); IBM PhD fellowship(International Business Machines (IBM)); ARO	This research was supported by NSF grants CCF-1320746 and CCF-1117055. C.-J.H acknowledges support from an IBM PhD fellowship. P.R. acknowledges the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1320894, IIS-1447574, and DMS-1264033.	Agarwal A., 2010, NIPS; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Becker S., 2011, SIAM J IMAGING SCI; Boyd S., 2003, CONVEX OPTIMIZATION; GARG R., 2009, ICML; Hoffman A.J., 1952, J RES NBS; Hou K., 2013, NEURAL INFORM PROCES; Hsieh C.- J., 2011, NIPS; Ji S., 2009, ICML; Lee J. D., 2012, NIPS; Negahban S., 2009, NIPS; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y, 2007, GRADIENT METHODS MIN; Scheinberg K., 2013, ARXIV13116547 CORL L; Tewari A., 2011, NIPS; Tseng P., 2009, MATH PROG B, V117; Wang P. -W., 2013, TECHNICAL REPORT; Xiao L, 2012, ICML; Yuan G. -X., 2012, J MACHINE LEARNING R, V13; Zhi-Quan Luo, 1993, Annals of Operations Research, V46-47, P157	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100036
C	Yoshikawa, Y; Iwata, T; Sawada, H		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Yoshikawa, Yuya; Iwata, Tomoharu; Sawada, Hiroshi			Latent Support Measure Machines for Bag-of-Words Data Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In many classification problems, the input is represented as a set of features, e.g., the bag-of-words (BoW) representation of documents. Support vector machines (SVMs) are widely used tools for such classification problems. The performance of the SVMs is generally determined by whether kernel values between data points can be de fined properly. However, SVMs for BoW representations have a major weakness in that the co-occurrence of different but semantically similar words cannot be reflected in the kernel calculation. To overcome the weakness, we propose a kernel-based discriminative classifier for BoW data, which we call the latent support measure machine (latent SMM). With the latent SMM, a latent vector is associated with each vocabulary term, and each document is represented as a distribution of the latent vectors for words appearing in the document. To represent the distributions efficiently, we use the kernel embeddings of distributions that hold high order moment information about distributions. Then the latent SMM finds a separating hyperplane that maximizes the margins between distributions of different classes while estimating latent vectors for words to improve the classification performance. In the experiments, we show that the latent SMM achieves state-of-the-art accuracy for BoW text classification, is robust with respect to its own hyper-parameters, and is useful to visualize words.	[Yoshikawa, Yuya] Nara Inst Sci & Technol, Nara 6300192, Japan; [Iwata, Tomoharu] NTT Commun Sci Labs, Kyoto 6190237, Japan; [Sawada, Hiroshi] NTT Serv Evolut Labs, Yokosuka, Kanagawa 2390847, Japan	Nara Institute of Science & Technology; Nippon Telegraph & Telephone Corporation	Yoshikawa, Y (corresponding author), Nara Inst Sci & Technol, Nara 6300192, Japan.	yoshikawa.yuya.yl9@is.naist.jp; iwata.tomoharu@lab.ntt.co.jp; sawada.hiroshi@lab.ntt.co.jp			JSPS [259867]	JSPS(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science)	This work was supported by JSPS Grant-in-Aid for JSPS Fellows (259867).	Blei David M., 2007, NIPS, P1; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; CARDOSOCACHOPO A, 2007, THESIS; Cherkassky V, 2004, NEURAL NETWORKS, V17, P113, DOI 10.1016/S0893-6080(03)00169-2; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427; Information Office of the State Council, 2003, P 26 IN FORMATION SY; Iwata Tomoharu, 2008, SIGKDD; Joachims T., 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]; Kolari P., 2006, AAAI SPRING S COMP A; Kudo Taku, 2001, P 2 M N AM CHAPT ASS, V816; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Muandet K., 2012, NIPS; Smola A., 2007, ALGORITHMIC LEARNING; Smola AJ, 1999, ADV NEUR IN, V11, P585; Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Tsochantaridis Ioannis, 2004, P 21 INT C MACH LEAR; Yang CH, 2007, PROCEEDINGS OF THE IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE, P275, DOI 10.1109/WI.2007.51; Zhu J., 2009, ICML	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102041
C	Yu, AW; Ma, WL; Yu, YL; Carbonell, JG; Sra, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Yu, Adams Wei; Ma, Wanli; Yu, Yaoliang; Carbonell, Jaime G.; Sra, Suvrit			Efficient Structured Matrix Rank Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				SYSTEM-IDENTIFICATION; APPROXIMATION	We study the problem of finding structured low-rank matrices using nuclear norm regularization where the structure is encoded by a linear map. In contrast to most known approaches for linearly structured rank minimization, we do not (a) use the full SVD; nor (b) resort to augmented Lagrangian techniques; nor (c) solve linear systems per iteration. Instead, we formulate the problem differently so that it is amenable to a generalized conditional gradient method, which results in a practical improvement with low per iteration computational cost. Numerical results show that our approach significantly outperforms state-of-the-art competitors in terms of running time, while effectively recovering low rank solutions in stochastic system realization and spectral compressed sensing problems.	[Yu, Adams Wei; Ma, Wanli; Yu, Yaoliang; Carbonell, Jaime G.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA; [Sra, Suvrit] Max Planck Inst Intelligent Syst, Stuttgart, Germany	Carnegie Mellon University; Max Planck Society	Yu, AW (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.	weiyu@cs.cmu.edu; mawanli@cs.cmu.edu; yaoliang@cs.cmu.edu; jgc@cs.cmu.edu; suvrit@tuebingen.mpg.de						Balle Borja, 2012, ADV NEURAL INFORM PR, P2159; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bredies K, 2009, COMPUT OPTIM APPL, V42, P173, DOI 10.1007/s10589-007-9083-3; Cadzow J. A., 1988, IEEE T SIGNAL PROCES, P39; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Cullum J., 2002, LANCZOS ALGORITHMS L, DOI [10.1137/1.9780898719192, DOI 10.1137/1.9780898719192]; Ding T, 2007, IEEE I CONF COMP VIS, P817; Duarte MF, 2013, APPL COMPUT HARMON A, V35, P111, DOI 10.1016/j.acha.2012.08.003; Fazel M., 2002, MATRIX RANK MINIMIZA; Fazel M, 2013, SIAM J MATRIX ANAL A, V34, P946, DOI 10.1137/110853996; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Harchaoui Z., 2012, NIPS WORKSH OPT ML; HUA YB, 1992, IEEE T SIGNAL PROCES, V40, P2267, DOI 10.1109/78.157226; Ishteva M, 2014, SIAM J MATRIX ANAL A, V35, P1180, DOI 10.1137/130931655; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Liu Z, 2013, SYST CONTROL LETT, V62, P605, DOI 10.1016/j.sysconle.2013.04.005; Liu Z, 2009, IEEE DECIS CONTR P, P4676, DOI 10.1109/CDC.2009.5400177; Liu Z, 2009, SIAM J MATRIX ANAL A, V31, P1235, DOI 10.1137/090755436; Mari J, 2000, IEEE T SIGNAL PROCES, V48, P2092, DOI 10.1109/78.847793; Markovsky I, 2008, AUTOMATICA, V44, P891, DOI 10.1016/j.automatica.2007.09.011; Rau JW, 2013, INT CONF MACH LEARN, P1418, DOI 10.1109/ICMLC.2013.6890805; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Rennie J. D., 2005, P 22 INT C MACHINE L, P713; Shin P. J., 2013, MAGNETIC RESONANCE M; Signoretto M., 2013, TECH REP, P13; Srebro N., 2004, NIPS; Zhang X., 2012, NIPS, P2915; Zhou J., 2012, SIAM DATA MINING TUT; Zhu X., 2011, TECHNICAL REPORT	29	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102085
C	Yun, SY; Lelarge, M; Proutiere, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Yun, Se-Young; Lelarge, Marc; Proutiere, Alexandre			Streaming, Memory Limited Algorithms for Community Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In this paper, we consider sparse networks consisting of a finite number of non-overlapping communities, i.e. disjoint clusters, so that there is higher density within clusters than across clusters. Both the intra- and inter-cluster edge densities vanish when the size of the graph grows large, making the cluster reconstruction problem nosier and hence difficult to solve. We are interested in scenarios where the network size is very large, so that the adjacency matrix of the graph is hard to manipulate and store. The data stream model in which columns of the adjacency matrix are revealed sequentially constitutes a natural framework in this setting. For this model, we develop two novel clustering algorithms that extract the clusters asymptotically accurately. The first algorithm is offline, as it needs to store and keep the assignments of nodes to clusters, and requires a memory that scales linearly with the network size. The second algorithm is online, as it may classify a node when the corresponding column is revealed and then discard this information. This algorithm requires a memory growing sub-linearly with the network size. To construct these efficient streaming memory-limited clustering algorithms, we first address the problem of clustering with partial information, where only a small proportion of the columns of the adjacency matrix is observed and develop, for this setting, a new spectral algorithm which is of independent interest.	[Yun, Se-Young] MSR Inria, 23 Ave Italie, F-75013 Paris, France; [Lelarge, Marc] Inria, F-75013 Paris, France; [Lelarge, Marc] ENS, F-75013 Paris, France; [Proutiere, Alexandre] KTH, EE Sch, ACL, S-10044 Stockholm, Sweden	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Royal Institute of Technology	Yun, SY (corresponding author), MSR Inria, 23 Ave Italie, F-75013 Paris, France.	seyoung.yun@inria.fr; marc.lelarge@ens.fr; alepro@kth.se	Yun, Seyoung/M-6903-2017		French Agence Nationale de la Recherche (ANR) [ANR-11-JS02-005-01]; ERC FSA grant; SSF ICT-Psi project	French Agence Nationale de la Recherche (ANR)(French National Research Agency (ANR)); ERC FSA grant; SSF ICT-Psi project	Work performed as part of MSR-INRIA joint research centre. M.L. acknowledges the support of the French Agence Nationale de la Recherche (ANR) under reference ANR-11-JS02-005-01 (GAP project).; A. Proutiere's research is supported by the ERC FSA grant, and the SSF ICT-Psi project.	Boppana R. B., 1987, 28th Annual Symposium on Foundations of Computer Science (Cat. No.87CH2471-1), P280, DOI 10.1109/SFCS.1987.22; Chatterjee S., 2012, ARXIV12121247; Chaudhuri Kamalika, 2012, J MACHINE LEARNING R, P35; Chen Y., 2012, NIPS, P2213; Coja-Oghlan A, 2010, COMB PROBAB COMPUT, V19, P227, DOI 10.1017/S0963548309990514; Dasgupta A, 2006, LECT NOTES COMPUT SC, V4168, P256; Decelle A., 2011, PHYS REV LETT, V107; Guha S, 2000, ANN IEEE SYMP FOUND, P359, DOI 10.1109/SFCS.2000.892124; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Jerrum M, 1998, DISCRETE APPL MATH, V82, P155, DOI 10.1016/S0166-218X(97)00133-9; Massoulie L., 2013, ABS13113085 CORR; McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929; Mitliagkas I., 2013, NIPS; Mossel E., 2012, ARXIV12021499; Yun S., 2014, COLT	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103035
C	Zhang, CC; Chaudhuri, K		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zhang, Chicheng; Chaudhuri, Kamalika			Beyond Disagreement-based Agnostic Active Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We study agnostic active learning, where the goal is to learn a classifier in a pre-specified hypothesis class interactively with as few label queries as possible, while making no assumptions on the true function generating the labels. The main algorithm for this problem is disagreement-based active learning, which has a high label requirement. Thus a major challenge is to find an algorithm which achieves better label complexity, is consistent in an agnostic setting, and applies to general classification problems. In this paper, we provide such an algorithm. Our solution is based on two novel contributions; first, a reduction from consistent active learning to confidence-rated prediction with guaranteed error, and second, a novel confidence-rated predictor.	[Zhang, Chicheng; Chaudhuri, Kamalika] Univ Calif San Diego, 9500 Gilman Dr, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Zhang, CC (corresponding author), Univ Calif San Diego, 9500 Gilman Dr, La Jolla, CA 92093 USA.	chichengzhang@ucsd.edu; kamalika@cs.ucsd.edu			NSF [IIS-1162581]	NSF(National Science Foundation (NSF))	We thank NSF under IIS-1162581 for research support. We thank Sanjoy Dasgupta and Yoav Freund for helpful discussions. CZ would like to thank Liwei Wang for introducing the problem of selective classification to him.	Awasthi P., 2014, STOC; Balcan M.-F., 2007, COLT; Balcan M. F, 2013, COLT; Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003; Beygelzimer A., 2010, NIPS; Beygelzimer Alina, 2009, ICML; COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1007/BF00993277; Dasgupta S., 2008, ICML; Dasgupta S, 2004, NIPS; Dasgupta S., 2007, NIPS; DASGUPTA S, 2005, NIPS; Dasgupta S, 2011, THEOR COMPUT SCI, V412, P1767, DOI 10.1016/j.tcs.2010.12.054; El- Yaniv R., 2012, JMLR; El-Yaniv R., 2011, NIPS; El-Yaniv R., 2010, JMLR; Freund Y, 1997, MACH LEARN, V28, P133, DOI 10.1023/A:1007330508534; Freund Y., 2004, ANN STAT, V32; Hanneke S, 2007, ICML; Hanneke S., 2013, STAT THEORY AC UNPUB; Hanneke S., 2012, ABS12073772 CORR; Hanneke S., 2009, COLT; Hsu D., 2010, THESIS; Kaariainen M., 2006, ALT; Koltchinskii V., 2010, JMLR; Li L., 2008, ICML; Naghshvar M., 2013, ALLERTON; Nowak RD, 2011, IEEE T INFORM THEORY, V57, P7893, DOI 10.1109/TIT.2011.2169298; Settles B., 2010, TECHNICAL REPORT; Shafer G., 2008, JMLR; Tsybakov AB, 2004, ANN STAT, V32, P135; Urner R., 2013, COLT	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101106
C	Zhang, J; Schwing, AG; Urtasun, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zhang, Jian; Schwing, Alexander G.; Urtasun, Raquel			Message Passing Inference for Large Scale Graphical Models with High Order Potentials	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					To keep up with the Big Data challenge, parallelized algorithms based on dual decomposition have been proposed to perform inference in Markov random fields. Despite this parallelization, current algorithms struggle when the energy has high order terms and the graph is densely connected. In this paper we propose a partitioning strategy followed by a message passing algorithm which is able to exploit pre-computations. It only updates the high-order factors when passing messages across machines. We demonstrate the effectiveness of our approach on the task of joint layout and semantic segmentation estimation from single images, and show that our approach is orders of magnitude faster than current methods.	[Zhang, Jian] Swiss Fed Inst Technol, Zurich, Switzerland; [Schwing, Alexander G.; Urtasun, Raquel] Univ Toronto, Toronto, ON, Canada	Swiss Federal Institutes of Technology Domain; ETH Zurich; University of Toronto	Zhang, J (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	jizhang@ethz.ch; aschwing@cs.toronto.edu; urtasun@cs.toronto.edu						Amini A. A., 1990, PAMI; Batra D., 2011, P AISTATS; Boykov Y., 2001, PAMI; COLLINS M, 2003, COMPUTATIONAL LINGUI; Dechter R., 2013, REASONING PROBABILIS; Elidan  G., 2006, P UAI; Ford L. R., 1956, CANADIAN J MATH; Hazan T., 2012, P ICML; Hazan T., 2010, T INFORM THEORY; Kohli P., 2010, P CVPR; Kohli P., 2009, IJCV; Koller D., 2009, PROBABILISTIC GRAPHI; Komodakis N., 2007, P ICCV, P1284; Krahenbuhl P., 2011, P NIPS; Lee D. C., 2010, P NIPS; Lempitsky V., 2010, PAMI; Li Y., 2013, P CVPR; Meltzer T, 2009, P UAI; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Salzmann M., 2013, P CVPR; Salzmann M., 2012, P ECCV; Schwing A., 2011, P CVPR; Schwing A. G., 2012, P CVPR; Silberman Nathan, 2012, P ECCV; Smith David, 2008, P EMNLP; Sontag D., 2007, P NIPS; Sontag D., 2012, P UAI; Sontag D., 2008, P NIPS; Sun D., 2014, P CVPR; Sutton C., 2007, P UAI; Tseng P., 1987, MATH PROGRAMMING; Valgaerts L., 2010, P EUR C COMP VIS HER; Vineet V., 2012, P ECCV; Wainwright M, 2008, GRAPHICAL MODELS EXP; WEISS Y, 2007, P UAI; Yanover C., 2008, J COMPUTATIONAL BIOL; Yao J., 2012, P CVPR; Zhang J., 2013, P ICCV	38	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103006
C	Zhang, YC; Sutton, C		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zhang, Yichuan; Sutton, Charles			Semi-Separable Hamiltonian Monte Carlo for Inference in Bayesian Hierarchical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Sampling from hierarchical Bayesian models is often difficult for MCMC methods, because of the strong correlations between the model parameters and the hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo (RMHMC) methods have significant potential advantages in this setting, but are computationally expensive. We introduce a new RMHMC method, which we call semi-separable Hamiltonian Monte Carlo, which uses a specially designed mass matrix that allows the joint Hamiltonian over model parameters and hyperparameters to decompose into two simpler Hamiltonians. This structure is exploited by a new integrator which we call the alternating blockwise leapfrog algorithm. The resulting method can mix faster than simpler Gibbs sampling while being simpler and more efficient than previous instances of RMHMC.	[Zhang, Yichuan; Sutton, Charles] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland	University of Edinburgh	Zhang, YC (corresponding author), Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.	Y.Zhang-60@sms.ed.ac.uk; c.sutton@inf.ed.ac.uk						[Anonymous], THESIS; [Anonymous], 2004, SIMULATING HAMILTONI; Betancourt M. J., 2012, ARXIV E PRINTS; Betancourt M. J., 2013, ARXIV E PRINTS; Christensen OF, 2005, J R STAT SOC B, V67, P253, DOI 10.1111/j.1467-9868.2005.00500.x; Geyer CJ, 1992, STAT SCI, V7, P473, DOI [10.1214/ss/1177011137, DOI 10.1214/SS/1177011137]; Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Kim S, 1998, REV ECON STUD, V65, P361, DOI 10.1111/1467-937X.00050; Lichman M, 2013, UCI MACHINE LEARNING; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Pakman A., 2013, ADV NEURAL INFORM PR, V26, P2490; Robert C, 2004, MONTE CARLO STAT MET, DOI DOI 10.1007/978-1-4757-4145-2; Wang Z., 2013, ICML, P1462; Zhang Y., 2012, ADV NEURAL INFORM PR	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103042
C	Zhang, YC; Chen, X; Zhou, DY; Jordan, MI		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zhang, Yuchen; Chen, Xi; Zhou, Dengyong; Jordan, Michael, I			Spectral Methods Meet EM: A Provably Optimal Algorithm for Crowdsourcing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The Dawid-Skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers. However, since the estimator maximizes a non-convex log-likelihood function, it is hard to theoretically justify its performance. In this paper, we propose a two-stage efficient algorithm for multi-class crowd labeling problems. The first stage uses the spectral method to obtain an initial estimate of parameters. Then the second stage refines the estimation by optimizing the objective function of the Dawid-Skene estimator via the EM algorithm. We show that our algorithm achieves the optimal convergence rate up to a logarithmic factor. We conduct extensive experiments on synthetic and real datasets. Experimental results demonstrate that the proposed algorithm is comparable to the most accurate empirical approach, while outperforming several other recently proposed methods.	[Zhang, Yuchen; Jordan, Michael, I] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Chen, Xi] NYU, New York, NY 10012 USA; [Zhou, Dengyong] Microsoft Res, Redmond, WA 98052 USA	University of California System; University of California Berkeley; New York University; Microsoft	Zhang, YC (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	yuczhang@berkeley.edu; xichen@nyu.edu; dengyong.zhou@microsoft.com; jordan@berkeley.edu	Zhang, Yuchen/GYI-8858-2022; Camps-Valls, Gustavo/A-2532-2011; Jordan, Michael I/C-5253-2013; , Gustavo/ABC-1706-2022	Camps-Valls, Gustavo/0000-0003-1683-2138; 				Anandkumar A., 2013, ANN C LEARN THEOR; Anandkumar A., 2012, 12046703 ARXIV; Anandkumar A., 2012, 12107559 ARXIV; Anandkumar A., 2012, ANN C LEARN THEOR; Chaganty A. T., 2013, 13063729 ARXIV; Chen X., 2013, P ICML; Dalvi Nilesh, 2013, P WORLD WID WEB C; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Deng J., 2009, P CVPR, P248, DOI DOI 10.1109/CVPR.2009.5206848; Gao Chao, 2014, ARXIV13105764; Ghosh Arpita, 2011, P ACM C EL COMM; Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025; Jain P., 2013, 13112972 ARXIV; Karger D. R., 2013, ACM SIGMETRICS; Karger DR, 2014, OPER RES, V62, P1, DOI 10.1287/opre.2013.1235; Lease Matthew, 2011, P TREC 2011; Lehmann E, 2003, THEORY POINT ESTIMAT; Liang Percy, 2013, NIPS SPECTR LEARN WO; Liu Q., 2012, ADV NEURAL INFORM PR, V25; Raykar VC, 2010, J MACH LEARN RES, V11, P1297; Snow  Rion, 2008, P EMNLP; Welinder P., 2010, NIPS; Whitehill Jacob, 2009, ADV NEURAL INFORM PR, V22; Zhang Y., 2014, ARXIV14063824; Zhou D., 2012, NIPS; Zhou  Dengyong, 2014, P ICML; Zou J., 2013, NIPS	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101102
C	Zhao, T; Yu, M; Wang, YM; Arora, R; Liu, H		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zhao, Tuo; Yu, Mo; Wang, Yiming; Arora, Raman; Liu, Han			Accelerated Mini-batch Randomized Block Coordinate Descent Method	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				SELECTION; SHRINKAGE; ONLINE; LASSO	We consider regularized empirical risk minimization problems. In particular, we minimize the sum of a smooth empirical risk function and a nonsmooth regularization function. When the regularization function is block separable, we can solve the minimization problems in a randomized block coordinate descent (RBCD) manner. Existing RBCD methods usually decrease the objective value by exploiting the partial gradient of a randomly selected block of coordinates in each iteration. Thus they need all data to be accessible so that the partial gradient of the block gradient can be exactly obtained. However, such a "batch" setting may be computationally expensive in practice. In this paper, we propose a mini-batch randomized block coordinate descent (MRBCD) method, which estimates the partial gradient of the selected block based on a mini-batch of randomly sampled data in each iteration. We further accelerate the MRBCD method by exploiting the semi-stochastic optimization scheme, which effectively reduces the variance of the partial gradient estimators. Theoretically, we show that for strongly convex functions, the MRBCD method attains lower overall iteration complexity than existing RBCD methods. As an application, we further trim the MRBCD method to solve the regularized sparse learning problems. Our numerical experiments shows that the MRBCD method naturally exploits the sparsity structure and achieves better computational performance than existing methods.	[Zhao, Tuo; Wang, Yiming; Arora, Raman] Johns Hopkins Univ, Baltimore, MD 21218 USA; [Yu, Mo] Harbin Inst Technol, Harbin, Heilongjiang, Peoples R China; [Zhao, Tuo; Liu, Han] Princeton Univ, Princeton, NJ 08544 USA	Johns Hopkins University; Harbin Institute of Technology; Princeton University	Zhao, T (corresponding author), Johns Hopkins Univ, Baltimore, MD 21218 USA.	tour@jhu.edu; myu25@jhu.edu; freewym@jhu.edu; arora@jhu.edu; hanliu@princeton.edu	Wang, Yiming/AAZ-4928-2021	Wang, Yiming/0000-0002-5588-8241	NSF [IIS1408910, IIS1332109]; NIH [R01HG06841, R01MH102339, R01GM083084]; China Scholarship Council; NSFC [61173073]	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); China Scholarship Council(China Scholarship Council); NSFC(National Natural Science Foundation of China (NSFC))	This work is partially supported by the grants NSF IIS1408910, NSF IIS1332109, NIH R01MH102339, NIH R01GM083084, and NIH R01HG06841. Yu is supported by China Scholarship Council and by NSFC 61173073.	Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Boyd S., 2009, CONVEX OPTIMIZATION; Duchi J, 2009, J MACH LEARN RES, V10, P2899; Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Konecny J., 2013, ARXIV PREPRINT ARXIV; Langford J, 2009, J MACH LEARN RES, V10, P777; Lin Xiao, 2014, ARXIV14034699; Liu H., 2009, P 26 ANN INT C MACHI, P649, DOI DOI 10.1145/1553374.1553458; Meier L, 2008, J R STAT SOC B, V70, P53, DOI 10.1111/j.1467-9868.2007.00627.x; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Nesterov Y., 2007, TECHNICAL REPORT; Richtarik P., 2011, ARXIV11072848; Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z; Roux R. N., 2012, ADV NEURAL INFORM PR, P2663; Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865; Sra S, 2012, OPTIMIZATION FOR MACHINE LEARNING, P1; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wang Huahua, 2014, ABS14070107 CORR; Wang L, 2006, STAT SINICA, V16, P589; Wu TT, 2008, ANN APPL STAT, V2, P224, DOI 10.1214/07-AOAS147; Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018; Zhu J, 2004, ADV NEUR IN, V16, P49; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103065
C	Zheng, CY; Pestilli, F; Rokem, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zheng, Charles Y.; Pestilli, Franco; Rokem, Ariel			Deconvolution of High Dimensional Mixtures via Boosting, with Application to Diffusion-Weighted MRI of Human Brain	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Diffusion-weighted magnetic resonance imaging (DWI) and fiber tractography are the only methods to measure the structure of the white matter in the living human brain. The diffusion signal has been modelled as the combined contribution from many individual fascicles of nerve fibers passing through each location in the white matter. Typically, this is done via basis pursuit, but estimation of the exact directions is limited due to discretization [1, 2]. The difficulties inherent in modeling DWI data are shared by many other problems involving fitting non-parametric mixture models. Ekanadaham et al. [3] proposed an approach, continuous basis pursuit, to overcome discretization error in the 1-dimensional case (e.g., spike-sorting). Here, we propose a more general algorithm that fits mixture models of any dimensionality without discretization. Our algorithm uses the principles of L2-boost [4], together with refitting of the weights and pruning of the parameters. The addition of these steps to L2-boost both accelerates the algorithm and assures its accuracy. We refer to the resulting algorithm as elastic basis pursuit, or EBP, since it expands and contracts the active set of kernels as needed. We show that in contrast to existing approaches to fitting mixtures, our boosting framework (1) enables the selection of the optimal bias-variance tradeoff along the solution path, and (2) scales with high-dimensional problems. In simulations of DWI, we find that EBP yields better parameter estimates than a non-negative least squares (NNLS) approach, or the standard model used in DWI, the tensor model, which serves as the basis for diffusion tensor imaging (DTI) [5]. We demonstrate the utility of the method in DWI data acquired in parts of the brain containing crossings of multiple fascicles of nerve fibers.	[Zheng, Charles Y.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Pestilli, Franco] Indiana Univ, Dept Psychol & Brain Sci, Bloomington, IN 47405 USA; [Rokem, Ariel] Stanford Univ, Dept Psychol, Stanford, CA 94305 USA	Stanford University; Indiana University System; Indiana University Bloomington; Stanford University	Zheng, CY (corresponding author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	snarles@stanford.edu; franpest@indiana.edu; arokem@stanford.edu			NIH [1T32GM096982]; NIH fellowship [F32-EY022294]; NSF [BCS1228397]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NIH fellowship(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF))	The authors thank Brian Wandell and Eero Simoncelli for useful discussions. CZ was supported through an NIH grant 1T32GM096982 to Robert Tibshirani and Chiara Sabatti, AR was supported through NIH fellowship F32-EY022294. FP was supported through NSF grant BCS1228397 to Brian Wandell	BASSER PJ, 1994, BIOPHYS J, V66, P259, DOI 10.1016/S0006-3495(94)80775-1; Behrens TEJ, 2007, NEUROIMAGE, V34, P144, DOI 10.1016/j.neuroimage.2006.09.018; Bro R, 1997, J CHEMOMETR, V11, P393, DOI 10.1002/(SICI)1099-128X(199709/10)11:5<393::AID-CEM483>3.3.CO;2-C; Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; Candes E. J., 2013, COMMUNICATIONS PURE; Dell'Acqua F, 2007, IEEE T BIO-MED ENG, V54, P462, DOI 10.1109/TBME.2006.888830; Demiriz A, 2002, MACH LEARN, V46, P225, DOI 10.1023/A:1012470815092; Ekanadham C, 2011, IEEE T SIGNAL PROCES, V59, P4735, DOI 10.1109/TSP.2011.2160058; GUDBJARTSSON H, 1995, MAGN RESON MED, V34; Lawson C. L., 1995, SOLVING LEAST SQUARE; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Sanchez-Bajo F, 2000, J APPL CRYSTALLOGR, V33, P259, DOI 10.1107/S0021889899015575; STEJSKAL EO, 1965, J CHEM PHYS, V42, P288, DOI 10.1063/1.1695690; Tournier JD, 2007, NEUROIMAGE, V35, P1459, DOI 10.1016/j.neuroimage.2007.02.016; Valiant G, 2011, ACM S THEORY COMPUT, P685	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102067
C	Zhong, K; Yen, IEH; Dhillon, IS; Ravikumar, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zhong, Kai; Yen, Ian E. H.; Dhillon, Inderjit S.; Ravikumar, Pradeep			Proximal Quasi-Newton for Computationally Intensive l(1)-regularized M-estimators	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We consider the class of optimization problems arising from computationally intensive l(1)-regularized M-estimators, where the function or gradient values are very expensive to compute. A particular instance of interest is the l(1)-regularized MLE for learning Conditional Random Fields (CRFs), which are a popular class of statistical models for varied structured prediction problems such as sequence labeling, alignment, and classification with label taxonomy. l(1)-regularized MLEs for CRFs are particularly expensive to optimize since computing the gradient values requires an expensive inference step. In this work, we propose the use of a carefully constructed proximal quasi-Newton algorithm for such computationally intensive M-estimation problems, where we employ an aggressive active set selection technique. In a key contribution of the paper, we show that the proximal quasi-Newton method is provably super-linearly convergent, even in the absence of strong convexity, by leveraging a restricted variant of strong convexity. In our experiments, the proposed algorithm converges considerably faster than current state-of-the-art on the problems of sequence labeling and hierarchical classification.	[Zhong, Kai] Univ Texas Austin, Inst Computat Engn & Sci, Austin, TX 78712 USA; [Yen, Ian E. H.; Dhillon, Inderjit S.; Ravikumar, Pradeep] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin; University of Texas System; University of Texas Austin	Zhong, K (corresponding author), Univ Texas Austin, Inst Computat Engn & Sci, Austin, TX 78712 USA.	zhongkai@ices.utexas.edu; ianyen@cs.utexas.edu; inderjit@cs.utexas.edu; pradeepr@cs.utexas.edu			NSF [IIS-1149803, IIS-1320894, IIS-1447574, DMS-1264033, CCF-1320746, CCF-1117055]; ARO [W911NF-12-1-0390]; National Initiative for Modeling and Simulation fellowship	NSF(National Science Foundation (NSF)); ARO; National Initiative for Modeling and Simulation fellowship	This research was supported by NSF grants CCF-1320746 and CCF-1117055. P.R. acknowledges the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1320894, IIS-1447574, and DMS-1264033. K.Z. acknowledges the support of the National Initiative for Modeling and Simulation fellowship	Agarwal A., 2010, NIPS; Boyd S., 2003, CONVEX OPTIMIZATION; Dennis J. E., 1974, MATH COMPUT, V28; Gao J., 2007, ICML; Hoffman A.J., 1952, J RES NBS; Hou K., 2014, NIPS; Hsieh C.- J., 2011, NIPS; Kassel R. H., 1995, THESIS; Lee J. D., 2012, ARXIV12061623; Lee J. D., 2012, NIPS; Lin Ting-Wei, 2014, NIPS; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Scheinberg K., 2013, ARXIV13116547 LEH U; Schmidt M., 2009, INT C ART INT STAT; Sokolovska N., 2009, ARXIV09091308; Tang X., 2013, ARXIV13036935; Taskar Ben, 2003, NIPS; Tseng P., 2009, MATH PROG B, V117; Tsuboi Y., 2011, P 25 AAAI C ART INT; Tsuruokas Y., 2009, P 47 ANN M ACL 4 IJC, V1, P477; Wainwright MJ, 2003, 649 U CAL DEP STAT; Wang P. -W., 2013, TECHNICAL REPORT; Xiao L, 2012, ICML; Yu J, 2010, J MACH LEARN RES, V11, P1145; Yuan GX, 2012, J MACH LEARN RES, V13, P1999; Yuan GX, 2010, J MACH LEARN RES, V11, P3183	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101055
C	Zhou, MY		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zhou, Mingyuan			Beta-Negative Binomial Process and Exchangeable Random Partitions for Mixed-Membership Modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The beta-negative binomial process (BNBP), an integer-valued stochastic process, is employed to partition a count vector into a latent random count matrix. As the marginal probability distribution of the BNBP that governs the exchangeable random partitions of grouped data has not yet been developed, current inference for the BNBP has to truncate the number of atoms of the beta process. This paper introduces an exchangeable partition probability function to explicitly describe how the BNBP clusters the data points of each group into a random number of exchangeable partitions, which are shared across all the groups. A fully collapsed Gibbs sampler is developed for the BNBP, leading to a novel nonparametric Bayesian topic model that is distinct from existing ones, with simple implementation, fast convergence, good mixing, and state-of-the-art predictive performance.	[Zhou, Mingyuan] Univ Texas Austin, IROM Dept, McCombs Sch Business, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Zhou, MY (corresponding author), Univ Texas Austin, IROM Dept, McCombs Sch Business, Austin, TX 78712 USA.	mingyuan.zhou@mccombs.utexas.edu	Zhou, Mingyuan/AAE-8717-2021					Blackwell D., 1973, ANN STAT; Blei D. M., 2003, J MACH LEARN RES; Broderick T., 2014, IEEE T PATTERN ANAL; Ferguson Thomas S, 1973, ANN STAT; Griffiths T., 2005, NIPS; Griffiths T. L., 2004, PNAS; Heaukulani Creighton, 2013, ARXIV14010062; Hjort N. L., 1990, ANN STAT; Lijoi A., 2010, BAYESIAN NONPARAMETR; LO AY, 1982, Z WAHRSCHEINLICHKEIT, V59, P55, DOI 10.1007/BF00575525; Madsen R. E., 2005, ICML; Mimno D., 2012, INT C MACH LEARN; Newman D., 2009, JMLR; Paisley J., 2011, AISTATS; Pitman J, 2006, COMBINATORIAL STOCHA; Porteous I., 2008, SIGKDD; Regazzini E., 2003, ANN STAT; SIBUYA M, 1979, ANN I STAT MATH, V31, P373, DOI 10.1007/BF02480295; Teh Y. W., 2006, JASA; Teh Yee Whye, 2014, ARXIV14074211; Thibaux R., 2007, AISTATS; Titsias M. K., 2008, NIPS; Wallach H., 2009, ICML; Wang C., 2011, AISTATS; Zhou M., 2014, ARXIV14103155; Zhou M., 2014, ARXIV4043331V2; Zhou M., 2012, AISTATS; Zhou M., 2014, IEEE T PATTERN ANAL	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100077
C	Zhu, CB; Xu, H; Leng, CL; Yan, SC		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zhu, Changbo; Xu, Huan; Leng, Chenlei; Yan, Shuicheng			Convex Optimization Procedure for Clustering: Theoretical Revisit	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In this paper, we present theoretical analysis of SON - a convex optimization procedure for clustering using a sum-of-norms (SON) regularization recently proposed in [8, 10, 11, 17]. In particular, we show if the samples are drawn from two cubes, each being one cluster, then SON can provably identify the cluster membership provided that the distance between the two cubes is larger than a threshold which (linearly) depends on the size of the cube and the ratio of numbers of samples in each cluster. To the best of our knowledge, this paper is the first to provide a rigorous analysis to understand why and when SON works. We believe this may provide important insights to develop novel convex optimization based algorithms for clustering.	[Zhu, Changbo] Natl Univ Singapore, Dept Math, Dept Elect & Comp Engn, Singapore, Singapore; [Xu, Huan] Natl Univ Singapore, Dept Mech Engn, Singapore, Singapore; [Leng, Chenlei] Univ Warwick, Dept Stat, Coventry, W Midlands, England; [Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore	National University of Singapore; National University of Singapore; University of Warwick; National University of Singapore	Zhu, CB (corresponding author), Natl Univ Singapore, Dept Math, Dept Elect & Comp Engn, Singapore, Singapore.	elezhuc@nus.edu.sg; mpexuh@nus.edu.sg; c.leng@warwick.ac.uk; eleyans@nus.edu.sg	Yan, Shuicheng/HCI-1431-2022		Ministry of Education of Singapore through AcRF Tier Two grant [R-265-000-443-112]; Microsoft Research Asia [R-263-000-B13-597]	Ministry of Education of Singapore through AcRF Tier Two grant(Ministry of Education, Singapore); Microsoft Research Asia(Microsoft)	The work of H. Xu was partially supported by the Ministry of Education of Singapore through AcRF Tier Two grant R-265-000-443-112. This work is also partially supported by the grant from Microsoft Research Asia with grant number R-263-000-B13-597.	Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Bertsekas D., CONVEX OPTIMIZATION; Bubeck Sebastien, 2009, ARXIV09075494; Chen Y., 2012, NIPS, P2213; Chen Yudong, 2014, P 31 INT C MACH LEAR; Chi E. C., 2013, ARXIV E PRINTS; Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547; Getoor Lise, 2011, P 28 INT C INT C MAC; Lindsten F, 2011, 2011 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP), P201, DOI 10.1109/SSP.2011.5967659; Lindsten Fredrik, 2011, 2992 LINKP U AUT CON; Liu G., 2010, P 27 INT C MACHINE L, P663, DOI DOI 10.1109/ICDMW.2010.64; Marchetti Yuliya, 2014, ARXIV14046289; McLachlan G., 2007, EM ALGORITHM EXTENSI, V382; McLachlan Geoffrey J, 2004, TECHNICAL REPORT; Pelckmans K., 2005, PASCAL WORKSH STAT O, P1; Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; van der Kloot WA, 2005, PSYCHOL METHODS, V10, P468, DOI 10.1037/1082-989X.10.4.468; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Xu R, 2005, IEEE T NEURAL NETWOR, V16, P645, DOI 10.1109/TNN.2005.845141	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100086
C	Zhu, J; Mao, JH; Yuille, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zhu, Jun; Mao, Junhua; Yuille, Alan			Learning From Weakly Supervised Data by The Expectation Loss SVM (e-SVM) algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In many situations we have some measurement of confidence on "positiveness" for a binary label. The "positiveness" is a continuous value whose range is a bounded interval. It quantifies the affiliation of each training data to the positive class. We propose a novel learning algorithm called expectation loss SVM (e-SVM) that is devoted to the problems where only the "positiveness" instead of a binary label of each training sample is available. Our e-SVM algorithm can also be readily extended to learn segment classifiers under weak supervision where the exact positiveness value of each training example is unobserved. In experiments, we show that the e-SVM algorithm can effectively address the segment proposal classification task under both strong supervision (e.g. the pixel-level annotations are available) and the weak supervision (e.g. only bounding-box annotations are available), and outperforms the alternative approaches. Besides, we further validate this method on two major tasks of computer vision: semantic segmentation and object detection. Our method achieves the state-of-the-art object detection performance on PASCAL VOC 2007 dataset.	[Zhu, Jun; Mao, Junhua; Yuille, Alan] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90024 USA	University of California System; University of California Los Angeles	Zhu, J (corresponding author), Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90024 USA.	jzh@ucla.edu; mjhustc@ucla.edu; yuille@stat.ucla.edu			National Science Foundation (NSF) [CCF-1317376]; National Institute of Health NIH [5R01EY022247-03]	National Science Foundation (NSF)(National Science Foundation (NSF)); National Institute of Health NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	We gratefully acknowledge funding support from the National Science Foundation (NSF) with award CCF-1317376, and from the National Institute of Health NIH Grant 5R01EY022247-03. We also thank the NVIDIA Corporation for providing GPUs in our experiments.	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Andrews S., 2002, SUPPORT VECTOR MACHI, P561; Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32; Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231; Cipoll Roberto, 2008, PROC CVPR IEEE, P1; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deng J., PASCAL VISUAL OBJECT; Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3; Everingham M., PASCAL VISUAL OBJECT; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Fidler S, 2013, PROC CVPR IEEE, P3294, DOI 10.1109/CVPR.2013.423; Girshick R., 2014, CVPR, DOI 10.1109/CVPR.2014.81; Jarvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lapin M, 2014, NEURAL NETWORKS, V53, P95, DOI 10.1016/j.neunet.2014.02.002; Li FX, 2010, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2010.5539839; Liu Y, 2013, PROC CVPR IEEE, P2075, DOI 10.1109/CVPR.2013.270; Muller Andreas, 2012, Partially Supervised Learning: First IAPR TC3 Workshop (PSL 2011). Revised Selected Papers, P110, DOI 10.1007/978-3-642-28258-4_12; Ren X., 2012, CVPR; Suykens JAK, 2002, NEUROCOMPUTING, V48, P85, DOI 10.1016/S0925-2312(01)00644-0; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Vezhnevets A, 2012, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2012.6247757; Yang X., 2005, IJCNN	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100066
C	Zhu, YC; Lafferty, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zhu, Yuancheng; Lafferty, John			Quantized Estimation of Gaussian Sequence Models in Euclidean Balls	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				DISTRIBUTIONS; RATES	A central result in statistical theory is Pinsker's theorem, which characterizes the minimax rate in the normal means model of nonparametric estimation. In this paper, we present an extension to Pinsker's theorem where estimation is carried out under storage or communication constraints. In particular, we place limits on the number of bits used to encode an estimator, and analyze the excess risk in terms of this constraint, the signal size, and the noise level. We give sharp upper and lower bounds for the case of a Euclidean ball, which establishes the Pareto-optimal minimax tradeoff between storage and risk in this setting.	[Zhu, Yuancheng; Lafferty, John] Univ Chicago, Dept Stat, Chicago, IL 60637 USA	University of Chicago	Zhu, YC (corresponding author), Univ Chicago, Dept Stat, Chicago, IL 60637 USA.				NSF [IIS-1116730]; AFOSR [FA9550-09-1-0373]; ONR [N000141210762]; Amazon AWS in Education Machine Learning Research grant	NSF(National Science Foundation (NSF)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); ONR(Office of Naval Research); Amazon AWS in Education Machine Learning Research grant	Research supported in part by NSF grant IIS-1116730, AFOSR grant FA9550-09-1-0373, ONR grant N000141210762, and an Amazon AWS in Education Machine Learning Research grant. The authors thank Andrew Barron, John Duchi, and Alfred Hero for valuable comments on this work.	Cai TT, 2012, J MULTIVARIATE ANAL, V107, P24, DOI 10.1016/j.jmva.2011.11.008; Cai T, 2013, J MACH LEARN RES, V14, P1837; Chandrasekaran V, 2013, P NATL ACAD SCI USA, V110, pE1181, DOI 10.1073/pnas.1302293110; Donoho D., 2000, WALD LECT I COUNTING; Draper SC, 2004, IEEE J SEL AREA COMM, V22, P966, DOI 10.1109/JSAC.2004.830875; Gallager R. G., 1968, INFORM THEORY RELIAB, V2; Jenkins JM, 2010, ASTROPHYS J LETT, V713, pL87, DOI 10.1088/2041-8205/713/2/L87; Johnstone IM, 2002, FUNCTION ESTIM UNPUB; Nussbaum M., 1999, ENCY STAT SCI, V3, P451; Pinsker M. S., 1980, Problems of Information Transmission, V16, P120; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Venkataramanan R, 2013, IEEE INT SYMP INFO, P1182, DOI 10.1109/ISIT.2013.6620413; WONG WH, 1995, ANN STAT, V23, P339, DOI 10.1214/aos/1176324524; Yang YH, 1999, ANN STAT, V27, P1564; Zhang Y., 2013, NEURAL INFORM PROCES, P2328	16	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100033
C	Zohrer, M; Pernkopf, F		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zoehrer, Matthias; Pernkopf, Franz			General Stochastic Networks for Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We extend generative stochastic networks to supervised learning of representations. In particular, we introduce a hybrid training objective considering a generative and discriminative cost function governed by a trade-off parameter lambda. We use a new variant of network training involving noise injection, i.e. walkback training, to jointly optimize multiple network layers. Neither additional regularization constraints, such as l1, l2 norms or dropout variants, nor pooling- or convolutional layers were added. Nevertheless, we are able to obtain state-of-the-art performance on the MNIST dataset, without using permutation invariant digits and outperform baseline models on sub-variants of the MNIST and rectangles dataset significantly.	[Zoehrer, Matthias; Pernkopf, Franz] Graz Univ Technol, Signal Proc & Speech Commun Lab, Graz, Austria	Graz University of Technology	Zohrer, M (corresponding author), Graz Univ Technol, Signal Proc & Speech Commun Lab, Graz, Austria.	matthias.zoehrer@tugraz.at; pernkopf@tugraz.at						Abhishek K., 2012, INT C MACH LEARN ICM; [Anonymous], 2013, ARXIV201313013557; Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024; Bengio Y., 2013, P 26 INT C NEUR INF, P899; Bengio Y., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556; Bengio Yoshua, 2013, 13061091 ARXIV, V1306, P1091; Bergstra J., 2010, PYTH SCI COMP C SCIP; Dahl G. E., 2010, ADV NEURAL INFORM PR, V23, P469, DOI DOI 10.5555/2997189.2997242; Deng L, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P1692; Glorot X, 2011, P 14 INT C ART INT S, V15, P315, DOI DOI 10.1177/1753193410395357; Goodfellow I. J., 2013, CORR; Hinton G., IMPROVING NEURAL NET, V1207, P1; Hinton G.E., 2012, NEURAL NETWORKS TRIC, P599, DOI 10.1007/978-3-642-35289-8_32; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Larochelle H., 2014, ONLINE COMPANION PAP; Larochelle H, 2012, J MACH LEARN RES, V13, P643; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y., 2014, MNIST DATABASE HANDW; Lee H., 2007, ADV NEURAL INF PROCE, P801; Li D., 2011, 12 ANN C INT SPEECH, P2285; Mesnil G., 2012, P ICML WORKSH UNS TR, P97; Ngiam J., 2011, ADV NEURAL INFORM PR, pp 24; Ozair S., 2013, CORR; Ranzato M.A., 2006, ADV NEURAL INFORM PR, V19, P1137, DOI DOI 10.7551/MITPRESS/7503.003.0147; Razavian A. S., 2014, INTERSPEECH; Rifai S., 2012, ADV NEURAL INFORM PR, P2294; Salakhutdinov R., 2009, P 12 INT C ART INT S, P448; Sanjoy D., 2013, INT C MACH LEARN, P1319, DOI DOI 10.5555/3042817.3043084; Smolensky P., 1986, PARALLEL DISTRIBUTED, V1, P194; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Wan L., 2013, UNKNOWN, P109, DOI DOI 10.1109/TPAMI.2017.2703082	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100057
C	Atwal, GS; Bialek, W		Thrun, S; Saul, K; Scholkopf, B		Atwal, GS; Bialek, W			Ambiguous model learning made unambiguous with 1/f priors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				DISSIPATION	What happens to the optimal interpretation of noisy data when there exists more than one equally plausible interpretation of the data? In a Bayesian model-learning framework the answer depends on the prior expectations of the dynamics of the model parameter that is to be inferred from the data. Local time constraints on the priors are insufficient to pick one interpretation over another. On the other hand, nonlocal time constraints, induced by a 1/f noise spectrum of the priors, is shown to permit learning of a specific model parameter even when there are infinitely many equally plausible interpretations of the data. This transition is inferred by a remarkable mapping of the model estimation problem to a dissipative physical system, allowing the use of powerful statistical mechanical methods to uncover the transition from indeterminate to determinate model learning.	Princeton Univ, Dept Phys, Princeton, NJ 08544 USA	Princeton University	Atwal, GS (corresponding author), Princeton Univ, Dept Phys, Princeton, NJ 08544 USA.							BIALEK W, 1995, PHYS REV LETT, V74, P3079; BRAY AJ, 1982, PHYS REV LETT, V49, P1545, DOI 10.1103/PhysRevLett.49.1545; CALDEIRA AO, 1981, PHYS REV LETT, V46, P211, DOI 10.1103/PhysRevLett.46.211; DEBOER E, 1976, HDB SENSORY PHYSL, V3, P479; FISHER GH, 1968, PERCEPT PSYCHOPHYS, V4, P189, DOI 10.3758/BF03210466; FISHER MPA, 1985, PHYS REV B, V32, P6190, DOI 10.1103/PhysRevB.32.6190; LEGGETT AJ, 1987, REV MOD PHYS, V59, P1, DOI 10.1103/RevModPhys.59.1	7	0	0	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1205	1212						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500150
C	Brand, M		Thrun, S; Saul, K; Scholkopf, B		Brand, M			Minimax embeddings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Spectral methods for nonlinear dimensionality reduction (NLDR) impose a neighborhood graph on point data and compute eigenfunctions of a quadratic form generated from the graph. We introduce a more general and more robust formulation of NLDR based on the singular value decomposition (SVD). In this framework, most spectral ALDR principles can be recovered by taking a subset of the constraints in a quadratic form built from local nullspaces on the manifold. The minimax formulation also opens up an interesting class of methods in which the graph is "decorated" with information at the vertices, offering discrete or continuous maps, reduced computational complexity, and immunity to some solution instabilities of eigenfunction approaches. Apropos, we show almost all NLDR methods based on eigenvalue decompositions (EVD) have a solution instability that increases faster than problem size. This pathology can be observed (and corrected via the minimax formulation) in problems as small as N < 100 points.	Mitsubishi Elect Res Labs, Cambridge, MA 02139 USA		Brand, M (corresponding author), Mitsubishi Elect Res Labs, Cambridge, MA 02139 USA.							BELKIN M, 2002, ADV NEURAL INFORMATI, V14; BRAND M, 2003, ADV NEURAL INFORMATI, V15; Chung F.R.K., 1997, CBMS REGIONAL C SERI, V92; FIEDLER M, 1975, CZECH MATH J, V25, P619; HE XF, 2002, TR200209 U CHIG COMP; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Stewart G., 1990, MATRIX PERTURBATION; TEH YW, 2003, P NIPS 15; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Tutte W.T., 1963, P LOND MATH SOC, V3, P743, DOI [DOI 10.1112/PLMS/S3-13.1.743, 10.1112/plms/s3-13.1.743]	10	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						505	512						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500064
C	Carreras, X; Marquez, L		Thrun, S; Saul, K; Scholkopf, B		Carreras, X; Marquez, L			Online learning via global feedback for phrase recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					This work presents an architecture based on perceptrons to recognize phrase structures, and an online learning algorithm to train the perceptrons together and dependently. The recognition strategy applies learning in two layers: a filtering layer, which reduces the search space by identifying plausible phrase candidates, and a ranking layer, which recursively builds the optimal phrase structure. We provide a recognition-based feedback rule which reflects to each local function its committed errors from a global point of view, and allows to train them together online as perceptrons. Experimentation on a syntactic parsing problem, the recognition of clause hierarchies, improves state-of-the-art results and evinces the advantages of our global training method over optimizing each function locally and independently.	Tech Univ Catalonia, TALP Res Ctr, LSI Dept, E-08034 Barcelona, Spain	Universitat Politecnica de Catalunya	Carreras, X (corresponding author), Tech Univ Catalonia, TALP Res Ctr, LSI Dept, Campus Nord UPC, E-08034 Barcelona, Spain.			Carreras, Xavier/0000-0001-7432-4540				CARRERAS X, 2002, P 1J ECML HELS FINL; COLLINS M, 2002, P EMNLP 02; Crammer K, 2003, J MACH LEARN RES, V3, P1025, DOI 10.1162/153244303322533188; Freund Y, 1999, MACH LEARN, V37, P277, DOI 10.1023/A:1007662407062; HARPELED S, 2003, ADV NEURAL INFORMATI, V15; KUDO T, 2002, P CONLL 2002L; KUDO T, 2001, P 2 C N AM CHAPT ASS; PUNYAKANOK V, 2001, ADV NEURAL INFORMATI, V13; Ratnaparkhi A, 1999, MACH LEARN, V34, P151, DOI 10.1023/A:1007502103375; SANG EFT, 2000, P CONLL 2000 LLL 200; TJONG EF, 2001, P CONLL 2001	11	0	0	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						233	240						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500030
C	Dayan, P; Hausser, M; London, M		Thrun, S; Saul, K; Scholkopf, B		Dayan, P; Hausser, M; London, M			Plasticity kernels and temporal statistics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				TIMING-DEPENDENT PLASTICITY; SYNAPTIC PLASTICITY; OBJECT RECOGNITION; PYRAMIDAL CELLS; BARREL CORTEX; POTENTIATION; RAT; COINCIDENCE; HIPPOCAMPUS; PREDICTION	Computational mysteries surround the kernels relating the magnitude and sign of changes in efficacy as a function of the time difference between pre- and post-synaptic activity at a synapse. One Important idea(34) is that kernels result from filtering, ie an attempt by synapses to eliminate noise corrupting learning. This idea has hitherto been applied to trace learning rules; we apply it to experimentally-defined kernels, using it to reverse-engineer assumed signal statistics. We also extend it to consider the additional goal for filtering of weighting learning according to statistical surprise, as in the Z-score transform. This provides a fresh view of observed kernels and can lead to different, and more natural, signal statistics.	UCL, GCNU, London, England	University of London; University College London	Dayan, P (corresponding author), UCL, GCNU, Gower St, London, England.		London, Michael/B-4175-2010	London, Michael/0000-0001-5137-1707				Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453; Abbott LF, 1996, CEREB CORTEX, V6, P406, DOI 10.1093/cercor/6.3.406; ATICK JJ, 1992, NEURAL COMPUT, V4, P559, DOI 10.1162/neco.1992.4.4.559; Bell CC, 1997, NATURE, V387, P278, DOI 10.1038/387278a0; Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998; BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982; Blum KI, 1996, NEURAL COMPUT, V8, P85, DOI 10.1162/neco.1996.8.1.85; BUIATTI M, 2003, IN PRESS VISION RES; Cateau H, 2003, NEURAL COMPUT, V15, P597, DOI 10.1162/089976603321192095; CHECHIK G, 2003, IN PRESS NEURAL COMP; Debanne D, 1998, J PHYSIOL-LONDON, V507, P237, DOI 10.1111/j.1469-7793.1998.237bu.x; DONG DW, 1995, NETWORK-COMP NEURAL, V6, P159, DOI 10.1088/0954-898X/6/2/003; EDELMAN S, 1991, BIOL CYBERN, V64, P209, DOI 10.1007/BF00201981; Egger V, 1999, NAT NEUROSCI, V2, P1098, DOI 10.1038/16026; Fairhall AL, 2001, NATURE, V412, P787, DOI 10.1038/35090500; Feldman DE, 2000, NEURON, V27, P45, DOI 10.1016/S0896-6273(00)00008-8; Foldiak P, 1991, NEURAL COMPUT, V3, P194, DOI 10.1162/neco.1991.3.2.194; Froemke RC, 2002, NATURE, V416, P433, DOI 10.1038/416433a; Ganguly K, 2000, NAT NEUROSCI, V3, P1018, DOI 10.1038/79838; Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y; Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0; Gerstner W, 1997, J COMPUT NEUROSCI, V4, P79, DOI 10.1023/A:1008820728122; HRTDYNRT E, 1997, J COMPUTATIONAL NEUR, V4, P79; Hull C. L., 1943, PRINCIPLES BEHAV; LEVY WB, 1983, NEUROSCIENCE, V8, P791, DOI 10.1016/0306-4522(83)90010-6; Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213; MINAI AA, 1993, INNS WORLD C NEUR NE, P505; PAVLOV P, 1927, CONDITIONED REFLEXES; Porr B, 2003, NEURAL COMPUT, V15, P831, DOI 10.1162/08997660360581921; Rao RPN, 2001, NEURAL COMPUT, V13, P2221, DOI 10.1162/089976601750541787; Sjostrom PJ, 2001, NEURON, V32, P1149, DOI 10.1016/s0896-6273(01)00542-6; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; SUTTON RS, 1981, PSYCHOL REV, V88, P135, DOI 10.1037/0033-295X.88.2.135; van Rossum MCW, 2000, J NEUROSCI, V20, P8812; Wallis G, 1997, PROG NEUROBIOL, V51, P167, DOI 10.1016/S0301-0082(96)00054-8; Wallis G, 1997, NEURAL COMPUT, V9, P883, DOI 10.1162/neco.1997.9.4.883; YU AJ, 2003, NIPS 2002	37	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1303	1310						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500162
C	de Farias, DP; Megiddo, N		Thrun, S; Saul, K; Scholkopf, B		de Farias, DP; Megiddo, N			How to combine expert (or novice) advice when actions impact the environment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					The so-called "experts algorithms" constitute a methodology for choosing actions repeatedly, when the rewards depend both on the choice of action and on the unknown current state of the environment. An experts algorithm has access to a set of strategies ("experts"), each of which may recommend which action to choose. The algorithm learns how to combine the recommendations of individual experts so that, in the long run, for any fixed sequence of states of the environment, it does as well as the best expert would have done relative to the same sequence. This methodology may not be suitable for situations where the evolution of states of the environment depends on past chosen actions, as is usually the case, for example, in a repeated non-zero-sum game. A new experts algorithm is presented and analyzed in the context of repeated games. It is shown that asymptotically, under certain conditions, it performs as well as the best available expert. This algorithm is quite different from previously proposed experts algorithms. It represents a shift from the paradigms of regret minimization and myopic optimization to consideration of the long-term effect of a player's actions on the opponent's actions or the environment. The importance of this shift is demonstrated by the fact that this algorithm is capable of inducing cooperation in the repeated Prisoner's Dilemma game, whereas previous experts algorithms converge to the suboptimal non-cooperative play.	MIT, Dept Mech Engn, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	de Farias, DP (corresponding author), MIT, Dept Mech Engn, Cambridge, MA 02139 USA.	pucci@mit.edu; megiddo@almaden.ibm.com						Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488; Foster DP, 1999, GAME ECON BEHAV, V29, P7, DOI 10.1006/game.1999.0740; Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738; Fudenberg Drew, 1997, THEORY LEARNING GAME	5	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						815	822						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500102
C	Dunn, NA; Conery, JS; Lockery, SR		Thrun, S; Saul, K; Scholkopf, B		Dunn, NA; Conery, JS; Lockery, SR			Circuit optimization predicts dynamic networks for chemosensory orientation in the nematode Caenorhabditis elegans	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				C-ELEGANS; NEURONS; CHEMOTAXIS; MOTORNEURONS; SENSITIVITY; MEMBRANE; ASCARIS	The connectivity of the nervous system of the nematode Caenorhabditis elegans has been described completely, but the analysis of the neuronal basis of behavior in this system is just beginning. Here, we used an optimization algorithm to search for patterns of connectivity sufficient to compute the sensorimotor transformation underlying C. elegans chemotaxis, a simple form of spatial orientation behavior in which turning probability is modulated by the rate of change of chemical concentration. Optimization produced differentiator networks with inhibitory feedback among all neurons. Further analysis showed that feedback regulates the latency between sensory input and behavior. Common patterns of connectivity between the model and biological networks suggest new functions for previously identified connections in the C. elegans nervous system.	Univ Oregon, Dept Comp Sci, Eugene, OR 97403 USA	University of Oregon	Lockery, SR (corresponding author), Univ Oregon, Dept Comp Sci, Eugene, OR 97403 USA.							BARGMANN CI, 1991, NEURON, V7, P729, DOI 10.1016/0896-6273(91)90276-6; CHALFIE M, 1985, J NEUROSCI, V5, P956; DAVIS RE, 1989, J NEUROSCI, V9, P403; DAVIS RE, 1989, J NEUROSCI, V9, P415; DUSENBERY DB, 1980, J COMP PHYSL, V136, P127; Goodman MB, 1998, NEURON, V20, P763, DOI 10.1016/S0896-6273(00)81014-4; Kerr R, 2000, NEURON, V26, P583, DOI 10.1016/S0896-6273(00)81196-4; Lockery SR, 1998, METHOD ENZYMOL, V293, P201; MUNRO EE, 1994, NEURAL COMPUT, V6, P405, DOI 10.1162/neco.1994.6.3.405; Nickell WT, 2002, J MEMBRANE BIOL, V189, P55, DOI 10.1007/s00232-002-1004-x; Pierce-Shimomura JT, 1999, J NEUROSCI, V19, P9557; RUTHERFORD TA, 1979, J NEMATOL, V11, P232; WARD S, 1973, P NATL ACAD SCI USA, V70, P817, DOI 10.1073/pnas.70.3.817; WARD S, 1975, J COMP NEUROL, V160, P313, DOI 10.1002/cne.901600305; WHITE JG, 1986, PHILOS T R SOC B, V314, P1, DOI 10.1098/rstb.1986.0056; WHITE JG, 1985, TRENDS NEUROSCI, V8, P277, DOI 10.1016/0166-2236(85)90102-X	16	0	0	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1279	1286						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500159
C	Fischer, BJ; Anderson, CH		Thrun, S; Saul, K; Scholkopf, B		Fischer, BJ; Anderson, CH			A probabilistic model of auditory space representation in the barn owl	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				COCHLEAR NUCLEUS; BRAIN-STEM; TYTO-ALBA; LOCATION; MAP	The barn owl is a nocturnal hunter, capable of capturing prey using auditory information alone [1]. The neural basis for this localization behavior is the existence of auditory neurons with spatial receptive fields [2]. We provide a mathematical description of the operations performed on auditory input signals by the barn owl that facilitate the creation of a representation of auditory space. To develop our model, we first formulate the sound localization problem solved by the barn owl as a statistical estimation problem. The implementation of the solution is constrained by the known neurobiology.	Washington Univ, Dept Elect & Syst Engn, St Louis, MO 63110 USA	Washington University (WUSTL)	Fischer, BJ (corresponding author), Washington Univ, Dept Elect & Syst Engn, St Louis, MO 63110 USA.							BRAINARD MS, 1992, J ACOUST SOC AM, V91, P1015, DOI 10.1121/1.402627; CARR CE, 1990, J NEUROSCI, V10, P3227; DUDA RO, 1994, BINAURAL SPATIAL HEA, P49; Keller CH, 1998, HEARING RES, V118, P13, DOI 10.1016/S0378-5955(98)00014-8; KNUDSEN EI, 1978, SCIENCE, V200, P795, DOI 10.1126/science.644324; Koppl C, 1997, J NEUROPHYSIOL, V77, P364, DOI 10.1152/jn.1997.77.1.364; Lewicki MS, 2002, NAT NEUROSCI, V5, P356, DOI 10.1038/nn831; MARTIN KD, 1995, THESIS MIT; MAZER JA, 1995, THESIS CALTECH; PAYNE RS, 1971, J EXP BIOL, V54, P535; Pena JL, 2001, SCIENCE, V292, P249, DOI 10.1126/science.1059201; Schwartz O, 2001, NAT NEUROSCI, V4, P819, DOI 10.1038/90526; SLANEY M, 1994, 45 APPL COMP INC; SULLIVAN WE, 1984, J NEUROSCI, V4, P1787; TAKAHASHI T, 1986, J NEUROSCI, V6, P3413; TAKAHASHI TT, 1994, J NEUROSCI, V14, P4780; Yang LC, 1999, J NEUROSCI, V19, P2313	17	0	0	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1351	1358						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500168
C	Gardiol, NH; Kaelbling, LP		Thrun, S; Saul, K; Scholkopf, B		Gardiol, NH; Kaelbling, LP			Envelope-based planning in relational MDPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					A mobile robot acting in the world is faced with a large amount of sensory data and uncertainty in its action outcomes. Indeed, almost all interesting sequential decision-making domains involve large state spaces and large, stochastic action sets. We investigate a way to act intelligently as quickly as possible in domains where finding a complete policy would take a hopelessly long time. This approach, Relational Envelope-based Planning (REBP) tackles large, noisy problems along two axes. First, describing a domain as a relational MDP (instead of as an atomic or propositionally-factored MDP) allows problem structure and dynamics to be captured compactly with a small set of probabilistic, relational rules. Second, an envelope-based approach to planning lets an agent begin acting quickly within a restricted part of the full state space and to judiciously expand its envelope as resources permit.	MIT, AI Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Gardiol, NH (corresponding author), MIT, AI Lab, Cambridge, MA 02139 USA.							AVRIM L, 1999, 5 EUR C PLANN; Blum AL, 1997, ARTIF INTELL, V90, P281, DOI 10.1016/S0004-3702(96)00047-1; BOUTILIER C, 2001, IJCAI; DEAN T, 1995, ARTIFICIAL INTELLIGE, V76; DRIESSENS K, 2001, EUR C MACH LEARN; GAZEN BC, 1997, P EUR C PLANN ECP 97; GEFFNER H, 1998, FALL AAAI S COGN ROB; Guestrin C., 2003, P 18 INT JOINT C ART, P1003; HOEY J, 1999, 15 C UNC ART INT; KOEHLER J, 1997, P EUR C PLANN ECP 97; NEBEL B, 1997, P EUR C PLANN ECP 97; Weld D. S., 1999, AI Magazine, V20, P93; WELD DS, 1998, P AAAI 98; YOON SW, 2002, 18 INT C UNC ART INT	14	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						783	790						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500098
C	Graepel, T; Herbrich, R		Thrun, S; Saul, K; Scholkopf, B		Graepel, T; Herbrich, R			Semidefinite programming by perceptron learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We present a modified version of the perceptron learning algorithm (PLA) which solves semidefinite programs (SDPs) in polynomial time. The algorithm is based on the following three observations: (i) Semidefinite programs are linear programs with infinitely many (linear) constraints; (ii) every linear program can be solved by a sequence of constraint satisfaction problems with linear constraints; (iii) in general, the perceptron learning algorithm solves a constraint satisfaction problem with linear constraints in finitely many updates. Combining the PLA with a probabilistic rescaling algorithm (which, on average, increases the size of the feasable region) results in a probabilistic algorithm for solving SDPs that runs in polynomial time. We present preliminary results which demonstrate that the algorithm works, but is not competitive with state-of-the-art interior point methods.	Microsoft Res Ltd, Cambridge, England	Microsoft	Graepel, T (corresponding author), Microsoft Res Ltd, Cambridge, England.	thoreg@microsoft.com; rherb@microsoft.com		Shawe-Taylor, John/0000-0002-2030-0073				Borchers B, 1999, OPTIM METHOD SOFTW, V11-2, P683, DOI 10.1080/10556789908805769; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; DUNAGAN J, 2002, MSRTR0292; Glineur F, 1998, MEMOIRE DEA; Graepel T, 2002, LECT NOTES COMPUT SC, V2415, P694; GRAEPEL T, 2004, ADV NEURAL INFORMATI, V16; Grotschel M, 1988, ALGORITHMS COMBINATO, V2; HELMBERG C, 2000, ZR0034 KONR ZUS ZENT; Li Y., 2002, PROC ICML, V2, P379; Novikoff, 1962, P S MATH THEOR AUT, P615; PATAKI G, 1995, CONE LPS SEMIDEFINIT; ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519; TOH KC, 1996, TR1177 CORN U; Vandenberghe L, 1996, SIAM REV, V38, P49, DOI 10.1137/1038003	14	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						457	464						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500058
C	Hatano, K; Warmuth, MK		Thrun, S; Saul, K; Scholkopf, B		Hatano, K; Warmuth, MK			Boosting versus covering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We investigate improvements of AdaBoost that can exploit the fact that the weak hypotheses are one-sided, i.e. either all its positive (or negative) predictions are correct. In particular, for any set of m labeled examples consistent with a, disjunction of k literals (which are one-sided in this case), AdaBoost constructs a consistent hypothesis by using O(k(2) log m) iterations. On the other hand, a greedy set covering algorithm finds a consistent hypothesis of size O(k log m). Our primary question is whether there is a simple boosting algorithm that performs as well as the greedy set covering. We first show that InfoBoost, a, modification of Ada,Boost proposed by Aslam for a different purpose, does perform as well as the greedy set covering algorithm. We then show that Ada,Boost requires Omega(k(2) log m) iterations for learning k-literal disjunctions. We achieve this with an adversary construction and as well as in simple experiments based on artificial data. Further we give a variant called SemiBoost that can handle the degenerate case when the given examples all have the same label. We conclude by showing that SemiBoost can be used to produce small conjunctions as well.	Tokyo Inst Technol, Tokyo 152, Japan	Tokyo Institute of Technology	Hatano, K (corresponding author), Tokyo Inst Technol, Tokyo 152, Japan.							Aslam J. A., 2000, P 13 ANN C COMP LEAR, P200; FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Kivinen J., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P134, DOI 10.1145/307400.307424; Lafferty J., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P125, DOI 10.1145/307400.307422; NATARAJAN BK, 1991, MACHINE LEARNING; RATSCH G, 2002, P 15 ANN C COMP LEAR, P334; RAZBOROV AA, 1992, J COMPUTATION COMPLE, V1, P277; Rivest R. L., 1987, Machine Learning, V2, P229, DOI 10.1023/A:1022607331053; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901	10	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1109	1116						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500138
C	Ji, G; Bilmes, J		Thrun, S; Saul, K; Scholkopf, B		Ji, G; Bilmes, J			Necessary intransitive likelihood-ratio classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					In pattern classification tasks, errors are introduced because of differences between the true model and the one obtained via model estimation. Using likelihood-ratio based classification, it is possible to correct for this discrepancy by finding class-pair specific terms to adjust the likelihood ratio directly, and that can make class-pair preference relationships intransitive. In this work, we introduce new methodology that makes necessary corrections to the likelihood ratio, specifically those that are necessary to achieve perfect classification (but not perfect likelihood-ratio correction which can be overkill). The new corrections, while weaker than previously reported such adjustments, are analytically challenging since they involve discontinuous functions, therefore requiring several approximations. We test a number of these new schemes on an isolated-word speech recognition task as well as on the UCI machine learning data sets. Results show that by using the bias terms calculated in this new way, classification accuracy can substantially improve over both the baseline and over our previous results.	Univ Washington, Dept Elect Engn, SSLI Lab, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Ji, G (corresponding author), Univ Washington, Dept Elect Engn, SSLI Lab, Seattle, WA 98195 USA.							AO MM, 1987, MEASURE THEORY INTEG; BILMES J, 2001, NEURAL INFORMATION P; BILMES J, 1999, IEEE INT C AC SPEECH; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Jones D.S., 1966, GEN FUNCTIONS; Kevorkian J, 2000, PARTIAL DIFFERENTIAL; Murphy PM., 1995, UCI REPOSITORY MACHI; PESS W, 1992, NUMERICAL RECIPES C; PITRELLI J, 1995, IEEE INT C AC SPEECH; Raiffa H., 1957, GAMES DECISIONS INTR; Straffin P.D., 1993, GAME THEORY STRATEGY	13	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						537	544						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500068
C	Jun, SC; Pearlmutter, BA		Thrun, S; Saul, K; Scholkopf, B		Jun, SC; Pearlmutter, BA			Subject-independent magnetoencephalographic source localization by a multilayer perceptron	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				MEG; COMPONENTS; ACCURACY	We describe a system that localizes a single dipole to reasonable accuracy from noisy magnetoencephalographic (MEG) measurements in real time. At its core is a multilayer perceptron (MLP) trained to map sensor signals and head position to dipole location. Including head position overcomes the previous need to retrain the MLP for each subject and session. The training dataset was generated by mapping randomly chosen dipoles and head positions through an analytic model and adding noise from real MEG recordings. After training, a localization took 0.7 ms with an average error of 0.90 cm. A few iterations of a Levenberg-Marquardt routine using the MLP's output as its initial guess took 15 ms and improved the accuracy to 0.53 cm, only slightly above the statistical limits on accuracy imposed by the noise. We applied these methods to localize single dipole sources from MEG components isolated by blind source separation and compared the estimated locations to those generated by standard manually-assisted commercial software.	Los Alamos Natl Lab, Biol & Quantum Phys Grp, Los Alamos, NM 87545 USA	United States Department of Energy (DOE); Los Alamos National Laboratory	Jun, SC (corresponding author), Los Alamos Natl Lab, Biol & Quantum Phys Grp, MS-D454, Los Alamos, NM 87545 USA.	jschan@lanl.gov; barak@cs.may.ie	Pearlmutter, Barak A/M-8791-2014; JUN, SUNG/AAJ-2677-2020; Pearlmutter, Barak A./AAL-8999-2020	Pearlmutter, Barak A/0000-0003-0521-4553; JUN, SUNG/0000-0001-5357-4436; 				ABEYRATNE U R, 1991, Brain Topography, V4, P3, DOI 10.1007/BF01129661; AHONEN AI, 1993, PHYS SCRIPTA, VT49A, P198, DOI 10.1088/0031-8949/1993/T49A/033; HAMALAINEN M, 1993, REV MOD PHYS, V65, P413, DOI 10.1103/RevModPhys.65.413; Jun SC, 2003, IEEE T BIO-MED ENG, V50, P786, DOI 10.1109/TBME.2003.812154; Jun SC, 2002, PHYS MED BIOL, V47, P2547, DOI 10.1088/0031-9155/47/14/312; Kinouchi Y, 1996, BRAIN TOPOGR, V8, P317, DOI 10.1007/BF01184791; Kwon H, 2002, PHYS MED BIOL, V47, P4145, DOI 10.1088/0031-9155/47/23/302; Leahy RM, 1998, ELECTROEN CLIN NEURO, V107, P159, DOI 10.1016/S0013-4694(98)00057-1; Press WH, 1988, NUMERICAL RECIPES C; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Sander TH, 2002, IEEE T BIO-MED ENG, V49, P345, DOI 10.1109/10.991162; Tang AC, 2003, MAGNETIC SOURCE IMAGING OF THE HUMAN BRAIN, P159; Tang AC, 2002, NEURAL COMPUT, V14, P1827, DOI 10.1162/089976602760128036; Tang AC, 2000, ADV NEUR IN, V12, P185; Tang AC, 2000, NEUROCOMPUTING, V32, P1115, DOI 10.1016/S0925-2312(00)00286-1; Van Hoey G, 2000, PHYS MED BIOL, V45, P997, DOI 10.1088/0031-9155/45/4/314; Vigario R, 2000, IEEE T BIO-MED ENG, V47, P589, DOI 10.1109/10.841330	17	0	0	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						741	748						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500093
C	Liu, T; Moore, AW; Gray, A		Thrun, S; Saul, K; Scholkopf, B		Liu, T; Moore, AW; Gray, A			Efficient exact k-NN and nonparametric classification in high dimensions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					This paper is about non-approximate acceleration of high dimensional nonparametric operations such as k nearest neighbor classifiers and the prediction phase of Support Vector Machine classifiers. We attempt to exploit the fact that even if we want exact answers to nonparametric queries, we usually do not need to explicitly find the datapoints close to the query, but merely need to ask questions about the properties about that set of datapoints. This offers a small amount of computational leeway, and we investigate how much that leeway can be exploited. For clarity, this paper concentrates on pure k-NN classification and the prediction phase of SVMs. We introduce new ball tree algorithms that on real-world datasets give accelerations of 2-fold up to 100-fold compared against highly optimized traditional ball-tree-based k-NN. These results include datasets with up to 106 dimensions and 105 records, and show non-trivial speedups while giving exact answers.	Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Liu, T (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.	tingliu@cs.cmu.edu; awm@cs.cmu.edu; agray@cs.cmu.edu						Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348; BAY SD, 1999, UCI KDD ARCH; Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; CIACCIA P, 1997, P 23 VLDB INT C SEPT; DENG K, 1995, P 12 INT JOINT C ART, P1233; Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745; Gionis A, 1999, P 25 VLDB C; GRAY A, 2001, ADV NEURAL INFORMATI, V13; Guttman A., 1984, P 3 ACM SIGACT SIGMO; HAMMERSLEY JM, 1950, ANN MATH STAT, V21, P447, DOI 10.1214/aoms/1177729805; Joachims T., 1998, MAKING LARGE SCALE S; Moore A.W., 2000, UAI; Omohundro S. M., 1987, Complex Systems, V1, P273; OMOHUNDRO SM, 1991, ADV NEURAL INFORMATI, V3; PELLEG D, 1999, P 5 INT C KNOWL DISC; Preparata F.P., 1985, COMPUTATIONAL GEOMET, V1; UHLMANN JK, 1991, INFORM PROCESS LETT, V40, P175, DOI 10.1016/0020-0190(91)90074-R; Zheng WF, 2000, J CHEM INF COMP SCI, V40, P185, DOI 10.1021/ci980033m	18	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						265	272						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500034
C	Mezard, M		Thrun, S; Saul, K; Scholkopf, B		Mezard, M			Message passing in random satisfiability problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				TEMPERATURE; PHASE	This talk surveys the recent development of message passing procedures for solving constraint satisfaction problems. The cavity method from statistical physics provides a generalization of the belief propagation strategy that is able to deal with the clustering of solutions in these problems. It allows to derive analytic results on their phase diagrams, and offers a new algorithmic framework.	Univ Paris 11, Lab Phys Theor & Modeles Stat, CNRS, F-91405 Orsay, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay	Mezard, M (corresponding author), Univ Paris 11, Lab Phys Theor & Modeles Stat, CNRS, Batiment 100, F-91405 Orsay, France.	mezard@lptms.u-psud.fr						Biroli G, 2000, EUR PHYS J B, V14, P551, DOI 10.1007/s100510051065; BRAUNSTEIN A, CONDMAT0212451; Cook S.A., 1971, P 3 ANN ACM S THEORY, P151, DOI [10.1145/800157.805047, DOI 10.1145/800157.805047]; Crawford JM, 1996, ARTIF INTELL, V81, P31, DOI 10.1016/0004-3702(95)00046-1; DUBOIS O, 2000, P 11 ACM SIAM S DISC, P124; DUBOIS O, 2001, THEORET COMP SCI, V265; Friedgut E, 1999, J AM MATH SOC, V12, P1017, DOI 10.1090/S0894-0347-99-00305-7; KIRKPATRICK S, 1994, SCIENCE, V264, P1297, DOI 10.1126/science.264.5163.1297; Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572; Mezard M, 2003, SCIENCE, V301, P1685, DOI 10.1126/science.1086309; Mezard M, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.056126; Mezard M, 2003, J STAT PHYS, V111, P505, DOI 10.1023/A:1022886412117; Mezard M, 2003, J STAT PHYS, V111, P1, DOI 10.1023/A:1022221005097; Mezard M, 2002, SCIENCE, V297, P812, DOI 10.1126/science.1073287; Mezard M, 2001, EUR PHYS J B, V20, P217, DOI 10.1007/PL00011099; MEZARD M, 1987, SPIN GLASS THOERY; Monasson R, 1997, PHYS REV E, V56, P1357, DOI 10.1103/PhysRevE.56.1357; Monasson R, 1999, NATURE, V400, P133, DOI 10.1038/22055; Montanari A, 2003, EUR PHYS J B, V33, P339, DOI 10.1140/epjb/e2003-00174-7; Mulet R, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.268701; PARISI G, CONDMAT0308510; PARISI G, CSCC0301015; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; 2001, IEEE T INFO THEORY, V47	24	0	0	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1061	1068						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500132
C	Nguyen, XL; Jordan, MI		Thrun, S; Saul, K; Scholkopf, B		Nguyen, XL; Jordan, MI			On the concentration of expectation and approximate inference in layered networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We present an analysis of concentration-of-expectation phenomena in layered Bayesian networks that use generalized linear models as the local conditional probabilities. This framework encompasses a wide variety of probability distributions, including both discrete and continuous random variables. We utilize ideas from large deviation analysis and the delta method to devise and evaluate a class of approximate inference algorithms for layered Bayesian networks that have superior asymptotic error bounds and very fast computation time.	Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Nguyen, XL (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.		Jordan, Michael I/C-5253-2013					Barber D, 1999, J ARTIF INTELL RES, V10, P435, DOI 10.1613/jair.567; BARBER D, 1999, NIPS, V11; Brown L. D., 1986, FUNDAMENTALS STAT EX; HECKERMAN D, 1989, P UAI; JORDAN M, 1998, LEARNING GRAPHICAL M; KEARNS MJ, 1998, P UAI; KEARNS MJ, 1999, NIPS, V11; McCullagh P, 1989, GEN LINEAR MODELS, V2nd, DOI [10.1007/978-1-4899-3242-6, DOI 10.1007/978-1-4899-3242-6]; Minka T., 2001, P UAI; NG AY, 2000, NIPS, V12; PLEFKA T, 1982, J PHYS A, V15; YEDIDIA JS, 2001, NIPS, V13	12	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						393	400						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500050
C	Paninski, L		Thrun, S; Saul, K; Scholkopf, B		Paninski, L			Design of experiments via information theory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We discuss an idea for collecting data in a relatively efficient manner. Our point of view is Bayesian and information-theoretic: on any given trial, we want to adaptively choose the input in such a way that the mutual information between the (unknown) state of the system and the (stochastic) output is maximal, given any prior information (including data collected on any previous trials). We prove a theorem that quantifies the effectiveness of this strategy and give a few illustrative examples comparing the performance of this adaptive technique to that of the more usual nonadaptive experimental design. For example, we are able to explicitly calculate the asymptotic relative efficiency of the "staircase method" widely employed in psychophysics research, and to demonstrate the dependence of this efficiency on the form of the "psychometric function" underlying the output responses.	NYU, Ctr Neural Sci, New York, NY 10003 USA	New York University	Paninski, L (corresponding author), NYU, Ctr Neural Sci, New York, NY 10003 USA.							BERGER J, 1989, BAYESIAN STAT, V4, P35; Chichilnisky EJ, 2001, NETWORK-COMP NEURAL, V12, P199, DOI 10.1088/0954-898X/12/2/306; CLARKE BS, 1990, IEEE T INFORM THEORY, V36, P453, DOI 10.1109/18.54897; CLARKE BS, 1994, J STAT PLAN INFER, V41, P37, DOI 10.1016/0378-3758(94)90153-8; DEIGNAN P, 2000, ACC; Kontsevich LL, 1999, VISION RES, V39, P2729, DOI 10.1016/S0042-6989(98)00285-5; MASCARO M, 2002, UNPUB OPTIMIZED NEUR; Paninski L, 2003, NETWORK-COMP NEURAL, V14, P437, DOI 10.1088/0954-898X/14/3/304; PELLI D G, 1987, Investigative Ophthalmology and Visual Science, V28, P366; SCHOLL HR, 1998, SHANNON OPTIMAL PRIO; Talagrand M, 1995, PUBLICATIONS IHES, V81, P73; Van der Vaart AW, 1998, ASYMPTOTIC STAT; WATSON AB, 1990, PERCEPT PSYCHOPHYS, V47, P87, DOI 10.3758/BF03208169; WATSON AB, 1983, PERCEPT PSYCHOPHYS, V33, P113, DOI 10.3758/BF03202828	14	0	0	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1319	1326						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500164
C	Pineau, J; Gordon, G; Thrun, S		Thrun, S; Saul, K; Scholkopf, B		Pineau, J; Gordon, G; Thrun, S			Applying metric-trees to belief-point POMDPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Recent developments in grid-based and point-based approximation algorithms for POMDPs have greatly improved the tractability of POMDP planning. These approaches operate on sets of belief points by individually learning a value function for each point. In reality, belief points exist in a highly-structured metric simplex, but current POMDP algorithms do not exploit this property. This paper presents a new metric-tree algorithm which can be used in the context of POMDP planning to sort belief points spatially, and then perform fast value function updates over groups of points. We present results showing that this approach can reduce computation in point-based POMDP algorithms for a wide range of problems.	Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Pineau, J (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.			Salguero Tejada, Carlos/0000-0003-0930-9277				Brafman R, 1997, P 14 NAT C ART INT A, P727; Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745; Hauskrecht M, 2000, J ARTIF INTELL RES, V13, P33, DOI 10.1613/jair.678; Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X; MOORE AW, 1999, ADV NURAL INFORMATIO, V11; MOORE AW, 2000, CMURITR0005; Pineau J., 2003, INT JOINT C ART INT; Poon K.M., 2001, THESIS HONG KONG U S; POUPART P, 2003, ADV NEURAL INFORMATI, V15; ROY N, 2003, ADV NEURAL INFORMATI, V15; UHLMANN JK, 1991, INFORM PROCESS LETT, V40, P175, DOI 10.1016/0020-0190(91)90074-R; ZHOU R, 2001, P 1M INT JOINT C ART	12	0	0	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						759	766						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500095
C	Rudin, C; Daubechies, I; Schapire, RE		Thrun, S; Saul, K; Scholkopf, B		Rudin, C; Daubechies, I; Schapire, RE			On the dynamics of boosting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					In order to understand AdaBoost's dynamics, especially its ability to maximize margins, we derive an associated simplified nonlinear iterated map and analyze its behavior in low-dimensional cases. We find stable cycles for these cases, which can explicitly be used to solve for AdaBoost's output. By considering AdaBoost as a dynamical system, we are able to prove Ratsch and Warmuth's conjecture that AdaBoost may fail to converge to a maximal-margin combined classifier when given a 'nonoptimal' weak learning algorithm. AdaBoost is known to be a coordinate descent method, but other known algorithms that explicitly aim to maximize the margin (such as AdaBoost* and arc-gv) are not. We consider a differentiable function for which coordinate ascent will yield a maximum margin solution. We then make a simple approximation to derive a new boosting algorithm whose updates are slightly more aggressive than those of arc-gv.	Princeton Univ, Progr Appl & Comp Math, Princeton, NJ 08544 USA	Princeton University	Rudin, C (corresponding author), Princeton Univ, Progr Appl & Comp Math, Fine Hall,Washington Rd, Princeton, NJ 08544 USA.	crudin@math.princeton.edu; ingrid@math.princeton.edu; schapire@cs.princeton.edu	Daubechies, Ingrid/B-5886-2012					Breiman L, 1999, NEURAL COMPUT, V11, P1493, DOI 10.1162/089976699300016106; COLLINS M, 2002, MACHINE LEARNING, V48; RATSCH G, 2002, UNPUB J MACHINE LEAR; RATSCH G, 2002, P 15 ANN C COMP LEAR, P334; ROSSET S, 2003, BOOSTING REGULARIZED; SCHAPIRE RE, 1999, P 16 INT JOINT C ART; Schatten G, 1998, J LAW MED ETHICS, V26, P29, DOI 10.1111/j.1748-720X.1998.tb01903.x	7	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1101	1108						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500137
C	Ruiz-Correa, S; Shapiro, LG; Meila, M; Berson, G		Thrun, S; Saul, K; Scholkopf, B		Ruiz-Correa, S; Shapiro, LG; Meila, M; Berson, G			Discriminating deformable shape classes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We present and empirically test a novel approach for categorizing 3-D free form object shapes represented by range data. In contrast to traditional surface-signature based systems that use alignment to match specific objects, we adapted the newly introduced symbolic-signature representation to classify deformable shapes [10]. Our approach constructs an abstract description of shape classes using an ensemble of classifiers that learn object class parts and their corresponding geometrical relationships from a set of numeric and symbolic descriptors. We used our classification engine in a series of large scale discrimination experiments on two well-defined classes that share many common distinctive features. The experimental results suggest that our method outperforms traditional numeric signature-based methodologies.	Univ Washington, Dept Elect Engn, Seattle, WA 98105 USA	University of Washington; University of Washington Seattle	Ruiz-Correa, S (corresponding author), Univ Washington, Dept Elect Engn, Seattle, WA 98105 USA.			Ruiz-Correa, Salvador/0000-0002-2918-6780				Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279; GOLLAND P, 2001, ADV NEURAL INFORM PR, P745; HAMMOND P, 2001, INTELLIGENT DATA ANA; Heisele B, 2002, ADV NEUR IN, V14, P1239; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; JONES KL, 1999, SMITHS RECOGNIZABLE; MARTIN J, 1998, IEEE T PATTER ANAL M, V2; MEDIN D, 1999, MIT ENCY COGNITIVE S; Osada R, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P154, DOI 10.1109/SMA.2001.923386; RUIZCORREA S, P IEEE COMP SOC INT, V2, P1126; RUIZCORREA S, P IEEE C COMP VIS PA, V1, P769; SCHOLKOPF B, 2002, LEARNING KERNALS	12	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1491	1498						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500185
C	Sahani, M; Nagarajan, SS		Thrun, S; Saul, K; Scholkopf, B		Sahani, M; Nagarajan, SS			Reconstructing MEG sources with unknown correlations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				MAGNETOENCEPHALOGRAPHY	Existing source location and recovery algorithms used in magnetoencephalographic imaging generally assume that the source activity at different brain locations is independent or that the correlation structure is known. However, electrophysiological recordings of local field potentials show strong correlations in aggregate activity over significant distances. Indeed, it seems very likely that stimulus-evoked activity would follow strongly correlated time-courses in different brain areas. Here, we present, and validate through simulations, a new approach to source reconstruction in which the correlation between sources is modelled and estimated explicitly by variational Bayesian methods, facilitating accurate recovery of source locations and the time-courses of their activation.	Univ Calif San Francisco, WM Keck Fdn, Ctr Integrat Neurosci, San Francisco, CA 94143 USA	University of California System; University of California San Francisco	Sahani, M (corresponding author), Univ Calif San Francisco, WM Keck Fdn, Ctr Integrat Neurosci, San Francisco, CA 94143 USA.	maneesh@phy.ucsf.edu; sri@radiology.ucsf.edu						ATTIAS H, 2000, ADV NEURAL INFO PROC, V12; Baillet S, 2001, IEEE SIGNAL PROC MAG, V18, P14, DOI 10.1109/79.962275; Bernasconi C, 2000, NEUROREPORT, V11, P689, DOI 10.1097/00001756-200003200-00007; David O, 2002, IEEE T BIO-MED ENG, V49, P975, DOI 10.1109/TBME.2002.802013; GHAHRAMANI S, 2000, ADV NEURAL INFO PROC, V12; HAMALAINEN M, 1993, REV MOD PHYS, V65, P413, DOI 10.1103/RevModPhys.65.413; Horn R.A., 2013, TOPICS MATRIX ANAL, DOI DOI 10.1017/CBO9780511840371; MacKay DJC, 1994, ASHRAE T, V100, P1053; Phillips C, 2002, NEUROIMAGE, V16, P678, DOI 10.1006/nimg.2002.1143; Rodriguez E, 1999, NATURE, V397, P430, DOI 10.1038/17120; SAHANI M, 2003, ADV NEURAL INFO PROC, V15; Sekihara K, 2002, IEEE T BIO-MED ENG, V49, P1534, DOI 10.1109/TBME.2002.805485; Sekihara K, 1996, IEEE T BIO-MED ENG, V43, P281, DOI 10.1109/10.486285; Sekihara K, 2001, IEEE T BIO-MED ENG, V48, P760, DOI 10.1109/10.930901; Tang AC, 2002, NEURAL COMPUT, V14, P1827, DOI 10.1162/089976602760128036	15	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						693	700						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500087
C	Srebro, N; Jaakkola, T		Thrun, S; Saul, K; Scholkopf, B		Srebro, N; Jaakkola, T			Linear dependent dimensionality reduction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We formulate linear dimensionality reduction as a semi-parametric estimation problem, enabling us to study its asymptotic behavior. We generalize the problem beyond additive Gaussian noise to (unknown) nonGaussian additive noise, and to unbiased non-additive models.	MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Srebro, N (corresponding author), MIT, Dept Elect Engn & Comp Sci, 77 Massachusetts Ave, Cambridge, MA 02139 USA.							Alter O, 2000, P NATL ACAD SCI USA, V97, P10101, DOI 10.1073/pnas.97.18.10101; ANDERSON TW, 1956, 3RD P BERK S MATH ST, V5, P111; AZAR Y, 2001, 33 ACM S THEOR COMP; COLLINS M, 2002, ADV NEURAL INFORMATI, V14; GORDON G, 2003, ADV NEURAL INFORMATI, V15; IRANI M, 2000, 6 EUR C COMP VIS; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Srebro N., 2003, 20 INT C MACH LEARN; Stewart G., 1990, MATRIX PERTURBATION; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; WAINWRIGHT MJ, 2000, ADV NEURAL INFORMATI, V12	11	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						145	152						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500019
C	Srivastava, A; Liu, XW; Mio, W; Klassen, E		Thrun, S; Saul, K; Scholkopf, B		Srivastava, A; Liu, XW; Mio, W; Klassen, E			A computational geometric approach to shape analysis in images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We present a geometric approach to statistical shape analysis of closed curves in images. The basic idea is to specify a space of closed curves satisfying given constraints, and exploit the differential geometry of this space to solve optimization and inference problems. We demonstrate this approach by: (i) defining and computing statistics of observed shapes, (ii) defining and learning a parametric probability model on shape space, and (iii) designing a binary hypothesis test on this space.	Florida State Univ, Dept Stat, Tallahassee, FL 32306 USA	State University System of Florida; Florida State University	Srivastava, A (corresponding author), Florida State Univ, Dept Stat, Tallahassee, FL 32306 USA.		Srivastava, Anuj/L-4705-2019; Srivastava, Anuj/F-7417-2011					Dryden I.L., 1998, STAT SHAPE ANAL, DOI [DOI 10.5555/1046920.1088707, 10.1002/9781119072492]; Duta N, 1999, LECT NOTES COMPUT SC, V1613, P370; KARCHER H, 1977, COMMUN PUR APPL MATH, V30, P509, DOI 10.1002/cpa.3160300502; KLASSEN E, 2004, IN PRESS IEEE PATTER, V26; Le HL, 2001, ADV APPL PROBAB, V33, P324, DOI 10.1017/S0001867800010818; MIO W, 2003, IN PRESS Q APPL MATH; Mumford D, 1994, ELASTICA COMPUTER VI, P491; Romeny B.M., 1994, GEOMETRY DRIVEN DIFF; Sebastian TB, 2003, IEEE T PATTERN ANAL, V25, P116, DOI 10.1109/TPAMI.2003.1159951; Sethian JA, 1996, LEVEL SET METHODS EV; Sharon E, 2000, IEEE T PATTERN ANAL, V22, P1117, DOI 10.1109/34.879792; Younes L, 1999, IMAGE VISION COMPUT, V17, P381, DOI 10.1016/S0262-8856(98)00125-5	12	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1579	1586						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500196
C	Suzuki, J; Sasaki, Y; Maeda, E		Thrun, S; Saul, K; Scholkopf, B		Suzuki, J; Sasaki, Y; Maeda, E			Kernels for structured natural language data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					This paper devises a novel kernel function for structured natural language data. In the field of Natural Language Processing, feature extraction consists of the following two steps: (1) syntactically and semantically analyzing raw data, i.e., character strings, then representing the results as discrete structures, such as parse trees and dependency graphs with part-of-speech tags; (2) creating (possibly high-dimensional) numerical feature vectors from the discrete structures. The new kernels, called Hierarchical Directed Acyclic Graph (HDAG) kernels, directly accept DAGs whose nodes can contain DAGs. HDAG data structures are needed to fully reflect the syntactic and semantic structures that natural language data inherently have. In this paper, we define the kernel function and show how it permits efficient calculation. Experiments demonstrate that the proposed kernels are superior to existing kernel functions, e.g., sequence kernels, tree kernels, and bag-of-words kernels.	NTT Corp, NTT Commun Sci Labs, Kyoto 6190237, Japan	Nippon Telegraph & Telephone Corporation	Suzuki, J (corresponding author), NTT Corp, NTT Commun Sci Labs, 2-4 Hikaridai, Kyoto 6190237, Japan.		Suzuki, Jun/AAW-4369-2021	Suzuki, Jun/0000-0003-2108-1340				[Anonymous], 1999, UCSCRL9910; Cancedda N, 2003, J MACH LEARN RES, V3, P1059, DOI 10.1162/153244303322533197; Collins M., 2001, UCSCRL0110; COLLINS MJ, 2001, P NEUR INF PROC SYST; Fellbaum Christiane, 1998, WORDNET ELECT DATABA; IKEHARA S, 1997, GOI TAIKEI JAPANESE, V1; Joachims T., 1998, P EUROPEAN C MACHINE, P137, DOI [10.1007/bfb0026683, 10.1007/BFb0026683]; Li XM, 2002, POWERCON 2002: INTERNATIONAL CONFERENCE ON POWER SYSTEM TECHNOLOGY, VOLS 1-4, PROCEEDINGS, P556, DOI 10.1109/ICPST.2002.1053604; Lodhi H, 2002, J MACH LEARN RES, V2, P419, DOI 10.1162/153244302760200687; SUZUKI J, 2003, WORKSH MULT SUMM QUE, P61; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; WATKINS C, 1999, CSDTR9811 ROYAL HOLL	12	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						643	650						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500081
C	Vermaak, J; Godsill, SJ; Doucet, A		Thrun, S; Saul, K; Scholkopf, B		Vermaak, J; Godsill, SJ; Doucet, A			Sequential Bayesian kernel regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				RELEVANCE VECTOR MACHINE	We propose a method for sequential Bayesian kernel regression. As is the case for the popular Relevance Vector Machine (RVM) [10, 11], the method automatically identifies the number and locations of the kernels. Our algorithm overcomes some of the computational difficulties related to batch methods for kernel regression. It is non-iterative, and requires only a single pass over the data. It is thus applicable to truly sequential data sets and batch data sets alike. The algorithm is based on a generalisation of Importance Sampling, which allows the design of intuitively simple and efficient proposal distributions for the model parameters. Comparative results on two standard data sets show our algorithm to compare favourably with existing batch estimation strategies.	Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England	University of Cambridge	Vermaak, J (corresponding author), Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.			Doucet, Arnaud/0000-0002-7662-419X				Bishop C.M., 2000, 16 C UNCERTAINTY ART, P46; Crisan D, 2001, STAT ENG IN, P17; DELMORAL P, 2002, CUEDFINFENGTR443; Doucet A., 2001, SEQUENTIAL MONTE CAR; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; Kitagawa Genshiro, 2021, J COMPUT GRAPH STAT, V5, P1, DOI [DOI 10.2307/1390750, 10.2307/1390750]; Neal R. M., 1998, Neural Networks and Machine Learning. Proceedings, P97; Tham S. S., 2002, P INT C MACH LEARN, P634; Tipping E., 2003, P 9 INT WORKSH ART I; Tipping ME, 2000, ADV NEUR IN, V12, P652; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Vapnik V.N, 1998, STAT LEARNING THEORY	13	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						113	120						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500015
C	Wang, XF; Sandholm, L		Thrun, S; Saul, K; Scholkopf, B		Wang, XF; Sandholm, L			Learning near-pareto-optimal conventions in polynomial time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We study how to learn to play a Pareto-optimal strict Nash equilibrium when there exist multiple equilibria and agents may have different preferences among the equilibria. We focus on repeated coordination games of non-identical interest where agents do not know the game structure up front and receive noisy payoffs. We design efficient near-optimal algorithms for both the perfect monitoring and the imperfect monitoring setting(where the agents only observe their own payoffs and the joint actions).	Carnegie Mellon Univ, Dept ECE, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Wang, XF (corresponding author), Carnegie Mellon Univ, Dept ECE, Pittsburgh, PA 15213 USA.							BELLARE, 1996, P 1 ACM ANN C COMP C; BOUTILIER, 1996, TARK; BRAFMAN, 2001, IJCAI; CLAUS, 1998, AAAI; FIECHTER, 1994, COLT; FUDENBERG, 1998, THEORY LEARNING GAME; GREENWALD, 2002, AAAI SPRING S; HU, 1998, ICML; KAELBLING, 1996, JAIR; KEARNS, 1998, ICML; LITTMAN, 2000, J COGNITIVE SYSTEM R, V2, P55; LITTMAN, 2001, ICML; LOVASZ, 1995, ELECT J COMBINATORIC; PIVAZYAN, 2002, AAAI; WANG, 2002, NIPS; YOUNG, J EC THEORY, V59, P93; YOUNG, 1993, ECONOMETRICA, V61, P57	17	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						863	870						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500108
C	Wyss, R; Verschure, PFMJ		Thrun, S; Saul, K; Scholkopf, B		Wyss, R; Verschure, PFMJ			Bounded invariance and the formation of place fields	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				HIPPOCAMPUS; CELLS; RAT; NAVIGATION	One current explanation of the view independent representation of space by the place-cells of the hippocampus is that they arise out of the summation of view dependent Gaussians. This proposal assumes that visual representations show bounded invariance. Here we investigate whether a recently proposed visual encoding scheme called the temporal population code can provide such representations. Our analysis is based on the behavior of a simulated robot in a virtual environment containing specific visual cues. Our results show that the temporal population code provides a representational substrate that can naturally account for the formation of place fields.	Univ Zurich, ETH, Inst Neuroinformat, Zurich, Switzerland	ETH Zurich; University of Zurich	Wyss, R (corresponding author), Univ Zurich, ETH, Inst Neuroinformat, Zurich, Switzerland.		Verschure, Paul/AAR-2340-2021	Verschure, Paul/0000-0003-3643-9544				Arleo A, 2000, BIOL CYBERN, V83, P287, DOI 10.1007/s004220000171; Burgess N, 1997, PHILOS T ROY SOC B, V352, P1535, DOI 10.1098/rstb.1997.0140; HUGHES A, 1979, VISION RES, V19, P569, DOI 10.1016/0042-6989(79)90143-3; KNIERIM JJ, 1995, J NEUROSCI, V15, P1648, DOI 10.1523/JNEUROSCI.15-03-01648.1995; O'Keefe J, 1998, PHILOS T ROY SOC B, V353, P1333, DOI 10.1098/rstb.1998.0287; O'Keefe J, 1987, HIPPOCAMPUS COGNITIV; OKEEFE J, 1971, BRAIN RES, V34, P171, DOI 10.1016/0006-8993(71)90358-1; OKeefe J, 1996, NATURE, V381, P425, DOI 10.1038/381425a0; QUIRK G, 1995, J NEURSCI, V10, P2008; Wyss R, 2003, P NATL ACAD SCI USA, V100, P324, DOI 10.1073/pnas.0136977100; Zhang KC, 1998, J NEUROPHYSIOL, V79, P1017, DOI 10.1152/jn.1998.79.2.1017	11	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1483	1490						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500184
C	Yu, Z; Mason, SG; Birch, GE		Thrun, S; Saul, K; Scholkopf, B		Yu, Z; Mason, SG; Birch, GE			Impact of an energy normalization transform on the performance of the LF-ASD brain computer interface	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					This paper presents an energy normalization transform as a method to reduce system errors in the LF-ASD brain-computer interface. The energy normalization transform has two major benefits to the system performance. First, it can increase class separation between the active and idle EEG data. Second, it can desensitize the system to the signal amplitude variability. For four subjects in the study, the benefits resulted in the performance improvement of the LF-ASD in the range from 7.7% to 18.9%, while for the fifth subject, who had the highest non-normalized accuracy of 90.5%, the performance did not change notably with normalization.	Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada	University of British Columbia	Yu, Z (corresponding author), Univ British Columbia, Dept Elect & Comp Engn, 2356 Main Mall, Vancouver, BC V6T 1Z4, Canada.							GREEN DM, 1996, SIGNAL DETECTION THE; Jasper H., 1949, ARCH PSYCHIAT NERVEN, V83, P163, DOI [10.1007/BF01062488, DOI 10.1007/BF01062488]; Mason SG, 2000, IEEE T BIO-MED ENG, V47, P1297, DOI 10.1109/10.871402; MASON SG, 1997, THESIS UBC; Pfurtscheller G, 1997, ELECTROEN CLIN NEURO, V103, P642, DOI 10.1016/S0013-4694(97)00080-1; Vaughan T M, 1996, IEEE Trans Rehabil Eng, V4, P425, DOI 10.1109/86.547945	6	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						725	732						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500091
C	Zhang, T		Thrun, S; Saul, K; Scholkopf, B		Zhang, T			An infinity-sample theory for multi-category large margin classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					The purpose of this paper is to investigate infinity-sample properties of risk minimization based multi-category classification methods. These methods can be considered as natural extensions to binary large margin classification. We establish conditions that guarantee the infinity-sample consistency of classifiers obtained in the risk minimization framework. Examples are provided for two specific forms of the general formulation, which extend a number of known methods. Using these examples, we show that some risk minimization formulations can also be used to obtain conditional probability estimates for the underlying problem. Such conditional probability information will be useful for statistical inferencing tasks beyond classification.	IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Zhang, T (corresponding author), IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.	tzhang@watson.ibm.com						BARTLETT PL, 2003, 638 U CAL STAT DEP; DESYATNIKOV I, 2003, COLT; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; JIANG W, 2004, ANN STAT, V32; LEE Y, 2002, IN PRESS J AM STAT A; Lin Y, 2002, DATA MIN KNOWL DISC, V6, P259, DOI 10.1023/A:1015469627679; LUGOSI G, 2004, ANN STAT, V32; Mannor S., 2003, J MACHINE LEARNING R, V4, P713; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901; Steinwart I, 2002, J COMPLEXITY, V18, P768, DOI 10.1006/jcom.2002.0642; ZHANG T, 2004, ANN STAT, V32	11	0	0	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1077	1084						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500134
C	Ascoli, GA; Samsonovich, A		Dietterich, TG; Becker, S; Ghahramani, Z		Ascoli, GA; Samsonovich, A			Bayesian morphometry of hippocampal cells suggests same-cell somatodendritic repulsion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				PARSIMONIOUS DESCRIPTION; DENDRITIC MORPHOLOGY	Visual inspection of neurons suggests that dendritic orientation may be determined both by internal constraints (e.g. membrane tension) and by external vector fields (e.g. neurotrophic gradients). For example, basal dendrites of pyramidal cells appear nicely fan-out. This regular orientation is hard to justify completely with a general tendency to grow straight, given the zigzags observed experimentally. Instead, dendrites could (A) favor a fixed ("external") direction, or (B) repel from their own soma. To investigate these possibilities quantitatively, reconstructed hippocampal cells were subjected to Bayesian analysis. The statistical model combined linearly factors A and B, as well as the tendency to grow straight. For all morphological classes, B was found to be significantly positive and consistently greater than A. In addition, when dendrites were artificially re-oriented according to this model, the resulting structures closely resembled real morphologies. These results suggest that somatodendritic repulsion may play a role in determining dendritic orientation. Since hippocampal cells are very densely packed and their dendritic trees highly overlap, the repulsion must be cell-specific. We discuss possible mechanisms underlying such specificity.	George Mason Univ, Krasnow Inst Adv Study, Fairfax, VA 22030 USA	George Mason University	Ascoli, GA (corresponding author), George Mason Univ, Krasnow Inst Adv Study, Fairfax, VA 22030 USA.		Samsonovich, Alexei/M-5778-2016	Samsonovich, Alexei/0000-0003-4788-4408				ASCOLI G, 2001, IN PRESS PHIL T R B; Ascoli GA, 1999, ANAT REC, V257, P195, DOI 10.1002/(SICI)1097-0185(19991215)257:6<195::AID-AR5>3.0.CO;2-H; Ascoli GA, 2000, NEUROCOMPUTING, V32, P1003, DOI 10.1016/S0925-2312(00)00272-1; BURKE RE, 1992, J NEUROSCI, V12, P2403; Cannon RC, 1998, J NEUROSCI METH, V84, P49, DOI 10.1016/S0165-0270(98)00091-0; ISHIZUKA N, 1995, J COMP NEUROL, V362, P17, DOI 10.1002/cne.903620103; RIHN LL, 1990, DEV BRAIN RES, V54, P115, DOI 10.1016/0165-3806(90)90071-6; vanPelt J, 1997, J THEOR BIOL, V186, P17, DOI 10.1006/jtbi.1996.0341	8	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						133	139						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100017
C	Beintema, JA; van den Berg, AV; Lappe, M		Dietterich, TG; Becker, S; Ghahramani, Z		Beintema, JA; van den Berg, AV; Lappe, M			Receptive field structure of flow detectors for heading perception	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				OPTIC FLOW; RESPONSE SELECTIVITY; MST NEURONS; MOTION; MECHANISMS; SENSITIVITY; STIMULI; MONKEY	Observer translation relative to the world creates image flow that expands from the observer's direction of translation (heading) from which the observer can recover heading direction. Yet, the image flow is often more complex, depending on rotation of the eye, scene layout and translation velocity. A number of models [1-4] have been proposed on how the human visual system extracts heading from flow in a neurophysiologically plausible way. These models represent heading by a set of neurons that respond to large image flow patterns and receive input from motion sensed at different image locations. We analysed these models to determine the exact receptive field of these heading detectors. We find most models predict that, contrary to widespread believe, the contributing motion sensors have a preferred motion directed circularly rather than radially around the detector's preferred heading. Moreover, the results suggest to look for more refined structure within the circular flow, such as bi-circularity or local motion-opponency.	Ruhr Univ Bochum, Dept Zool & Neurobiol, D-44780 Bochum, Germany	Ruhr University Bochum	Beintema, JA (corresponding author), Ruhr Univ Bochum, Dept Zool & Neurobiol, D-44780 Bochum, Germany.							ALLMAN J, 1985, ANNU REV NEUROSCI, V8, P407, DOI 10.1146/annurev.ne.08.030185.002203; Beintema JA, 1998, VISION RES, V38, P2155, DOI 10.1016/S0042-6989(97)00428-8; DUFFY CJ, 1991, J NEUROPHYSIOL, V65, P1346, DOI 10.1152/jn.1991.65.6.1346; DUFFY CJ, 1991, J NEUROPHYSIOL, V65, P1329, DOI 10.1152/jn.1991.65.6.1329; HEEGER DJ, 1992, INT J COMPUT VISION, V7, P95, DOI 10.1007/BF00128130; LAPPE M, 1993, NEURAL COMPUT, V5, P374, DOI 10.1162/neco.1993.5.3.374; Lappe M, 1996, J NEUROSCI, V16, P6265; Lappe M, 1993, ADV NEURAL INF PROCE, V6, P433; PERRONE JA, 1994, VISION RES, V34, P2917, DOI 10.1016/0042-6989(94)90060-4; Royden CS, 1997, J OPT SOC AM A, V14, P2128, DOI 10.1364/JOSAA.14.002128; TANAKA K, 1989, J NEUROPHYSIOL, V62, P626, DOI 10.1152/jn.1989.62.3.626	11	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						149	156						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100019
C	Bilmes, J; Ji, G; Mela, M		Dietterich, TG; Becker, S; Ghahramani, Z		Bilmes, J; Ji, G; Mela, M			Intransitive likelihood-ratio classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					In this work, we introduce an information-theoretic based correction term to the likelihood ratio classification method for multiple classes. Under certain conditions, the term is sufficient for optimally correcting the difference between the true and estimated likelihood ratio, and we analyze this in the Gaussian case. We find that the new correction term significantly improves the classification results when tested on medium vocabulary speech recognition tasks. Moreover, the addition of this term makes the class comparisons analogous to an intransitive game and we therefore use several tournament-like strategies to deal with this issue. We find that further small improvements are obtained by using an appropriate tournament. Lastly, we find that intransitivity appears to be a good measure of classification confidence.	Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Bilmes, J (corresponding author), Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.							Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; Duda R.O., 2000, PATTERN CLASSIFICATI; PITRELLI J, 1995, P IEEE INT C AC SPEE; Straffin P.D., 1993, GAME THEORY STRATEGY; [No title captured]	5	0	0	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1141	1148						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100142
C	Braun, ML; Buhmann, JM		Dietterich, TG; Becker, S; Ghahramani, Z		Braun, ML; Buhmann, JM			The noisy Euclidean traveling salesman problem and learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We consider noisy Euclidean traveling salesman problems in the plane, which are random combinatorial problems with underlying structure. Gibbs sampling is used to compute average trajectories, which estimate the underlying structure common to all instances. This procedure requires identifying the exact relationship between permutations and tours. In a learning setting, the average trajectory is used as a model to construct solutions to new instances sampled from the same source. Experimental results show that the average trajectory can in fact estimate the underlying structure and that overfitting effects occur if the trajectory adapts too closely to a single instance.	Univ Bonn, Inst Comp Sci, Dept 3, D-53117 Bonn, Germany	University of Bonn	Braun, ML (corresponding author), Univ Bonn, Inst Comp Sci, Dept 3, Romerstr 164, D-53117 Bonn, Germany.		Buhmann, Joachim/AAU-4760-2020					BUHMANN JM, 1999, ADV NEURAL INFORMATI, V12, P216; DURBIN R, 1987, NATURE, V326, P689, DOI 10.1038/326689a0; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; LIN S, 1973, OPER RES, V21, P498, DOI 10.1287/opre.21.2.498; Simic PD, 1990, NETWORK-COMP NEURAL, V1, P89, DOI 10.1088/0954-898X/1/1/007; [No title captured]	6	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						351	358						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100044
C	Brown, TX		Dietterich, TG; Becker, S; Ghahramani, Z		Brown, TX			Switch packet arbitration via queue-learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					In packet switches, packets queue at switch inputs and contend for outputs. The contention arbitration policy directly affects switch performance. The best policy depends on the current state of the switch and current traffic patterns. This problem is hard because the state space, possible transitions, and set of actions all grow exponentially with the size of the switch. We present a reinforcement learning formulation of the problem that decomposes the value function into many small independent value functions and enables an efficient action selection.	Univ Colorado, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Brown, TX (corresponding author), Univ Colorado, Boulder, CO 80309 USA.	timxb@colorado.edu						Boyan J. A., 1994, P INT C ADV NEURAL I, P671; BROWN TX, 1990, IEEE J SEL AREA COMM, V8, P1428, DOI 10.1109/49.62821; BROWN TX, 2001, UNPUB COMPUTER NETWO; BROWN TX, 1999, ADV NIPS, V11; GABOR Z, 1998, INT C MACH LEARN MAD; Hopcroft J. E., 1973, SIAM Journal on Computing, V2, P225, DOI 10.1137/0202019; Marbach P, 2000, IEEE J SEL AREA COMM, V18, P197, DOI 10.1109/49.824797; MCKEOWN N, 1996, P IEEE INFOCOM 9L; Park YK, 1997, IEEE J SEL AREA COMM, V15, P261, DOI 10.1109/49.552075; Pattavina A., 1998, SWITCHING THEORY ARC; Singh S, 1997, ADV NEUR IN, V9, P974; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tarjan R. E, 1983, DATA STRUCTURES NETW	13	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1337	1344						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100166
C	Cadez, IV; Bradley, PS		Dietterich, TG; Becker, S; Ghahramani, Z		Cadez, IV; Bradley, PS			Model based population tracking and automatic detection of distribution changes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				NOVELTY DETECTION	Probabilistic mixture models are used for a broad range of data analysis tasks such as clustering, classification, predictive modeling, etc. Due to their inherent probabilistic nature, mixture models can easily be combined with other probabilistic or non-probabilistic techniques thus forming more complex data analysis systems. In the case of online data (where there is a stream of data available) models can be constantly updated to reflect the most current distribution of the incoming data. However, in many business applications the models themselves represent a parsimonious summary of the data and therefore it is not desirable to change models frequently, much less with every new data point. In such a framework it becomes crucial to track the applicability of the mixture model and detect the point in time when the model fails to adequately represent the data. In this paper we formulate the problem of change detection and propose a principled solution. Empirical results over both synthetic and real-life data sets are presented.	Univ Calif Irvine, Dept Informat & Comp Sci, Irvine, CA 92612 USA	University of California System; University of California Irvine	Cadez, IV (corresponding author), Univ Calif Irvine, Dept Informat & Comp Sci, Irvine, CA 92612 USA.	icadez@ics.uci.edu; paulb@digimine.com						Barnett V., 1984, WILEY SERIES PROBABI; BRUCE AG, 1996, P 3 INT C NEUR NETS, P564; Cades I, 2001, P 7 ACM SIGKDD INT C, P37; Campbell C, 2001, ADV NEUR IN, V13, P395; Fawcett T., 1999, P 5 ACM SIGKDD INT C, P53, DOI [10.1145/312129.312195, DOI 10.1016/J.EC0LENG.2010.11.031]; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; Scholkopf B, 2000, ADV NEUR IN, V12, P582	8	0	0	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1345	1352						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100167
C	Cadez, IV; Smyth, P		Dietterich, TG; Becker, S; Ghahramani, Z		Cadez, IV; Smyth, P			Bayesian predictive profiles with applications to retail transaction data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Massive transaction data sets are recorded in a routine manner in telecommunications, retail commerce, and Web site management. In this paper we address the problem of inferring predictive individual profiles from such historical transaction data. We describe a generative mixture model for count data and use an an approximate Bayesian estimation framework that effectively combines an individual's specific history with more general population patterns. We use a large real-world retail transaction data set to illustrate how these profiles consistently outperform non-mixture and non-Bayesian techniques in predicting customer behavior in out-of-sample data.	Univ Calif Irvine, Irvine, CA 92697 USA	University of California System; University of California Irvine	Cadez, IV (corresponding author), Univ Calif Irvine, Irvine, CA 92697 USA.	icadez@ics.uci.edu; smyth@ics.uci.edu		Smyth, Padhraic/0000-0001-9971-8378				Agrawal R., 1993, SIGMOD Record, V22, P207, DOI 10.1145/170036.170072; Strehl A, 2000, PROC SPIE, V4057, P33, DOI 10.1117/12.381756; [No title captured]; [No title captured]	4	0	0	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1353	1360						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100168
C	Campbell, WM		Dietterich, TG; Becker, S; Ghahramani, Z		Campbell, WM			A sequence kernel and its application to speaker recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					A novel approach for comparing sequences of observations using an explicit-expansion kernel is demonstrated. The kernel is derived using the assumption of the independence of the sequence of observations and a mean-squared error training criterion. The use of an explicit expansion kernel reduces classifier model size and computation dramatically, resulting in model sizes and computation one-hundred times smaller in our application. The explicit expansion also preserves the computational advantages of an earlier architecture based on mean-squared error training. Training using standard support vector machine methodology gives accuracy that significantly exceeds the performance of state-of-the-art mean-squared error training for a speaker recognition task.	Motorola Human Interface Lab, Tempe, AZ 85284 USA		Campbell, WM (corresponding author), Motorola Human Interface Lab, 770 S River Pkwy, Tempe, AZ 85284 USA.	Bill.Campbell@motorola.com						CAMPBELL JP, 1995, INT CONF ACOUST SPEE, P341, DOI 10.1109/ICASSP.1995.479543; Campbell W. M., 2000, Signal Processing X Theories and Applications. Proceedings of EUSIPCO 2000. Tenth European Signal Processing Conference, P457; Campbell WM, 1999, INT CONF ACOUST SPEE, P321, DOI 10.1109/ICASSP.1999.758127; CAREY MJ, 1991, INT CONF ACOUST SPEE, P397, DOI 10.1109/ICASSP.1991.150360; Collobert R., 2000, IDIAPRR0017; Farrell KR, 1994, IEEE T SPEECH AUDI P, V2, P194, DOI 10.1109/89.260362; FINE S, 2001, P INT C AC SPEECH SI; GANAPATHIRAJU A, 2000, SPEECH TRANSCR WORKS; Platt JC, 2000, ADV NEUR IN, P61; Rabiner L., 1993, FUNDAMENTALS SPEECH; Reynolds D. A., 1995, Lincoln Laboratory Journal, V8, P173; Schurmann J, 1996, PATTERN CLASSIFICATI; SMITH N, 2001, CUEDFINFENGTR387; TOMMI S, 1998, ADV NEURAL INFORMATI, V11, P487; Wan V, 2000, NEURAL NETWORKS FOR SIGNAL PROCESSING X, VOLS 1 AND 2, PROCEEDINGS, P775, DOI 10.1109/NNSP.2000.890157	15	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1157	1163						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100144
C	Coughlan, JM; Yuille, AL		Dietterich, TG; Becker, S; Ghahramani, Z		Coughlan, JM; Yuille, AL			The g factor: Relating distributions on features to distributions on images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We describe the g-factor, which relates probability distributions on image features to distributions on the images themselves. The g-factor depends only on our choice of features and lattice quantization and is independent of the training image data. We illustrate the importance of the g-factor by analyzing how the parameters of Markov Random Field (i.e. Gibbs or log-linear) probability models of images are learned from data by maximum likelihood estimation. In particular, we study homogeneous MRF models which learn image distributions in terms of clique potentials corresponding to feature histogram statistics (cf. Minimax Entropy Learning (MEL) by Zhu, Wu and Mumford 1997 [11]). We first use our analysis of the g-factor to determine when the clique potentials decouple for different features. Second, we show that clique potentials can be computed analytically by approximating the g-factor. Third, we demonstrate a connection between this approximation and the Generalized Iterative Scaling algorithm (GIS), due to Darroch and Ratcliff 1972 [2], for calculating potentials. This connection enables us to use GIS to improve our multinomial approximation, using Bethe-Kikuchi[8] approximations to simplify the GIS procedure. We support our analysis by computer simulations.	Smith Kettlewell Eye Res Inst, San Francisco, CA 94115 USA	The Smith-Kettlewell Eye Research Institute	Coughlan, JM (corresponding author), Smith Kettlewell Eye Res Inst, 2318 Fillmore St, San Francisco, CA 94115 USA.							COUGHLAN JM, 1998, P NIPS98; DARROCH JN, 1972, ANN MATH STAT, V43, P1470, DOI 10.1214/aoms/1177692379; Domb C, 1972, PHASE TRANSITIONS CR, V2; KONISHI SM, 1999, P COMP VIS PATT REC; LEE AB, 2001, INT J COMPUTER VISIO, V41; PORTILLA J, 2000, INT J COMPUTER V OCT; TEH YW, 2001, P NIPS01; YEDIDIA JS, 2000, P NIPS00; YUILLE A, 2002, IN PRESS NEURAL COMP; ZHU S, 1997, NEURAL COMPUTATION, V9; Zhu SC, 1997, IEEE T PATTERN ANAL, V19, P1236, DOI 10.1109/34.632983	11	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1231	1238						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100153
C	de Farias, DP; Van Roy, B		Dietterich, TG; Becker, S; Ghahramani, Z		de Farias, DP; Van Roy, B			Approximate dynamic programming via linear programming	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					The curse of dimensionality gives rise to prohibitive computational requirements that render infeasible the exact solution of large-scale stochastic control problems. We study an efficient method based on linear programming for approximating solutions to such problems. The approach "fits" a linear combination of pre-selected basis functions to the dynamic programming cost-to-go function. We develop bounds on the approximation error and present experimental results in the domain of queueing network control, providing empirical support for the methodology.	Stanford Univ, Dept Management Sci & Engn, Stanford, CA 94305 USA	Stanford University	de Farias, DP (corresponding author), Stanford Univ, Dept Management Sci & Engn, Stanford, CA 94305 USA.							BERTSIMAS D, 2000, UNPUB ANN APPL PROBA; DEFARIAS DP, 2001, UNPUB LINEAR PROGRAM; DEFARIAS DP, 2001, UNPUB CONSTRAINT SAM; MANNE AS, 1960, MANAGE SCI, V6, P259, DOI 10.1287/mnsc.6.3.259; SCHWEITZER PJ, 1985, J MATH ANAL APPL, V110, P568, DOI 10.1016/0022-247X(85)90317-8	5	0	0	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						689	695						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100086
C	Eggert, J; Bauml, B		Dietterich, TG; Becker, S; Ghahramani, Z		Eggert, J; Bauml, B			Exact differential equation population dynamics for integrate-and-fire neurons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				SPIKING NEURONS; MODEL	Mesoscopical, mathematical descriptions of dynamics of populations of spiking neurons are getting increasingly important for the understanding of large-scale processes in the brain using simulations. In our previous work, integral equation formulations for population dynamics have been derived for a special type of spiking neurons. For Integrate-and-Fire type neurons, these formulations were only approximately correct. Here, we derive a mathematically compact, exact population dynamics formulation for Integrate-and-Fire type neurons. It can be shown quantitatively in simulations that the numerical correspondence with microscopically modeled neuronal populations is excellent.	HONDA R&D Europe Deutschland GmbH, Future Technol Res, D-63073 Offenbach, Germany	Honda Motor Company	Eggert, J (corresponding author), HONDA R&D Europe Deutschland GmbH, Future Technol Res, Carl Legien Str 30, D-63073 Offenbach, Germany.			Bauml, Berthold/0000-0002-4545-4765				Eggert J, 2001, NEURAL COMPUT, V13, P1923, DOI 10.1162/089976601750399254; Gerstner W, 2000, NEURAL COMPUT, V12, P43, DOI 10.1162/089976600300015899; GERSTNER W, 1992, NETWORK-COMP NEURAL, V3, P139, DOI 10.1088/0954-898X/3/2/004; Knight BW, 2000, NEURAL COMPUT, V12, P473, DOI 10.1162/089976600300015673; KNIGHT BW, 1972, J GEN PHYSIOL, V59, P734, DOI 10.1085/jgp.59.6.734; Li ZP, 1998, NEURAL COMPUT, V10, P903, DOI 10.1162/089976698300017557; Tuckwell H.C., 1988, INTRO THEORETICAL NE, DOI [10.1017/CBO9780511623202, DOI 10.1017/CBO9780511623202]; WILSON HR, 1972, BIOPHYS J, V12, P1, DOI 10.1016/S0006-3495(72)86068-5	8	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						205	212						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100026
C	El-Yaniv, R; Souroujon, O		Dietterich, TG; Becker, S; Ghahramani, Z		El-Yaniv, R; Souroujon, O			Iterative double clustering for unsupervised and semi-supervised learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We present a powerful meta-clustering technique called Iterative Double Clustering (IDC). The IDC method is a natural extension of the recent Double Clustering (DC) method of Slonim and Tishby that exhibited impressive performance on text categorization tasks [12]. Using synthetically generated data we empirically find that whenever the DC procedure is successful in recovering some of the structure hidden in the data, the extended IDC procedure can incrementally compute a significantly more accurate classification. IDC is especially advantageous when the data exhibits high attribute noise. Our simulation results also show the effectiveness of IDC in text categorization problems. Surprisingly, this unsupervised procedure can be competitive with a (supervised) SVM trained with a small training set. Finally, we propose a simple and natural extension of IDC for semi-supervised and transductive learning where we are given both labeled and unlabeled examples.	Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	El-Yaniv, R (corresponding author), Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.							Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; ELYANIV R, 1997, NIPS97; Guedalia ID, 1999, NEURAL COMPUT, V11, P521, DOI 10.1162/089976699300016755; Jain A. K., 1988, ALGORITHMS CLUSTERIN, V6; LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115; PEREIRA FC, 1999, 37 ALL C COMM COMP; Rose K, 1998, P IEEE, V86, P2210, DOI 10.1109/5.726788; SLONIM N, 2001, IN PRESS EUR C IR RE; SLONIM N, 2000, ACM SIGIR 2000; SLONIM N, 1999, NIPS99	10	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1025	1032						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100128
C	Frey, BJ; Kannan, A; Jojic, N		Dietterich, TG; Becker, S; Ghahramani, Z		Frey, BJ; Kannan, A; Jojic, N			Product analysis: Learning to model observations as products of hidden variables	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				COMPONENT ANALYSIS; NEURAL-NETWORKS	Factor analysis and principal components analysis can be used to model linear relationships between observed variables and linearly map high-dimensional data to a lower-dimensional hidden space. In factor analysis, the observations are modeled as a linear combination of normally distributed hidden variables. We describe a nonlinear generalization of factor analysis, called "product analysis", that models the observed variables as a linear combination of products of normally distributed hidden variables. Just as factor analysis can be viewed as unsupervised linear regression on unobserved, normally distributed hidden variables, product analysis can be viewed as unsupervised linear regression on products of unobserved, normally distributed hidden variables. The mapping between the data and the hidden space is nonlinear, so we use an approximate variational technique for inference and learning. Since product analysis is a generalization of factor analysis, product analysis always finds a higher data likelihood than factor analysis. We give results on pattern recognition and illumination-invariant image clustering.	Univ Toronto, Machine Learning Grp, Toronto, ON, Canada	University of Toronto	Frey, BJ (corresponding author), Univ Toronto, Machine Learning Grp, Toronto, ON, Canada.		Arputharaj, Kannan/AAN-4912-2020					BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; Bishop CM, 1998, NEURAL COMPUT, V10, P215, DOI 10.1162/089976698300017953; Diamantaras K.I., 1996, PRINCIPAL COMPONENT; FREY BJ, 2002, IN PRESS IEEE T PATT; Ghahramani Z, 1997, CRGTR961; HASTIE T, 1984, THESIS STANFORD U ST; JOJIC N, 2001, IN PRESS P IEEE INT; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; JORDAN M, 1998, LEARNING GRAPHICAL M; KAMBHATLA N, 1994, ADV NEURAL INFORMATI, V6, P152, DOI DOI 10.1109/ICNN.1993.298730; MACKAY DJC, 1995, NUCL INSTRUM METH A, V354, P73, DOI 10.1016/0168-9002(94)00931-7; NEAL RM, 1993, UNPUB NEW VIEW EM AL; RUBIN DB, 1982, PSYCHOMETRIKA, V47, P69, DOI 10.1007/BF02293851; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; TENENBAUM JB, 1997, ADV NEURAL INFORMATI, V9; Tipping ME, 1999, NEURAL COMPUT, V11, P443, DOI 10.1162/089976699300016728; WOLBERG WH, 1990, P NATL ACAD SCI	17	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						729	735						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100091
C	Hahnloser, RHR; Xie, XH; Seung, HS		Dietterich, TG; Becker, S; Ghahramani, Z		Hahnloser, RHR; Xie, XH; Seung, HS			A theory of neural integration in the head-direction system	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				CIRCUIT	Integration in the head-direction system is a computation by which horizontal angular head velocity signals from the vestibular nuclei are integrated to yield a neural representation of head direction. In the thalamus, the postsubiculum and the mammillary nuclei, the head-direction representation has the form of a place code: neurons have a preferred head direction in which their firing is maximal [Blair and Sharp, 1995, Blair et al., 1998, ?]. Integration is a difficult computation, given that head-velocities can vary over a large range. Previous models of the head-direction system relied on the assumption that the integration is achieved in a firing-rate-based attractor network with a ring structure. In order to correctly integrate head-velocity signals during high-speed head rotations, very fast synaptic dynamics had to be assumed. Here we address the question whether integration in the head-direction system is possible with slow synapses, for example excitatory NMDA and inhibitory GABA(B) type synapses. For neural networks with such slow synapses, rate-based dynamics are a good approximation of spiking neurons [Ermentrout, 1994]. We find that correct integration during high-speed head rotations imposes strong constraints on possible network architectures.	MIT, Howard Hughes Med Inst, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA	Howard Hughes Medical Institute; Massachusetts Institute of Technology (MIT)	Hahnloser, RHR (corresponding author), MIT, Howard Hughes Med Inst, Dept Brain & Cognit Sci, 77 Massachusetts Ave, Cambridge, MA 02139 USA.			Xie, Xiaohui/0000-0002-5479-6345; Hahnloser, Richard/0000-0002-4039-7773				Blair HT, 1998, NEURON, V21, P1387, DOI 10.1016/S0896-6273(00)80657-1; BLAIR HT, 1995, J NEUROSCI, V15, P6260; ERMENTROUT B, 1994, NEURAL COMPUT, V6, P679, DOI 10.1162/neco.1994.6.4.679; Goodridge JP, 2000, J NEUROPHYSIOL, V83, P3402, DOI 10.1152/jn.2000.83.6.3402; Redish AD, 1996, NETWORK-COMP NEURAL, V7, P671, DOI 10.1088/0954-898X/7/4/004; Seung HS, 1996, P NATL ACAD SCI USA, V93, P13339, DOI 10.1073/pnas.93.23.13339; Zhang K, 1996, J NEUROSCI, V16, P2112	7	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						221	228						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100028
C	Jacobs, D; Rokers, B; Rudra, A; Liu, ZL		Dietterich, TG; Becker, S; Ghahramani, Z		Jacobs, D; Rokers, B; Rudra, A; Liu, ZL			Fragment completion in humans and machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				NEURAL NETWORKS; WORD FRAGMENTS; MODEL; RECALL	Partial information can trigger a complete memory. At the same time, human memory is not perfect. A cue can contain enough information to specify an item in memory, but fail to trigger that item. In the context of word memory, we present experiments that demonstrate some basic patterns in human memory errors. We use cues that consist of word fragments. We show that short and long cues are completed more accurately than medium length ones and study some of the factors that lead to this behavior. We then present a novel computational model that shows some of the flexibility and patterns of errors that occur in human memory. This model iterates between bottom-up and top-down computations. These are tied together using a Markov model of words that allows memory to be accessed with a simple feature set, and enables a bottom-up process to compute a probability distribution of possible completions of word fragments, in a manner similar to models of visual perceptual completion.	NEC Res Inst, Princeton, NJ 08540 USA	NEC Corporation	Jacobs, D (corresponding author), NEC Res Inst, 4 Independence Way, Princeton, NJ 08540 USA.			Rokers, Bas/0000-0003-0573-5332				Anderson J. A., 1995, INTRO NEURAL NETWORK, DOI [10.7551/mitpress/3905.001.0001, DOI 10.7551/MITPRESS/3905.001.0001]; BAUM EB, 1988, BIOL CYBERN, V59, P217, DOI 10.1007/BF00332910; CARPENTER GA, 1987, APPL OPTICS, V26, P4919, DOI 10.1364/AO.26.004919; GRIMES D, 2001, NIPS; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Hintzman D. L., 1990, JEP LEARNING MEMORY, V17, P341; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; JACOBS D, 2000, ITERATIVE PROJECTION; JONES GV, 1984, MEM COGNITION, V12, P250, DOI 10.3758/BF03197673; KOSKO B, 1987, APPL OPTICS, V26, P4947, DOI 10.1364/AO.26.004947; MUMFORD D., 1993, ALGEBRAIC GEOMETRY I; OLOFSSON U, 1995, SCAND J PSYCHOL, V36, P59, DOI 10.1111/j.1467-9450.1995.tb00968.x; OLOFSSON U, 1992, SCAND J PSYCHOL, V33, P108, DOI 10.1111/j.1467-9450.1992.tb00890.x; Rao RPN, 1997, NEURAL COMPUT, V9, P721, DOI 10.1162/neco.1997.9.4.721; ROSS BH, 1981, MEM COGNITION, V9, P1, DOI 10.3758/BF03196946; RUMELHART DE, 1982, PSYCHOL REV, V89, P60, DOI 10.1037/0033-295X.89.1.60; SEIDENBERG MS, 1987, ATTENTION PERFORM, V12, P245; SHACTER D, 1994, MEMORY SYSTEMS; SOMMER F, 1997, NIPS, P676; SRINIVAS K, 1992, MEM COGNITION, V20, P219, DOI 10.3758/BF03199659; Tishby N., 1999, 37 ALL C COMM CONTR; Ullman S., 1996, HIGH LEVEL VISION OB; Williams LR, 1997, NEURAL COMPUT, V9, P837, DOI 10.1162/neco.1997.9.4.837	24	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						27	34						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100004
C	Kowalczyk, A; Smola, AJ; Williamson, RC		Dietterich, TG; Becker, S; Ghahramani, Z		Kowalczyk, A; Smola, AJ; Williamson, RC			Kernel machines and Boolean functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				NETWORKS	We give results about the learnability and required complexity of logical formulae to solve classification problems. These results are obtained by linking propositional logic with kernel machines. In particular we show that decision trees and disjunctive normal forms (DNF) can be represented by the help of a special kernel, linking regularized risk to separation margin. Subsequently we derive a number of lower bounds on the required complexity of logic formulae using properties of algorithms for generation of linear estimators, such as perceptron and maximal perceptron learning.	Telstra Tes Labs, Clayton, Vic 3168, Australia	Telstra	Kowalczyk, A (corresponding author), Telstra Tes Labs, Telstra, Clayton, Vic 3168, Australia.							ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Ben-David S, 2001, ADV NEUR IN, V13, P189; BERTSEKAS D, 1995, NONLINEAR PROGRAMMIN; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Cristianini N., 2000, INTRO SUPPORT VECTOR; FREUND Y, 1998, MACHINE LEARNING; KOWALCZYK A, 1994, IEEE T NEURAL NETWOR, V5, P698, DOI 10.1109/72.317722; KOWALCZYK A, 2000, ADV LARGE MARGIN CLA, P61; NOVIKOV ABJ, 1962, P S MATH THEOR AUT, V12, P615; Olshen R., 1984, CLASSIFICATION REGRE; QUINLAN JR, 1987, INT J MAN MACH STUD, V27, P221, DOI 10.1016/S0020-7373(87)80053-6; Shawe-Taylor J, 2000, ADV NEUR IN, P349	13	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						439	446						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100055
C	Latham, PE		Dietterich, TG; Becker, S; Ghahramani, Z		Latham, PE			Associative memory in realistic neuronal networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				NEURAL NETWORKS; NEOCORTICAL NEURONS; WORKING-MEMORY; CORTEX; DYNAMICS; CAT; MODEL; NOISE	Almost two decades ago, Hopfield [1] showed that networks of highly reduced model neurons can exhibit multiple attracting fixed points, thus providing a substrate for associative memory. It is still not clear, however, whether realistic neuronal networks can support multiple attractors. The main difficulty is that neuronal networks in vivo exhibit a stable background state at low firing rate, typically a few Hz. Embedding attractor is easy, doing so without destabilizing the background is not. Previous work [2,3] focused on the sparse coding limit, in which a vanishingly small number of neurons are involved in any memory. Here we investigate the case in which the number of neurons involved in a memory scales with the number of neurons in the network. In contrast to the sparse coding limit, we find that multiple attractors can co-exist robustly with a stable background state. Mean field theory is used to understand how the behavior of the network scales with its parameters, and simulations with analog neurons are presented.	Univ Calif Los Angeles, Dept Neurobiol, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Latham, PE (corresponding author), Univ Calif Los Angeles, Dept Neurobiol, Los Angeles, CA 90095 USA.							Amit DJ, 1997, NETWORK-COMP NEURAL, V8, P373, DOI 10.1088/0954-898X/8/4/003; Brunel N, 2000, NETWORK-COMP NEURAL, V11, P261, DOI 10.1088/0954-898X/11/4/302; Burkitt AN, 1996, NETWORK-COMP NEURAL, V7, P517, DOI 10.1088/0954-898X/7/3/004; FUSTER JM, 1971, SCIENCE, V173, P652, DOI 10.1126/science.173.3997.652; GILBERT CD, 1977, J PHYSIOL-LONDON, V268, P391, DOI 10.1113/jphysiol.1977.sp011863; GOLDMANRAKIC PS, 1995, NEURON, V14, P477, DOI 10.1016/0896-6273(95)90304-6; HERRERO JF, 1995, J NEUROPHYSIOL, V74, P1549, DOI 10.1152/jn.1995.74.4.1549; Hertz J., 1991, INTRO THEORY NEURAL, DOI DOI 10.1201/9780429499661; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; LAMOUR Y, 1985, NEUROSCIENCE, V16, P835, DOI 10.1016/0306-4522(85)90097-1; Latham PE, 2000, J NEUROPHYSIOL, V83, P808, DOI 10.1152/jn.2000.83.2.808; LATHAM PE, 1999, SOC NEUR ABSTR, V25, P2259; MIYASHITA Y, 1993, ANNU REV NEUROSCI, V16, P245, DOI 10.1146/annurev.ne.16.030193.001333; Ochi K, 1997, HEARING RES, V105, P105, DOI 10.1016/S0378-5955(96)00201-8; Romo R, 1999, NATURE, V399, P470, DOI 10.1038/20939; SALIMI I, 1994, BRAIN RES, V656, P263, DOI 10.1016/0006-8993(94)91469-9; Shriki O., 1998, Society for Neuroscience Abstracts, V24, P143; SOMPOLINSKY H, 1986, PHYS REV A, V34, P2571, DOI 10.1103/PhysRevA.34.2571; SZENTE MB, 1988, BRAIN RES, V461, P64, DOI 10.1016/0006-8993(88)90725-1; TREVES A, 1993, NETWORK-COMP NEURAL, V4, P259, DOI 10.1088/0954-898X/4/3/002; TSODYKS MV, 1988, EUROPHYS LETT, V6, P101, DOI 10.1209/0295-5075/6/2/002; van Vreeswijk C, 1998, NEURAL COMPUT, V10, P1321, DOI 10.1162/089976698300017214; vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724	23	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						237	244						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100030
C	Liu, SC; Kramer, J; Indiveri, G; Delbruck, T; Douglas, R		Dietterich, TG; Becker, S; Ghahramani, Z		Liu, SC; Kramer, J; Indiveri, G; Delbruck, T; Douglas, R			Orientation-selective a VLSI spiking neurons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				VISUAL-CORTEX	We describe a programmable multi-chip VLSI neuronal system that can be used for exploring spike-based information processing models. The system consists of a silicon retina, a PIC microcontroller, and a transceiver chip whose integrate-and-fire neurons are connected in a soft winner-take-all architecture. The circuit on this multi-neuron chip approximates a cortical microcircuit. The neurons can be configured for different computational properties by the virtual connections of a selected set of pixels on the silicon retina. The virtual wiring between the different chips is effected by an event-driven communication protocol that uses asynchronous digital pulses, similar to spikes in a neuronal system. We used the multi-chip spike-based system to synthesize orientation-tuned neurons using both a feedforward model and a feedback model. The performance of our analog hardware spiking model matched the experimental observations and digital simulations of continuous-valued neurons. The multi-chip VLSI system has advantages over computer neuronal models in that it is real-time, and the computational time does not scale with the size of the neuronal network.	Univ Zurich, Inst Neuroinformat, CH-8057 Zurich, Switzerland	University of Zurich	Liu, SC (corresponding author), Univ Zurich, Inst Neuroinformat, Winterthurerstr 190, CH-8057 Zurich, Switzerland.		Indiveri, Giacomo/A-4282-2010; Indiveri, Giacomo/AAX-3310-2020	Indiveri, Giacomo/0000-0002-7109-1689; Indiveri, Giacomo/0000-0002-7109-1689				BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844; Boahen KA, 1996, IEEE MICRO, V16, P30, DOI 10.1109/40.540078; BOAHEN KA, 1997, TELLURIDE NSF WORKSH; DOUGLAS RJ, 1991, J PHYSIOL-LONDON, V440, P735, DOI 10.1113/jphysiol.1991.sp018733; DOUGLAS RJ, 1995, SCIENCE, V269, P981, DOI 10.1126/science.7638624; HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837; KRAMER J, 2001, UNPUB INTEGRATED OPT; LAZZARO J, 1993, IEEE T NEURAL NETWOR, V4, P523, DOI 10.1109/72.217193; Liu SC, 2001, NEURAL NETWORKS, V14, P629, DOI 10.1016/S0893-6080(01)00054-5; SOMERS DC, 1995, J NEUROSCI, V15, P5448; WHATLEY A, 1997, TELLURIDE NSF WORKSH	11	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1107	1114						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100138
C	Meinecke, F; Ziehe, A; Kawanabe, M; Muller, KR		Dietterich, TG; Becker, S; Ghahramani, Z		Meinecke, F; Ziehe, A; Kawanabe, M; Muller, KR			Estimating the reliability of ICA projections	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				INDEPENDENT COMPONENT ANALYSIS; BLIND SEPARATION	When applying unsupervised learning techniques like ICA or temporal decorrelation, a key question is whether the discovered projections are reliable. In other words: can we give error bars or can we assess the quality of our separation? We use resampling methods to tackle these questions and show experimentally that our proposed variance estimations are strongly correlated to the separation error. We demonstrate that this reliability estimation can be used to choose the appropriate ICA-model, to enhance significantly the separation performance, and, most important, to mark the components that have a actual physical meaning. Application to 49-channel-data from an magnetoencephalography (MEG) experiment underlines the usefulness of our approach.	Fraunhofer FIRST IDA, D-12489 Berlin, Germany	Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss Birlinghoven	Meinecke, F (corresponding author), Fraunhofer FIRST IDA, Kekulestr 7, D-12489 Berlin, Germany.		Mueller, Klaus-Robert/Y-3547-2019	Mueller, Klaus-Robert/0000-0002-3861-7685				AMARI S, 1996, ADV NEURAL INFORMATI, V8, P882; [Anonymous], [No title captured]; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; BERAN R, 1985, ANN STAT, V13, P95, DOI 10.1214/aos/1176346579; CARDOSO JF, 1994, IEEE P F, V14, P362; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; DECO G, 1996, INFORMATION THEORETI; Efron B., 1994, MONOGR STAT APPL PRO, DOI DOI 10.1007/978-1-4899-4541-9; JUTTEN C, 1991, SIGNAL PROCESS, V24, P1, DOI 10.1016/0165-1684(91)90079-X; MAKEIG S, 2000, P 2 INT WORKSH IND C, P627; MEINECKE F, 2001, ICA 01; Shao J., 2012, JACKKNIFE BOOTSTRAP, DOI [10.1007/978-1-4612-0795-5, DOI 10.1007/978-1-4612-0795-5]; Wubbeler G, 2000, IEEE T BIO-MED ENG, V47, P594, DOI 10.1109/10.841331; ZIEHE A, 1995, P INT C ART NEUR NET, P675	14	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1181	1188						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100147
C	Munos, R		Dietterich, TG; Becker, S; Ghahramani, Z		Munos, R			Efficient resources allocation for Markov decision processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					It is desirable that a complex decision-making problem in an uncertain world be adequately modeled by a Markov Decision Process (MDP) whose structural representation is adaptively designed by a parsimonious resources allocation process. Resources include time and cost of exploration, amount of memory and computational time allowed for the policy or value function representation. Concerned about making the best use of the available resources, we address the problem of efficiently estimating where adding extra resources is highly needed in order to improve the expected performance of the resulting policy. Possible application in reinforcement learning (RL), when real-world exploration is highly costly, concerns the detection of those areas of the state-space that need primarily to be explored in order to improve the policy. Another application concerns approximation of continuous state-space stochastic control problems using adaptive discretization techniques for which highly efficient grid points allocation is mandatory to survive high dimensionality. Maybe surprisingly these two problems can be formulated under a common framework: for a given resource allocation, which defines a belief state over possible MDPs, find where adding new resources (thus decreasing the uncertainty of some parameters -transition probabilities or rewards) will most likely increase the expected performance of the new policy. To do so, we use sampling techniques for estimating the contribution of each parameter's probability distribution function (pdf) to the expected loss of using an approximate policy (such as the optimal policy of the most probable MDP) instead of the true (but unknown) policy.	Ecole Polytech, CMAP, F-91128 Palaiseau, France	Institut Polytechnique de Paris	Munos, R (corresponding author), Ecole Polytech, CMAP, F-91128 Palaiseau, France.							Dearden R., 1999, P UNC ART INT; MUNOS R, 2000, INT C MACH LEARN; MUNOS R, 2002, DECISION MAKING UNCE; Munos R., 1999, P 38 IEEE C DEC CONT; Puterman M. L., 1994, MARKOV DECISION PROC, DOI DOI 10.1002/9780470316887; RUST J, 1997, USING RANDOMIZATION	6	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1571	1578						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100195
C	Negishi, M; Hanson, SJ		Dietterich, TG; Becker, S; Ghahramani, Z		Negishi, M; Hanson, SJ			Grammar transfer in a second order recurrent neural network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					It has been known that people, after being exposed to sentences generated by an artificial grammar, acquire implicit grammatical knowledge and are able to transfer the knowledge to inputs that are generated by a modified grammar. We show that a second order recurrent neural network is able to transfer grammatical knowledge from one language (generated by a Finite State Machine) to another language which differ both in vocabularies and syntax. Representation of the grammatical knowledge in the network is analyzed using linear discriminant analysis.	Rutgers State Univ, Dept Psychol, Newark, NJ 07102 USA	Rutgers State University Newark; Rutgers State University New Brunswick	Negishi, M (corresponding author), Rutgers State Univ, Dept Psychol, 101 Warren St Smith Hall 301, Newark, NJ 07102 USA.							BROOKS LR, 1991, J EXP PSYCHOL GEN, V120, P316, DOI 10.1037/0096-3445.120.3.316; Dienes Z, 1999, COGNITIVE SCI, V23, P53, DOI 10.1207/s15516709cog2301_3; ELMAN JL, 1991, MACH LEARN, V7, P195, DOI 10.1007/BF00114844; GILES CL, 1992, NEURAL COMPUT, V4, P393, DOI 10.1162/neco.1992.4.3.393; HANSON SJ, 2001, UNPUB EMERGENCE EXPL; REBER AS, 1969, J EXP PSYCHOL, V81, P115, DOI 10.1037/h0027454; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270	7	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						67	73						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100009
C	Opper, M; Urbanczik, R		Dietterich, TG; Becker, S; Ghahramani, Z		Opper, M; Urbanczik, R			Asymptotic universality for learning curves of support vector machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				NETWORKS	Using methods of Statistical Physics, we investigate the role of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel.	Aston Univ, Sch Engn & Appl Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England	Aston University	Opper, M (corresponding author), Aston Univ, Sch Engn & Appl Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England.							CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Cristianini N., 2000, INTRO SUPPORT VECTOR, DOI [10.1017/CBO9780511801389, DOI 10.1017/CBO9780511801389]; Risau-Gusman S, 2000, PHYS REV E, V62, P7092, DOI 10.1103/PhysRevE.62.7092; Schoenberg IJ, 1938, ANN MATH, V39, P811, DOI 10.2307/1968466	6	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						479	486						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100060
C	Rangarajan, A; Yuille, AL		Dietterich, TG; Becker, S; Ghahramani, Z		Rangarajan, A; Yuille, AL			MIME: Mutual information minimization and entropy maximization for Bayesian belief propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Bayesian belief propagation in graphical models has been recently shown to have very close ties to inference methods based in statistical physics. After Yedidia et al. demonstrated that belief propagation fixed points correspond to extrema of the so-called Bethe free energy, Yuille derived a double loop algorithm that is guaranteed to converge to a local minimum of the Bethe free energy. Yuille's algorithm is based on a certain decomposition of the Bethe free energy and he mentions that other decompositions are possible and may even be fruitful. In the present work, we begin with the Bethe free energy and show that it has a principled interpretation as pairwise mutual information minimization and marginal entropy maximization (MIME). Next, we construct a family of free energy functions from a spectrum of decompositions of the original Bethe free energy. For each free energy in this family, we develop a new algorithm that is guaranteed to converge to a local minimum. Preliminary computer simulations are in agreement with this theoretical development.	Univ Florida, Dept Comp & Informat Sci & Engn, Gainesville, FL 32611 USA	State University System of Florida; University of Florida	Rangarajan, A (corresponding author), Univ Florida, Dept Comp & Informat Sci & Engn, Gainesville, FL 32611 USA.		; Rangarajan, Anand/A-8652-2009	Yuille, Alan L./0000-0001-5207-9249; Rangarajan, Anand/0000-0001-8695-8436				MJOLSNESS E, 1990, NEURAL NETWORKS, V3, P651, DOI 10.1016/0893-6080(90)90055-P; Rangarajan A, 1996, NEURAL COMPUT, V8, P1041, DOI 10.1162/neco.1996.8.5.1041; Rangarajan A, 2000, PATTERN RECOGN, V33, P635, DOI 10.1016/S0031-3203(99)00077-1; TEH YW, 2001, 200101 GCNU U COLL G; WAINWRIGHT M, 2001, P2510 LIDS MIT; Weiss Y, 2000, NEURAL COMPUT, V12, P1, DOI 10.1162/089976600300015880; YEDIDIA JS, 2001, ADV NEURAL INFORMATI, V13; YUILLE AL, 2001, UNPUB NEURAL COMPUTA	8	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						873	880						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100109
C	Rehder, B		Dietterich, TG; Becker, S; Ghahramani, Z		Rehder, B			Causal categorization with Bayes nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					A theory of categorization is presented in which knowledge of causal relationships between category features is represented as a Bayesian network. Referred to as causal-model theory, this theory predicts that objects are classified as category members to the extent they are likely to have been produced by a category s causal model. On this view, people have models of the world that lead them to expect a certain distribution of features in category members (e.g., correlations between feature pairs that are directly connected by causal relationships), and consider exemplars good category members when they manifest those expectations. These expectations include sensitivity to higher-order feature interactions that emerge from the asymmetries inherent in causal relationships.	NYU, Dept Psychol, New York, NY 10012 USA	New York University	Rehder, B (corresponding author), NYU, Dept Psychol, 6 Washington Pl, New York, NY 10012 USA.							GLUCK MA, 1988, J MEM LANG, V27, P166, DOI 10.1016/0749-596X(88)90072-1; MURPHY GL, 1985, PSYCHOL REV, V92, P289, DOI 10.1037/0033-295X.92.3.289; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Rehder B, 1999, PROCEEDINGS OF THE TWENTY FIRST ANNUAL CONFERENCE OF THE COGNITIVE SCIENCE SOCIETY, P595; Rehder B, 2001, J EXP PSYCHOL GEN, V130, P323, DOI 10.1037//0096-3445.130.3.323; Salmon Wesley C, 1984, SCI EXPLANATION CAUS; WALDMANN MR, 1995, J EXP PSYCHOL GEN, V124, P181, DOI 10.1037/0096-3445.124.2.181	7	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						99	105						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100013
C	Rucci, M; Casile, A		Dietterich, TG; Becker, S; Ghahramani, Z		Rucci, M; Casile, A			Eye movements and the maturation of cortical orientation selectivity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				LATERAL GENICULATE-NUCLEUS; VISUAL-CORTEX; CAT; NEURONS; KITTENS; ORGANIZATION; COLUMNS; INPUTS; MODEL; CELL	Neural activity appears to be a crucial component for shaping the receptive fields of cortical simple cells into adjacent, oriented subregions alternately receiving ON- and OFF-center excitatory geniculate inputs. It is known that the orientation selective responses of V I neurons are refined by visual experience. After eye opening, the spatiotemporal structure of neural activity in the early stages of the visual pathway depends both on the visual environment and on how the environment is scanned. We have used computational modeling to investigate how eye movements might affect the refinement of the orientation tuning of simple cells in the presence of a Hebbian scheme of synaptic plasticity. Levels of correlation between the activity of simulated cells were examined while natural scenes were scanned so as to model sequences of saccades and fixational eye movements, such as microsaccades, tremor and ocular drift. The specific patterns of activity required for a quantitatively accurate development of simple cell receptive fields with segregated ON and OFF subregions were observed during fixational eye movements, but not in the presence of saccades or with static presentation of natural visual input. These results suggest an important role for the eye movements occurring during visual fixation in the refinement of orientation selectivity.	Boston Univ, Dept Cognit & Neural Syst, Boston, MA 02215 USA	Boston University	Rucci, M (corresponding author), Boston Univ, Dept Cognit & Neural Syst, Boston, MA 02215 USA.			Rucci, Michele/0000-0002-3066-1964				BUISSERET P, 1995, PHYSIOL REV, V75, P323, DOI 10.1152/physrev.1995.75.2.323; BUISSERET P, 1978, NATURE, V272, P816, DOI 10.1038/272816a0; Cai DQ, 1997, J NEUROPHYSIOL, V78, P1045, DOI 10.1152/jn.1997.78.2.1045; FREEMAN RD, 1979, SCIENCE, V206, P1093, DOI 10.1126/science.493996; GARYBOBO E, 1986, VISION RES, V26, P557, DOI 10.1016/0042-6989(86)90004-0; HEIN A, 1979, SCIENCE, V204, P1321, DOI 10.1126/science.313076; Lee D, 1998, J NEUROPHYSIOL, V79, P922, DOI 10.1152/jn.1998.79.2.922; MASTRONARDE DN, 1983, J NEUROPHYSIOL, V49, P303, DOI 10.1152/jn.1983.49.2.303; MILLER KD, 1994, J NEUROSCI, V14, P409; MIYASHITA M, 1992, NEUROREPORT, V3, P69, DOI 10.1097/00001756-199201000-00018; OLIVIER E, 1993, EXP BRAIN RES, V93, P435; SINGER W, 1982, EXP BRAIN RES, V47, P22; WILSON RS, 1976, ANN HUM BIOL, V3, P1, DOI 10.1080/03014467600001091	13	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						261	267						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100033
C	Ruml, W		Dietterich, TG; Becker, S; Ghahramani, Z		Ruml, W			Constructing distributed representations using additive clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				INDCLUS MODELS; ADCLUS MODEL; ALGORITHM; MAPCLUS	If the promise of computational modeling is to be fully realized in higher-level cognitive domains such as language processing, principled methods must be developed to construct the semantic representations used in such models. In this paper, we propose the use of an established formalism from mathematical psychology, additive clustering, as a means of automatically constructing binary representations for objects using only pairwise similarity data. However, existing methods for the unsupervised learning of additive clustering models do not scale well to large problems. We present a new algorithm for additive clustering, based on a novel heuristic technique for combinatorial optimization. The algorithm is simpler than previous formulations and makes fewer independence assumptions. Extensive empirical tests on both human and synthetic data suggest that it is more effective than previous methods and that it also scales better to larger problems. By making additive clustering practical, we take a significant step toward scaling connectionist models beyond hand-coded examples.	Harvard Univ, Div Engn & Appl Sci, Cambridge, MA 02138 USA	Harvard University	Ruml, W (corresponding author), Harvard Univ, Div Engn & Appl Sci, 33 Oxford St, Cambridge, MA 02138 USA.							ARABIE P, 1980, PSYCHOMETRIKA, V45, P211, DOI 10.1007/BF02294077; BALUJA S, 1997, NIPS 9; BOESE KD, 1994, OPER RES LETT, V16, P101, DOI 10.1016/0167-6377(94)90065-5; CARROLL JD, 1983, PSYCHOMETRIKA, V48, P157, DOI 10.1007/BF02294012; CHATURVEDI A, 1994, J CLASSIF, V11, P155, DOI 10.1007/BF01195676; Clouse DS, 1996, PROCEEDINGS OF THE EIGHTEENTH ANNUAL CONFERENCE OF THE COGNITIVE SCIENCE SOCIETY, P290; HOJO H, 1982, JAPANESE PSYCHOL RES, V25, P191; Kernighan B. W., 1970, Bell System Technical Journal, V49, P291; Kiers HAL, 1997, J CLASSIF, V14, P297, DOI 10.1007/s003579900014; LEE MD, IN PRESS MACHINE LEA; Noelle DC, 1997, PROCEEDINGS OF THE NINETEENTH ANNUAL CONFERENCE OF THE COGNITIVE SCIENCE SOCIETY, P1000; RUML W, 1996, J OPTIMIZATION THEOR, V89; SHEPARD RN, 1979, PSYCHOL REV, V86, P87, DOI 10.1037/0033-295X.86.2.87; STARK PB, 1995, COMPUTATION STAT, V10, P129; TENENBAUM J, 1996, NIPS 8; VANMECHELEN I, 1995, PSYCHOL BELG, V35, P85	16	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						107	114						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100014
C	Schmitt, M		Dietterich, TG; Becker, S; Ghahramani, Z		Schmitt, M			Computing time lower bounds for recurrent sigmoidal neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				COMPUTATIONAL POWER; NETS; NOISE	Recurrent neural networks of analog units are computers for real-valued functions. We study the time complexity of real computation in general recurrent neural networks. These have sigmoidal, linear, and product units of unlimited order as nodes and no restrictions on the weights. For networks operating in discrete time, we exhibit a family of functions with arbitrarily high complexity, and we derive almost tight bounds on the time required to compute these functions. Thus, evidence is given of the computational limitations that time-bounded analog recurrent neural networks are subject to.	Ruhr Univ Bochum, Fak Math, Lehrstuhl Math & Informat, D-44780 Bochum, Germany	Ruhr University Bochum	Schmitt, M (corresponding author), Ruhr Univ Bochum, Fak Math, Lehrstuhl Math & Informat, D-44780 Bochum, Germany.							Anthony M., 1999, NEURAL NETWORK LEARN, V9; Balcazar JL, 1997, IEEE T INFORM THEORY, V43, P1175, DOI 10.1109/18.605580; Blum Lenore, 1998, COMPLEXITY REAL COMP, DOI DOI 10.1007/978-1-4612-0701-6; Carrasco RC, 2000, NEURAL COMPUT, V12, P2129, DOI 10.1162/089976600300015097; Durbin R, 1989, NEURAL COMPUT, V1, P133, DOI 10.1162/neco.1989.1.1.133; Gavalda R, 1999, NEURAL COMPUT, V11, P715, DOI 10.1162/089976699300016638; Haykin S., 1999, NEURAL NETWORKS COMP; Kilian J, 1996, INFORM COMPUT, V128, P48, DOI 10.1006/inco.1996.0062; Koiran P, 1998, DISCRETE APPL MATH, V86, P63, DOI 10.1016/S0166-218X(98)00014-6; Maass W, 1999, NEURAL COMPUT, V11, P771, DOI 10.1162/089976699300016656; Maass W, 1998, NEURAL COMPUT, V10, P1071, DOI 10.1162/089976698300017359; Maass W., 2001, REAL TIME COMPUTING; Omlin CW, 1996, J ACM, V43, P937, DOI 10.1145/235809.235811; SCHMITT M, 2002, IN PRESS NEURAL COMP, V14; SIEGELMANN HT, 1995, J COMPUT SYST SCI, V50, P132, DOI 10.1006/jcss.1995.1013; SIEGELMANN HT, 1999, PROGR THEORETICAL CO; Sima J, 2001, LECT NOTES COMPUT SC, V2130, P806	17	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						503	510						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100063
C	Segal, E; Koller, D; Ormoneit, D		Dietterich, TG; Becker, S; Ghahramani, Z		Segal, E; Koller, D; Ormoneit, D			Probabilistic abstraction hierarchies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				EXPRESSION	Many domains are naturally organized in an abstraction hierarchy or taxonomy, where the instances in "nearby" classes in the taxonomy are similar. In this paper, we provide a general probabilistic framework for clustering data into a set of classes organized as a taxonomy, where each class is associated with a probabilistic model from which the data was generated. The clustering algorithm simultaneously optimizes three things: the assignment of data instances to clusters, the models associated with the clusters, and the structure of the abstraction hierarchy. A unique feature of our approach is that it utilizes global optimization algorithms for both of the last two steps, reducing the sensitivity to noise and the propensity to local maxima that are characteristic of algorithms such as hierarchical agglomerative clustering that only take local steps. We provide a theoretical analysis for our algorithm, showing that it converges to a local maximum of the joint likelihood of model and data. We present experimental results on synthetic data, and on real data in the domains of gene expression and text.	Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Segal, E (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.		Segal, Eran/AAF-4855-2019					CHEESEMAN P, 1995, BAYESIAN CLASSIFICAT; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863; FRIEDMAN N, 1998, P UAI; FRIEDMAN N, 2001, P RECOMB; Gasch AP, 2000, MOL BIOL CELL, V11, P4241, DOI 10.1091/mbc.11.12.4241; HOFMANN T, 1999, P INT JOINT C ART IN; Hofmann Thomas, 1999, P IJCAI; Hughes TR, 2000, CELL, V102, P109, DOI 10.1016/S0092-8674(00)00015-5; Hwang F.K., 1992, ANN DISCRETE MATH, V53; KROGH A, 1994, J MOL BIOL, V235, P1501, DOI 10.1006/jmbi.1994.1104; MCCALLUM A, 1998, P ICML; MEILA M, 2000, MACH LEARN, V1, P1; SEGAL E, 2002, RECOMB	14	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						913	920						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100114
C	Song, Y; Goncalves, L; Perona, P		Dietterich, TG; Becker, S; Ghahramani, Z		Song, Y; Goncalves, L; Perona, P			Unsupervised learning of human motion models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					This paper presents an unsupervised learning algorithm that can derive the probabilistic dependence structure of parts of an object (a moving human body in our examples) automatically from unlabeled data. The distinguished part of this work is that it is based on unlabeled data, i.e., the training features include both useful foreground parts and background clutter and the correspondence between the parts and detected features are unknown. We use decomposable triangulated graphs to depict the probabilistic independence of parts, but the unsupervised technique is not limited to this type of graph. In the new approach, labeling of the data (part assignments) is taken as hidden variables and the EM algorithm is applied. A greedy algorithm is developed to select parts and to search for the optimal structure based on the differential entropy of these variables. The success of our algorithm is demonstrated by applying it to generate models of human motion automatically from unlabeled real image sequences.	CALTECH, Pasadena, CA 91125 USA	California Institute of Technology	Song, Y (corresponding author), CALTECH, 136-93, Pasadena, CA 91125 USA.	yangs@vision.caltech.edu; luis@vision.caltech.edu; perona@vision.caltech.edu						Amit Y, 1996, IEEE T PATTERN ANAL, V18, P225, DOI 10.1109/34.485529; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; FRIEDMAN N, 1998, AAAI 1998 TUT; JORDON MI, 1999, LEARNING GRAPHICAL M; Meila M, 2001, J MACH LEARN RES, V1, P1, DOI 10.1162/153244301753344605; Song Y, 2001, COMPUT VIS IMAGE UND, V81, P303, DOI 10.1006/cviu.2000.0890; Song Y, 2000, PROC CVPR IEEE, P810, DOI 10.1109/CVPR.2000.855904; Srebro N., 2001, P 17 C UNC ART INT U, P504; Tomasi C, 1991, CMUCS91132; Weber M, 2000, THESIS CALTECH; WEBER M, 2000, P ECCV, V1, P18	12	0	0	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1287	1294						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100160
C	Stauffer, C; Miller, E; Tieu, K		Dietterich, TG; Becker, S; Ghahramani, Z		Stauffer, C; Miller, E; Tieu, K			Transform-invariant image decomposition with similarity templates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Recent work has shown impressive transform-invariant modeling and clustering for sets of images of objects with similar appearance. We seek to expand these capabilities to sets of images of an object class that show considerable variation across individual instances (e.g. pedestrian images) using a representation based on pixel-wise similarities, similarity templates. Because of its invariance to the colors of particular components of an object, this representation enables detection of instances of an object class and enables alignment of those instances. Further, this model implicitly represents the regions of color regularity in the class-specific image set enabling a decomposition of that object class into component regions.	MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Stauffer, C (corresponding author), MIT, Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	stauffer@ai.mit.edu; emiller@ai.mit.edu; tieu@ai.mit.edu						BOYKO Y, 1999, ICCV; HOFMANN T, 1999, 99 UAI; JOJIC N, NIPS 12; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Miller EG, 2000, PROC CVPR IEEE, P464, DOI 10.1109/CVPR.2000.855856; Pereira F., 1993, P 31 ANN M ASS COMP, P183, DOI [DOI 10.3115/981574.981598, 10.3115/981574.981598]; SHI J, 1997, CVPR SAN JUAN PUERT; STAUFFER C, 2001, UNPUB CVPR; Stauffer C., 1999, P 1999 IEEE COMP SOC, V2, P246; VIOLA P, 1995, THESIS AI TR	10	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1295	1302						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100161
C	Torkkola, K		Dietterich, TG; Becker, S; Ghahramani, Z		Torkkola, K			Learning discriminative feature transforms to low dimensions in low dimensions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				SPEECH RECOGNITION	The marriage of Renyi entropy with Parzen density estimation has been shown to be a viable tool in learning discriminative feature transforms. However, it suffers from computational complexity proportional to the square of the number of samples in the training data. This sets a practical limit to using large databases. We suggest immediate divorce of the two methods and remarriage of Renyi entropy with a semi-parametric density estimation method, such as a Gaussian Mixture Models (GMM). This allows all of the computation to take place in the low dimensional target space, and it reduces computational complexity proportional to square of the number of components in the mixtures. Furthermore, a convenient extension to Hidden Markov Models as commonly used in speech recognition becomes possible.	Motorola Labs, Tempe, AZ 85284 USA	Legend Holdings; Lenovo	Torkkola, K (corresponding author), Motorola Labs, 7700 S River Pkwy,MD ML28, Tempe, AZ 85284 USA.							[Anonymous], 2017, PAIN PRACT, V17, P1015; BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224; Dasgupta S., 2000, P 16 C UNC ART INT, P143; FANO RM, 1961, TRANSMISSION INFORMA; FISHER JW, 1998, P IEEE WORLD C COMP, P1712; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; Kapur J., 1992, ENTROPY OPTIMIZATION; KAPUR JN, 1994, MEASURES INFORMATION; Kumar N, 1998, SPEECH COMMUN, V26, P283, DOI 10.1016/S0167-6393(98)00061-2; PRINCIPE JC, 1998, P SPIE98; PRINCIPE JC, 2000, UNSUPERVISED ADAPTIV; Saon G, 2001, ADV NEUR IN, V13, P800; Sinkkonen J, 2002, NEURAL COMPUT, V14, P217, DOI 10.1162/089976602753284509; TORKKOLA K, 2001, P IJCNN WASH DC US J, P2756; VLASSIS N, 2002, NEURAL COMPUTATION, V14; Xuan G.R., 1996, P 13 INT C PATT REC, V2, P195, DOI DOI 10.1109/ICPR.1996.546751; Yang H., 1999, P INT ICSC S ADV INT	17	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						969	976						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100121
C	Vollgraf, R; Obermayer, K		Dietterich, TG; Becker, S; Ghahramani, Z		Vollgraf, R; Obermayer, K			Multi dimensional ICA to separate correlated sources	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We present a new method for the blind separation of sources, which do not fulfill the independence assumption. In contrast to standard methods we consider groups of neighboring samples ("patches") within the observed mixtures. First we extract independent features from the observed patches. It turns out that the average dependencies between these features in different sources is in general lower than the dependencies between the amplitudes of different sources. We show that it might be the case that most of the dependencies is carried by only a small number of features. Is this case - provided these features can be identified by some heuristic - we project all patches into the subspace which is orthogonal to the subspace spanned by the "correlated" features. Standard ICA is then performed on the elements of the transformed patches (for which the independence assumption holds) and robustly yields a good estimate of the mixing matrix.	Tech Univ Berlin, Dept Elect Engn & Comp Sci, Berlin, Germany	Technical University of Berlin	Vollgraf, R (corresponding author), Tech Univ Berlin, Dept Elect Engn & Comp Sci, Berlin, Germany.	vro@cs.tu-berlin.de; oby@cs.tu-berlin.de						AMARI S, 1995, ADV NEURAL INFORMATI, V8; Attias H, 1998, NEURAL COMPUT, V10, P1373, DOI 10.1162/neco.1998.10.6.1373; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Cardoso Jean-Fran cois, 1997, IEEE SIGNAL PROCESSI; CARDOSO JF, 1996, P IEEE WORKSH SSAP C; Hyvarinen A, 1997, NEURAL COMPUT, V9, P1483, DOI 10.1162/neco.1997.9.7.1483; HYVARINEN A, 1998, P INT C ART NEUR NET, P541; MOLGEDEY L, 1994, PHYS REV LETT, V72, P3634, DOI 10.1103/PhysRevLett.72.3634; Zibulevsky M, 2001, NEURAL COMPUT, V13, P863, DOI 10.1162/089976601300014385	10	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						993	1000						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100124
C	Wang, X; Dietterich, TG		Dietterich, TG; Becker, S; Ghahramani, Z		Wang, X; Dietterich, TG			Stabilizing value function approximation with the BFBP algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We address the problem of non-convergence of online reinforcement learning algorithms (e.g., Q learning and SARSA(lambda)) by adopting an incremental-batch approach that separates the exploration process from the function fitting process. Our BFBP (Batch Fit to Best Paths) algorithm alternates between an exploration phase (during which trajectories are generated to try to find fragments of the optimal policy) and a function fitting phase (during which a function approximator is fit to the best known paths from start states to terminal states). An advantage of this approach is that batch value-function fitting is a global process, which allows it to address the tradeoffs in function approximation that cannot be handled by local, online algorithms. This approach was pioneered by Boyan and Moore with their GROWSUPPORT and ROUT algorithms. We show how to improve upon their work by applying a better exploration process and by enriching the function fitting procedure to incorporate Bellman error and advantage error measures into the objective function. The results show improved performance on several benchmark problems.	Oregon State Univ, Dept Comp Sci, Corvallis, OR 97331 USA	Oregon State University	Wang, X (corresponding author), Oregon State Univ, Dept Comp Sci, Corvallis, OR 97331 USA.	wangxi@cs.orst.edu; tgd@cs.orst.edu						BAIRD LC, 1995, ICML, P00030; Bartlett P. L, 2000, P 17 INT C MACH LEAR, P41; Boyan J. A., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P63; Boyan J. A., 1995, Advances in Neural Information Processing Systems 7, P369; GORDON GJ, 2001, NIPS 13, P1040; HARVEY WD, 1995, IJCAI 95, P825; Konda VR, 2000, ADV NEUR IN, V12, P1008; MOLL R, 1999, NIPS 11, P1017; Sutton RS, 1996, ADV NEUR IN, V8, P1038; Sutton RS, 2000, ADV NEUR IN, V12, P1057; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Zhang W, 1995, INT JOINT C ART INT, P1114	12	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1587	1594						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100197
C	Wenning, G; Obermayer, K		Dietterich, TG; Becker, S; Ghahramani, Z		Wenning, G; Obermayer, K			Activity driven adaptive stochastic resonance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				INFORMATION; NOISE; NEURONS; MODELS; INPUT	Cortical neurons might be considered as threshold elements integrating in parallel many excitatory and inhibitory inputs. Due to the apparent variability of cortical spike trains this yields a strongly fluctuating membrane potential, such that threshold crossings are highly irregular. Here we study how a neuron could maximize its sensitivity w.r.t. a relatively small subset of excitatory input. Weak signals embedded in fluctuations is the natural realm of stochastic resonance. The neuron's response is described in a hazard-function approximation applied to an Ornstein-Uhlenbeck process. We analytically derive an optimality criterium and give a learning rule for the adjustment of the membrane fluctuations, such that the sensitivity is maximal exploiting stochastic resonance. We show that adaptation depends only on quantities that could easily be estimated locally (in space and time) by the neuron. The main results are compared with simulations of a biophysically more realistic neuron model.	Tech Univ Berlin, Dept Elect Engn & Comp Sci, D-10587 Berlin, Germany	Technical University of Berlin	Wenning, G (corresponding author), Tech Univ Berlin, Dept Elect Engn & Comp Sci, Franklinstr 28-29, D-10587 Berlin, Germany.							Anderson JS, 2000, SCIENCE, V290, P1968, DOI 10.1126/science.290.5498.1968; Bulsara AR, 1996, PHYS REV E, V54, pR2185, DOI 10.1103/PhysRevE.54.R2185; Cover TM., 1999, ELEMENTS INFORM THEO; Gammaitoni L, 1998, REV MOD PHYS, V70, P223, DOI 10.1103/RevModPhys.70.223; Laughlin SB, 1998, NAT NEUROSCI, V1, P36, DOI 10.1038/236; Plesser HE, 2000, NEURAL COMPUT, V12, P367, DOI 10.1162/089976600300015835; Russell DF, 1999, NATURE, V402, P291, DOI 10.1038/46279; Salinas E, 2000, J NEUROSCI, V20, P6193, DOI 10.1523/JNEUROSCI.20-16-06193.2000; Shadlen MN, 1998, J NEUROSCI, V18, P3870; Shriki O., 1998, Society for Neuroscience Abstracts, V24, P143; TSODYKS MV, 1995, NETWORK-COMP NEURAL, V6, P111, DOI 10.1088/0954-898X/6/2/001; TUCKWELL HC, 1998, INTRO THEORETICAL NE, V2	12	0	0	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						301	308						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100038
C	Williams, LR; Zweck, JW		Dietterich, TG; Becker, S; Ghahramani, Z		Williams, LR; Zweck, JW			A rotation and translation invariant discrete saliency network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				CONTOUR	We describe a neural network which enhances and completes salient closed contours. Our work is different from all previous work in three important ways. First, like the input provided to V1 by LGN, the input to our computation is isotropic. That is, the input is composed of spots not edges. Second, our network computes a well defined function of the input based on a distribution of closed contours characterized by a random process. Third, even though our computation is implemented in a discrete network, its output is invariant to continuous rotations and translations of the input pattern.	Univ New Mexico, Dept Comp Sci, Albuquerque, NM 87131 USA	University of New Mexico	Williams, LR (corresponding author), Univ New Mexico, Dept Comp Sci, Albuquerque, NM 87131 USA.							[Anonymous], 2018, A A PRACT, V11, P321; COWAN JD, 1997, COGNITION COMPUTATIO; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; Golub Gene H., 2013, MATRIX COMPUTATION, V3; HEITGER R, 1993, P 4 INT C COMP VIS B; IVERSON LA, 1993, THESIS MCGILL U; Li ZP, 1998, NEURAL COMPUT, V10, P903, DOI 10.1162/089976698300017557; MUMFORD D., 1993, ALGEBRAIC GEOMETRY I; PARENT P, 1989, IEEE T PATTERN ANAL, V11, P823, DOI 10.1109/34.31445; SIMONCELLI EP, 1992, IEEE T INFORM THEORY, V38, P587, DOI 10.1109/18.119725; Williams LR, 2001, NEURAL COMPUT, V13, P1683, DOI 10.1162/08997660152469305; YEN S, 1996, NEURAL INFORMATION P, V9; ZWECK JW, 2000, P EUR C COMP VIS ECC	13	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1319	1326						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100164
C	Wrigley, SN; Brown, GJ		Dietterich, TG; Becker, S; Ghahramani, Z		Wrigley, SN; Brown, GJ			A neural oscillator model of auditory selective attention	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				SEGREGATION	A model of auditory grouping is described in which auditory attention plays a key role. The model is based upon an oscillatory correlation framework, in which neural oscillators representing a single perceptual stream are synchronised, and are desynchronised from oscillators representing other streams. The model suggests a mechanism by which attention can be directed to the high or low tones in a repeating sequence of tones with alternating frequencies. In addition, it simulates the perceptual segregation of a mistuned harmonic from a complex tone.	Univ Sheffield, Dept Comp Sci, Sheffield S1 4DP, S Yorkshire, England	University of Sheffield	Wrigley, SN (corresponding author), Univ Sheffield, Dept Comp Sci, Regen Court,211 Portobello St, Sheffield S1 4DP, S Yorkshire, England.			Wrigley, Stuart/0000-0002-3689-8292; Brown, Guy/0000-0001-8565-5476				ANSTIS S, 1985, J EXP PSYCHOL HUMAN, V11, P257, DOI 10.1037/0096-1523.11.3.257; Bregman A. S., 1990, AUDITORY SCENCE ANAL; BROWN GJ, 1994, COMPUT SPEECH LANG, V8, P297, DOI 10.1006/csla.1994.1016; Carlyon RP, 2001, J EXP PSYCHOL HUMAN, V27, P115, DOI 10.1037/0096-1523.27.1.115; DARWIN CJ, 1995, J ACOUST SOC AM, V98, P880, DOI 10.1121/1.413513; JOLIOT M, 1994, P NATL ACAD SCI USA, V91, P11748, DOI 10.1073/pnas.91.24.11748; MONDOR TA, 1994, PERCEPT PSYCHOPHYS, V56, P268, DOI 10.3758/BF03209761; Moore B. C. J., 1997, INTRO PSYCHOL HEARIN; SPENCE CJ, 1994, J EXP PSYCHOL HUMAN, V20, P555, DOI 10.1037/0096-1523.20.3.555; Wang DL, 1996, COGNITIVE SCI, V20, P409, DOI 10.1207/s15516709cog2003_3; Wang DLL, 1999, IEEE T NEURAL NETWOR, V10, P684, DOI 10.1109/72.761727	11	0	0	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1205	1212						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100150
C	Xie, XH; Giese, MA		Dietterich, TG; Becker, S; Ghahramani, Z		Xie, XH; Giese, MA			Generating velocity tuning by asymmetric recurrent connections	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				DIRECTION SELECTIVITY; VISUAL-CORTEX; SIMPLE CELLS; INHIBITION; DYNAMICS; MODEL	Asymmetric lateral connections are one possible mechanism that can account for the direction selectivity of cortical neurons. We present a mathematical analysis for a class of these models. Contrasting with earlier theoretical work that has relied on methods from linear systems theory, we study the network's nonlinear dynamic properties that arise when the threshold nonlinearity of the neurons is taken into account. We show that such networks have stimulus-locked traveling pulse solutions that are appropriate for modeling the responses of direction selective cortical neurons. In addition, our analysis shows that outside a certain regime of stimulus speeds the stability of this solutions breaks down giving rise to another class of solutions that are characterized by specific spatio-temporal periodicity. This predicts that if direction selectivity in the cortex is mainly achieved by asymmetric lateral connections lurching activity waves might be observable in ensembles of direction selective cortical neurons within appropriate regimes of the stimulus speed.	MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Xie, XH (corresponding author), MIT, Dept Brain & Cognit Sci, E25-618, Cambridge, MA 02139 USA.			Xie, Xiaohui/0000-0002-5479-6345				ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; AMARI SI, 1977, BIOL CYBERN, V27, P77, DOI 10.1007/BF00337259; Golomb D, 2000, NETWORK-COMP NEURAL, V11, P221, DOI 10.1088/0954-898X/11/3/304; Hansel D., 1998, METHODS NEURONAL MOD, P499; KOCH C, 1989, MODELS VISUAL CORTEX, P15; Maex R, 1996, J NEUROPHYSIOL, V75, P1515, DOI 10.1152/jn.1996.75.4.1515; Mineiro P, 1998, NEURAL COMPUT, V10, P353, DOI 10.1162/089976698300017791; REICHARDT W, 1961, PRINCIPLE EVALUATION; Sabatini SP, 1999, BIOL CYBERN, V80, P171, DOI 10.1007/s004220050515; Salinas E, 1996, P NATL ACAD SCI USA, V93, P11956, DOI 10.1073/pnas.93.21.11956; SUAREZ H, 1995, J NEUROSCI, V15, P6700; VANSANTEN JP, 1985, J OPT SOC AM A, V256, P300; WATSON AB, 1985, J OPT SOC AM A, V2, P322, DOI 10.1364/JOSAA.2.000322; WILSON HR, 1973, KYBERNETIK, V13, P55, DOI 10.1007/BF00288786	14	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						325	332						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100041
C	Yarlett, D; Ramscar, M		Dietterich, TG; Becker, S; Ghahramani, Z		Yarlett, D; Ramscar, M			A quantitative model of counterfactual reasoning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					In this paper we explore two quantitative approaches to the modelling of counterfactual reasoning - a linear and a noisy-OR model - based on information contained in conceptual dependency networks. Empirical data is acquired in a study and the fit of the models compared to it. We conclude by considering the appropriateness of non-parametric approaches to counterfactual reasoning, and examining the prospects for other parametric approaches in the future.	Univ Edinburgh, Div Informat, Edinburgh, Midlothian, Scotland	University of Edinburgh	Yarlett, D (corresponding author), Univ Edinburgh, Div Informat, Edinburgh, Midlothian, Scotland.	dany@cogsci.ed.ac.uk; michael@dai.ed.ac.uk						Byrne RMJ, 1999, MEM COGNITION, V27, P726, DOI 10.3758/BF03211565; DAWES RM, 1979, AM PSYCHOL, V34, P571, DOI 10.1037/0003-066X.34.7.571; Goodman N., 1983, FACT FICTION FORECAS, V4e ed.; GRIFFITHS T, 2001, ASSESSING INTERVENTI; KAHNEMAN D, 1986, PSYCHOL REV, V93, P136, DOI 10.1037/0033-295X.93.2.136; Kahneman D., 1982, JUDGMENT UNCERTAINTY; Kelley H.H., 1972, ATTRIBUTION PERCEIV, P151; Lewis D., 1973, COUNTERFACTUALS; Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Roese NJ, 1997, PSYCHOL BULL, V121, P133, DOI 10.1037/0033-2909.121.1.133; Sloman SA, 1998, COGNITIVE SCI, V22, P189, DOI 10.1207/s15516709cog2202_2; YARLETT DG, 2001, P 23 ANN C COGN SCI, P1154; YARLETT DG, UNPUB UNCERTAINTY CA	15	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						123	130						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100016
C	Zhang, T		Dietterich, TG; Becker, S; Ghahramani, Z		Zhang, T			Generalization performance of some learning problems in Hilbert functional spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We investigate the generalization performance of some learning problems in Hilbert functional Spaces. We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. This estimate allows us to derive generalization bounds on some learning formulations.	IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Zhang, T (corresponding author), IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.							Bousquet O, 2001, ADV NEUR IN, V13, P196; CIRSTIANINI N, 2000, INTRO SUPPORT VECTOR; Lee WS, 1998, IEEE T INFORM THEORY, V44, P1974, DOI 10.1109/18.705577; Rockafellar R. T., 1970, CONVEX ANAL; Yurinsky, 1995, SUMS GAUSSIAN VECTOR; Zhang T, 2001, ADV NEUR IN, V13, P357; ZHANG T, 2001, 14 ANN C COMP LEARN, P427	7	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						543	550						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100068
C	Zimmermann, HG; Neuneier, R; Grothmann, R		Dietterich, TG; Becker, S; Ghahramani, Z		Zimmermann, HG; Neuneier, R; Grothmann, R			Active portfolio-management based on error correction neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					This paper deals with a neural network architecture which establishes a portfolio management system similar to the Black/Litterman approach. This allocation scheme distributes funds across various securities or financial markets while simultaneously complying with specific allocation constraints which meet the requirements of an investor. The portfolio optimization algorithm is modeled by a feedforward neural network. The underlying expected return forecasts are based on error correction neural networks (ECNN), which utilize the last model error as an auxiliary input to evaluate their own misspecification. The portfolio optimization is implemented such that (i.) the allocations comply with investor's constraints and that (ii.) the risk of the portfolio can be controlled. We demonstrate the profitability of our approach by constructing internationally diversified portfolios across 21 different financial markets of the G7 contries. It turns out, that our approach is superior to a preset benchmark portfolio.	Siemens AG, Corp Technol, D-81730 Munich, Germany	Siemens AG; Siemens Germany	Zimmermann, HG (corresponding author), Siemens AG, Corp Technol, D-81730 Munich, Germany.							[Anonymous], 1998, NEURAL NETWORKS COMP; BLACK F, 1992, FINANCIAL ANAL J SEP; ELTON EJ, MODERN PORTFOLIO THE; LINTNER J, 1965, REV EC STAT      FEB; Markowitz H, 1952, J FINANC, V7, P77, DOI 10.1111/j.1540-6261.1952.tb01525.x; PEARLMATTER B, 1995, IEEE T NEURAL NETWOR, V6; SHARPE F, 1963, MANAGEMENT SCI, V9; ZIMMERMANN HG, FIELD GUIDE DYNAMICA; ZIMMERMANN HG, 2001, MODELING FORECASTING	9	0	0	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1465	1472						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100182
C	Bae, UM; Lee, SY		Leen, TK; Dietterich, TG; Tresp, V		Bae, UM; Lee, SY			Combining ICA and top-down attention for robust speech recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				BLIND SIGNAL SEPARATION	We present an algorithm which compensates for the mismatches between characteristics of real-world problems and assumptions of independent component analysis algorithm. To provide additional information to the ICA network, we incorporate top-down selective attention. An MLP classifier is added to the separated signal channel and the error of the classifier is backpropagated to the ICA network. This backpropagation process results in estimation of expected ICA output signal for the top-down attention. Then, the unmixing matrix is retrained according to a new cost function representing the backpropagated error as well as independence. It modifies the density of recovered signals to the density appropriate for classification. For noisy speech signal recorded in real environments, the algorithm improved the recognition performance and showed robustness against parametric changes.	Korea Adv Inst Sci & Technol, Brain Sci Res Ctr, Dept Elect Engn & Comp Sci, Taejon 305701, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Bae, UM (corresponding author), Korea Adv Inst Sci & Technol, Brain Sci Res Ctr, Dept Elect Engn & Comp Sci, 373-1 Kusong-Dong, Taejon 305701, South Korea.							Amari S, 1996, ADV NEUR IN, V8, P757; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; CARDOSO JF, 1996, IEEE T SIGNAL PROCES, V45, P434; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; Lee T.-W., 1998, INDEPENDENT COMPONEN; LEE TW, 1999, IN PRESS IEEE T PATT; Papoulis A., 1991, COMMUNICATIONS SIGNA, V3; Park HM, 1999, ELECTRON LETT, V35, P2011, DOI 10.1049/el:19991358; Park KY, 2000, NEURAL PROCESS LETT, V12, P41, DOI 10.1023/A:1009617830276; PARK KY, 1999, P IJCNN; SMARAGDIS P, 1997, THESIS MIT	11	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						765	771						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800108
C	Bhattacharyya, C; Keerthi, SS		Leen, TK; Dietterich, TG; Tresp, V		Bhattacharyya, C; Keerthi, SS			A variational mean-field theory for sigmoidal belief networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					A variational derivation of Plefka's mean-field theory is presented. This theory is then applied to sigmoidal belief networks with the aid of further approximations. Empirical evaluation on small scale networks show that the proposed approximations are quite competitive.	Indian Inst Sci, Bangalore 560012, Karnataka, India	Indian Institute of Science (IISC) - Bangalore	Bhattacharyya, C (corresponding author), Indian Inst Sci, Bangalore 560012, Karnataka, India.							BHATTACHARYYA C, 2000, J PHYS A, V33; BHATTACHARYYA C, 1999, IN PRESS J ARTIFICIA; BISHOP MC, 1997, ADV NEURAL INFORMATI, V10; GEORGES A, 1991, J PHYS A, V24; KAPPEN HJ, 1998, ADV NEURAL INFORMATI, V10; PLEFKA T, 1982, J PHYS A, V15; Saul Lawrence, 1996, J ARTIFICIAL INTELLI, V4	7	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						374	380						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800053
C	Brown, TX		Leen, TK; Dietterich, TG; Tresp, V		Brown, TX			Direct classification with indirect data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We classify an input space according to the outputs of a real-valued function. The function is not given, but rather examples of the function. We contribute a consistent classifier that avoids the unnecessary complexity of estimating the function.	Univ Colorado, Dept Elect & Comp Engn, Interdisciplinary Telecommun Program, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Brown, TX (corresponding author), Univ Colorado, Dept Elect & Comp Engn, Interdisciplinary Telecommun Program, Boulder, CO 80309 USA.	timxb@colorado.edu						Brown TX, 1997, ADV NEUR IN, V9, P932; BROWN TX, 1995, INT NEUR NETW SOC S, P153; BROWN TX, 1999, INFOCOMM 99, V1, P361; ESTRELLA AD, 1994, ELECTRON LETT, V30, P577, DOI 10.1049/el:19940371; Hiramatsu A, 1990, IEEE Trans Neural Netw, V1, P122, DOI 10.1109/72.80211; HIRAMATSU A, 1995, IEEE COMMUN MAG, V33, P58, DOI 10.1109/35.466221; Tong H, 1998, GLOBECOM 98: IEEE GLOBECOM 1998 - CONFERENCE RECORD, VOLS 1-6, P19, DOI 10.1109/GLOCOM.1998.775690; TRANGIA P, 1992, P IEEE GLOBECOM 92 O, P1303	8	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						381	387						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800054
C	Carlstrom, J		Leen, TK; Dietterich, TG; Tresp, V		Carlstrom, J			Decomposition of reinforcement learning for admission control of self-similar call arrival processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					This paper presents predictive gain scheduling, a technique for simplifying reinforcement learning problems by decomposition. Link admission control of self-similar call traffic is used to demonstrate the technique. The control problem is decomposed into on-line prediction of near-future call arrival rates, and precomputation of policies for Poisson call arrival processes. At decision time, the predictions are used to select among the policies. Simulations show that this technique results in significantly faster learning without any performance loss, compared to a reinforcement learning controller that does not decompose the problem.	Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Carlstrom, J (corresponding author), Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.	jakob@ee.technion.ac.il						Bertsekas D. P., 1995, DYNAMIC PROGRAMMING; Carlstrom J, 1999, TELETRAF SCI ENG, V3, P571; CARLSTROM J, 2000, 2000010 UPPS U DEP I; CARLSTROM J, 2000, 2000009 UPPS U DEP I; DZIONG Z, 1994, IEEE T COMMUN, V42, P2011, DOI 10.1109/TCOMM.1994.583415; Dziong Z., 1997, ATM NETWORK RESOURCE; Feldmann A., 1998, Computer Communication Review, V28, P5, DOI 10.1145/279345.279346; Haykin S., 1999, NEURAL NETWORKS COMP; Johan K., 1995, ADAPTIVE CONTROL; LELAND WE, 1994, IEEE ACM T NETWORK, V2, P1, DOI 10.1109/90.282603; MARBACH P, 2000, IEEE J SEL AREAS FEB; PAXSON V, 1995, IEEE ACM T NETWORK, V3, P226, DOI 10.1109/90.392383; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; TONG H, 2000, IEEE J SELECTED  FEB	14	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						1033	1039						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800145
C	Kristjansson, TT; Frey, BJ		Leen, TK; Dietterich, TG; Tresp, V		Kristjansson, TT; Frey, BJ			Keeping flexible active contours on track using Metropolis updates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Condensation, a form of likelihood-weighted particle filtering, has been successfully used to infer the shapes of highly constrained "active" contours in video sequences. However, when the contours are highly flexible (e.g. for tracking fingers of a hand), a computationally burdensome number of particles is needed to successfully approximate the contour distribution. We show how the Metropolis algorithm can be used to update a particle set representing a distribution over contours at each frame in a video sequence, We compare this method to condensation using a video sequence that requires highly flexible contours, and show that the new algorithm performs dramatically better that the condensation algorithm. We discuss the incorporation of this method into the "active contour" framework where a shape-subspace is used constrain shape variation.	Univ Waterloo, Waterloo, ON N2L 3G1, Canada	University of Waterloo	Kristjansson, TT (corresponding author), Univ Waterloo, Waterloo, ON N2L 3G1, Canada.							Blake A., 1998, ACTIVE CONTOURS, DOI [10.1007/978-1-4471-1555-7, DOI 10.1007/978-1-4471-1555-7]; COOTES TF, 1998, P EUR C COMP VIS, V2, P484; Freeman W. T., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1182, DOI 10.1109/ICCV.1999.790414; FREEMAN WT, 1998, ADV NEURAL INFORMATI, V10; HINTON GE, 2000, ADV NEURAL INFORMATI, V12; HOWE NR, 2000, ADV NEURAL INFORMATI, V12; ISARD M, 1998, P 5 EUR C COMP VIS, V1, P893; JEPSON A, P IEEE C COMP VIS PA; MacCormick J., 1999, P 7 IEEE INT C COMP; MACKAY DJC, 1999, LEARNING GRAPHICAL M; MATTHEWS I, 1998, P AUD VIS SPEECH PRO, P73; Neal R. M., 1993, PROBABILISTIC INFERE; PAPANIKOLOPOULO.N, 1991, P IEEE INT C ROB AUT, V1, P851; SULLIVAN J, 1999, P INT C COMP VIS; Terzopoulos D., 1992, ACTIVE VISION, P3; WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981; WEISS Y, 1997, P IEEE C COMP VIS PA	17	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						859	865						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800121
C	Neskovic, P; Davis, PC; Cooper, LN		Leen, TK; Dietterich, TG; Tresp, V		Neskovic, P; Davis, PC; Cooper, LN			Interactive Parts model: an application to recognition of on-line cursive script	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				MARKOV-MODELS; ONLINE	In this work, we introduce an Interactive Parts (IP) model as an alternative to Hidden Markov Models (HMMs). We tested both models on a database of on-line cursive script. We show that implementations of HMMs and the IP model, in which all letters are assumed to have the same average width, give comparable results. However, in contrast to HMMs, the IP model can handle duration modeling without an increase in computational complexity.	Brown Univ, Dept Phys, Providence, RI 02912 USA	Brown University	Neskovic, P (corresponding author), Brown Univ, Dept Phys, Providence, RI 02912 USA.							BENGIO Y, 1995, NEURAL COMPUT, V7, P1289, DOI 10.1162/neco.1995.7.6.1289; BOURLARD H, 1990, IEEE T PATTERN ANAL, V12, P1167, DOI 10.1109/34.62605; BURL M, 1998, P CVPR 98; BURL MC, 1996, P IEEE COMP SOC C CO; MITCHELL CD, 1993, P IEEE INT C AC SPEE, P331; MUMFORD D, 1994, LARGE SCALE NEURONAL, P125; NESKOVIC P, 1999, THESIS BROWN U; NESKOVIC P, 2000, 7 IWFHR; RUMELHART DE, 1993, COMPUTATIONAL LEARNI; SCHENKEL M, 1995, MACH VISION APPL, V8, P215, DOI 10.1007/BF01219589; Schuster-Bockler Benjamin, 2007, Curr Protoc Bioinformatics, VAppendix 3, p3A, DOI [10.1109/MASSP.1986.1165342, 10.1002/0471250953.bia03as18]	11	0	0	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						974	980						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800137
C	Pedersen, L; Apostolopoulos, D; Whittaker, R		Leen, TK; Dietterich, TG; Tresp, V		Pedersen, L; Apostolopoulos, D; Whittaker, R			Bayes networks on ice: Robotic search for Antarctic meteorites	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					A Bayes network based classifier for distinguishing terrestrial rocks from meteorites is implemented onboard the Nomad robot. Equipped with a camera, spectrometer and eddy current sensor, this robot searched the ice sheets of Antarctica and autonomously made the first robotic identification of a meteorite, in January 2000 at the Elephant Moraine. This paper discusses rock classification from a robotic platform, and describes the system onboard Nomad.	Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Pedersen, L (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.							APOSTOLOPOULOS D, 1999, FIELD SERV ROB C PIT; CASSIDY W, 1997, COMMUNICATION; DIETRICH R, 1979, ROCKS MINERALS; Gelman A, 2013, BAYESIAN DATA ANAL, P16; PEDERSEN L, 1998, P SPIE PHOT E C BOST; Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88; SPIEGELHALTER DJ, 1993, STAT SCI, V8, P219, DOI 10.1214/ss/1177010888	7	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						988	994						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800139
C	Vasconcelos, N; Lippman, A		Leen, TK; Dietterich, TG; Tresp, V		Vasconcelos, N; Lippman, A			Bayesian video shot segmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Prior knowledge about video structure can be used both as a means to improve the performance of content analysis and to extract features that allow semantic classification. We introduce statistical models for two important components of this structure, shot duration and activity, and demonstrate the usefulness of these models by introducing a Bayesian formulation for the shot segmentation problem. The new formulations is shown to extend standard thresholding methods in an adaptive and intuitive way, leading to improved segmentation accuracy.	MIT, Media Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Vasconcelos, N (corresponding author), MIT, Media Lab, 20 Ames St,E 15-354, Cambridge, MA 02139 USA.	nuno@media.mit.edu; lip@media.mit.edu		Vasconcelos, Nuno/0000-0002-9024-4302				BORDWELL D, 1986, FILM ART INTRO; BORECZKY JS, 1996, P SPIE C VIS COMM IM; DRAKE A, 1987, FUNDAMENTALS APPL PR; HOGG RV, 1993, PROBABILITY STAT INF; NIBLACK W, 1993, P SOC PHOTO-OPT INS, V1908, P173; Reisz K., 1968, TECHNIQUE FILM EDITI; SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487	7	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						1009	1015						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800142
C	Wong, KYM; Nishimori, H		Leen, TK; Dietterich, TG; Tresp, V		Wong, KYM; Nishimori, H			Stagewise processing in error-correcting codes and image restoration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				STATISTICAL-MECHANICS; NEURAL NETWORKS	We introduce stagewise processing in error-correcting codes and image restoration, by extracting information from the former stage and using it selectively to improve the performance of the latter one. Both mean-field analysis using the cavity method and simulations show that it has the advantage of being robust against uncertainties in hyperparameter estimation.	Hong Kong Univ Sci & Technol, Dept Phys, Kowloon, Hong Kong, Peoples R China	Hong Kong University of Science & Technology	Wong, KYM (corresponding author), Hong Kong Univ Sci & Technol, Dept Phys, Clear Water Bay, Kowloon, Hong Kong, Peoples R China.			Wong, Kwok Yee Michael/0000-0002-3078-4577				GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Kabashima Y, 1998, EUROPHYS LETT, V44, P668, DOI 10.1209/epl/i1998-00524-7; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; McEliece R., 1977, THEORY INFORM CODING, V3; Mezard M., 1987, SPIN GLASS THEORY IN; Nishimori H, 1999, PHYS REV E, V60, P132, DOI 10.1103/PhysRevE.60.132; PRYCE JM, 1995, J PHYS A-MATH GEN, V28, P511, DOI 10.1088/0305-4470/28/3/009; Wong KYM, 1996, EUROPHYS LETT, V36, P631, DOI 10.1209/epl/i1996-00279-7; Wong KYM, 1997, ADV NEUR IN, V9, P302; WONG KYM, 2000, UNPUB PHYS REV E; [No title captured]	11	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						343	349						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800049
C	Chechik, G; Meilijson, I; Ruppin, E		Solla, SA; Leen, TK; Muller, KR		Chechik, G; Meilijson, I; Ruppin, E			Effective learning requires neuronal remodeling of Hebbian synapses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				SYNAPTIC PLASTICITY; CONSTRAINTS	This paper revisits the classical neuroscience paradigm of Hebbian Learning. We find that a necessary requirement for effective associative memory learning is that the efficacies of the incoming synapses should be uncorrelated. This requirement is difficult to achieve in a robust manner by Hebbian synaptic learning, since it depends on network level information. Effective learning can yet be obtained by a neuronal process that maintains a zero sum of the incoming synaptic efficacies. This normalization drastically improves the memory capacity of associative networks, from an essentially bounded capacity to one that linearly scales with the network's size. It also enables the effective storage of patterns with heterogeneous coding levels in a single network. Such neuronal normalization can be successfully carried out by activity-dependent homeostasis of the neuron's synaptic efficacies, which was recently observed in cortical tissue. Thus, our findings strongly suggest that effective associative learning with Hebbian synapses alone is biologically implausible and that Hebbian synapses must be continuously remodeled by neuronally-driven regulatory processes in the brain.	Tel Aviv Univ, Sch Math Sci, IL-69978 Tel Aviv, Israel	Tel Aviv University	Chechik, G (corresponding author), Tel Aviv Univ, Sch Math Sci, IL-69978 Tel Aviv, Israel.		Ruppin, Eytan/R-9698-2017	Ruppin, Eytan/0000-0002-7862-3940				Bear MF, 1996, ANNU REV NEUROSCI, V19, P437, DOI 10.1146/annurev.ne.19.030196.002253; DAYAN P, 1991, BIOL CYBERN, V65, P253, DOI 10.1007/BF00206223; KEMPTER R, 1999, PHYS REV E, V65; MASSICA AG, 1998, NEURAL COMUT, V10, P451; Miller KD, 1996, NEURON, V17, P371, DOI 10.1016/S0896-6273(00)80169-5; MILLER KD, 1994, NEURAL COMPUT, V6, P100, DOI 10.1162/neco.1994.6.1.100; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; Palm G., 1995, MODELS NEURAL NETWOR, P79; SEJNOWSKI IJ, 1977, J THEOR BIOL, V69, P385, DOI 10.1016/0022-5193(77)90146-1; Tsodyks M. V., 1989, Modern Physics Letters B, V3, P555, DOI 10.1142/S021798498900087X; Turrigiano GG, 1998, NATURE, V391, P892, DOI 10.1038/36103; von der Malsburg C, 1973, Kybernetik, V14, P85; Willshaw D, 1990, NEURAL COMPUT, V2, P85, DOI 10.1162/neco.1990.2.1.85	13	0	0	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						96	102						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700014
C	Chklovskii, BB		Solla, SA; Leen, TK; Muller, KR		Chklovskii, BB			Optimal sizes of dendritic and axonal arbors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				MIDGET GANGLION-CELLS; MACAQUE MONKEY RETINA; BIPOLAR CELLS; STRIATE CORTEX; IMMUNOREACTIVITY	I consider a topographic projection between two neuronal layers with different densities of neurons. Given the number of output neurons connected to each input neuron (divergence or fan-out) and the number of input neurons synapsing on each output neuron (convergence or fan-in) I determine the widths of axonal and dendritic arbors which minimize the total volume of axons and dendrites. My analytical results can be sum marized qualitatively in the following rule: neurons of the sparser layer should have arbors wider than those of the denser layer. This agrees with the anatomical data from retinal and cerebellar neurons whose morphology and connectivity are known. The rule may be used to infer connectivity of neurons from their morphology.	Salk Inst, Sloan Ctr Theoret Neurobiol, La Jolla, CA 92037 USA	Salk Institute	Chklovskii, BB (corresponding author), Salk Inst, Sloan Ctr Theoret Neurobiol, La Jolla, CA 92037 USA.							ANDERSEN BB, 1992, J COMP NEUROL, V326, P549, DOI 10.1002/cne.903260405; BLASDEL GG, 1983, J NEUROSCI, V3, P1389; CAJAL SRY, 1995, HISTOLOGY NERVOUS SY, P95; CHERNIAK C, 1992, BIOL CYBERN, V66, P503, DOI 10.1007/BF00204115; CHKLOVSKII DB, 1999, UNPUB NATURE NEUROSC; DACEY DM, 1993, J NEUROSCI, V13, P5334, DOI 10.1523/JNEUROSCI.13-12-05334.1993; GRUNERT U, 1994, J COMP NEUROL, V348, P607, DOI 10.1002/cne.903480410; GRUNERT U, 1991, J NEUROSCI, V11, P2742; MILAM AH, 1993, VISUAL NEUROSCI, V10, P1, DOI 10.1017/S0952523800003175; MITCHISON G, 1991, P ROY SOC B-BIOL SCI, V245, P151, DOI 10.1098/rspb.1991.0102; PETERS A, 1994, CEREB CORTEX, V4, P215, DOI 10.1093/cercor/4.3.215; RODIECK RW, 1989, 1 STEPS SEEING; WASSLE H, 1989, EUR J NEUROSCI, V1, P421, DOI 10.1111/j.1460-9568.1989.tb00350.x; WATANABE M, 1989, J COMP NEUROL, V289, P434, DOI 10.1002/cne.902890308; Wiser AK, 1996, J NEUROSCI, V16, P2724; YOUNG MP, 1992, NATURE, V358, P152, DOI 10.1038/358152a0	16	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						108	114						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700016
C	Deco, G; Zihl, O		Solla, SA; Leen, TK; Muller, KR		Deco, G; Zihl, O			A neurodynamical approach to visual attention	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				SEARCH	The psychophysical evidence for "selective attention" originates mainly from visual search experiments. In this work, we formulate a hierarchical system of interconnected modules consisting in populations of neurons for modeling the underlying mechanisms involved in selective visual attention. We demonstrate that our neural system for visual search works across the visual field in parallel but due to the different intrinsic dynamics can show the two experimentally observed modes of visual attention, namely: the serial and the parallel search mode. In other words, neither explicit model of a focus of attention nor saliencies maps are used. The focus of attention appears as an emergent property of the dynamic behavior of the system. The neural population dynamics are handled in the framework of the mean-field approximation. Consequently, the whole process can be expressed as a system of coupled differential equations.	Siemens AG, Corp Technol, Neural Computat, D-81739 Munich, Germany	Siemens AG; Siemens Germany	Deco, G (corresponding author), Siemens AG, Corp Technol, Neural Computat, ZT IK 4,Otto Hahn Ring 6, D-81739 Munich, Germany.		DECO, GUSTAVO/A-6341-2008	DECO, GUSTAVO/0000-0002-8995-7583				CHELAZZI L, 1993, NATURE, V363, P345, DOI 10.1038/363345a0; DUNCAN J, 1989, PSYCHOL REV, V96, P433, DOI 10.1037/0033-295X.96.3.433; DUNCAN J, 1980, PSYCHOL REV, V87, P272, DOI 10.1037/0033-295X.87.3.272; TREISMAN A, 1988, Q J EXP PSYCHOL-A, V40, P201, DOI 10.1080/02724988843000104; Usher M, 1996, J COGNITIVE NEUROSCI, V8, P311, DOI 10.1162/jocn.1996.8.4.311	5	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						10	16						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700002
C	Ghiselli-Crippa, TB; Munro, PW		Solla, SA; Leen, TK; Muller, KR		Ghiselli-Crippa, TB; Munro, PW			Effects of spatial and temporal contiguity on the acquisition of spatial information	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Spatial information comes in two forms: direct spatial information (for example, retinal position) and indirect temporal contiguity information, since objects encountered sequentially are in general spatially close. The acquisition of spatial information by a neural network is investigated here. Given a spatial layout of several objects, networks are trained on a prediction task. Networks using temporal sequences with no direct spatial information are found to develop internal representations that show distances correlated with distances in the external layout. The influence of spatial information is analyzed by providing direct spatial information to the system during training that is either consistent with the layout or inconsistent with it. This approach allows examination of the relative contributions of spatial and temporal contiguity.	Univ Pittsburgh, Dept Informat Sci & Telecommun, Pittsburgh, PA 15260 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Ghiselli-Crippa, TB (corresponding author), Univ Pittsburgh, Dept Informat Sci & Telecommun, Pittsburgh, PA 15260 USA.							Caruana R, 1997, ADV NEUR IN, V9, P389; CLAYTON K, 1991, J EXP PSYCHOL LEARN, V17, P263; Curiel JM, 1998, J EXP PSYCHOL LEARN, V24, P202, DOI 10.1037/0278-7393.24.1.202; GHISELLICRIPPA TB, 1994, ADV NEURAL INFORMATI, V6, P1101; MCNAMARA TP, 1992, J EXP PSYCHOL LEARN, V18, P555, DOI 10.1037/0278-7393.18.3.555; SERVANSCHREIBER D, 1989, ADV NEURAL INFORMATI, V1, P643	6	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						17	23						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700003
C	Golden, RM		Solla, SA; Leen, TK; Muller, KR		Golden, RM			Kirchoff law Markov fields for analog circuit design	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Three contributions to developing an algorithm for assisting engineers in designing analog circuits are provided in this paper. First, a method for representing highly nonlinear and non-continuous analog circuits using Kirchoff current law potential functions within the context of a Markov field is described. Second, a relatively efficient algorithm for optimizing the Markov field objective function is briefly described and the convergence proof is briefly sketched. And third, empirical results illustrating the strengths and limitations of the approach are provided within the context of a JFET transistor design problem. The proposed algorithm generated a set of circuit components for the JFET circuit model that accurately generated the desired characteristic curves.	Univ Texas Dallas, Dallas, TX 75230 USA	University of Texas System; University of Texas Dallas	Golden, RM (corresponding author), Univ Texas Dallas, Dallas, TX 75230 USA.	RMGCONSULT@AOL.COM						Anderson J. A., 1995, INTRO NEURAL NETWORK, DOI [10.7551/mitpress/3905.001.0001, DOI 10.7551/MITPRESS/3905.001.0001]; Golden R.M., 1996, MATH METHODS NEURAL; SKILLING H, 1959, ELECT ENG CIRCUITS; Smolensky P., 1986, PARALLEL DISTRIBUTED, V1, P194; Vladimirescu A., 1994, THE SPICE BOOK; WINKLER G., 1995, IMAGE ANAL RANDOM FI	6	0	0	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						907	913						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700128
C	Isbell, CL; Husbands, P		Solla, SA; Leen, TK; Muller, KR		Isbell, CL; Husbands, P			The Parallel Problems Server: An interactive tool for large scale machine learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Imagine that you wish to classify data consisting of tens of thousands of examples residing in a twenty thousand dimensional space. How can one apply standard machine learning algorithms? We describe the Parallel Problems Server (PPServer) and MATLAB*P. In tandem they allow users of networked computers to work transparently on large data sets from within Matlab. This work is motivated by the desire to bring the many benefits of scientific computing algorithms and computational power to machine learning researchers. We demonstrate the usefulness of the system on a number of tasks. For example, we perform independent components analysis on very large text corpora consisting of tens of thousands of documents, making minimal changes to the original Bell and Sejnowski Matlab source (Bell and Sejnowski, 1995). Applying ML techniques to data previously beyond their reach leads to interesting analyses of both data and algorithms.	AT&T Labs, Florham Park, NJ 07932 USA	AT&T	Isbell, CL (corresponding author), AT&T Labs, 180 Pk Ave,Room A255, Florham Park, NJ 07932 USA.							BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Blackford L. S., 1997, SCALAPACK USERS GUID; DEBONET J, 1996, ADV NEURAL INFORMATI; DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9; FRAKES WB, 1992, INFORMATION RETRIEVA; Gropp W., 1994, SCI PROGRAMMING-NETH, DOI 10.7551/mitpress/7056.001.0001; ISBELL C, 1998, ADV NEURAL INFORMATI; KWOK KL, 1996, P 19 ANN INT ACM SIG, P187; MARON O, 1998, ADV NEURAL INFORMATI; Maschhoff K.J., 1996, PREL P COPP MOUNT C; OBRIEN GW, 1994, UTCS94259; SAHAMI M, 1996, P 13 INT MACH LEARN; SINGHAL A, 1997, P 20 INT C RES DEV I; PETSC PORTABLE EXTEN; [No title captured]; PPSERVER PARALLEL PR	16	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						703	709						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700100
C	Jojic, N; Frey, BJ		Solla, SA; Leen, TK; Muller, KR		Jojic, N; Frey, BJ			Topographic transformation as a discrete latent variable	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				RECOGNITION	Invariance to topographic transformations such as translation and shearing in an image has been successfully incorporated into feedforward mechanisms, e.g., "convolutional neural networks", "tangent propagation". We describe a way to add transformation invariance to a generative density model by approximating the nonlinear transformation manifold by a discrete set of transformations. An EM algorithm for the original model can be extended to the new model by computing expectations over the set of transformations. We show how to add a discrete transformation variable to Gaussian mixture modeling, factor analysis and mixtures of factor analysis. We give results on filtering microscopy images, face and facial pose clustering, and handwritten digit modeling and recognition.	Univ Illinois, Beckman Inst, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Jojic, N (corresponding author), Univ Illinois, Beckman Inst, Urbana, IL 61801 USA.							Bishop CM, 1998, NEURAL COMPUT, V10, P215, DOI 10.1162/089976698300017953; Ghahramani Z, 1997, CRGTR961; GOLEM R, 1998, SCANNING ELECT MICRO; Hinton GE, 1997, IEEE T NEURAL NETWOR, V8, P65, DOI 10.1109/72.554192; HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; SIMARD P, 1993, ADV NEURAL INFORMATI, V5; SIMARD P, 1992, ADV NEURAL INFORMATI, V4	8	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						477	483						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700068
C	Landolt, O; Gyger, S		Solla, SA; Leen, TK; Muller, KR		Landolt, O; Gyger, S			An oculo-motor system with multi-chip neuromorphic analog VLSI control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					A system emulating the functionality of a moving eye-hence the name oculo-motor system-has been built and successfully tested. It is made of an optical device for shifting the field of view of an image sensor by up to 45 degrees in any direction, four neuromorphic analog VLSI circuits implementing an oculo-motor control loop, and some off-the-shelf electronics, The custom integrated circuits communicate with each other primarily by non-arbitrated address-event buses. The system implements the behaviors of saliency-based saccadic exploration, and smooth pursuit of light spots. The duration of saccades ranges from 45 ms to 100 ms, which is comparable to human eye performance. Smooth pursuit operates on light sources moving at up to 50 degrees /s in the visual field.	CSEM SA, CH-2007 Neuchatel, Switzerland	Swiss Center for Electronics & Microtechnology (CSEM)	Landolt, O (corresponding author), CSEM SA, CH-2007 Neuchatel, Switzerland.							[Anonymous], 1994, SPRINGER INT SERIES; BOAHEN K, 1996, IEEE INT S CIRC SYST; HIGGINS CM, 1999, C ADV RES VLSI ATL M; HORIUCHI TK, 1997, THESIS CALTECH PASAD; HORIUCHI TK, 1994, ADV NEURAL PROCESSIN, V6; LANDE TS, 1998, NEUROMORPHIC SYSTEMS; LANDOLT O, 1998, PLACE CODING ANALOG; LANDOLT O, 1997, MICRONEURO 97; Mead, 1989, ANALOG VLSI NEURAL S; MORRIS TG, 1997, ANALOG INTEGRATE MAY, P13; MORTARA A, 1995, IEEE J SOLID STA JUN, P30; VENIER P, 1997, 6 INT C MICR NEUR NE	12	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						710	716						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700101
C	Li, S; Wong, KYM		Solla, SA; Leen, TK; Muller, KR		Li, S; Wong, KYM			Statistical dynamics of batch learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				MULTILAYER NEURAL NETWORKS; MICROSCOPIC EQUATIONS; PERCEPTRON	An important issue in neural computing concerns the description of learning dynamics with macroscopic dynamical variables. Recent progress on on-line learning only addresses the often unrealistic case of an infinite training set. We introduce a new framework to model batch learning of restricted sets of examples, widely applicable to any learning cost function, and fully taking into account the temporal correlations introduced by the recycling of the examples. For illustration we analyze the effects of weight decay and early stopping during the learning of teacher-generated examples.	Hong Kong Univ Sci & Technol, Dept Phys, Kowloon, Hong Kong, Peoples R China	Hong Kong University of Science & Technology	Li, S (corresponding author), Hong Kong Univ Sci & Technol, Dept Phys, Kowloon, Hong Kong, Peoples R China.							Bos S, 1998, PHYS REV E, V58, P833, DOI 10.1103/PhysRevE.58.833; Bos S, 1998, J PHYS A-MATH GEN, V31, P4835, DOI 10.1088/0305-4470/31/21/004; COOLEN ACC, 1998, ON LINE LEARNING NEU; COOLEN ACC, 1999, KCLMTH9933; HANSEN LK, 1997, IEEE INT C AC SPEECH, V4, P3205; Mezard M., 1987, SPIN GLASS THEORY IN; OPPER M, 1989, EUROPHYS LETT, V8, P389, DOI 10.1209/0295-5075/8/4/015; Saad D, 1997, PHYS REV LETT, V79, P2578, DOI 10.1103/PhysRevLett.79.2578; SAAD D, 1995, PHYS REV LETT, V74, P4337, DOI 10.1103/PhysRevLett.74.4337; TONG YW, 1999, IN PRESS P IJCNN 99; WONG KYM, 1995, EUROPHYS LETT, V30, P245, DOI 10.1209/0295-5075/30/4/010; Wong KYM, 1997, ADV NEUR IN, V9, P302	14	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						286	292						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700041
C	Movellan, JR; McClelland, JL		Solla, SA; Leen, TK; Muller, KR		Movellan, JR; McClelland, JL			Information factorization in connectionist models of perception	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				SPEECH-PERCEPTION; CONTEXT	We examine a psychophysical law that describes the influence of stimulus and context on perception. According to this law choice probability ratios factorize into components independently controlled by stimulus and context. It has been argued that this pat tern of results is incompatible with feedback models of perception. In this paper we examine this claim using neural network models defined via stochastic differential equations. We show that the law is related to a condition named channel separability and has little to do with the existence of feedback connections. In essence, channels are separable if they converge into the response units without direct lateral connections to other channels and if their sensors are not directly contaminated by external inputs to the other channels. Implications of the analysis for cognitive and computational neurosicence are discussed.	Univ Calif San Diego, Inst Neural Computat, Dept Cognit Sci, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Movellan, JR (corresponding author), Univ Calif San Diego, Inst Neural Computat, Dept Cognit Sci, La Jolla, CA 92093 USA.							Luce R, 1959, INDIVIDUAL CHOICE BE; Massaro D.W., 1998, PERCEIVING TALKING F; MASSARO DW, 1983, PERCEPT PSYCHOPHYS, V34, P338, DOI 10.3758/BF03203046; MASSARO DW, 1989, COGNITIVE PSYCHOL, V21, P398, DOI 10.1016/0010-0285(89)90014-5; MCCLELLAND JL, 1991, COGNITIVE PSYCHOL, V23, P1, DOI 10.1016/0010-0285(91)90002-6; MORTON J, 1969, PSYCHOL REV, V76, P165, DOI 10.1037/h0027366; Movellan JR, 1998, NEURAL COMPUT, V10, P1157, DOI 10.1162/089976698300017395; MOVELLAN JR, 1995, PDPCNS954 CARN MELL	8	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						45	51						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700007
C	Schoner, H; Stetter, M; Schiessl, I; Mayhew, JEW; Lund, JS; McLoughlin, N; Obermayer, K		Solla, SA; Leen, TK; Muller, KR		Schoner, H; Stetter, M; Schiessl, I; Mayhew, JEW; Lund, JS; McLoughlin, N; Obermayer, K			Application of blind separation of sources to optical recording of brain activity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				MONKEY STRIATE CORTEX	In the analysis of data recorded by optical imaging from intrinsic signals (measurement of changes of light reflectance from cortical tissue) the removal of noise and artifacts such as blood vessel patterns is a serious problem. Often bandpass filtering is used, but the underlying assumption that a spatial frequency exists, which separates the mapping component from other components (especially the global signal), is questionable. Here we propose alternative ways of processing optical imaging data, using blind source separation techniques based on the spatial decorrelation of the data. We first perform benchmarks on artificial data in order to select the way of processing, which is most robust with respect to sensor noise. We then apply it to recordings of optical imaging experiments from macaque primary visual cortex. We show that our BSS technique is able to extract ocular dominance and orientation preference maps from single condition stacks, for data, where standard post-processing procedures fail. Artifacts, especially blood vessel patterns, can often be completely removed from the maps. In summary, our method for blind source separation using extended spatial decorrelation is a superior technique for the analysis of optical recording data.	Tech Univ Berlin, Dept Comp Sci, Berlin, Germany	Technical University of Berlin	Schoner, H (corresponding author), Tech Univ Berlin, Dept Comp Sci, Berlin, Germany.	hfsch@cs.tu-berlin.de; moatl@cs.tu-berlin.de; ingos@cs.tu-berlin.de; j.e.mayhew@sheffield.ac.uk; j.lund@ucl.ac.uk; n.mcloughlin@ucl.ac.uk; oby@cs.tu-berlin.de						AMARI S, 1996, ADV NEURAL INFORMATI, V9; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; BLASDEL GG, 1986, NATURE, V321, P579, DOI 10.1038/321579a0; BLASDEL GG, 1992, J NEUROSCI, V12, P3115, DOI 10.1523/JNEUROSCI.12-08-03115.1992; Bonhoeffer Tobias, 1996, P55; Hyvarinen A, 1997, NEURAL COMPUT, V9, P1483, DOI 10.1162/neco.1997.9.7.1483; KOEHLER BU, 1999, P 1 ICA99 WORKSH AUS, V1, P359; MOLGEDEY L, 1994, PHYS REV LETT, V72, P3634, DOI 10.1103/PhysRevLett.72.3634; MULLER KR, 1999, P 1 ICA99 WORKSH AUS, V1, P87; PLATT J, 1991, ADV NEURAL INFORMATI, V4, P730; Ruger SM, 1996, ADV NEUR IN, V8, P225; SCHIESSL I, 1999, P 1 ICA99 WORKSH AUS, V1, P179; Stetter M., 1997, Society for Neuroscience Abstracts, V23, P455	13	0	1	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						949	955						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700134
C	Schuurmans, D		Solla, SA; Leen, TK; Muller, KR		Schuurmans, D			Greedy importance sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				INFERENCE	I present a simple variation of importance sampling that explicitly searches for important regions in the target distribution. I prove that the technique yields unbiased estimates, and show empirically it can reduce the variance of standard Monte Carlo estimators. This is achieved by concentrating samples in more significant regions of the sample space.	Univ Waterloo, Dept Comp Sci, Waterloo, ON N2L 3G1, Canada	University of Waterloo	Schuurmans, D (corresponding author), Univ Waterloo, Dept Comp Sci, Waterloo, ON N2L 3G1, Canada.							EVANS M, 1991, ANN STAT, V19, P382, DOI 10.1214/aos/1176347989; GEWEKE J, 1989, ECONOMETRICA, V57, P1317, DOI 10.2307/1913710; Gilks WR, 1996, MARKOV CHAIN MONTE C; Isard M., 1996, ECCV; JORDAN M, 1998, LEARNING GRAPHICAL M; Kanazawa K., 1995, UAI; KNUTH DE, 1975, MATH COMPUT, V29, P121, DOI 10.2307/2005469; MacKay D.J.C., 1998, LEARNING GRAPHICAL M; Neal R.M., 1993, PROBABILISTIC INFERE; Neal RM., 1996, BAYESIAN LEARNING NE, P29; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; SHACTER R, 1990, UNCERTAINTY ARTIFICI, V5; Tanner M. A, 1993, TOOLS STAT INFERENCE; [No title captured]	15	0	0	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						596	602						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700085
C	Weijters, T; van den Bosch, A; Postma, E		Solla, SA; Leen, TK; Muller, KR		Weijters, T; van den Bosch, A; Postma, E			Learning statistically neutral tasks without expert guidance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					In this paper, we question the necessity of levels of expert-guided abstraction in learning hard, statistically neutral classification tasks. We focus on two tasks, date calculation and parity-12, that are claimed to require intermediate levels of abstraction that must be defined by a human expert. We challenge this claim by demonstrating empirically that a single hidden-layer BP-SOM network can learn both tasks without guidance. Moreover, we analyze the network's solution for the parity-12 task and show that its solution makes use of an elegant intermediary checksum computation.	Eindhoven Univ, Eindhoven, Netherlands	Eindhoven University of Technology	Weijters, T (corresponding author), Eindhoven Univ, Eindhoven, Netherlands.		van den Bosch, Antal/AAC-2164-2019	van den Bosch, Antal/0000-0003-2493-656X				David E., 1986, PARALLEL DISTRIBUTED, P318, DOI DOI 10.5555/104279.104293; DEHAENE S, 1993, J EXP PSYCHOL GEN, V122, P371, DOI 10.1037/0096-3445.122.3.371; HILL AL, 1975, AM J PSYCHIAT, V132, P557; Hinton G, 1986, P 8 ANN C COGN SCI S, V1, DOI DOI 10.1016/J.NEUCOM.2013.03.009; Kohonen T., 1989, SELF ORG ASSOCIATIVE, V3rd; Newell A, 1972, HUMAN PROBLEM SOLVIN; NORRIS D, 1990, COGNITION, V35, P277, DOI 10.1016/0010-0277(90)90025-F; PRECHELT L, 1994, 2494 U KARLSR FAK IN; Quinlan J., 2014, C4 5 PROGRAMS MACHIN, DOI DOI 10.1007/BF00993309; THORNTON C, 1996, P 11 BIENN C CAN SOC, P362; Weijters A., 1997, Connection Science, V9, P235, DOI 10.1080/095400997116621	11	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						73	79						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700011
C	Yang, HH; Hermansky, H		Solla, SA; Leen, TK; Muller, KR		Yang, HH; Hermansky, H			Search for information bearing components in speech	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					In this paper, we use mutual information to characterize the distributions of phonetic and speaker/channel information in a time-frequency space. The mutual information (MI) between the phonetic label and one feature, and the joint mutual information (JMI) between the phonetic label and two or three features are estimated. The Miller's bias formulas for entropy and mutual information estimates are extended to include higher order terms. The MI and the JMI for speaker/channel recognition are also estimated. The results are complementary to those for phonetic classification. Our results show how the phonetic information is locally spread and how the speaker/channel information is globally spread in time and frequency.	Oregon Grad Inst Sci & Technol, Dept Elect & Comp Engn, Beaverton, OR 97006 USA		Yang, HH (corresponding author), Oregon Grad Inst Sci & Technol, Dept Elect & Comp Engn, 2000 NW,Walker Rd, Beaverton, OR 97006 USA.	hyang@ece.ogi.edu; hynek@ece.ogi.edu						Bilmes JA, 1998, INT CONF ACOUST SPEE, P469, DOI 10.1109/ICASSP.1998.674469; Cole R., 1994, ICSLP 94. 1994 International Conference on Spoken Language Processing, P1815; HERMANSKY H, 1990, J ACOUST SOC AM, V87, P1738, DOI 10.1121/1.399423; Miller G. A., 1954, INFORM THEORY PSYCHO, P95; Morris A., 1993, Computer Speech and Language, V7, P121, DOI 10.1006/csla.1993.1006; Yang H, 1999, INT CONF ACOUST SPEE, P225, DOI 10.1109/ICASSP.1999.758103; YANG HH, 1999, IN PRESS SPEECH COMM	7	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						803	809						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700114
C	Yang, ZY; Zemel, RS		Solla, SA; Leen, TK; Muller, KR		Yang, ZY; Zemel, RS			Managing uncertainty in cue combination	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				MOTION CUES; TEXTURE; DEPTH; INTEGRATION	We develop a hierarchical generative model to study cue combination. The model maps a global shape parameter to local cue-specific parameters, which in turn generate an intensity image. Inferring shape from images is achieved by inverting this model. Inference produces a probability distribution at each level; using distributions rather than a single value of underlying variables at each stage preserves information about the validity of each local cue for the given image. This allows the model, unlike standard combination models, to adaptively weight each cue based on general cue reliability and specific image context. We describe the results of a cue combination psychophysics experiment we conducted that allows a direct comparison with the model. The model provides a good fit to our data and a natural account for some interesting aspects of cue combination.	Duke Univ, Med Ctr, Dept Neurobiol, Durham, NC 27710 USA	Duke University	Yang, ZY (corresponding author), Duke Univ, Med Ctr, Dept Neurobiol, Box 3209, Durham, NC 27710 USA.							HORN BKP, 1977, ARTIF INTELL, V8, P201, DOI 10.1016/0004-3702(77)90020-0; Jacobs RA, 1999, VISION RES, V39, P4062, DOI 10.1016/S0042-6989(99)00120-0; JOHNSTON EB, 1994, VISION RES, V34, P2259, DOI 10.1016/0042-6989(94)90106-6; Knill D. C., 1996, PERCEPTION BAYESIAN, P239; Knill DC, 1998, VISION RES, V38, P1655, DOI 10.1016/S0042-6989(97)00324-6; LANDY MS, 1995, VISION RES, V35, P389, DOI 10.1016/0042-6989(94)00176-M; Malina M, 1997, J ENDOVASC SURG, V4, P23, DOI 10.1583/1074-6218(1997)004<0023:CAMAEG>2.0.CO;2; PENTLAND AP, 1984, IEEE T PATTERN ANAL, V6, P170, DOI 10.1109/TPAMI.1984.4767501; YOUNG MJ, 1993, VISION RES, V33, P2685, DOI 10.1016/0042-6989(93)90228-O; Yuille A. L., 1996, PERCEPTION BAYESIAN, P123, DOI DOI 10.1017/CBO9780511984037.006; Zemel RS, 1998, NEURAL COMPUT, V10, P403, DOI 10.1162/089976698300017818	11	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						869	875						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700123
C	Zlochin, M; Baram, Y		Solla, SA; Leen, TK; Muller, KR		Zlochin, M; Baram, Y			Manifold stochastic dynamics for Bayesian learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				MONTE-CARLO	We propose a new Markov Chain Monte Carlo algorithm which is st generalization of the stochastic dynamics method. The algorithm performs exploration of the state space using its intrinsic geometric structure, facilitating efficient sampling of complex distributions. Applied to Bayesian learning in neural networks, our algorithm was found to perform at least as well as the best state-of-the-art method while consuming considerably less time.	Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Zlochin, M (corresponding author), Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.							Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; ANDERSEN HC, 1980, J CHEM PHYS, V3, P589; Buntine W. L., 1991, Complex Systems, V5, P603; Chavel I., 1993, RIEMANNIAN GEOMETRY; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Gear CW., 1971, NUMERICAL INITIAL VA; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Gilks WR, 1996, MARKOV CHAIN MONTE C; HOROWITZ AM, 1991, PHYS LETT B, V268, P247, DOI 10.1016/0370-2693(91)90812-5; Hwang C.-R., 1993, ANN APPL PROBAB, V3, P897; MacKay D.J.C., 1992, THESIS CALTECH; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Neal RM., 1996, BAYESIAN LEARNING NE, P29	13	0	0	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						694	700						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700099
C	Adorjan, P; Obermayer, K		Kearns, MS; Solla, SA; Cohn, DA		Adorjan, P; Obermayer, K			Contrast adaptation in simple cells by changing the transmitter release probability	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				CAT VISUAL-CORTEX; SYNAPTIC DEPRESSION; PATTERN ADAPTATION; NEURONS	The contrast response function (CRF) of many neurons in the primary visual cortex saturates and shifts towards higher contrast values following prolonged presentation of high contrast visual stimuli. Using a recurrent neural network of excitatory spiking neurons with adapting synapses we show that both effects could be explained by a fast and a slow component in the synaptic adaptation. (i) Fast synaptic depression leads to saturation of the CRF and phase advance in the cortical response to high contrast stimuli. (ii) Slow adaptation of the synaptic transmitter release probability is derived such that the mutual information between the input and the output of a cortical neuron is maximal. This component-given by infomax learning rule-explains contrast adaptation of the averaged membrane potential (DC component) as well as the surprising experimental result, that the stimulus modulated component (F1 component) of a cortical cell's membrane potential adapts only weakly. Based on our results, we propose a new experiment to estimate the strength of the effective excitatory feedback to a cortical neuron, and we also suggest a relatively simple experimental test to justify our hypothesized synaptic mechanism for contrast adaptation.	Tech Univ Berlin, Dept Comp Sci, D-10587 Berlin, Germany	Technical University of Berlin	Adorjan, P (corresponding author), Tech Univ Berlin, Dept Comp Sci, FR2-1,Franklinstr 28-29, D-10587 Berlin, Germany.	adp@cs.tu-berlin.de; oby@cs.tu-berlin.de						Ahmed B, 1997, CEREB CORTEX, V7, P559, DOI 10.1093/cercor/7.6.559; BARLOW H, 1989, COMP NEUR S, P54; BELLAMY GR, 1995, RURAL HLTH FOCUS, V6, P1; Carandini M, 1997, SCIENCE, V276, P949, DOI 10.1126/science.276.5314.949; Carandini M, 1997, J NEUROSCI, V17, P8621; Carandini M, 1998, NEUROPHARMACOLOGY, V37, P501, DOI 10.1016/S0028-3908(98)00069-0; Chance FS, 1998, J NEUROSCI, V18, P4785; Engel AK, 1997, CEREB CORTEX, V7, P571, DOI 10.1093/cercor/7.6.571; FINLAYSON PG, 1995, EXP BRAIN RES, V106, P145; KAPLAN E, 1987, J PHYSIOL-LONDON, V391, P267, DOI 10.1113/jphysiol.1987.sp016737; LAUGHLIN SB, 1994, PROG RETIN EYE RES, V13, P165, DOI 10.1016/1350-9462(94)90009-4; McLean J, 1996, VISUAL NEUROSCI, V13, P1069, DOI 10.1017/S0952523800007720; OHZAWA I, 1985, J NEUROPHYSIOL, V54, P651, DOI 10.1152/jn.1985.54.3.651; SAUL AB, 1995, VISUAL NEUROSCI, V12, P191, DOI 10.1017/S0952523800007872; Senn W, 1996, NEURAL NETWORKS, V9, P575, DOI 10.1016/0893-6080(95)00109-3; STEMMLER M, 1999, ADV NEURAL INFORMATI, P11; Tsodyks MV, 1997, P NATL ACAD SCI USA, V94, P719, DOI 10.1073/pnas.94.2.719; VIDYASAGAR TR, 1990, NEUROSCIENCE, V36, P175, DOI 10.1016/0306-4522(90)90360-G	18	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						76	82						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700011
C	Baluja, S		Kearns, MS; Solla, SA; Cohn, DA		Baluja, S			Making templates rotationally invariant: An application to rotated digit recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					This paper describes a simple and efficient method to make template-based object classification invariant to in-plane rotations. The task is divided into two parts: orientation discrimination and classification. The key idea is to perform the orientation discrimination before the classification. This can be accomplished by hypothesizing, in turn, that the input image belongs to each class of interest. The image can then be rotated to maximize its similarity to the training images in each class (these contain the prototype object in an upright orientation). This process yields a set of images, at least one of which will have the object in an upright position. The resulting images call then be classified by models which have been trained with only upright examples. This approach has been successfully applied to two real-world vision-based tasks: rotated handwritten digit recognition and rotated face detection in cluttered scenes.	Carnegie Mellon Univ, Justsyst Pittsburgh Res Ctr, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Baluja, S (corresponding author), Carnegie Mellon Univ, Justsyst Pittsburgh Res Ctr, Pittsburgh, PA 15213 USA.							Baluja Shumeet, 1997, JPRCTR97001; GUYON I, 1989, INT JOINT C NEURAL N, V2, P127; JAIN A, 1997, 9733 TR MSUCPS; LeCun Y., 1995, Neural Networks: The Statistical Mechanics Perspective. Proceedings of the CTP-PBSRI. Joint Workshop on Theoretical Physics, P261; LeCun Y., 1995, ICANN '95. International Conference on Artificial Neural Networks. Neuronimes '95 Scientific Conference, P53; LECUN Y, 1990, ADV NEURAL INFORMATI, V2; LEE Y, 1991, NEURAL COMPUT, V3, P3; POMERLEAU DA, 1993, NEURAL NETWORK PRECE; ROWLEY H, 1998, IN PRESS P COMPUTER; ROWLEY HA, 1998, IEEE T PATTERN ANAL, V20; SATO T, 1998, IN PRESS IEEE INT WO; Satoh S, 1997, P IEEE C COMP VIS PA	12	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						847	853						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700119
C	Brown, LJ; Gonye, GE; Schwaber, JS		Kearns, MS; Solla, SA; Cohn, DA		Brown, LJ; Gonye, GE; Schwaber, JS			Non-linear PI control inspired by biological control systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				BARORECEPTOR REFLEX; NEURONS	A non-linear modification to PI control is motivated by a model of a signal transduction pathway active in mammalian blood pressure regulation. This control algorithm, labeled PII (proportional with intermittent integral), is appropriate for plants requiring exact set-point matching and disturbance attenuation in the presence of infrequent step changes in load disturbances or set-point. The proportional aspect of the controller is independently designed to be a disturbance attenuator and set-point matching is achieved by intermittently invoking an integral controller. The mechanisms observed in the Angiotensin II/AT1 signaling pathway are used to control the switching of the integral control. Improved performance over PI control is shown on a model of cyclopentenol production. A sign change in plant gain at the desirable operating point causes traditional PI control to result in an unstable system. Application of this new approach to this problem results in stable exact set-point matching for achievable set-points.	Dupont Co, Expt Stn, Wilmington, DE 19880 USA	DuPont	Brown, LJ (corresponding author), Dupont Co, Expt Stn, Wilmington, DE 19880 USA.			schwaber, james/0000-0003-0598-7345				FUXE K, 1988, CLIN EXP HYPERTENS A, V10, P143, DOI 10.3109/10641968809075969; HENSON MA, 1994, IND ENG CHEM RES, V33, P2453, DOI 10.1021/ie00034a030; Herbert J, 1996, REGUL PEPTIDES, V66, P13, DOI 10.1016/0167-0115(96)00044-4; KUMADA MN, 1988, PROGR NEUROPHYSIOL, V35, P331; KWATRA HS, 1997, CHEM ENG SCI; Li YW, 1996, CIRC RES, V78, P274, DOI 10.1161/01.RES.78.2.274; Ogunnaike B. A., 1995, PROCESS DYNAMICS MOD; Pottmann M, 1996, CHEM ENG SCI, V51, P931, DOI 10.1016/0009-2509(95)00338-X; Spyer K. M., 1990, CENTRAL REGULATION A, P168	9	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						975	981						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700137
C	Cauwenberghs, G; Waskiewicz, J		Kearns, MS; Solla, SA; Cohn, DA		Cauwenberghs, G; Waskiewicz, J			Analog VLSI cellular implementation of the boundary contour system	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				VISION	We present an analog VLSI cellular architecture implementing a simplified version of the Boundary Contour System (BCS) for real-time image processing. Inspired by neuromorphic models across several layers of visual cortex, the design integrates in each pixel the functions of simple cells, complex cells, hyper-complex cells, and bipole cells, in three orientations interconnected on a hexagonal grid. Analog current-mode CMOS circuits are used throughout to perform edge detection, local inhibition, directionally selective long-range diffusive kernels, and renormalizing global gain control. Experimental results from a fabricated 12 x 10 pixel prototype in 1.2 mu m CMOS technology demonstrate the robustness of the architecture in selecting image contours in a cluttered and noisy background.	Johns Hopkins Univ, Dept Elect & Comp Engn, Baltimore, MD 21218 USA	Johns Hopkins University	Cauwenberghs, G (corresponding author), Johns Hopkins Univ, Dept Elect & Comp Engn, 3400 N Charles St, Baltimore, MD 21218 USA.							ANDREOU AG, 1991, IEEE T NEURAL NETWOR, V2, P205, DOI 10.1109/72.80331; Boahen KA, 1996, IEEE MICRO, V16, P30, DOI 10.1109/40.540078; Fragniere E, 1997, ELECTRON LETT, V33, P1913, DOI 10.1049/el:19971348; GROSSBERG S, 1993, NEURAL NETWORKS, V6, P463, DOI 10.1016/S0893-6080(05)80052-8; GROSSBERG S, 1988, OPTICS NEWS      AUG, P5; GROSSBERG S, 1996, NEURAL NETWORKS  JAN, V9; HARRIS JG, 1990, SCIENCE, V248, P1209, DOI 10.1126/science.2349479; Li ZP, 1998, NEURAL COMPUT, V10, P903, DOI 10.1162/089976698300017557; McIlrath LD, 1996, IEEE J SOLID-ST CIRC, V31, P1239, DOI 10.1109/4.535407; Venier P, 1997, IEEE J SOLID-ST CIRC, V32, P177, DOI 10.1109/4.551909	10	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						657	663						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700093
C	Chance, FS; Nelson, SB; Abbott, LF		Kearns, MS; Solla, SA; Cohn, DA		Chance, FS; Nelson, SB; Abbott, LF			Recurrent cortical amplification produces complex cell responses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				CAT STRIATE CORTEX; RECEPTIVE-FIELD PROPERTIES; VISUAL-CORTEX; ORIENTATION SELECTIVITY; THALAMIC INPUT; AREA 17; ORGANIZATION; CONNECTIVITY; MECHANISMS; VELOCITY	Cortical amplification has been proposed as a mechanism for enhancing the selectivity of neurons in the primary visual cortex. Less appreciated is the fact that the same form of amplification can also be used to de-tune or broaden selectivity. Using a network model with recurrent cortical circuitry, we propose that the spatial phase invariance of complex cell responses arises through recurrent amplification of feedforward input. Neurons in the network respond like simple cells at low gain and complex cells at high gain. Similar recurrent mechanisms may play a role in generating invariant representations of feedforward input elsewhere in the visual processing pathway.	Brandeis Univ, Volen Ctr, Waltham, MA 02454 USA	Brandeis University	Chance, FS (corresponding author), Brandeis Univ, Volen Ctr, Waltham, MA 02454 USA.							ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; Alonso JM, 1998, NAT NEUROSCI, V1, P395, DOI 10.1038/1609; BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844; CHANCE FS, UNPUB COMPLEX CELLS; Chung S, 1998, NEURON, V20, P1177, DOI 10.1016/S0896-6273(00)80498-5; DOUGLAS RJ, 1995, SCIENCE, V269, P981, DOI 10.1126/science.7638624; FERSTER D, 1983, J PHYSIOL-LONDON, V342, P181, DOI 10.1113/jphysiol.1983.sp014846; Ferster D, 1996, NATURE, V380, P249, DOI 10.1038/380249a0; HAMMOND P, 1977, EXP BRAIN RES, V30, P275; HAMMOND P, 1975, EXP BRAIN RES, V22, P427; Hawken MJ, 1996, VISUAL NEUROSCI, V13, P477, DOI 10.1017/S0952523800008154; HOFFMANN KP, 1971, BRAIN RES, V32, P460, DOI 10.1016/0006-8993(71)90340-4; HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837; MALPELI JG, 1983, J NEUROPHYSIOL, V49, P595, DOI 10.1152/jn.1983.49.3.595; MALPELI JG, 1986, J NEUROPHYSIOL, V56, P1062, DOI 10.1152/jn.1986.56.4.1062; Mel BW, 1998, J NEUROSCI, V18, P4325; MIGNARD M, 1991, SCIENCE, V251, P1249, DOI 10.1126/science.1848727; MOVSHON JA, 1978, J PHYSIOL-LONDON, V283, P53, DOI 10.1113/jphysiol.1978.sp012488; MOVSHON JA, 1978, J PHYSIOL-LONDON, V283, P79, DOI 10.1113/jphysiol.1978.sp012489; MOVSHON JA, 1975, J PHYSIOL-LONDON, V249, P445, DOI 10.1113/jphysiol.1975.sp011025; Shulz D. E., 1993, Society for Neuroscience Abstracts, V19, P628; SILLITO AM, 1975, J PHYSIOL-LONDON, V250, P305, DOI 10.1113/jphysiol.1975.sp011056; SINGER W, 1975, J NEUROPHYSIOL, V38, P1080, DOI 10.1152/jn.1975.38.5.1080; SOMERS DC, 1995, J NEUROSCI, V15, P5448; Sompolinsky H, 1997, CURR OPIN NEUROBIOL, V7, P514, DOI 10.1016/S0959-4388(97)80031-1; TOYAMA K, 1981, J NEUROPHYSIOL, V46, P202, DOI 10.1152/jn.1981.46.2.202; TREVES A, 1993, NETWORK-COMP NEURAL, V4, P259, DOI 10.1088/0954-898X/4/3/002	27	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						90	96						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700013
C	Christianson, GB; Becker, S		Kearns, MS; Solla, SA; Cohn, DA		Christianson, GB; Becker, S			A model for associative multiplication	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				SKILL	Despite the fact that mental arithmetic is based on only a few hundred basic facts and some simple algorithms, humans have a difficult time mastering the subject, and even experienced individuals make mistakes. Associative multiplication, the process of doing multiplication by memory without the use of rules or algorithms, is especially problematic. Humans exhibit certain characteristic phenomena in performing associative multiplications, both in the type of error and in the error frequency. We propose a model for the process of associative multiplication, and compare its performance in both these phenomena with data from normal humans and from the model proposed by Anderson et al (1994).	CALTECH, Computat & Neural Syst, Pasadena, CA 91125 USA	California Institute of Technology	Christianson, GB (corresponding author), CALTECH, Computat & Neural Syst, 139-74, Pasadena, CA 91125 USA.			Becker, Suzanna/0000-0002-2645-070X				ANDERSON JA, 1995, INTRO NEURAL NETWORK, P493; ANDERSON JA, 1994, NEURAL NETWORKS KNOW, P311; CAMPBELL JID, 1985, CAN J PSYCHOL, V39, P338, DOI 10.1037/h0080065; Girelli L, 1996, CORTEX, V32, P49, DOI 10.1016/S0010-9452(96)80016-5; HAMMAN MS, 1986, COGNITION INSTRUCT, V3, P173; MANDLER G, 1982, J EXP PSYCHOL GEN, V111, P1, DOI 10.1037/0096-3445.111.1.1; MCCLOSKEY M, 1991, J EXP PSYCHOL LEARN, V17, P377, DOI 10.1037/0278-7393.17.3.377; Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V2, P7; SIEGLER RS, 1988, J EXP PSYCHOL GEN, V117, P258, DOI 10.1037/0096-3445.117.3.258; STAZYK EH, 1982, J EXPT PSYCHOL LEARN, V8, P355; WARRINGTON EK, 1982, Q J EXP PSYCHOL-A, V34, P31, DOI 10.1080/14640748208400856	11	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						17	23						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700003
C	Coggins, RJ; Wang, RJW; Jabri, MA		Kearns, MS; Solla, SA; Cohn, DA		Coggins, RJ; Wang, RJW; Jabri, MA			A micropower CMOS adaptive amplitude and shift invariant vector quantiser	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				ANALOG	In this paper we describe the architecture, implementation and experimental results for an Intracardiac Electrogram (ICEG) classification and compression chip. The chip processes and vector-quantises 30 dimensional analogue vectors while consuming a maximum of 2.5 mu W power for a heart rate of 60 beats per minute (1 vector per second) from a 3.3 V supply. This represents a significant advance on previous work which achieved ultra low power supervised morphology classification since the template matching scheme used in this chip enables unsupervised blind classification of abnormal rhythms and the computational support for low bit rate data compression. The adaptive template matching scheme used is tolerant to amplitude variations, and inter- and intra-sample time shifts.	Univ Sydney, Comp Engn Lab, Sch Elect & Informat Engn, Sydney, NSW 2006, Australia	University of Sydney	Coggins, RJ (corresponding author), Univ Sydney, Comp Engn Lab, Sch Elect & Informat Engn, J03, Sydney, NSW 2006, Australia.							Cauwenberghs G., 1995, Advances in Neural Information Processing Systems 7, P779; COGGINS R, 1995, IEEE J SOLID-ST CIRC, V30, P542, DOI 10.1109/4.384167; COGGINS RJ, 1996, THESIS U SYDNEY SYDN; COGGINS RJ, 1997, NEURAL NETWORKS SIGN, V7, P226; FURTH PM, 1995, ELECTRON LETT, V31, P545, DOI 10.1049/el:19950376; JABRI M, 1992, IEEE T NEURAL NETWOR, V3, P154, DOI 10.1109/72.105429; KRUMMENACHER F, 1988, IEEE J SOLID-ST CIRC, V23, P750, DOI 10.1109/4.315; NAVE G, 1993, IEEE T BIO-MED ENG, V40, P877, DOI 10.1109/10.245608; SEEVINCK E, 1988, ANAL SYNTHESIS TRANS; TYSON GT, 1993, P INT SOL STAT CIRC, P38	10	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						671	677						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700095
C	Coolen, ACC; Saad, D		Kearns, MS; Solla, SA; Cohn, DA		Coolen, ACC; Saad, D			Dynamics of supervised learning with restricted training sets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				PERCEPTRONS	We study the dynamics of supervised learning in layered neural networks, in the regime where the size p of the training set is proportional to the number N of inputs. Here the local fields are no longer described by Gaussian distributions. We use dynamical replica theory to predict the evolution of macroscopic observables, including the relevant error measures, incorporating the old formalism in the limit p/N --> infinity.	Univ London Kings Coll, Dept Math, London WC2R 2LS, England	University of London; King's College London	Coolen, ACC (corresponding author), Univ London Kings Coll, Dept Math, Strand, London WC2R 2LS, England.							BIEHL M, 1992, EUROPHYS LETT, V20, P733, DOI 10.1209/0295-5075/20/8/012; BIEHL M, 1995, J PHYS A-MATH GEN, V28, P643, DOI 10.1088/0305-4470/28/3/018; Coolen ACC, 1996, PHYS REV B, V53, P8184, DOI 10.1103/PhysRevB.53.8184; COOLEN ACC, 1998, UNPUB; HORNER H, 1992, Z PHYS B CON MAT, V87, P371, DOI 10.1007/BF01309290; HORNER H, 1992, Z PHYS B CON MAT, V86, P291, DOI 10.1007/BF01313839; KINOUCHI O, 1992, J PHYS A-MATH GEN, V25, P6243, DOI 10.1088/0305-4470/25/23/020; KINZEL W, 1990, EUROPHYS LETT, V13, P473, DOI 10.1209/0295-5075/13/5/016; Mace CWH, 1998, STAT COMPUT, V8, P55, DOI 10.1023/A:1008896910704; MEZARD M, 1987, SPIN GLASS THEORY BE; SAAD D, 1995, PHYS REV LETT, V74, P4337, DOI 10.1103/PhysRevLett.74.4337	11	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						197	203						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700028
C	Coughlan, JM; Yuille, AL		Kearns, MS; Solla, SA; Cohn, DA		Coughlan, JM; Yuille, AL			A phase space approach to Minimax Entropy learning and the Minutemax approximations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				NATURAL IMAGES; STATISTICS	There has been much recent work on measuring image statistics and on learning probability distributions on images. We observe that the mapping from images to statistics is many-to-one and show it can be quantified by a phase space factor. This phase space approach throws light on the Minimax Entropy technique for learning Gibbs distributions on images with potentials derived from image statistics and elucidates the ambiguities that are inherent to determining the potentials. In addition, it shows that if the phase factor can be approximated by an analytic distribution then this approximation yields a swift "Minutemax" algorithm that vastly reduces the computation time for Minimax entropy learning. An illustration of this concept, using a Gaussian to approximate the phase factor, gives a good approximation to the results of Zhu and Mumford (1997) in just seconds of CPU time. The phase space approach also gives insight into the multi-scale potentials found by Zhu and Mumford (1997) and suggests that the forms of the potentials are influenced greatly by phase space considerations. Finally, we prove that probability distributions learned in feature space alone are equivalent to Minimax Entropy learning with a multinomial approximation of the phase factor.	Smith Kettlewell Inst, San Francisco, CA 94115 USA	The Smith-Kettlewell Eye Research Institute	Coughlan, JM (corresponding author), Smith Kettlewell Inst, San Francisco, CA 94115 USA.			Yuille, Alan L./0000-0001-5207-9249				COUGHLAN JM, 1999, UNPUB PHASE SPACE MI; FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379; Knill DC, 1996, PERCEPTION BAYESIAN; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Ripley BD., 1996; RUDERMAN DL, 1994, PHYS REV LETT, V73, P814, DOI 10.1103/PhysRevLett.73.814; ZHU S, 1997, NEURAL COMPUTATION, V9; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420; ZHU SC, 1997, IEEE T PAMI, V19	9	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						761	767						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700107
C	Dror, G; Abramowicz, H; Horn, D		Kearns, MS; Solla, SA; Cohn, DA		Dror, G; Abramowicz, H; Horn, D			Vertex identification in high energy physics experiments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					In High Energy Physics experiments one has to sort through a high flux of events, at a rate of tens of MHz, and select the few that are of interest. One of the key factors in making this decision is the location of the vertex where the interaction, that led to the event, took place. Here we present a novel solution to the problem of finding the location of the vertex, based on two feedforward neural networks with fixed architectures, whose parameters are chosen so as to obtain a high accuracy. The system is tested on simulated data sets, and is shown to perform better than conventional algorithms.	Acad Coll Tel Aviv Yaffo, Dept Comp Sci, IL-64044 Tel Aviv, Israel		Dror, G (corresponding author), Acad Coll Tel Aviv Yaffo, Dept Comp Sci, IL-64044 Tel Aviv, Israel.			Horn, David/0000-0003-2708-186X				Abramowicz H, 1996, NUCL INSTRUM METH A, V378, P305, DOI 10.1016/0168-9002(96)00417-2; DENBY B, 1993, NEURAL COMPUT, V5, P505, DOI 10.1162/neco.1993.5.4.505; DERRICK M, 1992, PHYS LETT B, V293, P465, DOI 10.1016/0370-2693(92)90914-P; Duda R.O., 1973, J ROYAL STAT SOC SER; FOSTER B, 1994, NUCL INSTRUM METH A, V338, P254, DOI 10.1016/0168-9002(94)91313-7; Hough P. V. C., US Patent, Patent No. [3,069,654, 3069654]; HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455; MOZER MC, 1997, ADV NEURAL INFORMATI, V9, P925; QUADT A, 1997, THESIS U OXFORD	9	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						868	874						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700122
C	During, A; Coolen, ACC; Sherrington, D		Kearns, MS; Solla, SA; Cohn, DA		During, A; Coolen, ACC; Sherrington, D			Phase diagram and storage capacity of sequence storing neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				DYNAMICS	We solve the dynamics of Hopfield-type neural networks which store sequences of patterns, close to saturation. The asymmetry of the interaction matrix in such models leads to violation of detailed balance, ruling out an equilibrium statistical mechanical analysis. Using generating functional methods we derive exact closed equations for dynamical order parameters, viz. the sequence overlap and correlation and response functions, in the limit of an infinite system size. We calculate the time translation invariant solutions of these equations, describing stationary limit-cycles, which leads to a phase diagram. The effective retarded self-interaction usually appearing in symmetric models is here found to vanish, which causes a significantly enlarged storage capacity of alpha(c) approximate to 0.269, compared to alpha(c) approximate to 0.139 for Hopfield networks storing static patterns. Our results are tested against extensive computer simulations and excellent agreement is found.	Univ Oxford, Dept Phys, Oxford OX1 3NP, England	University of Oxford	During, A (corresponding author), Univ Oxford, Dept Phys, Oxford OX1 3NP, England.							AMARI S, 1988, NEURAL NETWORKS, V1, P63, DOI 10.1016/0893-6080(88)90022-6; AMIT DJ, 1985, PHYS REV LETT, V55, P1530, DOI 10.1103/PhysRevLett.55.1530; DEDOMINICIS C, 1978, PHYS REV B, V18, P4913, DOI 10.1103/PhysRevB.18.4913; During A, 1998, J PHYS A-MATH GEN, V31, P8607, DOI 10.1088/0305-4470/31/43/005; KUHN R, 1991, TEMPORAL ASS, P213; RIEGER H, 1988, J PHYS A-MATH GEN, V21, pL263, DOI 10.1088/0305-4470/21/4/014; SHERRINGTON D, 1975, PHYS REV LETT, V35, P1972	7	0	0	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						211	217						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700030
C	Held, M; Puzicha, J; Buhmann, JM		Kearns, MS; Solla, SA; Cohn, DA		Held, M; Puzicha, J; Buhmann, JM			Visualizing group structure	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Cluster analysis is a fundamental principle in exploratory data analysis, providing the user with a description of the group structure of given data, A key problem in this context is the interpretation and visualization of clustering solutions in high-dimensional or abstract data spaces. In particular, probabilistic descriptions of the group structure, essential to capture inter-cluster relationships, are hardly assessable by simple inspection of the probabilistic assignment variables. We present a novel approach to the visualization of group structure. It is based on a statistical model of the object assignments which have been observed or estimated by a probabilistic clustering procedure. The objects or data points are embedded in a low dimensional Euclidean space by approximating the observed data statistics with a Gaussian mixture model. The algorithm provides a new approach to the visualization of the inherent structure for a broad variety of data types, e.g. histogram data, proximity data and co-occurrence data. To demonstrate the power of the approach, histograms of textured images are visualized as an example of a large-scale data mining application.	Inst Informat 3, D-53117 Bonn, Germany	University of Bonn	Held, M (corresponding author), Inst Informat 3, Romerstr 164, D-53117 Bonn, Germany.		Buhmann, Joachim/AAU-4760-2020					Bishop CM, 1998, NEURAL COMPUT, V10, P215, DOI 10.1162/089976698300017953; BISHOP CM, 1998, NCRG96028 DEPT COMP; COX TF, 1994, MONOGRAPHS STAT APPL, V59; DUNN JC, 1975, J CYBERNETICS, V3, P32; GATH I, 1989, IEEE T PATTERN ANAL, V11, P773, DOI 10.1109/34.192473; Hofmann T, 1997, IEEE T PATTERN ANAL, V19, P1, DOI 10.1109/34.566806; HOFMANN T, 1909, ADV NEURAL INFORMATI, V11; PEREIRA F, 1993, 31ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P183; ROSE K, 1990, PATTERN RECOGN LETT, V11, P589, DOI 10.1016/0167-8655(90)90010-Y	9	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						452	458						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700064
C	Herschkowitz, D; Nadal, JP		Kearns, MS; Solla, SA; Cohn, DA		Herschkowitz, D; Nadal, JP			Unsupervised and supervised clustering: the mutual information between parameters and observations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Recent works in parameter estimation and neural coding have demonstrated that optimal performance are related to the mutual information between parameters and data. We consider the mutual information in the case where the dependency in the parameter (a vector theta) of the conditional p.d.f. of each observation (a vector zeta), is through the scalar product theta.zeta only. We derive bounds and asymptotic behaviour fur the mutual information and compare with results obtained on the same model with the "replica technique".	Ecole Normale Super, Lab Phys Stat, F-75231 Paris 05, France	UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite	Herschkowitz, D (corresponding author), Ecole Normale Super, Lab Phys Stat, 24 Rue Lhomond, F-75231 Paris 05, France.	herschko@lps.ens.fr; nadal@lps.ens.fr	Nadal, Jean-Pierre/HHM-8804-2022	Nadal, Jean-Pierre/0000-0003-0022-0647				BRUNEL N, 1998, IN PRESS NEURAL COMP; Buhot A, 1998, PHYS REV E, V57, P3326, DOI 10.1103/PhysRevE.57.3326; CLARKE BS, 1990, IEEE T INFORM THEORY, V36, P453, DOI 10.1109/18.54897; HAUSSLER D, 1995, 8 ANN WORKSH COMP LE, P402; NADAL JP, 1994, NEURAL COMPUT, V6, P489; OPPER M, 1995, PHYS REV LETT, V75, P3772, DOI 10.1103/PhysRevLett.75.3772; OPPER M, 1995, PHYSICS NEURAL NETWO, P151; Reimann P, 1996, PHYS REV E, V53, P3989, DOI 10.1103/PhysRevE.53.3989; Rissanen JJ, 1996, IEEE T INFORM THEORY, V42, P40, DOI 10.1109/18.481776; VANDENBROECK C, 1997, P TANC WORKSH; WATKIN TLH, 1994, J PHYS A-MATH GEN, V27, P1899, DOI 10.1088/0305-4470/27/6/016	11	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						232	238						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700033
C	Higgins, CM; Koch, C		Kearns, MS; Solla, SA; Cohn, DA		Higgins, CM; Koch, C			An integrated vision sensor for the computation of optical flow singular points	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				DIRECTION	A robust, integrative algorithm is presented for computing the position of the focus of expansion or axis of rotation (the singular point) in optical flow fields such as those generated by self-motion. Measurements are shown of a fully parallel CMOS analog VLSI motion sensor array which computes the direction of local motion (sign of optical flow) at each pixel and can directly implement this algorithm. The flow field singular point is computed in real time with a power consumption of less than 2 mW. Computation of the singular point for more general flow fields requires measures of field expansion and rotation, which it is shown can also be computed in real-time hardware, again using only the sign of the optical flow field. These measures, along with the location of the singular point, provide robust real-time self-motion information for the visual guidance of a moving platform such as a robot.	CALTECH, Div Biol, Pasadena, CA 91125 USA	California Institute of Technology	Higgins, CM (corresponding author), CALTECH, Div Biol, 139-74, Pasadena, CA 91125 USA.			Koch, Christof/0000-0001-6482-8067				DEUTSCHMANN R, 1997, P INT C ART NEUR NET, P1163; DEUTSCHMANN RA, 1998, P DTSCH ARB MUST; FERMULLER C, 1997, INT J COMPUT VISION, V21, P233; Indiveri G, 1996, ADV NEUR IN, V8, P720; MCQUIRK IS, 1996, 1577 MIT ART INT LAB; WARREN WH, 1988, NATURE, V336, P162, DOI 10.1038/336162a0	6	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						699	705						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700099
C	Hochreiter, S; Schmidhuber, J		Kearns, MS; Solla, SA; Cohn, DA		Hochreiter, S; Schmidhuber, J			Source separation as a by-product of regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				ALGORITHM; SIGNALS; MODELS	This paper reveals a previously ignored connection between two important fields: regularization and independent component analysis (ICA). We show that at least one representative of a broad class of algorithms (regularizers that reduce network complexity) extracts independent features as a by-product. This algorithm is Flat Minimum Search (FMS), a recent general method for finding low-complexity networks with high generalization capability. FMS works by minimizing both training error and required weight precision. According to our theoretical analysis the hidden layer of an FMS-trained autoassociator attempts at coding each input by a sparse code with as few simple features as possible. In experiments the method extracts optimal codes for difficult versions of the "noisy bars" benchmark problem by separating the underlying sources, whereas ICA and PCA fail. Real world images are coded with fewer bits per pixel than by ICA or PCA.	Tech Univ Munich, Fak Informat, D-80290 Munich, Germany	Technical University of Munich	Hochreiter, S (corresponding author), Tech Univ Munich, Fak Informat, D-80290 Munich, Germany.		Peters, Jan/P-6027-2019; Hochreiter, Sepp/AAI-5904-2020	Peters, Jan/0000-0002-5266-8091; Hochreiter, Sepp/0000-0001-7449-2528				Amari S, 1996, ADV NEUR IN, V8, P757; Barlow HB, 1989, NEURAL COMPUT, V1, P412, DOI 10.1162/neco.1989.1.3.412; BARROW HG, 1987, 1ST P IEEE ANN C NEU, V4, P115; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; CARDOSO JF, 1993, IEE PROC-F, V140, P362, DOI 10.1049/ip-f-2.1993.0054; DAYAN P, 1995, NEURAL COMPUT, V7, P565, DOI 10.1162/neco.1995.7.3.565; DECO G, 1994, 41 AG ZFE ST SN; FIELD DJ, 1994, NEURAL COMPUT, V6, P559, DOI 10.1162/neco.1994.6.4.559; FOLDIAK P, 1995, HDB BRAIN THEORY NEU, P895; Hinton GE, 1997, PHILOS T ROY SOC B, V352, P1177, DOI 10.1098/rstb.1997.0101; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Hochreiter S, 1997, NEURAL COMPUT, V9, P1, DOI 10.1162/neco.1997.9.1.1; HOCHREITER S, 1995, ADV NEURAL INFORMATI, V7, P529; HOCHREITER S, 1998, FKI22297 TU MUNCH FA; Kohonen T., 1988, SELF ORG ASS MEMORY; LEWICKI MS, 1998, IN PRESS ADV NEURAL, V10; LINSKER R, 1988, IEEE COMPUT, V21, P105, DOI DOI 10.1109/2.36; MOLGEDEY L, 1994, PHYS REV LETT, V72, P3634, DOI 10.1103/PhysRevLett.72.3634; MOZER MC, 1991, ADV NEURAL INFORMATI, V3, P627; Oja E., 1989, International Journal of Neural Systems, V1, P61, DOI 10.1142/S0129065789000475; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Rumelhart DE, 1986, PARALLEL DISTRIBUTED, V1, P151; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863; Watanabe S, 1985, PATTERN RECOGNITION; ZEMEL RS, 1994, ADV NEURAL INFORMATI, V6, P11	25	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						459	465						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700065
C	Lee, DD; Sompolinsky, H		Kearns, MS; Solla, SA; Cohn, DA		Lee, DD; Sompolinsky, H			Learning a continuous hidden variable model for binary data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					A directed generative model for binary data using a small number of hidden continuous units is investigated. A clipping nonlinearity distinguishes the model from conventional principal components analysis. The relationships between the correlations of the underlying continuous Gaussian variables and the binary output variables are utilized to learn the appropriate weights of the network. The advantages of this approach are illustrated on a translationally invariant binary distribution and on handwritten digit images.	Bell Labs, Lucent Technol, Murray Hill, NJ 07974 USA	Alcatel-Lucent; Lucent Technologies; AT&T	Lee, DD (corresponding author), Bell Labs, Lucent Technol, 600 Mt Ave, Murray Hill, NJ 07974 USA.	ddlee@bell-labs.com; haim@fiz.huji.ac.il	Sompolinsky, Haim/ABB-8398-2021; Lee, Daniel D./B-5753-2013	Lee, Daniel/0000-0003-4239-8777				ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; Bartholomew D. J., 1987, LATENT VARIABLE MODE; Bregler C., 1995, Advances in Neural Information Processing Systems 7, P973; CHRISTOFFERSSON A, 1975, PSYCHOMETRIKA, V40, P5, DOI 10.1007/BF02291477; COVER TM, 1965, IEEE TRANS ELECTRON, VEC14, P326, DOI 10.1109/PGEC.1965.264137; DeMers D., 1993, ADV NEURAL INFORM PR, P580; HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936; Hinton GE, 1997, IEEE T NEURAL NETWOR, V8, P65, DOI 10.1109/72.554192; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; TIPPING ME, 1999, ADV NEURAL INFORMATI, V11; VANVREESWIJK C, 1999, UNPUB NONLINEAR STAT	12	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						515	521						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700073
C	Li, ZP		Kearns, MS; Solla, SA; Cohn, DA		Li, ZP			A V1 model of pop out and asymmetry in visual search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				CORTEX; INTEGRATION	Visual search is the task of finding a target in an image against a background of distracters. Unique features of targets enable them to pop out against the background, while targets defined by lacks of features or conjunctions of features are more difficult to spot. It is known that the ease of target detection can change when the roles of figure and ground are switched. The mechanisms underlying the ease of pop out and asymmetry in visual search have been elusive. This paper shows that a model of segmentation in V1 based on intracortical interactions can explain many of the qualitative aspects of visual search.	Univ Coll London, London WC1E 6BT, England	University of London; University College London	Li, ZP (corresponding author), Univ Coll London, London WC1E 6BT, England.							Douglas R. J., 1990, SYNAPTIC ORG BRAIN, P389; Duncan J., 1989, PSYCHOL REV, V96, P1; GILBERT CD, 1992, NEURON, V9, P1, DOI 10.1016/0896-6273(92)90215-Y; GRAY CM, 1989, P NATL ACAD SCI USA, V86, P1698, DOI 10.1073/pnas.86.5.1698; JULESZ B, 1981, NATURE, V290, P91, DOI 10.1038/290091a0; KAPADIA MK, 1995, NEURON, V15, P843, DOI 10.1016/0896-6273(95)90175-2; KNIERIM JJ, 1992, J NEUROPHYSIOL, V67, P961, DOI 10.1152/jn.1992.67.4.961; LI Z, 1998, THEORETICAL ASPECTS; Li ZP, 1998, NEURAL COMPUT, V10, P903, DOI 10.1162/089976698300017557; ROCKLAND KS, 1983, J COMP NEUROL, V216, P303, DOI 10.1002/cne.902160307; Rubenstein B., 1990, J OPT SOC AM A, V9, P1632; TREISMAN A, 1988, PSYCHOL REV, V95, P15, DOI 10.1037/0033-295X.95.1.15; TREISMAN A, 1990, VISUAL PERCEPTION NE; White E.L., 1989, CORTICAL CIRCUITS, DOI [10.1007/978-1-4684-8721-3, DOI 10.1007/978-1-4684-8721-3]	14	0	0	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						796	802						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700112
C	Liu, ZL; Weinshall, D		Kearns, MS; Solla, SA; Cohn, DA		Liu, ZL; Weinshall, D			Mechanisms of generalization in perceptual learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				DISCRIMINATION; SPECIFICITY	The learning of many visual perceptual tasks has been shown to be specific to practiced stimuli, while new stimuli require re-learning from scratch. Here we demonstrate generalization using a novel paradigm in motion discrimination where learning has been previously shown to be specific. We trained subjects to discriminate the directions of moving dots, and verified the previous results that learning does not transfer from the trained direction to a new one. However, by tracking the subjects' performance across time in the new direction, we found that their rate of learning doubled. Therefore, learning generalized in a task previously considered too difficult for generalization. We also replicated, in the second experiment, transfer following training with "easy" stimuli. The specificity of perceptual learning and the dichotomy between learning of "easy" vs. "difficult" tasks were hypothesized to involve different learning processes, operating at different visual cortical areas. Here we show how to interpret these results in terms of signal detection theory. With the assumption of limited computational resources, we obtain the observed phenomena - direct transfer and change of learning rate - for increasing levels of task difficulty. It appears that human generalization concurs with the expected behavior of a generic discrimination system.	Rutgers State Univ, Newark, NJ 07102 USA	Rutgers State University Newark; Rutgers State University New Brunswick	Liu, ZL (corresponding author), Rutgers State Univ, Newark, NJ 07102 USA.							Ahissar M, 1997, NATURE, V387, P401, DOI 10.1038/387401a0; BALL K, 1982, SCIENCE, V218, P697, DOI 10.1126/science.7134968; FIORENTINI A, 1980, NATURE, V287, P43, DOI 10.1038/287043a0; GILBERT CD, 1994, P NATL ACAD SCI USA, V91, P1195, DOI 10.1073/pnas.91.4.1195; KARNI A, 1991, P NATL ACAD SCI USA, V88, P4966, DOI 10.1073/pnas.88.11.4966; Liu Z, 1995, PERCEPTION, V24, P21; LIU Z, 1995, LEARNING VISUAL SKIL; POGGIO T, 1992, SCIENCE, V256, P1018, DOI 10.1126/science.1589770; RAMACHANDRAN VS, 1976, NATURE, V262, P382, DOI 10.1038/262382a0; Rubin N, 1997, CURR BIOL, V7, P461, DOI 10.1016/S0960-9822(06)00217-X	10	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						45	51						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700007
C	Marrs, AD; Webb, AR		Kearns, MS; Solla, SA; Cohn, DA		Marrs, AD; Webb, AR			Exploratory data analysis using radial basis function latent variable models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				NEURAL NETWORKS; PRINCIPAL	Two developments of nonlinear latent variable models based on radial basis functions are discussed: in the first, the use of priors or constraints on allowable models is considered as a means of preserving data structure in low-dimensional representations for visualisation purposes. Also, a resampling approach is introduced which makes more effective use of the latent samples in evaluating the likelihood.	Def Res Agcy, Malvern WR14 3PS, Worcs, England		Marrs, AD (corresponding author), Def Res Agcy, St Andrews Rd, Malvern WR14 3PS, Worcs, England.							Bishop CM, 1998, NEURAL COMPUT, V10, P215, DOI 10.1162/089976698300017953; BISHOP CM, 1997, IEE INT C ART NEUR N, P465; HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; Hotelling H, 1933, J EDUC PSYCHOL, V24, P498, DOI 10.1037/h0070888; KRAMER MA, 1991, AICHE J, V37, P233, DOI 10.1002/aic.690370209; LEBLANC M, 1994, J AM STAT ASSOC, V89, P53; Lowe D, 1996, NEURAL COMPUT APPL, V4, P83, DOI 10.1007/BF01413744; Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720; SALMOND DJ, 1990, SPIE, V1305; Silverman B.W., 1986, DENSITY ESTIMATION S, V26; Tibshirani R., 1992, Statistics and Computing, V2, P183, DOI 10.1007/BF01889678; Webb AR, 1997, J CLASSIF, V14, P249, DOI 10.1007/s003579900012; Webb AR, 1996, STAT COMPUT, V6, P159, DOI 10.1007/BF00162527; WEBB AR, 1995, PATTERN RECOGN, V28, P753, DOI 10.1016/0031-3203(94)00135-9; WEBB AR, 1998, UNPUB SUPERVISED NON; WEST M, 1993, J ROY STAT SOC B MET, V55, P409	17	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						529	535						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700075
C	Meir, R; Maiorov, V		Kearns, MS; Solla, SA; Cohn, DA		Meir, R; Maiorov, V			On the optimality of incremental neural network algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				APPROXIMATION; RATES	We study the approximation of functions by two-layer feedforward neural networks, focusing on incremental algorithms which greedily add units, estimating single unit parameters at each stage. As opposed to standard algorithms for fixed architectures, the optimization at each stage is performed over a small number of parameters, mitigating many of the difficult numerical problems inherent in high-dimensional non-linear optimization. We establish upper bounds on the error incurred by the algorithm, when approximating functions from the Sobolev class, thereby extending previous results which only provided rates of convergence for functions in certain convex hulls of functional spaces. By comparing our results to recently derived lower bounds, we show that the greedy algorithms are nearly optimal. Combined with estimation error results for greedy algorithms, a strong case can be made for this type of approach.	Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Meir, R (corresponding author), Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.							Auer P, 1996, ADV NEUR IN, V8, P316; BARRON AR, 1988, COMPUTING SCI STATIS, P192; De Boor C., 1973, Journal of Approximation Theory, V8, P19, DOI 10.1016/0021-9045(73)90029-4; DELYON B, 1995, IEEE T NEURAL NETWOR, V6, P332, DOI 10.1109/72.363469; Donahue MJ, 1997, CONSTR APPROX, V13, P187; JONES LK, 1992, ANN STAT, V20, P608, DOI 10.1214/aos/1176348546; Lee WS, 1996, IEEE T INFORM THEORY, V42, P2118, DOI 10.1109/18.556601; LESHNO M, 1993, NEURAL NETWORKS, V6, P861, DOI 10.1016/S0893-6080(05)80131-5; MAIOROV VE, 1997, UNPUB ADV COMPUTATIO; MEIR R, 1998, UNPUB OPTIMALITY NEU; Rivest R.L., 1989, NIPS, P494	14	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						295	301						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700042
C	Rae, HC; Sollich, P; Coolen, AAC		Kearns, MS; Solla, SA; Cohn, DA		Rae, HC; Sollich, P; Coolen, AAC			On-line learning with restricted training sets: Exact solution as benchmark for general theories	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				PERCEPTRON; DYNAMICS	We solve the dynamics of on-line Hebbian learning in perceptrons exactly, for the regime where the size of the training set scales linearly with the number of inputs. We consider both noiseless and noisy teachers. Our calculation cannot be extended to non-Hebbian rules, but the solution provides a nice benchmark to test more general and advanced theories for solving the dynamics of learning with restricted training sets.	Univ London Kings Coll, Dept Math, London WC2R 2LS, England	University of London; King's College London	Rae, HC (corresponding author), Univ London Kings Coll, Dept Math, London WC2R 2LS, England.		Sollich, Peter/H-2174-2011; Sollich, Peter/ABC-2993-2020	Sollich, Peter/0000-0003-0169-7893; 				COOLEN ACC, 1998, UNPUB; COOLEN ACC, KCLMTH9808; HORNER H, 1992, Z PHYS B CON MAT, V87, P371, DOI 10.1007/BF01309290; Mace CWH, 1998, STAT COMPUT, V8, P55, DOI 10.1023/A:1008896910704; Sollich P, 1997, EUROPHYS LETT, V38, P477, DOI 10.1209/epl/i1997-00271-3; SOLLICH P, 1998, ADV NEURAL INFORMATI, V10	7	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						316	322						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700045
C	Skantzos, NS; Beckmann, CF; Coolen, ACC		Kearns, MS; Solla, SA; Cohn, DA		Skantzos, NS; Beckmann, CF; Coolen, ACC			Discontinuous recall transitions induced by competition between short- and long-range interactions in recurrent networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				NEURAL NETWORKS	We present exact analytical equilibrium solutions for a class of recurrent neural network models, with both sequential and parallel neuronal dynamics, in which there is a tunable competition between nearest-neighbour and long-range synaptic interactions. This competition is found to induce novel coexistence phenomena as well as discontinuous transitions between pattern recall states, 2-cycles and non-recall states.	Univ London Kings Coll, Dept Math, London WC2R 2LS, England	University of London; King's College London	Skantzos, NS (corresponding author), Univ London Kings Coll, Dept Math, Strand, London WC2R 2LS, England.		Beckmann, Christian F./E-6374-2012	Beckmann, Christian F./0000-0002-3373-3193				AMIT DJ, 1985, PHYS REV A, V32, P1007, DOI 10.1103/PhysRevA.32.1007; AMIT DJ, 1985, PHYS REV LETT, V55, P1530, DOI 10.1103/PhysRevLett.55.1530; BRANDT U, 1978, Z PHYS B CON MAT, V31, P237, DOI 10.1007/BF01352348; Castellanos A, 1998, J PHYS A-MATH GEN, V31, P6615, DOI 10.1088/0305-4470/31/31/009; COOLEN ACC, 1992, J PHYS A-MATH GEN, V25, P2593, DOI 10.1088/0305-4470/25/9/029; COOLEN ACC, 1996, ADV NEURAL INFORMATI; COOLEN ACC, 1997, STAT MECH NEURAL NET; COOLEN ACC, 1993, MATH APPROACHES NEUR, P23; Domany E., 1994, MODELS NEURAL NETWOR; PERETTO P, 1984, BIOL CYBERN, V50, P51, DOI 10.1007/BF00317939; SKANTZOS NS, 1998, UNPUB; Yeomans J. M., 1992, STAT MECH PHASE TRAN	12	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						337	343						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700048
C	Wheeler, KR; Dhawan, AP		Kearns, MS; Solla, SA; Cohn, DA		Wheeler, KR; Dhawan, AP			Basis selection for wavelet regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				SHRINKAGE	A wavelet basis selection procedure is presented for wavelet regression. Both the basis and threshold are selected using cross-validation. The method includes the capability of incorporating prior knowledge on the smoothness (or shape of the basis functions) into the basis selection procedure. The results of the method are demonstrated using widely published sampled functions. The results of the method are contrasted with other basis function based methods.	NASA, Ames Res Ctr, Caelum Res Corp, Moffett Field, CA 94035 USA	National Aeronautics & Space Administration (NASA); NASA Ames Research Center	Wheeler, KR (corresponding author), NASA, Ames Res Ctr, Caelum Res Corp, Mail Stop 269-1, Moffett Field, CA 94035 USA.							COHEN A, 1992, COMMUN PUR APPL MATH, V45, P485, DOI 10.1002/cpa.3160450502; Daubechies I, 1992, CBMS NSF REGIONAL C, V61; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Nason GP, 1996, J ROY STAT SOC B MET, V58, P463; SWELDENS W, 1995, 19956 IMI U S CAR DE; SWELDENS W, 1995, 19947 IMI U S CAR DE; Wahba G., 1990, SPLINE MODELS OBSERV; WHEELER K, 1996, THESIS U CINCINNATI	9	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						627	633						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700089
C	Bengio, Y; Bengio, S; Isabelle, JF; Singer, Y		Jordan, MI; Kearns, MJ; Solla, SA		Bengio, Y; Bengio, S; Isabelle, JF; Singer, Y			Shared context probabilistic transducers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Recently, a model for supervised learning of probabilistic transducers represented by suffix trees was introduced. However, this algorithm tends to build very large trees, requiring very large amounts of computer memory. In this paper, we propose a new, more compact, transducer model in which one shares the parameters of distributions associated to contexts yielding similar conditional output distributions. We illustrate the advantages of the proposed algorithm with comparative experiments on inducing a noun phrase recognizer.	Univ Montreal, Dept IRO, Montreal, PQ H3C 3J7, Canada	Universite de Montreal	Bengio, Y (corresponding author), Univ Montreal, Dept IRO, Montreal, PQ H3C 3J7, Canada.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						409	415						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700058
C	Blais, BS; Intrator, N; Shouval, H; Cooper, LN		Jordan, MI; Kearns, MJ; Solla, SA		Blais, BS; Intrator, N; Shouval, H; Cooper, LN			Receptive field formation in natural scene environments: comparison of single cell learning rules	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We study several statistically and biologically motivated learning rules using the same visual environment, one made up of natural scenes, and the same single cell neuronal architecture. This allows us to concentrate on the feature extraction and neuronal coding properties of these rules. Included in these rules are kurtosis and skewness maximization, the quadratic form of the BCM learning rule, and single cell ICA. Using a structure removal method, we demonstrate that receptive fields developed using these rules depend on a small portion of the distribution. We find that the quadratic form of the BCM rule behaves in a manner similar to a kurtosis maximization rule when the distribution contains kurtotic directions, although the BCM modification equations are computationally simpler.	Brown Univ, Dept Phys, Providence, RI 02912 USA	Brown University	Blais, BS (corresponding author), Brown Univ, Dept Phys, Providence, RI 02912 USA.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						423	429						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700060
C	Bohossian, V; Bruck, J		Jordan, MI; Kearns, MJ; Solla, SA		Bohossian, V; Bruck, J			Multiple threshold neural logic	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We introduce a new Boolean computing element related to the Linear Threshold element, which is the Boolean version of the neuron. Instead of the sign function, it computes an arbitrary (with polynomialy many transitions) Boolean function of the weighted sum of its inputs. We call the new computing element an LTM element, which stands for Linear Threshold with Multiple transitions. The paper consists of the following main contributions related to our study of LTM circuits: (i) the creation of efficient designs of LTM circuits for the addition of a multiple number of integers and the product of two integers. In particular, we show how to compute the addition of m integers with a single layer of LTM elements. (ii) a proof that the area of the VLSI layout is reduced from O(n(2)) in LT circuits to O(n) in LTM circuits, for n inputs symmetric Boolean functions, and (iii) the characterization of the computing power of LTM relative to LT circuits.	CALTECH, Pasadena, CA 91125 USA	California Institute of Technology	Bohossian, V (corresponding author), CALTECH, Mail Code 136-93, Pasadena, CA 91125 USA.			Bruck, Jehoshua/0000-0001-8474-0812					0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						252	258						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700036
C	Cohen, E; Ruppin, E		Jordan, MI; Kearns, MJ; Solla, SA		Cohen, E; Ruppin, E			On parallel versus serial processing: A computational study of visual search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					A novel neural network model of pre-attention processing in visual-search tasks is presented. Using displays of line orientations taken from Wolfe's experiments [1992], we study the hypothesis that the distinction between parallel versus serial processes arises from the availability of global information in the internal representations of the visual scene. The model operates in two phases. First, the visual displays are compressed via principal-component-analysis. Second, the compressed data is processed by a target detector module in order to identify the existence of a target in the display. Our main finding is that targets in displays which were found experimentally to be processed in parallel can be detected by the system, while targets in experimentally-serial displays cannot. This fundamental difference is explained via variance analysis of the compressed representations, providing a numerical criterion distinguishing parallel from serial displays. Our model yields a mapping of response-time slopes that is similar to Duncan and Humphreys's "search surface" [1989], providing an explicit formulation of their intuitive notion of feature similarity. It presents a neural realization of the processing that may underlie the classical metaphorical explanations of visual search.	Tel Aviv Univ, Dept Psychol, IL-69978 Tel Aviv, Israel	Tel Aviv University	Cohen, E (corresponding author), Tel Aviv Univ, Dept Psychol, IL-69978 Tel Aviv, Israel.		Ruppin, Eytan/R-9698-2017	Ruppin, Eytan/0000-0002-7862-3940					0	0	0	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						10	16						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700002
C	Grove, AJ; Roth, D		Jordan, MI; Kearns, MJ; Solla, SA		Grove, AJ; Roth, D			Linear concepts and hidden variables: An empirical study	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Some learning techniques for classification tasks work indirectly, by first trying to fit a full probabilistic model to the observed data. Whether this is a good idea or not depends on the robustness with respect to deviations from the postulated model. We study this question experimentally in a restricted, yet non-trivial and interesting case: we consider a conditionally independent attribute (CIA) model which postulates a single binary-valued hidden variable z on which all other attributes (i.e., the target and the observables) depend. In this model, finding the most likely value of any one variable (given known values for the others) reduces to testing a linear function of the observed values. We learn CIA with two techniques: the standard EM algorithm, and a new algorithm we develop based on covariances. We compare these, in a controlled fashion, against an algorithm (a version of Winnow) that attempts to find a good linear classifier directly. Our conclusions help delimit the fragility of using the CIA model for classification: once the data departs from this model, performance quickly degrades and drops below that of the directly-learned linear classifier.	NEC Res Inst, Princeton, NJ 08540 USA	NEC Corporation	Grove, AJ (corresponding author), NEC Res Inst, 4 Independence Way, Princeton, NJ 08540 USA.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						500	506						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700071
C	Grunewald, A; Neumann, H		Jordan, MI; Kearns, MJ; Solla, SA		Grunewald, A; Neumann, H			Detection of first and second order motion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					A model of motion detection is presented. The model contains three stages. The first stage is unoriented and is selective for contrast polarities. The next two stages work in parallel. A phase insensitive stage pools across different contrast polarities through a spatiotemporal filter and thus can detect first and second order motion. A phase sensitive stage keeps contrast polarities separate, each of which is filtered through a spatiotemporal filter, and thus only first order motion can be detected. Differential phase sensitivity can therefore account for the detection of first and second order motion. Phase insensitive detectors correspond to cortical complex cells, and phase sensitive detectors to simple cells.	CALTECH, Div Biol, Pasadena, CA 91125 USA	California Institute of Technology	Grunewald, A (corresponding author), CALTECH, Div Biol, Mail Code 216-76, Pasadena, CA 91125 USA.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						801	807						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700113
C	Hadden, LE		Jordan, MI; Kearns, MJ; Solla, SA		Hadden, LE			A neural network model of naive preference and filial imprinting in the domestic chick	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Filial imprinting in domestic chicks is of interest in psychology, biology, and computational modeling because it exemplifies simple, rapid, innately programmed learning which is biased toward learning about some objects. Horn et al. have recently discovered a naive visual preference for heads and necks which develops over the course of the first three days of life. The neurological basis of this predisposition is almost entirely unknown; that of imprinting-related learning is fairly dear. This project is the first model of the predisposition consistent with what is known about learning in imprinting. The model develops the predisposition appropriately, learns to "approach" a training object, and replicates one interaction between the two processes. Future work will replicate more interactions between imprinting and the predisposition in chicks, and analyze why the system works.	Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Hadden, LE (corresponding author), Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA.								0	0	0	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						31	37						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700005
C	Hornel, D		Jordan, MI; Kearns, MJ; Solla, SA		Hornel, D			MELONET I: Neural nets for inventing baroque-style chorale variations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					MELONET I is a multi-scale neural network system producing baroque-style melodic variations. Given a melody, the system invents a four-part chorale harmonization and a variation of any chorale voice, after being trained on music pieces of composers like J. S. Bach and J. Pachelbel. Unlike earlier approaches to the learning of melodic structure, the system is able to learn and reproduce high-order structure like harmonic, motif and phrase structure in melodic sequences. This is achieved by using mutually interacting feedforward networks operating at different time scales, in combination with Kohonen networks to classify and recognize musical structure. The results are chorale partitas In the style of J. Pachelbel. Their quality has been judged by experts to be comparable to improvisations invented by an experienced human organist.	Univ Karlsruhe, Inst Log Komplexitat & Deduktionssyst, D-76128 Karlsruhe, Germany	Helmholtz Association; Karlsruhe Institute of Technology	Hornel, D (corresponding author), Univ Karlsruhe, Inst Log Komplexitat & Deduktionssyst, Am Fasanengarten 5, D-76128 Karlsruhe, Germany.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						887	893						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700125
C	Houde, JF; Jordan, MI		Jordan, MI; Kearns, MJ; Solla, SA		Houde, JF; Jordan, MI			Adaptation in speech motor control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Human subjects are known to adapt their motor behavior to a shift of the visual field brought about by wearing prism glasses over their eyes. We have studied the analog of this effect in speech. Using a device that can feed back transformed speech signals in real time, we exposed subjects to alterations of their own speech feedback. We found that speakers learn to adjust their production of a vowel to compensate for feedback alterations that change the vowel's perceived phonetic identity; moreover, the effect generalizes across consonant contexts and to different vowels.	Univ Calif San Francisco, Keck Ctr, San Francisco, CA 94143 USA	University of California System; University of California San Francisco	Houde, JF (corresponding author), Univ Calif San Francisco, Keck Ctr, Box 0732, San Francisco, CA 94143 USA.		Jordan, Michael I/C-5253-2013						0	0	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						38	44						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700006
C	Hush, DR; Lozano, F; Horne, B		Jordan, MI; Kearns, MJ; Solla, SA		Hush, DR; Lozano, F; Horne, B			Function approximation with the sweeping hinge algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We present a computationally efficient algorithm for function approximation with piecewise linear sigmoidal nodes. A one hidden layer network is constructed one node at a time using the method of fitting the residual. The task of fitting individual nodes is accomplished using a new algorithm that searchs for the best fit by solving a sequence of Quadratic Programming problems. This approach offers significant advantages over derivative-based search algorithms (e.g. backpropagation and its extensions). Unique characteristics of this algorithm include: finite step convergence, a simple stopping criterion, a deterministic methodology for seeking "good" local minima, good scaling properties and a robust numerical implementation.	Univ New Mexico, Dept Elect & Comp Engn, Albuquerque, NM 87131 USA	University of New Mexico	Hush, DR (corresponding author), Univ New Mexico, Dept Elect & Comp Engn, Albuquerque, NM 87131 USA.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						535	541						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700076
C	James, G; Hastie, T		Jordan, MI; Kearns, MJ; Solla, SA		James, G; Hastie, T			The error coding and substitution PaCTs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					A new class of plug in classification techniques have recently been developed in the statistics and machine learning literature. A plug in classification technique (PaCT) is a method that takes a standard classifier (such as LDA or TREES) and plugs it into an algorithm to produce a new classifier. The standard classifier is known as the Plug in Classifier (PiC). These methods often produce large improvements over using a single classifier. In this paper we investigate one of these methods and give some motivation for its success.	Stanford Univ, Stanford, CA 94305 USA	Stanford University	James, G (corresponding author), Stanford Univ, Stanford, CA 94305 USA.			Hastie, Trevor/0000-0002-0164-3142					0	0	0	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						542	548						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700077
C	Kvale, M; Schreiner, CE		Jordan, MI; Kearns, MJ; Solla, SA		Kvale, M; Schreiner, CE			Perturbative m-sequences for auditory systems identification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					In this paper we present a new method for studying auditory systems based on m-sequences. The method allows us to perturbatively study the linear response of the system in the presence of various other stimuli, such as speech or sinusoidal modulations. This allows one to construct linear kernels (receptive fields) at the same time that other stimuli are being presented. Using the method we calculate the modulation transfer function of single units in the inferior colliculus of the cat at different operating points and discuss nonlinearities in the response.	Univ Calif San Francisco, Sloan Ctr Theoret Neurobiol, San Francisco, CA 94143 USA	University of California System; University of California San Francisco	Kvale, M (corresponding author), Univ Calif San Francisco, Sloan Ctr Theoret Neurobiol, Box 0444,513 Parnassus Ave, San Francisco, CA 94143 USA.			Schreiner, Christoph/0000-0002-4571-4328					0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						180	186						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700026
C	Lange, DH; Siegelmann, HT; Pratt, H; Inbar, GF		Jordan, MI; Kearns, MJ; Solla, SA		Lange, DH; Siegelmann, HT; Pratt, H; Inbar, GF			A generic approach for identification of event related brain potentials via a competitive neural network structure	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We present a novel generic approach to the problem of Event Related Potential identification and classification, based on a competitive Neural Net architecture. The network weights converge to the embedded signal patterns, resulting in the formation of a matched filter bank. The network performance is analyzed via a simulation study, exploring identification robustness under low SNR conditions and compared to the expected performance from an information theoretic perspective. The classifier is applied to real event-related potential data recorded during a classic odd-balk type paradigm; for the first time, within-session variable signal patterns are automatically identified, dismissing the strong and limiting requirement of a-priori stimulus-related selective grouping of the recorded data.	Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Lange, DH (corresponding author), Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						901	907						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700127
C	Liu, ZL; Kersten, D		Jordan, MI; Kearns, MJ; Solla, SA		Liu, ZL; Kersten, D			2D observers for human 3D object recognition?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Converging evidence has shown that human object recognition depends on familiarity with the images of an object. Further, the greater the similarity between objects, the stronger is the dependence on object appearance, and the more important two-dimensional (2D) image information becomes. These findings, how ever, do not rule out the use of 3D structural information in recognition, and the degree to which 3D information is used in visual memory is an important issue. Lu, Knill, & Kersten (1995) showed that any model that is restricted to rotations in the image plane of independent 2D templates could not account for human performance in discriminating novel object views. We now present results from models of generalized radial basis functions (GRBF), 2D nearest neighbor matching that allows 2D affine transformations, and a Bayesian statistical estimator that integrates over all possible 2D affine transformations. The performance of the human observers relative to each of the models is better for the novel views than for the familiar template views, suggesting that humans generalize better to novel views from template views. The Bayesian estimator yields the optimal performance with 2D affine transformations and independent 2D templates. Therefore, models of 2D affine matching operations with independent 2D templates are unlikely to account for human recognition performance.	NEC Res Inst, Princeton, NJ 08540 USA	NEC Corporation	Liu, ZL (corresponding author), NEC Res Inst, 4 Independence Way, Princeton, NJ 08540 USA.								0	0	0	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						829	835						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700117
C	Lumer, ED		Jordan, MI; Kearns, MJ; Solla, SA		Lumer, ED			Effects of spike timing underlying binocular integration and rivalry in a neural model of early visual cortex	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					In normal vision, the inputs from the two eyes are integrated into a single percept. When dissimilar images are presented to the two eyes, however, perceptual integration gives way to alternation between monocular inputs, a phenomenon called binocular rivalry. Although recent evidence indicates that binocular rivalry involves a modulation of neuronal responses in extrastriate cortex, the basic mechanisms responsible for differential processing of conflicting and congruent stimuli remain unclear. Using a neural network that models the mammalian early visual system, I demonstrate here that the desynchronized firing of cortical-like neurons that first receive inputs from the two eyes results in rivalrous activity patterns at later stages in the visual pathway. By contrast, synchronization of firing among these cells prevents such competition. The temporal coordination of cortical activity and its effects on neural competition emerge naturally from the network connectivity and from its dynamics. These results suggest that input-related differences in relative spike timing at an early stage of visual processing may give rise to the phenomena both of perceptual integration and rivalry in binocular vision.	Univ Coll London, Inst Neurol, Wellcome Dept Cognit Neurol, London WC1N 3BG, England	University of London; University College London	Lumer, ED (corresponding author), Univ Coll London, Inst Neurol, Wellcome Dept Cognit Neurol, 12 Queen Sq, London WC1N 3BG, England.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						187	193						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700027
C	Ma, S; Ji, CY		Jordan, MI; Kearns, MJ; Solla, SA		Ma, S; Ji, CY			Wavelet models for video time-series	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					In this work, we tackle the problem of time-series modeling of video traffic. Different from the existing methods which model the time-series in the time domain, we model the wavelet coefficients in the wavelet domain. The strength of the wavelet model includes (1) a unified approach to model both the long-range and the short-range dependence in the video traffic simultaneously, (2) a computationally efficient method on developing the model and generating high quality video traffic, and (3) feasibility of performance analysis using the model.	Rensselaer Polytech Inst, Dept Elect Comp & Syst Engn, Troy, NY 12180 USA	Rensselaer Polytechnic Institute	Ma, S (corresponding author), Rensselaer Polytech Inst, Dept Elect Comp & Syst Engn, Troy, NY 12180 USA.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						915	921						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700129
C	Meir, R		Jordan, MI; Kearns, MJ; Solla, SA		Meir, R			Structural risk minimization for nonparametric time series prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					The problem of time series prediction is studied within the uniform convergence framework of Vapnik and Chervonenkis. The dependence inherent in the temporal structure is incorporated into the analysis, thereby generalizing the available theory for memoryless processes. Finite sample bounds are calculated in terms of covering numbers of the approximating class, and the tradeoff between approximation and estimation is discussed. A complexity regularization approach is outlined, based on Vapnik's method of Structural Risk Minimization, and shown to be applicable in the context of mixing stochastic processes.	Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Meir, R (corresponding author), Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						308	314						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700044
C	Monaco, JF; Ward, DG; Barto, AG		Jordan, MI; Kearns, MJ; Solla, SA		Monaco, JF; Ward, DG; Barto, AG			Automated aircraft recovery via reinforcement learning: Initial experiments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Initial experiments described here were directed toward using reinforcement learning (RL) to develop an automated recovery system (ARS) for high-agility aircraft. An ARS is an outer-loop flight-control system designed to bring an aircraft from a range of out-of-control states to straight-and-level flight in minimum time while satisfying physical and physiological constraints. Here we report on results for a simple version of the problem involving only single-axis (pitch) simulated recoveries. Through simulated control experience using a medium-fidelity aircraft simulation, the RL system approximates an optimal policy for pitch-stick inputs to produce minimum-time transitions to straight-and-level flight in unconstrained cases while avoiding ground-strike. The RL system was also able to adhere to a pilot-station acceleration constraint while executing simulated recoveries.	Barron Associates Inc, Charlottesville, VA 22901 USA		Monaco, JF (corresponding author), Barron Associates Inc, Jordan Bldg,1160 Pepsi Pl,Suite 300, Charlottesville, VA 22901 USA.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						1022	1028						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700144
C	Mozer, MC; Sitton, M; Farah, M		Jordan, MI; Kearns, MJ; Solla, SA		Mozer, MC; Sitton, M; Farah, M			A superadditive-impairment theory of optic aphasia	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Accounts of neurological disorders often posit damage to a specific functional pathway of the brain. Farah (1990) has proposed an alternative class of explanations involving partial damage to multiple pathways. We explore this explanation for optic aphasia, a disorder in which severe performance deficits are observed when patients are asked to name visually presented objects, but surprisingly, performance is relatively normal on naming objects from auditory cues and on gesturing the appropriate use of visually presented objects. We model this highly specific deficit through partial damage to two pathways-one that maps visual input to semantics, and the other that maps semantics to naming responses. The effect of this damage is superadditive, meaning that tasks which require one pathway or the other show little or no performance deficit, but the damage is manifested when a task requires both pathways (i.e., naming visually presented objects). Our model explains other phenomena associated with optic aphasia, and makes testable experimental predictions.	Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Mozer, MC (corresponding author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						66	72						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700010
C	Ratsaby, J		Jordan, MI; Kearns, MJ; Solla, SA		Ratsaby, J			An incremental nearest neighbor algorithm with queries	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We consider the general problem of learning multi-category classification from labeled examples. We present experimental results for a nearest neighbor algorithm which actively selects samples from different pattern classes according to a querying rule instead of the a priori class probabilities. The amount of improvement of this query-based approach over the passive batch approach depends on the complexity of the Bayes rule. The principle on which this algorithm is based is general enough to be used in any learning algorithm which permits a model-selection criterion and for which the error rate of the classifier is calculable in terms of the complexity of the model.	NAP Inc, Hollis, NY USA		Ratsaby, J (corresponding author), Hamered St 2, Raanana, Israel.		Ratsaby, Joel/AAV-8979-2021						0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						612	618						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700087
C	Rattray, M; Saad, D		Jordan, MI; Kearns, MJ; Solla, SA		Rattray, M; Saad, D			Globally optimal on-line learning rules	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We present a method for determining the globally optimal on-line learning rule for a soft committee machine. under a statistical mechanics framework. This work complements previous results on locally optimal rules, where only the rate of change in generalization error was considered. We maximize the total reduction in generalization error over the whole learning process and show how the resulting rule can significantly outperform the locally optimal rule.	Aston Univ, Dept Comp Sci & Appl Math, Birmingham B4 7ET, W Midlands, England	Aston University	Rattray, M (corresponding author), Aston Univ, Dept Comp Sci & Appl Math, Birmingham B4 7ET, W Midlands, England.		Rattray, Magnus/B-4393-2009	Rattray, Magnus/0000-0001-8196-5565; Saad, David/0000-0001-9821-2623					0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						322	328						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700046
C	Ring, M		Jordan, MI; Kearns, MJ; Solla, SA		Ring, M			RCC cannot compute certain FSA, even with arbitrary transfer functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Existing proofs demonstrating the computational limitations of Recurrent Cascade Correlation and similar networks (Fahlman, 1991; Bachrach, 1988; Meter, 1988) explicitly limit their results to units having sigmoidal or hard-threshold transfer functions (Giles et al., 1995; and Kremer, 1996). The proof given here shows that for any finite, discrete transfer function used by the units of an RCC network, there are finite-state automata (FSA) that the network cannot model, no matter how many units are used. The proof also applies to continuous transfer functions with a finite number of fixed-points, such as sigmoid and radial-basis functions.	Schloss Birlinghoven, RWCP Theoret Fdn, GMD Lab, GMD,German Natl Res Ctr Informat Technol, D-53754 St Augustin, Germany	Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss Birlinghoven	Ring, M (corresponding author), Schloss Birlinghoven, RWCP Theoret Fdn, GMD Lab, GMD,German Natl Res Ctr Informat Technol, D-53754 St Augustin, Germany.								0	0	0	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						619	625						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700088
C	Schenkel, M; Latimer, C; Jabri, M		Jordan, MI; Kearns, MJ; Solla, SA		Schenkel, M; Latimer, C; Jabri, M			Comparison of human and machine word recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We present a study which is concerned with word recognition rates for heavily degraded documents. We compare human with machine reading capabilities in a series of experiments, which explores the interaction of word/non-word recognition, word frequency and legality of non-words with degradation level. We also study the influence of character segmentation, and compare human performance with that of our artificial neural network model for reading. We found that the proposed computer model uses word context as efficiently as humans, but performs slightly worse on the pure character recognition task.	Univ Sydney, Dept Elect Engn, Sydney, NSW 2006, Australia	University of Sydney	Schenkel, M (corresponding author), Univ Sydney, Dept Elect Engn, Sydney, NSW 2006, Australia.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						94	100						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700014
C	Torkkola, K		Jordan, MI; Kearns, MJ; Solla, SA		Torkkola, K			Blind separation of radio signals in fading channels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We apply information maximization / maximum likelihood blind source separation [2, 6] to complex valued signals mixed with complex valued nonstationary matrices. This case arises in radio communications with baseband signals. We incorporate known source signal distributions in the adaptation, thus making the algorithms less "blind". This results in drastic reduction of the amount of data needed for successful convergence. Adaptation to rapidly changing signal mixing conditions, such as to fading in mobile communications, becomes now feasible as demonstrated by simulations.	Motorola Inc, Phoenix Corp Res Labs, Tempe, AZ 85284 USA		Torkkola, K (corresponding author), Motorola Inc, Phoenix Corp Res Labs, 2100 E Elliot Rd,MD EL508, Tempe, AZ 85284 USA.								0	0	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						756	762						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700107
C	Turiel, A; Mato, G; Parga, N; Nadal, JP		Jordan, MI; Kearns, MJ; Solla, SA		Turiel, A; Mato, G; Parga, N; Nadal, JP			Self-similarity properties of natural images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Scale invariance is a fundamental property of ensembles of natural images [1]. Their non Gaussian properties [15, 16] are less well understood, but they indicate the existence of a rich statistical structure. In this work we present a detailed study of the marginal statistics of a variable related to the edges in the images. A numerical analysis shows that it exhibits extended self-similarity [3, 4, 5]. This is a scaling property stronger than self-similarity: all its moments can be expressed as a power of any given moment. More interesting, all the exponents can be predicted in terms of a multiplicative log-Poisson process. This is the very same model that was used very recently to predict the correct exponents of the structure functions of turbulent flows [6]. These results allow us to study the underlying multifractal singularities. In particular we find that the most singular structures are one-dimensional: the most singular manifold consists of sharp edges.	Univ Autonoma Madrid, Dept Fis Teor, E-28049 Madrid, Spain	Autonomous University of Madrid; Consejo Superior de Investigaciones Cientificas (CSIC); CSIC - UAM - Institut de Fisica Teorica (IFT)	Parga, N (corresponding author), Univ Autonoma Madrid, Dept Fis Teor, E-28049 Madrid, Spain.		Turiel, Antonio M/A-7936-2008; Nadal, Jean-Pierre/HHM-8804-2022	Turiel, Antonio M/0000-0001-6103-224X; Mato, German/0000-0003-3106-1423; Nadal, Jean-Pierre/0000-0003-0022-0647					0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						836	842						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700118
C	Verrelst, H; Timmerman, D; Moreau, Y; Vandewalle, J		Jordan, MI; Kearns, MJ; Solla, SA		Verrelst, H; Timmerman, D; Moreau, Y; Vandewalle, J			Use of a Multi-Layer Perceptron to predict malignancy in ovarian tumors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We discuss the development of a Multi-Layer Perceptron neural network classifier for use in preoperative differentiation between benign and malignant ovarian tumors. As the Mean Squared classification Error is not sufficient to make correct and objective assessments about the performance of the neural classifier, the concepts of sensitivity and specificity are introduced and combined in Receiver Operating Characteristic curves. Based on objective observations such as sonomorphologic criteria, color Doppler imaging and results from serum tumor markers, the neural network is able to make reliable predictions with a discriminating performance comparable to that of experienced gynecologists.	Katholieke Univ Leuven, Dept Elect Engn, B-3000 Louvain, Belgium	KU Leuven	Verrelst, H (corresponding author), Katholieke Univ Leuven, Dept Elect Engn, Kard Mercierlaan 94, B-3000 Louvain, Belgium.		Timmerman, Dirk/AAD-2363-2019	Timmerman, Dirk/0000-0002-3707-6645; Moreau, Yves/0000-0002-4647-6560					0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						978	984						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700138
C	Willett, D; Rigoll, G		Jordan, MI; Kearns, MJ; Solla, SA		Willett, D; Rigoll, G			Hybrid NN/HMM-based speech recognition with a discriminant neural feature extraction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					In this paper, we present a novel hybrid architecture for continuous speech recognition systems. It consists of a continuous HMM system extended by an arbitrary neural network that is used as a preprocessor that takes several frames of the feature vector as input to produce more discriminative feature vectors with respect to the underlying HMM system. This hybrid system is an extension of a state-of-the-art continuous HMM system, and in fact, it is the first hybrid system that really is capable of outperforming these standard systems with respect to the recognition accuracy. Experimental results show an relative error reduction of about 10% that we achieved on a remarkably good recognition system based on continuous HMMs for the Resource Management 1000-word continuous speech recognition task.	Gerhard Mercator Univ, Fac Elect Engn, Dept Comp Sci, Duisburg, Germany	University of Duisburg Essen	Willett, D (corresponding author), Gerhard Mercator Univ, Fac Elect Engn, Dept Comp Sci, Duisburg, Germany.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						763	769						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700108
C	Wilson, RC; Hancock, ER		Jordan, MI; Kearns, MJ; Solla, SA		Wilson, RC; Hancock, ER			Graph matching with hierarchical discrete relaxation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Our aim in this paper is to develop a Bayesian framework for matching hierarchical relational models. The goal is to make discrete label assignments so as to optimise a global cost function that draws information concerning the consistency of match from different levels of the hierarchy. Our Bayesian development naturally distinguishes between intra-level and inter-level constraints. This allows the impact of reassigning a match to be assessed not only at its own (or peer) level of representation, but also upon its parents and children in the hierarchy.	Univ York, Dept Comp Sci, York YO1 5DD, N Yorkshire, England	University of York - UK	Wilson, RC (corresponding author), Univ York, Dept Comp Sci, York YO1 5DD, N Yorkshire, England.		Hancock, Edwin/N-7548-2019; Hancock, Edwin R/C-6071-2008	Hancock, Edwin/0000-0003-4496-2028; Hancock, Edwin R/0000-0003-4496-2028					0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						689	695						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700098
C	Xiong, YS; Kwon, CL; Oh, JH		Jordan, MI; Kearns, MJ; Solla, SA		Xiong, YS; Kwon, CL; Oh, JH			The storage capacity of a fully-connected committee machine	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We study the storage capacity of a fully-connected committee machine with a large number K of hidden nodes. The storage capacity is obtained by analyzing the geometrical structure of the weight space related to the internal representation. By examining the asymptotic behavior of order parameters in the limit of large K, the storage capacity alpha(c) is found to be proportional to K root ln K up to the leading order. This result satisfies the mathematical bound given by Mitchison and Durbin, whereas the replica-symmetric solution in a conventional Gardner's approach violates this bound.	Pohang Inst Sci & Technol, Dept Phys, Pohang, Kyongbuk, South Korea	Pohang University of Science & Technology (POSTECH)	Xiong, YS (corresponding author), Pohang Inst Sci & Technol, Dept Phys, Hyoja San 31, Pohang, Kyongbuk, South Korea.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						378	384						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700054
C	Zimmermann, HG; Neuneier, R		Jordan, MI; Kearns, MJ; Solla, SA		Zimmermann, HG; Neuneier, R			The observer-observation dilemma in neuro-forecasting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We explain how the training data can be separated into clean information and unexplainable noise. Analogous to the data, the neural network is separated into a time invariant structure used for forecasting, and a noisy part. We propose a unified theory connecting the optimization algorithms for cleaning and learning together with algorithms that control the data noise and the parameter noise. The combined algorithm allows a data-driven local control of the liability of the network parameters and therefore an improvement in generalization. The approach is proven to be very useful at the task of forecasting the German bond market.	Siemens AG, Corp Technol, D-81730 Munich, Germany	Siemens AG; Siemens Germany	Zimmermann, HG (corresponding author), Siemens AG, Corp Technol, D-81730 Munich, Germany.	Georg.Zimmermann@mchp.siemens.de; Ralph.Neuneier@mchp.siemens.de							0	0	1	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						992	998						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700140
C	Baram, Y		Mozer, MC; Jordan, MI; Petsche, T		Baram, Y			Consistent classification, firm and soft	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				A classifier is called consistent with respect to a given set of class-labeled points if it correctly classifies the set. We consider classifiers defined by unions of local separators and propose algorithms for consistent classifier reduction. The expected complexities of the proposed algorithms are derived along with the expected classifier sizes. In particular, the proposed approach yields a consistent reduction of the nearest neighbor classifier, which performs ''firm'' classification, assigning each new object to a class, regardless of the data structure. The proposed reduction method suggests a notion of ''soft'' classification, allowing for indecision with respect to objects which are insufficiently or ambiguously supported by the data. The performances of the proposed classifiers in predicting stock behavior are compared to that achieved by the nearest neighbor method.	TECHNION ISRAEL INST TECHNOL,DEPT COMP SCI,IL-32000 HAIFA,ISRAEL	Technion Israel Institute of Technology	Baram, Y (corresponding author), NASA,AMES RES CTR,NATL RES COUNCIL,MS 210-9,MOFFETT FIELD,CA 94035, USA.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						326	332						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00046
C	Chatterjee, C; Roychowdhury, VP		Mozer, MC; Jordan, MI; Petsche, T		Chatterjee, C; Roychowdhury, VP			Self-organizing and adaptive algorithms for generalized eigen-decomposition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The paper is developed in two parts where we discuss a new approach to self-organization in a single-layer linear feed-forward network. First, two novel algorithms for self-organization are derived from a two-layer linear hetero-associative network performing a one-of-m classification, and trained with the constrained least-mean-squared classification error criterion. Second, two adaptive algorithms are derived from these self-organizing procedures to compute the principal generalized eigenvectors of two correlation matrices from two sequences of random vectors, These novel adaptive algorithms can be implemented in a single-layer linear feed-forward network. We give a rigorous convergence analysis of the adaptive algorithms by using stochastic approximation theory. As an example, we consider a problem of online signal detection in digital mobile communications.			Chatterjee, C (corresponding author), NEWPORT CORP,1791 DEERE AVE,IRVINE,CA 92606, USA.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						396	402						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00056
C	Clouse, DS; Giles, CL; Horne, BG; Cottrell, GW		Mozer, MC; Jordan, MI; Petsche, T		Clouse, DS; Giles, CL; Horne, BG; Cottrell, GW			Representation and induction of finite state machines using time-delay neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				This work investigates the representational and inductive capabilities of time-delay neural networks (TDNNs) in general, and of two subclasses of TDNN, those with delays only on the inputs (IDNN), and those which include delays on hidden units (HDNN). Both architectures are capable of representing the same class of languages, the definite memory machine (DMM) languages, but the delays on the hidden units in the HDNN helps it outperform the IDNN on problems composed of repeated features over short time windows.			Clouse, DS (corresponding author), UNIV CALIF SAN DIEGO,DEPT COMP SCI & ENGN,LA JOLLA,CA 92093, USA.			Cottrell, Garrison/0000-0001-7538-1715					0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						403	409						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00057
C	Dayan, P		Mozer, MC; Jordan, MI; Petsche, T		Dayan, P			An hierarchical model of visual rivalry	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Binocular rivalry is the alternating percept that can result when the two eyes see different scenes. Recent psychophysical evidence supports an account for one component of binocular rivalry similar to that for other bistable percepts. We test the hypothesis(19,16,18) 16 that alternation can be generated by competition between top-down cortical explanations for the inputs, rather than by direct competition between the inputs. Recent neurophysiological evidence shows that some binocular neurons are modulated with the changing percept; others are not, even if they are selective between the stimuli presented to the eyes. We extend our model to a hierarchy to address these effects.			Dayan, P (corresponding author), MIT,DEPT BRAIN & COGNIT SCI,E25-210,CAMBRIDGE,MA 02139, USA.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						48	48						1	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00007
C	Dimitrov, A; Cowan, JD		Mozer, MC; Jordan, MI; Petsche, T		Dimitrov, A; Cowan, JD			Spatial decorrelation in orientation tuned cortical cells	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				In this paper we propose a model for the lateral connectivity of orientation-selective cells in the visual cortex based on information-theoretic considerations. We study the properties of the input signal to the visual cortex and find new statistical structures which have not been processed in the retino-geniculate pathway. Applying the idea that the system optimizes the representation of incoming signals, we derive the lateral connectivity that will achieve this for a set of local orientation-selective patches, as well as the complete spatial structure of a layer of such patches. We compare the results with various physiological measurements.			Dimitrov, A (corresponding author), UNIV CHICAGO,DEPT MATH,CHICAGO,IL 60637, USA.		Dimitrov, Alexander/E-8577-2014	Dimitrov, Alexander/0000-0002-8812-3536					0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						852	858						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00120
C	Gabbiani, F; Wessel, R; Metzner, W; Koch, C		Mozer, MC; Jordan, MI; Petsche, T		Gabbiani, F; Wessel, R; Metzner, W; Koch, C			Extraction of temporal features in the electrosensory system of weakly electric fish	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The encoding of random time-varying stimuli in single spike trains of electrosensory neurons in the weakly electric fish Eigenmannia was investigated using methods of statistical signal processing. At the first stage of the electrosensory system, spike trains were found to encode faithfully the detailed time-course of random stimuli, while at the second stage neurons responded specifically to features in the temporal waveform of the stimulus. Therefore stimulus information is processed at the second stage of the electrosensory system by extracting temporal features from the faithfully preserved image of the environment sampled at the first stage.			Gabbiani, F (corresponding author), CALTECH,DEPT BIOL,PASADENA,CA 91125, USA.			Koch, Christof/0000-0001-6482-8067					0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						62	68						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00009
C	Gray, MS; Pouget, A; Zemel, RS; Nowlan, SJ; Sejnowski, TJ		Mozer, MC; Jordan, MI; Petsche, T		Gray, MS; Pouget, A; Zemel, RS; Nowlan, SJ; Sejnowski, TJ			Selective integration: A model for disparity estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Local disparity information is often sparse and noisy, which creates two conflicting demands when estimating disparity in an image region: the need to spatially average to get; an accurate estimate, and the problem of not averaging over discontinuities. We have developed a network model of disparity estimation based on disparity-selective neurons, such as those found in the early stages of processing in visual cortex. The model can accurately estimate multiple disparities in a region, which may be caused by transparency or occlusion, in real images and random-dot stereograms. The use of a selection mechanism to selectively integrate reliable local disparity estimates results in superior performance compared to standard back-propagation and cross-correlation approaches. In addition, the representations learned with this selection mechanism are consistent with recent neurophysiological results of von der Heydt, Zhou, Friedman, and Poggio [8] for cells in cortical visual area V2. Combining multi-scale biologically-plausible image processing with the power of the mixture-of-experts learning algorithm represents a promising approach that yields both high performance and new insights into visual system function.			Gray, MS (corresponding author), UNIV CALIF SAN DIEGO,DEPT BIOL,LA JOLLA,CA 92093, USA.		Sejnowski, Terrence/AAV-5558-2021						0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						866	872						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00122
C	Grossberg, S; Williamson, JR		Mozer, MC; Jordan, MI; Petsche, T		Grossberg, S; Williamson, JR			ARTEX: A self-organizing architecture for classifying image regions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				A self-organizing architecture is developed for image region classification. The system consists of a preprocessor that utilizes multiscale filtering, competition, cooperation, and diffusion to compute a vector of image boundary and surface properties, notably texture and brightness properties. This vector inputs to a system that incrementally learns noisy multidimensional mappings and their probabilities. The architecture is applied to difficult real-world image classification problems, including classification of synthetic aperture radar and natural texture images, and outperforms a recent state-of-the-art system at classifying natural textures.			Grossberg, S (corresponding author), BOSTON UNIV,CTR ADAPT SYST,677 BEACON ST,BOSTON,MA 02215, USA.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						873	879						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00123
C	Hansen, EA; Barto, AG; Zilberstein, S		Mozer, MC; Jordan, MI; Petsche, T		Hansen, EA; Barto, AG; Zilberstein, S			Reinforcement learning fbr mixed open-loop and closed-loop control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Closed-loop control relies on sensory feedback that is usually assumed to be free. But if sensing incurs a cost, it may be cost-effective to take sequences of actions in open-loop mode. We describe a reinforcement learning algorithm that learns to combine open-loop and closed-loop control when sensing incurs a cost. Although we assume reliable sensors, use of open-loop control means that actions must sometimes be taken when the current state of the controlled system is uncertain. This is a special case of the hidden-state problem in reinforcement learning, and to cope, our algorithm relies on short-term memory. The main result of the paper is a rule that significantly limits exploration of possible memory states by pruning memory states for which the estimated value of information is greater than its cost. We prove that this rule allows convergence to an optimal policy.			Hansen, EA (corresponding author), UNIV MASSACHUSETTS,DEPT COMP SCI,AMHERST,MA 01003, USA.			Zilberstein, Shlomo/0000-0001-9817-7848					0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						1026	1032						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00144
C	Kamimura, R		Mozer, MC; Jordan, MI; Petsche, T		Kamimura, R			Unification of information maximization and minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				In the present paper, we propose a method to unify information maximization and minimization in hidden units. The information maximization and minimization are performed on two different levels: collective and individual level. Thus, two kinds of information: collective and individual information are defined. By maximizing collective information and by minimizing individual information simple networks can be generated in terms of the number of connections and the number of hidden units. Obtained networks are expected to give better generalization and improved interpretation of internal representations. This method was applied to the inference of the maximum onset principle of an artificial language. In this problem, it was shown that the individual information minimization is not contradictory to the collective information maximization. In addition, experimental results confirmed improved generalization performance, because over-training can significantly be suppressed.			Kamimura, R (corresponding author), TOKAI UNIV,INFORMAT SCI LAB,1117 KITAKANAME,HIRATSUKA,KANAGAWA 25912,JAPAN.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						508	514						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00072
C	Kowalczyk, A; Ferra, H		Mozer, MC; Jordan, MI; Petsche, T		Kowalczyk, A; Ferra, H			MLP can provably generalise much better than VC-bounds indicate	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Results of a study of the worst case learning curves for a particular class of probability distribution on input space to MLP with hard threshold hidden units are presented. It is shown in particular, that in the thermodynamic limit for scaling by the number of connections to the first hidden layer, although the true learning curve behaves as approximate to alpha(-1) for alpha approximate to 1, its VC-dimension based bound is trivial (= 1) and its VC-entropy bound is trivial for alpha less than or equal to 6.2. It is also shown that bounds following the true learning curve can be derived from a formalism based on the density of error patterns.			Kowalczyk, A (corresponding author), TELSTRA RES LABS,770 BLACKBURN RD,CLAYTON,VIC 3168,AUSTRALIA.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						190	196						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00027
C	Lazzaro, J; Wawrzynek, J; Lippmann, R		Mozer, MC; Jordan, MI; Petsche, T		Lazzaro, J; Wawrzynek, J; Lippmann, R			A micropower analog VLSI HMM state decoder for wordspotting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We describe the implementation of a hidden Markov model state decoding system, a component for a wordspotting speech recognition system. The key specification for this state decoder design is microwatt power dissipation; this requirement led to a continuous-time, analog circuit implementation. We characterize the operation of a 10-word (81 state) state decoder test chip.			Lazzaro, J (corresponding author), UNIV CALIF BERKELEY,DIV COMP SCI,BERKELEY,CA 94720, USA.		Lippmann, Richard P/G-3832-2018	Lippmann, Richard P/0000-0003-0904-0984					0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						727	733						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00103
C	Leite, JAF; Hancock, ER		Mozer, MC; Jordan, MI; Petsche, T		Leite, JAF; Hancock, ER			Contour organisation with the EM algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				This paper describes how the early visual process of contour organisation can be realised using the EM algorithm. The underlying computational representation is based on fine spline coverings. According to our EM approach the adjustment of spline parameters draws on an iterative weighted least-squares fitting process. The expectation step of our EM procedure computes the likelihood of the data using a mixture model defined over the set of spline coverings. These splines are limited in their spatial extent using Gaussian windowing functions. The maximisation of the likelihood leads to a set of linear equations in the spline parameters which solve the weighted least squares problem. We evaluate the technique on the localisation of road structures in aerial infra-red images.			Leite, JAF (corresponding author), UNIV YORK,DEPT COMP SCI,YORK YO1 5DD,N YORKSHIRE,ENGLAND.		Hancock, Edwin R/C-6071-2008; Hancock, Edwin/N-7548-2019	Hancock, Edwin R/0000-0003-4496-2028; Hancock, Edwin/0000-0003-4496-2028					0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						880	886						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00124
C	Li, ZP		Mozer, MC; Jordan, MI; Petsche, T		Li, ZP			A neural model of visual contour integration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We introduce a neurobiologically plausible model of contour integration from visual inputs of individual oriented edges. The model is composed of interacting excitatory neurons and inhibitory interneurons, receives visual inputs via oriented receptive fields (RFs) like those in V1. The RF centers are distributed in space. At each location, a finite number of cells tuned to orientations spanning 180 degrees compose a model hypercolumn. Cortical interactions modify neural activities produced by visual inputs, selectively amplifying activities for edge elements belonging to smooth input contours. Elements within one contour produce synchronized neural activities. We show analytically and empirically that contour enhancement and neural synchrony increase with contour length, smoothness and closure, as observed experimentally. This model gives testable predictions, and in addition, introduces a feedback mechanism allowing higher visual centers to enhance, suppress, and segment contours.			Li, ZP (corresponding author), HONG KONG UNIV SCI & TECHNOL,CLEAR WATER BAY,HONG KONG,HONG KONG.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						69	75						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00010
C	Martignon, L; Deco, G; Laskey, K; Vaadia, E		Mozer, MC; Jordan, MI; Petsche, T		Martignon, L; Deco, G; Laskey, K; Vaadia, E			Learning exact patterns of quasi-synchronization among spiking neurons from data on multi-unit recordings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				This paper develops arguments for a family of temporal log-linear models to represent spatio-temporal correlations among the spiking events in a group of neurons. The models can represent not just pairwise correlations but also correlations of higher order. Methods are discussed for inferring the existence or absence of correlations and estimating their strength. A frequentist and a Bayesian approach to correlation detection are compared. The frequentist method is based on G(2) statistic with estimates obtained via the Max-Ent principle. In the Bayesian approach a Markov Chain Monte Carlo Model Composition (MC3) algorithm is applied to search over connectivity structures and Laplace's method is used to approximate their posterior probability. Performance of the methods was tested on synthetic data. The methods were applied to experimental data obtained by the fourth author by means of measurements carried out on behaving Rhesus monkeys at the Hadassah Medical School of the Hebrew University. As conjectured, neural connectivity structures need not be neither hierarchical nor decomposable.			Martignon, L (corresponding author), MAX PLANCK INST PSYCHOL RES,LEOPOLDSTR 24,D-80802 MUNICH,GERMANY.		DECO, GUSTAVO/A-6341-2008; Laskey, Kathryn/G-8977-2015	DECO, GUSTAVO/0000-0002-8995-7583; Laskey, Kathryn/0000-0002-3106-140X					0	0	0	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						76	82						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00011
C	Parberry, I; Tseng, HL		Mozer, MC; Jordan, MI; Petsche, T		Parberry, I; Tseng, HL			Are Hopfield networks faster than conventional computers?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				It is shown that conventional computers can be exponentially faster than planar Hopfield networks: although there are planar Hopfield networks that take exponential time to converge, a stable state of an arbitrary planar Hopfield network can be found by a conventional computer in polynomial time. The theory of PLS-completeness gives strong evidence that such a separation is unlikely for nonplanar Hopfield networks, and it is demonstrated that this is also the case for several restricted classes of nonplanar Hopfield networks, including those who interconnection graphs are the class of bipartite graphs, graphs of degree 3, the dual of the knight's graph, the 8-neighbor mesh, the hypercube, the butterfly, the cube-connected cycles, and the shuffle-exchange graph.			Parberry, I (corresponding author), UNIV N TEXAS,DEPT COMP SCI,POB 13886,DENTON,TX 76203, USA.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						239	245						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00034
C	Pareigis, S		Mozer, MC; Jordan, MI; Petsche, T		Pareigis, S			Multi-grid methods for reinforcement learning in controlled diffusion processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Reinforcement learning methods for discrete and semi-Markov decision problems such as Real-Time Dynamic Programming can be generalized for Controlled Diffusion Processes. The optimal control problem reduces to a boundary value problem for a fully nonlinear second-order elliptic differential equation of Hamilton-Jacobi-Bellman (HJB-) type. Numerical analysis provides multigrid methods for this kind of equation. In the case of Learning Control, however, the systems of equations on the various grid-levels are obtained using observed information (transitions and local cost). To ensure consistency, special attention needs to be directed toward the type of time and space discretization during the observation. An algorithm for multi-grid observation is proposed. The multi-grid algorithm is demonstrated on a simple queuing problem.			Pareigis, S (corresponding author), CHRISTIAN ALBRECHTS UNIV KIEL,LEHRSTUHL PRAKT MATH,GUTENBERGSTR 76-78,KIEL,GERMANY.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						1033	1039						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00145
C	Pawelzik, KR; Ernst, U; Wolf, F; Geisel, T		Mozer, MC; Jordan, MI; Petsche, T		Pawelzik, KR; Ernst, U; Wolf, F; Geisel, T			Orientation contrast sensitivity from long-range interactions in visual cortex	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Recently Sillito and coworkers (Nature 378, pp. 492, 1935) demonstrated that stimulation beyond the classical receptive field (cRF) can not only modulate, but radically change a neuron's response to oriented stimuli. They revealed that patch-suppressed cells when stimulated with contrasting orientations inside and outside their cRF can strongly respond to stimuli oriented orthogonal to their nominal preferred orientation. Here we analyze the emergence of such complex response patterns in a simple model of primary visual cortex. We show that the observed sensitivity for orientation contrast can be explained by a delicate interplay between local isotropic interactions and patchy long-range connectivity between distant iso-orientation domains. In particular we demonstrate that the observed properties might arise without specific connections between sites with cross-oriented cRFs.			Pawelzik, KR (corresponding author), UNIV FRANKFURT,INST THEORET PHYS,ROBERT MAYER STR 8,D-60054 FRANKFURT,GERMANY.		Wolf, Fred/D-5771-2011	Wolf, Fred/0000-0002-7068-3762					0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						90	96						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00013
C	Peper, F; Noda, H		Mozer, MC; Jordan, MI; Petsche, T		Peper, F; Noda, H			Hebb learning of features based on their information content	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				This paper investigates the stationary points of a Hebb learning rule with a sigmoid nonlinearity in it. We show mathematically that when the input has a low information content, as measured by the input's variance, this learning rule suppresses learning, that is, forces the weight vector to converge to the zero vector. When the information content exceeds a certain value, the rule will automatically begin to learn a feature in the input. Our analysis suggests that under certain conditions it is the first principal component that is learned. The weight vector length remains bounded, provided the variance of the input is finite. Simulations confirm the theoretical results derived.			Peper, F (corresponding author), COMMUN RES LABS,NISHI KU,588-2 IWAOKA,KOBE,HYOGO 65124,JAPAN.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						246	252						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00035
C	Pineda, FJ; Cauwenberghs, G; Edwards, RT		Mozer, MC; Jordan, MI; Petsche, T		Pineda, FJ; Cauwenberghs, G; Edwards, RT			Bangs, clicks, snaps, thuds and whacks: An architecture for acoustic transient processing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We propose a neuromorphic architecture for real-time processing of acoustic transients in analog VLSI. We show how judicious normalization of a time-frequency signal allows an elegant and robust implementation of a correlation algorithm. The algorithm uses binary multiplexing instead of analog-analog multiplication. This removes the need for analog storage and analog-multiplication. Simulations show that the resulting algorithm has the same out-of-sample classification performance (similar to 93% correct) as a baseline template-matching algorithm.			Pineda, FJ (corresponding author), JOHNS HOPKINS UNIV,APPL PHYS LAB,JOHNS HOPKINS RD,LAUREL,MD 20723, USA.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						734	740						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00104
C	Sabatini, SP; Solari, F; Bisio, GM		Mozer, MC; Jordan, MI; Petsche, T		Sabatini, SP; Solari, F; Bisio, GM			An architectural mechanism for direction-tuned cortical simple cells: The role of mutual inhibition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				A linear architectural model of cortical simple cells is presented. The model evidences how mutual inhibition, occurring through synaptic coupling functions asymmetrically distributed in space, can be a possible basis for a wide variety of spatio-temporal simple cell response properties, including direction selectivity and velocity tuning. While spatial asymmetries are included explicitly in the structure of the inhibitory interconnections, temporal asymmetries originate from the specific mutual inhibition scheme considered. Extensive simulations supporting the model are reported.			Sabatini, SP (corresponding author), UNIV GENOA,DEPT BIOPHYS & ELECT ENGN,PSPC RES GRP,VIA OPERA PIA 11A,I-16145 GENOA,ITALY.		Sabatini, Silvio P/A-5500-2012; Solari, Fabio/O-4729-2016	Sabatini, Silvio P/0000-0002-0557-7306; Solari, Fabio/0000-0002-8111-0409					0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						104	110						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00015
C	Singh, S; Dayan, P		Mozer, MC; Jordan, MI; Petsche, T		Singh, S; Dayan, P			Analytical mean squared error curves in temporal difference learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We have calculated analytical expressions for how the bias and variance of the estimators provided by various temporal difference value estimation algorithms change with offline updates over trials in absorbing Markov chains using lookup table representations. We illustrate classes of learning curve behavior in various chains, and show the manner in which TD is sensitive to the choice of its step-size and eligibility trace parameters.			Singh, S (corresponding author), UNIV COLORADO,DEPT COMP SCI,BOULDER,CO 80309, USA.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						1054	1060						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00148
C	Stensmo, M; Sejnowski, TJ		Mozer, MC; Jordan, MI; Petsche, T		Stensmo, M; Sejnowski, TJ			Learning decision theoretic utilities through reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Probability models can be used to predict outcomes and compensate for missing data, but even a perfect model cannot be used to make decisions unless the utility of the outcomes, or preferences between them, are also provided. This arises in many real-world problems, such as medical diagnosis, where the cost of the test as well as the expected improvement in the outcome must be considered. Relatively little work has been done on learning the utilities of outcomes for optimal decision making. In this paper, we show how temporal-difference reinforcement learning (TD(lambda)) can be used to determine decision theoretic utilities within the context of a mixture model and apply this new approach to a problem in medical diagnosis. TD(lambda) learning of utilities reduces the number of tests that have to be done to achieve the same level of performance compared with the probability model alone, which results in significant cost savings and increased efficiency.			Stensmo, M (corresponding author), UNIV CALIF BERKELEY,DIV COMP SCI,BERKELEY,CA 94720, USA.		Sejnowski, Terrence/AAV-5558-2021						0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						1061	1067						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00149
C	Tang, AC; Bartels, AM; Sejnowski, TJ		Mozer, MC; Jordan, MI; Petsche, T		Tang, AC; Bartels, AM; Sejnowski, TJ			Cholinergic modulation preserves spike timing under physiologically realistic fluctuating input	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Neuromodulation can change not only the mean firing rate of a neuron, but also its pattern of firing. Therefore, a reliable neural coding scheme, whether a rate coding or a spike time based coding, must be robust in a dynamic neuromodulatory environment. The common observation that cholinergic modulation leads to a reduction in spike frequency adaptation implies a modification of spike timing, which would make a neural code based on precise spike timing difficult to maintain. In this paper, the effects of cholinergic modulation were studied to test the hypothesis that precise spike timing can serve as a reliable neural code. Using the whole cell patch-clamp technique in rat neocortical slice preparation and compartmental modeling techniques, we show that cholinergic modulation, surprisingly, preserved spike timing in response to a fluctuating inputs that resembles in vivo conditions. This result suggests that in vivo spike timing may be much more resistant to changes in neuromodulator concentrations than previous physiological studies have implied.			Tang, AC (corresponding author), SALK INST BIOL STUDIES,HOWARD HUGHES MED INST,COMPUTAT NEUROBIOL LAB,LA JOLLA,CA 92037, USA.		Sejnowski, Terrence/AAV-5558-2021						0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						111	117						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00016
C	Tsitsiklis, JN; VanRoy, B		Mozer, MC; Jordan, MI; Petsche, T		Tsitsiklis, JN; VanRoy, B			Approximate solutions to optimal stopping problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We propose and analyze an algorithm that approximates solutions to the problem of optimal stopping in a discounted irreducible aperiodic Markov chain. The scheme involves the use of linear combinations of fixed basis functions to approximate a Q-function. The weights of the linear combination are incrementally updated through an iterative process similar to Q-learning, involving simulation of the underlying Markov chain. Due to space limitations. we only provide an overview of a proof of convergence (with probability 1) and bounds on the approximation error. This is the first theoretical result that establishes the soundness of a Q-learning-like algorithm when combined with arbitrary linear function approximators to salve a sequential decision problem. Though this paper focuses on the case of finite state spaces, the results extend naturally to continuous and unbounded state spaces, which are addressed in a forthcoming full-length paper.			Tsitsiklis, JN (corresponding author), MIT,INFORMAT & DECIS SYST LAB,77 MASSACHUSETTS AVE,CAMBRIDGE,MA 02139, USA.								0	0	0	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						1082	1088						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00152
C	Bohossian, V; Bruck, J		Touretzky, DS; Mozer, MC; Hasselmo, ME		Bohossian, V; Bruck, J			On neural networks with minimal weights	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						CALTECH,PASADENA,CA 91125	California Institute of Technology									0	0	0	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						246	252						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00035
C	Campbell, PK; Dale, M; Ferra, HL; Kowalczyk, A		Touretzky, DS; Mozer, MC; Hasselmo, ME		Campbell, PK; Dale, M; Ferra, HL; Kowalczyk, A			Experiments with neural networks for real time implementation of control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						TELSTRA RES LABS,CLAYTON,VIC 3168,AUSTRALIA	Telstra									0	0	0	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						973	979						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00137
C	Flake, GW		Touretzky, DS; Mozer, MC; Hasselmo, ME		Flake, GW			The capacity of a bump	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV MARYLAND,INST ADV COMP STUDIES,COLLEGE PK,MD 20742	University System of Maryland; University of Maryland College Park									0	0	0	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						556	562						5	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00079
C	Gourley, R		Touretzky, DS; Mozer, MC; Hasselmo, ME		Gourley, R			Harmony networks do not work	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SIMON FRASER UNIV,SCH COMP SCI,BURNABY,BC V5A 1S6,CANADA	Simon Fraser University									0	0	0	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						31	37						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00005
C	Hasselmo, ME; Cekic, M		Touretzky, DS; Mozer, MC; Hasselmo, ME		Hasselmo, ME; Cekic, M			Cholinergic suppression of transmission may allow combined associative memory function and self-organization in the neocortex.	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						HARVARD UNIV,DEPT PSYCHOL,CAMBRIDGE,MA 02138	Harvard University									0	0	0	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						131	137						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00019
C	Jabri, MA; Wang, RJ		Touretzky, DS; Mozer, MC; Hasselmo, ME		Jabri, MA; Wang, RJ			A novel channel selection system in cochlear implants using artificial neural network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV SYDNEY,DEPT ELECT ENGN,SYST ENGN & DESIGN AUTOMAT LAB,SYDNEY,NSW 2006,AUSTRALIA	University of Sydney									0	0	0	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						910	916						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00128
C	Kowalczyk, A; Szymanski, J; Bartlett, PL; Williamson, RC		Touretzky, DS; Mozer, MC; Hasselmo, ME		Kowalczyk, A; Szymanski, J; Bartlett, PL; Williamson, RC			Examples of learning curves from a modified VC-formalism.	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						TELSTRA RES LABS,CLAYTON,VIC 3168,AUSTRALIA	Telstra									0	0	0	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						344	350						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00049
C	Lemarie, B; Gilloux, M; Leroux, M		Touretzky, DS; Mozer, MC; Hasselmo, ME		Lemarie, B; Gilloux, M; Leroux, M			Handwritten word recognition using contextual hybrid radial basis function network hidden Markov models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SRTP,F-44063 NANTES,FRANCE										0	0	0	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						764	770						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00108
C	Mansour, Y; Sahar, S		Touretzky, DS; Mozer, MC; Hasselmo, ME		Mansour, Y; Sahar, S			Implementation issues in the Fourier transform algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						TEL AVIV UNIV,DEPT COMP SCI,IL-69978 TEL AVIV,ISRAEL	Tel Aviv University									0	0	0	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						260	266						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00037
C	Marchand, M; Hadjifaradji, S		Touretzky, DS; Mozer, MC; Hasselmo, ME		Marchand, M; Hadjifaradji, S			Strong unimodality and exact learning of constant depth mu-perceptron networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV OTTAWA,DEPT COMP SCI,OTTAWA,ON K1N 6N5,CANADA	University of Ottawa									0	0	0	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						288	294						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00041
C	Matsuoka, Y		Touretzky, DS; Mozer, MC; Hasselmo, ME		Matsuoka, Y			Primitive manipulation learning with connectionism.	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						MIT,ARTIFICIAL INTELLIGENCE LAB,CAMBRIDGE,MA 02139	Massachusetts Institute of Technology (MIT)									0	0	0	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						889	895						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00125
C	Miller, DJ; Rao, A; Rose, K; Gersho, A		Touretzky, DS; Mozer, MC; Hasselmo, ME		Miller, DJ; Rao, A; Rose, K; Gersho, A			An information-theoretic learning algorithm for neural network classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						PENN STATE UNIV,DEPT ELECT ENGN,STATE COLL,PA 16802	Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University									0	0	0	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						591	597						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00084
C	Rehfuss, S; Hammerstrom, D		Touretzky, DS; Mozer, MC; Hasselmo, ME		Rehfuss, S; Hammerstrom, D			Model matching and SFMD computation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						OREGON GRAD INST SCI & TECHNOL,DEPT COMP SCI & ENGN,PORTLAND,OR 97291										0	0	0	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						713	719						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00101
C	Semenov, SA; Shuvalova, IB		Touretzky, DS; Mozer, MC; Hasselmo, ME		Semenov, SA; Shuvalova, IB			Some results on convergent unlearning algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						INST PHYS & TECHNOL,MOSCOW 119034,RUSSIA	Moscow Institute of Physics & Technology									0	0	0	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						358	364						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00051
C	ShaweTaylor, J; Zhao, JY		Touretzky, DS; Mozer, MC; Hasselmo, ME		ShaweTaylor, J; Zhao, JY			Generalisation of a class of continuous neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV LONDON,ROYAL HOLLOWAY & BEDFORD NEW COLL,DEPT COMP SCI,EGHAM TW20 0EX,SURREY,ENGLAND	University of London; Royal Holloway University London				Shawe-Taylor, John/0000-0002-2030-0073					0	0	0	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						267	273						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00038
C	Shustorovich, A; Thrasher, CW		Touretzky, DS; Mozer, MC; Hasselmo, ME		Shustorovich, A; Thrasher, CW			Kodak Imagelink(TM) OCR - Alphanumeric handprint module	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						EASTMAN KODAK CO,BUSINESS IMAGING SYST,ROCHESTER,NY 14653	Eastman Kodak									0	0	0	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						778	784						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00110
C	Tani, J; Fukumura, N		Touretzky, DS; Mozer, MC; Hasselmo, ME		Tani, J; Fukumura, N			A dynamical systems approach for a learnable autonomous robot	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SONY COMP SCI LAB INC,SHINAGAWA KU,TOKYO 141,JAPAN	Sony Corporation									0	0	0	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						989	995						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00139
C	Wu, LH; Moody, J		Touretzky, DS; Mozer, MC; Hasselmo, ME		Wu, LH; Moody, J			A smoothing regularizer for recurrent neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	Advances in Neural Information Processing Systems		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						OREGON GRAD INST, DEPT COMP SCI, PORTLAND, OR 97291 USA										0	0	0	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						458	464						7	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00065
C	ANASTASIO, TJ		MOODY, JE; HANSON, SJ; LIPPMANN, RP		ANASTASIO, TJ			LEARNING IN THE VESTIBULAR SYSTEM - SIMULATIONS OF VESTIBULAR COMPENSATION USING RECURRENT BACKPROPAGATION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						603	610						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00074
C	BASRI, R; ULLMAN, S		MOODY, JE; HANSON, SJ; LIPPMANN, RP		BASRI, R; ULLMAN, S			LINEAR OPERATOR FOR OBJECT RECOGNITION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						452	459						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00056
C	BECKER, S; HINTON, GE		MOODY, JE; HANSON, SJ; LIPPMANN, RP		BECKER, S; HINTON, GE			LEARNING TO MAKE COHERENT PREDICTIONS IN DOMAINS WITH DISCONTINUITIES	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO											Becker, Suzanna/0000-0002-2645-070X					0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						372	379						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00046
C	BENGIO, Y; FLAMMIA, G; DEMORI, R; KOMPE, R		MOODY, JE; HANSON, SJ; LIPPMANN, RP		BENGIO, Y; FLAMMIA, G; DEMORI, R; KOMPE, R			NEURAL NETWORK - GAUSSIAN MIXTURE HYBRID FOR SPEECH RECOGNITION OR DENSITY-ESTIMATION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						175	182						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00022
C	BERNANDER, O; KOCH, C; DOUGLAS, RJ		MOODY, JE; HANSON, SJ; LIPPMANN, RP		BERNANDER, O; KOCH, C; DOUGLAS, RJ			NETWORK ACTIVITY DETERMINES SPATIOTEMPORAL INTEGRATION IN SINGLE CELLS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						43	50						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00006
C	BERTONI, A; CAMPADELLI, P; MORPURGO, A; PANIZZA, S		MOODY, JE; HANSON, SJ; LIPPMANN, RP		BERTONI, A; CAMPADELLI, P; MORPURGO, A; PANIZZA, S			POLYNOMIAL UNIFORM-CONVERGENCE OF RELATIVE FREQUENCIES TO PROBABILITIES	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						904	911						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00111
C	BRODY, C		MOODY, JE; HANSON, SJ; LIPPMANN, RP		BRODY, C			FAST LEARNING WITH PREDICTIVE FORWARD MODELS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						563	570						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00069
C	BULSARA, AR; JACOBS, EW; MOSS, F		MOODY, JE; HANSON, SJ; LIPPMANN, RP		BULSARA, AR; JACOBS, EW; MOSS, F			SINGLE NEURON MODEL - RESPONSE TO WEAK MODULATION IN THE PRESENCE OF NOISE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						67	74						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00009
C	CHIANG, AM; CHUANG, ML; LAFRANCHISE, JR		MOODY, JE; HANSON, SJ; LIPPMANN, RP		CHIANG, AM; CHUANG, ML; LAFRANCHISE, JR			CCD NEURAL NETWORK PROCESSORS FOR PATTERN-RECOGNITION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						741	747						7	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00091
C	COOPER, PR; PROKOPOWICZ, PN		MOODY, JE; HANSON, SJ; LIPPMANN, RP		COOPER, PR; PROKOPOWICZ, PN			MARKOV RANDOM-FIELDS CAN BRIDGE LEVELS OF ABSTRACTION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						396	403						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00049
C	ESKANDAR, EN; HERTZ, JA; RICHMOND, BJ; OPTICAN, LM; KJAER, T		MOODY, JE; HANSON, SJ; LIPPMANN, RP		ESKANDAR, EN; HERTZ, JA; RICHMOND, BJ; OPTICAN, LM; KJAER, T			DECODING OF NEURONAL SIGNALS IN VISUAL-PATTERN RECOGNITION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						356	363						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00044
C	FANTY, M; COLE, RA; ROGINSKI, K		MOODY, JE; HANSON, SJ; LIPPMANN, RP		FANTY, M; COLE, RA; ROGINSKI, K			ENGLISH ALPHABET RECOGNITION WITH TELEPHONE SPEECH	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						199	206						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00025
C	FREEMAN, DT		MOODY, JE; HANSON, SJ; LIPPMANN, RP		FREEMAN, DT			COMPUTER RECOGNITION OF WAVE LOCATION IN GRAPHICAL DATA BY A NEURAL NETWORK	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						706	713						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00087
C	GOMI, H; KAWATO, M		MOODY, JE; HANSON, SJ; LIPPMANN, RP		GOMI, H; KAWATO, M			RECOGNITION OF MANIPULATED OBJECTS BY MOTOR LEARNING	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						547	554						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00067
C	GUPTA, P; TOURETZKY, DS		MOODY, JE; HANSON, SJ; LIPPMANN, RP		GUPTA, P; TOURETZKY, DS			A CONNECTIONIST LEARNING APPROACH TO ANALYZING LINGUISTIC STRESS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						225	232						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00028
C	GUYON, I; VAPNIK, V; BOSER, B; BOTTOU, L; SOLLA, SA		MOODY, JE; HANSON, SJ; LIPPMANN, RP		GUYON, I; VAPNIK, V; BOSER, B; BOTTOU, L; SOLLA, SA			STRUCTURAL RISK MINIMIZATION FOR CHARACTER-RECOGNITION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	1	0	1	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						471	479						9	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00058
C	HARRIS, JG		MOODY, JE; HANSON, SJ; LIPPMANN, RP		HARRIS, JG			SEGMENTATION CIRCUITS USING CONSTRAINED OPTIMIZATION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						797	804						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00098
C	HIRAYAMA, M; VATIKIOTISBATESON, E; KAWATO, M; JORDAN, MI		MOODY, JE; HANSON, SJ; LIPPMANN, RP		HIRAYAMA, M; VATIKIOTISBATESON, E; KAWATO, M; JORDAN, MI			FORWARD DYNAMICS MODELING OF SPEECH MOTOR CONTROL USING PHYSIOLOGICAL DATA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										Jordan, Michael I/C-5253-2013						0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						191	198						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00024
C	HOBSON, JA; MAMELAK, AN; SUTTON, JP		MOODY, JE; HANSON, SJ; LIPPMANN, RP		HOBSON, JA; MAMELAK, AN; SUTTON, JP			MODELS WANTED - MUST FIT DIMENSIONS OF SLEEP AND DREAMING	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						3	10						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00001
C	HWANG, JN; LI, H; MAECHLER, M; MARTIN, RD; SCHIMERT, J		MOODY, JE; HANSON, SJ; LIPPMANN, RP		HWANG, JN; LI, H; MAECHLER, M; MARTIN, RD; SCHIMERT, J			A COMPARISON OF PROJECTION PURSUIT AND NEURAL NETWORK REGRESSION MODELING	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1159	1166						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00143
C	INTRATOR, N; BULTHOFF, HH; GOLD, JI; EDELMAN, S		MOODY, JE; HANSON, SJ; LIPPMANN, RP		INTRATOR, N; BULTHOFF, HH; GOLD, JI; EDELMAN, S			3D OBJECT RECOGNITION USING UNSUPERVISED FEATURE-EXTRACTION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										Bülthoff, Heinrich H/J-6579-2012; Bülthoff, Heinrich/AAC-8818-2019	Bülthoff, Heinrich H/0000-0003-2568-0607; 					0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						460	467						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00057
C	JAIN, AN		MOODY, JE; HANSON, SJ; LIPPMANN, RP		JAIN, AN			GENERALIZATION PERFORMANCE IN PARSEC - A STRUCTURED CONNECTIONIST PARSING ARCHITECTURE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						209	216						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00026
C	JI, CY; PSALTIS, D		MOODY, JE; HANSON, SJ; LIPPMANN, RP		JI, CY; PSALTIS, D			THE VC-DIMENSION VERSUS THE STATISTICAL CAPACITY OF MULTILAYER NETWORKS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						928	935						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00114
C	JUDD, S		MOODY, JE; HANSON, SJ; LIPPMANN, RP		JUDD, S			CONSTANT-TIME LOADING OF SHALLOW 1-DIMENSIONAL NETWORKS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						863	870						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00106
C	KEELER, J; RUMELHART, DE		MOODY, JE; HANSON, SJ; LIPPMANN, RP		KEELER, J; RUMELHART, DE			A SELF-ORGANIZING INTEGRATED SEGMENTATION AND RECOGNITION NEURAL NET	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	2	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						496	503						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00061
C	KIRK, D; FLEISCHER, K; WATTS, L; BARR, A		MOODY, JE; HANSON, SJ; LIPPMANN, RP		KIRK, D; FLEISCHER, K; WATTS, L; BARR, A			CONSTRAINED OPTIMIZATION APPLIED TO THE PARAMETER SETTING PROBLEM FOR ANALOG CIRCUITS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						789	796						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00097
C	KOHN, P; BILMES, J; MORGAN, N; BECK, J		MOODY, JE; HANSON, SJ; LIPPMANN, RP		KOHN, P; BILMES, J; MORGAN, N; BECK, J			SOFTWARE FOR ANN TRAINING ON A RING ARRAY PROCESSOR	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	0	0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						781	788						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00096
