PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
J	Hu, JF; Sun, JX; Lin, ZH; Lai, JH; Zeng, WJ; Zheng, WS				Hu, Jian-Fang; Sun, Jiangxin; Lin, Zihang; Lai, Jian-Huang; Zeng, Wenjun; Zheng, Wei-Shi			APANet: Auto-Path Aggregation for Future Instance Segmentation Prediction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature extraction; Image segmentation; Predictive models; Aggregates; Task analysis; Adaptation models; Hidden Markov models; Future prediction; Future instance segmentation prediction; Instance segmentation; auto-path aggregation		Despite the remarkable progress achieved in conventional instance segmentation, the problem of predicting instance segmentation results for unobserved future frames remains challenging due to the unobservability of future data. Existing methods mainly address this challenge by forecasting features of future frames. However, these methods always treat features of multiple levels (e.g., coarse-to-fine pyramid features) independently and do not exploit them collaboratively, which results in inaccurate prediction for future frames; and moreover, such a weakness can partially hinder self-adaption of a future segmentation prediction model for different input samples. To solve this problem, we propose an adaptive aggregation approach called Auto-Path Aggregation Network (APANet), where the spatio-temporal contextual information obtained in the features of each individual level is selectively aggregated using the developed "auto-path". The "auto-path" connects each pair of features extracted at different pyramid levels for task-specific hierarchical contextual information aggregation, which enables selective and adaptive aggregation of pyramid features in accordance with different videos/frames. Our APANet can be further optimized jointly with the Mask R-CNN head as a feature decoder and a Feature Pyramid Network (FPN) feature encoder, forming a joint learning system for future instance segmentation prediction. We experimentally show that the proposed method can achieve state-of-the-art performance on three video-based instance segmentation benchmarks for future instance segmentation prediction.	[Hu, Jian-Fang; Sun, Jiangxin; Lin, Zihang; Lai, Jian-Huang; Zheng, Wei-Shi] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Peoples R China; [Hu, Jian-Fang; Lai, Jian-Huang] Guangdong Prov Key Lab Informat Secur Technol, Guangzhou 510275, Peoples R China; [Sun, Jiangxin] Pazhou Lab, Guangzhou, Peoples R China; [Zeng, Wenjun] Microsoft Res Asia, Beijing 100080, Peoples R China; [Zheng, Wei-Shi] Sun Yat Sen Univ, Minist Educ, Key Lab Machine Intelligence & Adv Comp, Guangzhou 510275, Peoples R China; [Zheng, Wei-Shi] Peng Cheng Lab, Shenzhen 518066, Peoples R China	Sun Yat Sen University; Pazhou Lab; Microsoft; Microsoft Research Asia; Sun Yat Sen University; Peng Cheng Laboratory	Zheng, WS (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Peoples R China.; Zheng, WS (corresponding author), Sun Yat Sen Univ, Minist Educ, Key Lab Machine Intelligence & Adv Comp, Guangzhou 510275, Peoples R China.	hujf5@mail.sysu.edu; sunjx5@mail2.sysu.edu.cn; linzh59@mail2.sysu.edu.cn; stsljh@mail.sysu.edu; wezeng@microsoft.com; wszheng@ieee.org			National Key Research and Development Program of China [2018YFB1004903]; NSFC [U1911401, 62076260, U1811461, U1803120]; Guangdong NSF Project [2020B1515120085, 2018B030312002]; Research Projects of Zhejiang Lab [2019KD0AB03]; Key-Area Research and Development Program of Guangzhou [202007030004]; Guangzhou Research Project [201902010037]	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Guangdong NSF Project; Research Projects of Zhejiang Lab; Key-Area Research and Development Program of Guangzhou; Guangzhou Research Project	This work was supported partially by National Key Research and Development Program of China (2018YFB1004903), the NSFC under Grants (U1911401, 62076260, U1811461, U1803120), Guangdong NSF Project (No. 2020B1515120085, No. 2018B030312002, Guangzhou Research Project (201902010037), and Research Projects of Zhejiang Lab (No. 2019KD0AB03), and the Key-Area Research and Development Program of Guangzhou (202007030004). Jian-Fang Hu and Jiangxin Sun are the co-first authors of the article.	Arnab A, 2017, PROC CVPR IEEE, P879, DOI 10.1109/CVPR.2017.100; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Bai M, 2017, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2017.305; Baker B., 2017, ARXIV 170510823; Bender G, 2018, PR MACH LEARN RES, V80; Bhattacharyya A, 2018, PROC CVPR IEEE, P4194, DOI 10.1109/CVPR.2018.00441; Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925; Byeon W, 2018, LECT NOTES COMPUT SC, V11220, P781, DOI 10.1007/978-3-030-01270-0_46; CAI H., 2018, P INT C LEARN REPR; Cai H, 2018, AAAI CONF ARTIF INTE, P2787; Chen HT, 2020, PROC CVPR IEEE, P1638, DOI 10.1109/CVPR42600.2020.00171; Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen XL, 2019, IEEE I CONF COMP VIS, P2061, DOI 10.1109/ICCV.2019.00215; Chen XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1503, DOI 10.1145/3123266.3123349; Chiu HK, 2020, IEEE ROBOT AUTOM LET, V5, P4202, DOI 10.1109/LRA.2020.2992184; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343; Dai JF, 2016, LECT NOTES COMPUT SC, V9910, P534, DOI 10.1007/978-3-319-46466-4_32; Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254; Elsken T., 2017, ARXIV 171104528; Elsken T., 2018, PROC INT C LEARN REP; Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221; Fangyun Wei, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P527, DOI 10.1007/978-3-030-58607-2_31; Fayyaz M., 2016, ARXIV160805971; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Gao H, 2019, IEEE I CONF COMP VIS, P9005, DOI 10.1109/ICCV.2019.00910; Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20; He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI 10.1109/TPAMI.2018.2844175; Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657; Jin HF, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1946, DOI 10.1145/3292500.3330648; Kandasamy Kirthevasan, 2018, ADV NEURAL INFORM PR, P2020, DOI DOI 10.5555/3326943.3327130; Kirillov A, 2017, PROC CVPR IEEE, P7322, DOI 10.1109/CVPR.2017.774; Kwon YH, 2019, PROC CVPR IEEE, P1811, DOI 10.1109/CVPR.2019.00191; Li Y., 2020, P IEEE CVF C COMP VI, P8553; Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472; Liang Justin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9128, DOI 10.1109/CVPR42600.2020.00915; Liang XD, 2018, IEEE T PATTERN ANAL, V40, P2978, DOI 10.1109/TPAMI.2017.2775623; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin Z., 2019, PROC DAVIS CHALLENGE; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Liu H, 2019, PROCEEDINGS OF THE THIRD INTERNATIONAL SYMPOSIUM - EDUCATIONAL RESEARCH AND EDUCATIONAL TECHNOLOGY, 2019, P3; Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913; Liu S, 2017, IEEE I CONF COMP VIS, P3516, DOI 10.1109/ICCV.2017.378; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Luc P, 2018, LECT NOTES COMPUT SC, V11213, P593, DOI 10.1007/978-3-030-01240-3_36; Luc P, 2017, IEEE I CONF COMP VIS, P648, DOI 10.1109/ICCV.2017.77; Ma LZ, 2019, 2019 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE (WI 2019), P500, DOI 10.1145/3350546.3360740; Mathieu Michael, 2016, ICLR; Morris BT, 2011, IEEE T PATTERN ANAL, V33, P2287, DOI 10.1109/TPAMI.2011.64; Neven D, 2019, PROC CVPR IEEE, P8829, DOI 10.1109/CVPR.2019.00904; Oh J., 2015, P ADV NEUR INF PROC, P2863; Pham H, 2018, PR MACH LEARN RES, V80; Pinheiro P. O. O., 2015, ADV NEURAL INFORM PR, V28, P1990; Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Real E, 2017, PR MACH LEARN RES, V70; Ren S., 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.169; Rochan M, 2018, ARXIV 180707946; Rodriguez M. D., 2007, P INT C MULT, P353, DOI DOI 10.1145/1291233.1291310; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rufeng Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10223, DOI 10.1109/CVPR42600.2020.01024; Saric J, 2019, LECT NOTES COMPUT SC, V11824, P189, DOI 10.1007/978-3-030-33676-9_13; Seguin G, 2016, PROC CVPR IEEE, P3678, DOI 10.1109/CVPR.2016.400; Seguin G, 2015, IEEE T PATTERN ANAL, V37, P1643, DOI 10.1109/TPAMI.2014.2369050; Shi XJ, 2015, ADV NEUR IN, V28; Shi YG, 2018, LECT NOTES COMPUT SC, V11214, P305, DOI 10.1007/978-3-030-01249-6_19; Sida Peng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8530, DOI 10.1109/CVPR42600.2020.00856; Sofiiuk K, 2019, IEEE I CONF COMP VIS, P7354, DOI 10.1109/ICCV.2019.00745; Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44; Suganuma M, 2017, PROCEEDINGS OF THE 2017 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'17), P497, DOI 10.1145/3071178.3071229; Sun JX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2043, DOI 10.1145/3343031.3350949; Vondrick C, 2016, PROC CVPR IEEE, P98, DOI 10.1109/CVPR.2016.18; Wang YJ, 2020, PROC CVPR IEEE, P6957, DOI 10.1109/CVPR42600.2020.00699; Wang YJ, 2019, INT J COMPUT VISION, V127, P625, DOI 10.1007/s11263-018-1130-2; Wang YB, 2017, ADV NEUR IN, V30; Xie D, 2018, IEEE T PATTERN ANAL, V40, P1639, DOI 10.1109/TPAMI.2017.2728788; Xinlong Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P649, DOI 10.1007/978-3-030-58523-5_38; Xu K, 2019, PROC CVPR IEEE, P1379, DOI 10.1109/CVPR.2019.00147; Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388; Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271; Yuan Y., 2018, ARXIV 180900916; Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11; Yuqing Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9310, DOI 10.1109/CVPR42600.2020.00933; Zhang F, 2019, IEEE I CONF COMP VIS, P6797, DOI 10.1109/ICCV.2019.00690; Zhang H, 2019, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2019.00064; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhi Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P282, DOI 10.1007/978-3-030-58452-8_17; Zhou Q, 2019, WORLD WIDE WEB, V22, P555, DOI 10.1007/s11280-018-0556-3; Zoph B., 2017, P1; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	98	2	2	20	29	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3386	3403		10.1109/TPAMI.2021.3058679	http://dx.doi.org/10.1109/TPAMI.2021.3058679			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33571087				2022-12-18	WOS:000805820500006
J	Peng, X; Gao, L; Wang, YF; Kneip, L				Peng, Xin; Gao, Ling; Wang, Yifu; Kneip, Laurent			Globally-Optimal Contrast Maximisation for Event Cameras	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cameras; Estimation; Motion estimation; Optical sensors; Optical imaging; Three-dimensional displays; Optimization; Event cameras; motion estimation; optical flow; contrast maximisation; global optimality; branch and bound	VISUAL ODOMETRY; ROTATION SPACE; LOCALIZATION; OPTIMIZATION	Event cameras are bio-inspired sensors that perform well in challenging illumination conditions and have high temporal resolution. However, their concept is fundamentally different from traditional frame-based cameras. The pixels of an event camera operate independently and asynchronously. They measure changes of the logarithmic brightness and return them in the highly discretised form of time-stamped events indicating a relative change of a certain quantity since the last event. New models and algorithms are needed to process this kind of measurements. The present work looks at several motion estimation problems with event cameras. The flow of the events is modelled by a general homographic warping in a space-time volume, and the objective is formulated as a maximisation of contrast within the image of warped events. Our core contribution consists of deriving globally optimal solutions to these generally non-convex problems, which removes the dependency on a good initial guess plaguing existing methods. Our methods rely on branch-and-bound optimisation and employ novel and efficient, recursive upper and lower bounds derived for six different contrast estimation functions. The practical validity of our approach is demonstrated by a successful application to three different event camera motion estimation problems.	[Peng, Xin; Gao, Ling; Kneip, Laurent] ShanghaiTech Univ, Sch Informat Sci & Technol, Mobile Percept Lab, Shanghai 201210, Peoples R China; [Peng, Xin] Chinese Acad Sci, Shanghai Inst Microsyst & Informat Technol, Shanghai 200050, Peoples R China; [Peng, Xin] Univ Chinese Acad Sci, Beijing 100864, Peoples R China; [Kneip, Laurent] Shanghai Engn Res Ctr Intelligent Vis & Imaging, Shanghai 201210, Peoples R China; [Wang, Yifu] Australian Natl Univ, Canberra, ACT 0200, Australia	ShanghaiTech University; Chinese Academy of Sciences; Shanghai Institute of Microsystem & Information Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Australian National University	Peng, X (corresponding author), ShanghaiTech Univ, Sch Informat Sci & Technol, Mobile Percept Lab, Shanghai 201210, Peoples R China.; Peng, X (corresponding author), Chinese Acad Sci, Shanghai Inst Microsyst & Informat Technol, Shanghai 200050, Peoples R China.	pengxin1@shanghaitech.edu.cn; gaoling@shanghaitech.edu.cn; usasuper@126.com; kneip.laurent@gmail.com		Gao, Ling/0000-0003-2294-5155	Natural Science Foundation of Shanghai [19ZR1434000]; Natural Science Foundation of China [61950410612]	Natural Science Foundation of Shanghai(Natural Science Foundation of Shanghai); Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	The authors would like to thank Prof. Kyros Kutulakos for his kind advice and Mr. Daqi Liu who kindly offered his code. The authors would also like to thank the fundings sponsored by the Natural Science Foundation of Shanghai Grant No. 19ZR1434000 and Natural Science Foundation of China Grant No. 61950410612.	Aires KRT, 2008, APPLIED COMPUTING 2008, VOLS 1-3, P1607; Bardow P, 2016, PROC CVPR IEEE, P884, DOI 10.1109/CVPR.2016.102; Bazin Jean-Charles, 2012, P AS C COMP VIS, P2; Belbachir AN, 2011, IEEE T IND ELECTRON, V58, P770, DOI 10.1109/TIE.2010.2095390; Ben-Afia A, 2014, IET RADAR SONAR NAV, V8, P1059, DOI 10.1049/iet-rsn.2013.0389; Benosman R, 2014, IEEE T NEUR NET LEAR, V25, P407, DOI 10.1109/TNNLS.2013.2273537; Brandli C, 2014, IEEE J SOLID-ST CIRC, V49, P2333, DOI 10.1109/JSSC.2014.2342715; Breuel TM, 2003, COMPUT VIS IMAGE UND, V90, P258, DOI 10.1016/S1077-3142(03)00026-2; Brown M, 2019, PATTERN RECOGN, V93, P36, DOI 10.1016/j.patcog.2019.04.002; Brown M, 2015, IEEE I CONF COMP VIS, P2111, DOI 10.1109/ICCV.2015.244; Bulow H, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P3368, DOI 10.1109/IROS.2009.5354505; Bustos AP, 2016, IEEE T PATTERN ANAL, V38, P2227, DOI 10.1109/TPAMI.2016.2517636; Butime J, 2006, VISAPP 2006: PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 2, P457; Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754; Campbell D, 2019, PROC CVPR IEEE, P11788, DOI 10.1109/CVPR.2019.01207; Campbell D, 2020, IEEE T PATTERN ANAL, V42, P328, DOI 10.1109/TPAMI.2018.2848650; Campbell D, 2017, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2017.10; Campbell D, 2016, PROC CVPR IEEE, P5685, DOI 10.1109/CVPR.2016.613; Chadha A., 2019, ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Proceedings, P7968, DOI 10.1109/ICASSP.2019.8683606; Chankyu Lee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P366, DOI 10.1007/978-3-030-58526-6_22; Che EZ, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19040810; Chen H., 2020, PROC AAAI C ARTIF IN, P10534; Cook M, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P770, DOI 10.1109/IJCNN.2011.6033299; Dekai Zhu, 2019, 2019 IEEE International Conference on Robotics and Biomimetics (ROBIO), P2225, DOI 10.1109/ROBIO49542.2019.8961878; Delbruck T, 2014, IEEE INT SYMP CIRC S, P2636, DOI 10.1109/ISCAS.2014.6865714; Enqvist O., 2009, PROC BRIT MACH VIS C, V2, P3; Fuentes-Pacheco J, 2015, ARTIF INTELL REV, V43, P55, DOI 10.1007/s10462-012-9365-8; Gallego G, 2019, PROC CVPR IEEE, P12272, DOI 10.1109/CVPR.2019.01256; Gallego G, 2018, PROC CVPR IEEE, P3867, DOI 10.1109/CVPR.2018.00407; Gallego G, 2017, IEEE ROBOT AUTOM LET, V2, P632, DOI 10.1109/LRA.2016.2647639; Gao L, 2020, IEEE INT CONF ROBOT, P2696, DOI 10.1109/ICRA40945.2020.9196595; Gehrig M, 2020, IEEE INT CONF ROBOT, P4195, DOI 10.1109/ICRA40945.2020.9197133; Haessig G, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-40064-0; Ham H., 2019, INT J ELECT COMPUT E, V9, P2394, DOI 10.11591/ijece.v9i4.pp2394-2402; Hartley RI, 2007, IEEE I CONF COMP VIS, P534; Hartley RI, 2009, INT J COMPUT VISION, V82, P64, DOI 10.1007/s11263-008-0186-9; Holmgren D. E., 2004, PHOTOGRAMM REC, V19, P415; Hongmin Li, 2016, Advances in Brain-Inspired Cognitive Systems. 8th International Conference, BICS 2016. Proceedings: LNAI 10023, P138, DOI 10.1007/978-3-319-49685-6_13; HORN BKP, 1981, P SOC PHOTO-OPT INST, V281, P319; Hu L., 2020, ARXIV 200207988; Huang K, 2019, PROC CVPR IEEE, P12698, DOI 10.1109/CVPR.2019.01299; Kepple Daniel R., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P500, DOI 10.1007/978-3-030-58539-6_30; Kim H., 2014, P BRIT MACH VIS C; Kim H, 2016, LECT NOTES COMPUT SC, V9910, P349, DOI 10.1007/978-3-319-46466-4_21; Kim Hanme, 2014, BRIT MACH VIS C BMVC, DOI [10.5244/C.28.26, DOI 10.5244/C.28.26]; Kim JH, 2008, PROC CVPR IEEE, P2633; Kleeman L., 2017, P AUSTR C ROB AUT AC, P1; Kneip L, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.16; Kneip L, 2014, IEEE INT CONF ROBOT, P1, DOI 10.1109/ICRA.2014.6906582; Liu DQ, 2020, PROC CVPR IEEE, P6348, DOI 10.1109/CVPR42600.2020.00638; Liu YL, 2019, IEEE T IMAGE PROCESS, V28, P2599, DOI 10.1109/TIP.2018.2887207; Liu YL, 2018, LECT NOTES COMPUT SC, V11216, P460, DOI 10.1007/978-3-030-01258-8_28; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2; Mitrokhin A., 2018, 2018 IEEE RSJ INT C, P1; Mueggler E, 2018, IEEE T ROBOT, V34, P1425, DOI 10.1109/TRO.2018.2858287; Mueggler E, 2017, INT J ROBOT RES, V36, P142, DOI 10.1177/0278364917691115; Olsson C, 2009, IEEE T PATTERN ANAL, V31, P783, DOI 10.1109/TPAMI.2008.131; Pan LY, 2019, PROC CVPR IEEE, P6813, DOI 10.1109/CVPR.2019.00698; Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223; Peng X., 2019, PROC IEEERSJ INT C I, P2052; Pfeuffer F, 2012, ANN OPER RES, V196, P737, DOI 10.1007/s10479-010-0760-8; Rebecq H., 2016, P BRIT MACH VIS C, P1; Rebecq H, 2021, IEEE T PATTERN ANAL, V43, P1964, DOI 10.1109/TPAMI.2019.2963386; Rebecq H, 2018, INT J COMPUT VISION, V126, P1394, DOI 10.1007/s11263-017-1050-6; Rebecq H, 2017, IEEE ROBOT AUTOM LET, V2, P593, DOI 10.1109/LRA.2016.2645143; Rodriguez-Gomez JP, 2020, IEEE INT CONF ROBOT, P8518, DOI 10.1109/ICRA40945.2020.9197341; Scaramuzza D, 2011, IEEE ROBOT AUTOM MAG, V18, P80, DOI 10.1109/MRA.2011.943233; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Seitz S.M., 2006, P IEEE COMPUTER SOC, P519; Songnan Lin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P695, DOI 10.1007/978-3-030-58598-3_41; Stoffregen T, 2019, IEEE I CONF COMP VIS, P7243, DOI 10.1109/ICCV.2019.00734; Stoffregen T, 2019, PROC CVPR IEEE, P12292, DOI 10.1109/CVPR.2019.01258; Weikersdorfer David, 2013, Computer Vision Systems. 9th International Conference, ICVS 2013. Proceedings: LNCS 7963, P133, DOI 10.1007/978-3-642-39402-7_14; Xin Peng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P51, DOI 10.1007/978-3-030-58574-7_4; Xu QW, 2019, IEEE IMAGE PROC, P320, DOI 10.1109/ICIP.2019.8802933; Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405; Yang JL, 2014, LECT NOTES COMPUT SC, V8689, P111, DOI 10.1007/978-3-319-10590-1_8; Ye C., 2019, LEARNING DENSE OPTIC; Yinqiang Zheng, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2953, DOI 10.1109/CVPR.2011.5995352; Yu L, 2007, IEEE INT C BIOINFORM, P9, DOI 10.1109/BIBM.2007.19; Zhang Y, 2018, P CHIN HIGHR EARTH O, P164; Zhou Y., 2020, ARXIV 200715548; Zhou Y, 2018, LECT NOTES COMPUT SC, V11205, P242, DOI 10.1007/978-3-030-01246-5_15; Zhu Alex Zihao, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4465, DOI 10.1109/ICRA.2017.7989517; Zhu AZ, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Zhu AZ, 2019, PROC CVPR IEEE, P989, DOI 10.1109/CVPR.2019.00108; Zhu ALZH, 2018, LECT NOTES COMPUT SC, V11210, P438, DOI 10.1007/978-3-030-01231-1_27; Zhu AZH, 2017, PROC CVPR IEEE, P5816, DOI 10.1109/CVPR.2017.616	89	2	2	10	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3479	3495		10.1109/TPAMI.2021.3053243	http://dx.doi.org/10.1109/TPAMI.2021.3053243			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33471749	Green Submitted			2022-12-18	WOS:000805820500012
J	Singh, B; Najibi, M; Sharma, A; Davis, LS				Singh, Bharat; Najibi, Mahyar; Sharma, Abhishek; Davis, Larry S.			Scale Normalized Image Pyramids With AutoFocus for Object Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature extraction; Training; Object detection; Computational efficiency; Spatial resolution; Semantics; Detectors; Object detection; image pyramids; foveal vision; scale-space theory; deep-learning	EDGE	We present an efficient foveal framework to perform object detection. A scale normalized image pyramid (SNIP) is generated that, like human vision, only attends to objects within a fixed size range at different scales. Such a restriction of objects' size during training affords better learning of object-sensitive filters, and therefore, results in better accuracy. However, the use of an image pyramid increases the computational cost. Hence, we propose an efficient spatial sub-sampling scheme which only operates on fixed-size sub-regions likely to contain objects (as object locations are known during training). The resulting approach, referred to as Scale Normalized Image Pyramid with Efficient Resampling or SNIPER, yields up to 3x speed-up during training. Unfortunately, as object locations are unknown during inference, the entire image pyramid still needs processing. To this end, we adopt a coarse-to-fine approach, and predict the locations and extent of object-like regions which will be processed in successive scales of the image pyramid. Intuitively, it's akin to our active human-vision that first skims over the field-of-view to spot interesting regions for further processing and only recognizes objects at the right resolution. The resulting algorithm is referred to as AutoFocus and results in a 2.5-5x speed-up during inference when used with SNIP. Code: https://github.com/mahyarnajibi/SNIPER.	[Singh, Bharat; Najibi, Mahyar; Sharma, Abhishek; Davis, Larry S.] Univ Maryland, Comp Sci Dept, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park	Singh, B (corresponding author), Univ Maryland, Comp Sci Dept, College Pk, MD 20742 USA.	bharat@cs.umd.edu; najibi@cs.umd.edu; abhisharayiya@gmail.com; lsd@cs.umd.edu			Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA) [D17PC00287, D17PC00345]	Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA)	The authors would like to thank an Amazon Machine Learning gift for the AWS credits used for this research. The research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via DOI/IBC Contract Numbers D17PC00287 and D17PC00345. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes not withstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied of IARPA, DOI/IBC or the U.S. Government.	[Anonymous], 2018, ECCV 2018 OPEN IMAGE; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314; Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593; BURT PJ, 1981, COMPUT VISION GRAPH, V16, P20, DOI 10.1016/0146-664X(81)90092-7; Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chao Peng, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6181, DOI 10.1109/CVPR.2018.00647; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen Q, 2013, PROC CVPR IEEE, P3190, DOI 10.1109/CVPR.2013.410; Chen Z, 2018, LECT NOTES COMPUT SC, V11212, P74, DOI 10.1007/978-3-030-01237-3_5; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755; Crowley J. L., 1981, CMURITR8207; Dai JF, 2016, ADV NEUR IN, V29; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Hanson A. R., 1974, ADA004964 U MASS AMH; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton G., 2015, ARXIV150302531; Huang Z., 2020, ARXIV 200801365; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Klinger A., 1971, PATTERN SEARCH STAT, P303, DOI [10.1016/b978-0-12-604550-5.50019-5, DOI 10.1016/B978-0-12-604550-5.50019-5]; Kong T., 2019, ARXIV 190403797; Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98; Krasin Ivan, 2017, OPENIMAGES PUBLIC DA, V2, P18; Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Li Zeming, 2017, ARXIV171107264; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; Lindeberg T., 1994, SCALE SPACE THEORY C; Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913; Long S., 2018, INT J COMPUT VISION; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luo WJ, 2016, ADV NEUR IN, V29; MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Najibi M, 2019, IEEE I CONF COMP VIS, P9744, DOI 10.1109/ICCV.2019.00984; Najibi M, 2017, IEEE I CONF COMP VIS, P4885, DOI 10.1109/ICCV.2017.522; Narang S., 2018, PROC INT C LEARN REP; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377; Tanimoto S., 1975, COMPUTER GRAPHICS IM, V4, P104; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951; UHR L, 1972, IEEE T COMPUT, VC 21, P758, DOI 10.1109/T-C.1972.223579; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Witkin A., 1984, P IEEE INT C AC SPEE, V9, P150, DOI DOI 10.1109/ICASSP.1984.1172729; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yang F, 2019, IEEE I CONF COMP VIS, P8310, DOI 10.1109/ICCV.2019.00840; Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999; Zhai Y, 2018, PROC CVPR IEEE, P4139, DOI 10.1109/CVPR.2018.00435; Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644; Zhou X., 2019, ARXIV; Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953; Zhu YS, 2017, IEEE I CONF COMP VIS, P4146, DOI 10.1109/ICCV.2017.444	76	2	2	8	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3749	3766		10.1109/TPAMI.2021.3058945	http://dx.doi.org/10.1109/TPAMI.2021.3058945			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33577449	Green Submitted			2022-12-18	WOS:000805820500030
J	Wang, YX; Zhang, X; Shen, YR; Du, BW; Zhao, GR; Lizhen, LCC; Wen, HK				Wang, Yanxiang; Zhang, Xian; Shen, Yiran; Du, Bowen; Zhao, Guangrong; Lizhen, Lizhen Cui Cui; Wen, Hongkai			Event-Stream Representation for Human Gaits Identification Using Deep Neural Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Sensors; Voltage control; Feature extraction; Cameras; Task analysis; Gait recognition; Convolution; Gait recognition; dynamic vision sensors; graph-based convolutional networks	RECOGNITION; IMAGE	Dynamic vision sensors (event cameras) have recently been introduced to solve a number of different vision tasks such as object recognition, activities recognition, tracking, etc. Compared with the traditional RGB sensors, the event cameras have many unique advantages such as ultra low resources consumption, high temporal resolution and much larger dynamic range. However, these cameras only produce noisy and asynchronous events of intensity changes, i.e., event-streams rather than frames, where conventional computer vision algorithms can't be directly applied. In our opinion the key challenge for improving the performance of event cameras in vision tasks is finding the appropriate representations of the event-streams so that cutting-edge learning approaches can be applied to fully uncover the spatio-temporal information contained in the event-streams. In this paper, we focus on the event-based human gait identification task and investigate the possible representations of the event-streams when deep neural networks are applied as the classifier. We propose new event-based gait recognition approaches basing on two different representations of the event-stream, i.e., graph and image-like representations, and use graph-based convolutional network (GCN) and convolutional neural networks (CNN) respectively to recognize gait from the event-streams. The two approaches are termed as EV-Gait-3DGraph and EV-Gait-IMG. To evaluate the performance of the proposed approaches, we collect two event-based gait datasets, one from real-world experiments and the other by converting the publicly available RGB gait recognition benchmark CASIA-B. Extensive experiments show that EV-Gait-3DGraph achieves significantly higher recognition accuracy than other competing methods when sufficient training samples are available. However, EV-Gait-IMG converges more quickly than graph-based approaches while training and shows good accuracy with only few number of training samples (less than ten). So image-like presentation is preferable when the amount of training data is limited.	[Wang, Yanxiang; Shen, Yiran; Lizhen, Lizhen Cui Cui] Shandong Univ, Sch Software & C Fair, Jinan 250100, Shandong, Peoples R China; [Zhang, Xian; Zhao, Guangrong] Harbin Engn Univ, Coll Comp Sci & Technol, Harbin 150001, Peoples R China; [Du, Bowen; Wen, Hongkai] Univ Warwick, Dept Comp Sci, Coventry CV4 7AL, W Midlands, England	Shandong University; Harbin Engineering University; University of Warwick	Shen, YR (corresponding author), Shandong Univ, Sch Software & C Fair, Jinan 250100, Shandong, Peoples R China.	fancyswift@outlook.com; zhangxian@hrbeu.edu.cn; yiran.shen@sdu.edu.cn; b.du@dcs.warwick.ac.uk; 379745905@hrbeu.edu.cn; clz@sdu.edu.cn; hongkai.wen@dcs.warwick.ac.uk	Wen, Hongkai/GLN-4621-2022	Wen, Hongkai/0000-0003-1159-090X	National Natural Science Foundation of China [61702133, 91846205]; National Key RD Program [2017YFB1400100]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key RD Program	This work was supported in part by National Natural Science Foundation of China under Grants 61702133 and 91846205 and the National Key R&D Program under Grant 2017YFB1400100. The authors would like to thank NVIDIA for GPU donations.	Alotaibi M, 2017, COMPUT VIS IMAGE UND, V164, P103, DOI 10.1016/j.cviu.2017.10.004; Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781; [Anonymous], 2019, POINTMATCHER LIB TUT; [Anonymous], 2021, PYTORCH GEOMETRIC DO; [Anonymous], 2020, INTEL BOARD; Atwood J, 2016, C WORKSH NEUR INF PR, P1993; Bashir K., 2009, 3 INT C IMAGING CRIM, P1, DOI DOI 10.1049/IC.2009.0230; Berner Raphael, 2013, 2013 Symposium on VLSI Circuits, pC186; Bi Y, 2020, IEEE T IMAGE PROCESS, V29, P9084, DOI 10.1109/TIP.2020.3023597; Bi Y, 2019, IEEE I CONF COMP VIS, P491, DOI 10.1109/ICCV.2019.00058; Bruna J., 2014, PROC INT C LEARN REP; Conradt J, 2009, IEEE INT SYMP CIRC S, P781, DOI 10.1109/ISCAS.2009.5117867; Defferrard M, 2016, ADV NEUR IN, V29; Delbruck T, 2015, IEEE INT SYMP CIRC S, P2409, DOI 10.1109/ISCAS.2015.7169170; Delbruck T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00223; DVS, 2019, DVS128; Fey M, 2018, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2018.00097; Ge Gao, 2016, 2016 2nd International Conference on Frontiers of Signal Processing (ICFSP), P93, DOI 10.1109/ICFSP.2016.7802963; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Goffredo M, 2010, IEEE T SYST MAN CY B, V40, P997, DOI 10.1109/TSMCB.2009.2031091; Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu YH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00405; Kim H, 2016, LECT NOTES COMPUT SC, V9910, P349, DOI 10.1007/978-3-319-46466-4_21; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Kueng B, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P16, DOI 10.1109/IROS.2016.7758089; Kusakunniran W, 2012, IEEE T CIRC SYST VID, V22, P966, DOI 10.1109/TCSVT.2012.2186744; Lagorce X, 2017, IEEE T PATTERN ANAL, V39, P1346, DOI 10.1109/TPAMI.2016.2574707; Lam THW, 2011, PATTERN RECOGN, V44, P973, DOI 10.1016/j.patcog.2010.10.011; Lee KH, 2001, INT J ADV MANUF TECH, V18, P201, DOI 10.1007/s001700170075; Lee L, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P155, DOI 10.1109/AFGR.2002.1004148; Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337; Liu SC, 2010, CURR OPIN NEUROBIOL, V20, P288, DOI 10.1016/j.conb.2010.03.007; Liu ZY, 2004, INT C PATT RECOG, P211, DOI 10.1109/ICPR.2004.1333741; Micheli A, 2009, IEEE T NEURAL NETWOR, V20, P498, DOI 10.1109/TNN.2008.2010350; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Mueggler E, 2015, 2015 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR); Nair V., 2010, P 27 INT C MACHINE L, P807, DOI DOI 10.5555/3104322.3104425; Park PKJ, 2015, IEEE IMAGE PROC, P932, DOI 10.1109/ICIP.2015.7350936; Paszke A, 2019, ADV NEUR IN, V32; Posch C, 2011, IEEE J SOLID-ST CIRC, V46, P259, DOI 10.1109/JSSC.2010.2085952; Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556; Rebecq H., 2017, P BRIT MACH VIS C, P1, DOI DOI 10.5244/C.31.16; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810; Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178; Shiraga K, 2016, INT CONF BIOMETR; Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192; Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11; Sironi A, 2018, PROC CVPR IEEE, P1731, DOI 10.1109/CVPR.2018.00186; Tao DC, 2007, IEEE T PATTERN ANAL, V29, P1700, DOI 10.1109/TPAMI.2007.1096; Wang L, 2003, IEEE T PATTERN ANAL, V25, P1505, DOI 10.1109/TPAMI.2003.1251144; Wang L, 2003, IEEE T IMAGE PROCESS, V12, P1120, DOI 10.1109/TIP.2003.815251; Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933; Wang Y., 2020, ARXIV 200613164; Wang YX, 2019, PROC CVPR IEEE, P6351, DOI 10.1109/CVPR.2019.00652; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Wolf T, 2016, IEEE IMAGE PROC, P4165, DOI 10.1109/ICIP.2016.7533144; Wu ZF, 2017, IEEE T PATTERN ANAL, V39, P209, DOI 10.1109/TPAMI.2016.2545669; Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386; Yu SQ, 2006, INT C PATT RECOG, P441; Zhang L, 2017, IEEE INT CONF COMP V, P3120, DOI 10.1109/ICCVW.2017.369; Zhu A. Z., 2018, ARXIV 180206898	65	2	2	21	21	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3436	3449		10.1109/TPAMI.2021.3054886	http://dx.doi.org/10.1109/TPAMI.2021.3054886			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33502972	Green Accepted			2022-12-18	WOS:000805820500009
J	Zhang, YQ; Cheung, YM				Zhang, Yiqun; Cheung, Yiu-ming			Learnable Weighting of Intra-Attribute Distances for Categorical Data Clustering with Nominal and Ordinal Attributes	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Clustering algorithms; Weight measurement; Measurement; Loss measurement; Encoding; Task analysis; Partitioning algorithms; Categorical data clustering; nominal-and-ordinal attribute; intra-attribute distance; learnable weighting	K-MEANS ALGORITHM; SIMILARITY	The success of categorical data clustering generally much relies on the distance metric that measures the dissimilarity degree between two objects. However, most of the existing clustering methods treat the two categorical subtypes, i.e., nominal and ordinal attributes, in the same way when calculating the dissimilarity without considering the relative order information of the ordinal values. Moreover, there would exist interdependence among the nominal and ordinal attributes, which is worth exploring for indicating the dissimilarity. This paper will therefore study the intrinsic difference and connection of nominal and ordinal attribute values from a perspective akin to the graph. Accordingly, we propose a novel distance metric to measure the intra-attribute distances of nominal and ordinal attributes in a unified way, meanwhile preserving the order relationship among ordinal values. Subsequently, we propose a new clustering algorithm to make the learning of intra-attribute distance weights and partitions of data objects into a single learning paradigm rather than two separate steps, whereby circumventing a suboptimal solution. Experiments show the efficacy of the proposed algorithm in comparison with the existing counterparts.	[Zhang, Yiqun] Guangdong Univ Technol, Sch Comp, Guangzhou 510080, Peoples R China; [Zhang, Yiqun; Cheung, Yiu-ming] Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Peoples R China	Guangdong University of Technology; Hong Kong Baptist University	Cheung, YM (corresponding author), Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Peoples R China.	yqzhang@gdut.edu.cn; ymc@comp.hkbu.edu.hk	Zhang, YiQun/HCI-2427-2022		National Natural Science Foundation of China [61672444]; Hong Kong Baptist University (HKBU), Research Committee, Initiation Grant Faculty Niche Research Areas (IG-FNRA) [RC-FNRA-IG/18-19/SCI/03]; Innovation and Technology Fund of Innovation and Technology Commission of the Government of the Hong Kong SAR [ITS/339/18]; HKBU Interdisciplinary Research Clusters Matching Scheme (IRCMS) [RC-IRCMs/18-19/01]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Hong Kong Baptist University (HKBU), Research Committee, Initiation Grant Faculty Niche Research Areas (IG-FNRA); Innovation and Technology Fund of Innovation and Technology Commission of the Government of the Hong Kong SAR; HKBU Interdisciplinary Research Clusters Matching Scheme (IRCMS)	This work was supported in part by the National Natural Science Foundation of China under Grant 61672444, in part by Hong Kong Baptist University (HKBU), Research Committee, Initiation Grant Faculty Niche Research Areas (IG-FNRA) 2018/19, under Grant RC-FNRA-IG/18-19/SCI/03, in part by the Innovation and Technology Fund of Innovation and Technology Commission of the Government of the Hong Kong SAR under Project ITS/339/18, and in part by the HKBU Interdisciplinary Research Clusters Matching Scheme (IRCMS) under Project: RC-IRCMs/18-19/01.	Agresti, 2010, ANAL ORDINAL CATEGOR, DOI [10.1002/9780470594001, DOI 10.1002/9780470594001]; Agresti A, 2003, CATEGORICAL DATA ANA; Ahmad A, 2007, PATTERN RECOGN LETT, V28, P110, DOI 10.1016/j.patrec.2006.06.006; Alamuri M, 2014, IEEE IJCNN, P1907, DOI 10.1109/IJCNN.2014.6889941; Bai L, 2011, PATTERN RECOGN, V44, P2843, DOI 10.1016/j.patcog.2011.04.024; BALL GH, 1967, BEHAV SCI, V12, P153, DOI 10.1002/bs.3830120210; Batagelj V, 2006, DATA SCI CLASSIFICAT; Boriah S., 2008, P 8 SIAM INT C DAT M, P243, DOI 10.1137/1.9781611972788.22; Chan EY, 2004, PATTERN RECOGN, V37, P943, DOI 10.1016/j.patcog.2003.11.003; Cheung YM, 2013, PATTERN RECOGN, V46, P2228, DOI 10.1016/j.patcog.2013.01.027; Demsar J, 2006, J MACH LEARN RES, V7, P1; dos Santos TRL, 2015, EXPERT SYST APPL, V42, P1247, DOI 10.1016/j.eswa.2014.09.012; Dua D., 2017, UCI MACHINE LEARNING; Estevez PA, 2009, IEEE T NEURAL NETWOR, V20, P189, DOI 10.1109/TNN.2008.2005601; Garro V, 2016, IEEE T PATTERN ANAL, V38, P1258, DOI 10.1109/TPAMI.2015.2477823; Gates AJ, 2017, J MACH LEARN RES, V18; Gentile C, 2014, PR MACH LEARN RES, V32, P757; He X., 2005, P ADV NEUR INF PROC, P507; Hu QH, 2012, IEEE T FUZZY SYST, V20, P69, DOI 10.1109/TFUZZ.2011.2167235; Huang JZX, 2005, IEEE T PATTERN ANAL, V27, P657, DOI 10.1109/TPAMI.2005.95; Huang ZX, 1998, DATA MIN KNOWL DISC, V2, P283, DOI 10.1023/A:1009769707641; Huang ZX, 1997, P 1 PAC AS C KNOWL D, P21; Ienco D, 2012, ACM T KNOWL DISCOV D, V6, DOI 10.1145/2133360.2133361; Ienco D, 2009, LECT NOTES COMPUT SC, V5772, P83, DOI 10.1007/978-3-642-03915-7_8; Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011; Jia H, 2018, IEEE T NEUR NET LEAR, V29, P3308, DOI 10.1109/TNNLS.2017.2728138; Jia H, 2016, IEEE T NEUR NET LEAR, V27, P1065, DOI 10.1109/TNNLS.2015.2436432; Jian SL, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1937; Jian SL, 2019, IEEE T KNOWL DATA EN, V31, P853, DOI 10.1109/TKDE.2018.2848902; Jian SL, 2018, IEEE T KNOWL DATA EN, V30, P1810, DOI 10.1109/TKDE.2018.2808532; Jing L, 2007, IEEE T KNOWL DATA EN, V19, P1026, DOI 10.1109/TKDE.2007.1048; Johnson VE., 2006, ORDINAL DATA MODELIN; Kullback S, 1997, INFORM THEORY STAT; Le SQ, 2005, PATTERN RECOGN LETT, V26, P2549, DOI 10.1016/j.patrec.2005.06.002; Li S., 2015, ARXIV 151003164; Li S, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P539, DOI 10.1145/2911451.2911548; Li Shuai, 2016, ARXIV160500596; Li T, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL CONFERENCE ON MANAGEMENT SCIENCE & ENGINEERING, VOLS 1 AND 2, P531; Lin D., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P296; Lovasz L., 1986, MATCHING THEORY; MacKay D. J. C., 2003, INFORM THEORY INFERE, P468; Mahadik K., 2020, P 34 ACM INT C SUP, P1; MITCHELL TOM M., 1997, MACH LEARN, P2; Nascimento MCV, 2011, EUR J OPER RES, V211, P221, DOI 10.1016/j.ejor.2010.08.012; Nock R, 2006, IEEE T PATTERN ANAL, V28, P1223, DOI 10.1109/TPAMI.2006.168; Qian YH, 2016, IEEE T NEUR NET LEAR, V27, P2047, DOI 10.1109/TNNLS.2015.2451151; RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Sandler R, 2011, IEEE T PATTERN ANAL, V33, P1590, DOI 10.1109/TPAMI.2011.18; Santos JM, 2009, LECT NOTES COMPUT SC, V5769, P175, DOI 10.1007/978-3-642-04277-5_18; Schaeffer SE, 2007, COMPUT SCI REV, V1, P27, DOI 10.1016/j.cosrev.2007.05.001; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Witten IH, 2011, MOR KAUF D, P1; Xu R, 2005, IEEE T NEURAL NETWOR, V16, P645, DOI 10.1109/TNN.2005.845141; Yeung DS, 2002, IEEE T PATTERN ANAL, V24, P556, DOI 10.1109/34.993562; Zhang MJ, 2020, IEEE T NEUR NET LEAR, V31, P2623, DOI 10.1109/TNNLS.2019.2933590; Zhang YQ, 2022, IEEE T CYBERNETICS, V52, P758, DOI 10.1109/TCYB.2020.2983073; Zhang YQ, 2020, AAAI CONF ARTIF INTE, V34, P6869; Zhang YQ, 2020, IEEE T NEUR NET LEAR, V31, P39, DOI 10.1109/TNNLS.2019.2899381; Zhang YQ, 2018, LECT NOTES COMPUT SC, V11177, P247, DOI 10.1007/978-3-030-01851-1_24; Zhu CZ, 2022, IEEE T PATTERN ANAL, V44, P533, DOI 10.1109/TPAMI.2020.3010953	63	2	2	5	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3560	3576		10.1109/TPAMI.2021.3056510	http://dx.doi.org/10.1109/TPAMI.2021.3056510			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33534702				2022-12-18	WOS:000805820500017
J	Gao, SQ; Zhuang, XH				Gao, Shangqi; Zhuang, Xiahai			Rank-One Network: An Effective Framework for Image Restoration	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image restoration; Image reconstruction; Image denoising; Neural networks; Task analysis; Matrix decomposition; Noise reduction; Image restoration; rank one; super resolution; neural network	MATRIX COMPLETION; SPARSE	The principal rank-one (RO) components of an image represent the self-similarity of the image, which is an important property for image restoration. However, the RO components of a corrupted image could be decimated by the procedure of image denoising. We suggest that the RO property should be utilized and the decimation should be avoided in image restoration. To achieve this, we propose a new framework comprised of two modules, i.e., the RO decomposition and RO reconstruction. The RO decomposition is developed to decompose a corrupted image into the RO components and residual. This is achieved by successively applying RO projections to the image or its residuals to extract the RO components. The RO projections, based on neural networks, extract the closest RO component of an image. The RO reconstruction is aimed to reconstruct the important information, respectively from the RO components and residual, as well as to restore the image from this reconstructed information. Experimental results on four tasks, i.e., noise-free image super-resolution (SR), realistic image SR, gray-scale image denoising, and color image denoising, show that the method is effective and efficient for image restoration, and it delivers superior performance for realistic image SR and color image denoising. Our source code is available online.	[Gao, Shangqi; Zhuang, Xiahai] Fudan Univ, Sch Data Sci, Shanghai 200433, Peoples R China	Fudan University	Zhuang, XH (corresponding author), Fudan Univ, Sch Data Sci, Shanghai 200433, Peoples R China.	181109080005@fudan.edu.cn; zxh@fudan.edu.cn	Gao, Shangqi/AAD-2280-2021	Gao, Shangqi/0000-0003-4567-1636	National Natural Science Foundation of China [61971142, 62011540404]; development fund for Shanghai talents [2020015]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); development fund for Shanghai talents	This work was supported by the National Natural Science Foundation of China under Grants (61971142 and 62011540404) and the development fund for Shanghai talents (no. 2020015). The authors would like to thank Lei Li, Fuping Wu, Dengqiang Jia, Hangqi Zhou, and Xinzhe Luo for useful comments and proof read of the manuscript. Our source code is available at https://zmiclab.github.io/projects.html	Amit Y., 2007, ICML 07 P 24 INT C M, P17, DOI DOI 10.1145/1273496.1273499; ANDREWS HC, 1976, IEEE T ACOUST SPEECH, V24, P26, DOI 10.1109/TASSP.1976.1162766; Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135; Bioucas-Dias JM, 2007, IEEE T IMAGE PROCESS, V16, P2992, DOI 10.1109/TIP.2007.909319; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Celebi M. E., 2014, ADV LOW LEVEL COLOR; Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793; Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dong WS, 2019, IEEE T PATTERN ANAL, V41, P2305, DOI 10.1109/TPAMI.2018.2873610; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Evgeniou A., 2007, ADV NEURAL INF PROCE, V19, P41, DOI DOI 10.2139/SSRN.1031158; Franzen Rich, 1999, KODAK LOSSLESS TRUE; Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747; Glorot X., 2010, PROC MACH LEARN RES, P249; Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; He XY, 2019, PROC CVPR IEEE, P1732, DOI 10.1109/CVPR.2019.00183; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; Hu Y, 2013, IEEE T PATTERN ANAL, V35, P2117, DOI 10.1109/TPAMI.2012.271; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jan J, 2006, MED IMAGE PROCESSING; Jensen J.R., 2015, INTRO DIGITAL IMAGE, V4th; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Jolliffe I, 1986, J AM STAT ASSOC, DOI [10.1007/978-1-4757-1904-8_8, DOI 10.2307/1270093]; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kingma D.P., 2015, INT C LEARN REPR ICL; Komodakis N., 2006, IEEE COMPUTER SOC C, V1, P442, DOI DOI 10.1109/CVPR.2006.141; Koren Y., 2008, P 14 ACM SIGKDD INT, P426, DOI DOI 10.1145/1401890.1401944; Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618; Lebrun M, 2015, IMAGE PROCESS ON LIN, V5, P1, DOI 10.5201/ipol.2015.125; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Liu D, 2018, ADV NEUR IN, V31; Lu CY, 2020, IEEE T PATTERN ANAL, V42, P925, DOI 10.1109/TPAMI.2019.2891760; Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Milanfar P, 2013, IEEE SIGNAL PROC MAG, V30, P106, DOI 10.1109/MSP.2011.2179329; Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412; Plotz Tobias, 2018, ADV NEURAL INFORM PR, V31, P1087; Roth S, 2005, PROC CVPR IEEE, P860; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481; Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349; Shi F, 2015, IEEE T MED IMAGING, V34, P2459, DOI 10.1109/TMI.2015.2437894; Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677; Tikhnove A., 1986, SOLUTIONS ILL POSED; Timofte R, 2018, IEEE COMPUT SOC CONF, P965, DOI 10.1109/CVPRW.2018.00130; Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8; Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Yu J., 2018, ARXIV 180808718; Zeyde Roman, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47; Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zhang L, 2011, J ELECTRON IMAGING, V20, DOI 10.1117/1.3600632; Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262	67	2	2	17	25	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2022	44	6					3224	3238		10.1109/TPAMI.2020.3046476	http://dx.doi.org/10.1109/TPAMI.2020.3046476			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1R1DD	33351749	Green Submitted			2022-12-18	WOS:000803117500032
J	Wang, ZW; Lu, JW; Wu, ZY; Zhou, J				Wang, Ziwei; Lu, Jiwen; Wu, Ziyi; Zhou, Jie			Learning Efficient Binarized Object Detectors With Information Compression	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Detectors; Object detection; Redundancy; Complexity theory; Feature extraction; Quantization (signal); Mutual information; Binary neural networks; object detection; information bottleneck; automatic information compression; sparse priors	NEURAL-NETWORK	In this paper, we propose a binarized neural network learning method (BiDet) for efficient object detection. Conventional network binarization methods directly quantize the weights and activations in one-stage or two-stage detectors with constrained representational capacity, so that the information redundancy in the networks causes numerous false positives and degrades the performance significantly. On the contrary, our BiDet fully utilizes the representational capacity of the binary neural networks by redundancy removal, through which the detection precision is enhanced with alleviated false positives. Specifically, we generalize the information bottleneck (IB) principle to object detection, where the amount of information in the high-level feature maps is constrained and the mutual information between the feature maps and object detection is maximized. Meanwhile, we learn sparse object priors so that the posteriors are concentrated on informative detection prediction with false positive elimination. Since BiDet employs a fixed IB trade-off to balance the total and relative information contained in the high-level feature maps, the information compression leads to ineffective utilization of the network capacity or insufficient redundancy removal for input in different complexity. To address this, we further present binary neural networks with automatic information compression (AutoBiDet) to automatically adjust the IB trade-off for each input according to the complexity. Moreover, we further propose the class-aware sparse object priors by assigning different sparsity to objects in various classes, so that the false positives are alleviated more effectively without recall decrease. Extensive experiments on the PASCAL VOC and COCO datasets show that our BiDet and AutoBiDet outperform the state-of-the-art binarized object detectors by a sizable margin.	[Wang, Ziwei; Lu, Jiwen; Wu, Ziyi] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Dept Automat, Beijing 100084, Peoples R China; [Zhou, Jie] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Dept Automat, Beijing 100084, Peoples R China; [Zhou, Jie] Tsinghua Univ, Tsinghua Shenzhen Int, Grad Sch, Shenzhen 518055, Peoples R China	Tsinghua University; Tsinghua University; Tsinghua University; University Town of Shenzhen; Tsinghua Shenzhen International Graduate School	Lu, JW (corresponding author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Dept Automat, Beijing 100084, Peoples R China.	wang-zw18@mails.tsinghua.edu.cn; lujiwen@tsinghua.edu.cn; wuzy17@mails.tsinghua.edu.cn; jzhou@tsinghua.edu.cn	; Lu, Jiwen/C-5291-2009	Wang, Ziwei/0000-0001-9225-8495; Lu, Jiwen/0000-0002-6121-5529; Wu, Ziyi/0000-0002-8247-5872	National Key Research and Development Program of China [2017YFA0700802]; National Natural Science Foundation of China [61822603, U1813218, U1713214]; Beijing Academy of Artificial Intelligence (BAAI); Institute for Guo Qiang, Tsinghua University; Shenzhen Fundamental Research Fund (Subject Arrangement) [JCYJ20170412170602564]	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Academy of Artificial Intelligence (BAAI); Institute for Guo Qiang, Tsinghua University; Shenzhen Fundamental Research Fund (Subject Arrangement)	This work was supported in part by the National Key Research and Development Program of China under Grant 2017YFA0700802, in part by the National Natural Science Foundation of China under Grant 61822603, Grant U1813218, and Grant U1713214, in part by Beijing Academy of Artificial Intelligence (BAAI), in part by a grant from the Institute for Guo Qiang, Tsinghua University, and in part by in part by the Shenzhen Fundamental Research Fund (Subject Arrangement) under Grant JCYJ20170412170602564. Part of thiswork was presented in [56]. Code: https://github.com/ZiweiWangTHU/BiDet.git.	Ba J., 2017, P 3 INT C LEARN REPR; Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314; Brock A., 2018, ARXIV 180911096; Chen GB, 2017, ADV NEUR IN, V30; Courbariaux M, 2015, ADV NEUR IN, V28; Dai B, 2018, PR MACH LEARN RES, V80; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Gong RH, 2019, IEEE I CONF COMP VIS, P4851, DOI 10.1109/ICCV.2019.00495; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Hinton G, 1994, ADV NEURAL INFORM PR, V6, DOI DOI 10.1021/jp906511z; Howard A.G., 2017, MOBILENETS EFFICIENT; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Hubara I, 2016, ADV NEUR IN, V29; Kim H, 2019, PROC CVPR IEEE, P12561, DOI 10.1109/CVPR.2019.01285; Kingma D. P., 2013, AUTO ENCODING VARIAT; [李凡杰 Li Fanjie], 2016, [低温工程, Cryogenics], P1; Li RD, 2019, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR.2019.00292; Li Zeming, 2017, ARXIV171107264; Lin SH, 2019, IEEE T PATTERN ANAL, V41, P2889, DOI 10.1109/TPAMI.2018.2873305; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu H, 2019, PROCEEDINGS OF THE THIRD INTERNATIONAL SYMPOSIUM - EDUCATIONAL RESEARCH AND EDUCATIONAL TECHNOLOGY, 2019, P3; Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; Miyato T., 2018, INT C LEARN REPR; Molchanov P, 2019, PROC CVPR IEEE, P11256, DOI 10.1109/CVPR.2019.01152; Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479; Nguyen C. V., 2020, ARXIV 200212462; Pan X., 2018, LECT NOTES COMPUT SC, P267, DOI DOI 10.1007/978-3-030-01237-3_17; Peng B, 2018, LECT NOTES COMPUT SC, V11212, P307, DOI 10.1007/978-3-030-01237-3_19; Qin Z, 2019, IEEE I CONF COMP VIS, P6717, DOI 10.1109/ICCV.2019.00682; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shen YM, 2019, IEEE INT CONF COMP V, P2883, DOI 10.1109/ICCVW.2019.00350; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293; Tishby N., 2000, ARXIV PHYSICS0004057; Tran AT, 2019, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2019.00148; Ullrich Karen, 2017, ICLR; Wang J, 2019, AAAI CONF ARTIF INTE, P1190; Wang RJ, 2018, ADV NEUR IN, V31; Wang ZW, 2020, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR42600.2020.00212; Wang ZW, 2019, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.2019.00066; Welling M, 2017, P 31 C NEUR INF PROC, P3288; Wu TL, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21100924; Wu Tongshuang, 2020, P 2020 CHI C HUMAN F, P1; Yang JW, 2019, PROC CVPR IEEE, P7300, DOI 10.1109/CVPR.2019.00748; Zhang H, 2019, PR MACH LEARN RES, V97; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhao CL, 2019, PROC CVPR IEEE, P2775, DOI 10.1109/CVPR.2019.00289; Zhou K, 2016, DESTECH TRANS COMP; Zhou S, 2016, ARXIV 160606160	66	2	2	4	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2022	44	6					3082	3095		10.1109/TPAMI.2021.3050464	http://dx.doi.org/10.1109/TPAMI.2021.3050464			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1R1DD	33428567				2022-12-18	WOS:000803117500022
J	Wen, YX; Lin, JH; Chen, K; Chen, CLP; Jia, K				Wen, Yuxin; Lin, Jiehong; Chen, Ke; Chen, C. L. Philip; Jia, Kui			Geometry-Aware Generation of Adversarial Point Clouds	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Adversarial example; point cloud; object surface geometry	SHAPE	Machine learning models have been shown to be vulnerable to adversarial examples. While most of the existing methods for adversarial attack and defense work on the 2D image domain, a few recent attempts have been made to extend them to 3D point cloud data. However, adversarial results obtained by these methods typically contain point outliers, which are both noticeable and easy to defend against using the simple techniques of outlier removal. Motivated by the different mechanisms by which humans perceive 2D images and 3D shapes, in this paper we propose the new design of geometry-aware objectives, whose solutions favor (the discrete versions of) the desired surface properties of smoothness and fairness. To generate adversarial point clouds, we use a targeted attack misclassification loss that supports continuous pursuit of increasingly malicious signals. Regularizing the targeted attack loss with our proposed geometry-aware objectives results in our proposed method, Geometry-Aware Adversarial Attack (GeoA(3)). The results of GeoA(3) tend to be more harmful, arguably harder to defend against, and of the key adversarial characterization of being imperceptible to humans. While the main focus of this paper is to learn to generate adversarial point clouds, we also present a simple but effective algorithm termed Geo(+)A(3)-IterNormPro, with Iterative Normal Projection (IterNorPro) that solves a new objective function Geo(+)A(3), towards surface-level adversarial attacks via generation of adversarial point clouds. We quantitatively evaluate our methods on both synthetic and physical objects in terms of attack success rate and geometric regularity. For a qualitative evaluation, we conduct subjective studies by collecting human preferences from Amazon Mechanical Turk. Comparative results in comprehensive experiments confirm the advantages of our proposed methods. Our source codes are publicly available at https://github.com/Yuxin-Wen/GeoA3.	[Wen, Yuxin; Lin, Jiehong; Chen, Ke; Jia, Kui] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Peoples R China; [Wen, Yuxin; Lin, Jiehong; Chen, Ke; Jia, Kui] Pazhou Lab, Guangzhou 510335, Peoples R China; [Wen, Yuxin; Lin, Jiehong; Chen, Ke; Jia, Kui] Peng Cheng Lab, Shenzhen 518005, Peoples R China; [Chen, C. L. Philip] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Peoples R China; [Chen, C. L. Philip] Pazhou Lab, Guangzhou 510335, Peoples R China	South China University of Technology; Pazhou Lab; Peng Cheng Laboratory; South China University of Technology; Pazhou Lab	Jia, K (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Peoples R China.; Jia, K (corresponding author), Pazhou Lab, Guangzhou 510335, Peoples R China.; Jia, K (corresponding author), Peng Cheng Lab, Shenzhen 518005, Peoples R China.	wen.yuxin@mail.scut.edu.cn; lin.jiehong@mail.scut.edu.cn; chenk@scut.edu.cn; philip.chen@ieee.org; kuijia@scut.edu.cn	WEN, YUXIN/AAH-3106-2021	WEN, YUXIN/0000-0003-3719-9001; Lin, Jiehong/0000-0001-7495-6070	National Natural Science Foundation of China [61771201, 61902131]; Program for Guangdong Introducing Innovative and Entrepreneurial Teams [2017ZT07X183]; Fundamental Research Funds for the Central Universities [2019MS022]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Program for Guangdong Introducing Innovative and Entrepreneurial Teams; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	This work was supported in part by the National Natural Science Foundation of China (Grants 61771201 and 61902131), the Program for Guangdong Introducing Innovative and Entrepreneurial Teams (Grant 2017ZT07X183), and the Fundamental Research Funds for the Central Universities (Grant 2019MS022).	Ba J., 2017, P 3 INT C LEARN REPR; Botsch Mario, 2010, POLYGON MESH PROCESS, P3; Cao Y, 2019, ARXIV 190705418; Cao YL, 2019, PROCEEDINGS OF THE 2019 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'19), P2267, DOI 10.1145/3319535.3339815; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dev K., 2016, ARXIV160804953; Erler Philipp, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P108, DOI 10.1007/978-3-030-58558-7_7; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Gambino M, 2013, DO OUR BRAINS FIND C; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Ilyas A, 2019, ADV NEUR IN, V32; Koenderink J., 1996, PERCEPTION, V15, P5; Kurakin A, 2018, ICLR, P99, DOI DOI 10.1201/9781351251389-8; Lappin JS, 2011, ATTEN PERCEPT PSYCHO, V73, P2353, DOI 10.3758/s13414-011-0197-4; Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730; Li S, 2021, IEEE T PATTERN ANAL, V43, P1352, DOI 10.1109/TPAMI.2019.2948352; Liu D., 2019, ARXIV 190806062; Liu D, 2019, IEEE IMAGE PROC, P2279, DOI 10.1109/ICIP.2019.8803770; Madry A., 2018, ARXIV PREPRINT ARXIV; Miura K. T., 2014, INT J AUTOMATION TEC, V8, P304, DOI [10.20965/ijat.2014.p0304, DOI 10.20965/IJAT.2014.P0304]; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Norman JF, 2008, ACTA PSYCHOL, V129, P198, DOI 10.1016/j.actpsy.2008.06.002; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Qi CR, 2017, ADV NEUR IN, V30; Rozsa A, 2016, IEEE COMPUT SOC CONF, P410, DOI 10.1109/CVPRW.2016.58; Schmidt L, 2018, ADV NEUR IN, V31; Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392; Su D, 2018, LECT NOTES COMPUT SC, V11216, P644, DOI 10.1007/978-3-030-01258-8_39; Su JW, 2019, IEEE T EVOLUT COMPUT, V23, P828, DOI 10.1109/TEVC.2019.2890858; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tang JP, 2019, PROC CVPR IEEE, P4536, DOI 10.1109/CVPR.2019.00467; Tang LL, 2022, IEEE T CYBERNETICS, V52, P4949, DOI 10.1109/TCYB.2020.3025798; Tsai T, 2020, AAAI CONF ARTIF INTE, V34, P954; Wang KQ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3726; Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Wicker M, 2019, PROC CVPR IEEE, P11759, DOI 10.1109/CVPR.2019.01204; Wiyatno RR, 2019, IEEE I CONF COMP VIS, P4821, DOI 10.1109/ICCV.2019.00492; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Xiang C, 2019, PROC CVPR IEEE, P9128, DOI 10.1109/CVPR.2019.00935; Xiang Y, 2014, IEEE WINT CONF APPL, P75, DOI 10.1109/WACV.2014.6836101; Yang J., 2019, ARXIV 190210899; Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029; Zhang HY, 2019, PR MACH LEARN RES, V97; Zheng TH, 2019, IEEE I CONF COMP VIS, P1598, DOI 10.1109/ICCV.2019.00168; Zhou H, 2019, IEEE I CONF COMP VIS, P1961, DOI 10.1109/ICCV.2019.00205	53	2	2	4	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2022	44	6					2984	2999		10.1109/TPAMI.2020.3044712	http://dx.doi.org/10.1109/TPAMI.2020.3044712			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1R1DD	33320808	Green Submitted			2022-12-18	WOS:000803117500016
J	Gu, XW; Angelov, PP; Zhang, C; Atkinson, PM				Gu, Xiaowei; Angelov, Plamen P.; Zhang, Ce; Atkinson, Peter M.			A Semi-Supervised Deep Rule-Based Approach for Complex Satellite Sensor Image Analysis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Satellites; Image segmentation; Feature extraction; Semantics; Semisupervised learning; Mathematical model; Prototypes; deep rule-based system; deep learning; satellite sensor image analysis; semi-supervised learning	CONVOLUTIONAL NEURAL-NETWORKS; SEMISUPERVISED CLASSIFICATION; SCENE CLASSIFICATION	Large-scale (large-area), fine spatial resolution satellite sensor images are valuable data sources for Earth observation while not yet fully exploited by research communities for practical applications. Often, such images exhibit highly complex geometrical structures and spatial patterns, and distinctive characteristics of multiple land-use categories may appear at the same region. Autonomous information extraction from these images is essential in the field of pattern recognition within remote sensing, but this task is extremely challenging due to the spectral and spatial complexity captured in satellite sensor imagery. In this research, a semi-supervised deep rule-based approach for satellite sensor image analysis (SeRBIA) is proposed, where large-scale satellite sensor images are analysed autonomously and classified into detailed land-use categories. Using an ensemble feature descriptor derived from pre-trained AlexNet and VGG-VD-16 models, SeRBIA is capable of learning continuously from both labelled and unlabelled images through self-adaptation without human involvement or intervention. Extensive numerical experiments were conducted on both benchmark datasets and real-world satellite sensor images to comprehensively test the validity and effectiveness of the proposed method. The novel information mining technique developed here can be applied to analyse large-scale satellite sensor images with high accuracy and interpretability, across a wide range of real-world applications.	[Gu, Xiaowei] Aberystwyth Univ, Dept Comp Sci, Aberystwyth SY23 3DB, Dyfed, Wales; [Angelov, Plamen P.] Univ Lancaster, Sch Comp & Commun, Lancaster LA1 4WA, England; [Zhang, Ce; Atkinson, Peter M.] Univ Lancaster, Lancaster Environm Ctr, Lancaster LA1 4YQ, England; [Zhang, Ce] UK Ctr Ecol & Hydrol, Lancaster LA1 4AP, England	Aberystwyth University; Lancaster University; Lancaster University; UK Centre for Ecology & Hydrology (UKCEH)	Gu, XW (corresponding author), Aberystwyth Univ, Dept Comp Sci, Aberystwyth SY23 3DB, Dyfed, Wales.	xig4@aber.ac.uk; p.angelov@lancaster.ac.uk; c.zhang9@lancaster.ac.uk; pma@lancaster.ac.uk	Angelov, Plamen/AAE-8284-2019	Angelov, Plamen/0000-0002-5770-934X; Gu, Xiaowei/0000-0001-9116-4761				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Anwer RM, 2018, ISPRS J PHOTOGRAMM, V138, P74, DOI 10.1016/j.isprsjprs.2018.01.023; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Bian XY, 2017, IEEE J-STARS, V10, P2889, DOI 10.1109/JSTARS.2017.2683799; Bruzzone L, 2006, IEEE T GEOSCI REMOTE, V44, P3363, DOI 10.1109/TGRS.2006.877950; Chaib S, 2017, IEEE T GEOSCI REMOTE, V55, P4775, DOI 10.1109/TGRS.2017.2700322; Cheng G, 2017, P IEEE, V105, P1865, DOI 10.1109/JPROC.2017.2675998; Cheng G, 2014, INT GEOSCI REMOTE SE, P2479, DOI 10.1109/IGARSS.2014.6946975; Cheriyadat AM, 2014, IEEE T GEOSCI REMOTE, V52, P439, DOI 10.1109/TGRS.2013.2241444; Cristianini N., 2000, INTRO SUPPORT VECTOR; Cunningham P., 2007, MULTIPLE CLASSIFIER, P1, DOI [10.1145/3459665, DOI 10.1145/3459665]; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dopido I, 2013, IEEE T GEOSCI REMOTE, V51, P4032, DOI 10.1109/TGRS.2012.2228275; dos Santos JA, 2010, VISAPP 2010: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 2, P203; Gan JR, 2016, IEEE GEOSCI REMOTE S, V13, P1626, DOI 10.1109/LGRS.2016.2598567; Girshick R., IEEE T PATTERN ANAL, V38, P142; Gu X., 2019, P INT JOINT C NEUR N, P17; Gu XW, 2018, IEEE SYS MAN CYBERN, P2778, DOI 10.1109/SMC.2018.00474; Gu XW, 2018, APPL SOFT COMPUT, V68, P53, DOI 10.1016/j.asoc.2018.03.032; Gu XW, 2018, IEEE GEOSCI REMOTE S, V15, P345, DOI 10.1109/LGRS.2017.2787421; Guo YD, 2019, IEEE T PATTERN ANAL, V41, P1294, DOI 10.1109/TPAMI.2018.2837742; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hu F, 2015, REMOTE SENS-BASEL, V7, P14680, DOI 10.3390/rs71114680; Kothari NS, 2020, IEEE J-STARS, V13, P329, DOI 10.1109/JSTARS.2019.2961985; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Liu Wei, 2010, ICML; Lowe D.G., 2004, IJCV, V60, DOI [10.1023/B:VISI.0000029664.99615.94, DOI 10.1023/B:VISI.0000029664.99615.94]; Newsam, 2010, P 18 SIGSPATIAL INT, P2, DOI DOI 10.1145/1869790.1869829; Pratama M., IEEE T FUZZY SYST, V26, P2552; Scott GJ, 2017, IEEE GEOSCI REMOTE S, V14, P549, DOI 10.1109/LGRS.2017.2657778; Wang J, 2013, J MACH LEARN RES, V14, P771; Wang ZM, 2017, IEEE T GEOSCI REMOTE, V55, P3071, DOI 10.1109/TGRS.2017.2650938; Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929; Xia GS, 2017, IEEE T GEOSCI REMOTE, V55, P3965, DOI 10.1109/TGRS.2017.2685945; Xia GS, 2010, INT ARCH PHOTOGRAMM, V38, P298; Xiang SM, 2010, IEEE T PATTERN ANAL, V32, P2039, DOI 10.1109/TPAMI.2010.35; Yin JH, 2015, IEEE J-STARS, V8, P23, DOI 10.1109/JSTARS.2014.2375066; Yu YL, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/8639367; Zhang C, 2019, REMOTE SENS ENVIRON, V221, P173, DOI 10.1016/j.rse.2018.11.014; Zhang LP, 2016, IEEE GEOSC REM SEN M, V4, P22, DOI 10.1109/MGRS.2016.2540798; Zhang W., 2019, REMOTE SENS-BASEL, V11; Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749; Zhou DY, 2004, ADV NEUR IN, V16, P321; Zou Q, 2015, IEEE GEOSCI REMOTE S, V12, P2321, DOI 10.1109/LGRS.2015.2475299	46	2	2	3	17	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2022	44	5					2281	2292		10.1109/TPAMI.2020.3048268	http://dx.doi.org/10.1109/TPAMI.2020.3048268			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1C1XU	33378259	Green Submitted, Green Accepted			2022-12-18	WOS:000792921400007
J	Zheng, T; Zhang, GQ; Han, L; Xu, L; Fang, L				Zheng, Tian; Zhang, Guoqing; Han, Lei; Xu, Lan; Fang, Lu			BuildingFusion: Semantic-Aware Structural Building-Scale 3D Reconstruction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image reconstruction; Three-dimensional displays; Semantics; Liquid crystal displays; Collaboration; Two dimensional displays; Geometry; 3D reconstruction; collaborative reconstruction; semantic understanding; loop closure detection; scene structure	LOOP-CLOSURE DETECTION; REAL-TIME	Scalable geometry reconstruction and understanding is an important yet unsolved task. Current methods often suffer from false loop closures when there are similar-looking rooms in the scene, and often lack online scene understanding. We propose BuildingFusion, a semantic-aware structural building-scale reconstruction system, which not only allows building-scale dense reconstruction collaboratively, but also provides semantic and structural information on-the-fly. Technically, the robustness to similar places is enabled by a novel semantic-aware room-level loop closure detection(LCD) method. The insight lies in that even though local views may look similar in different rooms, the objects inside and their locations are usually different, implying that the semantic information forms a unique and compact representation for place recognition. To achieve that, a 3D convolutional network is used to learn instance-level embeddings for similarity measurement and candidate selection, followed by a graph matching module for geometry verification. On the system side, we adopt a centralized architecture to enable collaborative scanning. Each agent reconstructs a part of the scene, and the combination is activated when the overlaps are found using room-level LCD, which is performed on the server. Extensive comparisons demonstrate the superiority of the semantic-aware room-level LCD over traditional image-based LCD. Live demo on the real-world building-scale scenes shows the feasibility of our method with robust, collaborative, and real-time performance.	[Zheng, Tian; Zhang, Guoqing; Han, Lei] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China; [Zheng, Tian; Zhang, Guoqing; Han, Lei] Beijing Natl Res Ctr Informat Sci & Technol, Beijing 100084, Peoples R China; [Zheng, Tian; Zhang, Guoqing] Tsinghua Univ, Tsinghua Berkeley Shenzhen Inst, Shenzhen 518000, Peoples R China; [Xu, Lan] HiSilicon Technol CO LTD, Shanghai 200000, Peoples R China; [Fang, Lu] ShanghaiTech Univ, Shanghai 200000, Peoples R China	Tsinghua University; Tsinghua University; ShanghaiTech University	Fang, L (corresponding author), ShanghaiTech Univ, Shanghai 200000, Peoples R China.	zhengt19@mails.tsinghua.edu.cn; zhanggq18@mails.tsinghua.edu.cn; lhanaf@connect.ust.hk; lxuan@connect.ust.hk; fanglu@tsinghua.edu.cn		Xu, Lan/0000-0002-8807-7787	Natural Science Foundation of China (NSFC) [61722209, 61860206003]; Shenzhen Science and Technology Research and Development Funds [JCYJ20180507183706645, ZDYBH201900000002]	Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Shenzhen Science and Technology Research and Development Funds	This work was supported in part by the Natural Science Foundation of China (NSFC) under contract Nos. 61722209 and 61860206003, in part by the Shenzhen Science and Technology Research and Development Funds (JCYJ20180507183706645 and ZDYBH201900000002). This work was conducted at Tsinghua University. Tian Zheng, Guoqing Zhang, and Lei Han contributed equally to this work.	Ahmed S., 2012, Proceedings of the 10th IAPR International Workshop on Document Analysis Systems (DAS 2012), P339, DOI 10.1109/DAS.2012.22; Angeli A, 2008, IEEE T ROBOT, V24, P1027, DOI 10.1109/TRO.2008.2004514; Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170; Cavallari T, 2017, PROC CVPR IEEE, P218, DOI 10.1109/CVPR.2017.31; Chauve AL, 2010, PROC CVPR IEEE, P1261, DOI 10.1109/CVPR.2010.5539824; Chebrolu N., 2015, P WORKSH PLANN PERC; Choy C, 2019, IEEE I CONF COMP VIS, P8957, DOI 10.1109/ICCV.2019.00905; Dai A, 2020, PROC CVPR IEEE, P846, DOI 10.1109/CVPR42600.2020.00093; Dai A, 2018, PROC CVPR IEEE, P4578, DOI 10.1109/CVPR.2018.00481; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Dai A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3054739; Delhumeau Jonathan, 2013, ACM MM, P653, DOI DOI 10.1145/2502081.2502171; Dong SY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322942; Dube R., 2017, P IEEE INT C ROB AUT, P5266; Duchenne O, 2011, IEEE T PATTERN ANAL, V33, P2383, DOI 10.1109/TPAMI.2011.110; Endres F, 2012, IEEE INT CONF ROBOT, P1691, DOI 10.1109/ICRA.2012.6225199; Finman R., 2015, P INT C ROB AUT WORK, V76; Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158; Golodetz S, 2018, IEEE T VIS COMPUT GR, V24, P2895, DOI 10.1109/TVCG.2018.2868533; Graham B., 2017, ABS170601307 CORR, V1706; Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961; Han L., 2018, DIVERSITY SPATIAL DI; Han L, 2020, PROC CVPR IEEE, P2937, DOI 10.1109/CVPR42600.2020.00301; Han L, 2020, IEEE T VIS COMPUT GR, V26, P2012, DOI 10.1109/TVCG.2020.2973477; Han L, 2017, IEEE INT CON MULTI, P139, DOI 10.1109/ICME.2017.8019479; Handa A, 2014, IEEE INT CONF ROBOT, P1524, DOI 10.1109/ICRA.2014.6907054; Hermans Alexander, 2017, ARXIV170307737; Kahler O, 2016, LECT NOTES COMPUT SC, V9912, P500, DOI 10.1007/978-3-319-46484-8_30; Khan S, 2015, IEEE INT CONF ROBOT, P5441, DOI 10.1109/ICRA.2015.7139959; Kim UH, 2020, IEEE T CYBERNETICS, V50, P4921, DOI 10.1109/TCYB.2019.2931042; Klingensmith M., 2015, ROBOTICS SCI SYSTEMS; Labbe M, 2013, IEEE T ROBOT, V29, P734, DOI 10.1109/TRO.2013.2242375; Li YK, 2017, IEEE I CONF COMP VIS, P1270, DOI 10.1109/ICCV.2017.142; LIN J, 2019, ARXIV190311351V2; McCormac John, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4628, DOI 10.1109/ICRA.2017.7989538; Mohanarajah G, 2015, IEEE T AUTOM SCI ENG, V12, P423, DOI 10.1109/TASE.2015.2408456; Mur-Artal R, 2014, IEEE INT CONF ROBOT, P846, DOI 10.1109/ICRA.2014.6906953; Narita G, 2019, IEEE INT C INT ROBOT, P4205, DOI 10.1109/IROS40897.2019.8967890; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Nie YY, 2020, PROC CVPR IEEE, P52, DOI 10.1109/CVPR42600.2020.00013; Poiesi F, 2017, 14TH EUROPEAN CONFERENCE ON VISUAL MEDIA PRODUCTION (CVMP), DOI 10.1145/3150165.3150166; Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112; Riazuelo L, 2014, ROBOT AUTON SYST, V62, P401, DOI 10.1016/j.robot.2013.11.007; Rosinol A., 2020, ABS200206289; Rosinol A, 2020, IEEE INT CONF ROBOT, P1689, DOI 10.1109/ICRA40945.2020.9196885; Rusu RB, 2009, IEEE INT CONF ROBOT, P1848; Shahbazi H, 2011, IEEE INT C INT ROBOT, P1228, DOI 10.1109/IROS.2011.6048862; Shotton J, 2013, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2013.377; Steinbrucker F, 2013, IEEE I CONF COMP VIS, P3264, DOI 10.1109/ICCV.2013.405; Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773; Uy MA, 2018, PROC CVPR IEEE, P4470, DOI 10.1109/CVPR.2018.00470; Valentin J, 2015, PROC CVPR IEEE, P4400, DOI 10.1109/CVPR.2015.7299069; Wald J, 2020, PROC CVPR IEEE, P3960, DOI 10.1109/CVPR42600.2020.00402; Wald J, 2018, IEEE ROBOT AUTOM LET, V3, P3402, DOI 10.1109/LRA.2018.2852782; Whelan T., 2016, INT J ROB RES, V35, P1697, DOI [10.1177/0278364916669237, DOI 10.1177/0278364916669237]; Wijmans E., 2017, CVPR, P308; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330; Zhou Y, 2019, IEEE I CONF COMP VIS, P7383, DOI 10.1109/ICCV.2019.00748	60	2	2	4	20	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2022	44	5					2328	2345		10.1109/TPAMI.2020.3042881	http://dx.doi.org/10.1109/TPAMI.2020.3042881			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1C1XU	33290210				2022-12-18	WOS:000792921400010
J	Chandramouli, P; Gandikota, KV; Gorlitz, A; Kolb, A; Moeller, M				Chandramouli, Paramanand; Gandikota, Kanchana Vaishnavi; Gorlitz, Andreas; Kolb, Andreas; Moeller, Michael			A Generative Model for Generic Light Field Reconstruction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image reconstruction; Task analysis; Spatial resolution; Data models; Feature extraction; Apertures; Training; Light field reconstruction; deep generative model; view synthesis; coded aperture		Recently deep generative models have achieved impressive progress in modeling the distribution of training data. In this work, we present for the first time a generative model for 4D light field patches using variational autoencoders to capture the data distribution of light field patches. We develop a generative model conditioned on the central view of the light field and incorporate this as a prior in an energy minimization framework to address diverse light field reconstruction tasks. While pure learning-based approaches do achieve excellent results on each instance of such a problem, their applicability is limited to the specific observation model they have been trained on. On the contrary, our trained light field generative model can be incorporated as a prior into any model-based optimization approach and therefore extend to diverse reconstruction tasks including light field view synthesis, spatial-angular super resolution and reconstruction from coded projections. Our proposed method demonstrates good reconstruction, with performance approaching end-to-end trained networks, while outperforming traditional model-based approaches on both synthetic and real scenes. Furthermore, we show that our approach enables reliable light field recovery despite distortions in the input.	[Chandramouli, Paramanand; Gandikota, Kanchana Vaishnavi; Gorlitz, Andreas; Kolb, Andreas; Moeller, Michael] Univ Siegen, Dept Comp Sci, D-57076 Siegen, Germany	Universitat Siegen	Gandikota, KV (corresponding author), Univ Siegen, Dept Comp Sci, D-57076 Siegen, Germany.	paramanand.chandramouli@uni-siegen.de; kanchana.gandikota@uni-siegen.de; andreas.goerlitz@uni-siegen.de; andreas.kolb@uni-siegen.de; michael.moeller@uni-siegen.de	Gandikota, Kanchana Vaishnavi/AAE-1562-2022; Kolb, Andreas/A-2067-2012	Gandikota, Kanchana Vaishnavi/0000-0003-0292-7523; Moeller, Michael/0000-0002-0492-6527; Kolb, Andreas/0000-0003-4753-7801	German Research Foundation (DFG) [CH 2530/1-1]	German Research Foundation (DFG)(German Research Foundation (DFG))	This work was supported in part by the German Research Foundation (DFG) under grant CH 2530/1-1 "Deep Models for Handheld Light Field Acquisition". Paramanand Chandramouli and Kanchana Vaishnavi Gandikota contributed equally to this work.	Alperovich A, 2018, PROC CVPR IEEE, P9145, DOI 10.1109/CVPR.2018.00953; [Anonymous], 2019, ARXIV190312364; [Anonymous], 2018, HEIDELBERG COLLABORA; Ashok A, 2010, PROC SPIE, V7690, DOI 10.1117/12.852738; Babacan SD, 2012, IEEE T IMAGE PROCESS, V21, P4746, DOI 10.1109/TIP.2012.2210237; Bau D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323023; Blocker CJ, 2019, IEEE INT CONF COMP V, P3933, DOI 10.1109/ICCVW.2019.00487; Bora A, 2017, PR MACH LEARN RES, V70; Brock AM, 2018, PROCEEDINGS PERVASIVE DISPLAYS 2018: THE 7TH ACM INTERNATIONAL SYMPOSIUM ON PERVASIVE DISPLAYS, DOI 10.1145/3205873.3205877; Chandramouli P, 2019, IEEE INT CONF COMP V, P3315, DOI 10.1109/ICCVW.2019.00413; Chang JHR, 2017, IEEE I CONF COMP VIS, P5889, DOI 10.1109/ICCV.2017.627; Chaurasia G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487238; Chen B, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3366371; Clark Aidan, 2019, ARXIV190706571; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gul MSK, 2018, IEEE T IMAGE PROCESS, V27, P2146, DOI 10.1109/TIP.2018.2794181; Gupta M, 2017, IEEE COMPUT SOC CONF, P1277, DOI 10.1109/CVPRW.2017.168; Heber Stefan, 2014, Computer Vision - ECCV 2014. 13th European Conference. Proceedings: LNCS 8694, P751, DOI 10.1007/978-3-319-10599-4_48; Heber S, 2016, PROC CVPR IEEE, P3746, DOI 10.1109/CVPR.2016.407; Hegde C, 2018, ANN ALLERTON CONF, P166, DOI 10.1109/ALLERTON.2018.8636031; Inagaki Y., 2018, P EUR C COMP VIS SEP, P418; Intergovernmental Panel Climate Change Working Grp III, 2014, CLIMATE CHANGE 2014: MITIGATION OF CLIMATE CHANGE, P1; Jeon HG, 2015, PROC CVPR IEEE, P1547, DOI 10.1109/CVPR.2015.7298762; Jin J, 2020, AAAI CONF ARTIF INTE, V34, P11141; Johannsen O, 2016, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2016.355; Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kauvar I, 2015, ACM T GRAPHIC, V34, DOI [10.1145/2682631, 10.1145/2816795.2818070]; Kingma D.P, P 3 INT C LEARNING R; Kingma DP, 2 INT C LEARN REPR I, P1; Latorre F, 2019, ADV NEUR IN, V32; Liang CK, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360654; Marwah K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461914; Meng N, 2019, IEEE IMAGE PROC, P4659, DOI 10.1109/ICIP.2019.8803480; Meng N, 2021, IEEE T PATTERN ANAL, V43, P873, DOI 10.1109/TPAMI.2019.2945027; Mildenhall B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322980; Nabati O, 2018, IEEE INT CONF COMPUT; Navarro Julia, 2019, ARXIV190511271; Ng, 2005, LIGHT FIELD PHOTOGRA; Raj A., 2016, STANFORD LYTRO LIGHT; Sajjadi MSM, 2016, LECT NOTES COMPUT SC, V9796, P426, DOI 10.1007/978-3-319-45886-1_35; Schedl David C, 2015, IEEE ICCP, P1, DOI DOI 10.1109/ICCPHOT.2015.7168365; Shah V, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4609; Srinivasan PP, 2019, PROC CVPR IEEE, P175, DOI 10.1109/CVPR.2019.00026; Srinivasan PP, 2017, IEEE I CONF COMP VIS, P2262, DOI 10.1109/ICCV.2017.246; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Vagharshakyan S, 2018, IEEE T PATTERN ANAL, V40, P133, DOI 10.1109/TPAMI.2017.2653101; Veeraraghavan A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239520; Wang Y., 2018, P EUR C COMP VIS ECC; Wang YW, 2017, IEEE T VIS COMPUT GR, V23, P2357, DOI 10.1109/TVCG.2016.2628743; Wanner S, 2014, IEEE T PATTERN ANAL, V36, P606, DOI 10.1109/TPAMI.2013.147; Wu GC, 2019, IEEE T IMAGE PROCESS, V28, P3261, DOI 10.1109/TIP.2019.2895463; Wu GC, 2019, IEEE T PATTERN ANAL, V41, P1681, DOI 10.1109/TPAMI.2018.2845393; Xu SJ, 2019, INT CONF ACOUST SPEE, P2967, DOI 10.1109/ICASSP.2019.8683641; Yeung HWF, 2018, LECT NOTES COMPUT SC, V11210, P138, DOI 10.1007/978-3-030-01231-1_9	60	2	2	7	33	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2022	44	4					1712	1724		10.1109/TPAMI.2020.3039841	http://dx.doi.org/10.1109/TPAMI.2020.3039841			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	ZN1PQ	33226937	Green Submitted			2022-12-18	WOS:000764815300007
J	Mai, ST; Jacobsen, J; Amer-Yahia, S; Spence, I; Tran, NP; Assent, I; Nguyen, QVH				Mai, Son T.; Jacobsen, Jon; Amer-Yahia, Sihem; Spence, Ivor; Nhat-Phuong Tran; Assent, Ira; Quoc Viet Hung Nguyen			Incremental Density-Based Clustering on Multicore Processors	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Clustering algorithms; Multicore processing; Databases; Instruction sets; Electronic mail; Time factors; Clustering methods; Density-based clustering; anytime clustering; incremental clustering; active clustering; multicore CPUs	ALGORITHM; DBSCAN	The density-based clustering algorithm is a fundamental data clustering technique with many real-world applications. However, when the database is frequently changed, how to effectively update clustering results rather than reclustering from scratch remains a challenging task. In this work, we introduce IncAnyDBC, a unique parallel incremental data clustering approach to deal with this problem. First, IncAnyDBC can process changes in bulks rather than batches like state-of-the-art methods for reducing update overheads. Second, it keeps an underlying cluster structure called the object node graph during the clustering process and uses it as a basis for incrementally updating clusters wrt. inserted or deleted objects in the database by propagating changes around affected nodes only. In additional, IncAnyDBC actively and iteratively examines the graph and chooses only a small set of most meaningful objects to produce exact clustering results of DBSCAN or to approximate results under arbitrary time constraints. This makes it more efficient than other existing methods. Third, by processing objects in blocks, IncAnyDBC can be efficiently parallelized on multicore CPUs, thus creating a work-efficient method. It runs much faster than existing techniques using one thread while still scaling well with multiple threads. Experiments are conducted on various large real datasets for demonstrating the performance of IncAnyDBC.	[Mai, Son T.; Nhat-Phuong Tran] Queens Univ Belfast, Belfast BT7 1NN, Antrim, North Ireland; [Spence, Ivor] Queens Univ Belfast, Sch Elect Elect Engn & Comp Sci, Artificial Intelligence Res Theme, Belfast BT7 1NN, Antrim, North Ireland; [Amer-Yahia, Sihem] Univ Grenoble Alpes, F-38400 St Martin Dheres, France; [Quoc Viet Hung Nguyen] Griffith Univ, Brisbane, Qld 4222, Australia; [Jacobsen, Jon] Aarhus Univ, DK-8000 Aarhus, Denmark; [Assent, Ira] Aarhus Univ, Comp Sci, DK-8000 Aarhus, Denmark	Queens University Belfast; Queens University Belfast; Communaute Universite Grenoble Alpes; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); Griffith University; Aarhus University; Aarhus University	Nguyen, QVH (corresponding author), Griffith Univ, Brisbane, Qld 4222, Australia.	thaison.mai@qub.ac.uk; jon@cs.au.dk; sihem.amer-yahia@imag.fr; i.spence@qub.ac.uk; n.tran@qub.ac.uk; ira@cs.au.dk; quocviethung.nguyen@griffith.edu.au		Nguyen, Quoc Viet Hung/0000-0002-9687-1315; Assent, Ira/0000-0002-1091-9948; Reis, AlessanRSS/0000-0001-8486-7469; Spence, Ivor/0000-0002-0289-6790	French National Research Agency [ANR-15-IDEX-02]; Villum Postdoc Fellowship; Newton Fund [528154944]	French National Research Agency(French National Research Agency (ANR)); Villum Postdoc Fellowship; Newton Fund	The authors would like to thank anonymous reviewers for their helpful comments that significantly improve the paper. This work was supported by the French National Research Agency (ANR-15-IDEX-02), a Villum Postdoc Fellowship, and a Newton Fund (ID 528154944).	Ackerman M, 2014, ADV NEUR IN, V27; Aggarwal CC, 2014, CH CRC DATA MIN KNOW, P1; Andrade G, 2013, PROCEDIA COMPUT SCI, V18, P369, DOI 10.1016/j.procs.2013.05.200; B_ohm C., 2013, PROC SIAM INT C DATA, P112; Bhattacharjee P, 2017, LECT NOTES COMPUT SC, V10193, P568, DOI 10.1007/978-3-319-56608-5_50; Bi-Ru Dai, 2012, 2012 IEEE 5th International Conference on Cloud Computing (CLOUD), P59, DOI 10.1109/CLOUD.2012.42; Bohm C., 2009, ACM C INF KNOWL MAN, P661; Borah B, 2004, PROCEEDINGS OF INTERNATIONAL CONFERENCE ON INTELLIGENT SENSING AND INFORMATION PROCESSING, P92; Brecheisen S, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P43, DOI 10.1109/ICDM.2004.10082; Brecheisen S, 2006, LECT NOTES ARTIF INT, V3918, P179; Campello Ricardo J. G. B., 2013, Advances in Knowledge Discovery and Data Mining. 17th Pacific-Asia Conference (PAKDD 2013). Proceedings, P160, DOI 10.1007/978-3-642-37456-2_14; Campello RJGB, 2015, ACM T KNOWL DISCOV D, V10, DOI 10.1145/2733381; Cao F, 2006, SIAM PROC S, P328, DOI 10.1137/1.9781611972764.29; Charikar M., 1997, P 29 ANN ACM S THEOR, P626; Dash M, 2001, SEVENTH INTERNATIONAL CONFERENCE ON DATABASE SYSTEMS FOR ADVANCED APPLICATIONS, PROCEEDINGS, P32, DOI 10.1109/DASFAA.2001.916361; De Lucia G, 2007, MON NOT R ASTRON SOC, V375, P2, DOI 10.1111/j.1365-2966.2006.11287.x; Ester M., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P323; Ester M., 1996, P 2 INT C KNOWL DISC, P226; Frank A, 2019, UCI MACHINE LEARNING; Gan JH, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1493, DOI 10.1145/3035918.3064050; Gan JH, 2017, ACM T DATABASE SYST, V42, DOI 10.1145/3083897; Gan JH, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P519, DOI 10.1145/2723372.2737792; Gotz M., 2015, WORKSHOP MACHINE LEA, P2, DOI DOI 10.1145/2834892.2834894; Gunawan A., 2013, FASTER ALGORITHM DBS; Han DW, 2018, IEEE INT CONF BIG DA, P305, DOI 10.1109/BigData.2018.8622258; Han J, 2012, MOR KAUF D, P1; He YB, 2011, INT C PAR DISTRIB SY, P473, DOI 10.1109/ICPADS.2011.83; Hu X, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2107, DOI 10.1145/3132847.3133112; Januzaj E, 2004, LECT NOTES ARTIF INT, V3202, P231; Kriegel HP, 2017, KNOWL INF SYST, V52, P341, DOI 10.1007/s10115-016-1004-2; Lee J.G., 2007, P 2007 ACM SIGMOD IN, DOI [DOI 10.1145/1247480.1247546, 10.1145/1247480.1247546]; Lin J, 2004, LECT NOTES COMPUT SC, V2992, P106; Lulli A, 2016, PROC VLDB ENDOW, V10, P157; Mai ST, 2019, IEEE T KNOWL DATA EN, V31, P1239, DOI 10.1109/TKDE.2018.2828086; Mai ST, 2018, DATA MIN KNOWL DISC, V32, P1121, DOI 10.1007/s10618-018-0562-1; Mai ST, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1025, DOI 10.1145/2939672.2939750; Mai ST, 2017, PROC INT CONF DATA, P349, DOI 10.1109/ICDE.2017.94; Mai ST, 2015, KNOWL INF SYST, V45, P319, DOI 10.1007/s10115-014-0797-0; Mai ST, 2012, IEEE DATA MINING, P1014, DOI 10.1109/ICDM.2012.95; Patwary MMA, 2014, INT CONF HIGH PERFOR, P560, DOI 10.1109/SC.2014.51; Patwary MMA, 2012, INT CONF HIGH PERFOR; Phung D, 2009, PERVASIVE MOB COMPUT, V5, P714, DOI 10.1016/j.pmcj.2009.07.005; Sakai T, 2017, LECT NOTES COMPUT SC, V10179, P133, DOI 10.1007/978-3-319-55705-2_10; Schubert E, 2017, ACM T DATABASE SYST, V42, DOI 10.1145/3068335; Singh S, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1533, DOI 10.1145/2505515.2507837; Song H, 2018, INT CONF MANAGE DATA, P1173, DOI 10.1145/3183713.3196887; Tan PN, 2005, INTRO DATA MINING, V1st; Tianrun Li, 2016, Advances in Databases and Information Systems. 20th East European Conference, ADBIS 2016. Proceedings: LNCS 9809, P31, DOI 10.1007/978-3-319-44039-2_3; Wan L, 2009, ACM T KNOWL DISCOV D, V3, DOI 10.1145/1552303.1552307; Wang X, 2003, LECT NOTES ARTIF INT, V2637, P563; Welton B, 2013, INT CONF HIGH PERFOR, DOI 10.1145/2503210.2503262; Xu XW, 1998, PROC INT CONF DATA, P324, DOI 10.1109/ICDE.1998.655795	52	2	2	5	18	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2022	44	3					1338	1356		10.1109/TPAMI.2020.3023125	http://dx.doi.org/10.1109/TPAMI.2020.3023125			19	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YU4MA	32915725	Green Accepted			2022-12-18	WOS:000752018000020
J	Ortega, JE; Forcada, ML; Sanchez-Martinez, F				Ortega, John E.; Forcada, Mikel L.; Sanchez-Martinez, Felipe			Fuzzy-Match Repair Guided by Quality Estimation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Fuzzy-match repair; computer-aided translation; translation memories; quality estimation	TRANSLATION	Computer-aided translation tools based on translation memories are widely used to assist professional translators. A translation memory (TM) consists of a set of translation units (TU) made up of source- and target-language segment pairs. For the translation of a new source segment s', these tools search the TM and retrieve the TUs (s, t) whose source segments are more similar to s'. The translator then chooses a TU and edit the target segment t to turn it into an adequate translation of s'. Fuzzy-match repair (FMR) techniques can be used to automatically modify the parts oft that need to be edited. We describe a language-independent FMR method that first uses machine translation to generate, given s' and (s, t), a set of candidate fuzzy-match repaired segments, and then chooses the best one by estimating their quality. An evaluation on three different language pairs shows that the selected candidate is a good approximation to the best (oracle) candidate produced and is closer to reference translations than machine-translated segments and unrepaired fuzzy matches (t). In addition, a single quality estimation model trained on a mix of data from all the languages performs well on any of the languages used.	[Ortega, John E.; Forcada, Mikel L.; Sanchez-Martinez, Felipe] Univ Alacant, Dept Llenguatges & Sistemes Informat, Alicante 03690, Spain	Universitat d'Alacant	Sanchez-Martinez, F (corresponding author), Univ Alacant, Dept Llenguatges & Sistemes Informat, Alicante 03690, Spain.	jeo10@alu.ua.es; mlf@dlsi.ua.es; fsanchez@dlsi.ua.es	Sánchez-Martínez, Felipe/G-9689-2016	Sánchez-Martínez, Felipe/0000-0002-2295-2630; Ortega, John/0000-0003-1274-5504; Ortega, John Evan/0000-0002-2328-3205	Spanish Government through the EFFORTUNE Project [TIN-2015-69632-R]	Spanish Government through the EFFORTUNE Project	This work was supported by the Spanish Government through the EFFORTUNE Project [TIN-2015-69632-R].	Avramidis E, 2013, MACH TRANSL, V27, P239, DOI 10.1007/s10590-013-9144-6; Basak D, 2007, NEURAL INFORM PROCES, V11, P203, DOI DOI 10.1007/978-1-4302-5990-9_4; Bicici E, 2008, LECT NOTES COMPUT SC, V4919, P454, DOI 10.1007/978-3-540-78135-6_39; Blatz J, 2004, COL 2004 P 20 INT C, P315; Bojar Ondrej, 2014, P 9 WORKSH STAT MACH, P12, DOI DOI 10.3115/V1/W14-3302; Bowker L., 2002, COMPUTER AIDED TRANS; Breiman L., 2019, RANDOMFOREST BREIMAN; Bulte B., 2018, P 21 ANN C EUROPEAN, P69; Bulte B, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1800; Carl M., 2003, RECENT ADV EXAMPLE B; Dapat S., 2011, P 15 ANN C EUR ASS M, P201; Espla-Gomis M., 2011, MACH TRANSL, P172; Espla-Gomis M, 2015, J ARTIF INTELL RES, V53, P169, DOI 10.1613/jair.4630; Forcada ML, 2011, MACH TRANSL, V25, P127, DOI 10.1007/s10590-011-9090-0; Fukuyama F, 2016, ANNU REV POLIT SCI, V19, P89, DOI 10.1146/annurev-polisci-042214-044240; Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1; Geurts P., 2011, JMLR WORKSHOP C PROC, V14, P49; Gu JT, 2018, AAAI CONF ARTIF INTE, P5133; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Hewavitharana S., 2005, P 10 ANN C EUR ASS M, P126; Hokamp C, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1535, DOI 10.18653/v1/P17-1141; Koehn P, 2007, 45 ANN M ASS COMP LI, P177, DOI DOI 10.3115/1557769.1557821; Koehn P., 2010, P AMTA WORKSHOP MT R, P21; Koehn Philipp, 2010, STAT MACHINE TRANSLA; Kranias L., 2004, P 4 INT C LANGUAGE R, P331; Li LY, 2016, BALT J MOD COMPUT, V4, P165; Liu Y, 2019, COMPUT SPEECH LANG, V54, P176, DOI 10.1016/j.csl.2018.09.006; Louppe G., 2013, ADV NEURAL INFORM PR, V26, P431, DOI DOI 10.5555/2999611.2999660; Marino JB, 2006, COMPUT LINGUIST, V32, P527, DOI 10.1162/coli.2006.32.4.527; Olshen R., 1984, CLASSIFICATION REGRE; Ortega J.E., 2016, P 12 BIENN C ASS MAC, VVolume 1, P27; Rao C. R, 1973, LINEAR STAT INFERENC; Simard M., 2009, P MT SUMMIT 12, P120; Specia L., 2011, P 15 C EUROPEAN ASS, P73; Specia L, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2015): SYSTEM DEMONSTRATIONS, P115; Specia Lucia, 2009, P 13 ANN C EUROPEAN, P28; Steinberger R, 2012, LREC 2012 - EIGHTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P454; Strobl C, 2008, BMC BIOINFORMATICS, V9, DOI 10.1186/1471-2105-9-307; WAGNER RA, 1974, J ACM, V21, P168, DOI 10.1145/321796.321811; ZHECHEV V, 2010, P 4 WORKSH SYNT STRU, P43	40	2	2	4	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2022	44	3					1264	1277		10.1109/TPAMI.2020.3021361	http://dx.doi.org/10.1109/TPAMI.2020.3021361			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YU4MA	32877333	Green Published			2022-12-18	WOS:000752018000015
J	Zeng, H; Li, LD; Cao, ZS; Zhang, L				Zeng, Hui; Li, Lida; Cao, Zisheng; Zhang, Lei			Grid Anchor Based Image Cropping: A New Benchmark and An Efficient Model	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Agriculture; Measurement; Databases; Robustness; Benchmark testing; Training; Image cropping; photo cropping; image aesthetics; deep learning	VISUAL-ATTENTION	Image cropping aims to improve the composition as well as aesthetic quality of an image by removing extraneous content from it. Most of the existing image cropping databases provide only one or several human-annotated bounding boxes as the groundtruths, which can hardly reflect the non-uniqueness and flexibility of image cropping in practice. The employed evaluation metrics such as intersection-over-union cannot reliably reflect the real performance of a cropping model, either. This work revisits the problem of image cropping, and presents a grid anchor based formulation by considering the special properties and requirements (e.g., local redundancy, content preservation, aspect ratio) of image cropping. Our formulation reduces the searching space of candidate crops from millions to no more than ninety. Consequently, a grid anchor based cropping benchmark is constructed, where all crops of each image are annotated and more reliable evaluation metrics are defined. To meet the practical demands of robust performance and high efficiency, we also design an effective and lightweight cropping model. By simultaneously considering the region of interest and region of discard, and leveraging multi-scale information, our model can robustly output visually pleasing crops for images of different scenes. With less than 2.5M parameters, our model runs at a speed of 200 FPS on one single GTX 1080Ti GPU and 12 FPS on one i7-6800K CPU. The code is available at: https://github.com/HuiZeng/Grid-Anchor-based-Image-Cropping-Pytorch.	[Zeng, Hui; Li, Lida; Zhang, Lei] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China; [Cao, Zisheng] DJI Innovat Co Ltd, Camera Grp, Shenzhen 518057, Peoples R China	Hong Kong Polytechnic University	Zhang, L (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.	cshzeng@comp.polyu.edu.hk; cslli@comp.polyu.edu.hk; zisheng.cao@dji.com; cslzhang@comp.polyu.edu.hk	Zhang, Hui/HHN-8494-2022	Zhang, Lei/0000-0002-2078-4215; Li, Lida/0000-0001-9386-194X; Cao, Zisheng/0000-0002-7037-8039	Hong Kong RGC RIF Grant [R5001-18]	Hong Kong RGC RIF Grant	This work was supported by the Hong Kong RGC RIF Grant (R5001-18).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Bhatt N., 2015, US Patent, Patent No. [9,158,455, 9158455]; Chedeau C. S. B., 2017, US Patent, Patent No. [9,607,235, 9607235]; Chen JS, 2016, PROC CVPR IEEE, P507, DOI 10.1109/CVPR.2016.61; Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4; Chen YL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P37, DOI 10.1145/3123266.3123274; Chen YL, 2017, IEEE WINT CONF APPL, P226, DOI 10.1109/WACV.2017.32; Cheng B., 2010, P 18 ACM INT C MULTI, P291; Chor A., 2006, US Patent App., Patent No. [10/956,628, 10956628]; Ciocca G, 2007, IEEE T CONSUM ELECTR, V53, P1622, DOI 10.1109/TCE.2007.4429261; Deng YB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P870, DOI 10.1145/3240508.3240531; Deng YB, 2017, IEEE SIGNAL PROC MAG, V34, P80, DOI 10.1109/MSP.2017.2696576; Downing E. O., 2015, US Patent, Patent No. [9,020,298, 9020298]; Fang C, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1105, DOI 10.1145/2647868.2654979; Freixenet J, 2002, LECT NOTES COMPUT SC, V2352, P408, DOI 10.1007/3-540-47977-5_27; Glorot X., 2010, PROC MACH LEARN RES, P249; Guo GJ, 2018, IEEE T MULTIMEDIA, V20, P2073, DOI 10.1109/TMM.2018.2794262; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Jogo N., 2007, US Patent, Patent No. [7,209,149, 7209149]; Kingma D.P, P 3 INT C LEARNING R; Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Li DB, 2018, PROC CVPR IEEE, P8193, DOI 10.1109/CVPR.2018.00855; Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x; Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498; Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; Marchesotti L, 2009, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2009.5459467; Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954; Nishiyama M, 2009, P 17 ACM INT C MULTI, P669; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Santella A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P771; Stentiford F., 2007, P WORKSH COMP ATT AP, P1; Suh B, 2003, P 16 ANN ACM S USER, P95, DOI [10.1145/964696.964707, DOI 10.1145/964696.964707]; Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899; Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724; Wang WG, 2017, IEEE I CONF COMP VIS, P2205, DOI 10.1109/ICCV.2017.240; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wei ZJ, 2018, PROC CVPR IEEE, P5437, DOI 10.1109/CVPR.2018.00570; Wikipedia contributors, 2019, PEARSON CORRELATION; Wikipedia contributors, 2018, CROPP IM WIK FREE EN; Wikipedia contributors, 2019, SPEARM RANK CORR COE; Wikipedia contributors, 2018, RUL THIRDS WIK FREE; Wikipedia contributors, 2019, EV INF RETR WIK FREE; Yan JZ, 2013, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2013.130; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zeng H, 2019, PROC CVPR IEEE, P5942, DOI 10.1109/CVPR.2019.00610; Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P94, DOI 10.1109/TMM.2013.2286817; Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P802, DOI 10.1109/TIP.2012.2223226; Zhang MJ, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P438	53	2	2	4	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2022	44	3					1304	1319		10.1109/TPAMI.2020.3024207	http://dx.doi.org/10.1109/TPAMI.2020.3024207			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YU4MA	32931429	Green Submitted			2022-12-18	WOS:000752018000018
J	Lee, Y; Jeong, J; Yun, J; Cho, W; Yoon, KJ				Lee, Yeonkun; Jeong, Jaeseok; Yun, Jongseob; Cho, Wonjune; Yoon, Kuk-Jin			SpherePHD: Applying CNNs on 360 degrees Images With Non-Euclidean Spherical PolyHeDron Representation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Omni-directional cameras; 360 Degree; convolutional neural network; detection network; semantic segmentation; depth estimation; icosahedron; non-euclidean deep learning		Omni-directional images are becoming more prevalent for understanding the scene of all directions around a camera, as they provide a much wider field-of-view (FoV) compared to conventional images. In this work, we present a novel approach to represent omni-directional images and suggest how to apply CNNs on the proposed image representation. The proposed image representation method utilizes a spherical polyhedron to reduce distortion introduced inevitably when sampling pixels on a non-Euclidean spherical surface around the camera center. To apply convolution operation on our representation of images, we stack the neighboring pixels on top of each pixel and multiply with trainable parameters. This approach enables us to apply the same CNN architectures used in conventional euclidean 2D images on our proposed method in a straightforward manner. Compared to the previous work, we additionally compare different designs of kernels that can be applied to our proposed method. We also show that our method outperforms in monocular depth estimation task compared to other state-of-the-art representation methods of omni-directional images. In addition, we propose a novel method to fit bounding ellipses of arbitrary orientation using object detection networks and apply it to an omni-directional real-world human detection dataset.	[Lee, Yeonkun; Jeong, Jaeseok; Yun, Jongseob; Cho, Wonjune; Yoon, Kuk-Jin] Korea Adv Inst Sci & Technol, Visual Intelligence Lab, 291 Daehak Ro, Daejeon 34141, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Yoon, KJ (corresponding author), Korea Adv Inst Sci & Technol, Visual Intelligence Lab, 291 Daehak Ro, Daejeon 34141, South Korea.	dldusrjs@kaist.ac.kr; jason.jeong@kaist.ac.kr; jseob@kaist.ac.kr; wonjune@kaist.ac.kr; kjyoon@kaist.ac.kr		Jeong, Jaeseok/0000-0002-9836-2979; yun, gugjin/0000-0002-1634-2756	Institute of Information & Communications Technology Planning & Evaluation (IITP) - Korea government (MSIT) [B0101-15-0266, 2020-0-00004]; IITP Grant by the Korea Government (MSIT) [2020-0-00440]; Next-Generation Information Computing Development Program through the National Research Foundation of Korea(NRF) - Ministry of Science, ICT [NRF-2017M3C4A7069369]	Institute of Information & Communications Technology Planning & Evaluation (IITP) - Korea government (MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); IITP Grant by the Korea Government (MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); Next-Generation Information Computing Development Program through the National Research Foundation of Korea(NRF) - Ministry of Science, ICT(National Research Foundation of KoreaMinistry of Science, ICT & Future Planning, Republic of Korea)	This work was supported by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. B0101-15-0266, Development of High Performance Visual BigData Discovery Platform for Large-Scale Realtime Data Analysis and No. 2020-0-00004), IITP Grant by the Korea Government (MSIT) (No.2020-0-00440, Development of Artificial Intelligence Technology that Continuously Improves Itself as the Situation Changes in the Real World), and Next-Generation Information Computing Development Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Science, ICT (NRF-2017M3C4A7069369). The authors would also like to thank the Computer Vision Laboratory at Hanyang University for providing the indoor 360 degrees human detection dataset. Jaeseok Jeong, Jongseob Yun, and Wonjune Cho contributed equally to this work.	Armeni I, 2017, ARXIV; Bourke P., 2017, CONVERTING EQUIRECTA; Brown C., 2017, BRINGING PIXELS FRON; Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154; Cohen Taco S, 2018, ICLR; Coors B, 2018, LECT NOTES COMPUT SC, V11213, P525, DOI 10.1007/978-3-030-01240-3_32; Cruz-Mota J, 2012, INT J COMPUT VISION, V98, P217, DOI 10.1007/s11263-011-0505-4; de La Garanderie GP, 2018, LECT NOTES COMPUT SC, V11217, P812, DOI 10.1007/978-3-030-01261-8_48; Delibasis KK, 2018, J IMAGING, V4, DOI 10.3390/jimaging4060073; Demonceaux C, 2011, IMAGE VISION COMPUT, V29, P840, DOI 10.1016/j.imavis.2011.09.007; Eigen D, 2014, ADV NEUR IN, V27; Furui S, 2012, IEEE SIGNAL PROC MAG, V29, P16, DOI 10.1109/MSP.2012.2209906; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Hansen Peter, 2007, 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems, P1689, DOI 10.1109/IROS.2007.4399266; Hu HN, 2017, PROC CVPR IEEE, P1396, DOI 10.1109/CVPR.2017.153; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lai WS, 2018, IEEE T VIS COMPUT GR, V24, P2610, DOI 10.1109/TVCG.2017.2750671; Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954]; Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283; Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152; Monroy R, 2018, SIGNAL PROCESS-IMAGE, V69, P26, DOI 10.1016/j.image.2018.05.005; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352; Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683; Snyder JP, 1992, CARTOGRAPHICA, V29, P10, DOI [10.3138/27H7-8K88-4882-1752, DOI 10.3138/27H7-8K88-4882-1752, 10.3138/27h7-8k88-4882-1752]; Su YC, 2017, ADV NEUR IN, V30; Su YC, 2017, LECT NOTES COMPUT SC, V10114, P154, DOI 10.1007/978-3-319-54190-7_10; Su YC, 2017, PROC CVPR IEEE, P1368, DOI 10.1109/CVPR.2017.150; Tairu H.M., 2015, IPSJ T COMPUT VIS AP, V7, P84, DOI 10.5897/IJWREE2015.0569; Tateno K, 2018, LECT NOTES COMPUT SC, V11220, P732, DOI 10.1007/978-3-030-01270-0_43; Vidanapathirana M., 2017, P FUT TECHN C FTC, P1036; Xiao JX, 2012, PROC CVPR IEEE, P2695, DOI 10.1109/CVPR.2012.6247991; Yang WY, 2018, INT C PATT RECOG, P2190, DOI 10.1109/ICPR.2018.8546070; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700	35	2	2	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2022	44	2					834	847		10.1109/TPAMI.2020.2997045	http://dx.doi.org/10.1109/TPAMI.2020.2997045			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YC9LS	32750773				2022-12-18	WOS:000740006100023
J	Sommer, S; Bronstein, A				Sommer, Stefan; Bronstein, Alex			Horizontal Flows and Manifold Stochastics in Geometric Deep Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Manifolds; Convolution; Machine learning; Geometry; Stochastic processes; Bridges; Neural networks; Geometric deep learning; stochastic analysis on manifolds; geometric statistics; frame bundle; curvature; bridge sampling	SIMULATION; DIFFUSION	We introduce two constructions in geometric deep learning for 1) transporting orientation-dependent convolutional filters over a manifold in a continuous way and thereby defining a convolution operator that naturally incorporates the rotational effect of holonomy; and 2) allowing efficient evaluation of manifold convolution layers by sampling manifold valued random variables that center around a weighted diffusion mean. Both methods are inspired by stochastics on manifolds and geometric statistics, and provide examples of how stochastic methods - here horizontal frame bundle flows and non-linear bridge sampling schemes, can be used in geometric deep learning. We outline the theoretical foundation of the two methods, discuss their relation to Euclidean deep networks and existing methodology in geometric deep learning, and establish important properties of the proposed constructions.	[Sommer, Stefan] Univ Copenhagen, Dept Comp Sci, DK-1165 Copenhagen, Denmark; [Bronstein, Alex] Technion Israel Inst Technol, Dept Comp Sci, IL-3200003 Haifa, Israel	University of Copenhagen; Technion Israel Institute of Technology	Sommer, S (corresponding author), Univ Copenhagen, Dept Comp Sci, DK-1165 Copenhagen, Denmark.	sommer@di.ku.dk; alexbronst@gmail.com	Sommer, Stefan/G-3138-2013	Sommer, Stefan/0000-0001-6784-0328	CSGB Centre for Stochastic Geometry and Advanced Bioimaging - Villum foundation; Villum Foundation [00022924]; Novo Nordisk Foundation [NNF18OC0052000]	CSGB Centre for Stochastic Geometry and Advanced Bioimaging - Villum foundation; Villum Foundation(Villum Fonden); Novo Nordisk Foundation(Novo Nordisk FoundationNovocure Limited)	The work presented in this article was supported by the CSGB Centre for Stochastic Geometry and Advanced Bioimaging funded by a Grant from the Villum foundation, the Villum Foundation Grant 00022924, and the Novo Nordisk Foundation Grant NNF18OC0052000.	Atwood J., 2016, ADV NEURAL INFORM PR, P1993, DOI DOI 10.5555/3157096.3157320; Boscaini Davide, 2016, P 30 INT C NEUR INF, P2; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Bruna J., 2014, C TRACK P; Chakraborty R., 2018, ARXIV180906211; Chakraborty R, 2019, LECT NOTES COMPUT SC, V11492, P112, DOI 10.1007/978-3-030-20351-1_9; Cheng M. C. N., 2019, ARXIV190602481; Cohen TS, 2019, PR MACH LEARN RES, V97; Cohen TS, 2016, PR MACH LEARN RES, V48; Delyon B, 2006, STOCH PROC APPL, V116, P1660, DOI 10.1016/j.spa.2006.04.004; Elworthy K.D., 1988, LECT NOTES MATH, P277; EMERY M, 1989, STOCHASTIC CALCULUS; Frechet M., 1948, ANN I H POINCARE, V10, P215; Gal Y, 2016, PR MACH LEARN RES, V48; Goh A, 2011, NEUROIMAGE, V56, P1181, DOI 10.1016/j.neuroimage.2011.01.053; Hsu E. P., 2002, GRAD STUD MATH, V38; Kolar I., 1993, NATURAL OPERATIONS D; Kondor R, 2018, PR MACH LEARN RES, V80; Kuhnel L, 2019, APPL MATH COMPUT, V356, P411, DOI 10.1016/j.amc.2019.03.044; Lim Y, 2014, LINEAR ALGEBRA APPL, V453, P59, DOI 10.1016/j.laa.2014.04.002; Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112; Montgomery R., 2006, TOUR SUBRIEMANNIAN G; Monti F., P IEEE C COMP VIS PA; Pennec X, 2006, J MATH IMAGING VIS, V25, P127, DOI 10.1007/s10851-006-6228-4; Poulenard A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275102; Said S, 2012, IEEE T INFORM THEORY, V58, P3521, DOI 10.1109/TIT.2012.2185680; Schauer M, 2017, BERNOULLI, V23, P2917, DOI 10.3150/16-BEJ833; Schonsheck Stefan C, 2018, ARXIV180507857; Sharp N, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3243651; Sommer S, 2017, LECT NOTES COMPUT SC, V10551, P79, DOI 10.1007/978-3-319-67675-3_8; Sommer S, 2017, J GEOM MECH, V9, P391, DOI 10.3934/jgm.2017015; Sommer Stefan, 2015, Inf Process Med Imaging, V24, P193, DOI 10.1007/978-3-319-19992-4_15; Sommer S, 2010, LECT NOTES COMPUT SC, V6316, P43, DOI 10.1007/978-3-642-15567-3_4; Thompson J., 2016, BROWNIAN BRIDGES SUB, DOI [10.1007/s11118-017-9667-1, DOI 10.1007/S11118-017-9667-1]	34	2	2	1	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2022	44	2					811	822		10.1109/TPAMI.2020.2994507	http://dx.doi.org/10.1109/TPAMI.2020.2994507			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YC9LS	32406826	Green Submitted			2022-12-18	WOS:000740006100021
J	Wang, JD; Tu, ZW; Fu, JL; Sebe, N; Belongie, S				Wang, Jingdong; Tu, Zhuowen; Fu, Jianlong; Sebe, Nicu; Belongie, Serge			Guest Editorial: Introduction to the Special Section on Fine-Grained Visual Categorization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									[Wang, Jingdong; Fu, Jianlong] Microsoft Res Asia, Beijing 100080, Peoples R China; [Tu, Zhuowen] Univ Calif San Diego, Dept Cognit Sci, San Diego, CA 92093 USA; [Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci DISI, I-38123 Trento, Italy; [Belongie, Serge] Cornell Univ, Dept Comp Sci, Ithaca, NY 14850 USA	Microsoft; Microsoft Research Asia; University of California System; University of California San Diego; University of Trento; Cornell University	Wang, JD (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.	jingdw@microsoft.com; ztu@ucsd.edu; jianf@microsoft.com; niculae.sebe@unitn.it; sjb344@cornell.edu		Belongie, Serge/0000-0002-0388-5217					0	2	2	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2022	44	2					560	562		10.1109/TPAMI.2021.3065094	http://dx.doi.org/10.1109/TPAMI.2021.3065094			3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YC9LS		Bronze			2022-12-18	WOS:000740006100002
J	Wang, QZ; Wan, J; Chan, AB				Wang, Qingzhong; Wan, Jia; Chan, Antoni B.			On Diversity in Image Captioning: Metrics and Methods	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Measurement; Semantics; Learning (artificial intelligence); Vegetation; Legged locomotion; Training; Computational modeling; Image captioning; diverse captions; reinforcement learning; policy gradient; adversarial training; diversity metric		Diversity is one of the most important properties in image captioning, as it reflects various expressions of important concepts presented in an image. However, the most popular metrics cannot well evaluate the diversity of multiple captions. In this paper, we first propose a metric to measure the diversity of a set of captions, which is derived from latent semantic analysis (LSA), and then kernelize LSA using CIDEr (R. Vedantam et al., 2015) similarity. Compared with mBLEU (R. Shetty et al., 2017), our proposed diversity metrics show a relatively strong correlation to human evaluation. We conduct extensive experiments, finding there is a large gap between the performance of the current state-of-the-art models and human annotations considering both diversity and accuracy; the models that aim to generate captions with higher CIDEr scores normally obtain lower diversity scores, which generally learn to describe images using common words. To bridge this "diversity" gap, we consider several methods for training caption models to generate diverse captions. First, we show that balancing the cross-entropy loss and CIDEr reward in reinforcement learning during training can effectively control the tradeoff between diversity and accuracy of the generated captions. Second, we develop approaches that directly optimize our diversity metric and CIDEr score using reinforcement learning. These proposed approaches using reinforcement learning (RL) can be unified into a self-critical (S. J. Rennie et al., 2017) framework with new RL baselines. Third, we combine accuracy and diversity into a single measure using an ensemble matrix, and then maximize the determinant of the ensemble matrix via reinforcement learning to boost diversity and accuracy, which outperforms its counterparts on the oracle test. Finally, inspired by determinantal point processes (DPP), we develop a DPP selection algorithm to select a subset of captions from a large number of candidate captions. The experimental results show that maximizing the determinant of the ensemble matrix outperforms other methods considerably improving diversity and accuracy.	[Wang, Qingzhong; Wan, Jia; Chan, Antoni B.] City Univ Hong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China		Chan, AB (corresponding author), City Univ Hong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.	qingzwang2-c@my.cityu.edu.hk; jiawan1998@gmail.com; abchan@cityu.edu.hk	; CHAN, Antoni B./D-7858-2013	Wang, Qingzhong/0000-0003-1562-8098; Wan, Jia/0000-0001-8198-1629; CHAN, Antoni B./0000-0002-2886-2513	City University of Hong Kong [7004682, 7005218]; NVIDIA Corporation	City University of Hong Kong(City University of Hong Kong); NVIDIA Corporation	This work was supported by the Strategic Research Grant from the City University of Hong Kong (Project NO. 7004682 and 7005218). The authors are grateful for the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research.	Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636; Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Aneja J, 2018, PROC CVPR IEEE, P5561, DOI 10.1109/CVPR.2018.00583; Chan AB, 2018, ARXIV PREPRINT ARXIV; Chen FH, 2018, PROC CVPR IEEE, P1345, DOI 10.1109/CVPR.2018.00146; Chen LM, 2018, ADV NEUR IN, V31; Cornia M, 2019, PROC CVPR IEEE, P8299, DOI 10.1109/CVPR.2019.00850; Cui Y, 2018, PROC CVPR IEEE, P5804, DOI 10.1109/CVPR.2018.00608; Dai B, 2017, ADV NEUR IN, V30; Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323; Dauphin YN, 2017, PR MACH LEARN RES, V70; DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9; Denkowski Michael, 2014, P 9 WORKSH STAT MACH, P376, DOI DOI 10.3115/V1/W14-3348; Deshpande A, 2019, PROC CVPR IEEE, P10687, DOI 10.1109/CVPR.2019.01095; Dognin P, 2019, PROC CVPR IEEE, P10455, DOI 10.1109/CVPR.2019.01071; Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754; Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2; Gan Z, 2017, PROC CVPR IEEE, P1141, DOI 10.1109/CVPR.2017.127; Gehring J, 2017, PR MACH LEARN RES, V70; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goutte C, 2005, LECT NOTES COMPUT SC, V3408, P345; Gu JX, 2017, IEEE I CONF COMP VIS, P1231, DOI 10.1109/ICCV.2017.138; Guo T., 2018, P 2018 C EMP METH NA, P751; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hossain MZ, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3295748; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; Jingna Mao, 2015, 2015 IEEE Biomedical Circuits and Systems Conference (BioCAS), P1, DOI 10.1109/BioCAS.2015.7348279; Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kilickaya M, 2017, 15TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2017), VOL 1: LONG PAPERS, P199; Kingma D.P., 2015, INT C LEARN REPR, P1; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kulesza Alex, 2011, P C UNC ART INT UAI; Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162; Kusner MJ, 2015, PR MACH LEARN RES, V37, P957; Li D., 2018, ARXIV180400861; Li S., 2011, P 15 C COMPUTATIONAL, P220; Lin C.-Y., 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.3115/V1/D14-1020; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu SQ, 2017, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2017.100; Liu XH, 2018, LECT NOTES COMPUT SC, V11219, P353, DOI 10.1007/978-3-030-01267-0_21; Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345; Luo RT, 2018, PROC CVPR IEEE, P6964, DOI 10.1109/CVPR.2018.00728; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Papadopoulo T, 2000, LECT NOTES COMPUT SC, V1842, P554; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Park CC, 2019, IEEE T PATTERN ANAL, V41, P999, DOI 10.1109/TPAMI.2018.2824816; Pedersoli M, 2017, IEEE I CONF COMP VIS, P1251, DOI 10.1109/ICCV.2017.140; Ren Z, 2017, PROC CVPR IEEE, P1151, DOI 10.1109/CVPR.2017.128; Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Schuster Sebastian, 2015, P 4 WORKSH VIS LANG, P70, DOI DOI 10.18653/V1/W15-2812; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Shetty R, 2017, IEEE I CONF COMP VIS, P4155, DOI 10.1109/ICCV.2017.445; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tan Y. H., 2016, P AS C COMP VIS, P101; Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wang LW, 2017, ADV NEUR IN, V30; Wang QZ, 2019, PROC CVPR IEEE, P4190, DOI 10.1109/CVPR.2019.00432; Wang Qingzhong, 2018, P AS C COMP VIS, P21; Wang WX, 2019, AAAI CONF ARTIF INTE, P8957; Wang Z., 2016, P 25 INT JOINT C ART, P2957; Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang Z., 2016, ADV NEURAL INF PROCE, V29; Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Zheng Y, 2019, PROC CVPR IEEE, P8387, DOI 10.1109/CVPR.2019.00859	72	2	2	5	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2022	44	2					1035	1049		10.1109/TPAMI.2020.3013834	http://dx.doi.org/10.1109/TPAMI.2020.3013834			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YC9LS	32749960				2022-12-18	WOS:000740006100035
J	Della Maggiora, G; Castillo-Passi, C; Qiu, WQ; Liu, S; Milovic, C; Sekino, M; Tejos, C; Uribe, S; Irarrazaval, P				Della Maggiora, Gabriel; Castillo-Passi, Carlos; Qiu, Wenqi; Liu, Shuang; Milovic, Carlos; Sekino, Masaki; Tejos, Cristian; Uribe, Sergio; Irarrazaval, Pablo			DeepSPIO: Super Paramagnetic Iron Oxide Particle Quantification Using Deep Learning in Magnetic Resonance Imaging	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Machine learning; deep learning; neural networks; MRI; quantification; susceptibility; QSM; SPIO	SUSCEPTIBILITY; INHOMOGENEITY; INVERSION; MAP	The susceptibility of super paramagnetic iron oxide (SPIO) particles makes them a useful contrast agent for different purposes in MRI. These particles are typically quantified with relaxometry or by measuring the inhomogeneities they produced. These methods rely on the phase, which is unreliable for high concentrations. We present in this study a novel Deep Learning method to quantify the SPIO concentration distribution. We acquired the data with a new sequence called View Line in which the field map information is encoded in the geometry of the image. The novelty of our network is that it uses residual blocks as the bottleneck and multiple decoders to improve the gradient flow in the network. Each decoder predicts a different part of the wavelet decomposition of the concentration map. This decomposition improves the estimation of the concentration, and also it accelerates the convergence of the model. We tested our SPIO concentration reconstruction technique with simulated images and data from actual scans from phantoms. The simulations were done using images from the IXI dataset, and the phantoms consisted of plastic cylinders containing agar with SPIO particles at different concentrations. In both experiments, the model was able to quantify the distribution accurately.	[Della Maggiora, Gabriel; Castillo-Passi, Carlos; Milovic, Carlos; Tejos, Cristian; Uribe, Sergio; Irarrazaval, Pablo] Pontificia Univ Catolica Chile, Dept Elect Engn, Santiago 7820436, Chile; [Della Maggiora, Gabriel; Castillo-Passi, Carlos; Milovic, Carlos; Tejos, Cristian; Uribe, Sergio; Irarrazaval, Pablo] Pontificia Univ Catolica Chile, Biomed Imaging Ctr, Santiago 7820436, Chile; [Della Maggiora, Gabriel; Castillo-Passi, Carlos; Milovic, Carlos; Tejos, Cristian; Uribe, Sergio; Irarrazaval, Pablo] Millennium Nucleus Cardiovasc Magnet Resonance, Santiago, Chile; [Castillo-Passi, Carlos; Uribe, Sergio; Irarrazaval, Pablo] Pontificia Univ Catolica Chile, Inst Biol & Med Engn, Santiago 7820436, Chile; [Qiu, Wenqi; Liu, Shuang; Sekino, Masaki] Univ Tokyo, Sch Engn, Dept Bioengn, Tokyo 1138654, Japan	Pontificia Universidad Catolica de Chile; Pontificia Universidad Catolica de Chile; Pontificia Universidad Catolica de Chile; University of Tokyo	Della Maggiora, G (corresponding author), Pontificia Univ Catolica Chile, Dept Elect Engn, Santiago 7820436, Chile.; Della Maggiora, G (corresponding author), Pontificia Univ Catolica Chile, Biomed Imaging Ctr, Santiago 7820436, Chile.	gedellamaggiora@uc.cl; cncastillo@uc.cl; qiu@bee.t.u-tokyo.ac.jp; liu@bee.t.u-tokyo.ac.jp; cmilovic@uc.cl; sekino@bee.t.u-tokyo.ac.jp; ctejos@uc.cl; suribe@uc.cl; pim@uc.cl	Milovic, Carlos/AFJ-9094-2022; URIBE Arancibia, SERGIO/GRR-3235-2022; Milovic, Carlos/AAE-2561-2022	Milovic, Carlos/0000-0002-1196-6703; Milovic, Carlos/0000-0002-1196-6703; Tejos, Cristian/0000-0002-8367-155X; Irarrazaval, Pablo/0000-0002-5186-2642; della Maggiora, Gabriel/0000-0002-6320-5105; Castillo-Passi, Carlos/0000-0001-6227-0477; Sekino, Masaki/0000-0002-3387-1932	ANID [PIA-Anillo ACT1416, ACT192064, Fondecyt 1191710]; Millenium Science Initiative of the Ministry of Economy, Development, and Tourism, grantNucleus for CardiovascularMagnetic Resonance	ANID; Millenium Science Initiative of the Ministry of Economy, Development, and Tourism, grantNucleus for CardiovascularMagnetic Resonance	The authors would like to gratefully acknowledge ANID for funding this research through PIA-Anillo ACT1416, ACT192064, Fondecyt 1191710, and theMillenium Science Initiative of the Ministry of Economy, Development, and Tourism, grantNucleus for CardiovascularMagnetic Resonance.	Abadi M, 2015, P 12 USENIX S OPERAT; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Blinn J. F., 1982, Computer Graphics, V16, DOI 10.1145/965145.801290; CHO ZH, 1988, MED PHYS, V15, P7, DOI 10.1118/1.596162; Chollet F., 2015, KERAS; Coenegrachts K, 2008, J MAGN RESON IMAGING, V27, P117, DOI 10.1002/jmri.21247; de Rochefort L, 2010, MAGN RESON MED, V63, P194, DOI 10.1002/mrm.22187; Fu X., 2018, ARXIV180506173; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kee Y, 2017, IEEE T BIO-MED ENG, V64, P2531, DOI 10.1109/TBME.2017.2749298; King DB, 2015, ACS SYM SER, V1214, P1; Koch KM, 2009, MAGN RESON MED, V61, P381, DOI 10.1002/mrm.21856; Langley J, 2011, MAGN RESON MED, V65, P1461, DOI 10.1002/mrm.22727; Lehtinen J, 2018, PR MACH LEARN RES, V80; Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954]; Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005; Liu J, 2012, NEUROIMAGE, V59, P2560, DOI 10.1016/j.neuroimage.2011.08.082; Liu S., 2019, B0 FIELD MAP ESTIMAT; Liu T, 2011, NMR BIOMED, V24, P1129, DOI 10.1002/nbm.1670; Liu T, 2009, MAGN RESON MED, V61, P196, DOI 10.1002/mrm.21828; Lu WM, 2009, MAGN RESON MED, V62, P66, DOI 10.1002/mrm.21967; McAuley G, 2010, MAGN RESON MED, V63, P106, DOI 10.1002/mrm.22185; Milovic C, 2018, MAGN RESON MED, V80, P814, DOI 10.1002/mrm.27073; Mundhenk T. N., 2020, ARXIV191111293; Ramachandran P., 2017, SEARCHING ACTIVATION; Rasmussen K. G. B., 2018, DEEPQSM USING DEEP L, DOI [10.1101/278036, DOI 10.1101/278036]; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rousselle F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024193; Salomir R, 2003, CONCEPT MAGN RESON B, V19B, P26, DOI 10.1002/cmr.b.10083; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shmueli K, 2009, MAGN RESON MED, V62, P1510, DOI 10.1002/mrm.22135; Wang Y.-X. J., 2011, QUANT IMAGING MED SU, V1; Xu B, 2015, COMPUT INTEL NEUROSC, V2015, DOI 10.1155/2015/832093; Ye JC, 2019, PR MACH LEARN RES, V97; Yoon J, 2018, NEUROIMAGE, V179, P199, DOI 10.1016/j.neuroimage.2018.06.030	39	2	2	8	28	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2022	44	1					143	153		10.1109/TPAMI.2020.3012103	http://dx.doi.org/10.1109/TPAMI.2020.3012103			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XM0XY	32750834	Green Published			2022-12-18	WOS:000728561300012
J	Pramod, RT; Arun, SP				Pramod, R. T.; Arun, S. P.			Improving Machine Vision Using Human Perceptual Representations: The Case of Planar Reflection Symmetry for Object Classification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Object recognition; computational models of vision; perception and psychophysics	VISUAL-SEARCH; IMAGES	Achieving human-like visual abilities is a holy grail for machine vision, yet precisely how insights from human vision can improve machines has remained unclear. Here, we demonstrate two key conceptual advances: First, we show that most machine vision models are systematically different from human object perception. To do so, we collected a large dataset of perceptual distances between isolated objects in humans and asked whether these perceptual data can be predicted by many common machine vision algorithms. We found that while the best algorithms explain similar to 70 percent of the variance in the perceptual data, all the algorithms we tested make systematic errors on several types of objects. In particular, machine algorithms underestimated distances between symmetric objects compared to human perception. Second, we show that fixing these systematic biases can lead to substantial gains in classification performance. In particular, augmenting a state-of-the-art convolutional neural network with planar/reflection symmetry scores along multiple axes produced significant improvements in classification accuracy (1-10 percent) across categories. These results show that machine vision can be improved by discovering and fixing systematic differences from human vision.	[Pramod, R. T.] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Arun, S. P.] Indian Inst Sci, Ctr Neurosci, Dept Elect Commun Engn, Bangalore 560012, Karnataka, India	Massachusetts Institute of Technology (MIT); Indian Institute of Science (IISC) - Bangalore	Pramod, RT (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	pramodrt9@gmail.com; sparun@iisc.ac.in	Arun, SP/ABF-8457-2021	Arun, SP/0000-0001-9602-5066; Pramod, Raghavendrarao Taranath/0000-0002-5933-7893	India Alliance [500027/Z/09/Z, IA/S/17/1/503081]	India Alliance(Wellcome Trust DBT India Alliance)	The authors would like to thank Krithika Mohan and N Apurva Ratan Murty for sharing their data for inclusion in the dataset, and members of the Vision Lab for insightful discussions. This work was supported by Intermediate and Senior Fellowships to SPA from the India Alliance (Grants Ref: 500027/Z/09/Z and IA/S/17/1/503081).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arun, 2016, ARXIV161107218; Arun SP, 2012, VISION RES, V74, P86, DOI 10.1016/j.visres.2012.04.005; Bertamini M, 2014, SYMMETRY-BASEL, V6, P975, DOI 10.3390/sym6040975; Brainard DH, 1997, SPATIAL VISION, V10, P433, DOI 10.1163/156856897X00357; Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963; Cohen TS, 2016, PR MACH LEARN RES, V48; DiCarlo JJ, 2007, TRENDS COGN SCI, V11, P333, DOI 10.1016/j.tics.2007.06.010; Eberhardt S, 2016, ADV NEUR IN, V29; Fong RC, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-23618-6; Funk C, 2017, IEEE I CONF COMP VIS, P793, DOI 10.1109/ICCV.2017.92; Gao L, 2019, ARXIV191006511; Geirhos R, 2018, ADV NEUR IN, V31; Gens R., 2014, NIPS; Gonzalez-Garcia A, 2018, INT J COMPUT VISION, V126, P476, DOI 10.1007/s11263-017-1048-0; Guclu U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; JI P, 2019, MULTIMEDIA TOOLS APP, V78; Jia D, 2013, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2013.81; Katti H, 2017, ATTEN PERCEPT PSYCHO, V79, P2021, DOI 10.3758/s13414-017-1359-9; Kay KN, 2008, NATURE, V452, P352, DOI 10.1038/nature06713; Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915; Kriegeskorte N, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00245; Kriegeskorte N, 2008, FRONT SYST NEUROSCI, V2, DOI 10.3389/neuro.06.004.2008; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Leeds DD, 2013, J VISION, V13, DOI 10.1167/13.13.25; Li B, 2016, GRAPH MODELS, V83, P2, DOI 10.1016/j.gmod.2015.09.003; Liu JC, 2013, IEEE COMPUT SOC CONF, P200, DOI 10.1109/CVPRW.2013.155; Lu YX, 2009, FOUND TRENDS COMPUT, V5, P1, DOI 10.1561/0600000008; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Mitchell, 1988, TAO CHING NEW ENGLIS; Pizlo Z., 2014, MAKING MACHINE SEES, DOI 10.1093/acprof:oso/9780199922543.001.0001; Pramod RT, 2018, PSYCHOL SCI, V29, P95, DOI 10.1177/0956797617729808; Pramod RT, 2016, PROC CVPR IEEE, P1601, DOI 10.1109/CVPR.2016.177; Pramod RT, 2016, J VISION, V16, DOI 10.1167/16.5.8; Pramod RT, 2014, J VISION, V14, DOI 10.1167/14.4.6; Radosavovic I, 2018, PROC CVPR IEEE, P4119, DOI 10.1109/CVPR.2018.00433; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; RichardWebster B, 2019, IEEE T PATTERN ANAL, V41, P2280, DOI 10.1109/TPAMI.2018.2849989; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Scheirer WJ, 2014, IEEE T PATTERN ANAL, V36, P1679, DOI 10.1109/TPAMI.2013.2297711; Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236; Sripati AP, 2010, J NEUROSCI, V30, P1258, DOI 10.1523/JNEUROSCI.1908-09.2010; Sunder S, 2016, J VISION, V16, DOI 10.1167/16.15.3; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Vighneshvel T, 2013, J VISION, V13, DOI 10.1167/13.11.10; Wagemans J, 2012, PSYCHOL BULL, V138, P1172, DOI 10.1037/a0029333; Wagemans J, 2012, PSYCHOL BULL, V138, P1218, DOI 10.1037/a0029334; Wah C, 2011, IEEE I CONF COMP VIS, P2524, DOI 10.1109/ICCV.2011.6126539; Wilder J, 2019, COGNITION, V182, P307, DOI 10.1016/j.cognition.2018.09.014; Wolfe JM, 2001, PERCEPT PSYCHOPHYS, V63, P381, DOI 10.3758/BF03194406; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111; YU S, 2018, PROC INT S VIS COMPU, P139; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhivago KA, 2014, J NEUROPHYSIOL, V112, P2745, DOI 10.1152/jn.00532.2014	55	2	2	6	25	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2022	44	1					228	241		10.1109/TPAMI.2020.3008107	http://dx.doi.org/10.1109/TPAMI.2020.3008107			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	XM0XY	32750809	hybrid, Green Accepted			2022-12-18	WOS:000728561300017
J	Zhu, JX; Teolis, S; Biassou, N; Tabb, A; Jabin, PE; Lavi, O				Zhu, Junxi; Teolis, Spencer; Biassou, Nadia; Tabb, Amy; Jabin, Pierre-Emmanuel; Lavi, Orit			Tracking the Adaptation and Compensation Processes of Patients' Brain Arterial Network to an Evolving Glioblastoma	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Brain arterial network; adaptation; GBM; patient data; network topology and hemodynamics	GLOBAL OPTIMIZATION; ROBUST; REGISTRATION; TORTUOSITY; ACCURATE; MODEL	The brain's vascular network dynamically affects its development and core functions. It rapidly responds to abnormal conditions by adjusting properties of the network, aiding stabilization and regulation of brain activities. Tracking prominent arterial changes has clear clinical and surgical advantages. However, the arterial network functions as a system; thus, local changes may imply global compensatory effects that could impact the dynamic progression of a disease. We developed automated personalized system-level analysis methods of the compensatory arterial changes and mean blood flow behavior from a patient's clinical images. By applying our approach to data from a patient with aggressive brain cancer compared with healthy individuals, we found unique spatiotemporal patterns of the arterial network that could assist in predicting the evolution of glioblastoma over time. Our personalized approach provides a valuable analysis tool that could augment current clinical assessments of the progression of glioblastoma and other neurological disorders affecting the brain.	[Zhu, Junxi; Teolis, Spencer; Lavi, Orit] NCI, Integrat Canc Dynam Unit, Lab Cell Biol, CCR,NIH, Bethesda, MI 20892 USA; [Jabin, Pierre-Emmanuel] Univ Maryland, Dept Math, College Pk, MD 20742 USA; [Biassou, Nadia] NIH, Clin Ctr, Bethesda, MD 20814 USA; [Tabb, Amy] ARS, USDA, Washington, DC 20250 USA	National Institutes of Health (NIH) - USA; NIH National Cancer Institute (NCI); University System of Maryland; University of Maryland College Park; National Institutes of Health (NIH) - USA; NIH Clinical Center (CC); United States Department of Agriculture (USDA)	Lavi, O (corresponding author), NCI, Integrat Canc Dynam Unit, Lab Cell Biol, CCR,NIH, Bethesda, MI 20892 USA.	junxi.zhi@nih.gov; spencer.teolis@nih.gov; biassou@cc.nih.gov; amy.tabb@usda.gov; pjabin@cscamm.umd.edu; orit.lavi@nih.gov		Zhu, Junxi/0000-0001-5058-5738; Jabin, Pierre-Emmanuel/0000-0001-5998-0347	Intramural Research Program of the NIH, National Cancer Institute, Center for Cancer Research	Intramural Research Program of the NIH, National Cancer Institute, Center for Cancer Research(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Cancer Institute (NCI))	OL would like to thank Dr. Michael M. Gottesman (Laboratory of Cell Biology, CCR, NCI, NIH) and Dr. Tom Misteli (Laboratory of Receptor Biology and Gene Expression, CCR, NCI, NIH) for their critical comments and support. The authors would also like to thank Mr. George Leiman (LCB, NCI, NIH) for editorial assistance, and Andrea Beri from NIH/CC/BTRIS program for providing the data. This work was supported by the Intramural Research Program of the NIH, National Cancer Institute, Center for Cancer Research. All authors wrote, read and approved the manuscript.	Armento A, 2017, GLIOBLASTOMA, P73, DOI 10.15586/codon.glioblastoma.2017.ch5; Blanco PJ, 2015, IEEE T BIO-MED ENG, V62, P736, DOI 10.1109/TBME.2014.2364522; Bogunovic H., 2012, THESIS U POMPEU FABR; Chang JM, 2017, BMC MED INFORM DECIS, V17, DOI 10.1186/s12911-017-0564-8; Cuddapah VA, 2014, NAT REV NEUROSCI, V15, P455, DOI 10.1038/nrn3765; Edvinsson L., 1993, CEREBRAL BLOOD FLOW, V1185; Esmaeili M, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-19420-z; Fedorov A, 2012, MAGN RESON IMAGING, V30, P1323, DOI 10.1016/j.mri.2012.05.001; Frangi AF, 1998, LECT NOTES COMPUT SC, V1496, P130, DOI 10.1007/BFb0056195; Goriely A, 2015, BIOMECH MODEL MECHAN, V14, P931, DOI 10.1007/s10237-015-0662-4; Govyadinov PA, 2019, IEEE T VIS COMPUT GR, V25, P1760, DOI 10.1109/TVCG.2018.2818701; Greve DN, 2009, NEUROIMAGE, V48, P63, DOI 10.1016/j.neuroimage.2009.06.060; Lv M, 2018, SEMIN ROENTGENOL, V53, P45, DOI 10.1053/j.ro.2017.11.005; Jenkinson M, 2002, NEUROIMAGE, V17, P825, DOI 10.1006/nimg.2002.1132; Jenkinson M, 2001, MED IMAGE ANAL, V5, P143, DOI 10.1016/S1361-8415(01)00036-6; Jenkinson M, 2012, NEUROIMAGE, V62, P782, DOI 10.1016/j.neuroimage.2011.09.015; Jerman T, 2015, PROC SPIE, V9413, DOI 10.1117/12.2081147; KASSAB GS, 1993, AM J PHYSIOL, V265, pH350, DOI 10.1152/ajpheart.1993.265.1.H350; Kim BJ, 2016, STROKE, V47, P2548, DOI 10.1161/STROKEAHA.116.013736; Kornienko VN, 2009, DIAGNOSTIC NEURORADI; Krabbe-Hartkamp MJ, 1998, RADIOLOGY, V207, P103, DOI 10.1148/radiology.207.1.9530305; Lal BK, 2017, J VASC SURG, V66, P1083, DOI 10.1016/j.jvs.2017.04.038; Li X, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-23842-0; Lieberman Frank, 2017, F1000Res, V6, P1892, DOI 10.12688/f1000research.11493.1; Lim M, 2018, NAT REV CLIN ONCOL, V15, P422, DOI 10.1038/s41571-018-0003-5; Liu Y, 2007, AM J PHYSIOL-HEART C, V292, pH1336, DOI 10.1152/ajpheart.00906.2006; Lorthois S, 2014, MICROVASC RES, V91, P99, DOI 10.1016/j.mvr.2013.11.003; Lusebrink F, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.32; Mattern H, 2018, MAGN RESON MED, V80, P248, DOI 10.1002/mrm.27033; Mickevicius NJ, 2015, J NEURO-ONCOL, V125, P393, DOI 10.1007/s11060-015-1928-5; Moccia S, 2018, COMPUT METH PROG BIO, V158, P71, DOI 10.1016/j.cmpb.2018.02.001; Mut F, 2014, INT J NUMER METH BIO, V30, P755, DOI 10.1002/cnm.2627; Noch EK, 2018, WORLD NEUROSURG, V116, P505, DOI 10.1016/j.wneu.2018.04.022; Ozgur A, 2008, BIOINFORMATICS, V24, pI277, DOI 10.1093/bioinformatics/btn182; Oliphant TE, 2007, COMPUT SCI ENG, V9, P10, DOI 10.1109/MCSE.2007.58; Painter PR, 2006, THEOR BIOL MED MODEL, V3, DOI 10.1186/1742-4682-3-31; Pock T. G., 2004, THESIS U GRAZ; Rajapakse VN, 2020, SCI ADV, V6, DOI 10.1126/sciadv.aaz4125; Rempfler M, 2017, LECT NOTES COMPUT SC, V10551, P42, DOI 10.1007/978-3-319-67675-3_5; Rosea JL, 2010, EUR SIGNAL PR CONF, P1781; Scorcioni R, 2008, NAT PROTOC, V3, P866, DOI 10.1038/nprot.2008.51; Swanson KR, 2000, CELL PROLIFERAT, V33, P317, DOI 10.1046/j.1365-2184.2000.00177.x; Sweeney MD, 2018, NAT NEUROSCI, V21, P1318, DOI 10.1038/s41593-018-0234-x; Tabb A, 2018, IEEE WINT CONF APPL, P1935, DOI 10.1109/WACV.2018.00214; Turcotte DL, 1998, J THEOR BIOL, V193, P577, DOI 10.1006/jtbi.1998.0723; Tustison NJ, 2010, IEEE T MED IMAGING, V29, P1310, DOI 10.1109/TMI.2010.2046908; Villanueva-Meyer JE, 2017, NEUROSURGERY, V81, P397, DOI 10.1093/neuros/nyx103; Wales DJ, 1997, J PHYS CHEM A, V101, P5111, DOI 10.1021/jp970984n; Wang JX, 2011, BMC SYST BIOL, V5, DOI 10.1186/1752-0509-5-S3-S10; Wang SD, 2012, ONCOGENE, V31, P5132, DOI 10.1038/onc.2012.16; Wright SN, 2013, NEUROIMAGE, V82, P170, DOI 10.1016/j.neuroimage.2013.05.089; Zarrinkoob L, 2015, J CEREBR BLOOD F MET, V35, P648, DOI 10.1038/jcbfm.2014.241	52	2	2	5	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2022	44	1					488	501		10.1109/TPAMI.2020.3008379	http://dx.doi.org/10.1109/TPAMI.2020.3008379			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XM0XY	32750811	hybrid			2022-12-18	WOS:000728561300035
J	Zhang, H; Chen, B; Cong, YL; Guo, DD; Liu, HW; Zhou, MY				Zhang, Hao; Chen, Bo; Cong, Yulai; Guo, Dandan; Liu, Hongwei; Zhou, Mingyuan			Deep Autoencoding Topic Model With Scalable Hybrid Bayesian Inference	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Analytical models; Probabilistic logic; Artificial neural networks; Decoding; Bayes methods; Nonhomogeneous media; Data models; Deep topic model; Bayesian inference; SG-MCMC; document classification; feature extraction	DIRICHLET; COUNT	To build a flexible and interpretable model for document analysis, we develop deep autoencoding topic model (DATM) that uses a hierarchy of gamma distributions to construct its multi-stochastic-layer generative network. In order to provide scalable posterior inference for the parameters of the generative network, we develop topic-layer-adaptive stochastic gradient Riemannian MCMC that jointly learns simplex-constrained global parameters across all layers and topics, with topic and layer specific learning rates. Given a posterior sample of the global parameters, in order to efficiently infer the local latent representations of a document under DATM across all stochastic layers, we propose a Weibull upward-downward variational encoder that deterministically propagates information upward via a deep neural network, followed by a Weibull distribution based stochastic downward generative model. To jointly model documents and their associated labels, we further propose supervised DATM that enhances the discriminative power of its latent representations. The efficacy and scalability of our models are demonstrated on both unsupervised and supervised learning tasks on big corpora.	[Zhang, Hao; Chen, Bo; Cong, Yulai; Guo, Dandan; Liu, Hongwei] Xidian Univ, Collaborat Innovat Ctr Informat Sensing & Underst, Natl Lab Radar Signal Proc, Xian 710071, Shaanxi, Peoples R China; [Zhou, Mingyuan] Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA	Xidian University; University of Texas System; University of Texas Austin	Chen, B (corresponding author), Xidian Univ, Collaborat Innovat Ctr Informat Sensing & Underst, Natl Lab Radar Signal Proc, Xian 710071, Shaanxi, Peoples R China.	zhanghao_xidian@163.com; bchen@mail.xidian.edu.cn; yulaicong@gmail.com; gdd_xidian@126.com; hwliu@xidian.edu.cn; mingyuan.zhou@mccombs.utexas.edu	Zhou, Mingyuan/AAE-8717-2021; chen, bo/AAC-7188-2022		Program for Oversea Talent by Chinese Central Government; 111 Project [B18039]; NSFC [61771361]; NSFC for Distinguished Young Scholars [61525105]; Shaanxi Innovation Team Project; U.S. National Science Foundation [IIS1812699]	Program for Oversea Talent by Chinese Central Government; 111 Project(Ministry of Education, China - 111 Project); NSFC(National Natural Science Foundation of China (NSFC)); NSFC for Distinguished Young Scholars(National Natural Science Foundation of China (NSFC)National Science Fund for Distinguished Young Scholars); Shaanxi Innovation Team Project; U.S. National Science Foundation(National Science Foundation (NSF))	The work of B. Chen was supported by the Program for Oversea Talent by Chinese Central Government, the 111 Project (No. B18039), and NSFC (61771361). The work of H. Liu was supported by the NSFC for Distinguished Young Scholars (61525105) and Shaanxi Innovation Team Project. The work of M. Zhou was supported by the Award IIS1812699 from the U.S. National Science Foundation.	Al-Rfou R, 2016, THEANO PYTHON FRAMEW; Alireza M., 2016, P INT C LEARN REPR W; Blei, 2011, P 17 ACM SIGKDD INT, P448, DOI DOI 10.1145/2020408.2020480; Blei D., 2006, NIPS, V18, P147, DOI [10.5555/2976248.2976267, DOI 10.5555/2976248.2976267, DOI 10.1145/1143844.1143859]; Blei DM, 2004, ADV NEUR IN, V16, P17; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Cong Y., 2017, P 34 INT C MACH LEAR, P864; Cong YL, 2017, BAYESIAN ANAL, V12, P1017, DOI 10.1214/17-BA1052; Dai Andrew M., 2015, ADV NEURAL INFORM PR; Dai Z., 2016, INT C LEARN REPR; Dumoulin Vincent, 2017, ICLR 2017, P4; Foulds J, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P446; Gan Z, 2015, PR MACH LEARN RES, V37, P1823; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Henao R, 2015, ADV NEUR IN, V28; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Hoffman M., 2010, ONLINE LEARNING LATE, P856; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Ishaan G, 2017, P INT C LEARN REPR; Johnson R, 2016, PR MACH LEARN RES, V48; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D.P., 2014, P 2 INT C LEARN REPR, V19; Knowles David A, 2015, ARXIV150901631; Lacoste-Julien S, 2009, ADV NEURAL INFORM PR, P897, DOI DOI 10.1007/S10618-010-0175-9; Larochelle H., 2012, ADV NEURAL INFORM PR, P2708; Lee CH, 2016, INT CONF ACOUST SPEE, P2279, DOI 10.1109/ICASSP.2016.7472083; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; Li CX, 2015, ADV NEUR IN, V28; Ma Y.A., 2015, ARXIV150604696, P2917; Maas A., 2011, P 49 ANN M ASS COMPU, P142; Marsaglia G, 2000, ACM T MATH SOFTWARE, V26, P363, DOI 10.1145/358407.358414; Mcauliffe Jon D., 2008, P ADV NEURAL INFORM, P121; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Naesseth CA, 2017, PR MACH LEARN RES, V54, P489; Neven, 2014, ADV NEURAL INFORM PR, P3203; Paisley J, 2015, IEEE T PATTERN ANAL, V37, P256, DOI 10.1109/TPAMI.2014.2318728; Paisley John, 2011, P 14 INT C ART INT S, P74; Patterson S., 2013, P 26 INT C NEUR INF, P3102; Polatkan G, 2015, IEEE T PATTERN ANAL, V37, P346, DOI 10.1109/TPAMI.2014.2321404; Ranganath R, 2015, JMLR WORKSH CONF PRO, V38, P762; Ranganath R, 2014, JMLR WORKSH CONF PRO, V33, P814; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Ruiz Francisco R, 2016, ADV NEURAL INFORM PR, P460; Salakhutdinov R, 2009, ADV NEURAL INFORM PR; Sonderby CK, 2016, ADV NEUR IN, V29; Srivastava Akash, 2017, ARXIV170301488; Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI [DOI 10.1162/NEC0_A_00311, DOI 10.1109/CVPR.2013.49]; Srivastava N., 2013, P 20 9 C UNCERTAINTY, P616; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Wallach H., 2009, P 26 ANN INT C MACHI, V382, P1105, DOI DOI 10.1145/1553374.1553515; Wang CJ, 2018, AAAI CONF ARTIF INTE, P2492; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Zhang H., 2018, 6 INT C LEARNING REP, DOI 10.48550/arXiv.1710.09412; Zhang T., 2015, P 2015 C N AM CHAPT, P103, DOI DOI 10.3115/V1/N15-1011; Zhou MY, 2018, ADV NEUR IN, V31; Zhou MY, 2015, ADV NEUR IN, V28; Zhou MY, 2016, J MACH LEARN RES, V17, P1; Zhou MY, 2016, J AM STAT ASSOC, V111, P1144, DOI 10.1080/01621459.2015.1075407; Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211; Zhou Mingyuan, 2012, AISTATS, P1462; Zhu J, 2014, J MACH LEARN RES, V15, P1073; Zhu J, 2012, J MACH LEARN RES, V13, P2237	67	2	2	3	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2021	43	12					4306	4322		10.1109/TPAMI.2020.3003660	http://dx.doi.org/10.1109/TPAMI.2020.3003660			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WR0MQ	32750790	Green Submitted			2022-12-18	WOS:000714203900013
J	Grasshof, S; Ackermann, H; Brandt, SS; Ostermann, J				Grasshof, Stella; Ackermann, Hanno; Brandt, Sami Sebastian; Ostermann, Joern			Multilinear Modelling of Faces and Expressions	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Tensile stress; Three-dimensional displays; Shape; Solid modeling; Data models; Two dimensional displays; Analytical models; Statistical shape model; tensor model; HOSVD; expression transfer; person transfer; 3D-reconstruction	MORPHABLE MODEL; 3D; SHAPE; RECONSTRUCTION; DATABASE; MOTION	In this work, we present a new versatile 3D multilinear statistical face model, based on a tensor factorisation of 3D face scans, that decomposes the shapes into person and expression subspaces. Investigation of the expression subspace reveals an inherent low-dimensional substructure, and further, a star-shaped structure. This is due to two novel findings. (1) Increasing the strength of one emotion approximately forms a linear trajectory in the subspace. (2) All these trajectories intersect at a single point - not at the neutral expression as assumed by almost all prior works-but at an apathetic expression. We utilise these structural findings by reparameterising the expression subspace by the fourth-order moment tensor centred at the point of apathy. We propose a 3D face reconstruction method from single or multiple 2D projections by assuming an uncalibrated projective camera model. The non-linearity caused by the perspective projection can be neatly included into the model. The proposed algorithm separates person and expression subspaces convincingly, and enables flexible, natural modelling of expressions for a wide variety of human faces. Applying the method on independent faces showed that morphing between different persons and expressions can be performed without strong deformations.	[Grasshof, Stella; Ackermann, Hanno] Leibniz Univ Hannover, D-30167 Hannover, Germany; [Grasshof, Stella] IT Univ Copenhagen, DK-2300 Copenhagen, Denmark; [Ostermann, Joern] Leibniz Univ Hannover, Inst Informat Verarbeitung, D-30167 Hannover, Germany; [Brandt, Sami Sebastian] IT Univ Copenhagen, Dept Comp Sci, DK-2300 Copenhagen, Denmark; [Brandt, Sami Sebastian] Univ Copenhagen, DK-1165 Copenhagen, Denmark	Leibniz University Hannover; IT University Copenhagen; Leibniz University Hannover; IT University Copenhagen; University of Copenhagen	Grasshof, S (corresponding author), Leibniz Univ Hannover, D-30167 Hannover, Germany.	stgr@itu.dk; ackermann@tnt.uni-hannover.de; sami.brandt@itu.dk; ostermann@tnt.uni-hannover.de		Grasshof, Stella/0000-0002-6791-7425; Brandt, Sami/0000-0003-2141-9815; Ostermann, Jorn/0000-0002-6743-3324	German Research Foundation [DFG RO 2497/12-2]	German Research Foundation(German Research Foundation (DFG))	This article was supported by German Research Foundation Grant DFG RO 2497/12-2.	Ackermann H., 2011, 2011 Conference for Visual Media Production, P77, DOI 10.1109/CVMP.2011.15; Ackermann H, 2008, LECT NOTES COMPUT SC, V4931, P153; Alex M, 2002, LECT NOTES COMPUT SC, V2350, P447; Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311; Amberg B, 2008, IEEE INT CONF AUTOMA, P667; Amos B, 2016, CMU SCH COMPUTER SCI, V6; Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207; Awiszus M, 2018, IEEE COMPUT SOC CONF, P1199, DOI 10.1109/CVPRW.2018.00156; Bagdanov Andrew D, 2011, P 2011 JOINT ACM WOR, P79, DOI DOI 10.1145/2072572.2072597; Bas A, 2017, LECT NOTES COMPUT SC, V10117, P377, DOI 10.1007/978-3-319-54427-4_28; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Brandt S., 2002, Proceedings of the Statistical Methods in Video Processing Workshop, P109; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; Brunton A, 2014, LECT NOTES COMPUT SC, V8689, P297, DOI 10.1007/978-3-319-10590-1_20; Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249; Cao C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462012; Chang FJ, 2019, INT J COMPUT VISION, V127, P930, DOI 10.1007/s11263-019-01151-x; Chu B, 2014, PROC CVPR IEEE, P1907, DOI 10.1109/CVPR.2014.245; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696; Garrido P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2890493; Garrido P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508380; Golyanik Vladislav, 2016, 2016 IEEE WINT C APP, P1, DOI DOI 10.1109/ICCV.2017.322; Grasshof S, 2017, PROCEEDINGS OF THE FIFTEENTH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS - MVA2017, P298, DOI 10.23919/MVA.2017.7986860; Grasshof S, 2017, IEEE INT CONF AUTOMA, P658, DOI 10.1109/FG.2017.83; Hartley R., 2004, ROBOTICA; Hasler N, 2009, COMPUT GRAPH FORUM, V28, P337, DOI 10.1111/j.1467-8659.2009.01373.x; Hasler N, 2010, PROC CVPR IEEE, P1823, DOI 10.1109/CVPR.2010.5539853; Huber Patrik, 2016, VISIGRAPP 2016. 11th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications. Proceedings: VISAPP 2016, P79; Huber P., 2018, SURREY FACE MODEL; Jain A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866174; Jensen KH, 2016, IEEE T IMAGE PROCESS, V25, P540, DOI 10.1109/TIP.2015.2504901; Joo D, 2018, PROC CVPR IEEE, P1635, DOI 10.1109/CVPR.2018.00176; Kanatani K, 2006, LECT NOTES COMPUT SC, V3954, P147; Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241; King DE, 2009, J MACH LEARN RES, V10, P1755; Kostinger M, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS); Lee E, 2008, PSYCHIAT RES, V157, P77, DOI 10.1016/j.psychres.2007.02.005; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6; Sela M, 2017, IEEE I CONF COMP VIS, P1585, DOI 10.1109/ICCV.2017.175; Sun YJ, 2008, SENSYS'08: PROCEEDINGS OF THE 6TH ACM CONFERENCE ON EMBEDDED NETWORKED SENSOR SYSTEMS, P1, DOI 10.1145/1460412.1460414; Thies J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818056; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71; van der Schalk J, 2011, EMOTION, V11, P907, DOI 10.1037/a0023853; Vlasic D, 2005, ACM T GRAPHIC, V24, P426, DOI 10.1145/1073204.1073209; Wandt B, 2016, IEEE T PATTERN ANAL, V38, P1505, DOI 10.1109/TPAMI.2016.2553028; Wandt Bastian, 2018, P EUR C COMP VIS ECC; Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23	50	2	2	1	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2021	43	10					3540	3554		10.1109/TPAMI.2020.2986496	http://dx.doi.org/10.1109/TPAMI.2020.2986496			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UK8RG	32305893				2022-12-18	WOS:000692232400021
J	Dang, Z; Yi, KM; Hu, YL; Wang, F; Fua, P; Salzmann, M				Dang, Zheng; Yi, Kwang Moo; Hu, Yinlin; Wang, Fei; Fua, Pascal; Salzmann, Mathieu			Eigendecomposition-Free Training of Deep Networks for Linear Least-Square Problems	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Eigenvalues and eigenfunctions; Three-dimensional displays; Machine learning; Optimization; Computer vision; Task analysis; Training; End-to-end learning; eigendecomposition; singular value decomposition; geometric vision		Many classical Computer Vision problems, such as essential matrix computation and pose estimation from 3D to 2D correspondences, can be tackled by solving a linear least-square problem, which can be done by finding the eigenvector corresponding to the smallest, or zero, eigenvalue of a matrix representing a linear system. Incorporating this in deep learning frameworks would allow us to explicitly encode known notions of geometry, instead of having the network implicitly learn them from data. However, performing eigendecomposition within a network requires the ability to differentiate this operation. While theoretically doable, this introduces numerical instability in the optimization process in practice. In this paper, we introduce an eigendecomposition-free approach to training a deep network whose loss depends on the eigenvector corresponding to a zero eigenvalue of a matrix predicted by the network. We demonstrate that our approach is much more robust than explicit differentiation of the eigendecomposition using two general tasks, outlier rejection and denoising, with several practical examples including wide-baseline stereo, the perspective-n-point problem, and ellipse fitting. Empirically, our method has better convergence properties and yields state-of-the-art results.	[Dang, Zheng; Hu, Yinlin] Xi An Jiao Tong Univ, Natl Engn Lab Visual Informat Proc & Applicat, Xian 710000, Shaanxi, Peoples R China; [Yi, Kwang Moo] Univ Victoria, Visual Comp Grp, Victoria, BC V8P 5C2, Canada; [Wang, Fei; Fua, Pascal; Salzmann, Mathieu] Ecole Polytech Fed Lausanne, Comp Vis Lab, CH-1015 Lausanne, Switzerland	Xi'an Jiaotong University; University of Victoria; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Wang, F (corresponding author), Ecole Polytech Fed Lausanne, Comp Vis Lab, CH-1015 Lausanne, Switzerland.	dangzheng713@stu.xjtu.edu.cn; kyi@uvic.ca; yinlin.hu@epfl.ch; wfx@mail.xjtu.edu.cn; pascal.fua@epfl.ch; mathieu.salzmann@epfl.ch	; Yi, Kwang Moo/C-2612-2016	Hu, Yinlin/0000-0003-2614-5200; Yi, Kwang Moo/0000-0001-9036-3822; Salzmann, Mathieu/0000-0002-8347-8637	National Major Science and Technology Projects of China [2019ZX01008103]; National Natural Science Foundation of China (NSFC) [61603291]; Natural Science Basic Research Plan in Shaanxi Province of China [2018JM6057]; Fundamental Research Funds for the Central Universities; Swiss Commission for Technology and Innovation project [25939.1]	National Major Science and Technology Projects of China; National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Natural Science Basic Research Plan in Shaanxi Province of China; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Swiss Commission for Technology and Innovation project	This work was supported in part by National Major Science and Technology Projects of China (No. 2019ZX01008103), National Natural Science Foundation of China (NSFC) (No. 61603291), Natural Science Basic Research Plan in Shaanxi Province of China (No. 2018JM6057), the Fundamental Research Funds for the Central Universities and Swiss Commission for Technology and Innovation project (No. 25939.1).	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302; Brachmann E., 2016, P IEEE C COMP VIS PA P IEEE C COMP VIS PA, P6684; Cantzler H., 2005, RANDOM SAMPLE CONSEN; Crivellaro A, 2018, IEEE T PATTERN ANAL, V40, P1465, DOI 10.1109/TPAMI.2017.2708711; Dang Z., 2018, P EUR C COMP VIS ECC, P768; Ferraz L, 2014, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2014.71; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fitzgibbon A, 1999, IEEE T PATTERN ANAL, V21, P476, DOI 10.1109/34.765658; Garro V, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P262, DOI 10.1109/3DIMPVT.2012.40; Giles M.B., 2008, ADV AUTOMATIC DIFFER, V64, P35, DOI [10.1007/978-3-540-68942-34, DOI 10.1007/978-3-540-68942-34]; Gould S., 2016, ARXIV160705447; Halir R, 1998, WSCG '98, VOL 1, P125; Handa A, 2016, LECT NOTES COMPUT SC, V9915, P67, DOI 10.1007/978-3-319-49409-8_9; Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Heinly J, 2015, PROC CVPR IEEE, P3287, DOI 10.1109/CVPR.2015.7298949; Huang Z, 2017, CVPR; Huang ZW, 2017, AAAI CONF ARTIF INTE, P2036; Ionescu C, 2015, IEEE I CONF COMP VIS, P2965, DOI 10.1109/ICCV.2015.339; Jaderberg M, 2015, ADV NEUR IN, V28; Kanatani K., 2016, SYNTH LECT COMPUT VI, V6, P1, DOI DOI 10.2200/S00713ED1V01Y201603COV008; Kingma D.P, P 3 INT C LEARNING R; Kneip L, 2011, PROC CVPR IEEE; Law MT, 2017, PR MACH LEARN RES, V70; Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6; Li SQ, 2012, IEEE T PATTERN ANAL, V34, P1444, DOI 10.1109/TPAMI.2012.41; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Murray I., 2016, DIFFERENTIATION CHOL; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; Papadopoulo T, 2000, LECT NOTES COMPUT SC, V1842, P554; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Raguram R, 2013, IEEE T PATTERN ANAL, V35, P2022, DOI 10.1109/TPAMI.2012.257; Ranftl R., 2018, P EUR C COMP VIS P EUR C COMP VIS, P485; Rousseeuw P.J., 1987, ROBUST REGRESSION OU; SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451; Simpson D. G., 1997, BREAKTHROUGHS STAT, P433; Strecha C., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587706; Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832; Ummenhofer B, 2017, PROC CVPR IEEE, P5622, DOI 10.1109/CVPR.2017.596; Wu CC, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P127, DOI 10.1109/3DV.2013.25; Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458; Yi KM, 2018, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2018.00282; Zamir AR, 2016, LECT NOTES COMPUT SC, V9907, P535, DOI 10.1007/978-3-319-46487-9_33; Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561; Zhang ZY, 1997, IMAGE VISION COMPUT, V15, P59, DOI 10.1016/S0262-8856(96)01112-2; Zheng YQ, 2013, IEEE I CONF COMP VIS, P2344, DOI 10.1109/ICCV.2013.291	47	2	2	1	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2021	43	9					3167	3182		10.1109/TPAMI.2020.2978812	http://dx.doi.org/10.1109/TPAMI.2020.2978812			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TU6DH	32149625	Green Submitted			2022-12-18	WOS:000681124300024
J	Lartigue, T; Bottani, S; Baron, S; Colliot, O; Durrleman, S; Allassonniere, S				Lartigue, Thomas; Bottani, Simona; Baron, Stephanie; Colliot, Olivier; Durrleman, Stanley; Allassonniere, Stephanie		Alzheimers Dis Neuroimaging Initia	Gaussian Graphical Model Exploration and Selection in High Dimension Low Sample Size Setting	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Correlation; Covariance matrices; Measurement; Graphical models; Gaussian distribution; Sparse representation; Alzheimer's disease; Gaussian graphical models; model selection; high dimension low sample size; sparse matrices; maximum likelihood estimation	MAXIMUM-LIKELIHOOD-ESTIMATION; COVARIANCE ESTIMATION; SPARSE ESTIMATION; LASSO	Gaussian graphical models (GGM) are often used to describe the conditional correlations between the components of a random vector. In this article, we compare two families of GGM inference methods: the nodewise approach and the penalised likelihood maximisation. We demonstrate on synthetic data that, when the sample size is small, the two methods produce graphs with either too few or too many edges when compared to the real one. As a result, we propose a composite procedure that explores a family of graphs with a nodewise numerical scheme and selects a candidate among them with an overall likelihood criterion. We demonstrate that, when the number of observations is small, this selection method yields graphs closer to the truth and corresponding to distributions with better KL divergence with regards to the real distribution than the other two. Finally, we show the interest of our algorithm on two concrete cases: first on brain imaging data, then on biological nephrology data. In both cases our results are more in line with current knowledge in each field.	[Lartigue, Thomas] Ecole Polytech, IP, CMAP, CNRS, Paris, France; [Lartigue, Thomas] INRIA, Aramis Project Team, F-91128 Palaiseau, France; [Bottani, Simona; Colliot, Olivier; Durrleman, Stanley] Sorbonne Univ, CNRS UMR 7225, Inserm U1127, Aramis Project Team,Inria,Inst Cerveau & Moelle E, F-75004 Paris, France; [Baron, Stephanie] Hop Europeen Georges Pompidou, AP HP, F-75015 Paris, France; [Allassonniere, Stephanie] Sorbonne Univ, INSERM, Univ Paris, Ctr Rech Cordeliers, F-75006 Paris, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Cite; Inria; Centre National de la Recherche Scientifique (CNRS); CNRS - National Institute for Biology (INSB); Inria; Institut National de la Sante et de la Recherche Medicale (Inserm); UDICE-French Research Universities; Sorbonne Universite; Assistance Publique Hopitaux Paris (APHP); Hopital Universitaire Europeen Georges-Pompidou - APHP; UDICE-French Research Universities; Universite Paris Cite; Institut National de la Sante et de la Recherche Medicale (Inserm); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite	Lartigue, T (corresponding author), Ecole Polytech, IP, CMAP, CNRS, Paris, France.; Lartigue, T (corresponding author), INRIA, Aramis Project Team, F-91128 Palaiseau, France.	thomas.lartigue@inria.fr; simona.bottani@icm-institute.org; stephanie.baron@aphp.fr; olivier.colliot@upmc.fr; stanley.durrleman@inria.fr; stephanie.allassonniere@parisdescartes.fr	Baron, Stephanie/AAY-5703-2020; Lartigue, Thomas/AAP-2709-2021	Baron, Stephanie/0000-0002-0375-6968; Lartigue, Thomas/0000-0002-7820-0032	European Research Council (ERC) [678304]; European Union [666992, 826421]; French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" Program [ANR-19P3IA-0001, ANR-10IAIHU-06]	European Research Council (ERC)(European Research Council (ERC)European Commission); European Union(European Commission); French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" Program(French National Research Agency (ANR))	This work was supported by the European Research Council (ERC) under grant agreement no 678304, the European Union's Horizon 2020 research and Innovation Program under grant agreement no 666992 (EuroPOND) and No 826421 (TVB-Cloud), and the French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" Program, reference ANR-19P3IA-0001 (PRAIRIE 3IA Institute) and reference ANR-10IAIHU-06 (IHU-A-ICM). The authors would like to thank Pascal Houillier for his insightful comments on the nephrological experiments. Data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (http://www.loni.ucla.edu/ADNI).As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing ofADNI investigators can be found at https://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_ Acknowledgement_List.pdf	Banerjee O, 2008, J MACH LEARN RES, V9, P485; Bullmore ET, 2009, NAT REV NEUROSCI, V10, P186, DOI 10.1038/nrn2575; Cai T, 2011, J AM STAT ASSOC, V106, P594, DOI 10.1198/jasa.2011.tm10155; DEMPSTER AP, 1972, BIOMETRICS, V28, P157, DOI 10.2307/2528966; Duchi J., 2008, P 24 C UNC ART INT; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Fan JQ, 2009, ANN APPL STAT, V3, P521, DOI 10.1214/08-AOAS215; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Giraud C, 2008, ELECTRON J STAT, V2, P542, DOI 10.1214/08-EJS228; Giraud C, 2012, STAT APPL GENET MOL, V11, DOI 10.1515/1544-6115.1625; Holmes CJ, 1998, J COMPUT ASSIST TOMO, V22, P324, DOI 10.1097/00004728-199803000-00032; Hsieh C.-J., 2011, ADV NEURAL INFORM PR, P2330; Jankov~a J., 2019, HDB GRAPHICAL MODELS, P325; Jankova J, 2017, TEST-SPAIN, V26, P143, DOI 10.1007/s11749-016-0503-5; Jankova J, 2015, ELECTRON J STAT, V9, P1205, DOI 10.1214/15-EJS1031; Lam C, 2009, ANN STAT, V37, P4254, DOI 10.1214/09-AOS720; Levina E, 2008, ANN APPL STAT, V2, P245, DOI 10.1214/07-AOAS139; Li B, 2012, J AM STAT ASSOC, V107, P152, DOI 10.1080/01621459.2011.644498; Li L, 2010, MATH PROGRAM COMPUT, V2, P291, DOI 10.1007/s12532-010-0020-6; Mazumder R, 2012, ELECTRON J STAT, V6, P2125, DOI 10.1214/12-EJS740; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Pedregosa F., 2011, J MACH LEARN RES, V12, P2825; Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631; Rocha G. V., 2008, 759 DEP STAT; Rothman AJ, 2008, ELECTRON J STAT, V2, P494, DOI 10.1214/08-EJS176; Scheinberg Katya, 2010, P ADV NEUR INF PROC, P2101; Sun TN, 2013, J MACH LEARN RES, V14, P3385; Tournier JD, 2012, INT J IMAG SYST TECH, V22, P53, DOI 10.1002/ima.22005; Uhler C, 2012, ANN STAT, V40, P238, DOI 10.1214/11-AOS957; Wang CJ, 2010, SIAM J OPTIMIZ, V20, P2994, DOI 10.1137/090772514; Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018; Yuan M, 2010, J MACH LEARN RES, V11, P2261; Yuan XM, 2012, J SCI COMPUT, V51, P261, DOI 10.1007/s10915-011-9507-1; Zhou SH, 2011, J MACH LEARN RES, V12, P2975	37	2	2	2	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2021	43	9					3196	3213		10.1109/TPAMI.2020.2980542	http://dx.doi.org/10.1109/TPAMI.2020.2980542			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TU6DH	32175856	Green Submitted			2022-12-18	WOS:000681124300026
J	Ma, XC; Blaschko, MB				Ma, Xingchen; Blaschko, Matthew B.			Additive Tree-Structured Conditional Parameter Spaces in Bayesian Optimization: A Novel Covariance Function and a Fast Implementation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Optimization; Additives; Mathematical model; Linear programming; Bayes methods; Neural networks; Data models; Nonparametric statistics; global optimization; parameter learning	SEARCH	Bayesian optimization (BO) is a sample-efficient global optimization algorithm for black-box functions which are expensive to evaluate. Existing literature on model based optimization in conditional parameter spaces are usually built on trees. In this work, we generalize the additive assumption to tree-structured functions and propose an additive tree-structured covariance function, showing improved sample-efficiency, wider applicability and greater flexibility. Furthermore, by incorporating the structure information of parameter spaces and the additive assumption in the BO loop, we develop a parallel algorithm to optimize the acquisition function and this optimization can be performed in a low dimensional space. We demonstrate our method on an optimization benchmark function, on a neural network compression problem and on pruning pre-trained VGG16 and ResNet50 models. Experimental results show our approach significantly outperforms the current state of the art for conditional parameter optimization including SMAC, TPE and Jenatton et al. (2017).	[Ma, Xingchen; Blaschko, Matthew B.] Katholieke Univ Leuven, Dept Elect Engn, Ctr Proc Speech & Images, Kasteelpk Arenberg, B-103001 Leuven, Belgium	KU Leuven	Ma, XC (corresponding author), Katholieke Univ Leuven, Dept Elect Engn, Ctr Proc Speech & Images, Kasteelpk Arenberg, B-103001 Leuven, Belgium.	xingchen.ma@esat.kuleuven.be; matthew.blaschko@esat.kuleuven.be		Blaschko, Matthew/0000-0002-2640-181X; Ma, Xingchen/0000-0001-9667-3768	Flemish Government under the "Onderzoeksprogramma Artifici_ele Intelligentie (AI) Vlaanderen" Programme; Onfido	Flemish Government under the "Onderzoeksprogramma Artifici_ele Intelligentie (AI) Vlaanderen" Programme; Onfido	The work of Xingchen Ma was supported by Onfido. This work was supported by the Flemish Government under the "Onderzoeksprogramma Artifici_ele Intelligentie (AI) Vlaanderen" Programme.	[Anonymous], 2016, GPYOPT BAYESIAN OPTI; Bergstra J., 2013, JMLR WORKSHOP C P IC, V28, P115, DOI [10.5555/3042817.3042832, DOI 10.5555/3042817.3042832]; Bergstra J, 2012, J MACH LEARN RES, V13, P281; Bergstra James S, 2011, ADV NEURAL INFORM PR, P2546, DOI [10.5555/2986459.2986743, DOI 10.5555/2986459.2986743]; Berkenkamp F, 2019, J MACH LEARN RES, V20; Brochu E, 2010, ARXIV PREPRINT ARXIV; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Clevert D., 2016, P 4 INT C LEARN REPR; Duvenaud D.K., 2011, ADV NEURAL INFORM PR, P226; Frazier P.I., 2018, ARXIVPREPRINTARXIV18; Frazier P, 2009, INFORMS J COMPUT, V21, P599, DOI 10.1287/ijoc.1080.0314; Fulton W, 2000, B AM MATH SOC, V37, P209, DOI 10.1090/S0273-0979-00-00865-X; Gardner JR, 2017, PR MACH LEARN RES, V54, P1311; Gy_orfi L., DISTRIBUTION FREE TH; Hennig P, 2012, J MACH LEARN RES, V13, P1809; Hutter F., 2013, ABS13105738 CORR ABS13105738 CORR; Jenatton R, 2017, PR MACH LEARN RES, V70; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kandasamy K, 2015, PR MACH LEARN RES, V37, P295; Kim J., 2019, ABS190108350 CORR ABS190108350 CORR; Klein A, 2017, PR MACH LEARN RES, V54, P528; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; Levesque JC, 2017, IEEE IJCNN, P286, DOI 10.1109/IJCNN.2017.7965867; Ma XC, 2020, PR MACH LEARN RES, V108, P1015; Ma XC, 2019, IEEE I CONF COMP VIS, P10273, DOI 10.1109/ICCV.2019.01037; Maas Andrew L, 2013, P ICML CIT, V30, P3, DOI DOI 10.21437/INTERSPEECH.2016-1230; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rolland Paul, 2018, INT C ART INT STAT, P298; Seeger MW, 2008, IEEE T INFORM THEORY, V54, P2376, DOI 10.1109/TIT.2007.915707; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Swersky K., 2014, ABS14094011 CORR ABS14094011 CORR; Wainwright M.J., 2019, HIGH DIMENSIONAL STA, DOI DOI 10.1017/9781108627771; Wang Z., 2013, INT JOINT C ART INT	38	2	2	4	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2021	43	9					3024	3036		10.1109/TPAMI.2020.3026019	http://dx.doi.org/10.1109/TPAMI.2020.3026019			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TU6DH	32960762	Green Submitted, Green Published			2022-12-18	WOS:000681124300014
J	Yi, R; Ye, ZP; Zhao, W; Yu, MJ; Lai, YK; Liu, YJ				Yi, Ran; Ye, Zipeng; Zhao, Wang; Yu, Minjing; Lai, Yu-Kun; Liu, Yong-Jin			Feature-Aware Uniform Tessellations on Video Manifold for Content-Sensitive Supervoxels	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Supervoxels; video over-segmentation; video manifold; low-level video features; centroidal Voronoi tessellation	SEGMENTATION	Over-segmenting a video into supervoxels has strong potential to reduce the complexity of downstream computer vision applications. Content-sensitive supervoxels (CSSs) are typically smaller in content-dense regions (i.e., with high variation of appearance and/or motion) and larger in content-sparse regions. In this paper, we propose to compute feature-aware CSSs (FCSSs) that are regularly shaped 3D primitive volumes well aligned with local object/region/motion boundaries in video. To compute FCSSs, we map a video to a 3D manifold embedded in a combined color and spatiotemporal space, in which the volume elements of video manifold give a good measure of the video content density. Then any uniform tessellation on video manifold can induce CSS in the video. Our idea is that among all possible uniform tessellations on the video manifold, FCSS finds one whose cell boundaries well align with local video boundaries. To achieve this goal, we propose a novel restricted centroidal Voronoi tessellation method that simultaneously minimizes the tessellation energy (leading to uniform cells in the tessellation) and maximizes the average boundary distance (leading to good local feature alignment). Theoretically our method has an optimal competitive ratio O(1), and its time and space complexities are O(NK) and O(N + K) for computing K supervoxels in an N-voxel video. We also present a simple extension of FCSS to streaming FCSS for processing long videos that cannot be loaded into main memory at once. We evaluate FCSS, streaming FCSS and ten representative supervoxel methods on four video datasets and two novel video applications. The results show that our method simultaneously achieves state-of-the-art performance with respect to various evaluation criteria.	[Yi, Ran; Ye, Zipeng; Zhao, Wang; Liu, Yong-Jin] Tsinghua Univ, Dept Comp Sci & Technol, MOE Key Lab Pervas Comp, Beijing 100084, Peoples R China; [Yu, Minjing] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China; [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF10 3AT, Wales	Tsinghua University; Tianjin University; Cardiff University	Liu, YJ (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, MOE Key Lab Pervas Comp, Beijing 100084, Peoples R China.; Yu, MJ (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China.	yr16@mails.tsinghua.edu.cn; yezp17@mails.tsinghua.edu.cn; zhao-w19@mails.tsinghua.edu.cn; minjingyu@tju.edu.cn; LaiY4@cardiff.ac.uk; liuyongjin@tsinghua.edu.cn	Yi, Ran/AAU-6636-2021; Liu, Yong/GWQ-6163-2022	Yi, Ran/0000-0003-1858-3358; Ye, Zipeng/0000-0002-4322-7550	Natural Science Foundation of China [61725204, 61521002]	Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the Natural Science Foundation of China (61725204, 61521002). Ran Yi and Zipeng Ye are co-first authors.	Ailon N., 2009, PROC 22 ADV NEURAL I, P10; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5; Cai YQ, 2016, COMPUT GRAPH FORUM, V35, P199, DOI 10.1111/cgf.13017; Chang J, 2013, PROC CVPR IEEE, P2051, DOI 10.1109/CVPR.2013.267; Chen A. Y. C., 2010, Proceedings of 2010 Western New York Image Processing Workshop (WNYIPW), P14, DOI 10.1109/WNYIPW.2010.5649773; Corso JJ, 2008, IEEE T MED IMAGING, V27, P629, DOI 10.1109/TMI.2007.912817; Du Q, 1999, SIAM REV, V41, P637, DOI 10.1137/S0036144599352836; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185; Fowlkes C, 2001, PROC CVPR IEEE, P231; Galasso F, 2013, IEEE I CONF COMP VIS, P3527, DOI 10.1109/ICCV.2013.438; Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893; Jain SD, 2014, LECT NOTES COMPUT SC, V8692, P656, DOI 10.1007/978-3-319-10593-2_43; Ke Y, 2007, IEEE I CONF COMP VIS, P1424; Lee SH, 2017, IEEE I CONF COMP VIS, P3630, DOI 10.1109/ICCV.2017.390; Levinshtein A, 2012, INT J COMPUT VISION, V100, P99, DOI 10.1007/s11263-012-0527-6; Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96; Li CL, 2015, PROC CVPR IEEE, P5519, DOI 10.1109/CVPR.2015.7299191; Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273; Liang YL, 2016, IEEE T CIRC SYST VID, V26, P928, DOI 10.1109/TCSVT.2015.2406232; Liu YJ, 2018, IEEE T PATTERN ANAL, V40, P653, DOI 10.1109/TPAMI.2017.2686857; Liu YJ, 2016, PROC CVPR IEEE, P651, DOI 10.1109/CVPR.2016.77; Lu JS, 2015, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2015.7299000; Ming-Yu Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2097, DOI 10.1109/CVPR.2011.5995323; Moore A. P., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587471; Oneata D, 2014, LECT NOTES COMPUT SC, V8691, P737, DOI 10.1007/978-3-319-10578-9_48; Paris S., 2007, P IEEE C COMP VIS PA, P1; Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065; Reso M, 2019, IEEE T PATTERN ANAL, V41, P1441, DOI 10.1109/TPAMI.2018.2832628; Reso M, 2013, IEEE I CONF COMP VIS, P385, DOI 10.1109/ICCV.2013.55; Sharon E, 2000, PROC CVPR IEEE, P70, DOI 10.1109/CVPR.2000.855801; Sharon E, 2006, NATURE, V442, P810, DOI 10.1038/nature04977; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Stein A, 2007, IEEE I CONF COMP VIS, P110; Sun DQ, 2014, INT J COMPUT VISION, V106, P115, DOI 10.1007/s11263-013-0644-x; Sundberg P., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2233, DOI 10.1109/CVPR.2011.5995364; Veksler O, 2010, LECT NOTES COMPUT SC, V6315, P211, DOI 10.1007/978-3-642-15555-0_16; Wang P, 2013, INT J COMPUT VISION, V103, P1, DOI 10.1007/s11263-012-0588-6; Wei D., 2016, ADV NEURAL INFORM PR, P604; Xu CL, 2016, INT J COMPUT VISION, V119, P272, DOI 10.1007/s11263-016-0906-5; Xu CL, 2012, LECT NOTES COMPUT SC, V7577, P626, DOI 10.1007/978-3-642-33783-3_45; Yi R, 2018, PROC CVPR IEEE, P646, DOI 10.1109/CVPR.2018.00074; Yi R, 2018, IEEE IMAGE PROC, P2212, DOI 10.1109/ICIP.2018.8451659; Yu CP, 2015, IEEE I CONF COMP VIS, P3155, DOI 10.1109/ICCV.2015.361	45	2	2	2	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2021	43	9					3183	3195		10.1109/TPAMI.2020.2979714	http://dx.doi.org/10.1109/TPAMI.2020.2979714			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TU6DH	32167886	Green Accepted			2022-12-18	WOS:000681124300025
J	Asano, Y; Zheng, YQ; Nishino, K; Sato, I				Asano, Yuta; Zheng, Yinqiang; Nishino, Ko; Sato, Imari			Depth Sensing by Near-Infrared Light Absorption in Water	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Absorption; Shape; Cameras; Optical surface waves; Surface texture; Optical imaging; Depth recovery; light absorption; multispectral imaging	STRUCTURED LIGHT	This paper introduces a novel depth recovery method based on light absorption in water. Water absorbs light at almost all wavelengths whose absorption coefficient is related to the wavelength. Based on the Beer-Lambert model, we introduce a bispectral depth recovery method that leverages the light absorption difference between two near-infrared wavelengths captured with a distant point source and orthographic cameras. Through extensive analysis, we show that accurate depth can be recovered irrespective of the surface texture and reflectance, and introduce algorithms to correct for nonidealities of a practical implementation including tilted light source and camera placement, nonideal bandpass filters and the perspective effect of the camera with a diverging point light source. We construct a coaxial bispectral depth imaging system using low-cost off-the-shelf hardware and demonstrate its use for recovering the shapes of complex and dynamic objects in water. We also present a trispectral variant to further improve robustness to extremely challenging surface reflectance. Experimental results validate the theory and practical implementation of this novel depth recovery paradigm, which we refer to as shape from water.	[Asano, Yuta] Tokyo Inst Technol, Sch Engn, Meguro, Tokyo 1528550, Japan; [Zheng, Yinqiang; Sato, Imari] Natl Inst Informat, Chiyoda City, Tokyo 1018430, Japan; [Nishino, Ko] Kyoto Univ, Dept Intelligence Sci & Technol, Kyoto 6068501, Japan	Tokyo Institute of Technology; Research Organization of Information & Systems (ROIS); National Institute of Informatics (NII) - Japan; Kyoto University	Zheng, YQ (corresponding author), Natl Inst Informat, Chiyoda City, Tokyo 1018430, Japan.	asano.y.ac@m.titech.ac.jp; yqzheng@nii.ac.jp; kon@i.kyoto-u.ac.jp; imarik@nii.ac.jp			JSPS KAKENHI [18J15114, 17K20143, JP15H05918]; JST ACT-I Grant [JPMJPR16UI]; Office of Naval Research [N00014-16-1-2158 (N00014-14-1-0316)]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); JST ACT-I Grant; Office of Naval Research(Office of Naval Research)	This work was supported in part by JSPS KAKENHI Grant No. 18J15114 to Y.A., JST ACT-I Grant No. JPMJPR16UI to Y.Z., the Office of Naval Research grant N00014-16-1-2158 (N00014-14-1-0316) and JSPS KAKENHI 17K20143 to K.N., and JSPS KAKENHI Grant No. JP15H05918 to I.S.	Akkaynak D, 2018, PROC CVPR IEEE, P6723, DOI 10.1109/CVPR.2018.00703; Akkaynak D, 2017, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.2017.68; Asano Y, 2016, LECT NOTES COMPUT SC, V9910, P635, DOI 10.1007/978-3-319-46466-4_38; Batlle J, 1998, PATTERN RECOGN, V31, P963, DOI 10.1016/S0031-3203(97)00074-5; Berman D., 2017, P BRIT MACH VIS C BM, V1, P1; Chiang JY, 2012, IEEE T IMAGE PROCESS, V21, P1756, DOI 10.1109/TIP.2011.2179666; Dancu A., 2014, SIGGRAPH ASIA 2014 T, P1; Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671; Hansard Miles, 2012, TIME FLIGHT CAMERAS, P2; Hartley R., 2003, MULTIPLE VIEW GEOMET; He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]; Kratz L, 2009, IEEE I CONF COMP VIS, P1701, DOI 10.1109/ICCV.2009.5459382; Murez Z, 2015, IEEE I CONF COMP VIS, P3415, DOI 10.1109/ICCV.2015.390; Narasimhan SG, 2005, IEEE I CONF COMP VIS, P420; Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723; Nishino K, 2012, INT J COMPUT VISION, V98, P263, DOI 10.1007/s11263-011-0508-1; Qian YM, 2018, LECT NOTES COMPUT SC, V11207, P776, DOI 10.1007/978-3-030-01219-9_46; Reinhard E., 2008, COLOR IMAGING FUNDAM; Schechner YY, 2001, PROC CVPR IEEE, P325; Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Yano T, 2013, IPSJ T CVA, V5, P65, DOI DOI 10.2197/ipsjtcva.5.65; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284	23	2	3	3	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG 1	2021	43	8					2611	2622		10.1109/TPAMI.2020.2973986	http://dx.doi.org/10.1109/TPAMI.2020.2973986			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TF2YV	32078532				2022-12-18	WOS:000670578800007
J	Kim, S; Min, D; Lin, S; Sohn, K				Kim, Seungryong; Min, Dongbo; Lin, Stephen; Sohn, Kwanghoon			Dense Cross-Modal Correspondence Estimation With the Deep Self-Correlation Descriptor	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Strain; Lighting; Estimation; Benchmark testing; Imaging; Robustness; Visualization; Cross-modal correspondence; pyramidal structure; self-correlation; local self-similarity; non-rigid deformation	REGISTRATION; IMAGES	We present the deep self-correlation (DSC) descriptor for establishing dense correspondences between images taken under different imaging modalities, such as different spectral ranges or lighting conditions. We encode local self-similar structure in a pyramidal manner that yields both more precise localization ability and greater robustness to non-rigid image deformations. Specifically, DSC first computes multiple self-correlation surfaces with randomly sampled patches over a local support window, and then builds pyramidal self-correlation surfaces through average pooling on the surfaces. The feature responses on the self-correlation surfaces are then encoded through spatial pyramid pooling in a log-polar configuration. To better handle geometric variations such as scale and rotation, we additionally propose the geometry-invariant DSC (GI-DSC) that leverages multi-scale self-correlation computation and canonical orientation estimation. In contrast to descriptors based on deep convolutional neural networks (CNNs), DSC and GI-DSC are training-free (i.e., handcrafted descriptors), are robust to cross-modality, and generalize well to various modality variations. Extensive experiments demonstrate the state-of-the-art performance of DSC and GI-DSC on challenging cases of cross-modal image pairs having photometric and/or geometric variations.	[Kim, Seungryong] Ecole Polytech Fed Lausanne EPFL, Sch Comp & Commun Sci IC, CH-1015 Lausanne, Switzerland; [Kim, Seungryong] Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea; [Min, Dongbo] Ewha Womans Univ, Dept Comp Sci & Engn, Seoul 03760, South Korea; [Lin, Stephen] Microsoft Res Asia, Beijing 100080, Peoples R China; [Sohn, Kwanghoon] Yonsei Univ, Sch Elect & Elect Engn, Seoul 120749, South Korea	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Korea University; Ewha Womans University; Microsoft; Microsoft Research Asia; Yonsei University	Sohn, K (corresponding author), Yonsei Univ, Sch Elect & Elect Engn, Seoul 120749, South Korea.	seungryong_kim@korea.ac.kr; dbmin@ewha.ac.kr; stevelin@microsoft.com; khsohn@yonsei.ac.kr		Min, Dongbo/0000-0003-4825-5240	Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) - Ministry of Science and ICT [NRF-2017M3C4A7069370]; Yonsei University Research Fund (Yonsei Frontier Lab. Young Researcher Supporting Program) of 2018; R&D program for Advanced Integrated-intelligence for IDentification (AIID) through the National Research Foundation of Korea (NRF) - Ministry of Science and ICT [2018M3E3A1057303]	Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) - Ministry of Science and ICT; Yonsei University Research Fund (Yonsei Frontier Lab. Young Researcher Supporting Program) of 2018; R&D program for Advanced Integrated-intelligence for IDentification (AIID) through the National Research Foundation of Korea (NRF) - Ministry of Science and ICT	This work was supported by Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT (NRF-2017M3C4A7069370). The work of S. Kim was supported in part by the Yonsei University Research Fund (Yonsei Frontier Lab. Young Researcher Supporting Program) of 2018. The work of D. Min was supported by the R&D program for Advanced Integrated-intelligence for IDentification (AIID) through the National Research Foundation of Korea (NRF) funded by Ministry of Science and ICT (2018M3E3A1057303).	Aguilera CA, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17040873; Aguilera CA, 2016, IEEE COMPUT SOC CONF, P267, DOI 10.1109/CVPRW.2016.40; Barnes C, 2010, LECT NOTES COMPUT SC, V6313, P29; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Brown M, 2011, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2011.5995637; Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222; Chatfield Ken, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P264, DOI 10.1109/ICCVW.2009.5457691; Donahue J, 2014, PR MACH LEARN RES, V32; Dong JM, 2015, PROC CVPR IEEE, P5097, DOI 10.1109/CVPR.2015.7299145; Fischer P, 2014, ARXIV14055769; Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964; Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26; Grauman K, 2005, IEEE I CONF COMP VIS, P1458; HaCohen Y, 2013, IEEE I CONF COMP VIS, P2384, DOI 10.1109/ICCV.2013.296; Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948; Hassner T, 2012, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2012.6247842; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; Heinrich MP, 2012, MED IMAGE ANAL, V16, P1423, DOI 10.1016/j.media.2012.05.008; Heo YS, 2013, IEEE T PATTERN ANAL, V35, P1094, DOI 10.1109/TPAMI.2012.167; Heo YS, 2011, IEEE T PATTERN ANAL, V33, P807, DOI 10.1109/TPAMI.2010.136; Hur J, 2015, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR.2015.7298745; Hwang S, 2015, PROC CVPR IEEE, P1037, DOI 10.1109/CVPR.2015.7298706; Irani M, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P959, DOI 10.1109/ICCV.1998.710832; Kim J, 2013, PROC CVPR IEEE, P2307, DOI 10.1109/CVPR.2013.299; Kim S, 2017, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2017.73; Kim S, 2017, IEEE T PATTERN ANAL, V39, P1712, DOI 10.1109/TPAMI.2016.2615619; Kim S, 2016, LECT NOTES COMPUT SC, V9912, P679, DOI 10.1007/978-3-319-46484-8_41; Kim S, 2015, PROC CVPR IEEE, P2103, DOI 10.1109/CVPR.2015.7298822; Kokkinos I., 2008, 2008 IEEE C COMP VIS, P1, DOI DOI 10.1109/CVPR.2008.4587798; Krishnan D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531402; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Lee HS, 2013, PROC CVPR IEEE, P273, DOI 10.1109/CVPR.2013.42; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu JB, 2013, PROC CVPR IEEE, P1854, DOI 10.1109/CVPR.2013.242; Min DB, 2014, IEEE T IMAGE PROCESS, V23, P5638, DOI 10.1109/TIP.2014.2366600; Palmero C, 2016, INT J COMPUT VISION, V118, P217, DOI 10.1007/s11263-016-0901-x; Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777; Pinggera P, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.26; Pluim JPW, 2003, IEEE T MED IMAGING, V22, P986, DOI 10.1109/TMI.2003.815867; Qiu WC, 2014, IEEE WINT CONF APPL, P1112, DOI 10.1109/WACV.2014.6835734; Saleem S, 2014, IEEE SIGNAL PROC LET, V21, P400, DOI 10.1109/LSP.2014.2304073; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Seidenari L, 2014, IEEE T PATTERN ANAL, V36, P1033, DOI 10.1109/TPAMI.2013.232; Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366222; Shechtman E, 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383198; Shen XY, 2014, LECT NOTES COMPUT SC, V8692, P309, DOI 10.1007/978-3-319-10593-2_21; Simo-Serra E, 2015, IEEE I CONF COMP VIS, P118, DOI 10.1109/ICCV.2015.22; Simo-Serra E, 2015, INT J COMPUT VISION, V115, P136, DOI 10.1007/s11263-015-0805-1; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Simonyan K, 2014, IEEE T PATTERN ANAL, V36, P1573, DOI 10.1109/TPAMI.2014.2301163; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Tau M, 2016, IEEE T PATTERN ANAL, V38, P875, DOI 10.1109/TPAMI.2015.2474356; Tian YR, 2017, PROC CVPR IEEE, P6128, DOI 10.1109/CVPR.2017.649; Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77; Torabi A, 2013, PATTERN RECOGN, V46, P578, DOI 10.1016/j.patcog.2012.07.026; Trulls E, 2013, PROC CVPR IEEE, P2890, DOI 10.1109/CVPR.2013.372; Trzcinski T, 2015, IEEE T PATTERN ANAL, V37, P597, DOI 10.1109/TPAMI.2014.2343961; Wang ZH, 2011, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2011.6126294; Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175; Xu JT, 2016, INT J COMPUT VISION, V119, P179, DOI 10.1007/s11263-016-0886-5; Yan Q, 2013, IEEE I CONF COMP VIS, P1537, DOI 10.1109/ICCV.2013.194; Yang HS, 2014, PROC CVPR IEEE, P3406, DOI 10.1109/CVPR.2014.435; Ye YX, 2014, ISPRS J PHOTOGRAMM, V90, P83, DOI 10.1016/j.isprsjprs.2014.01.009; Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28; Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064; Zbontar J., 2016, J MACH LEARN RES, V17, P2	68	2	2	3	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2021	43	7					2345	2359		10.1109/TPAMI.2020.2965528	http://dx.doi.org/10.1109/TPAMI.2020.2965528			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UL3FK	31940519				2022-12-18	WOS:000692540900013
J	Saragadam, V; DeZeeuw, M; Baraniuk, RG; Veeraraghavan, A; Sankaranarayanan, AC				Saragadam, Vishwanath; DeZeeuw, Michael; Baraniuk, Richard G.; Veeraraghavan, Ashok; Sankaranarayanan, Aswin C.			SASSI - Super-Pixelated Adaptive Spatio-Spectral Imaging	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Computational photography; hyperspectral imaging; adaptive imaging; hyperspectral fusion; superpixels	DYNAMIC-RANGE; RESOLUTION; DESIGN	We introduce a novel video-rate hyperspectral imager with high spatial, temporal and spectral resolutions. Our key hypothesis is that spectral profiles of pixels within each super-pixel tend to be similar. Hence, a scene-adaptive spatial sampling of a hyperspectral scene, guided by its super-pixel segmented image, is capable of obtaining high-quality reconstructions. To achieve this, we acquire an RGB image of the scene, compute its super-pixels, from which we generate a spatial mask of locations where we measure high-resolution spectrum. The hyperspectral image is subsequently estimated by fusing the RGB image and the spectral measurements using a learnable guided filtering approach. Due to low computational complexity of the superpixel estimation step, our setup can capture hyperspectral images of the scenes with little overhead over traditional snapshot hyperspectral cameras, but with significantly higher spatial and spectral resolutions. We validate the proposed technique with extensive simulations as well as a lab prototype that measures hyperspectral video at a spatial resolution of 600 x 900 pixels, at a spectral resolution of 10 nm over visible wavebands, and achieving a frame rate at 18fps.	[Saragadam, Vishwanath; Baraniuk, Richard G.; Veeraraghavan, Ashok] Rice Univ, ECE Dept, Houston, TX 77025 USA; [DeZeeuw, Michael; Sankaranarayanan, Aswin C.] Carnegie Mellon Univ, ECE Dept, Pittsburgh, PA 15213 USA	Rice University; Carnegie Mellon University	Saragadam, V (corresponding author), Rice Univ, ECE Dept, Houston, TX 77025 USA.	saswin@andrew.cmu.edu; mdezeeuw2@andrew.cmu.edu; richb@rice.edu; vashok@rice.edu; saswin@andrew.cmu.edu	Saragadam, Vishwanath/AAG-7114-2020	Saragadam, Vishwanath/0000-0001-8028-7520; Sankaranarayanan, Aswin/0000-0003-0906-4046; Baraniuk, Richard/0000-0002-0721-8999; Veeraraghavan, Ashok/0000-0001-5043-7460	National Geospatial Intelligence Agency through Academic Research Program [HM0476-17-1-2000]; NSF SaTC [1801382]; NSF Expeditions in Computing [1730147, 1730574]	National Geospatial Intelligence Agency through Academic Research Program; NSF SaTC; NSF Expeditions in Computing(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE))	This work was supported in part by the National Geospatial Intelligence Agency through Academic Research Program under Grant HM0476-17-1-2000, in part by the NSF SaTC under Grant 1801382, and in part by the NSF Expeditions in Computing under Grants 1730147 and 1730574.	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Arad B, 2016, LECT NOTES COMPUT SC, V9911, P19, DOI 10.1007/978-3-319-46478-7_2; Arce GR, 2014, IEEE SIGNAL PROC MAG, V31, P105, DOI 10.1109/MSP.2013.2278763; Arguello H, 2013, IEEE T IMAGE PROCESS, V22, P941, DOI 10.1109/TIP.2012.2222899; Bayer B. E., 1976, U.S. Patent, Patent No. [3,971,065, 3971065, 3 971 065]; Bioucas-Dias JM, 2007, IEEE T IMAGE PROCESS, V16, P2992, DOI 10.1109/TIP.2007.909319; Bub G, 2010, NAT METHODS, V7, P209, DOI [10.1038/NMETH.1429, 10.1038/nmeth.1429]; Cao X, 2011, PROC CVPR IEEE, P297, DOI 10.1109/CVPR.2011.5995418; Chakrabarti A, 2011, PROC CVPR IEEE, P193, DOI 10.1109/CVPR.2011.5995660; Choi I, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130810; Cloutis EA, 1996, INT J REMOTE SENS, V17, P2215, DOI 10.1080/01431169608948770; Fang XJ, 2014, PROC SPIE, V9273, DOI 10.1117/12.2071445; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Goel M, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP 2015), P145, DOI 10.1145/2750858.2804282; Gruev V, 2010, OPT EXPRESS, V18, P19087, DOI 10.1364/OE.18.019087; He KM, 2010, LECT NOTES COMPUT SC, V6311, P1; He W., 2020, ARXIV201215104; Hirsch M, 2014, IEEE INT CONF COMPUT; Hui Z, 2018, PROC CVPR IEEE, P6209, DOI 10.1109/CVPR.2018.00650; Kauvar I, 2015, ACM T GRAPHIC, V34, DOI [10.1145/2682631, 10.1145/2816795.2818070]; Kawakami R., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2329, DOI 10.1109/CVPR.2011.5995457; Kim A, FASTSLIC OPTIMIZED S; Kittle D, 2010, APPL OPTICS, V49, P6824, DOI 10.1364/AO.49.006824; Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96; Lichtman JW, 2005, NAT METHODS, V2, P910, DOI 10.1038/NMETH817; Lin X, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661262; Loncan L, 2015, IEEE GEOSC REM SEN M, V3, P27, DOI 10.1109/MGRS.2015.2440094; Ma CG, 2014, OPT LETT, V39, P937, DOI 10.1364/OL.39.000937; Mian A, 2012, OPT EXPRESS, V20, P10658, DOI 10.1364/OE.20.010658; Narasimhan SG, 2005, IEEE T PATTERN ANAL, V27, P518, DOI 10.1109/TPAMI.2005.76; Nayar SK, 2000, PROC CVPR IEEE, P472, DOI 10.1109/CVPR.2000.855857; Ng, 2005, LIGHT FIELD PHOTOGRA; O'Toole M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866165; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Rueda H, 2017, EUR SIGNAL PR CONF, P463, DOI 10.23919/EUSIPCO.2017.8081250; Rueda H, 2016, APPL OPTICS, V55, P9584, DOI 10.1364/AO.55.009584; Saragadam V., SASSI SUPER PIXELATE; Saragadam V, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3345553; Wagadarikar A, 2008, APPL OPTICS, V47, pB44, DOI 10.1364/AO.47.000B44; Wang LZ, 2015, PROC CVPR IEEE, P4942, DOI 10.1109/CVPR.2015.7299128; Wu HK, 2018, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2018.00197; Yang CS, 2020, PHYS REV LETT, V124, DOI 10.1103/PhysRevLett.124.023902; Yasuma F, 2010, IEEE T IMAGE PROCESS, V19, P2241, DOI 10.1109/TIP.2010.2046811; Zhi TC, 2019, PROC CVPR IEEE, P8691, DOI 10.1109/CVPR.2019.00890	44	2	2	2	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2021	43	7					2233	2244		10.1109/TPAMI.2021.3075228	http://dx.doi.org/10.1109/TPAMI.2021.3075228			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	SQ8XR	33891546	Green Submitted			2022-12-18	WOS:000660633500001
J	Jie, ZQ; Sun, P; Li, X; Feng, JS; Liu, W				Jie, Zequn; Sun, Peng; Li, Xin; Feng, Jiashi; Liu, Wei			Anytime Recognition with Routing Convolutional Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Routing; Neural networks; Task analysis; Benchmark testing; Computational modeling; Reinforcement learning; Dynamic neural network; fast inference; image classification; semantic segmentation		Achieving an automatic trade-off between accuracy and efficiency for a single deep neural network is highly desired in time-sensitive computer vision applications. To achieve anytime prediction, existing methods only embed fixed exits to neural networks and make the predictions with the fixed exits for all the samples (refer to the "latest-all" strategy). However, it is observed that the latest exit within a time budget does not always provide a more accurate prediction than the earlier exits for testing samples of various difficulties, making the "latest-all" strategy a sub-optimal solution. Motivated by this, we propose to improve the anytime prediction accuracy by allowing each sample to adaptively select its own optimal exit within a specific time budget. Specifically, we propose a new Routing Convolutional Network (RCN). For any given time budget, it adaptively selects the optimal layer as exit for a specific testing sample. To learn an optimal policy for sample routing, a Q-network is embedded into the RCN at each exit, considering both potential information gain and time-cost. To further boost the anytime prediction accuracy, the exits and the Q-networks are optimized alternately to mutually boost each other under the cost-sensitive environment. Apart from applying to whole image classification, RCN can also be adapted to dense prediction tasks, e.g., scene parsing, to achieve the pixel-level anytime prediction. Extensive experimental results on CIFAR-10, CIFAR-100, and ImageNet classification benchmarks, and Cityscapes scene parsing benchmark demonstrate the efficacy of the proposed RCN for anytime recognition.	[Jie, Zequn; Sun, Peng; Liu, Wei] Tencent AI Lab, Shenzhen 518057, Peoples R China; [Li, Xin] Tsinghua Univ, Deparment Elect Engn, Beijing 100084, Peoples R China; [Feng, Jiashi] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore	Tencent; Tsinghua University; National University of Singapore	Jie, ZQ (corresponding author), Tencent AI Lab, Shenzhen 518057, Peoples R China.	zequn.nus@gmail.com; pengsun000@gmail.com; lixincn2015@gmail.com; elefjia@nus.edu.sg; wl2223@columbia.edu	Feng, Jiashi/AGX-6209-2022	Liu, Wei/0000-0002-3865-8145				Benbouzid D., 2012, P 29 INT C MACH LEAR, P747; Bengio E., 2015, CONDITIONAL COMPUTAT; Bolukbasi T., 2017, 34 INT C MACH LEARN; Cai ZW, 2015, IEEE I CONF COMP VIS, P3361, DOI 10.1109/ICCV.2015.384; Chen M., 2012, P INT C ART INT STAT, P218; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Denoyer L., 2014, ARXIV14100510; Feng Nan, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P2952, DOI 10.1109/ICASSP.2014.6854141; Gao T., 2011, ADV NEURAL INFORM PR, V24, P1062; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hegde J, 2008, PROG NEUROBIOL, V84, P405, DOI 10.1016/j.pneurobio.2007.09.001; Hu H., 2017, SOLVING NEW 3D BIN P; Huang GL, 2017, IEEE ICC; Ji SH, 2007, PATTERN RECOGN, V40, P1474, DOI 10.1016/j.patcog.2006.11.008; Karayev S, 2014, PROC CVPR IEEE, P572, DOI 10.1109/CVPR.2014.80; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kusner MJ, 2014, AAAI CONF ARTIF INTE, P1939; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; Li HX, 2015, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR.2015.7299170; Li X, 2017, DECIS SUPPORT SYST, V95, P49, DOI 10.1016/j.dss.2016.12.001; Liu L, 2017, IEEE ICC; Liu ZW, 2016, LECT NOTES COMPUT SC, V9906, P229, DOI 10.1007/978-3-319-46475-6_15; Luo WH, 2020, IEEE T PATTERN ANAL, V42, P1317, DOI 10.1109/TPAMI.2019.2899570; McGill M., 2017, ARXIV170306217; Melville P., 2008, P INT C NEUR INF PRO; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Murthy VN, 2016, PROC CVPR IEEE, P2240, DOI 10.1109/CVPR.2016.246; Ramanan D., 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.21; Saligrama V., 2015, ARXIV150205925; Sheng V. S., 2006, P 23 INT C MACH LEAR, P809; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Teerapittayanon S, 2016, INT C PATT RECOG, P2464, DOI 10.1109/ICPR.2016.7900006; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Trapeznikov K, 2013, ARTIF INTELL, P581; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; WANG J, 2015, ADV NEURAL INFORM PR, P2152; Wang J, 2014, LECT NOTES COMPUT SC, V8690, P647, DOI 10.1007/978-3-319-10605-2_42; Wang X., 2017, APPL MULTIVARIATE ST; Xu Z.E., 2013, ICML 2013, P133; Zhang C., 2010, MSRTR201066	42	2	2	0	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2021	43	6					1875	1886		10.1109/TPAMI.2019.2959322	http://dx.doi.org/10.1109/TPAMI.2019.2959322			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	SA8YQ	31869778				2022-12-18	WOS:000649590200005
J	Lee, J; Kim, JH; Oh, HS				Lee, Jongmin; Kim, Jang-Hyun; Oh, Hee-Seok			Spherical Principal Curves	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Manifolds; Dimensionality reduction; Data analysis; Surface treatment; Analytical models; Data models; Shape; Dimension reduction; feature extraction; principal geodesic analysis; principal curve; spherical domain	MANIFOLDS	This paper presents a new approach for dimension reduction of data observed on spherical surfaces. Several dimension reduction techniques have been developed in recent years for non-euclidean data analysis. As a pioneer work, (Hauberg 2016) attempted to implement principal curves on Riemannian manifolds. However, this approach uses approximations to process data on Riemannian manifolds, resulting in distorted results. This study proposes a new approach to project data onto a continuous curve to construct principal curves on spherical surfaces. Our approach lies in the same line of (Hastie and Stuetzle et al. 1989) that proposed principal curves for data on euclidean space. We further investigate the stationarity of the proposed principal curves that satisfy the self-consistency on spherical surfaces. The results on the real data analysis and simulation examples show promising empirical characteristics of the proposed approach.	[Lee, Jongmin; Oh, Hee-Seok] Seoul Natl Univ, Dept Stat, Seoul 08826, South Korea; [Kim, Jang-Hyun] Seoul Natl Univ, Dept Comp Sci & Engn, Seoul 08826, South Korea	Seoul National University (SNU); Seoul National University (SNU)	Oh, HS (corresponding author), Seoul Natl Univ, Dept Stat, Seoul 08826, South Korea.	sjhsjm92@snu.ac.kr; blue378@snu.ac.kr; heeseok@stats.snu.ac.kr		Lee, Jongmin/0000-0003-1723-4615; Kim, Jang-Hyun/0000-0001-8433-2712	National Research Foundation of Korea (NRF) - Korea government [2018R1D1A1B 07042933, 2020R1A4A1018207]	National Research Foundation of Korea (NRF) - Korea government(National Research Foundation of Korea)	This work was supported by the National Research Foundation of Korea (NRF) funded by the Korea government (2018R1D1A1B 07042933; 2020R1A4A1018207). Jongmin Lee and Jang-Hyun Kim contributed equally to this work.	Bhattacharya R, 2005, ANN STAT, V33, P1225, DOI 10.1214/009053605000000093; Bhattacharya R, 2003, ANN STAT, V31, P1; Bhattacharya RN, 2012, APPL STOCH MODEL BUS, V28, P222, DOI 10.1002/asmb.910; Biau G, 2012, IEEE T INFORM THEORY, V58, P1924, DOI 10.1109/TIT.2011.2173157; Boothby W., 1986, INTRO DIFFERENTIABLE; Buss SR, 2001, ACM T GRAPHIC, V20, P95, DOI 10.1145/502122.502124; Cippitelli E, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/4351435; Duchamp T, 1996, ANN STAT, V24, P1511; Fletcher PT, 2007, SIGNAL PROCESS, V87, P250, DOI 10.1016/j.sigpro.2005.12.018; Fletcher PT, 2004, IEEE T MED IMAGING, V23, P995, DOI 10.1109/TMI.2004.831793; GRAY NH, 1980, J INT ASS MATH GEOL, V12, P173, DOI 10.1007/BF01091203; HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936; Hauberg S, 2016, IEEE T PATTERN ANAL, V38, P1915, DOI 10.1109/TPAMI.2015.2496166; Huckemann S, 2006, ADV APPL PROBAB, V38, P299, DOI 10.1239/aap/1151337073; Huckemann S, 2010, STAT SINICA, V20, P1; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Ionescu C, 2011, IEEE I CONF COMP VIS, P2220, DOI 10.1109/ICCV.2011.6126500; Jung SK, 2012, BIOMETRIKA, V99, P551, DOI 10.1093/biomet/ass022; Jung S, 2011, ANN APPL STAT, V5, P578, DOI 10.1214/10-AOAS370; Kegl B, 2000, IEEE T PATTERN ANAL, V22, P281, DOI 10.1109/34.841759; KENDALL DG, 1984, B LOND MATH SOC, V16, P81, DOI 10.1112/blms/16.2.81; Mallasto A, 2018, PROC CVPR IEEE, P5580, DOI 10.1109/CVPR.2018.00585; Mardia K. V., 2014, STAT DIRECTIONAL DAT; Mardia K.V., 1977, APPL STAT, V26, P238, DOI DOI 10.2307/2346963; Panaretos VM, 2014, J AM STAT ASSOC, V109, P424, DOI 10.1080/01621459.2013.849199; Pennec X, 2006, J MATH IMAGING VIS, V25, P127, DOI 10.1007/s10851-006-6228-4; Siddiqi K, 2008, COMPUT IMAGING VIS, V37, P1, DOI 10.1007/978-1-4020-8658-8; Srivastava A, 2002, IEEE T SIGNAL PROCES, V50, P299, DOI 10.1109/78.978385	28	2	2	1	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2021	43	6					2165	2171		10.1109/TPAMI.2020.3025327	http://dx.doi.org/10.1109/TPAMI.2020.3025327			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	SA8YQ	32956037	Green Submitted			2022-12-18	WOS:000649590200027
J	Andre, F; Kermarrec, AM; Le Scouarnec, N				Andre, Fabien; Kermarrec, Anne-Marie; Le Scouarnec, Nicolas			Quicker ADC : Unlocking the Hidden Potential of Product Quantization With SIMD	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image databases; information search and retrieval; nearest neighbor search; product quantization; SIMD	SEARCH	Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a foundation of many multimedia retrieval systems. A common approach is to rely on Product Quantization, which allows the storage of large vector databases in memory and efficient distance computations. Yet, implementations of nearest neighbor search with Product Quantization have their performance limited by the many memory accesses they perform. Following this observation, Andre et al. proposed Quick ADC with up to 6x faster implementations of PQ m x 4 product quantizers (PQ) leveraging specific SIMD instructions. Quicker ADC is a generalization of Quick ADC not limited to PQ m x 4 codes and supporting AVX-512, the latest revision of SIMD instruction set. In doing so, Quicker ADC faces the challenge of using efficiently 5,6 and 7-bit shuffles that do not align to computer bytes or words. To this end, we introduce (i) irregular product quantizers combining sub-quantizers of different granularity and (ii) split tables allowing lookup tables larger than registers. We evaluate Quicker ADC with multiple indexes including Inverted Multi-Indexes and IVF HNSW and show that it outperforms the reference optimized implementations (i.e., FAISS and polysemous codes) for numerous configurations. Finally, we release an open-source fork of FAISS enhanced with Quicker ADC.	[Kermarrec, Anne-Marie] Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland; [Kermarrec, Anne-Marie] Mediego, F-35510 Cesson Sevigne, France; [Le Scouarnec, Nicolas] Broadpeak, F-35510 Cesson Sevigne, France	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Le Scouarnec, N (corresponding author), Broadpeak, F-35510 Cesson Sevigne, France.	fabien.andre90@gmail.com; anne-marie.kermarrec@mediego.com; nlescoua@gmail.com		Le Scouarnec, Nicolas/0000-0001-9062-3508				Andre F, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P164, DOI 10.1145/3078971.3078992; Andre F, 2015, PROC VLDB ENDOW, V9, P288; Babenko A, 2017, IEEE I CONF COMP VIS, P4895, DOI 10.1109/ICCV.2017.523; Babenko A, 2016, PROC CVPR IEEE, P2055, DOI 10.1109/CVPR.2016.226; Babenko A, 2015, PROC CVPR IEEE, P4240, DOI 10.1109/CVPR.2015.7299052; Babenko A, 2015, IEEE T PATTERN ANAL, V37, P1247, DOI 10.1109/TPAMI.2014.2361319; Babenko A, 2014, PROC CVPR IEEE, P931, DOI 10.1109/CVPR.2014.124; Babenko A, 2012, PROC CVPR IEEE, P3069, DOI 10.1109/CVPR.2012.6248038; Baranchuk D, 2018, LECT NOTES COMPUT SC, V11216, P209, DOI 10.1007/978-3-030-01258-8_13; Blalock DW, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P727, DOI 10.1145/3097983.3098195; Douze M, 2018, PROC CVPR IEEE, P3646, DOI 10.1109/CVPR.2018.00384; Douze M, 2016, LECT NOTES COMPUT SC, V9906, P785, DOI 10.1007/978-3-319-46475-6_48; egou H. J, 2017, FAISS LIB EFFICIENT; Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240; Hofmann J., 2014, P 2014 WORKSH PROGR, P57, DOI DOI 10.1145/2568058.2568068; Jain H, 2017, IEEE I CONF COMP VIS, P833, DOI 10.1109/ICCV.2017.96; Jegou H, 2011, INT CONF ACOUST SPEE, P861; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Johnson Jeff, 2017, BILLION SCALE SIMILA; Klein B, 2019, PROC CVPR IEEE, P5036, DOI 10.1109/CVPR.2019.00518; Klomp A., AFEWMISSING SSE INTR; Krapac J., 2014, P INT C MULT RETR, P431; Le Scouarnec N., 2019, ARXIV190506900; Le Scouarnec N, 2018, PROCEEDINGS OF THE 2018 SYMPOSIUM ON ARCHITECTURES FOR NETWORKING AND COMMUNICATIONS SYSTEMS (ANCS '18), P41, DOI 10.1145/3230718.3232629; Lento Gregory., 2014, OPTIMIZING PERFORMAN; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Malkov YA, 2020, IEEE T PATTERN ANAL, V42, P824, DOI 10.1109/TPAMI.2018.2889473; Matsui Y, 2018, IEEE T MULTIMEDIA, V20, P1809, DOI 10.1109/TMM.2017.2774009; Mula W, 2018, COMPUT J, V61, P111, DOI 10.1093/comjnl/bxx046; Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Wang J., 2014, PROC INT C INT C MAC; Wu X., 2017, ADV NEURAL INFORM PR, P5749; Xia Y, 2013, IEEE I CONF COMP VIS, P3416, DOI 10.1109/ICCV.2013.424; Xie LX, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P3, DOI 10.1145/2671188.2749289; Yu T, 2018, LECT NOTES COMPUT SC, V11205, P191, DOI 10.1007/978-3-030-01246-5_12	36	2	2	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2021	43	5					1666	1677		10.1109/TPAMI.2019.2952606	http://dx.doi.org/10.1109/TPAMI.2019.2952606			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RJ3YD	31722477	Green Submitted			2022-12-18	WOS:000637533800013
J	Li, J; Song, YF; Zhu, JF; Cheng, LL; Su, Y; Ye, L; Yuan, PC; Han, SM				Li, Jia; Song, Yafei; Zhu, Jianfeng; Cheng, Lele; Su, Ying; Ye, Lin; Yuan, Pengcheng; Han, Shumin			Learning From Large-Scale Noisy Web Data With Ubiquitous Reweighting for Image Classification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Noise measurement; Deep learning; Task analysis; Training; Annotations; Solid modeling; Visualization; Image classification; noisy web data; CNNs; ubiquitous reweighting; deep learning		Many important advances of deep learning techniques have originated from the efforts of addressing the image classification task on large-scale datasets. However, the construction of clean datasets is costly and time-consuming since the Internet is overwhelmed by noisy images with inadequate and inaccurate tags. In this paper, we propose a Ubiquitous Reweighting Network (URNet) that can learn an image classification model from noisy web data. By observing the web data, we find that there are five key challenges, i.e., imbalanced class sizes, high intra-classes diversity and inter-class similarity, imprecise instances, insufficient representative instances, and ambiguous class labels. With these challenges in mind, we assume every training instance has the potential to contribute positively by alleviating the data bias and noise via reweighting the influence of each instance according to different class sizes, large instance clusters, its confidence, small instance bags, and the labels. In this manner, the influence of bias and noise in the data can be gradually alleviated, leading to the steadily improving performance of URNet. Experimental results in the WebVision 2018 challenge with 16 million noisy training images from 5000 classes show that our approach outperforms state-of-the-art models and ranks first place in the image classification task.	[Li, Jia; Yuan, Pengcheng] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China; [Li, Jia] Beihang Univ, Beijing Adv Innovat Ctr Big Data & Brain Comp, Beijing, Peoples R China; [Li, Jia] Peng Cheng Lab, Shenzhen 518055, Peoples R China; [Song, Yafei] Peking Univ, Sch Elect Engn & Comp Sci, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China; [Zhu, Jianfeng; Cheng, Lele; Su, Ying; Ye, Lin; Yuan, Pengcheng; Han, Shumin] Comp Vis Technol Dept Baidu, Beijing 100871, Peoples R China	Beihang University; Beihang University; Peng Cheng Laboratory; Peking University	Li, J (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Han, SM (corresponding author), Comp Vis Technol Dept Baidu, Beijing 100871, Peoples R China.	jiali@buaa.edu.cn; songyf@pku.edu.cn; zhujianfeng03@baidu.com; chenglele@baidu.com; suying02@baidu.com; yelin02@baidu.com; yuanpengcheng@buaa.edu.cn; hanshumin@baidu.com		Song, Yafei/0000-0003-3537-1015; Li, Jia/0000-0002-4346-8696	National Natural Science Foundation of China [61922006, 61672072, 61532003]; Beijing Nova Program [Z181100006218063]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Nova Program(Beijing Municipal Science & Technology Commission)	This work was supported in part by the National Natural Science Foundation of China under Grants 61922006, 61672072, and 61532003, and also supported by the Beijing Nova Program (Z181100006218063).	Alpaydin E, 2015, PATTERN RECOGN, V48, P2831, DOI 10.1016/j.patcog.2015.04.006; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bojanowski Piotr., 2017, TACL, V5, P135, DOI [10.1162/tacl_a_00051, DOI 10.1162/TACL_A_00051]; Chen CY, 2015, IEEE I CONF COMP VIS, P2722, DOI [10.1109/ICCV.2015.104, 10.1109/ICCV.2015.312]; Fu JL, 2015, IEEE I CONF COMP VIS, P1985, DOI 10.1109/ICCV.2015.230; Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Levine S, 2018, INT J ROBOT RES, V37, P421, DOI 10.1177/0278364917710318; Li WX, 2015, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2015.7299056; Li Wen, 2017, ARXIV170802862; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Ng AY, 2002, ADV NEUR IN, V14, P849; Niu L, 2018, PROC CVPR IEEE, P7689, DOI 10.1109/CVPR.2018.00802; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Richard S. Zemel, 2014, Arxiv, DOI arXiv:1411.2539; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582; Teh Eu Wern, 2016, BRIT MACH VIS C BMVC; Veit A, 2017, PROC CVPR IEEE, P6575, DOI 10.1109/CVPR.2017.696; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541; Wang YS, 2018, PROC CVPR IEEE, P8688, DOI 10.1109/CVPR.2018.00906; Wu JJ, 2015, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR.2015.7298968; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yin W., 2015, ARXIV151205193; Zhuang BH, 2017, PROC CVPR IEEE, P2915, DOI 10.1109/CVPR.2017.311	35	2	2	1	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2021	43	5					1808	1814		10.1109/TPAMI.2019.2961910	http://dx.doi.org/10.1109/TPAMI.2019.2961910			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RJ3YD	31880542	Green Submitted			2022-12-18	WOS:000637533800023
J	Zhao, YF; Li, J; Zhang, Y; Song, YF; Tia, YH				Zhao, Yifan; Li, Jia; Zhang, Yu; Song, Yafei; Tia, Yonghong			Ordinal Multi-Task Part Segmentation With Recurrent Prior Generation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature extraction; Task analysis; Shape; Automobiles; Solid modeling; Image segmentation; Semantics; Semantic part segmentation; ordinal multi-task; recurrent; part relationship		Semantic object part segmentation is a fundamental task in object understanding and geometric analysis. The clear understanding of part relationships can be of great use to the segmentation process. In this work, we propose a novel Ordinal Multi-task Part Segmentation (OMPS) approach which explicitly models the part ordinal relationship to guide the segmentation process in a recurrent manner. Quantitative and qualitative experiments are conducted first to explore the mutual impacts among object parts and then an ordinal part inference algorithm is formulated via experimental observations. Specifically, our framework is mainly composed of two modules, the forward module to segment multiple parts as individual subtasks with prior knowledge, and the recurrent module to generate appropriate part priors with the ordinal inference algorithm. These two modules work iteratively to optimize the segmentation performance and the network parameters. Experimental results show that our approach outperforms the state-of-the-art models on human and vehicle part parsing benchmarks. Comprehensive evaluations are conducted to demonstrate the effectiveness of our approach in object part segmentation.	[Zhao, Yifan; Li, Jia] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China; [Li, Jia; Tia, Yonghong] Peng Cheng Lab, Shenzhen 518055, Peoples R China; [Zhang, Yu] SenseTime Grp Ltd, Beijing 100084, Peoples R China; [Song, Yafei; Tia, Yonghong] Peking Univ, Sch Elect Engn & Comp Sci, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China	Beihang University; Peng Cheng Laboratory; Peking University	Li, J (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.	zhaoyf@buaa.edu.cn; jiali@buaa.edu.cn; zhangyulb@gmail.com; songyf@pku.edu.cn; yhtian@pku.edu.cn		Li, Jia/0000-0002-4346-8696; Song, Yafei/0000-0003-3537-1015; Zhao, Yifan/0000-0002-5691-013X	National Key R&D Program of China [2017YFB1002400]; National Natural Science Foundation of China [61672072, 61922006, 61825101, 61532003]; Beijing Nova Program [Z181100006218063]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Nova Program(Beijing Municipal Science & Technology Commission)	This work is partially supported by grants from the National Key R&D Program of China under Grant 2017YFB1002400, the National Natural Science Foundation of China under Grants 61672072, 61922006, 61825101 and 61532003, and also supported by Beijing Nova Program(Z181100006218063).	Azizpour H, 2012, LECT NOTES COMPUT SC, V7572, P836, DOI 10.1007/978-3-642-33718-5_60; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chang Angel X., 2015, ARXIV151203012CSGR P; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254; Chu X, 2016, PROC CVPR IEEE, P4715, DOI 10.1109/CVPR.2016.510; Cipoll Roberto, 2008, PROC CVPR IEEE, P1; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dong J, 2014, PROC CVPR IEEE, P843, DOI 10.1109/CVPR.2014.113; Eslami S.M.A, 2012, ADV NEURAL INFORM PR, V25, P100; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Fang HS, 2018, PROC CVPR IEEE, P70, DOI 10.1109/CVPR.2018.00015; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314; Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hejrati M., 2012, NIPS; Khelifi F, 2010, IEEE T IMAGE PROCESS, V19, P981, DOI 10.1109/TIP.2009.2038637; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Ladicky L, 2010, LECT NOTES COMPUT SC, V6314, P424, DOI 10.1007/978-3-642-15561-1_31; Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063; Liang XD, 2017, PROC CVPR IEEE, P2175, DOI 10.1109/CVPR.2017.234; Liang XD, 2016, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2016.347; Liang XD, 2016, LECT NOTES COMPUT SC, V9905, P125, DOI 10.1007/978-3-319-46448-0_8; Liang XD, 2015, IEEE I CONF COMP VIS, P1386, DOI 10.1109/ICCV.2015.163; Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526; Long MS, 2017, ADV NEUR IN, V30; Lu W., 2014, P BRIT MACH VIS C BM; Nie XC, 2018, LECT NOTES COMPUT SC, V11209, P519, DOI 10.1007/978-3-030-01228-1_31; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Prechelt L, 1998, NEURAL NETWORKS, V11, P761, DOI 10.1016/S0893-6080(98)00010-0; Ruvolo P., 2013, AAAI C ARTIFICIAL IN, P862; Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Song YF, 2017, IEEE I CONF COMP VIS, P580, DOI 10.1109/ICCV.2017.70; Sturgess P., 2009, BRIT MACH VIS C BMVC; Wang HY, 2015, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2015.7298788; Wang P, 2015, IEEE I CONF COMP VIS, P1573, DOI 10.1109/ICCV.2015.184; Wang Y, 2012, J MACH LEARN RES, V13, P3075; Xia FT, 2016, AAAI CONF ARTIF INTE, P3632; Xia FT, 2017, PROC CVPR IEEE, P6080, DOI 10.1109/CVPR.2017.644; Xia FT, 2016, LECT NOTES COMPUT SC, V9909, P648, DOI 10.1007/978-3-319-46454-1_39; Yamaguchi K, 2013, IEEE I CONF COMP VIS, P3519, DOI 10.1109/ICCV.2013.437; Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101; Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741; Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zhao J, 2017, IEEE COMPUT SOC CONF, P1595, DOI 10.1109/CVPRW.2017.204; Zhu L, 2008, LECT NOTES COMPUT SC, V5303, P759	64	2	2	2	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2021	43	5					1636	1648		10.1109/TPAMI.2019.2953854	http://dx.doi.org/10.1109/TPAMI.2019.2953854			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RJ3YD	31751267				2022-12-18	WOS:000637533800011
J	Agarwal, S; Pryhuber, A; Thomas, RR				Agarwal, Sameer; Pryhuber, Andrew; Thomas, Rekha R.			Ideals of the Multiview Variety	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cameras; Algebra; Optimization; Computer vision; Kernel; Terminology; Google; Projective geometry; structure-from-motion; nonlinear algebra		The multiview variety of an arrangement of cameras is the Zariski closure of the images of world points in the cameras. The prime vanishing ideal of this complex projective variety is called the multiview ideal. We show that the bifocal and trifocal polynomials from the cameras generate the multiview ideal when the foci are distinct. In the computer vision literature, many sets of (determinantal) polynomials have been proposed to describe the multiview variety. We establish precise algebraic relationships between the multiview ideal and these various ideals. When the camera foci are noncoplanar, we prove that the ideal of bifocal polynomials saturate to give the multiview ideal. Finally, we prove that all the ideals we consider coincide when dehomogenized, to cut out the space of finite images.	[Agarwal, Sameer] Google Inc, Mountain View, CA 94043 USA; [Pryhuber, Andrew; Thomas, Rekha R.] Univ Washington, Dept Math, Seattle, WA 98105 USA	Google Incorporated; University of Washington; University of Washington Seattle	Agarwal, S (corresponding author), Google Inc, Mountain View, CA 94043 USA.	sameeragarwal@google.com; pryhuber@uw.edu; rrthomas@uw.edu		Agarwal, Sameer/0000-0003-0315-2272	U.S. National Science Foundation [DMS-1719538]	U.S. National Science Foundation(National Science Foundation (NSF))	The authors would like to thank the referees of this article for their careful reading and suggestions. In particular, their comments helped fill a gap in the proof of the main theorem of Section 5. Andrew Pryhuber and Rekha R. Thomas acknowledge support from the U.S. National Science Foundation through the grant DMS-1719538.	Aholt C, 2013, CAN J MATH, V65, P961, DOI 10.4153/CJM-2012-023-2; Aholt C, 2012, LECT NOTES COMPUT SC, V7572, P654, DOI 10.1007/978-3-642-33718-5_47; [Anonymous], 2012, INVITATION 3 D VISIO; Blekherman G., 2012, SEMIDEFINITE OPTIMIZ; Broida JG, 1989, COMPREHENSIVE INTRO; Conca A, 2018, J COMB ALGEBRA, V2, P231, DOI 10.4171/JCA/2-3-2; Cox D. A., 2015, IDEALS VARIETIES ALG; FAUGERAS O, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P951, DOI 10.1109/ICCV.1995.466832; Grayson Daniel R, 2002, MACAULAY 2 SOFTWARE, P4; Harris J., 1995, ALGEBRAIC GEOMETRY 1; Hartley R., 2004, ROBOTICA; Heyden A, 1997, MATH METHOD APPL SCI, V20, P1135, DOI 10.1002/(SICI)1099-1476(19970910)20:13<1135::AID-MMA908>3.0.CO;2-9; Kahl F, 2007, INT J COMPUT VISION, V74, P3, DOI 10.1007/s11263-006-0015-y; Lasserre JB, 2001, SIAM J OPTIMIZ, V11, P796, DOI 10.1137/S1052623400366802; Li BL, 2018, INT MATH RES NOTICES, V2018, P4190, DOI 10.1093/imrn/rnx003; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Marshall M., 2008, MATH SURVEYS MONOGRA, V146; MAYBANK S, 1993, THEORY RECONSTRUCTIO; Parrilo PA, 2003, MATH PROGRAM, V96, P293, DOI 10.1007/s10107-003-0387-5; Trager M., 2018, P EUR C COMP VIS; Trager M, 2015, IEEE I CONF COMP VIS, P909, DOI 10.1109/ICCV.2015.110; TRIGGS B, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P338, DOI 10.1109/ICCV.1995.466920	23	2	2	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2021	43	4					1279	1292		10.1109/TPAMI.2019.2950631	http://dx.doi.org/10.1109/TPAMI.2019.2950631			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QT3YJ	31675319	Green Submitted			2022-12-18	WOS:000626525300013
J	Barman, A; Shah, SK				Barman, Arko; Shah, Shishir K.			A Graph-Based Approach for Making Consensus-Based Decisions in Image Search and Person Re-Identification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Search problems; Image matching; Approximation algorithms; Computer vision; Shape; Image retrieval; Probes; Image matching; image retrieval; image search; person re-identification; graph theory; consensus-based decision-making; ant colony optimization	AGGREGATION; DISTANCE; FUSION	Image matching and retrieval is the underlying problem in various directions of computer vision research, such as image search, biometrics, and person re-identification. The problem involves searching for the closest match to a query image in a database of images. This work presents a method for generating a consensus amongst multiple algorithms for image matching and retrieval. The proposed algorithm, Shortest Hamiltonian Path Estimation (SHaPE), maps the process of ranking candidates based on a set of scores to a graph-theoretic problem. This mapping is extended to incorporate results from multiple sets of scores obtained from different matching algorithms. The problem of consensus-based decision-making is solved by searching for a suitable path in the graph under specified constraints using a two-step process. First, a greedy algorithm is employed to generate an approximate solution. In the second step, the graph is extended and the problem is solved by applying Ant Colony Optimization. Experiments are performed for image search and person re-identification to illustrate the efficiency of SHaPE in image matching and retrieval. Although SHaPE is presented in the context of image retrieval, it can be applied, in general, to any problem involving the ranking of candidates based on multiple sets of scores.	[Barman, Arko; Shah, Shishir K.] Univ Houston, Dept Comp Sci, Houston, TX 77204 USA	University of Houston System; University of Houston	Barman, A (corresponding author), Univ Houston, Dept Comp Sci, Houston, TX 77204 USA.	abarman@uh.edu; sshah@central.uh.edu		Barman, Arko/0000-0002-5357-5786				Aledo JA, 2013, APPL MATH COMPUT, V222, P632, DOI 10.1016/j.amc.2013.07.081; Barman A., 2018, P 15 IEEE INT C ADV, P1; Barman A, 2017, IEEE I CONF COMP VIS, P1124, DOI 10.1109/ICCV.2017.127; Barman A, 2017, 2017 IEEE INTERNATIONAL SYMPOSIUM ON TECHNOLOGIES FOR HOMELAND SECURITY (HST); Barman A, 2016, TENTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS AND IMAGE PROCESSING (ICVGIP 2016), DOI 10.1145/3009977.3010018; Barras C, 2003, INT CONF ACOUST SPEE, P49; Bedagkar-Gala A, 2014, IMAGE VISION COMPUT, V32, P270, DOI 10.1016/j.imavis.2014.02.001; Blum C, 2005, PHYS LIFE REV, V2, P353, DOI 10.1016/j.plrev.2005.10.001; Brandt J, 2010, PROC CVPR IEEE, P1815, DOI 10.1109/CVPR.2010.5539852; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dang Y, 2011, APPL INTEGER PROGRAM; Prates RFD, 2015, IEEE IMAGE PROC, P1975, DOI 10.1109/ICIP.2015.7351146; DENEUBOURG JL, 1990, J INSECT BEHAV, V3, P159, DOI 10.1007/BF01417909; Diestel R., 2012, GRADUATE TEXTS MATH, V173; Do TT, 2018, IEEE T PATTERN ANAL, V40, P626, DOI 10.1109/TPAMI.2017.2686861; Dorigo M., 1997, IEEE Transactions on Evolutionary Computation, V1, P53, DOI 10.1109/4235.585892; Dwork C., 2001, P 10 INT C WORLD WID, P613, DOI [10.1145/371920.372165, DOI 10.1145/371920.372165]; Eisenbach M, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P184, DOI 10.1109/AVSS.2012.81; Farah Mohamed, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P591, DOI 10.1145/1277741.1277843; Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926; Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15; Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21; Hajebi K., 2011, PROC INT JOINT C ART, P1312, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-222; Harwood B, 2016, PROC CVPR IEEE, P5713, DOI 10.1109/CVPR.2016.616; Hu Y, 2013, IEEE COMPUT SOC CONF, P794, DOI 10.1109/CVPRW.2013.119; HUISKES MJ, 2010, P INT C MULT INF RET, P527; Jain A, 2005, PATTERN RECOGN, V38, P2270, DOI 10.1016/j.patcog.2005.01.012; Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24; Kalantidis Y, 2014, PROC CVPR IEEE, P2329, DOI 10.1109/CVPR.2014.298; Nguyen K, 2015, IEEE T HUM-MACH SYST, V45, P132, DOI 10.1109/THMS.2014.2361437; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li W., 2013, LNCS, V7724, P31, DOI [10.1007/978-3-642-37331-2, DOI 10.1007/978-3-642-37331-2]; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Liu H, 2019, IEEE T PATTERN ANAL, V41, P941, DOI 10.1109/TPAMI.2018.2819978; Liu W, 2011, PROC CVPR IEEE, P849, DOI 10.1109/CVPR.2011.5995315; Liu XL, 2016, IEEE T IMAGE PROCESS, V25, P907, DOI 10.1109/TIP.2015.2505180; Liu ZQ, 2017, IEEE T IMAGE PROCESS, V26, P3128, DOI 10.1109/TIP.2017.2660244; Loy CC, 2010, INT J COMPUT VISION, V90, P106, DOI 10.1007/s11263-010-0347-5; Mahalanobis PC, 2018, SANKHYA SER A, V80, P1, DOI 10.1007/s13171-019-00164-5; Mai L, 2017, PROC CVPR IEEE, P1121, DOI 10.1109/CVPR.2017.125; Malkov YA, 2020, IEEE T PATTERN ANAL, V42, P824, DOI 10.1109/TPAMI.2018.2889473; Manmatha R., 2001, SIGIR Forum, P267; Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152; Nister D, 2006, IEEE COMP SOC C COMP, V2, P2161, DOI DOI 10.1109/CVPR.2006.264; Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Piras L, 2017, INFORM FUSION, V37, P50, DOI 10.1016/j.inffus.2017.01.003; Scheirer W, 2010, LECT NOTES COMPUT SC, V6313, P481; Snelick R, 2005, IEEE T PATTERN ANAL, V27, P450, DOI 10.1109/TPAMI.2005.57; Stutzle T, 2004, ANT COLONY OPTIMIZAT; Tan SB, 2018, IEEE T CIRC SYST VID, V28, P356, DOI 10.1109/TCSVT.2016.2555739; Tolias G, 2016, INT J COMPUT VISION, V116, P247, DOI 10.1007/s11263-015-0810-4; van de Weijer J, 2007, PROC CVPR IEEE, P1898; Wang M, 2012, IEEE T IMAGE PROCESS, V21, P4649, DOI 10.1109/TIP.2012.2207397; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Wu PC, 2016, IEEE T KNOWL DATA EN, V28, P454, DOI 10.1109/TKDE.2015.2477296; Wu SH, 2019, INT J PAVEMENT ENG, V20, P33, DOI 10.1080/10298436.2016.1248204; Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113; Zhang ST, 2015, IEEE T PATTERN ANAL, V37, P803, DOI 10.1109/TPAMI.2014.2346201; Zhang SL, 2015, IEEE T PATTERN ANAL, V37, P2573, DOI 10.1109/TPAMI.2015.2417573; Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349; Zhao R, 2017, IEEE T PATTERN ANAL, V39, P356, DOI 10.1109/TPAMI.2016.2544310; Zheng L, 2015, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2015.7298783; Zheng L, 2014, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2014.250	65	2	2	3	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2021	43	3					753	765		10.1109/TPAMI.2019.2944597	http://dx.doi.org/10.1109/TPAMI.2019.2944597			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QE6IS	31567073				2022-12-18	WOS:000616309900001
J	Bahonar, H; Mirzaei, A; Sadri, S; Wilson, RC				Bahonar, Hoda; Mirzaei, Abdolreza; Sadri, Saeed; Wilson, Richard C.			Graph Embedding Using Frequency Filtering	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Spectral graph embedding; graph Fourier transform; heat kernel; frequency filtering; graph classification		The target of graph embedding is to embed graphs in vector space such that the embedded feature vectors follow the differences and similarities of the source graphs. In this paper, a novel method named Frequency Filtering Embedding (FFE) is proposed which uses graph Fourier transform and Frequency filtering as a graph Fourier domain operator for graph feature extraction. Frequency filtering amplifies or attenuates selected frequencies using appropriate filter functions. Here, heat, anti-heat, part-sine and identity filter sets are proposed as the filter functions. A generalized version of FFE named GeFFE is also proposed by defining pseudo-Fourier operators. This method can be considered as a general framework for formulating some previously defined invariants in other works by choosing a suitable filter bank and defining suitable pseudo-Fourier operators. This flexibility empowers GeFFE to adapt itself to the properties of each graph dataset unlike the previous spectral embedding methods and leads to superior classification accuracy relative to the others. Utilizing the proposed part-sine filter set, which its members filter different parts of the spectrum in turn, improves the classification accuracy of GeFFE method. Additionally, GeFFE resolves the cospectrality problem entirely in tested datasets.	[Bahonar, Hoda; Mirzaei, Abdolreza; Sadri, Saeed] Isfahan Univ Technol, Dept Elect & Comp Engn, Esfahan 8415683111, Iran; [Wilson, Richard C.] Univ York, Dept Comp Sci, York YO10 5DD, N Yorkshire, England	Isfahan University of Technology; University of York - UK	Bahonar, H (corresponding author), Isfahan Univ Technol, Dept Elect & Comp Engn, Esfahan 8415683111, Iran.	h.bahonar@ec.iut.ac.ir; mirzaei@cc.iut.ac.ir; sadri@cc.iut.ac.ir; Richard.Wilson@york.ac.uk		Wilson, Richard Charles/0000-0001-7265-3033				Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396; Aziz F, 2013, IEEE T NEUR NET LEAR, V24, P977, DOI 10.1109/TNNLS.2013.2248093; Bahonar H, 2018, PATTERN RECOGN, V74, P518, DOI 10.1016/j.patcog.2017.09.030; Bai L, 2015, PATTERN RECOGN, V48, P344, DOI 10.1016/j.patcog.2014.03.028; Bai L, 2014, PATTERN RECOGN, V47, P1172, DOI 10.1016/j.patcog.2013.09.010; Bishop C.M, 2006, PATTERN RECOGN; Bonev B, 2013, COMPUT VIS IMAGE UND, V117, P214, DOI 10.1016/j.cviu.2012.11.007; Borzeshi EZ, 2013, PATTERN RECOGN, V46, P1648, DOI 10.1016/j.patcog.2012.11.020; Bracewell R., 1986, FOURIER TRANSFORM IT; BRAULT JW, 1971, ASTRON ASTROPHYS, V13, P169; Bunke H., 2001, Advances in Pattern Recognition - ICAPR 2001. Second International Conference. Proceedings (Lecture Notes in Computer Science Vol.2013), P1; Bunke H, 2012, PATTERN RECOGN LETT, V33, P811, DOI 10.1016/j.patrec.2011.04.017; Bunke H, 2011, PATTERN RECOGN, V44, P1057, DOI 10.1016/j.patcog.2010.11.015; Chung FR, 1997, SPECTRAL GRAPH THEOR, V92; Crovella M, 2003, IEEE INFOCOM SER, P1848; DEBNATH AK, 1991, J MED CHEM, V34, P786, DOI 10.1021/jm00106a046; Duin RPW, 2012, PATTERN RECOGN LETT, V33, P826, DOI 10.1016/j.patrec.2011.04.019; El-Ghawalby H, 2009, LECT NOTES COMPUT SC, V5654, P124, DOI 10.1007/978-3-642-03596-8_8; Escolano F, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.036206; Fischer A., 2008, 19 INT C PATT REC 20, P1; Fukunaga K., 2013, INTRO STAT PATTERN R; Gartner T, 2003, LECT NOTES ARTIF INT, V2777, P129, DOI 10.1007/978-3-540-45167-9_11; Geng Li, 2012, Statistical Analysis and Data Mining, V5, P265, DOI 10.1002/sam.11153; Gibert J, 2012, PATTERN RECOGN LETT, V33, P1980, DOI 10.1016/j.patrec.2012.03.017; Gibert J, 2012, PATTERN RECOGN, V45, P3072, DOI 10.1016/j.patcog.2012.01.009; Gunter S, 2002, PATTERN RECOGN LETT, V23, P405, DOI 10.1016/S0167-8655(01)00173-8; Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005; Haussler D., 1999, CONVOLUTIONS KERNELS; Kim WS, 2012, INT CONF ACOUST SPEE, P813, DOI 10.1109/ICASSP.2012.6288008; Kondor R.I., 2002, P 19 INT C MACHINE L, P315; Kriegel HP, 2008, LECT NOTES COMPUT SC, V5069, P150; Lim JS, 1990, 2 DIMENSIONAL SIGNAL; Luo B, 2003, PATTERN RECOGN, V36, P2213, DOI 10.1016/S0031-3203(03)00084-0; Luqman MM, 2013, PATTERN RECOGN, V46, P551, DOI 10.1016/j.patcog.2012.07.029; Mousavi SF, 2017, PATTERN RECOGN, V61, P245, DOI 10.1016/j.patcog.2016.07.043; Pekalska E, 2000, INT C PATT RECOG, P12, DOI 10.1109/ICPR.2000.906008; Ren P, 2011, IEEE T NEURAL NETWOR, V22, P233, DOI 10.1109/TNN.2010.2091969; Ren P, 2008, LECT NOTES COMPUT SC, V5342, P308; Riesen K, 2007, LECT NOTES COMPUT SC, V4472, P220; Riesen K, 2009, INT J PATTERN RECOGN, V23, P1053, DOI 10.1142/S021800140900748X; Riesen K, 2009, IMAGE VISION COMPUT, V27, P950, DOI 10.1016/j.imavis.2008.04.004; Riesen K, 2008, LECT NOTES COMPUT SC, V5342, P287; Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192; Sidere N, 2008, LECT NOTES COMPUT SC, V5342, P45, DOI 10.1007/978-3-540-89689-0_9; Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x; Vega M. Alvarez, 2011, THESIS UTAH STATE U; Wernicke S, 2006, BIOINFORMATICS, V22, P1152, DOI 10.1093/bioinformatics/btl038; Wilson RC, 2005, IEEE T PATTERN ANAL, V27, P1112, DOI 10.1109/TPAMI.2005.145; Wilson RC, 2014, INT C PATT RECOG, P100, DOI 10.1109/ICPR.2014.27; Xiao B., 2007, THESIS U YORK UK; Xiao B, 2010, IMAGE VISION COMPUT, V28, P1003, DOI 10.1016/j.imavis.2009.05.011; Xiao B, 2009, PATTERN RECOGN, V42, P2589, DOI 10.1016/j.patcog.2008.12.029; Zhang F, 2008, PATTERN RECOGN, V41, P3328, DOI 10.1016/j.patcog.2008.05.007	54	2	2	0	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2021	43	2					473	484		10.1109/TPAMI.2019.2929519	http://dx.doi.org/10.1109/TPAMI.2019.2929519			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PS2ZD	31369368	Green Accepted			2022-12-18	WOS:000607795000001
J	Kalash, M; Islam, MA; Bruce, NDB				Kalash, Mahmoud; Islam, Md Amirul; Bruce, Neil D. B.			Relative Saliency and Ranking: Models, Metrics, Data and Benchmarks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Saliency; saliency ranking; salient instance; salient object detection; relative rank; dataset; benchmark	OBJECT DETECTION	Salient object detection is a problem that has been considered in detail and many solutions have been proposed. In this paper, we argue that work to date has addressed a problem that is relatively ill-posed. Specifically, there is not universal agreement about what constitutes a salient object when multiple observers are queried. This implies that some objects are more likely to be judged salient than others, and implies a relative rank exists on salient objects. Initially, we present a novel deep learning solution based on a hierarchical representation of relative saliency and stage-wise refinement. Further to this, we present data, analysis and baseline benchmark results towards addressing the problem of salient object ranking. Methods for deriving suitable ranked salient object instances are presented, along with metrics suitable to measuring algorithm performance. In addition, we show how a derived dataset can be successively refined to provide cleaned results that correlate well with pristine ground truth in its characteristics and value for training and testing models. Finally, we provide a comparison among prevailing algorithms that address salient object ranking or detection to establish initial baselines providing a basis for comparison with future efforts addressing this problem. The source code and data are publicly available via our project page: ryersonvisionlab.github.io/cocosalrank	[Kalash, Mahmoud] Univ Manitoba, Dept Comp Sci, Winnipeg, MB R3T 2N2, Canada; [Islam, Md Amirul; Bruce, Neil D. B.] Ryerson Univ, Dept Comp Sci, Toronto, ON M5G 1M1, Canada; [Islam, Md Amirul; Bruce, Neil D. B.] Vector Inst, Toronto, ON M5G 1M1, Canada	University of Manitoba; Toronto Metropolitan University	Islam, MA (corresponding author), Ryerson Univ, Dept Comp Sci, Toronto, ON M5G 1M1, Canada.; Islam, MA (corresponding author), Vector Inst, Toronto, ON M5G 1M1, Canada.	kalashm@cs.umanitoba.ca; amirul@scs.ryerson.ca; bruce@scs.ryerson.ca	Islam, Md Amirul/GZA-9682-2022		NSERC Canada Discovery Grants program; Ryerson Graduate Fellowship; University of Manitoba GETS; UMGF fund; NVIDIA Corporation	NSERC Canada Discovery Grants program; Ryerson Graduate Fellowship; University of Manitoba GETS; UMGF fund; NVIDIA Corporation	The authors acknowledge financial support from the NSERC Canada Discovery Grants program, Ryerson Graduate Fellowship, University of Manitoba GETS and UMGF funding, and the support of the NVIDIA Corporation GPU Grant Program.	Acik A, 2014, J VISION, V14, DOI 10.1167/14.1.2; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bruce NDB, 2015, VISION RES, V116, P95, DOI 10.1016/j.visres.2015.01.010; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487; Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12; Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He SF, 2017, IEEE I CONF COMP VIS, P1059, DOI 10.1109/ICCV.2017.120; Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688; Hu P, 2017, PROC CVPR IEEE, P540, DOI 10.1109/CVPR.2017.65; Islam M. A., 2018, P BRIT MACH VIS C; Islam M. A., 2017, P BRIT MACH VIS C; Islam MA, 2018, ARXIV180611266; Islam MA, 2018, PROC CVPR IEEE, P7142, DOI 10.1109/CVPR.2018.00746; Islam Md Amirul, 2017, ARXIV170300551; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Jia SY, 2015, LECT NOTES COMPUT SC, V9491, P411, DOI 10.1007/978-3-319-26555-1_46; JIANG M, 2015, PROC CVPR IEEE, P1072, DOI DOI 10.1109/CVPR.2015.7298710; Kim J, 2014, PROC CVPR IEEE, P883, DOI 10.1109/CVPR.2014.118; Koehler K, 2014, J VISION, V14, DOI 10.1167/14.3.14; Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78; Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58; Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184; Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306; Li X, 2013, IEEE I CONF COMP VIS, P3328, DOI 10.1109/ICCV.2013.413; Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370; Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80; Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698; Najibi M, 2018, IEEE WINT CONF APPL, P1432, DOI 10.1109/WACV.2018.00161; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rolnick D., 2017, ARXIV170510694; Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960; Shi KY, 2013, PROC CVPR IEEE, P2115, DOI 10.1109/CVPR.2013.275; Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50; Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433; Wang WG, 2018, PROC CVPR IEEE, P1711, DOI 10.1109/CVPR.2018.00184; Xia CQ, 2017, PROC CVPR IEEE, P4399, DOI 10.1109/CVPR.2017.468; Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407; Zhang JM, 2016, PROC CVPR IEEE, P5733, DOI 10.1109/CVPR.2016.618; Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31; Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081; Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731	55	2	2	2	17	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2021	43	1					204	219		10.1109/TPAMI.2019.2927203	http://dx.doi.org/10.1109/TPAMI.2019.2927203			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PC7WN	31283470	Green Submitted			2022-12-18	WOS:000597206900014
J	Fu, YW; Wang, XM; Dong, HZ; Jiang, YG; Wang, M; Xue, XY; Sigal, L				Fu, Yanwei; Wang, Xiaomei; Dong, Hanze; Jiang, Yu-Gang; Wang, Meng; Xue, Xiangyang; Sigal, Leonid			Vocabulary-Informed Zero-Shot and Open-Set Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Semantics; Vocabulary; Training data; Prototypes; Image recognition; Visualization; Learning systems; Vocabulary-informed learning; generalized zero-shot learning; open-set recognition; zero-shot learning	OBJECT; RECOGNITION; REPRESENTATION; CLASSIFICATION; MARGIN	Despite significant progress in object categorization, in recent years, a number of important challenges remain; mainly, the ability to learn from limited labeled data and to recognize object classes within large, potentially open, set of labels. Zero-shot learning is one way of addressing these challenges, but it has only been shown to work with limited sized class vocabularies and typically requires separation between supervised and unsupervised classes, allowing former to inform the latter but not vice versa. We propose the notion of vocabulary-informed learning to alleviate the above mentioned challenges and address problems of supervised, zero-shot, generalized zero-shot and open set recognition using a unified framework. Specifically, we propose a weighted maximum margin framework for semantic manifold-based recognition that incorporates distance constraints from (both supervised and unsupervised) vocabulary atoms. Distance constraints ensure that labeled samples are projected closer to their correct prototypes, in the embedding space, than to others. We illustrate that resulting model shows improvements in supervised, zero-shot, generalized zero-shot, and large open set recognition, with up to 310K class vocabulary on Animal with Attributes and ImageNet datasets.	[Fu, Yanwei; Dong, Hanze] Fudan Univ, Sch Data Sci, Shanghai 200433, Peoples R China; [Fu, Yanwei] AITRICS, Seoul, South Korea; [Wang, Xiaomei; Jiang, Yu-Gang; Xue, Xiangyang] Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200433, Peoples R China; [Wang, Meng] Hefei Univ Technol, Sch Comp & Informat Sci, Hefei, Peoples R China; [Sigal, Leonid] Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1Z4, Canada	Fudan University; Fudan University; Hefei University of Technology; University of British Columbia	Jiang, YG (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200433, Peoples R China.	yanweifu@fudan.edu.cn; 17110240025@fudan.edu.cn; hzdong15@fudan.edu.cn; ygj@fudan.edu.cn; eric.wangmeng@gmail.com; xyxue@fudan.edu.cn; lsigal@cs.ubc.ca		Dong, Hanze/0000-0002-8846-1260; Fu, Yanwei/0000-0002-6595-6893; Xue, Xiangyang/0000-0002-4897-9209	NSFC [61702108, 61622204]; STCSM Project [16JC1420400]; Shanghai Municipal Science and Technology Major Project [2017SHZDZX01, 2018SHZDZX01]; ZJLab; Eastern Scholar [TP2017006]	NSFC(National Natural Science Foundation of China (NSFC)); STCSM Project(Science & Technology Commission of Shanghai Municipality (STCSM)); Shanghai Municipal Science and Technology Major Project; ZJLab; Eastern Scholar	This work was supported in part by NSFC Project (61702108, 61622204), STCSM Project (16JC1420400), Eastern Scholar (TP2017006), Shanghai Municipal Science and Technology Major Project (2017SHZDZX01, 2018SHZDZX01) and ZJLab.	Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911; Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111; Amit Y., 2007, ICML 07 P 24 INT C M, P17, DOI DOI 10.1145/1273496.1273499; Bart E, 2005, PROC CVPR IEEE, P672; Bendale A, 2016, PROC CVPR IEEE, P1563, DOI 10.1109/CVPR.2016.173; Bendale A, 2015, PROC CVPR IEEE, P1893, DOI 10.1109/CVPR.2015.7298799; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; Changpinyo S, 2017, IEEE I CONF COMP VIS, P3496, DOI 10.1109/ICCV.2017.376; Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575; Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng J, 2014, LECT NOTES COMPUT SC, V8689, P48, DOI 10.1007/978-3-319-10590-1_4; Dinu Georgiana, 2015, WORKSH TRACK INT C L; Dong H., 2018, CORR; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Fei-Fei L, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1134, DOI 10.1109/ICCV.2003.1238476; Felix R, 2018, LECT NOTES COMPUT SC, V11210, P21, DOI 10.1007/978-3-030-01231-1_2; Fleuret F., 2006, P INT C NEUR INF PRO, P371; Friedlander MP, 2012, SIAM J SCI COMPUT, V34, pA1380, DOI 10.1137/110830629; Frome Andrea, 2013, NEURIPS; Fu YW, 2018, IEEE SIGNAL PROC MAG, V35, P112, DOI 10.1109/MSP.2017.2763441; Fu YW, 2016, PROC CVPR IEEE, P5337, DOI 10.1109/CVPR.2016.576; Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354; Fu YW, 2014, LECT NOTES COMPUT SC, V8690, P584, DOI 10.1007/978-3-319-10605-2_38; Fu YW, 2012, LECT NOTES COMPUT SC, V7575, P530, DOI 10.1007/978-3-642-33765-9_38; Fu YW, 2014, IEEE T PATTERN ANAL, V36, P303, DOI 10.1109/TPAMI.2013.128; FU ZY, 2015, PROC CVPR IEEE, P2635, DOI DOI 10.1109/CVPR.2015.7298879; Guadarrama S, 2014, ROBOTICS SCI SYSTEMS, V2, P6; Hertz T., 2006, P 23 INT C MACH LEAR, P401, DOI DOI 10.1145/1143844.1143895; Huth AG, 2012, NEURON, V76, P1210, DOI 10.1016/j.neuron.2012.10.014; Hwang SN, 2014, I C INF COMM TECH CO, P277, DOI 10.1109/ICTC.2014.6983135; Jayaraman D, 2014, ADV NEUR IN, V27; Johnson N.L., 1994, CONTINUOUS UNIVARIAT, V1; Koch G., 2015, ICML DEEP LEARNING W; Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473; Kotz S, 2000, EXTREME VALUE DISTRI; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250; Kuznetsova A., 2016, P AAAI C ART INT, P135; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Larochelle H., 2008, AAAI, V1, P3; Lazaridou A, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P270; Lee YJ, 2005, IEEE T KNOWL DATA EN, V17, P678, DOI 10.1109/TKDE.2005.77; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Long T., 2018, P 2018 ACM MULT C MU, P4281; Long T, 2018, PATTERN RECOGN LETT, V109, P27, DOI 10.1016/j.patrec.2017.09.030; Long Y, 2017, PROC CVPR IEEE, P6165, DOI 10.1109/CVPR.2017.653; Mikolov T, 2013, P 26 INT C NEURAL IN, P3111; Norouzi Mohammad, 2014, ICLR; Palatucci Mark, 2009, ADV NEURAL INFORM PR, P1410; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Rohrbach M, 2011, PROC CVPR IEEE, P1641, DOI 10.1109/CVPR.2011.5995627; Rohrbach M, 2010, PROC CVPR IEEE, P910, DOI 10.1109/CVPR.2010.5540121; Rudd EM, 2018, IEEE T PATTERN ANAL, V40, P762, DOI 10.1109/TPAMI.2017.2707495; Sanderson M, 2010, NAT LANG ENG, V16, P100, DOI 10.1017/S1351324909005129; SATTAR H, 2015, PROC CVPR IEEE, P981, DOI DOI 10.1109/CVPR.2015.7298700; Schapire RE, 1998, ANN STAT, V26, P1651; Scheirer WJ, 2014, IEEE T PATTERN ANAL, V36, P2317, DOI 10.1109/TPAMI.2014.2321392; Scheirer WJ, 2013, IEEE T PATTERN ANAL, V35, P1757, DOI 10.1109/TPAMI.2012.256; Shigeto Y, 2015, LECT NOTES ARTIF INT, V9284, P135, DOI 10.1007/978-3-319-23528-8_9; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Thrun S., 1996, LEARNING LEARN INTRO; Tommasi T, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.87; Torralba A, 2010, COMMUN ACM, V53, P107, DOI 10.1145/1666420.1666446; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; Torralba A, 2007, IEEE T PATTERN ANAL, V29, P854, DOI 10.1109/TPAMI.2007.1055; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Verma VK, 2018, PROC CVPR IEEE, P4281, DOI 10.1109/CVPR.2018.00450; Verma VK, 2017, LECT NOTES ARTIF INT, V10535, P792, DOI 10.1007/978-3-319-71246-8_48; Vilalta R, 2002, ARTIF INTELL REV, V18, P77, DOI 10.1023/A:1019956318069; Weston Jason, 2011, 22 INT JOINT C ART I; Wolf L, 2005, PROC CVPR IEEE, P359; Wu ZX, 2016, PROC CVPR IEEE, P3112, DOI 10.1109/CVPR.2016.339; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xian YQ, 2017, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2017.328; Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15; Yu FLX, 2013, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2013.105; Yu YL, 2018, IEEE T NEUR NET LEAR, V29, P4116, DOI 10.1109/TNNLS.2017.2753852; Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321; Zhang T., 2004, P 21 INT C MACH LEAR, P116, DOI 10.1145/1015330.1015332; Zhang T, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P313, DOI 10.1145/2623330.2623710; Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhi-Hua Zhou, 2014, Artificial Neural Networks in Pattern Recognition. 6th IAPR TC 3 International Workshop, ANNPR 2014. Proceedings: LNCS 8774, P1, DOI 10.1007/978-3-319-11656-3_1	93	2	2	5	27	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2020	42	12					3136	3152		10.1109/TPAMI.2019.2922175	http://dx.doi.org/10.1109/TPAMI.2019.2922175			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	OP2KH	31199251				2022-12-18	WOS:000587912800012
J	Su, B; Wu, Y				Su, Bing; Wu, Ying			Learning Low-Dimensional Temporal Representations with Latent Alignments	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature extraction; Hidden Markov models; Training; Motion segmentation; Dimensionality reduction; Three-dimensional displays; Data models; Dimensionality reduction; latent alignment; temporal sequences; discriminant analysis	ACTION RECOGNITION; DISCRIMINANT-ANALYSIS; REDUCTION; MODELS; SEGMENTATION; POSE	Low-dimensional discriminative representations enhance machine learning methods in both performance and complexity. This has motivated supervised dimensionality reduction (DR), which transforms high-dimensional data into a discriminative subspace. Most DR methods require data to be i.i.d. However, in some domains, data naturally appear in sequences, where the observations are temporally correlated. We propose a DR method, namely, latent temporal linear discriminant analysis (LT-LDA), to learn low-dimensional temporal representations. We construct the separability among sequence classes by lifting the holistic temporal structures, which are established based on temporal alignments and may change in different subspaces. We jointly learn the subspace and the associated latent alignments by optimizing an objective that favors easily separable temporal structures. We show that this objective is connected to the inference of alignments and thus allows for an iterative solution. We provide both theoretical insight and empirical evaluations on several real-world sequence datasets to show the applicability of our method.	[Su, Bing] Chinese Acad Sci, Inst Software, Sci & Technol Integrated Informat Syst Lab, Beijing 100190, Peoples R China; [Wu, Ying] Northwestern Univ, Dept Elect & Comp Engn, Evanston, IL 60208 USA	Chinese Academy of Sciences; Institute of Software, CAS; Northwestern University	Su, B (corresponding author), Chinese Acad Sci, Inst Software, Sci & Technol Integrated Informat Syst Lab, Beijing 100190, Peoples R China.	subingats@gmail.com; yingwu@ece.northwestern.edu	Su, Bing/ABC-4813-2020	Koochak, Atousa/0000-0001-6547-2728	National Natural Science Foundation of China [61603373]; Youth Innovation Promotion Association CAS [2019110]; National Science Foundation [IIS-1619078, IIS-1815561]; Army Research Office ARO [W911NF-16-1-0138]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Youth Innovation Promotion Association CAS; National Science Foundation(National Science Foundation (NSF)); Army Research Office ARO	The authors would like to thank the anonymous reviewers for their valuable comments. This work was supported in part by the National Natural Science Foundation of China under Grant No.61603373, Youth Innovation Promotion Association CAS No. 2019110, National Science Foundation grant IIS-1619078, IIS-1815561, and the Army Research Office ARO W911NF-16-1-0138.	Baradel F., 2017, ARXIV170310106; Barbic J, 2004, PROC GRAPH INTERF, P185; Ben Tanfous A, 2018, PROC CVPR IEEE, P2840, DOI 10.1109/CVPR.2018.00300; Bian W, 2011, IEEE T PATTERN ANAL, V33, P1037, DOI 10.1109/TPAMI.2010.189; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Cavazza J, 2016, INT C PATT RECOG, P408, DOI 10.1109/ICPR.2016.7899668; Cherian A, 2017, PROC CVPR IEEE, P1581, DOI 10.1109/CVPR.2017.172; De la Torre Fernando, 2006, P 23 INT C MACH LEAR, DOI DOI 10.1145/1143844.1143875; Deng YX, 2017, IEEE DEVICE RES CONF; Dhillon I., 2005, TR0425 U TEX DEP COM; Ding C., 2007, P INT C MACH LEARN, V227, P521, DOI [DOI 10.1145/1273496.1273562, 10.1145/1273496.1273562]; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Escalera S, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P365; Escalera S, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2522848.2532595; Fernando B, 2017, IEEE T PATTERN ANAL, V39, P773, DOI 10.1109/TPAMI.2016.2558148; Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176; Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x; Flamary R, 2012, IEEE T SIGNAL PROCES, V60, P648, DOI 10.1109/TSP.2011.2173685; Fox EB, 2014, ANN APPL STAT, V8, P1281, DOI 10.1214/14-AOAS742; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; Gong D, 2012, LECT NOTES COMPUT SC, V7574, P229, DOI 10.1007/978-3-642-33712-3_17; Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685; Harandi M, 2014, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2014.132; Jaeggli T, 2009, INT J COMPUT VISION, V83, P121, DOI 10.1007/s11263-008-0158-0; Ji XP, 2018, SIGNAL PROCESS, V143, P56, DOI 10.1016/j.sigpro.2017.08.016; Jia CC, 2016, IEEE T IMAGE PROCESS, V25, P4641, DOI 10.1109/TIP.2016.2589320; Jin Z, 2001, PATTERN RECOGN, V34, P1405, DOI 10.1016/S0031-3203(00)00084-4; Koniusz P, 2016, LECT NOTES COMPUT SC, V9908, P37, DOI 10.1007/978-3-319-46493-0_3; Kruger B, 2017, IEEE T MULTIMEDIA, V19, P797, DOI 10.1109/TMM.2016.2635030; Lajugie R., 2014, ADV NEURAL INFORM PR, P1817; Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115; Lee K, 2015, AAAI CONF ARTIF INTE, P2736; Li S, 2015, IEEE I CONF COMP VIS, P4453, DOI 10.1109/ICCV.2015.506; Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273; Lichman M, 2013, UCI MACHINE LEARNING; Liu JY, 2019, IEEE T CIRC SYST VID, V29, P2667, DOI 10.1109/TCSVT.2018.2799968; Loog M, 2004, IEEE T PATTERN ANAL, V26, P732, DOI 10.1109/TPAMI.2004.13; Loog M, 2001, IEEE T PATTERN ANAL, V23, P762, DOI 10.1109/34.935849; Nguyen M. H., 2012, ARTIF INTELL, P520; Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29; Nikitidis S, 2014, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2014.140; Qiao RZ, 2017, PATTERN RECOGN, V66, P202, DOI 10.1016/j.patcog.2017.01.015; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055; Scholkopf B., 2001, LEARNING KERNELS SUP; Shahroudy A, 2018, IEEE T PATTERN ANAL, V40, P1045, DOI 10.1109/TPAMI.2017.2691321; Shyr A, 2010, PROC CVPR IEEE, P3610, DOI 10.1109/CVPR.2010.5539922; Simonyan K, 2014, ADV NEUR IN, V27; Soomro K., CORR; Su B., 2018, P 35 INT C MACH LEAR, P4768; Su B, 2018, IEEE T IMAGE PROCESS, V27, P4052, DOI 10.1109/TIP.2018.2836312; Su B, 2017, IEEE T IMAGE PROCESS, V26, P5784, DOI 10.1109/TIP.2017.2745212; Su B, 2018, IEEE T PATTERN ANAL, V40, P77, DOI 10.1109/TPAMI.2017.2665545; Su B, 2017, IEEE T IMAGE PROCESS, V26, P3579, DOI 10.1109/TIP.2017.2704438; Su B, 2016, LECT NOTES COMPUT SC, V9908, P202, DOI 10.1007/978-3-319-46493-0_13; Su B, 2015, PROC CVPR IEEE, P4539, DOI 10.1109/CVPR.2015.7299084; Su B, 2013, IEEE I CONF COMP VIS, P889, DOI 10.1109/ICCV.2013.115; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Takada H, 2016, INT C PATT RECOG, P1809, DOI 10.1109/ICPR.2016.7899899; Taylor G.W., 2007, ADV NEURAL INFORM PR, P1345; Trigeorgis G, 2018, IEEE T PATTERN ANAL, V40, P1128, DOI 10.1109/TPAMI.2017.2710047; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167; Wang J, 2013, IEEE I CONF COMP VIS, P2688, DOI 10.1109/ICCV.2013.334; Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813; Wang L, 2015, IEEE I CONF COMP VIS, P4570, DOI 10.1109/ICCV.2015.519; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Yao A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.67; Ye J., 2004, P 21 ACM INT C MACH, P113; Ye J., 2007, PROC NIPS 07, V20, P1649; Ye JP, 2005, J MACH LEARN RES, V6, P483; YOUNG S, 2001, HTK BOOK; Zanfir M, 2013, IEEE I CONF COMP VIS, P2752, DOI 10.1109/ICCV.2013.342; Zhang Yu., 2010, P ADV NEURAL INFORM, P2568; Zhou F., 2009, ADV NEURAL INFORM PR, V22, P2286; Zhou F, 2013, IEEE T PATTERN ANAL, V35, P582, DOI 10.1109/TPAMI.2012.137; Zhou F, 2012, PROC CVPR IEEE, P1282, DOI 10.1109/CVPR.2012.6247812; Zhu ML, 2006, IEEE T PATTERN ANAL, V28, P1274, DOI 10.1109/TPAMI.2006.172	84	2	2	0	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2020	42	11					2842	2857		10.1109/TPAMI.2019.2919303	http://dx.doi.org/10.1109/TPAMI.2019.2919303			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NX0AD	31144626				2022-12-18	WOS:000575381000008
J	Sussman, DL; Park, Y; Priebe, CE; Lyzinski, V				Sussman, Daniel L.; Park, Youngser; Priebe, Carey E.; Lyzinski, Vince			Matched Filters for Noisy Induced Subgraph Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Noise measurement; Approximation algorithms; Correlation; Social networking (online); Computer vision; Stochastic processes; Symmetric matrices; Multiple graph inference; subgraph detection; graph matching	GRAPH; ALGORITHM	The problem of finding the vertex correspondence between two noisy graphs with different number of vertices where the smaller graph is still large has many applications in social networks, neuroscience, and computer vision. We propose a solution to this problem via a graph matching matched filter: centering and padding the smaller adjacency matrix and applying graph matching methods to align it to the larger network. The centering and padding schemes can be incorporated into any algorithm that matches using adjacency matrices. Under a statistical model for correlated pairs of graphs, which yields a noisy copy of the small graph within the larger graph, the resulting optimization problem can be guaranteed to recover the true vertex correspondence between the networks. However, there are currently no efficient algorithms for solving this problem. To illustrate the possibilities and challenges of such problems, we use an algorithm that can exploit a partially known correspondence and show via varied simulations and applications to Drosophila and human connectomes that this approach can achieve good performance.	[Sussman, Daniel L.] Boston Univ, Dept Math & Stat, Boston, MA 02215 USA; [Park, Youngser; Priebe, Carey E.] Johns Hopkins Univ, Dept Appl Math & Stat, Baltimore, MD 21218 USA; [Lyzinski, Vince] Univ Massachusetts, Dept Math, Amherst, MA 01003 USA	Boston University; Johns Hopkins University; University of Massachusetts System; University of Massachusetts Amherst	Sussman, DL (corresponding author), Boston Univ, Dept Math & Stat, Boston, MA 02215 USA.	sussman@bu.edu; youngser@jhu.edu; cep@jhu.edu; vlyzinski@umass.edu		Sussman, Daniel/0000-0002-8307-2610	Air Force Research Laboratory [FA8750-18-2-0066]; DARPA [FA8750-18-2-0066]; NIH [BRAIN U01-NS108637]; MIT Lincoln Labs	Air Force Research Laboratory; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); MIT Lincoln Labs	This material is based on research sponsored by the Air Force Research Laboratory and DARPA under agreement number FA8750-18-2-0066. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory and DARPA or the U.S. Government. Vince Lyzinski also gratefully acknowledge the support of NIH grant BRAIN U01-NS108637. This work is also partially supported by a grant from MIT Lincoln Labs.	Aflalo Y, 2015, P NATL ACAD SCI USA, V112, P2942, DOI 10.1073/pnas.1401651112; Airoldi EM, 2008, J MACH LEARN RES, V9, P1981; Akoglu L, 2015, DATA MIN KNOWL DISC, V29, P626, DOI 10.1007/s10618-014-0365-y; ALON N, 1995, J ASSOC COMPUT MACH, V42, P844, DOI 10.1145/210332.210337; [Anonymous], 1987, N HOLLAND M; [Anonymous], **DATA OBJECT**, DOI DOI 10.5281/ZENODO.1161284; Athreya A, 2018, J MACH LEARN RES, V18; Bomze ImmanuelM., 1999, HDB COMBINATORIAL OP, P1; Bonnici V, 2013, BMC BIOINFORMATICS, V14, DOI 10.1186/1471-2105-14-S7-S13; Bunke H, 1997, PATTERN RECOGN LETT, V18, P689, DOI 10.1016/S0167-8655(97)00060-3; Burkard RE, 1999, HDB COMBINATORIAL OP, P1713; Caelli T, 2004, IEEE T PATTERN ANAL, V26, P515, DOI 10.1109/TPAMI.2004.1265866; Carletti V, 2018, IEEE T PATTERN ANAL, V40, P804, DOI 10.1109/TPAMI.2017.2696940; Carletti Vincenzo, 2016, THESIS; Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272; Conte D, 2004, INT J PATTERN RECOGN, V18, P265, DOI 10.1142/S0218001404003228; Cordella LP, 2004, IEEE T PATTERN ANAL, V26, P1367, DOI 10.1109/TPAMI.2004.75; Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006; Durante D., 2016, J AM STAT ASS, V112, P1; Dutta A, 2018, PATTERN RECOGN, V76, P596, DOI 10.1016/j.patcog.2017.12.003; Egozi A, 2013, IEEE T PATTERN ANAL, V35, P18, DOI 10.1109/TPAMI.2012.51; Eichler K, 2017, NATURE, V548, P175, DOI 10.1038/nature23455; Emmert-Streib F, 2016, INFORM SCIENCES, V346, P180, DOI 10.1016/j.ins.2016.01.074; Fang F., 2018, ARXIV180709299; Fiori M., 2013, ADV NEURAL INFORM PR, V26, P127; Fishkind D., 2018, ARXIV12090367; Fishkind D. E., 2018, ARXIVMATHCO180808502; Foggia P, 2014, INT J PATTERN RECOGN, V28, DOI 10.1142/S0218001414500013; GHAHRAMAN DE, 1980, IEEE T SYST MAN CYB, V10, P181, DOI 10.1109/TSMC.1980.4308468; Hoff PD, 2002, J AM STAT ASSOC, V97, P1090, DOI 10.1198/016214502388618906; Hoff PD, 2009, COMPUT MQTH ORGAN TH, V15, P261, DOI 10.1007/s10588-008-9040-4; Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107; Kiar G, 2018, BIORXIV, DOI [10.1101/188706, DOI 10.1101/188706]; Kuramochi M, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P313, DOI 10.1109/ICDM.2001.989534; Lovasz L, 2012, LARGE NETWORKS GRAPH, V60; Lyzinski V., 2017, ARXIV170502294; Lyzinski V, 2016, IEEE T PATTERN ANAL, V38, P60, DOI 10.1109/TPAMI.2015.2424894; Lyzinski V, 2015, PARALLEL COMPUT, V47, P70, DOI 10.1016/j.parco.2015.03.004; Lyzinski V, 2014, J MACH LEARN RES, V15, P3513; MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003; Nickel C. L. M., 2006, THESIS; Samsi S., 2017, 2017 IEEE HIGH PERFO, P1; Schmidt MC, 2009, J PARALLEL DISTR COM, V69, P417, DOI 10.1016/j.jpdc.2009.01.003; Slota GM, 2013, PROC INT CONF PARAL, P210, DOI 10.1109/ICPP.2013.30; Solnon C, 2010, ARTIF INTELL, V174, P850, DOI 10.1016/j.artint.2010.05.002; ULLMANN JR, 1976, J ACM, V23, P31, DOI 10.1145/321921.321925; Vogelstein JT, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0121002; Yartseva L, 2013, P 1 ACM C ONL SOC NE, P119, DOI [10.1145/2512938.2512952, DOI 10.1145/2512938.2512952]; Young SJ, 2007, LECT NOTES COMPUT SC, V4863, P138; Zaslavskiy M, 2009, IEEE T PATTERN ANAL, V31, P2227, DOI 10.1109/TPAMI.2008.245	50	2	2	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2020	42	11					2887	2900		10.1109/TPAMI.2019.2914651	http://dx.doi.org/10.1109/TPAMI.2019.2914651			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NX0AD	31059426	Green Accepted, Green Submitted			2022-12-18	WOS:000575381000011
J	Zhou, JH; Wu, Y				Zhou, Jiahuan; Wu, Ying			Learning Visual Instance Retrieval from Failure: Efficient Online Local Metric Adaptation from Negative Samples	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Measurement; Probes; Visualization; Testing; Feature extraction; Training; Image retrieval; Visual instance retrieval; online metric adaptation; hard negative samples	PERSON REIDENTIFICATION; NEURAL-NETWORK	Existing visual instance retrieval (VIR) approaches attempt to learn a faithful global matching metric or discriminative feature embedding offline to cover enormous visual appearance variations, so as to directly use it online on various unseen probes for retrieval. However, their requirement for a huge set of positive training pairs is very demanding in practice and the performance is largely constrained for the unseen testing samples due to the severe data shifting issue. In contrast, this paper advocates a different paradigm: part of the learning can be performed online but with nominal costs, so as to achieve online metric adaptation for different query probes. By exploiting easily-available negative samples, we propose a novel solution to achieve the optimal local metric adaptation effectively and efficiently. The insight of our method is the local hard negative samples can actually provide tight constraints to fine tune the metric locally. Our local metric adaptation method is generally applicable to be used on top of any offline-learned baselines. In addition, this paper gives in-depth theoretical analyses of the proposed method to guarantee the reduction of the classification error both asymptotically and practically. Extensive experiments on various VIR tasks have confirmed our effectiveness and superiority.	[Zhou, Jiahuan; Wu, Ying] Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60208 USA	Northwestern University	Zhou, JH (corresponding author), Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60208 USA.	jzt011@eecs.northwestern.edu; yingwu@eecs.northwestern.edu		Koochak, Atousa/0000-0001-6547-2728				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38; Bak S, 2017, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR.2017.171; Barman A, 2017, IEEE I CONF COMP VIS, P1124, DOI 10.1109/ICCV.2017.127; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Chang C.-C., 2011, ACM T INTEL SYST TEC, V2, P1, DOI [10.1145/1961189.1961199, DOI 10.1145/1961189.1961199]; Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225; Chen DP, 2015, PROC CVPR IEEE, P1565, DOI 10.1109/CVPR.2015.7298764; Chen YB, 2017, IEEE INT CONF COMP V, P2590, DOI 10.1109/ICCVW.2017.304; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI [10.1109/TPAMI.2007.1115, 10.1109/TP'AMI.2007.1115]; Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005; Duan YQ, 2018, PROC CVPR IEEE, P2780, DOI 10.1109/CVPR.2018.00294; Feroz Ali T. M., 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11217), P123, DOI 10.1007/978-3-030-01261-8_8; Fetaya E, 2015, PR MACH LEARN RES, V37, P162; Garcia J, 2015, IEEE I CONF COMP VIS, P1305, DOI 10.1109/ICCV.2015.154; Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15; Gray D., 2007, P 10 IEEE INT WORKSH; Guanwen Zhang, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P677, DOI 10.1007/978-3-642-37431-9_52; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Iandola F.N., 2016, ARXIV PREPRINT ARXIV; Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117; Kostinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Li Z, 2013, PROC CVPR IEEE, P3610, DOI 10.1109/CVPR.2013.463; Liao SC, 2015, IEEE I CONF COMP VIS, P3685, DOI 10.1109/ICCV.2015.420; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Lin X, 2018, 2018 5TH INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CONTROL ENGINEERING (ICISCE 2018), P718, DOI 10.1109/ICISCE.2018.00154; Liong VE, 2015, PATTERN RECOGN LETT, V68, P288, DOI 10.1016/j.patrec.2015.05.001; Lisanti G., 2014, P INT C DISTRIBUTED, P1, DOI DOI 10.1145/2659021.2659036; Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762; Loy CC, 2009, PROC CVPR IEEE, P1988, DOI 10.1109/CVPRW.2009.5206827; Ma LY, 2014, IEEE T IMAGE PROCESS, V23, P3656, DOI 10.1109/TIP.2014.2331755; Martinel N, 2016, LECT NOTES COMPUT SC, V9908, P858, DOI 10.1007/978-3-319-46493-0_52; Pedagadi S, 2013, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2013.426; Philbin J, 2008, PROC CVPR IEEE, P2285; Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40; Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566; Radenovic F, 2018, PROC CVPR IEEE, P5706, DOI 10.1109/CVPR.2018.00598; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Schapire Robert E., 2012, BOOSTING FDN ALGORIT; Shi HL, 2016, LECT NOTES COMPUT SC, V9905, P732, DOI 10.1007/978-3-319-46448-0_44; Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562; Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25; Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30; Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410; Ustinova E., 2016, ADV NEURAL INFORM PR, V29, P4170; Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23; Wang Y, 2018, PROC CVPR IEEE, P8042, DOI 10.1109/CVPR.2018.00839; Wang ZD, 2017, IEEE I CONF COMP VIS, P379, DOI 10.1109/ICCV.2017.49; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Xiong F, 2014, LECT NOTES COMPUT SC, V8695, P1, DOI 10.1007/978-3-319-10584-0_1; Ye M, 2015, COMPUTING, CONTROL, INFORMATION AND EDUCATION ENGINEERING, P1005; Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139; Zhang ST, 2012, LECT NOTES COMPUT SC, V7573, P660, DOI 10.1007/978-3-642-33709-3_47; Zhang SL, 2013, IEEE I CONF COMP VIS, P1673, DOI 10.1109/ICCV.2013.210; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhang Y, 2016, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR.2016.143; Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng L, 2015, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2015.7298783; Zheng L, 2014, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2014.250; Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138; Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389; Zhou JH, 2017, IEEE I CONF COMP VIS, P2439, DOI 10.1109/ICCV.2017.265; Zhou Y, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00679	74	2	2	0	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2020	42	11					2858	2873		10.1109/TPAMI.2019.2918208	http://dx.doi.org/10.1109/TPAMI.2019.2918208			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NX0AD	31135350				2022-12-18	WOS:000575381000009
J	Zuo, XX; Wang, S; Zheng, JB; Pan, ZG; Yang, RG				Zuo, Xinxin; Wang, Sen; Zheng, Jiangbin; Pan, Zhigeng; Yang, Ruigang			Detailed Surface Geometry and Albedo Recovery from RGB-D Video under Natural Illumination	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Lighting; Shape; Geometry; Sensors; Image color analysis; Cameras; Color; Depth enhancement; intrinsic decomposition; shape from shading	INTRINSIC IMAGE DECOMPOSITION; PHOTOMETRIC STEREO; MULTIVIEW; RETINEX; SHAPE	This article presents a novel approach for depth map enhancement from an RGB-D video sequence. The basic idea is to exploit the photometric information in the color sequence to resolve the inherent ambiguity of shape from shading problem. Instead of making any assumption about surface albedo or controlled object motion and lighting, we use the lighting variations introduced by casual object movement. We are effectively calculating photometric stereo from a moving object under natural illuminations. One of the key technical challenges is to establish correspondences over the entire image set. We, therefore, develop a lighting insensitive robust pixel matching technique that out-performs optical flow method in presence of lighting variations. An adaptive reference frame selection procedure is introduced to get more robust to imperfect lambertian reflections. In addition, we present an expectation-maximization framework to recover the surface normal and albedo simultaneously, without any regularization term. We have validated our method on both synthetic and real datasets to show its superior performance on both surface details recovery and intrinsic decomposition.	[Zuo, Xinxin; Wang, Sen; Zheng, Jiangbin] Northwestern Polytech Univ, Xian 710072, Peoples R China; [Zuo, Xinxin; Yang, Ruigang] Univ Kentucky, Lexington, KY 40506 USA; [Pan, Zhigeng] Hangzhou Normal Univ, DMI Res Ctr, Hangzhou 311121, Zhejiang, Peoples R China; [Yang, Ruigang] Natl Engn Lab Deep Learning Teclmol & Applicat, Beijing 100085, Peoples R China	Northwestern Polytechnical University; University of Kentucky; Hangzhou Normal University	Zheng, JB (corresponding author), Northwestern Polytech Univ, Xian 710072, Peoples R China.; Yang, RG (corresponding author), Univ Kentucky, Lexington, KY 40506 USA.	xinxin.zuo@uky.edu; wangsen1312@gmail.com; zhengjb@nwpu.edu.cn; zgpan@hznu.edu.cn; ryang@cs.uky.edu	Zuo, Xinxin/AAI-8439-2020	Zuo, Xinxin/0000-0002-7116-9634; Wang, Sen/0000-0002-1808-5239; Yang, Ruigang/0000-0001-5296-6307	USDA grant [2018-67021-27416]; US NFS [IIP-1543172]; Chinese National Key RD project [2017YFB1002803]; NSFC [61972321]; Innovation Chain of Shaanxi Province Industrial Area [2017 ZDXM-GY-094]; NSERC Discovery Grant [RGPIN-2019-04575]; University of Alberta-Huawei Joint Innovation collaboration grant [201902]	USDA grant(United States Department of Agriculture (USDA)); US NFS; Chinese National Key RD project; NSFC(National Natural Science Foundation of China (NSFC)); Innovation Chain of Shaanxi Province Industrial Area; NSERC Discovery Grant(Natural Sciences and Engineering Research Council of Canada (NSERC)); University of Alberta-Huawei Joint Innovation collaboration grant	This work is partially supported by the USDA grant (2018-67021-27416), US NFS (IIP-1543172), Chinese National Key R&D project (2017YFB1002803), NSFC (No. 61972321), Innovation Chain of Shaanxi Province Industrial Area (2017 ZDXM-GY-094), NSERC Discovery Grant (No. RGPIN-2019-04575), and the University of Alberta-Huawei Joint Innovation collaboration grant (No. 201902).	Barron J. T., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2521, DOI 10.1109/CVPR.2011.5995392; Barron JT, 2016, IEEE T PATTERN ANAL, V38, P690, DOI 10.1109/TPAMI.2015.2439286; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Barrow H. G., 1978, COMPUTER VISION SYST; Basri R, 2007, INT J COMPUT VISION, V72, P239, DOI 10.1007/s11263-006-8815-7; Bi S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766946; Bohme M, 2010, COMPUT VIS IMAGE UND, V114, P1329, DOI 10.1016/j.cviu.2010.08.001; Bonneel N, 2017, COMPUT GRAPH FORUM, V36, P593, DOI 10.1111/cgf.13149; Boxin Shi, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P361, DOI 10.1109/3DV.2014.9; Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143; Chatterjee A, 2015, PROC CVPR IEEE, P933, DOI 10.1109/CVPR.2015.7298695; Chen QF, 2013, IEEE I CONF COMP VIS, P241, DOI 10.1109/ICCV.2013.37; Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127; Finlayson G, 2003, PATTERN RECOGN LETT, V24, P1679, DOI 10.1016/S0167-8655(02)00324-0; Garces E, 2012, COMPUT GRAPH FORUM, V31, P1415, DOI 10.1111/j.1467-8659.2012.03137.x; Guo W, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (ICMA), P1331, DOI 10.1109/ICMA.2017.8016010; Haefner B, 2018, PROC CVPR IEEE, P164, DOI 10.1109/CVPR.2018.00025; Han Y, 2013, IEEE I CONF COMP VIS, P1617, DOI 10.1109/ICCV.2013.204; Haque SM, 2014, PROC CVPR IEEE, P2283, DOI 10.1109/CVPR.2014.292; Hernandez C, 2008, IEEE T PATTERN ANAL, V30, P548, DOI 10.1109/TPAMI.2007.70820; Horn B, 1970, THESIS; Jeon J, 2014, LECT NOTES COMPUT SC, V8695, P218, DOI 10.1007/978-3-319-10584-0_15; Kim K, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P199, DOI 10.1109/ICCVW.2015.35; Kolmogorov V, 2015, IEEE T PATTERN ANAL, V37, P919, DOI 10.1109/TPAMI.2014.2363465; Kong N, 2014, LECT NOTES COMPUT SC, V8690, P360; Laffont PY, 2015, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2015.57; Laffont PY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366221; Lakdawalla A., 2007, P 1 INT WORKSH PHOT; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; Lee KJ, 2012, LECT NOTES COMPUT SC, V7577, P327, DOI 10.1007/978-3-642-33783-3_24; Maier R, 2017, IEEE I CONF COMP VIS, P3133, DOI 10.1109/ICCV.2017.338; Min DB, 2012, IEEE T IMAGE PROCESS, V21, P1176, DOI 10.1109/TIP.2011.2163164; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Or-El R, 2015, PROC CVPR IEEE, P5407, DOI 10.1109/CVPR.2015.7299179; Park J, 2014, IEEE T IMAGE PROCESS, V23, P5559, DOI 10.1109/TIP.2014.2361034; Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Simakov D, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1202; Storath M, 2015, INVERSE PROBL, V31, DOI 10.1088/0266-5611/31/2/025003; Tappen MF, 2005, IEEE T PATTERN ANAL, V27, P1459, DOI 10.1109/TPAMI.2005.185; Ti CP, 2015, PROC CVPR IEEE, P4334, DOI 10.1109/CVPR.2015.7299062; Wu CL, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661232; Wu CL, 2011, IEEE I CONF COMP VIS, P1108, DOI 10.1109/ICCV.2011.6126358; Wu CL, 2011, IEEE T VIS COMPUT GR, V17, P1082, DOI [10.1109/TVCG.2010.224, 10.1109/TPDS.2010.224]; Wu TP, 2006, IEEE T PATTERN ANAL, V28, P1830, DOI 10.1109/TPAMI.2006.224; Wu TP, 2010, IEEE T PATTERN ANAL, V32, P546, DOI 10.1109/TPAMI.2009.15; Yang JY, 2014, IEEE T IMAGE PROCESS, V23, P3443, DOI 10.1109/TIP.2014.2329776; Yu LF, 2013, PROC CVPR IEEE, P1415, DOI 10.1109/CVPR.2013.186; Zhang L, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P618, DOI 10.1109/ICCV.2003.1238405; Zhang Q, 2012, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2012.6247962; Zhao Q, 2012, IEEE T PATTERN ANAL, V34, P1437, DOI 10.1109/TPAMI.2012.77; Zhou QY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601134; Zhou ZL, 2013, PROC CVPR IEEE, P1482, DOI 10.1109/CVPR.2013.195	53	2	2	0	22	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2020	42	10					2720	2734		10.1109/TPAMI.2019.2955459	http://dx.doi.org/10.1109/TPAMI.2019.2955459			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NL5QY	31765304	Green Submitted			2022-12-18	WOS:000567471300028
J	Liu, YJ; Han, YH; Ye, ZP; Lai, YK				Liu, Yong-Jin; Han, Yiheng; Ye, Zipeng; Lai, Yu-Kun			Ranking-Preserving Cross-Source Learning for Image Retargeting Quality Assessment	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Measurement; Training; Benchmark testing; Quality assessment; Neural networks; Computational modeling; Predictive models; Image retargeting; image quality assessment; learning to rank; general regression neural network	COLOR	Image retargeting techniques adjust images into different sizes and have attracted much attention recently. Objective quality assessment (OQA) of image retargeting results is often desired to automatically select the best results. Existing OQA methods train a model using some benchmarks (e.g., RetargetMe), in which subjective scores evaluated by users are provided. Observing that it is challenging even for human subjects to give consistent scores for retargeting results of different source images (diff-source-results), in this paper we propose a learning-based OQA method that trains a General Regression Neural Network (GRNN) model based on relative scores-which preserve the ranking-of retargeting results of the same source image (same-source-results). In particular, we develop a novel training scheme with provable convergence that learns a common base scalar for same-source-results. With this source specific offset, our computed scores not only preserve the ranking of subjective scores for same-source-results, but also provide a reference to compare the diff-source-results. We train and evaluate our GRNN model using human preference data collected in RetargetMe. We further introduce a subjective benchmark to evaluate the generalizability of different OQA methods. Experimental results demonstrate that our method outperforms ten representative OQA methods in ranking prediction and has better generalizability to different datasets.	[Liu, Yong-Jin; Han, Yiheng; Ye, Zipeng] Tsinghua Univ, Dept Comp Sci & Technol, MOE Key Lab Pervas Comp, BNRist, Beijing 100084, Peoples R China; [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF10 3AT, Wales	Tsinghua University; Cardiff University	Liu, YJ (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, MOE Key Lab Pervas Comp, BNRist, Beijing 100084, Peoples R China.	liuyongjin@tsinghua.edu.cn; hyh18@mails.tsinghua.edu.cn; yezp17@mails.tsinghua.edu.cn; laiy4@cardiff.ac.uk	Liu, Yong/GWQ-6163-2022	Ye, Zipeng/0000-0002-4322-7550	Natural Science Foundation of China [61725204, 61521002, U1736220]; Royal Society-Newton Advanced Fellowship [NA150431]	Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Royal Society-Newton Advanced Fellowship	This work was supported by the Natural Science Foundation of China (61725204, 61521002, U1736220) and Royal Society-Newton Advanced Fellowship (NA150431).	Barrow HG, 1977, P 5 INT JOINT C ART; Castillo S., 2011, S APPL PERC GRAPH VI, P7; Chen Y, 2017, PROC CVPR IEEE, P4743, DOI 10.1109/CVPR.2017.504; Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23; David, 1988, METHOD PAIRED COMP; Du K.L., 2013, NEURAL NETWORKS STAT; Kasutani E, 2001, IEEE IMAGE PROC, P674, DOI 10.1109/ICIP.2001.959135; Latouche G., 1999, INTRO MATRIX ANAL ME; Liang Y, 2017, IEEE T VIS COMPUT GR, V23, P1099, DOI 10.1109/TVCG.2016.2517641; Liu C, 2008, LECT NOTES COMPUT SC, V5304, P28, DOI 10.1007/978-3-540-88690-7_3; Liu YJ, 2011, COMPUT GRAPH FORUM, V30, P583, DOI 10.1111/j.1467-8659.2011.01881.x; Ma L, 2012, IEEE INT SYMP CIRC S, P2677, DOI 10.1109/ISCAS.2012.6271858; Ma L, 2012, IEEE J-STSP, V6, P626, DOI 10.1109/JSTSP.2012.2211996; Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424; Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935; Novak C. L., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P599, DOI 10.1109/CVPR.1992.223129; Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199; Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186; Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329; Shamir A., 2009, P ACM SIGGRAPH ASIA, P11; Simakov D., 2008, 2008 IEEE CVPR, P1; SPECHT DF, 1991, IEEE T NEURAL NETWOR, V2, P568, DOI 10.1109/72.97934; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Zhang L, 2012, PROCEEDINGS OF THE ASME 5TH ANNUAL DYNAMIC SYSTEMS AND CONTROL DIVISION CONFERENCE AND JSME 11TH MOTION AND VIBRATION CONFERENCE, DSCC 2012, VOL 2, P1; Zhang YB, 2018, IEEE T IMAGE PROCESS, V27, P451, DOI 10.1109/TIP.2017.2761556; Zhang YB, 2016, IEEE T IMAGE PROCESS, V25, P4286, DOI 10.1109/TIP.2016.2585884; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	28	2	2	0	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2020	42	7					1798	1805		10.1109/TPAMI.2019.2923998	http://dx.doi.org/10.1109/TPAMI.2019.2923998			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MC0DH	31226069	Green Accepted			2022-12-18	WOS:000542967200022
J	Wang, C; Wu, MY; Wang, ZY; Wang, L; Sheng, H; Yu, JY				Wang, Cen; Wu, Minye; Wang, Ziyu; Wang, Liao; Sheng, Hao; Yu, Jingyi			Neural Opacity Point Cloud	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Rendering (computer graphics); Geometry; Hair; Visualization; Image color analysis; Cameras; Computational photography	IMAGE; VIDEO	Fuzzy objects composed of hair, fur, or feather are impossible to scan even with the latest active or passive 3D scanners. We present a novel and practical neural rendering (NR) technique called neural opacity point cloud (NOPC) to allow high quality rendering of such fuzzy objects at any viewpoint. NOPC employs a learning-based scheme to extract geometric and appearance features on 3D point clouds including their opacity. It then maps the 3D features onto virtual viewpoints where a new U-Net based NR manages to handle noisy and incomplete geometry while maintaining translation equivariance. Comprehensive experiments on existing and new datasets show our NOPC can produce photorealistic rendering on inputs from multi-view setups such as a turntable system for hair and furry toy captures.	[Wang, Cen; Wu, Minye; Wang, Ziyu; Wang, Liao; Yu, Jingyi] ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China; [Wu, Minye] Chinese Acad Sci, Shanghai Inst Microsyst & Informat Technol, Shanghai 200031, Peoples R China; [Wu, Minye] Univ Chinese Acad Sci, Beijing 100049, Peoples R China; [Sheng, Hao] Beihang Univ, Beijing Adv Innovat Ctr Big Data & Brain Comp, Sch Comp Sci & Engn, State Key Lab Software Dev Environm, Beijing 100191, Peoples R China	ShanghaiTech University; Chinese Academy of Sciences; Shanghai Institute of Microsystem & Information Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Beihang University	Yu, JY (corresponding author), ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China.	wangcen@shanghaitech.edu.cn; wumy@shanghaitech.edu.cn; wangzy6@shanghaitech.edu.cn; wangla@shanghaitech.edu.cn; shenghao@buaa.edu.cn; yujingyi@shanghaitech.edu.cn	Wu, Minye/AAT-6694-2021	Wu, Minye/0000-0002-8163-9513	National Key Research and Development Program [2018YFB2100500]; NSFC [61976138, 61977047]; STCSM [2015F0203-000-06]; SHMEC [2019-01-07-00-01-E00003]	National Key Research and Development Program; NSFC(National Natural Science Foundation of China (NSFC)); STCSM(Science & Technology Commission of Shanghai Municipality (STCSM)); SHMEC	This work was supported by the National Key Research and Development Program (2018YFB2100500), the programs of NSFC (61976138 and 61977047), STCSM (2015F0203-000-06), and SHMEC (2019-01-07-00-01-E00003).	Aksoy Y, 2017, PROC CVPR IEEE, P228, DOI 10.1109/CVPR.2017.32; Aliev Kara-Ali, 2019, ARXIV190608240; Bouguet J.-Y., 2008, CAMERA CALIBRATION T, V1080; Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309; Carranza J, 2003, ACM T GRAPHIC, V22, P569, DOI 10.1145/882262.882309; Casas D, 2015, COMPUT GRAPH FORUM, V34, P173, DOI 10.1111/cgf.12756; Chaurasia G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487238; Chen AP, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203192; Chen B, 2019, ACTA OCEANOL SIN, V38, P1, DOI 10.1007/s13131-019-1472-2; Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18; Chen S. E., 1993, Computer Graphics Proceedings, P279, DOI 10.1145/166117.166153; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Chuang Yung-Yu, 2001, COMP VIS PATT REC 20, V2, pII; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Du RF, 2018, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2018), DOI 10.1145/3190834.3190843; Eisemann M, 2008, COMPUT GRAPH FORUM, V27, P409, DOI 10.1111/j.1467-8659.2008.01138.x; Feng XX, 2016, LECT NOTES COMPUT SC, V9906, P204, DOI 10.1007/978-3-319-46475-6_13; Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595; Gastal ESL, 2010, COMPUT GRAPH FORUM, V29, P575, DOI 10.1111/j.1467-8659.2009.01627.x; Grady L, 2005, PROCEEDINGS OF THE FIFTH IASTED INTERNATIONAL CONFERENCE ON VISUALIZATION, IMAGING, AND IMAGE PROCESSING, P423; Hedman P., 2018, ACM T GRAPHIC, V37, P1; Hedman Peter, 2016, ACM T GRAPHIC, V35, P6; Heigl B., 1999, MUSTERERKENNUNG 1999, P94; Jiahui Yu, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P4470, DOI 10.1109/ICCV.2019.00457; Kaiming He, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2049, DOI 10.1109/CVPR.2011.5995495; Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251; Levin A., 2006, IEEE C COMP VIS PATT; Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI 10.1109/ICCV.2017.478; Long GC, 2016, LECT NOTES COMPUT SC, V9910, P434, DOI 10.1007/978-3-319-46466-4_26; Martin-Brualla R, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275099; Matusik W, 2002, ACM T GRAPHIC, V21, P427, DOI 10.1145/566570.566599; Penner E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130855; Qiqi Hou, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P4129, DOI 10.1109/ICCV.2019.00423; Shaofan Cai, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P8818, DOI 10.1109/ICCV.2019.00891; Shen X, 2016, LECT NOTES COMPUT SC, V9905, P92, DOI 10.1007/978-3-319-46448-0_6; Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721; Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035; Thies Justus, 2018, ARXIV181110720; Wang J, 2005, IEEE I CONF COMP VIS, P936; Wang J, 2007, FOUND TRENDS COMPUT, V3, P97, DOI 10.1561/0600000019; Xu Ning, 2017, P IEEE C COMP VIS PA, P2970; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718; Zhou CL, 2009, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON POWER ENGINEERING 2009 (ICOPE-09), VOL 2, P111	43	2	2	1	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2020	42	7					1570	1581		10.1109/TPAMI.2020.2986777	http://dx.doi.org/10.1109/TPAMI.2020.2986777			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MC0DH	32305897				2022-12-18	WOS:000542967200004
J	Hane, C; Tulsiani, S; Malik, J				Hane, Christian; Tulsiani, Sohubham; Malik, Jitendra			Hierarchical Surface Prediction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Geometry; Image color analysis; Shape; Octrees; Color; Surface reconstruction; Single view reconstruction; high resolution; voxel grid; geometry prediction	RECONSTRUCTION; OPTIMIZATION	Recently, Convolutional Neural Networks have shown promising results for 3D geometry prediction. They can make predictions from very little input data such as a single color image. A major limitation of such approaches is that they only predict a coarse resolution voxel grid, which does not capture the surface of the objects well. We propose a general framework, called hierarchical surface prediction (HSP), which facilitates prediction of high resolution voxel grids. The main insight is that it is sufficient to predict high resolution voxels around the predicted surfaces. The exterior and interior of the objects can be represented with coarse resolution voxels. This allows us to predict significantly higher resolution voxel grids around the surface, from which triangle meshes can be extracted. Additionally it allows us to predict properties such as surface color which are only defined on the surface. Our approach is not dependent on a specific input type. We show results for geometry prediction from color images and depth images. Our analysis shows that our high resolution predictions are more accurate than low resolution predictions.	[Hane, Christian; Tulsiani, Sohubham; Malik, Jitendra] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA; [Hane, Christian] Google, Mountain View, CA 94043 USA; [Tulsiani, Sohubham; Malik, Jitendra] Facebook, Menlo Pk, CA USA	University of California System; University of California Berkeley; Google Incorporated; Facebook Inc	Hane, C (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.; Hane, C (corresponding author), Google, Mountain View, CA 94043 USA.	chaene@eecs.berkeley.edu; shubhtuls@eecs.berkeley.edu; malik@eecs.berkeley.edu			Early Postdoc. Mobility fellowship from the Swiss National Science Foundation [165245]; Intel/NSF VEC award [IIS-153909]; NSF [IIS-1212798]	Early Postdoc. Mobility fellowship from the Swiss National Science Foundation(Swiss National Science Foundation (SNSF)); Intel/NSF VEC award; NSF(National Science Foundation (NSF))	C. Hane received funding from the Early Postdoc. Mobility fellowship No. 165245 from the Swiss National Science Foundation. The project received funding from the Intel/NSF VEC award IIS-153909 and NSF award IIS-1212798.	[Anonymous], [No title captured]; Bane C, 2017, INT CONF 3D VISION, P412, DOI 10.1109/3DV.2017.00054; Bao SY, 2013, PROC CVPR IEEE, P1264, DOI 10.1109/CVPR.2013.167; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Cashman TJ, 2013, IEEE T PATTERN ANAL, V35, P232, DOI 10.1109/TPAMI.2012.68; Chang A. X., 2015, ABS151203012 CORR; Chen JW, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461940; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dame A, 2013, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2013.170; Delaunoy Amael, 2008, BMVC 2008 BRIT MACH, P1; Dosovitskiy A, 2017, IEEE T PATTERN ANAL, V39, P692, DOI 10.1109/TPAMI.2016.2567384; Eigen D, 2014, ADV NEUR IN, V27; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Gargallo P, 2007, IEEE I CONF COMP VIS, P1364; Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29; Hane C, 2014, PROC CVPR IEEE, P652, DOI 10.1109/CVPR.2014.89; Hoiem D, 2005, ACM T GRAPHIC, V24, P577, DOI 10.1145/1073204.1073232; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Kar A, 2015, PROC CVPR IEEE, P1966, DOI 10.1109/CVPR.2015.7298807; King DB, 2015, ACS SYM SER, V1214, P1; Kolev K, 2007, LECT NOTES COMPUT SC, V4679, P441; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; Labatut P, 2007, IEEE I CONF COMP VIS, P504; Ladicky L, 2014, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2014.19; Lempitsky V, 2007, IEEE I CONF COMP VIS, P620; Li XX, 2017, PROC CVPR IEEE, P6459, DOI 10.1109/CVPR.2017.684; Liu SB, 2011, PROC CVPR IEEE, P913, DOI 10.1109/CVPR.2011.5995334; Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI 10.1145/37401.37422; Niessner M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508374; Oswald MR, 2009, LECT NOTES COMPUT SC, V5748, P171, DOI 10.1007/978-3-642-03798-6_18; Prasad M., 2006, IEEE COMP SOC C COMP, P1345; Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291; Rezende Danilo Jimenez, 2016, P INT C NEUR INF PRO, P5003; Riegler G, 2017, INT CONF 3D VISION, P57, DOI 10.1109/3DV.2017.00017; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Rosca Mihaela, 2017, ARXIV170604987; Savinov N, 2016, PROC CVPR IEEE, P5460, DOI 10.1109/CVPR.2016.589; Saxena AK, 2005, ARKIVOC, P1, DOI 10.3998/ark.5550190.0006.201; Sinha A, 2017, PROC CVPR IEEE, P791, DOI 10.1109/CVPR.2017.91; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Steinbrucker F, 2014, IEEE INT CONF ROBOT, P2021, DOI 10.1109/ICRA.2014.6907127; Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230; Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20; Toeppe E., 2010, AS C COMP VIS ACCV, P53; Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30; Tulsiani S, 2017, PROC CVPR IEEE, P1466, DOI 10.1109/CVPR.2017.160; Yan X., 2016, P 30 INT C NEUR INF, P1704; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22; Zach C, 2007, IEEE I CONF COMP VIS, P1213; Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957	53	2	3	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2020	42	6					1348	1361		10.1109/TPAMI.2019.2896296	http://dx.doi.org/10.1109/TPAMI.2019.2896296			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	LR3TM	30714908	hybrid			2022-12-18	WOS:000535615700005
J	Novotny, D; Larlus, D; Vedaldi, A				Novotny, David; Larlus, Diane; Vedaldi, Andrea			Capturing the Geometry of Object Categories from Video Supervision	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Geometry; Shape; Solid modeling; Estimation; Image reconstruction; Training; Monocular pose estimation; monocular depth estimation; point-cloud estimation; geometry reconstruction		We propose an unsupervised method to learn the 3D geometry of object categories by looking around them. Differently from traditional approaches, this method does not require CAD models or manual supervision. Instead, using only video sequences showing object instances from a moving viewpoint, the method learns a deep neural network that can predict several aspects of the 3D geometry of such objects from single images. The network has three components. The first is a Siamese viewpoint factorization network that robustly aligns the input videos and learns to predict the absolute viewpoint of the object from a single image. The second is a depth estimation network that performs monocular depth prediction. The third is a shape completion network that predicts the full 3D shape of the object from the output of the monocular depth prediction module. While the three modules solve very different task, we show that they all benefit significantly from allowing networks to perform probabilistic predictions. This results in a self-assessment mechanism which is crucial for obtaining high quality predictions. Our network achieves state-of-the-art results on viewpoint prediction, depth estimation, and 3D point cloud estimation on public benchmarks.	[Novotny, David; Vedaldi, Andrea] Univ Oxford, VGG, Oxford OX1 2JD, England; [Novotny, David; Larlus, Diane] NAVER Labs Europe, F-38240 Meylan, France	University of Oxford	Novotny, D (corresponding author), Univ Oxford, VGG, Oxford OX1 2JD, England.	david@robots.ox.ac.uk; diane.larlus@naverlabs.com; vedaldi@robots.ox.ac.uk	Vedaldi, Andrea/B-9071-2015	Vedaldi, Andrea/0000-0003-1374-2858	NAVER LABS Europe; ERC [677195-IDIU]	NAVER LABS Europe; ERC(European Research Council (ERC)European Commission)	The authors gratefully acknowledge the support of NAVER LABS Europe and ERC 677195-IDIU.	Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293; [Anonymous], [No title captured]; [Anonymous], [No title captured]; [Anonymous], [No title captured]; [Anonymous], [No title captured]; Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487; Bane C, 2017, INT CONF 3D VISION, P412, DOI 10.1109/3DV.2017.00054; Bansal A, 2016, PROC CVPR IEEE, P5965, DOI 10.1109/CVPR.2016.642; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Carreira J, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2435707; Chang A. X., 2015, ABS151203012 CORR; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Crocco M, 2016, PROC CVPR IEEE, P4141, DOI 10.1109/CVPR.2016.449; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Eigen D, 2014, ADV NEUR IN, V27; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Fitzgibbon A. W., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P311, DOI 10.1007/BFb0055675; Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45; Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Glasner D, 2011, IEEE I CONF COMP VIS, P1275, DOI 10.1109/ICCV.2011.6126379; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254; Gupta S, 2015, PROC CVPR IEEE, P4731, DOI 10.1109/CVPR.2015.7299105; Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Horn B. K., 1989, SHAPE SHADING, P123; Kar A, 2015, PROC CVPR IEEE, P1966, DOI 10.1109/CVPR.2015.7298807; Karsch K, 2014, IEEE T PATTERN ANAL, V36, P2144, DOI 10.1109/TPAMI.2014.2316835; Kozubowski TJ, 2013, J MULTIVARIATE ANAL, V113, P59, DOI 10.1016/j.jmva.2012.02.010; Ladicky L, 2014, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2014.19; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Lhuillier M, 2005, IEEE T PATTERN ANAL, V27, P418, DOI 10.1109/TPAMI.2005.44; Li B, 2015, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2015.7298715; Li HS, 2011, IEEE T PATTERN ANAL, V33, P1116, DOI 10.1109/TPAMI.2010.169; Liebelt J., 2008, P 2008 IEEE C COMPUT, P1; Lim JJ, 2013, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2013.372; Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Morvan Y, 2007, THESIS TU EINDHOVEN; Mottaghi R, 2015, PROC CVPR IEEE, P418, DOI 10.1109/CVPR.2015.7298639; Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103; Novotny D, 2017, IEEE I CONF COMP VIS, P5228, DOI 10.1109/ICCV.2017.558; Ozuysal M, 2009, PROC CVPR IEEE, P778, DOI 10.1109/CVPRW.2009.5206633; Pepik B., 2014, P INT C LEARN REPR I, P1; Pepik B, 2012, PROC CVPR IEEE, P3362, DOI 10.1109/CVPR.2012.6248075; Prasad M., 2006, IEEE COMP SOC C COMP, P1345; Rezende Danilo Jimenez, 2016, P INT C NEUR INF PRO, P5003; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Roberts Lawrence G, 1963, THESIS, P2; Rock J, 2015, PROC CVPR IEEE, P2484, DOI 10.1109/CVPR.2015.7298863; Savarese S, 2007, IEEE I CONF COMP VIS, P1245; Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Sedaghat N, 2015, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2015.155; Sinha A, 2017, PROC CVPR IEEE, P791, DOI 10.1109/CVPR.2017.91; Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Sun M, 2009, PROC CVPR IEEE, P1247, DOI 10.1109/CVPRW.2009.5206723; Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0; Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20; Toeppe E, 2013, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2013.30; Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30; Tulsiani S, 2015, PROC CVPR IEEE, P1510, DOI 10.1109/CVPR.2015.7298758; UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573; Ummenhofer B, 2017, PROC CVPR IEEE, P5622, DOI 10.1109/CVPR.2017.596; Xiang Y, 2016, LECT NOTES COMPUT SC, V9912, P160, DOI 10.1007/978-3-319-46484-8_10; Xiang Y, 2014, IEEE WINT CONF APPL, P75, DOI 10.1109/WACV.2014.6836101; Yang Y, 2018, J OBSTET GYNAECOL, V38, P206, DOI 10.1080/01443615.2017.1342230; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700; Zhu SQ, 2010, PROC CVPR IEEE, P1165, DOI 10.1109/CVPR.2010.5540085	76	2	2	0	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2020	42	2					261	275		10.1109/TPAMI.2018.2871117	http://dx.doi.org/10.1109/TPAMI.2018.2871117			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	KE2KB	30235118	Green Submitted			2022-12-18	WOS:000508386100002
J	Oh, SJ; Benenson, R; Fritz, M; Schiele, B				Oh, Seong Joon; Benenson, Rodrigo; Fritz, Mario; Schiele, Bernt			Person Recognition in Personal Photo Collections	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Face; Training; Task analysis; Social network services; Face recognition; Computer vision; person recognition; social media	FACE; REPRESENTATION; PERFORMANCE	People nowadays share large parts of their personal lives through social media. Being able to automatically recognise people in personal photos may greatly enhance user convenience by easing photo album organisation. For human identification task, however, traditional focus of computer vision has been face recognition and pedestrian re-identification. Person recognition in social media photos sets new challenges for computer vision, including non-cooperative subjects (e.g., backward viewpoints, unusual poses) and great changes in appearance. To tackle this problem, we build a simple person recognition framework that leverages convnet features from multiple image regions (head, body, etc.). We propose new recognition scenarios that focus on the time and appearance gap between training and testing samples. We present an in-depth analysis of the importance of different features according to time and viewpoint generalisability. In the process, we verify that our simple approach achieves the state of the art result on the PIPA [1] benchmark, arguably the largest social media based benchmark for person recognition to date with diverse poses, viewpoints, social groups, and events. Compared the conference version of the paper [2] , this paper additionally presents (1) analysis of a face recogniser (DeepID2+ [3] ), (2) new method naeil2 that combines the conference version method naeil and DeepID2+ to achieve state of the art results even compared to post-conference works, (3) discussion of related work since the conference version, (4) additional analysis including the head viewpoint-wise breakdown of performance, and (5) results on the open-world setup.	[Oh, Seong Joon; Benenson, Rodrigo; Fritz, Mario; Schiele, Bernt] Max Planck Inst Informat, Comp Vis & Multimodal Comp Grp, D-66123 Saarbrucken, Germany; [Oh, Seong Joon] LINE Plus, Goyang Si, South Korea; [Benenson, Rodrigo] Google, CH-8002 Zurich, Switzerland; [Fritz, Mario] CISPA Helmholtz Ctr iG, D-66123 Saarbrucken, Germany	Max Planck Society; Google Incorporated	Oh, SJ (corresponding author), Max Planck Inst Informat, Comp Vis & Multimodal Comp Grp, D-66123 Saarbrucken, Germany.; Oh, SJ (corresponding author), LINE Plus, Goyang Si, South Korea.	coallaoh@linecorp.com; rodrigo.benenson@gmail.com; fritz@cispa.saarland; schiele@mpi-inf.mpg.de			German Research Foundation [DFG CRC 1223]	German Research Foundation(German Research Foundation (DFG))	This research was supported by the German Research Foundation (DFG CRC 1223).	Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016; Anguelov D., 2007, P IEEE C COMP VIS PA, P1, DOI [10.1109/CVPR.2007.383057, DOI 10.1109/CVPR.2007.383057]; Bak S, 2014, IEEE WINT CONF APPL, P363, DOI 10.1109/WACV.2014.6836077; Barbosa IB, 2018, COMPUT VIS IMAGE UND, V167, P50, DOI 10.1016/j.cviu.2017.12.002; Barbosa IB, 2012, LECT NOTES COMPUT SC, V7583, P433, DOI 10.1007/978-3-642-33863-2_43; Bedagkar-Gala A, 2014, IMAGE VISION COMPUT, V32, P270, DOI 10.1016/j.imavis.2014.02.001; Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303; Cao XD, 2013, IEEE I CONF COMP VIS, P3208, DOI 10.1109/ICCV.2013.398; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Chen BC, 2014, LECT NOTES COMPUT SC, V8694, P768, DOI 10.1007/978-3-319-10599-4_49; Chen D, 2012, LECT NOTES COMPUT SC, V7574, P566, DOI 10.1007/978-3-642-33712-3_41; Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389; Chen JC, 2016, IEEE WINT CONF APPL; Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145; Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149; Cheng DS, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.68; Connor P, 2018, COMPUT VIS IMAGE UND, V167, P1, DOI 10.1016/j.cviu.2018.01.007; Cui JY, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P367; Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350; Deng J., 2012, ARXIV180107698; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng YB, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P789, DOI 10.1145/2647868.2654966; Ding CX, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2845089; Everingham M., 2006, BMVC, DOI DOI 10.5244/C.20.92; Everingham M, 2009, IMAGE VISION COMPUT, V27, P545, DOI 10.1016/j.imavis.2008.04.018; Gallagher AC, 2008, PROC CVPR IEEE, P1073; Gong SG, 2014, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-1-4471-6296-4_1; Gray D., 2007, IEEE INT WORKSH PERF, V3, P1; Guillaumin M, 2009, IEEE I CONF COMP VIS, P498, DOI 10.1109/ICCV.2009.5459197; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hu Y, 2014, CHIN CONT DECIS CONF, P652, DOI 10.1109/CCDC.2014.6852247; Huang G., 2007, LABELED FACES WILD D, P10; Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3; Kemelmacher-Shlizerman I, 2016, PROC CVPR IEEE, P4873, DOI 10.1109/CVPR.2016.527; Klare B. F., 2015, ALGORITHMS, V13, P4; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar V, 2017, PROC CVPR IEEE, P6797, DOI 10.1109/CVPR.2017.719; Li HX, 2016, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2016.145; Li W., 2013, LNCS, V7724, P31, DOI [10.1007/978-3-642-37331-2, DOI 10.1007/978-3-642-37331-2]; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461; Li Y, 2017, PROC CVPR IEEE, P5660, DOI 10.1109/CVPR.2017.600; Liu Y., 2017, ARXIV171000870; Lu C, 2015, AAAI CONF ARTIF INTE, P3811; Maltoni Davide, 2009, HDB FINGERPRINT RECO, DOI [10.1007/978-1-84882-254-2, DOI 10.1007/978-1-84882-254-2]; Mathialagan CS, 2015, PROC CVPR IEEE, P4858, DOI 10.1109/CVPR.2015.7299119; Mathias M, 2014, LECT NOTES COMPUT SC, V8692, P720, DOI 10.1007/978-3-319-10593-2_47; Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250; Munaro M, 2014, ADV COMPUT VIS PATT, P161, DOI 10.1007/978-1-4471-6296-4_8; Nech A, 2017, PROC CVPR IEEE, P3406, DOI 10.1109/CVPR.2017.363; Oh SJ, 2016, LECT NOTES COMPUT SC, V9907, P19, DOI 10.1007/978-3-319-46487-9_2; Oh SJ, 2015, IEEE I CONF COMP VIS, P3862, DOI 10.1109/ICCV.2015.440; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Otto C, 2018, IEEE T PATTERN ANAL, V40, P289, DOI 10.1109/TPAMI.2017.2679100; Parkhi Omkar M., 2015, BRIT MACH VIS C; Ranjan R., 2017, ARXIV PREPRINT ARXIV; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Sankaranarayanan S., 2016, P IEEE INT C BIOMETR, P1; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Song Y, 2006, LECT NOTES COMPUT SC, V3953, P382, DOI 10.1007/11744078_30; Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Wu AC, 2017, IEEE T IMAGE PROCESS, V26, P2588, DOI 10.1109/TIP.2017.2675201; Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140; Yi D., 2014, LEARNING FACE REPRES, V1411, P7923; Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16; Zhang L, 2003, P 11 ACM INT C MULT, P355, DOI DOI 10.1145/957013.957090; Zhang N, 2015, PROC CVPR IEEE, P4804, DOI 10.1109/CVPR.2015.7299113; Zhao R, 2013, IEEE I CONF COMP VIS, P2528, DOI 10.1109/ICCV.2013.314; Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881; Zhou E., 2013, ARXIV150104690; Zhu ZY, 2013, IEEE I CONF COMP VIS, P113, DOI 10.1109/ICCV.2013.21	81	2	4	0	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2020	42	1					203	220		10.1109/TPAMI.2018.2877588	http://dx.doi.org/10.1109/TPAMI.2018.2877588			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JV3VQ	30387721	Green Submitted			2022-12-18	WOS:000502294300016
J	Ajanthan, T; Hartley, R; Salzmann, M				Ajanthan, Thalaiyasingam; Hartley, Richard; Salzmann, Mathieu			Memory Efficient Max Flow for Multi-Label Submodular MRFs	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Max-flow; mutli-label submodular; memory efficiency; flow encoding; graphical models	MARKOV RANDOM-FIELDS; ENERGY MINIMIZATION; ALGORITHMS	Multi-label submodular Markov Random Fields (MRFs) have been shown to be solvable using max-flow based on an encoding of the labels proposed by Ishikawa, in which each variable X-i is represented by l nodes (where l is the number of labels) arranged in a column. However, this method in general requires 2l(2) edges for each pair of neighbouring variables. This makes it inapplicable to realistic problems with many variables and labels, due to excessive memory requirement. In this paper, we introduce a variant of the max-flow algorithm that requires much less storage. Consequently, our algorithm makes it possible to optimally solve multi-label submodular problems involving large numbers of variables and labels on a standard computer.	[Ajanthan, Thalaiyasingam; Hartley, Richard] Australian Natl Univ, Canberra, ACT 2601, Australia; [Ajanthan, Thalaiyasingam; Hartley, Richard] Data61 CSIRO, Canberra, ACT 2601, Australia; [Ajanthan, Thalaiyasingam] Univ Oxford, Oxford OX1 3PJ, England; [Salzmann, Mathieu] Ecole Polytech Fed Lausanne, CVLab, CH-1015 Lausanne, Switzerland	Australian National University; Commonwealth Scientific & Industrial Research Organisation (CSIRO); University of Oxford; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Ajanthan, T (corresponding author), Australian Natl Univ, Canberra, ACT 2601, Australia.; Ajanthan, T (corresponding author), Data61 CSIRO, Canberra, ACT 2601, Australia.; Ajanthan, T (corresponding author), Univ Oxford, Oxford OX1 3PJ, England.	ajanthan@robots.ox.ac.uk; richard.hartley@anu.edu.au; mathieu.salzmann@epfl.ch		Salzmann, Mathieu/0000-0002-8347-8637	Australian Government; ARC through the ICT Centre of Excellence program	Australian Government(Australian GovernmentCGIAR); ARC through the ICT Centre of Excellence program(Australian Research Council)	Data61 (formerly NICTA) is funded by the Australian Government and ARC through the ICT Centre of Excellence program.	Ajanthan T, 2016, PROC CVPR IEEE, P5867, DOI 10.1109/CVPR.2016.632; Ajanthan T, 2015, PROC CVPR IEEE, P5144, DOI 10.1109/CVPR.2015.7299150; Andres Bjoern, 2012, ARXIV12060111; Andrew D., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587464; Arora C, 2014, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2014.175; Birchfield S, 1998, IEEE T PATTERN ANAL, V20, P401, DOI 10.1109/34.677269; Boros E, 2002, DISCRETE APPL MATH, V123, P155, DOI 10.1016/S0166-218X(01)00336-5; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Chandran BG, 2009, OPER RES, V57, P358, DOI 10.1287/opre.1080.0572; Cormen T.H., 2001, INTRO ALGORITHMS, V6; Ford L. R. J., 1962, FLOWS NETWORKS; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Goldberg AV, 2015, LECT NOTES COMPUT SC, V9294, P619, DOI 10.1007/978-3-662-48350-3_52; Goldberg AV, 2011, LECT NOTES COMPUT SC, V6942, P457, DOI 10.1007/978-3-642-23719-5_39; GOLDBERG AV, 1988, J ACM, V35, P921, DOI 10.1145/48014.61051; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Ishikawa H, 2003, IEEE T PATTERN ANAL, V25, P1333, DOI 10.1109/TPAMI.2003.1233908; Jamriska O, 2012, PROC CVPR IEEE, P3673, DOI 10.1109/CVPR.2012.6248113; Kappes JH, 2015, INT J COMPUT VISION, V115, P155, DOI 10.1007/s11263-015-0809-x; Kohli P, 2005, IEEE I CONF COMP VIS, P922; Kolmogorov V., 2005, P 21 C UNC ART INT, P316, DOI DOI 10.5555/3020336.3020376; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Komodakis N, 2011, IEEE T PATTERN ANAL, V33, P531, DOI 10.1109/TPAMI.2010.108; Orlin JB, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P765; Savchynskyy B., 2012, UAI P, P746; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Scharstein D, 2003, PROC CVPR IEEE, P195; Schlesinger D., 2006, TRANSFORMING ARBITRA; Shekhovtsov A, 2013, INT J COMPUT VISION, V104, P315, DOI 10.1007/s11263-012-0571-2; Strandmark P, 2010, PROC CVPR IEEE, P2085, DOI 10.1109/CVPR.2010.5539886; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; Tarlow D., 2011, P C UNC ART INT, P671; Torr P., 2009, ADV NEURAL INFORM PR, P889; Veksler O, 2012, INT J COMPUT VISION, V98, P1, DOI 10.1007/s11263-011-0491-6; Verma T, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.61; Vineet Vibhav, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563095; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938; Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036	38	2	2	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	2019	41	4					886	900		10.1109/TPAMI.2018.2819675	http://dx.doi.org/10.1109/TPAMI.2018.2819675			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HO0HP	29993772	Green Submitted			2022-12-18	WOS:000460583500008
J	Gella, S; Keller, F; Lapata, M				Gella, Spandana; Keller, Frank; Lapata, Mirella			Disambiguating Visual Verbs	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Computer vision; distributed representations; natural language processing		In this article, we introduce a new task, visual sense disambiguation for verbs: given an image and a verb, assign the correct sense of the verb, i.e., the one that describes the action depicted in the image. Just as textual word sense disambiguation is useful for a wide range of NLP tasks, visual sense disambiguation can be useful for multimodal tasks such as image retrieval, image description, and text illustration. We introduce a new dataset, which we call VerSe (short for Verb Sense) that augments existing multimodal datasets (COCO and TUHOI) with verb and sense labels. We explore supervised and unsupervised models for the sense disambiguation task using textual, visual, and multimodal embeddings. We also consider a scenario in which we must detect the verb depicted in an image prior to predicting its sense (i.e., there is no verbal information associated with the image). We find that textual embeddings perform well when gold-standard annotations (object labels and image descriptions) are available, while multimodal embeddings perform well on unannotated images.	[Gella, Spandana; Keller, Frank; Lapata, Mirella] Univ Edinburgh, Sch Informat, Inst Language Cognit & Computat, 10 Crichton St, Edinburgh EH8 9AB, Midlothian, Scotland	University of Edinburgh	Gella, S (corresponding author), Univ Edinburgh, Sch Informat, Inst Language Cognit & Computat, 10 Crichton St, Edinburgh EH8 9AB, Midlothian, Scotland.	spandanagella@gmail.com; keller@inf.ed.ac.uk; mlap@inf.ed.ac.uk			European Research Council [681760]; Leverhulme Trust [IAF-2017-019]	European Research Council(European Research Council (ERC)European Commission); Leverhulme Trust(Leverhulme Trust)	The authors would like to thank the reviewers for their helpful comments. The authors gratefully acknowledge the support of the European Research Council (Lapata: award 681760) and of the Leverhulme Trust (Keller: award IAF-2017-019).	Andrew Galen, 2013, ICML; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Baker Collin F., 1998, P 36 ANN M ASS COMP, P86, DOI DOI 10.3115/980845.980860; BARNARD K., 2003, P C N AM CHAPT ASS C, V6, P1; Bernardi R., 2014, P 3 WORKSH VIS LANG, P17; Bernardi R, 2016, J ARTIF INTELL RES, V55, P409, DOI 10.1613/jair.4900; Bernardi Raffaella, 2013, P 3 ACM C INT C MULT, P231; Beth Levin, 1993, ENGLISH VERB CLASSES; Brody S., 2008, P 22 INT C COMP LING, P65; Chao YW, 2015, IEEE I CONF COMP VIS, P1017, DOI 10.1109/ICCV.2015.122; Chen X, 2015, CORR, V1504, P325; Chen XL, 2015, PROC CVPR IEEE, P5298, DOI 10.1109/CVPR.2015.7299167; Delaitre V., 2010, BMVC, DOI DOI 10.5244/C.24.97; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754; Gella S., 2017, P 2017 C EMP METH NA, P2829; Gong YC, 2014, LECT NOTES COMPUT SC, V8692, P529, DOI 10.1007/978-3-319-10593-2_35; Gupta A, 2009, IEEE T PATTERN ANAL, V31, P1775, DOI 10.1109/TPAMI.2009.83; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814; Hodosh M, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4188; Hovy, 2015, P 2015 C N AM CHAPT, P683, DOI DOI 10.3115/V1/N15-1070; Hovy Eduard, 2006, P HUM LANG TECHN C N, P57, DOI DOI 10.3115/1614049.1614064; Ikizler N, 2008, P INT C PATT REC, P1, DOI [DOI 10.1109/ICPR.2008.4761663, 10.1109/icpr.2008.4761663]; Ikizler-Cinbis N, 2010, LECT NOTES COMPUT SC, V6311, P494, DOI 10.1007/978-3-642-15549-9_36; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Lesk M., 1986, ANN INT C SYSTEMS DO, P24, DOI DOI 10.1145/318723.318728; Li LJ, 2007, LECT NOTES ARTIF INT, V4456, P1; Lin DK, 1997, 35TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 8TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P64; Loeff N., 2006, P COLING ACL, P547; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; Ma SG, 2017, PATTERN RECOGN, V68, P334, DOI 10.1016/j.patcog.2017.01.027; Mallya A, 2016, LECT NOTES COMPUT SC, V9905, P414, DOI 10.1007/978-3-319-46448-0_25; Maron O., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P341; Maron O, 1998, ADV NEUR IN, V10, P570; McCarthy Diana, 2004, P 42 ANN M ASS COMP, P279, DOI DOI 10.3115/1218955.1218991; Mikolov T., 2013, ARXIV; Miller G.A., 1990, INT J LEXICOGRAPHY, V3, P235, DOI DOI 10.1093/IJL/3.4.235; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Ramanathan V, 2015, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR.2015.7298713; Ronchi M. R., 2015, P BRIT MACH VIS C BM, P1; Rothe S, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1793; Saenko Kate, 2008, P 22 ANN C NEUR INF, P1393; Schuler Karin Kipper, 2005, THESIS; Sener F, 2012, LECT NOTES COMPUT SC, V7585, P263, DOI 10.1007/978-3-642-33885-4_27; Vilar D., 2006, P LREC, P697; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Viola P., 2005, ADV NEURAL INFORM PR, P1417; Wang WR, 2015, PR MACH LEARN RES, V37, P1083; Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966; Yao BP, 2011, IEEE I CONF COMP VIS, P1331, DOI 10.1109/ICCV.2011.6126386; Yao Bangpeng, 2010, CVPR, DOI DOI 10.1109/CVPR.2010.5540234; Yatskar M, 2016, PROC CVPR IEEE, P5534, DOI 10.1109/CVPR.2016.597; Young P., 2014, P TACL, V2, P67, DOI 10.1162/tacl_a_00166; Zhong Zhi, 2010, P ACL 2010 SYST DEM, P78	60	2	2	0	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2019	41	2					311	322		10.1109/TPAMI.2017.2786699	http://dx.doi.org/10.1109/TPAMI.2017.2786699			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HI0RN	29990137	Green Accepted			2022-12-18	WOS:000456150600004
J	Escalera, S; Baro, X; Guyon, I; Escalante, HJ; Tzimiropoulos, G; Valstar, M; Pantic, M; Cohn, J; Kanade, T				Escalera, Sergio; Baro, Xavier; Guyon, Isabelle; Escalante, Hugo Jair; Tzimiropoulos, Georgios; Valstar, Michel; Pantic, Maja; Cohn, Jeffrey; Kanade, Takeo			Guest Editorial: The Computational Face	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									[Escalera, Sergio] Univ Barcelona, Dept Math & Informat, E-08007 Barcelona, Spain; [Escalera, Sergio] Comp Vis Ctr, Barcelona 08007, Spain; [Baro, Xavier] Univ Oberta Catalunya, Barcelona 08018, Spain; [Baro, Xavier] Univ Oberta Catalunya, Comp Sci Multimedia & Telecommun Dept, Barcelona 08018, Spain; [Baro, Xavier] Comp Vis Ctr, Barcelona 08018, Spain; [Guyon, Isabelle] Univ Paris Saclay, INRIA, UPSud, Big Data, F-91190 St Aubin, France; [Guyon, Isabelle] ChaLearn, Berkeley, CA 94708 USA; [Escalante, Hugo Jair] Inst Nacl Astrofis Opt & Eect, Puebla 72840, Mexico; [Tzimiropoulos, Georgios; Valstar, Michel] Univ Nottingham, Sch Comp Sci, Nottingham NG7 2RD, England; [Pantic, Maja] Imperial Coll, Affect & Behav Comp, London SW7 2AZ, England; [Cohn, Jeffrey] Univ Pittsburgh, Psychol & Psychiat, Pittsburgh, PA 15260 USA; [Kanade, Takeo] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	University of Barcelona; Centre de Visio per Computador (CVC); UOC Universitat Oberta de Catalunya; UOC Universitat Oberta de Catalunya; Centre de Visio per Computador (CVC); Inria; UDICE-French Research Universities; Universite Paris Saclay; Instituto Nacional de Astrofisica, Optica y Electronica; University of Nottingham; Imperial College London; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; Carnegie Mellon University	Escalera, S (corresponding author), Univ Barcelona, Dept Math & Informat, E-08007 Barcelona, Spain.; Escalera, S (corresponding author), Comp Vis Ctr, Barcelona 08007, Spain.	sergio@maia.ub.es; xbaro@uoc.edu; guyon@chalearn.org; hugojair@inaoep.mx; yorgos.tzimiropoulos@nottingham.ac.uk; michel.valstar@nottingham.ac.uk; m.pantic@imperial.ac.uk; jeffcohn@cs.cmu.edu; Takeo.Kanade@cs.cmu.edu	Baró, Xavier/A-4064-2011; Escalera, Sergio/L-2998-2015	Baró, Xavier/0000-0001-5338-3007; Escalera, Sergio/0000-0003-0617-8873; Tzimiropoulos, Georgios/0000-0002-1803-5338; Guyon, Isabelle/0000-0002-9266-1783; Valstar, Michel/0000-0003-2414-161X	MINECO/FEDER, UE [TIN2015-66951-C2-2-R, TIN2016-74946-P]; CERCA Programme/Generalitat de Catalunya; INAOE, CONACyT-Mexico [241306]; ChaLearn Looking at People; Microsoft Research; Google; NVIDIA Coorporation; Amazon; Facebook; Disney Research	MINECO/FEDER, UE(Spanish Government); CERCA Programme/Generalitat de Catalunya; INAOE, CONACyT-Mexico; ChaLearn Looking at People; Microsoft Research(Microsoft); Google(Google Incorporated); NVIDIA Coorporation; Amazon; Facebook(Facebook Inc); Disney Research	This project has been partially supported by the Spanish projects TIN2015-66951-C2-2-R and TIN2016-74946-P (MINECO/FEDER, UE) and CERCA Programme/Generalitat de Catalunya and by INAOE, CONACyT-Mexico under grant 241306. We thank ChaLearn Looking at People sponsors for their support, including Microsoft Research, Google, NVIDIA Coorporation, Amazon, Facebook, and Disney Research.	Booth J, 2018, IEEE T PATTERN ANAL, V40, P2638, DOI 10.1109/TPAMI.2018.2832138; Chrysos G. G., 2018, IEEE T PATTERN ANAL; Escalera S, 2017, IEEE IJCNN, P1594, DOI 10.1109/IJCNN.2017.7966041; Escalera S, 2016, IEEE COMPUT SOC CONF, P706, DOI 10.1109/CVPRW.2016.93; Escalera S, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P243, DOI 10.1109/ICCVW.2015.40; Han H, 2018, IEEE T PATTERN ANAL, V40, P2597, DOI 10.1109/TPAMI.2017.2738004; Kononenko D, 2018, IEEE T PATTERN ANAL, V40, P2696, DOI 10.1109/TPAMI.2017.2737423; Li W., 2018, IEEE T PATTERN ANAL, DOI [10.11069/TPAMI.2018.2791608, DOI 10.11069/TPAMI.2018.2791608]; Liu H, 2018, IEEE T PATTERN ANAL, V40, P2546, DOI 10.1109/TPAMI.2017.2734779; Masse B., 2018, IEEE T PATTERN ANAL, DOI [10.1109/TPAMI.2017.2785819, DOI 10.1109/TPAMI.2017.2785819]; Robinson J. P., 2018, IEEE T PATTERN ANAL, DOI [10.1109/TPAMI.2018.2826849, DOI 10.1109/TPAMI.2018.2826849]; Sagonas C., 2018, IEEE T PATTERN ANAL, DOI [10.11069/TPAMI.2017.2784421, DOI 10.11069/TPAMI.2017.2784421]; Tan Z., 2018, IEEE T PATTERN ANAL, DOI [10.1109/TPAMI.2017.2738004, DOI 10.1109/TPAMI.2017.2738004]; Wang MJ, 2018, IEEE T PATTERN ANAL, V40, P2682, DOI 10.1109/TPAMI.2017.2783940; Wang W., 2018, IEEE T PATTERN ANAL, DOI [10.1106/TPAMI.2018.2810881, DOI 10.1106/TPAMI.2018.2810881]; Yu Y, 2018, IEEE T PATTERN ANAL, V40, P2653, DOI 10.1109/TPAMI.2018.2841403	16	2	2	1	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2018	40	11					2541	2545		10.1109/TPAMI.2018.2869610	http://dx.doi.org/10.1109/TPAMI.2018.2869610			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GW2AF		Green Submitted, Bronze			2022-12-18	WOS:000446683700001
J	Yonetani, R; Kitani, KM; Sato, Y				Yonetani, Ryo; Kitani, Kris M.; Sato, Yoichi			Ego-Surfing: Person Localization in First-Person Videos Using Ego-Motion Signatures	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						First-person video; people identification; dense trajectory	FACE DETECTION; RECOGNITION	We envision a future time when wearable cameras are worn by the masses and recording first-person point-of-view videos of everyday life. While these cameras can enable new assistive technologies and novel research challenges, they also raise serious privacy concerns. For example, first-person videos passively recorded by wearable cameras will necessarily include anyone who comes into the view of a camera-with or without consent. Motivated by these benefits and risks, we developed a self-search technique tailored to first-person videos. The key observation of our work is that the egocentric head motion of a target person (i.e., the self) is observed both in the point-of-view video of the target and observer. The motion correlation between the target person's video and the observer's video can then be used to identify instances of the self uniquely. We incorporate this feature into the proposed approach that computes the motion correlation over densely-sampled trajectories to search for a target individual in observer videos. Our approach significantly improves self-search performance over several well-known face detectors and recognizers. Furthermore, we show how our approach can enable several practical applications such as privacy filtering, target video retrieval, and social group clustering.	[Yonetani, Ryo; Sato, Yoichi] Univ Tokyo, Inst Ind Sci, Tokyo 1138654, Japan; [Kitani, Kris M.] Carnegie Mellon Univ, Robot Inst, Comp Vis Grp, Pittsburgh, PA 15213 USA	University of Tokyo; Carnegie Mellon University	Yonetani, R (corresponding author), Univ Tokyo, Inst Ind Sci, Tokyo 1138654, Japan.	yonetani@iis.u-tokyo.ac.jp; kkitani@cs.cmu.edu; ysato@iis.u-tokyo.ac.jp		Sato, Yoichi/0000-0003-0097-4537	CREST JST; Kayamori Foundation of Informational Science Advancement	CREST JST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST)); Kayamori Foundation of Informational Science Advancement(Kayamori Foundation of Informational Science Advancement)	This research was supported by CREST JST and the Kayamori Foundation of Informational Science Advancement.	Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244; Alletto S., 2014, P INT C PATT REC ICP, P1; Alletto S, 2015, PATTERN RECOGN, V48, P4082, DOI 10.1016/j.patcog.2015.06.006; Alletto S, 2014, IEEE COMPUT SOC CONF, P594, DOI 10.1109/CVPRW.2014.91; [Anonymous], 2012, 2012 IEEE COMP VIS P; Ardeshir S, 2016, LECT NOTES COMPUT SC, V9909, P253, DOI 10.1007/978-3-319-46454-1_16; Arev I, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601198; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; Faloutsos C., 1994, SIGMOD Record, V23, P419, DOI 10.1145/191843.191925; Fan C., 2017, PROC GLOBAL TELECOMM, P1; Farneback G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50; Fathi A, 2012, PROC CVPR IEEE, P1226, DOI 10.1109/CVPR.2012.6247805; Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800; Gong SG, 2014, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-1-4471-6296-4_1; Hesch Joel A, 2012, P IEEE C COMP VIS PA, P15; Hoshen Y, 2016, PROC CVPR IEEE, P4284, DOI 10.1109/CVPR.2016.464; Kasahara S, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1561, DOI [10.1145/2858036.2858495, 10.1145/2929490.2929495]; Keogh E., KNOWL INFORM SYST, V3, P263; Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820; Leung T.-S., 2014, P IEEE C COMP VIS PA, P153; Lu Z, 2013, PROC CVPR IEEE, P2714, DOI 10.1109/CVPR.2013.350; Lucas B.D., 1981, IJCAI 81 P 7 INT JOI, P674, DOI DOI 10.1109/HPDC.2004.1323531; Park HS, 2012, ANN DAAAM, V23, P1; Park HS, 2015, PROC CVPR IEEE, P4777, DOI 10.1109/CVPR.2015.7299110; Park HS, 2013, IEEE I CONF COMP VIS, P3503, DOI 10.1109/ICCV.2013.435; Poleg Y., 2014, P AS C COMP VIS ACCV, P1; Rehg JM, 2013, PROC CVPR IEEE, P3414, DOI 10.1109/CVPR.2013.438; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Ryoo MS, 2013, PROC CVPR IEEE, P2730, DOI 10.1109/CVPR.2013.352; Salvagnini P, 2013, IEEE IMAGE PROC, P3552, DOI 10.1109/ICIP.2013.6738733; Schutze H., 2008, INTRO INFORM RETRIEV, V39; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Tang T.J.J., 2014, ACM ISWC, P119; Tian Y, 2013, IEEE ANN INT CONF CY, P153, DOI 10.1109/CYBER.2013.6705437; Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273; Vezzani R, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543596; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Xu C., 2012, P ECCV, P1; Xu CL, 2012, PROC CVPR IEEE, P1202, DOI 10.1109/CVPR.2012.6247802; Xu J, 2015, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2015.7298836; Ye Z., 2015, P IEEE INT C AUT FAC; Ye ZF, 2012, UBICOMP'12: PROCEEDINGS OF THE 2012 ACM INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING, P699; Yonetani R, 2015, PROC CVPR IEEE, P5445, DOI 10.1109/CVPR.2015.7299183; Zhang C., 2010, MSRTR201066; Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881; Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014	49	2	2	1	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2018	40	11					2749	2761		10.1109/TPAMI.2017.2771767	http://dx.doi.org/10.1109/TPAMI.2017.2771767			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GW2AF	29990151	Green Submitted			2022-12-18	WOS:000446683700017
J	Nasihatkon, B; Kahl, F				Nasihatkon, Behrooz; Kahl, Fredrik			Multiresolution Search of the Rigid Motion Space for Intensity-Based Registration	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Intensity-based image registration; multiresolution registration; global optimization; Lipschitz optimization; early elimination	SUCCESSIVE ELIMINATION ALGORITHM	We study the relation between the correlation-based target functions of low-resolution and high-resolution intensity-based registration for the class of rigid transformations. Our results show that low-resolution target values can tightly bound the high-resolution target function in natural images. This can help with analyzing and better understanding the process of multiresolution image registration. It also gives a guideline for designing multiresolution algorithms in which the search space in higher resolution registration is restricted given the fitness values for lower resolution image pairs. To demonstrate this, we incorporate our multiresolution technique into a Lipschitz global optimization framework. We show that using the multiresolution scheme can result in large gains in the efficiency of such algorithms. The method is evaluated by applying to the problems of 2D registration, 3D rotation search, and the detection of reflective symmetry in 2D and 3D images.	[Nasihatkon, Behrooz; Kahl, Fredrik] Chalmers Univ Technol, Dept Signals & Syst, S-41258 Gothenburg, Sweden; [Nasihatkon, Behrooz; Kahl, Fredrik] MedTech West, S-41345 Gothenburg, Sweden; [Nasihatkon, Behrooz] Sharif Univ Technol, ICT Innovat Ctr, Tehran, Iran; [Kahl, Fredrik] Lund Univ, Ctr Math Sci, S-22362 Lund, Sweden	Chalmers University of Technology; Sharif University of Technology; Lund University	Nasihatkon, B (corresponding author), Chalmers Univ Technol, Dept Signals & Syst, S-41258 Gothenburg, Sweden.; Nasihatkon, B (corresponding author), MedTech West, S-41345 Gothenburg, Sweden.; Nasihatkon, B (corresponding author), Sharif Univ Technol, ICT Innovat Ctr, Tehran, Iran.	nasihatkon@kntu.ac.ir; fredrik.kahl@chalmers.se	Nasihatkon, Behrooz/AAT-9724-2020					Bustos AP, 2014, PROC CVPR IEEE, P3930, DOI 10.1109/CVPR.2014.502; Chen YS, 2001, IEEE T IMAGE PROCESS, V10, P1212, DOI 10.1109/83.935037; Corvi M., 1995, Proceedings. International Conference on Image Processing (Cat. No.95CB35819), P224, DOI 10.1109/ICIP.1995.537621; Cremers D., 2008, P IEEE C COMP VIS PA, P1; Di Stefano L, 2003, MACH VISION APPL, V13, P213; Forstmann BU, 2014, SCI DATA, V1, DOI 10.1038/sdata.2014.50; Gao XQ, 2000, IEEE T IMAGE PROCESS, V9, P501, DOI 10.1109/83.826786; Gharavi-Alkhansari M, 2001, IEEE T IMAGE PROCESS, V10, P526, DOI 10.1109/83.913587; Hansen P., 1995, HDB GLOBAL OPTIMIZAT; Hel-Or Y, 2005, IEEE T PATTERN ANAL, V27, P1430, DOI 10.1109/TPAMI.2005.184; Huttenlocher D. P., 1992, 921321 TR CORN U; Kazhdan M, 2004, ALGORITHMICA, V38, P201, DOI 10.1007/s00453-003-1050-5; Korman S, 2013, PROC CVPR IEEE, P2331, DOI 10.1109/CVPR.2013.302; Lee CH, 1997, IEEE T IMAGE PROCESS, V6, P1587, DOI 10.1109/83.641419; Li H., 2007, P ICCV, P1, DOI DOI 10.1109/ICCV.2007.4409077; LI W, 1995, IEEE T IMAGE PROCESS, V4, P105, DOI 10.1109/83.350809; Maes F, 1999, Med Image Anal, V3, P373, DOI 10.1016/S1361-8415(99)80030-9; Mattoccia S, 2008, IEEE T IMAGE PROCESS, V17, P528, DOI 10.1109/TIP.2008.919362; Olsson C, 2009, IEEE T PATTERN ANAL, V31, P783, DOI 10.1109/TPAMI.2008.131; Ouyang WL, 2012, IEEE T PATTERN ANAL, V34, P127, DOI 10.1109/TPAMI.2011.106; Ouyang WL, 2010, IEEE T PATTERN ANAL, V32, P165, DOI 10.1109/TPAMI.2009.104; Pfeuffer F, 2012, ANN OPER RES, V196, P737, DOI 10.1007/s10479-010-0760-8; Thevenaz P, 2000, IEEE T IMAGE PROCESS, V9, P2083, DOI 10.1109/83.887976; Tombari F, 2009, IEEE T PATTERN ANAL, V31, P129, DOI 10.1109/TPAMI.2008.46; Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120; Yang JL, 2013, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2013.184; Yao JH, 2012, LECT NOTES COMPUT SC, V7512, P509, DOI 10.1007/978-3-642-33454-2_63	28	2	2	0	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	2018	40	1					179	191		10.1109/TPAMI.2017.2654245	http://dx.doi.org/10.1109/TPAMI.2017.2654245			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	FP7IH	28103547	Green Submitted			2022-12-18	WOS:000417806000014
J	Noh, YK; Hamm, J; Park, FC; Zhang, BT; Lee, DD				Noh, Yung-Kyun; Hamm, Jihun; Park, Frank Chongwoo; Zhang, Byoung-Tak; Lee, Daniel D.			Fluid Dynamic Models for Bhattacharyya-Based Discriminant Analysis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Discriminant analysis; dimensionality reduction; fluid dynamics; Gauss principle of least constraint; Gaussian processes	FEATURE-EXTRACTION; FEATURE-SELECTION; ERROR; DIVERGENCE; OPTIMALITY; FRAMEWORK	Classical discriminant analysis attempts to discover a low-dimensional subspace where class label information is maximally preserved under projection. Canonical methods for estimating the subspace optimize an information-theoretic criterion that measures the separation between the class-conditional distributions. Unfortunately, direct optimization of the information-theoretic criteria is generally non-convex and intractable in high-dimensional spaces. In this work, we propose a novel, tractable algorithm for discriminant analysis that considers the class-conditional densities as interacting fluids in the high-dimensional embedding space. We use the Bhattacharyya criterion as a potential function that generates forces between the interacting fluids, and derive a computationally tractable method for finding the low-dimensional subspace that optimally constrains the resulting fluid flow. We show that this model properly reduces to the optimal solution for homoscedastic data as well as for heteroscedastic Gaussian distributions with equal means. We also extend this model to discover optimal filters for discriminating Gaussian processes and provide experimental results and comparisons on a number of datasets.	[Noh, Yung-Kyun; Park, Frank Chongwoo] Seoul Natl Univ, Dept Mech & Aerosp Engn, Seoul 08826, South Korea; [Hamm, Jihun] Ohio State Univ, Dept Comp Sci & Engn, Bryan, OH 43506 USA; [Zhang, Byoung-Tak] Seoul Natl Univ, Sch Comp Sci & Engn, Seoul 08826, South Korea; [Lee, Daniel D.] Univ Penn, Dept Elect & Syst Engn, Philadelphia, PA 19104 USA	Seoul National University (SNU); University System of Ohio; Ohio State University; Seoul National University (SNU); University of Pennsylvania	Noh, YK (corresponding author), Seoul Natl Univ, Dept Mech & Aerosp Engn, Seoul 08826, South Korea.	nohyung@snu.ac.kr; hamm.95@osu.edu; fcp@snu.ac.kr; btzhang@snu.ac.kr; ddlee@seas.upenn.edu			NSRI; BK21Plus [MITIP-10048320]; AFOSR; OFRN-C4ISR; NSF [IIS EAGER 1550757]; BMRR; Soft Robot ERC; U.S. NSF; ONR; ARL; DOT; DARPA;  [IITP-R0126-16-1072];  [KEIT-10060086];  [KEIT-10044009]	NSRI; BK21Plus; AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); OFRN-C4ISR; NSF(National Science Foundation (NSF)); BMRR; Soft Robot ERC; U.S. NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); ARL(United States Department of DefenseUS Army Research Laboratory (ARL)); DOT; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); ; ; 	YKN is supported by grants from NSRI, BK21Plus, MITIP-10048320, AFOSR, JHH from OFRN-C4ISR, NSF IIS EAGER 1550757, FCP from BK21Plus, MITIP-10048320, BMRR, Soft Robot ERC, BTZ from IITP-R0126-16-1072, KEIT-10060086, KEIT-10044009, and DDL from the U.S. NSF, ONR, ARL, AFOSR, DOT, DARPA.	Abou-Moustafa KT, 2010, PROC CVPR IEEE, P3602, DOI 10.1109/CVPR.2010.5539925; Alvarez M., 2009, ARTIF INTELL, P9; Baudat G, 2000, NEURAL COMPUT, V12, P2385, DOI 10.1162/089976600300014980; Campbell N. A., 1984, AUSTR J STAT, V26, P86, DOI DOI 10.1111/J.1467-842X.1984.TB01271.X; Carneiro G, 2009, IMAGE VISION COMPUT, V27, P131, DOI 10.1016/j.imavis.2006.06.008; Choi E, 2003, PATTERN RECOGN, V36, P1703, DOI 10.1016/S0031-3203(03)00035-9; Das K, 2008, PATTERN RECOGN, V41, P1548, DOI 10.1016/j.patcog.2007.10.001; De la Torre F, 2003, INT J COMPUT VISION, V54, P117, DOI 10.1023/A:1023709501986; De la Torre F., 2005, PROC 22 INT C MACH L, P177; Duda R.O., 2000, PATTERN CLASSIFICATI; Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Evans D.J., 2008, STAT MECH NONEQUILIB; Fano R. M., 1961, TRANSMISSION INFORM; Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x; FUKUNAGA K, 1983, IEEE T PATTERN ANAL, V5, P671, DOI 10.1109/TPAMI.1983.4767461; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; Hamsici OC, 2008, IEEE T PATTERN ANAL, V30, P647, DOI 10.1109/TPAMI.2007.70717; Hastie T, 1996, J ROY STAT SOC B MET, V58, P155; HELLMAN ME, 1970, IEEE T INFORM THEORY, V16, P368, DOI 10.1109/TIT.1970.1054466; Hillel A., 2003, P 20 INT C MACH LEAR, P11; HUBER PJ, 1985, ANN STAT, V13, P435, DOI 10.1214/aos/1176349519; Iosifidis A, 2013, IEEE T NEUR NET LEAR, V24, P1491, DOI 10.1109/TNNLS.2013.2258937; Kumar N, 1998, SPEECH COMMUN, V26, P283, DOI 10.1016/S0167-6393(98)00061-2; Lifshitz E. M., 1987, FLUID MECH, V6; LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115; Liu XW, 2004, IEEE T PATTERN ANAL, V26, P662, DOI 10.1109/TPAMI.2004.1273986; Loog M, 2004, IEEE T PATTERN ANAL, V26, P732, DOI 10.1109/TPAMI.2004.13; Mackey L., 2009, P ADV NEUR INF PROC, V21, P1017; Mika S., 1999, Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468), P41, DOI 10.1109/NNSP.1999.788121; Nenadic Z, 2007, IEEE T PATTERN ANAL, V29, P1394, DOI 10.1109/TPAMI.2007.1156; North B, 2000, IEEE T PATTERN ANAL, V22, P1016, DOI 10.1109/34.877523; Padmanabhan M, 2005, IEEE T SPEECH AUDI P, V13, P512, DOI 10.1109/TSA.2005.848876; Principe J. C., 1999, J VLSI SYST, V26, P61; PRINCIPE JC, 2000, UNSUPERVISED ADAPTIV; Rao C.R., 1945, BULL CALCUTTA MATH S, V37, P81, DOI DOI 10.1007/978-1-4612-0919-5_15; Saon G., 2000, NIPS, P800; Scholkopf B., 2001, LEARNING KERNELS SUP; Tao DC, 2007, IEEE DATA MINING, P302, DOI 10.1109/ICDM.2007.105; Tao DC, 2009, IEEE T PATTERN ANAL, V31, P260, DOI 10.1109/TPAMI.2008.70; Torkkola K., 2003, Journal of Machine Learning Research, V3, P1415, DOI 10.1162/153244303322753742; Urtasun R., 2007, P 24 INT C MACH LEAR, P927; Vasconcelos N, 2003, PROC CVPR IEEE, P762; Vasconcelos N., 2002, P ADV NEUR INF PROC, P1375; Yang SH, 2012, IEEE T KNOWL DATA EN, V24, P1422, DOI 10.1109/TKDE.2011.92; You D, 2011, IEEE T PATTERN ANAL, V33, P631, DOI 10.1109/TPAMI.2010.173; You D, 2010, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR.2010.5539952; Zhao MJ, 2013, J MACH LEARN RES, V14, P1033; Zhu ML, 2006, IEEE T PATTERN ANAL, V28, P1274, DOI 10.1109/TPAMI.2006.172; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	51	2	2	0	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	2018	40	1					92	105		10.1109/TPAMI.2017.2666148	http://dx.doi.org/10.1109/TPAMI.2017.2666148			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	FP7IH	28186879	hybrid			2022-12-18	WOS:000417806000008
J	Chen, DP; Yuan, ZJ; Hua, G; Wang, JD; Zheng, NN				Chen, Dapeng; Yuan, Zejian; Hua, Gang; Wang, Jingdong; Zheng, Nanning			Multi-Timescale Collaborative Tracking	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Visual tracking; multi-timescale; descriptive; discriminative; regressive; context; collaboration	ROBUST OBJECT TRACKING; VISUAL TRACKING; SPATIOTEMPORAL CONTEXT; MODEL	We present the multi-timescale collaborative tracker for single object tracking. The tracker simultaneously utilizes different types of "forces", namely attraction, repulsion and support, to take advantage of their complementary strengths. We model the three forces via three components that are learned from the sample sets with different timescales. The long-term descriptive component attracts the target sample, while the medium-term discriminative component repulses the target from the background. They are collaborated in the appearance model to benefit each other. The short-term regressive component combines the votes of the auxiliary samples to predict the target's position, forming the context-aware motion model. The appearance model and the motion model collaboratively determine the target state, and the optimal state is estimated by a novel coarse-to-fine search strategy. We have conducted an extensive set of experiments on the standard 50 video benchmark. The results confirm the effectiveness of each component and their collaboration, outperforming current state-of-the-art methods.	[Chen, Dapeng; Yuan, Zejian; Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China; [Hua, Gang] Stevens Inst Technol, Hoboken, NJ 07030 USA; [Wang, Jingdong] Microsoft Res, Beijing, Peoples R China	Xi'an Jiaotong University; Stevens Institute of Technology; Microsoft	Chen, DP (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.	dapengchenxjtu@foxmail.com; yuan.ze.jian@mail.xjtu.edu.cn; ghua@stevens.edu; jingdw@microsoft.com; nnzheng@mail.xjtu.edu.cn	Wang, Jingdong/E-9920-2017	Wang, Jingdong/0000-0002-4888-4445	National Basic Research Program of China [2015CB351703]; National Natural Science Foundation of China [61573280, 61231018]; 111 Project [B13043]	National Basic Research Program of China(National Basic Research Program of China); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); 111 Project(Ministry of Education, China - 111 Project)	This work was supported by National Basic Research Program of China (No. 2015CB351703), National Natural Science Foundation of China (No. 61573280, No. 61231018), and 111 Project (No. B13043).	Adam A., 2006, IEEE C COMP VIS PATT; [Anonymous], 2007, INT C MACH LEARN; Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226; Bordes A, 2005, LECT NOTES ARTIF INT, V3720, P505, DOI 10.1007/11564096_48; Bordes A, 2008, LECT NOTES ARTIF INT, V5211, P146, DOI 10.1007/978-3-540-87479-9_28; Chen DP, 2013, IEEE I CONF COMP VIS, P1113, DOI 10.1109/ICCV.2013.142; Chen DP, 2014, LECT NOTES COMPUT SC, V8689, P345, DOI 10.1007/978-3-319-10590-1_23; Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143; Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x; Dinh TB, 2011, PROC CVPR IEEE, P1177, DOI 10.1109/CVPR.2011.5995733; Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038; Gall J, 2011, IEEE T PATTERN ANAL, V33, P2188, DOI 10.1109/TPAMI.2011.70; Grabner H, 2010, PROC CVPR IEEE, P1285, DOI 10.1109/CVPR.2010.5539819; Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251; Henriques J. F., 2012, EUR C COMP VIS, P702, DOI DOI 10.1007/978-3-642-33765-9_50; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Hong S, 2014, LECT NOTES COMPUT SC, V8689, P1, DOI 10.1007/978-3-319-10590-1_1; Hong ZB, 2014, LECT NOTES COMPUT SC, V8694, P155, DOI 10.1007/978-3-319-10599-4_11; Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; Krahnstoever N, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P494; Kwon J, 2011, IEEE I CONF COMP VIS, P1195, DOI 10.1109/ICCV.2011.6126369; Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821; Liu BY, 2013, IEEE T PATTERN ANAL, V35, P2968, DOI 10.1109/TPAMI.2012.215; Mei X, 2011, IEEE T PATTERN ANAL, V33, P2259, DOI 10.1109/TPAMI.2011.66; Nguyen HT, 2007, IEEE T PATTERN ANAL, V29, P52, DOI 10.1109/TPAMI.2007.250599; Oron S, 2014, LECT NOTES COMPUT SC, V8693, P142, DOI 10.1007/978-3-319-10602-1_10; Park DW, 2012, PROC CVPR IEEE, P1964, DOI 10.1109/CVPR.2012.6247898; Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7; Saffari Amir, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1393, DOI 10.1109/ICCVW.2009.5457447; Scholkopf B, 2002, LECT NOTES ARTIF INT, V2600, P41; Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891; Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230; Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Vapnik VN, 1998, STAT LEARNING THEORY, DOI DOI 10.1007/978-1-4419-1428-6_5864; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; Wang HZ, 2007, IEEE T PATTERN ANAL, V29, P1661, DOI [10.1109/TPAMI.2007.1112, 10.1109/TPAMl.2007.1112]; Wen LY, 2014, IEEE T IMAGE PROCESS, V23, P785, DOI 10.1109/TIP.2013.2293430; Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226; Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312; Xing JL, 2013, IEEE I CONF COMP VIS, P665, DOI 10.1109/ICCV.2013.88; Yang M, 2009, IEEE T PATTERN ANAL, V31, P1195, DOI 10.1109/TPAMI.2008.146; Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9; Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882; Zhou BL, 2015, INT J COMPUT VISION, V111, P50, DOI 10.1007/s11263-014-0735-3	48	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	2017	39	1					141	155		10.1109/TPAMI.2016.2539956	http://dx.doi.org/10.1109/TPAMI.2016.2539956			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	EF6DP					2022-12-18	WOS:000390421300013
J	Strelow, D; Wang, QF; Si, L; Eriksson, A				Strelow, Dennis; Wang, Qifan; Si, Luo; Eriksson, Anders			General, Nested, and Constrained Wiberg Minimization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Low-rank matrix factorization; L-1 minimization; successive linear programming; structure-from-motion; multiple instance learning	ALGORITHM; FACTORIZATION	Wiberg matrix factorization breaks a matrix Y into low-rank factors U and V by solving for V in closed form given U, linearizing V(U) about U, and iteratively minimizing parallel to Y - UV (U)parallel to(2) with respect to U only. This approach factors the matrix while effectively removing V from the minimization. Recently Eriksson and van den Hengel extended this approach to L-1, minimizing parallel to Y - UV (U)parallel to(1). We generalize their approach beyond factorization to minimize parallel to Y - f (U, V)parallel to(1) for more general functions f(U, V) that are nonlinear in each of two sets of variables. We demonstrate the idea with a practical Wiberg algorithm for L-1 bundle adjustment. One Wiberg minimization can be nested inside another, effectively removing two of three sets of variables from a minimization. We demonstrate this idea with a nested Wiberg algorithm for L-1 projective bundle adjustment, solving for camera matrices, points, and projective depths. Wiberg minimization also generalizes to handle nonlinear constraints, and we demonstrate this idea with Constrained Wiberg Minimization for Multiple Instance Learning (CWM-MIL), which removes one set of variables from the constrained optimization. Our experiments emphasize isolating the effect of Wiberg by comparing against the algorithm it modifies, successive linear programming.	[Strelow, Dennis; Wang, Qifan] Google, 1600 Amphitheatre Pkwy, Mountain View, CA 95118 USA; [Si, Luo] Purdue Univ, Dept Comp Sci, 300 N Univ St, W Lafayette, IN 47907 USA; [Eriksson, Anders] Queensland Univ Technol, Sch Elect Engn & Comp Sci, GPO Box 2434, Brisbane, Qld 4001, Australia	Google Incorporated; Purdue University System; Purdue University; Purdue University West Lafayette Campus; Queensland University of Technology (QUT)	Strelow, D (corresponding author), Google, 1600 Amphitheatre Pkwy, Mountain View, CA 95118 USA.	strelow@google.com; wqfcr@google.com; lsi@purdue.edu; anders.eriksson@qut.edu.au	wang, qi/HGV-1859-2022	Eriksson, Anders/0000-0003-2652-7110	Australian Research Council [DE130101775]	Australian Research Council(Australian Research Council)	The authors thank Emilie Danna for her linear programming advice, which made the large-scale rover result possible, as described in Section 4.3; to Jay Yagnik, Luca Bertelli, Mei Han, Vivek Kwatra, Mohamed Eldawy, and Rich Gossweiler for feedback on early versions of this paper; Jim Teza, Chris Urmson, Michael Wagner, and David Wettergreen for capturing the "rover" sequence. They also want to thank the anonymous reviewers, who went far beyond the call of duty, for their time and expertise; and the reviewers of an earlier version for suggesting the experimental comparison between L<INF>1</INF> and least squares bundle adjustment and pointing out the Huber norm. Dr. Eriksson's research was supported by the Australian Research Council through the project DE130101775.	Andrews S., 2002, NIPS, V2, P561; Bazaraa MS., 2013, NONLINEAR PROGRAMMIN; Bergeron C, 2012, IEEE T PATTERN ANAL, V34, P1068, DOI 10.1109/TPAMI.2011.194; Bertsimas D., 1997, INTRO LINEAR OPTIMIZ; Borwein J., 2000, CMS BOOKS MATH; Chen YX, 2006, IEEE T PATTERN ANAL, V28, P1931, DOI 10.1109/TPAMI.2006.248; Eriksson A., 2010, P COMP VIS PATT REC; Eriksson A, 2012, IEEE T PATTERN ANAL, V34, P1681, DOI 10.1109/TPAMI.2012.116; Fackler P. L., 2005, NOTES MATRIX CALCULU; Forsyth David A, 2012, COMPUTER VISION MODE; Fu ZY, 2009, PROC CVPR IEEE, P911, DOI 10.1109/CVPRW.2009.5206655; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Hu Y, 2008, PROC CVPR IEEE, P85; Ke QF, 2005, PROC CVPR IEEE, P739; Liu YG, 2012, IMAGE VISION COMPUT, V30, P915, DOI 10.1016/j.imavis.2012.06.012; Mangasarian OL, 2008, J OPTIMIZ THEORY APP, V137, P555, DOI 10.1007/s10957-007-9343-5; Okatani T., 2007, INT J COMPUT VIS, V72; Okatani T, 2011, IEEE I CONF COMP VIS, P842, DOI 10.1109/ICCV.2011.6126324; Oliensis J, 2007, IEEE T PATTERN ANAL, V29, P2217, DOI 10.1109/TPAMI.2007.1132; Petersen K. B., 2012, MATRIX COOKBOOK; Poelman C., 1995, THESIS; RICHARDS FS, 1961, J ROY STAT SOC B, V23, P469; RUHE A, 1980, SIAM REV, V22, P318, DOI 10.1137/1022057; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Strelow D, 2012, LECT NOTES COMPUT SC, V7578, P195, DOI 10.1007/978-3-642-33786-4_15; Strelow D, 2012, PROC CVPR IEEE, P1584, DOI 10.1109/CVPR.2012.6247850; Wang QF, 2014, AAAI CONF ARTIF INTE, P1334; Wang QF, 2012, LECT NOTES COMPUT SC, V7575, P660, DOI 10.1007/978-3-642-33765-9_47; Wang YX, 2015, INT J COMPUT VISION, V111, P315, DOI 10.1007/s11263-014-0746-0; Wiberg T, 1976, P 2 S COMP STAT, P229; Yang C., 2006, COMPUTER VISION PATT, P2057, DOI DOI 10.1109/CVPR.2006.250; Zhang D, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P149; Zhang DM, 2013, IEEE SOFTWARE, V30, P30, DOI 10.1109/MS.2013.94; Zheng Y., 2013, P IEEE C COMP VIS PA	34	2	3	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2016	38	9					1803	1815		10.1109/TPAMI.2015.2487987	http://dx.doi.org/10.1109/TPAMI.2015.2487987			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DT4EK	26452252				2022-12-18	WOS:000381432700007
J	Liu, LQ; Wang, L; Shen, CH				Liu, Lingqiao; Wang, Lei; Shen, Chunhua			A Generalized Probabilistic Framework for Compact Codebook Creation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Max-margin estimation; compact codebook; bag-of-features model; image classification		Compact and discriminative visual codebooks are preferred in many visual recognition tasks. In the literature, a number of works have taken the approach of hierarchically merging visual words of an initial large-sized codebook, but implemented this approach with different merging criteria. In this work, we propose a single probabilistic framework to unify these merging criteria, by identifying two key factors: the function used to model the class-conditional distribution and the method used to estimate the distribution parameters. More importantly, by adopting new distribution functions and/or parameter estimation methods, our framework can readily produce a spectrum of novel merging criteria. Three of them are specifically discussed in this paper. For the first criterion, we adopt the multinomial distribution with the Bayesian method; For the second criterion, we integrate the Gaussian distribution with maximum likelihood parameter estimation. For the third criterion, which shows the best merging performance, we propose a max-margin-based parameter estimation method and apply it with the multinomial distribution. Extensive experimental study is conducted to systematically analyze the performance of the above three criteria and compare them with existing ones. As demonstrated, the best criterion within our framework achieves the overall best merging performance among the compared merging criteria developed in the literature.	[Liu, Lingqiao; Shen, Chunhua] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia; [Wang, Lei] Univ Wollongong, Sch Comp & Informat Technol, Wollongong, NSW 2500, Australia	University of Adelaide; University of Wollongong	Liu, LQ; Shen, CH (corresponding author), Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.; Wang, L (corresponding author), Univ Wollongong, Sch Comp & Informat Technol, Wollongong, NSW 2500, Australia.	lingqiao.liu@adelaide.edu.au; leiw@uow.edu.au; chunhua.shen@adelaide.edu.au	Wang, Lei/D-9079-2013; Wang, Lei/AAL-9684-2020	Wang, Lei/0000-0002-0961-0441; liu, lingqiao/0000-0003-3584-795X	Data to Decisions Cooperative Research Centre; ARC [LP0991757]; ARC Future Fellowship [FT120100969]	Data to Decisions Cooperative Research Centre; ARC(Australian Research Council); ARC Future Fellowship(Australian Research Council)	This work was in part supported by the Data to Decisions Cooperative Research Centre; ARC Linkage Grant LP0991757; and ARC Future Fellowship (FT120100969).	Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fulkerson B, 2008, LECT NOTES COMPUT SC, V5302, P179, DOI 10.1007/978-3-540-88682-2_15; Griffin G., CALTECH 256 OBJECT C; Herbich R, 2001, J MACH LEARN RES, V1, P245, DOI 10.1162/153244301753683717; Jaynes E. T., 1988, KLUWER ACAD PUBLISHE, V11, P25; Jurie F, 2005, IEEE I CONF COMP VIS, P604; Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Lazebnik S, 2009, IEEE T PATTERN ANAL, V31, P1294, DOI 10.1109/TPAMI.2008.138; Liu J, 2008, SCI SIGNAL, V1, DOI [10.1126/stke.114re1, 10.1126/scisignal.123tr3]; Liu LQ, 2011, PROC CVPR IEEE, P1537, DOI 10.1109/CVPR.2011.5995628; Minka Thomas P, 2003, ESTIMATING DIRICHLET; Moosmann F., 2007, ADV NEURAL INF PROCE, V19, P985; Opelt A, 2005, LECT NOTES COMPUT SC, V3540, P862; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Porikli F, 2005, PROC CVPR IEEE, P829, DOI 10.1109/CVPR.2005.188; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Shen C, 2008, PATTERN RECOGN, V41, P3644, DOI 10.1016/j.patcog.2008.06.015; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Slonim N, 2000, ADV NEUR IN, V12, P617; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; WANG JJ, 2010, PROC CVPR IEEE, P3360, DOI DOI 10.1109/CVPR.2010.5540018; Wang L, 2008, LECT NOTES COMPUT SC, V5305, P719, DOI 10.1007/978-3-540-88693-8_53; Winn J, 2005, IEEE I CONF COMP VIS, P1800	27	2	2	0	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2016	38	2					224	237		10.1109/TPAMI.2015.2441069	http://dx.doi.org/10.1109/TPAMI.2015.2441069			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DD5UI	26761730	Green Submitted			2022-12-18	WOS:000369989600003
J	Behl, A; Mohapatra, P; Jawahar, CV; Kumar, MP				Behl, Aseem; Mohapatra, Pritish; Jawahar, C. V.; Kumar, M. Pawan			Optimizing Average Precision Using Weakly Supervised Data	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Weakly supervised learning; average precision; Latent SVM	RECOGNITION	Many tasks in computer vision, such as action classification and object detection, require us to rank a set of samples according to their relevance to a particular visual category. The performance of such tasks is often measured in terms of the average precision (AP). Yet it is common practice to employ the support vector machine (SVM) classifier, which optimizes a surrogate 0-1 loss. The popularity of SVMcan be attributed to its empirical performance. Specifically, in fully supervised settings, SVM tends to provide similar accuracy to AP-SVM, which directly optimizes an AP-based loss. However, we hypothesize that in the significantly more challenging and practically useful setting of weakly supervised learning, it becomes crucial to optimize the right accuracy measure. In order to test this hypothesis, we propose a novel latent AP-SVM that minimizes a carefully designed upper bound on the AP-based loss function over weakly supervised samples. Using publicly available datasets, we demonstrate the advantage of our approach over standard loss-based learning frameworks on three challenging problems: action classification, character recognition and object detection.	[Behl, Aseem; Mohapatra, Pritish; Jawahar, C. V.] IIIT Hyderabad, Ctr Visual Informat Technol, Hyderabad 500032, Andhra Pradesh, India; [Kumar, M. Pawan] Cent Paris & INRIA Saclay, Paris, France	International Institute of Information Technology Hyderabad	Behl, A (corresponding author), IIIT Hyderabad, Ctr Visual Informat Technol, Hyderabad 500032, Andhra Pradesh, India.	aseembehl@gmail.com; pritish.mohapatra@research.iiit.ac.in; jawahar@iiit.ac.in; pawan.kumar@ecp.fr		Kumar, Manish/0000-0002-1311-0976; Jawahar, C. V./0000-0001-6767-7057	European Research Council under the European Communitys Seventh Framework Programme /ERC [259112]; INRIA International Internship Programme	European Research Council under the European Communitys Seventh Framework Programme /ERC; INRIA International Internship Programme	Pritish Mohapatra is the corresponding author. This work was partially funded by the European Research Council under the European Communitys Seventh Framework Programme (FP7/2007-2013)/ERC Grant agreement number 259112, and the INRIA International Internship Programme.	[Anonymous], 2007, PASCAL VISUAL OBJECT; Blaschko M., 2010, ADV NEURAL INFORM PR; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Delaitre V., 2010, BMVC, DOI DOI 10.5244/C.24.97; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deselaers T, 2010, LECT NOTES COMPUT SC, V6314, P452, DOI 10.1007/978-3-642-15561-1_33; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Felzenszwalb PF, 2010, DISCRIMINATIVELY TRA; Girshick R., 2014, P IEEE C COMPUTER VI, DOI 10.1109/CVPR.2014.81; JOACHIMS T, 1999, ADV KERNEL METHODS; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Korsah G. A., 2007, CMURITR0727; Kumar M.P., 2012, P INT C MACH LEARN, P465; Lan T, 2012, IEEE T PATTERN ANAL, V34, P1549, DOI 10.1109/TPAMI.2011.228; Lin YQ, 2011, PROC CVPR IEEE, P1689, DOI 10.1109/CVPR.2011.5995477; Maji S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3177, DOI 10.1109/CVPR.2011.5995631; Miller K., 2012, P 15 INT C ART INT S, P779; Mishra A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.127; Pandey M, 2011, IEEE I CONF COMP VIS, P1307, DOI 10.1109/ICCV.2011.6126383; Prest A, 2012, IEEE T PATTERN ANAL, V34, P601, DOI 10.1109/TPAMI.2011.158; Russakovsky O, 2012, LECT NOTES COMPUT SC, V7573, P1, DOI 10.1007/978-3-642-33709-3_1; SHALEV- SHWARTZ S., 2007, P 24 INT C MACH LEAR, P807, DOI [DOI 10.1145/1273496.1273598, 10.1145/1273496.1273598]; Smola AJ, 2005, P 10 INT WORKSH ART, P325; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Vapnik V.N, 1998, STAT LEARNING THEORY; Vedaldi A, 2009, IEEE I CONF COMP VIS, P606, DOI 10.1109/ICCV.2009.5459183; Vezhnevets A, 2012, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2012.6247757; Wang HY, 2010, LECT NOTES COMPUT SC, V6312, P435, DOI 10.1007/978-3-642-15552-9_32; Wang Y, 2010, LECT NOTES COMPUT SC, V6315, P155, DOI 10.1007/978-3-642-15555-0_12; Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757; Yang L., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/DYSPAN.2008.47; Yang WL, 2010, PROC CVPR IEEE, P2030, DOI 10.1109/CVPR.2010.5539879; Yao BP, 2011, IEEE I CONF COMP VIS, P1331, DOI 10.1109/ICCV.2011.6126386; Yisong Yue, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P271; Yu C.-N. J., 2009, P 26 ANN INT C MACHI, P1169, DOI [10.1145/1553374.1553523, DOI 10.1145/1553374.1553523]; Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958	38	2	3	0	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2015	37	12					2545	2557		10.1109/TPAMI.2015.2414435	http://dx.doi.org/10.1109/TPAMI.2015.2414435			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CW2OK	26539857	Green Submitted			2022-12-18	WOS:000364831700015
J	Liang, S; Luo, J; Liu, WY; Wei, YC				Liang, Shuang; Luo, Jun; Liu, Wenyin; Wei, Yichen			Sketch Matching on Topology Product Graph	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Sketch matching; topology relations; similarity metrics	RETRIEVAL	Sketch matching is the fundamental problem in sketch based interfaces. After years of study, it remains challenging when there exists large irregularity and variations in the hand drawn sketch shapes. While most existing works exploit topology relations and graph representations for this problem, they are usually limited by the coarse topology exploration and heuristic (thus suboptimal) similarity metrics between graphs. We present a new sketch matching method with two novel contributions. We introduce a comprehensive definition of topology relations, which results in a rich and informative graph representation of sketches. For graph matching, we propose topology product graph that retains the full correspondence for matching two graphs. Based on it, we derive an intuitive sketch similarity metric whose exact solution is easy to compute. In addition, the graph representation and new metric naturally support partial matching, an important practical problem that received less attention in the literature. Extensive experimental results on a real challenging dataset and the superior performance of our method show that it outperforms the state-of-the-art.	[Liang, Shuang] Tongji Univ, Sch Software Engn, Shanghai 200092, Peoples R China; [Luo, Jun] Huawei Noahs Ark Lab, Hong Kong, Hong Kong, Peoples R China; [Liu, Wenyin] City Univ Hong Kong, Multimedia Software Engn Res Ctr, Hong Kong, Hong Kong, Peoples R China; [Wei, Yichen] Microsoft Res Asia, Beijing, Peoples R China	Tongji University; Huawei Technologies; City University of Hong Kong; Microsoft; Microsoft Research Asia	Liang, S (corresponding author), Tongji Univ, Sch Software Engn, Shanghai 200092, Peoples R China.	shuangliang@tongji.edu.cn; luo.jun1@huawei.com; liuwenyin@gmail.com; yichenw@microsoft.com		Liang, Shuang/0000-0003-0457-6093	National Science Foundation of China [61305091, 61305094, 11271351]; Fundamental Research Funds for the Central Universities [2100219038]; Shanghai Pujiang Program [13PJ1408200]	National Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Shanghai Pujiang Program(Shanghai Pujiang Program)	This work was supported by The National Science Foundation of China (No. 61305091, No. 61305094, No. 11271351), The Fundamental Research Funds for the Central Universities (No. 2100219038), and Shanghai Pujiang Program (No. 13PJ1408200), Shuang Liang is the corresponding author.	Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463; Berchtold S., 1997, SIGMOD Record, V26, P564, DOI 10.1145/253262.253407; Demirci MF, 2008, COMPUT VIS IMAGE UND, V110, P312, DOI 10.1016/j.cviu.2007.09.012; Eitz M., 2012, ACM T GRAPHIC, V31, P1, DOI DOI 10.1145/2185520.2335395; Fonseca MJ, 2011, SKETCH-BASED INTERFACES AND MODELING, P181, DOI 10.1007/978-1-84882-812-4_7; Horn R.A., 2013, MATRIX ANAL, P321; Jin XY, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P256, DOI 10.1109/PCCGA.2002.1167869; Konc J, 2007, MATCH-COMMUN MATH CO, V58, P569; Leung WH, 2002, IEEE IMAGE PROC, P908; Li CH, 2000, PROCEEDINGS OF THE 3RD WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION, VOLS 1-5, P2549, DOI 10.1109/WCICA.2000.862507; Liang S, 2008, PATTERN RECOGN LETT, V29, P1733, DOI 10.1016/j.patrec.2008.05.004; Liang SN, 2005, COMPUTER GRAPHICS, IMAGING AND VISION: NEW TRENDS, P24; Liu WY, 2010, LECT NOTES COMPUT SC, V6020, P130; Llados J, 2001, IEEE T PATTERN ANAL, V23, P1137, DOI 10.1109/34.954603; Lu T., 2014, HDB DOCUMENT IMAGE P; Luqman MM, 2010, LECT NOTES COMPUT SC, V6020, P12; Pelillo M, 1999, IEEE T PATTERN ANAL, V21, P1105, DOI 10.1109/34.809105; Robson J.M., 2001, 125101 LABRI U BORD; SEZGIN TM, 2001, THESIS MIT CAMBRIDGE; Shuang Liang, 2011, 2011 10th IEEE International Conference on Cognitive Informatics & Cognitive Computing (ICCI-CC 2011), P340, DOI 10.1109/COGINF.2011.6016163; Sousa P, 2010, J VISUAL LANG COMPUT, V21, P69, DOI 10.1016/j.jvlc.2009.12.001; Sun X, 2013, P 21 ACM INT C MULT, P233, DOI DOI 10.1145/2502081.2502281; van Leuken Reinier H., 2008, Proceedings of the Fifth IASTED International Conference on Signal Processing, Pattern Recognition, and Applications, P93; Xu Xiaogang, 2004, International Journal on Document Analysis and Recognition, V7, P44, DOI 10.1007/s10032-004-0126-3	24	2	2	0	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	2015	37	8					1723	1729		10.1109/TPAMI.2014.2369031	http://dx.doi.org/10.1109/TPAMI.2014.2369031			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CM3ON	26353007				2022-12-18	WOS:000357591900015
J	Kim, J; Grauman, K				Kim, Jaechul; Grauman, Kristen			Boundary Preserving Dense Local Regions	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Local feature; feature matching; shapes; segmentation; object recognition; distance transform	OBJECT; RECOGNITION; FEATURES; KERNEL; SCALE	We propose a dense local region detector to extract features suitable for image matching and object recognition tasks. Whereas traditional local interest operators rely on repeatable structures that often cross object boundaries (e.g., corners, scale-space blobs), our sampling strategy is driven by segmentation, and thus preserves object boundaries and shape. At the same time, whereas existing region-based representations are sensitive to segmentation parameters and object deformations, our novel approach to robustly sample dense sites and determine their connectivity offers better repeatability. In extensive experiments, we find that the proposed region detector provides significantly better repeatability and localization accuracy for object matching compared to an array of existing feature detectors. In addition, we show our regions lead to excellent results on two benchmark tasks that require good feature matching: weakly supervised foreground discovery and nearest neighbor-based object recognition.	[Kim, Jaechul] Amazon, Hyderabad, Andhra Pradesh, India; [Grauman, Kristen] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	Amazon.com; University of Texas System; University of Texas Austin	Kim, J (corresponding author), Amazon, Hyderabad, Andhra Pradesh, India.	jaechulk@amazon.com; grauman@cs.utexas.edu			ONR [N00014-12-1-0068]	ONR(Office of Naval Research)	We thank the anonymous reviewers for their helpful suggestions. This research was supported in part by ONR N00014-12-1-0068. This work has been done while J. Kim was at the University of Texas at Austin.	Alexe B, 2010, LECT NOTES COMPUT SC, V6315, P380, DOI 10.1007/978-3-642-15555-0_28; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Arbelaez P, 2009, PROC CVPR IEEE, P2294, DOI 10.1109/CVPRW.2009.5206707; Berg AC, 2005, PROC CVPR IEEE, P26; BERG AC, 2005, THESIS U CALIFORNIA; Boiman O., 2008, P IEEE C COMP VIS PA; Bosch A., 2007, P 6 ACM INT C IM VID, V5, P401, DOI [10.1145/1282280.1282340, DOI 10.1145/1282280]; Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P2089, DOI 10.1109/TPAMI.2007.1126; Duchenne O, 2011, IEEE I CONF COMP VIS, P1792, DOI 10.1109/ICCV.2011.6126445; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Ferrari V, 2006, LECT NOTES COMPUT SC, V3953, P14, DOI 10.1007/11744078_2; Frome A., 2007, ICCV; Galleguillos C, 2008, LECT NOTES COMPUT SC, V5302, P193, DOI 10.1007/978-3-540-88682-2_16; Grauman K, 2005, IEEE I CONF COMP VIS, P1458; Gu CH, 2009, PROC CVPR IEEE, P1030, DOI 10.1109/CVPRW.2009.5206727; Jurie F, 2004, PROC CVPR IEEE, P90; Kim J, 2012, LECT NOTES COMPUT SC, V7578, P444, DOI 10.1007/978-3-642-33786-4_33; Kim J, 2013, PROC CVPR IEEE, P2307, DOI 10.1109/CVPR.2013.299; Kim J, 2011, PROC CVPR IEEE, P1553, DOI 10.1109/CVPR.2011.5995526; Kim J, 2010, PROC CVPR IEEE, P2344, DOI 10.1109/CVPR.2010.5539923; Koniusz P., 2009, P BRIT MACH VIS C; Lazebnik S., 2006, P IEEE INT C COMP VI, P2169, DOI DOI 10.1109/CVPR.2006.68; Lee Y., 2011, P IEEE INT C COMP VI, P444; Lee YJ, 2009, INT J COMPUT VISION, V85, P143, DOI 10.1007/s11263-009-0252-y; Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482; Levinshtein A, 2009, IEEE I CONF COMP VIS, P2162, DOI 10.1109/ICCV.2009.5459472; Liu C, 2009, PROC CVPR IEEE, P1972, DOI 10.1109/CVPRW.2009.5206536; Malisiewicz T., 2007, P BRIT MACH VIS C; Matas J., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P384; Meng H, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION, VOLS I-V, CONFERENCE PROCEEDINGS, P88, DOI 10.1109/ICMA.2007.4303521; Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Nowak E, 2006, LECT NOTES COMPUT SC, V3954, P490; Pantofaru C., 2006, P PATCH WORKSH BP CO; Quack T, 2007, IEEE I CONF COMP VIS, P612; Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Sabuncu MR, 2008, IEEE T IMAGE PROCESS, V17, P788, DOI 10.1109/TIP.2008.918951; Sebastian TB, 2004, IEEE T PATTERN ANAL, V26, P550, DOI 10.1109/TPAMI.2004.1273924; Todorovic S, 2008, PROC CVPR IEEE, P195; Tuytelaars T, 2010, PROC CVPR IEEE, P2281, DOI 10.1109/CVPR.2010.5539911; van de Sande KEA, 2011, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2011.6126456; Varma M., 2007, P IEEE INT C COMP VI, V1, P1, DOI DOI 10.1109/ICCV.2007.4408875; Zhang H, 2006, 2006 IEEE COMP SOC C, P2126, DOI [10.1109/CVPR.2006.301, DOI 10.1109/CVPR.2006.301]	44	2	2	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	2015	37	5					931	943		10.1109/TPAMI.2014.2360689	http://dx.doi.org/10.1109/TPAMI.2014.2360689			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CF4PS	26353319	Green Submitted			2022-12-18	WOS:000352533000003
J	Nock, R; Ali, WBH; D'Ambrosio, R; Nielsen, F; Barlaud, M				Nock, Richard; Ali, Wafa Bel Haj; D'Ambrosio, Roberto; Nielsen, Frank; Barlaud, Michel			Gentle Nearest Neighbors Boosting over Proper Scoring Rules	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Boosting; proper scoring rules; nearest neighbors		Tailoring nearest neighbors algorithms to boosting is an important problem. Recent papers study an approach, UNN, which provably minimizes particular convex surrogates under weak assumptions. However, numerical issues make it necessary to experimentally tweak parts of the UNN algorithm, at the possible expense of the algorithm's convergence and performance. In this paper, we propose a lightweight Newton-Raphson alternative optimizing proper scoring rules from a very broad set, and establish formal convergence rates under the boosting framework that compete with those known for UNN. To the best of our knowledge, no such boosting-compliant convergence rates were previously known in the popular Gentle Adaboost's lineage. We provide experiments on a dozen domains, including Caltech and SUN computer vision databases, comparing our approach to major families including support vector machines, (Ada)boosting and stochastic gradient descent. They support three major conclusions: (i) GNNB significantly outperforms UNN, in terms of convergence rate and quality of the outputs, (ii) GNNB performs on par with or better than computationally intensive large margin approaches, (iii) on large domains that rule out those latter approaches for computational reasons, GNNB provides a simple and competitive contender to stochastic gradient descent. Experiments include a divide-and-conquer improvement of GNNB exploiting the link with proper scoring rules optimization.	[Ali, Wafa Bel Haj; Barlaud, Michel] Univ Nice Sophia Antipolis, CNRS, F-06189 Nice, France; [D'Ambrosio, Roberto] Univ Campus Biomed Rome, Rome, Italy; [Nielsen, Frank] Sony Comp Sci Labs Inc, Fundamental Res Lab, Tokyo, Japan; [Barlaud, Michel] Univ Nice Sophia Antipolis, Inst Univ France, F-06189 Nice, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Cote d'Azur; University Campus Bio-Medico - Rome Italy; Sony Corporation; Institut Universitaire de France; UDICE-French Research Universities; Universite Cote d'Azur	Nock, R (corresponding author), NICTA, Canberra, ACT, Australia.	richard.nock@nicta.com.au; belhajal@i3s.unice.fr; r.dambrosio@unicampus.it; nielsen@lix.polytechnique.fr; barlaud@i3s.unice.fr		Nielsen, Frank/0000-0001-5728-0726	Australian Government; Australian Research Council through the ICT Centre of Excellence program	Australian Government(Australian GovernmentCGIAR); Australian Research Council through the ICT Centre of Excellence program(Australian Research Council)	The authors would like to thank the reviewers for insightful comments. Codes used (GNNB and manifold learning) available upon request to Richard Nock and Michel Barlaud NICTA is funded by the Australian Government as represented by the Department of Broadband, Communication and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program.	[Anonymous], 2007, CNSTR2007001 CALTECH; Bache K., UCI MACHINE LEARNING; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Blum A, 2006, LECT NOTES COMPUT SC, V3940, P52; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Cucala L, 2009, J AM STAT ASSOC, V104, P263, DOI 10.1198/jasa.2009.0125; D'Ambrosio Roberto, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P314, DOI 10.1007/978-3-642-33460-3_26; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Garcia-Pedrajas N, 2009, EXPERT SYST APPL, V36, P10570, DOI 10.1016/j.eswa.2009.02.065; Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437; Grunwald PD, 2004, ANN STAT, V32, P1367, DOI 10.1214/009053604000000553; Kakade S., 2009, ABS09100610 CORR; Nock R., 2013, P 6 ACM SIGGRAPH AS; Nock R., 2008, NIPS 21, P1201; Nock R, 2013, ACM-IEEE J CONF DIG, P313; Nock R, 2012, INT J COMPUT VISION, V100, P294, DOI 10.1007/s11263-012-0539-2; Nock R, 2009, IEEE T PATTERN ANAL, V31, P2048, DOI 10.1109/TPAMI.2008.225; Perronnin F, 2012, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2012.6248090; Quinlan J., 2014, C4 5 PROGRAMS MACHIN, DOI DOI 10.1007/BF00993309; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901; Sebban M, 2003, J MACH LEARN RES, V3, P863, DOI 10.1162/jmlr.2003.3.4-5.863; SHALEV- SHWARTZ S., 2007, P 24 INT C MACH LEAR, P807, DOI [DOI 10.1145/1273496.1273598, 10.1145/1273496.1273598]; Vernet Elodie, 2011, NIPS 24, P1224; Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970	30	2	3	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	2015	37	1					80	93		10.1109/TPAMI.2014.2307877	http://dx.doi.org/10.1109/TPAMI.2014.2307877			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AX5ML	26353210	Green Submitted			2022-12-18	WOS:000346970600008
J	Shi, QF; Reid, M; Caetano, T; van den Hengel, A; Wang, ZH				Shi, Qinfeng; Reid, Mark; Caetano, Tiberio; Van den Hengel, Anton; Wang, Zhenhua			A Hybrid Loss for Multiclass and Structured Prediction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Conditional random fields; support vector machines; hybrid loss; fisher consistency; structured learning	CONSISTENCY	We propose a novel hybrid loss for multiclass and structured prediction problems that is a convex combination of a log loss for Conditional Random Fields (CRFs) and a multiclass hinge loss for Support Vector Machines (SVMs). We provide a sufficient condition for when the hybrid loss is Fisher consistent for classification. This condition depends on a measure of dominance between labels-specifically, the gap between the probabilities of the best label and the second best label. We also prove Fisher consistency is necessary for parametric consistency when learning models such as CRFs. We demonstrate empirically that the hybrid loss typically performs least as well as-and often better than-both of its constituent losses on a variety of tasks, such as human action recognition. In doing so we also provide an empirical comparison of the efficacy of probabilistic and margin based approaches to multiclass and structured prediction.	[Shi, Qinfeng; Van den Hengel, Anton; Wang, Zhenhua] Univ Adelaide, Australian Ctr Visual Technol, Adelaide, SA 5005, Australia; [Shi, Qinfeng; Van den Hengel, Anton; Wang, Zhenhua] Univ Adelaide, Comp Vis Grp, Adelaide, SA 5005, Australia; [Reid, Mark; Caetano, Tiberio] Australian Natl Univ, Canberra, ACT 0200, Australia; [Reid, Mark; Caetano, Tiberio] NICTA, Canberra, ACT, Australia	University of Adelaide; University of Adelaide; Australian National University; Australian National University	Shi, QF (corresponding author), Univ Adelaide, Australian Ctr Visual Technol, Adelaide, SA 5005, Australia.	javen.shi@adelaide.edu.au; mark.reid@anu.edu.au; Tiberio.Caetano@nicta.com.au; anton.vandenhengel@adelaide.edu.au; zhenhua.wang01@adelaide.edu.au		van den Hengel, Anton/0000-0003-3027-8364	Australian Government; Digital Economy and the Australian Research Council through the ICT Centre of Excellence program; Australian Research Council [DP0988439, DP140102270]; Australian Research Council Discovery Early Career Researcher Award funding scheme [DE120101161]; Australian Centre for Visual Technologies; Computer Vision group of The University of Adelaide	Australian Government(Australian GovernmentCGIAR); Digital Economy and the Australian Research Council through the ICT Centre of Excellence program(Australian Research Council); Australian Research Council(Australian Research Council); Australian Research Council Discovery Early Career Researcher Award funding scheme(Australian Research Council); Australian Centre for Visual Technologies; Computer Vision group of The University of Adelaide	The bulk of this research was performed while Q. Shi was with NICTA. NICTA was funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program. This research was partly supported under Australian Research Council Discovery Projects funding scheme (DP0988439 and DP140102270) and Australian Research Council Discovery Early Career Researcher Award funding scheme (DE120101161) and supported by The Australian Centre for Visual Technologies and The Computer Vision group of The University of Adelaide. The authors would like to thank the anonymous reviewers of an earlier version of this work for their constructive feedback, particularly regarding the Theorems 1 and 2.	BakIr G., 2007, PREDICTING STRUCTURE; Bottou L., 2010, STOCHASTIC GRADIENT; BYRD RH, 1994, MATH PROGRAM, V63, P129, DOI 10.1007/BF01582063; Chen DR, 2006, J MACH LEARN RES, V7, P2435; CoNLL, 2000, SHAR TASK C COMP NAT; Crammer K., 2000, P 13 ANN C COMP LEAR, P35; Dalal N., 2005, HISTOGRAMS ORIENTED; Koller D., 2009, PROBABILISTIC GRAPHI; Kudo T., 2010, CRF YET ANOTHER CRF; Liu Y., 2007, P INT C MACH LEARN; Lovegrove S, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.93; Lugosi G, 2004, ANN STAT, V32, P30; Patron-Perez A, 2012, IEEE T PATTERN ANAL, V34, P2441, DOI 10.1109/TPAMI.2012.24; Reid MD, 2010, J MACH LEARN RES, V11, P2387; Sha F, 2003, HLT-NAACL 2003: HUMAN LANGUAGE TECHNOLOGY CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE MAIN CONFERENCE, P213; Shalev-Shwartz Shai, 2007, P INT C MACH LEARN; Tewari A, 2007, J MACH LEARN RES, V8, P1007; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Zhang Zhihua, 2009, P 12 C ART INT STAT	20	2	2	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	2015	37	1					2	12		10.1109/TPAMI.2014.2306414	http://dx.doi.org/10.1109/TPAMI.2014.2306414			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AX5ML	26353204	Green Submitted			2022-12-18	WOS:000346970600002
J	Lott, GK				Lott, Gus K., III			Direct Orthogonal Distance to Quadratic Surfaces in 3D	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Orthogonal distance regression; direct methods; quadratic surface; foot-point; projective geometry		Discovering the orthogonal distance to a quadratic surface is a classic geometric task in vision, modeling, and robotics. I describe a simple, efficient, and stable direct solution for the orthogonal distance (foot-point) to an arbitrary quadratic surface from a general finite 3D point. The problem is expressed as the intersection of three quadratic surfaces, two of which are derived from the requirement of orthogonality of two non-coincident planes with the tangent plane to the quadric. A sixth order single-variable polynomial is directly generated in one coordinate of the surface point. The method detects intersection points at infinity and operates smoothly across all real quadratic surface classes. The method also geometrically detects continuums of orthogonal points (i.e., from the exact center of a sphere). I discuss algorithm performance, compare it to a state-of-the-art estimator, demonstrate the algorithm on synthetic data, and describe extension to arbitrary dimension.	Mitre Corp, Mclean, VA 22102 USA	MITRE Corporation	Lott, GK (corresponding author), Mitre Corp, 7525 Colshire Dr, Mclean, VA 22102 USA.	glott@mitre.org						Ahn SJ, 2002, IEEE T PATTERN ANAL, V24, P620, DOI 10.1109/34.1000237; Aigner M., 2008, P IEEE INT C SHAP MO; Akritas Alkiviadis G., 2005, NONLINEAR ANAL-MODEL, V10, P297; Atieg A, 2003, J COMPUT APPL MATH, V158, P277, DOI 10.1016/S0377-0427(03)00448-5; Chernov N., 2011, COMPUTER VISION, P285; CHIONH EW, 1991, ACM T GRAPHIC, V10, P378, DOI 10.1145/116913.116917; Cox D.A., 2005, USING ALGEBRAIC GEOM, V185; Eberly D., 2008, DISATANCE POINT GENE; GLASSNER A, 1990, GRAPHICS GEMS, V1; Hartley R., 2004, ROBOTICA; Hartley R, 2012, IEEE T PATTERN ANAL, V34, P2303, DOI 10.1109/TPAMI.2012.43; Morera D.M., 2003, REVISTA INVESTIGACIO, V24, P153; Rouhani M, 2012, IEEE T IMAGE PROCESS, V21, P2089, DOI 10.1109/TIP.2011.2170080; Semple J.G, 1952, ALGEBRAIC PROJECTIVE; Wijewickrema S.N.R., 2006, 2006 MON U CLAYT SCH; Xu ZQ, 2005, COMPUT AIDED GEOM D, V22, P515, DOI 10.1016/j.cagd.2005.02.001; Yan DM, 2012, COMPUT AIDED DESIGN, V44, P1072, DOI 10.1016/j.cad.2012.04.005; Yang C., 2009, P IEEE 11 INT C COMP	18	2	2	1	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2014	36	9					1888	1892		10.1109/TPAMI.2014.2302451	http://dx.doi.org/10.1109/TPAMI.2014.2302451			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AM9OE	26352239				2022-12-18	WOS:000340210100014
J	Kim, M				Kim, Minyoung			Conditional Alignment Random Fields for Multiple Motion Sequence Alignment	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Conditional random fields; sequence alignment; dynamic time warping; probabilistic models	HIDDEN MARKOV-MODELS	We consider the multiple time-series alignment problem, typically focusing on the task of synchronizing multiple motion videos of the same kind of human activity. Finding an optimal global alignment of multiple sequences is infeasible, while there have been several approximate solutions, including iterative pairwise warping algorithms and variants of hidden Markov models. In this paper, we propose a novel probabilistic model that represents the conditional densities of the latent target sequences which are aligned with the given observed sequences through the hidden alignment variables. By imposing certain constraints on the target sequences at the learning stage, we have a sensible model for multiple alignments that can be learned very efficiently by the EM algorithm. Compared to existing methods, our approach yields more accurate alignment while being more robust to local optima and initial configurations. We demonstrate its efficacy on both synthetic and real-world motion videos including facial emotions and human activities.	Seoul Natl Univ Sci & Technol, Dept Elect & IT Media Engn, Seoul 139743, South Korea	Seoul National University of Science & Technology	Kim, M (corresponding author), Seoul Natl Univ Sci & Technol, Dept Elect & IT Media Engn, 232 Gongneung Ro, Seoul 139743, South Korea.	mikim21@gmail.com						Baak A., 2010, P EUR C COMP VIS SEP; Baak A., 2008, P ACM INT C MULT INF; BARTON GJ, 1987, J MOL BIOL, V198, P327, DOI 10.1016/0022-2836(87)90316-0; BERNDT D, 1994, P AAAI WORKSH KNOWL; Blackshields Gordon, 2006, In Silico Biol, V6, P321; Durbin R., 1998, BIOL SEQUENCE ANAL P; Eddy SR, 1998, BIOINFORMATICS, V14, P755, DOI 10.1093/bioinformatics/14.9.755; Kim M, 2009, IEEE T PATTERN ANAL, V31, P1847, DOI 10.1109/TPAMI.2009.37; Lafferty J., 2001, CONDITIONAL RANDOM F; LIEN J, 1999, J ROBOTICS AUTONOMOU; Listgarten J., 2005, P NEUR INF PROC SYST, V17; Muller M., 2009, P ACM SIGGRAPH EUR S; NOTREDAME C, 2007, PLOS COMPUTATIONAL B, V3; Pons-Moll G., 2010, P IEEE C COMP VIS PA; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; TIAN TP, 2005, P IEEE WORKSH COMP V; Tian Y., 2004, P COMP VIS PATT REC; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Yang P., 2009, P IEEE INT C COMP VI	19	2	2	0	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2013	35	11					2803	2809		10.1109/TPAMI.2013.95	http://dx.doi.org/10.1109/TPAMI.2013.95			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	223SU	24051737				2022-12-18	WOS:000324830900018
J	Appia, V; Yezzi, A				Appia, Vikram; Yezzi, Anthony			Symmetric Fast Marching Schemes for Better Numerical Isotropy	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Fast marching methods; isotropic fast marching; segmentation; FMM; Eikonal equation; global minimal path	ACTIVE CONTOUR MODELS; EFFICIENT ALGORITHMS; GLOBAL MINIMUM	Existing fast marching methods solve the Eikonal equation using a continuous (first-order) model to estimate the accumulated cost, but a discontinuous (zero-order) model for the traveling cost at each grid point. As a result the estimate of the accumulated cost (calculated numerically) at a given point will vary based on the direction of the arriving front, introducing an anisotropy into the discrete algorithm even though the continuous partial differential equation (PDE) is itself isotropic. To remove this anisotropy, we propose two very different schemes. In the first model, we utilize a continuous interpolation of the traveling cost, which is not biased by the direction of the propagating front. In the second model, we upsample the traveling cost on a higher resolution grid to overcome the directional bias. We show the significance of removing the directional bias in the computation of the cost in some applications of the fast marching method, demonstrating that both methods make the discrete implementation more isotropic, in accordance with the underlying continuous PDE.	[Appia, Vikram] Texas Instruments Inc, Embedded Proc R&D Ctr, Imaging Technol Lab, Dallas, TX USA; [Appia, Vikram] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA; [Yezzi, Anthony] Georgia Inst Technol, Sch Elect & Comp Engn, Lab Computat Comp Vis, Atlanta, GA 30332 USA	Texas Instruments; University System of Georgia; Georgia Institute of Technology; University System of Georgia; Georgia Institute of Technology	Appia, V (corresponding author), Texas Instruments Inc, Embedded Proc R&D Ctr, Imaging Technol Lab, Dallas, TX USA.		Yezzi, Anthony/AAB-4235-2020		US National Institutes of Health [R01-HL-085417]; US National Science Foundation [CCF-0728911]; EmTech seed grant; NATIONAL HEART, LUNG, AND BLOOD INSTITUTE [R01HL085417] Funding Source: NIH RePORTER	US National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); US National Science Foundation(National Science Foundation (NSF)); EmTech seed grant; NATIONAL HEART, LUNG, AND BLOOD INSTITUTE(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Heart Lung & Blood Institute (NHLBI))	This work was partially supported by US National Institutes of Health grant R01-HL-085417, US National Science Foundation grant CCF-0728911, as well as an EmTech seed grant.	ADALSTEINSSON D, 1995, J COMPUT PHYS, V118, P269, DOI 10.1006/jcph.1995.1098; Appia V., 2011, P IEEE INT C COMP VI; Appia V., 2010, P 11 EUR C COMP VI 6; Appia V., 2010, P SPIE, V7623, P2010; Appleton B, 2005, J MATH IMAGING VIS, V23, P67, DOI 10.1007/s10851-005-4968-1; Chan T, 1999, LECT NOTES COMPUT SC, V1682, P141; Chopp DL, 2001, SIAM J SCI COMPUT, V23, P230, DOI 10.1137/S106482750037617X; Cohen LD, 1997, INT J COMPUT VISION, V24, P57, DOI 10.1023/A:1007922224810; Cohen LD, 1996, PROC CVPR IEEE, P666, DOI 10.1109/CVPR.1996.517144; Danielsson P.-E., 2003, IMAGE ANAL, P631; Deschamps T, 2001, MED IMAGE ANAL, V5, P281, DOI 10.1016/S1361-8415(01)00046-9; Hassouna MS, 2007, IEEE T PATTERN ANAL, V29, P1563, DOI 10.1109/TPAMI.2007.1154; Kaul V., 2010, P BRIT MACH VIS C, P1; Kim S, 2001, SIAM J SCI COMPUT, V22, P2178, DOI 10.1137/S1064827500367130; Korn G.A., 2000, MATH HDB SCI ENG; Li H, 2007, IEEE T MED IMAGING, V26, P1213, DOI 10.1109/TMI.2007.903696; Lin Q, 2003, THESIS LINKOPINGS U; Polymenakos LC, 1998, IEEE T AUTOMAT CONTR, V43, P278, DOI 10.1109/9.661081; Sethian J. A., 1999, LEVEL SET METHODS FA; TSITSIKLIS JN, 1995, IEEE T AUTOMAT CONTR, V40, P1528, DOI 10.1109/9.412624; Weber O, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409625.1409626; Yatziv L, 2006, J COMPUT PHYS, V212, P393, DOI 10.1016/j.jcp.2005.08.005; Zhao HK, 2005, MATH COMPUT, V74, P603	23	2	2	0	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2013	35	9					2298	U260		10.1109/TPAMI.2013.52	http://dx.doi.org/10.1109/TPAMI.2013.52			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	186GB	23868786				2022-12-18	WOS:000322029000018
J	Zisserman, A; Winn, J; Fitzgibbon, A; Van Gool, L; Sivic, J; Williams, C; Hogg, D				Zisserman, Andrew; Winn, John; Fitzgibbon, Andrew; Van Gool, Luc; Sivic, Josef; Williams, Chris; Hogg, David			In Memoriam: Mark Everingham	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Biographical-Item														Hogg, David/0000-0002-6125-9564				EVERINGHAM M, PUBLICATION LIST	1	2	2	0	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2012	34	11					2081	2082		10.1109/TPAMI.2012.204	http://dx.doi.org/10.1109/TPAMI.2012.204			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	005MR	23289128				2022-12-18	WOS:000308755000001
J	Kahler, O; Denzler, J				Kaehler, Olaf; Denzler, Joachim			Tracking and Reconstruction in a Combined Optimization Approach	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Structure-from-motion; direct reconstruction; constrained tracking; local minima	ALGORITHMS	We present a novel approach to the structure-from-motion problem which combines the search for correspondences and geometric reconstruction, rather than treating these as separate steps. Through the combination of the two steps, we achieve an implicit feedback of 3D information to aid the correspondence search, and at the same time we avoid an explicit model for tracking errors. The reconstruction results are therefore optimal in case of, for example, Gaussian noise on image intensities. We also present an efficient online framework for structure-from-motion with our combined approach, thoroughly evaluate the method in experiments and compare the results to state-of-the-art methods.	[Kaehler, Olaf] Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England; [Denzler, Joachim] Univ Jena, Lehrstuhl Digitale Bildverarbeitung, D-07737 Jena, Germany	University of Oxford; Friedrich Schiller University of Jena	Kahler, O (corresponding author), Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.	olaf@robots.ox.ac.uk; joachim.denzler@uni-jena.de						Chiuso A, 2000, INT J COMPUT VISION, V39, P195, DOI 10.1023/A:1026563712076; Cobzas D, 2005, 2nd Canadian Conference on Computer and Robot Vision, Proceedings, P129, DOI 10.1109/CRV.2005.4; Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049; Engels C, 2006, P PHOT COMP VIS C BO, P266; Habbecke M., 2006, INT FALL WORKSH VIS, P73; Hager GD, 1998, IEEE T PATTERN ANAL, V20, P1025, DOI 10.1109/34.722606; Hartley R., 2003, MULTIPLE VIEW GEOMET; HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629; Irani M., 2000, VISION ALGORITHMS TH, V1883, P267, DOI DOI 10.1007/3-540-44480-7; Jin HL, 2003, VISUAL COMPUT, V19, P377, DOI 10.1007/s00371-003-0202-6; Kahler O, 2008, LECT NOTES COMPUT SC, V5096, P274, DOI 10.1007/978-3-540-69321-5_28; Kahler O, 2007, VISAPP 2007: PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOLUME IU/MTSV, P447; Kahler O, 2007, LECT NOTES COMPUT SC, V4713, P102; Kahler O., 2008, P VIS MOD VIS C; KOCH R, 1993, IEEE T PATTERN ANAL, V15, P556, DOI 10.1109/34.216725; Kolmogorov V, 2002, LECT NOTES COMPUT SC, V2352, P82; Ladikos A, 2007, VISAPP 2007: PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOLUME IU/MTSV, P325; Lepetit Vincent, 2005, Foundations and Trends in Computer Graphics and Vision, V1, P1, DOI 10.1561/0600000001; Lucas B.D., 1981, IJCAI 81 P 7 INT JOI, P674, DOI DOI 10.1109/HPDC.2004.1323531; MA Y, 2004, INTERD APPL, V26, P1; MAYBANK S, 1993, THEORY RECONSTRUCTIO; Mei C., 2006, P BRIT MACH VIS C, V2, P619; NEGAHDARIPOUR S, 1990, J OPT SOC AM A, V7, P279, DOI 10.1364/JOSAA.7.000279; NETRAVALI AN, 1985, AT&T TECH J, V64, P335, DOI 10.1002/j.1538-7305.1985.tb00436.x; Oliensis J, 2005, INT J COMPUT VISION, V61, P259, DOI 10.1023/B:VISI.0000045326.88734.8b; Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a; Rother C, 2002, INT C PATT RECOG, P737, DOI 10.1109/ICPR.2002.1048408; Schweighofer G, 2006, IEEE T PATTERN ANAL, V28, P2024, DOI 10.1109/TPAMI.2006.252; Shashua A., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P196; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Silveira G, 2008, IEEE T ROBOT, V24, P969, DOI 10.1109/TRO.2008.2004829; Sminchisescu C, 2005, INT J COMPUT VISION, V61, P81, DOI 10.1023/B:VISI.0000042935.43630.46; Triggs B., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P89, DOI 10.1007/BFb0055661; Triggs B., 2000, P EUR C COMP VIS, P522; Triggs B., 2000, LECT NOTES COMPUTER, V1883, P298, DOI [DOI 10.1007/3-540-44480-7, DOI 10.1007/3-540-44480-7_21]; WHEELER MD, 1995, CMUCS95215; Xiang T, 2003, INT J COMPUT VISION, V51, P111, DOI 10.1023/A:1021627622971; Zelnik-Manor L, 2002, IEEE T PATTERN ANAL, V24, P214, DOI 10.1109/34.982901	38	2	3	0	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2012	34	2					387	401		10.1109/TPAMI.2011.141	http://dx.doi.org/10.1109/TPAMI.2011.141			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	862PJ	21768650				2022-12-18	WOS:000298105500015
J	Irle, A; Kauschke, J				Irle, Albrecht; Kauschke, Jonas			On Kleinberg's Stochastic Discrimination Procedure	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Pattern recognition; stochastic discrimination	PATTERN-RECOGNITION	A new condition for high accuracy on test sets is given for the method of stochastic discrimination (SD), a pattern recognition method introduced by Kleinberg. This condition provides a simple explanation for the observed good generalization properties and overtraining-resistance. We also show that the method of SD remains valid if the original assumption of uniform distribution on a finite space for resampling is relaxed.	[Irle, Albrecht; Kauschke, Jonas] Univ Kiel, Math Inst, D-24098 Kiel, Germany	University of Kiel	Irle, A (corresponding author), Univ Kiel, Math Inst, Ludewig Meyn Str 4, D-24098 Kiel, Germany.	irle@math.uni-kiel.de; kauschke@math.uni-kiel.de						Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655; Chen DC, 2003, ANN STAT, V31, P1393, DOI 10.1214/aos/1065705112; DURRETT R, 1991, PROBABILITY THEOR EX; FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136; Freund Y, 1996, P 13 INT C MACH LEAR, P148, DOI DOI 10.5555/3091696.3091715; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Hastie T., 2009, ELEMENTS STAT LEARNI, V2nd, DOI DOI 10.1007/978-0-387-21606-5; Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601; IRLE A, 2009, KLEINBERGS STOCHASTI; Kleinberg E. M., 1990, ANN MATH ARTIF INTEL, V1, P207; Kleinberg EM, 1996, ANN STAT, V24, P2319; Kleinberg EM, 2000, IEEE T PATTERN ANAL, V22, P473, DOI 10.1109/34.857004	12	2	2	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2011	33	7					1482	1486		10.1109/TPAMI.2010.225	http://dx.doi.org/10.1109/TPAMI.2010.225			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	763QE	21173444				2022-12-18	WOS:000290574000015
J	Datta, A; Sheikh, Y; Kanade, T				Datta, Ankur; Sheikh, Yaser; Kanade, Takeo			Linearized Motion Estimation for Articulated Planes	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Registration; motion; tracking	EXTRACTION; IMAGES; MODELS	In this paper, we describe the explicit application of articulation constraints for estimating the motion of a system of articulated planes. We relate articulations to the relative homography between planes and show that these articulations translate into linearized equality constraints on a linear least-squares system, which can be solved efficiently using a Karush-Kuhn-Tucker system. The articulation constraints can be applied for both gradient-based and feature-based motion estimation algorithms and to illustrate this, we describe a gradient-based motion estimation algorithm for an affine camera and a feature-based motion estimation algorithm for a projective camera that explicitly enforces articulation constraints. We show that explicit application of articulation constraints leads to numerically stable estimates of motion. The simultaneous computation of motion estimates for all of the articulated planes in a scene allows us to handle scene areas where there is limited texture information and areas that leave the field of view. Our results demonstrate the wide applicability of the algorithm in a variety of challenging real-world cases such as human body tracking, motion estimation of rigid, piecewise planar scenes, and motion estimation of triangulated meshes.	[Datta, Ankur; Sheikh, Yaser; Kanade, Takeo] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Datta, A (corresponding author), Carnegie Mellon Univ, Inst Robot, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.	ankurd@cs.cmu.edu; yaser@cs.cmu.edu; tk@cs.cmu.edu			DENSO Corporation, Japan	DENSO Corporation, Japan	The research described in this paper was supported by the DENSO Corporation, Japan. The authors thank Adrien Bartoli for sharing data used during experimentation. They also thank Mathieu Salzmann for sharing data sets for nonrigid surface reconstruction.	AGARWAL A, 2004, P EUR C COMP VIS, P54; Anandan P., 1987, Proceedings of the First International Conference on Computer Vision (Cat. No.87CH2465-3), P219; ANANDAN P, 1999, P IEEE INT C COMP VI, P983; AYER S, 1995, P IEEE INT C COMP VI; BARTOLI A, 2004, P ANN BRIT MACH VIS; Beauchemin SS, 1995, ACM COMPUT SURV, V27, P433, DOI 10.1145/212094.212141; BELLILE VG, 2006, P INT C IM PROC; BELLILE VG, 2007, P ANN BRIT MACH VIS; BERGEN JR, 1992, P 2 EUR C COMP VIS; Black M.J., 1995, P IEEE INT C COMP VI; Black MJ, 1998, INT J COMPUT VISION, V26, P63, DOI 10.1023/A:1007939232436; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Bregler C, 2004, INT J COMPUT VISION, V56, P179, DOI 10.1023/B:VISI.0000011203.00237.9b; Bregler C., 1998, P IEEE C COMP VIS PA; BURT PJ, 1983, P IEEE C COMP VIS PA; CHAM T, 1999, P IEEE C COMP VIS PA; COHEN LD, 1993, IEEE T PATTERN ANAL, V15, P1131, DOI 10.1109/34.244675; Cootes TF, 2004, LECT NOTES COMPUT SC, V2034, P316; Cootes Timothy F, 1998, P EUR C COMP VIS; DELINGETTE H, 1991, P SOC PHOTO-OPT INS, P21; DEMIRDJIAN D, 2003, P IEEE INT C COMP VI; FENNEMA C, 1979, GRAPHICAL MODEL IMAG, V9, P301; Fleet D. J., 2006, HDB MATH MODELS COMP; GAVRILA DM, 1996, P IEEE C COMP VIS PA; Gill P. E., 1981, PRACTICAL OPTIMIZATI; Glazer F., 1983, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, P432; Haritaoglu I., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P877, DOI 10.1007/BFb0055710; Hartley R., 2004, ROBOTICA; HOWE N, 1999, ADV NEURAL INFORM PR; HUANG Y, 2002, P INT C PATT REC; JOHANSSON B, 1999, P IEEE INT C COMP VI; JU S, 1996, P INT C AUT FAC GEST; JU S, 1996, P IEEE C COMP VIS PA; Kakadiaris I, 2000, IEEE T PATTERN ANAL, V22, P1453, DOI 10.1109/34.895978; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KE Q, 2001, P IEEE C COMP VIS PA; LEE HJ, 1985, COMPUT VISION GRAPH, V30, P148, DOI 10.1016/0734-189X(85)90094-5; LIM J, 2005, P IEEE C COMP VIS PA; Little J., 1988, P ICCV 88, P454; LLADO X, 2005, P ANN BRIT MACH VIS; Lowe D., 2003, INT J COMPUTER VISIO, V20; Lucas B. D., 1981, P IM UND WORKSH; McInerney T., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P518, DOI 10.1109/ICCV.1993.378169; METAXAS D, 1993, IEEE T PATTERN ANAL, V15, P580, DOI 10.1109/34.216727; NAGEL H, 1983, GRAPHICAL MODEL IMAG, V21, P85; PENTLAND AP, 1990, INT J COMPUT VISION, V4, P107, DOI 10.1007/BF00127812; PRITCHETT P, 1998, 3D STRUCTURE MULTIPL; REHG JM, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P612, DOI 10.1109/ICCV.1995.466882; RUF A, 1999, P IEEE INT C COMP VI; Salzmann M., 2008, P IEEE C COMP VIS PA; Sawhney HS, 1996, IEEE T PATTERN ANAL, V18, P814, DOI 10.1109/34.531801; SCHUNCK B, 1980, DETERMINING OPTICAL; SCLAROFF S, 1998, P IEEE INT C COMP VI; Semple J.G, 1952, ALGEBRAIC PROJECTIVE; Sheikh Y., 2008, P 8 IEEE INT C AUT F; Sigal L., 2004, P IEEE C COMP VIS PA; Sigal L., 2006, P IEEE C COMP VIS PA; STANFORD LT, 2003, ADV NEURAL INFORM PR, P1555; Szeliski R, 2000, PROC CVPR IEEE, P246, DOI 10.1109/CVPR.2000.855826; URAS S, 1988, BIOL CYBERN, V60, P79, DOI 10.1007/BF00202895; VANGOOL L, 1995, P WORKSH GEOM MOD IN; WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981; Weiss Y, 1997, PROC CVPR IEEE, P520, DOI 10.1109/CVPR.1997.609375; WEISS Y, 1997, P IEEE C COMP VIS PA; Yacoob Y, 2000, INT J COMPUT VISION, V36, P5, DOI 10.1023/A:1008173322902; Yamamoto M., 2000, P IEEE C COMP VIS PA; Zelnik-Manor L, 2002, IEEE T PATTERN ANAL, V24, P214, DOI 10.1109/34.982901	67	2	3	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	2011	33	4					780	793		10.1109/TPAMI.2010.134	http://dx.doi.org/10.1109/TPAMI.2010.134			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	721QT	21330689				2022-12-18	WOS:000287370400010
J	Srivastava, A; Damon, JN; Dryden, IL; Jermyn, IH				Srivastava, Anuj; Damon, James N.; Dryden, Ian L.; Jermyn, Ian H.			Introduction to the Special Section on Shape Analysis and Its Applications in Image Understanding	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									[Srivastava, Anuj] Florida State Univ, Dept Stat, Tallahassee, FL 32306 USA; [Damon, James N.] Univ N Carolina, Dept Math, Chapel Hill, NC 27599 USA; [Dryden, Ian L.] Univ Nottingham, Sch Math Sci, Nottingham NG7 2RD, England; [Dryden, Ian L.] Univ S Carolina, Dept Stat, Columbia, SC 29208 USA; [Jermyn, Ian H.] INRIA, ARIANA Grp, Sophia Antipolis, France	State University System of Florida; Florida State University; University of North Carolina; University of North Carolina Chapel Hill; University of Nottingham; University of South Carolina; University of South Carolina System; University of South Carolina Columbia; Inria	Srivastava, A (corresponding author), Florida State Univ, Dept Stat, Tallahassee, FL 32306 USA.	asrivastava@fsu.edu; jndamon@math.unc.edu; dryden@mailbox.sc.edu; Ian.Jermyn@sophia.inria.fr	Dryden, Ian L/C-8742-2017; Srivastava, Anuj/L-4705-2019; Srivastava, Anuj/F-7417-2011; Dryden, Ian/F-2790-2011	Dryden, Ian L/0000-0003-4900-3571; Srivastava, Anuj/0000-0001-7406-0338					0	2	3	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	2010	32	4					577	578		10.1109/TPAMI.2010.42	http://dx.doi.org/10.1109/TPAMI.2010.42			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	555XA	20306598				2022-12-18	WOS:000274548800001
J	Latecki, LJ; Sobel, M; Lakaemper, R				Latecki, Longin Jan; Sobel, Marc; Lakaemper, Rolf			Piecewise Linear Models with Guaranteed Closeness to the Data	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Maximal likelihood estimate (MLE); expectation maximization (EM); Kullback-Leibler divergence (KLD); sparse EM; piecewise linear approximation	MIXTURE-MODELS; ALGORITHM	This paper addresses the problem of piecewise linear approximation of point sets without any constraints on the order of data points or the number of model components (line segments). We point out two problems with the maximum likelihood estimate (MLE) that present serious drawbacks in practical applications. One is that the parametric models obtained using a classical MLE framework are not guaranteed to be close to data points. It is typically impossible, in this classical framework, to detect whether a parametric model fits the data well or not. The second problem is related to accurately choosing the optimal number of model components. We first fit a nonparametric density to the data points and use it to define a neighborhood of the data. Observations inside this neighborhood are deemed informative; those outside the neighborhood are deemed uninformative for our purpose. This provides us with a means to recognize when models fail to properly fit the data. We then obtain maximum likelihood estimates by optimizing the Kullback-Leibler Divergence (KLD) between the nonparametric data density restricted to this neighborhood and a mixture of parametric models. We prove that, under the assumption of a reasonably large sample size, the inferred model components are close to their ground-truth model component counterparts. This holds independently of the initial number of assumed model components or their associated parameters. Moreover, in the proposed approach, we are able to estimate the number of significant model components without any additional computation.	[Latecki, Longin Jan; Lakaemper, Rolf] Temple Univ, Dept Comp & Informat Sci, Philadelphia, PA 19122 USA; [Sobel, Marc] Fox Sch Business & Management, Dept Stat, Philadelphia, PA 19122 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); Temple University	Latecki, LJ (corresponding author), Temple Univ, Dept Comp & Informat Sci, 1805 N Broad St, Philadelphia, PA 19122 USA.	lateck@temple.edu; marc.sobel@temple.edu; lakamper@temple.edu		Latecki, Longin Jan/0000-0002-5102-8244	US National Science Foundation (NSF) [IIS-0534929, IIS-0812118]; US Department of Energy (DOE) [DE-FG52-06NA27508]	US National Science Foundation (NSF)(National Science Foundation (NSF)); US Department of Energy (DOE)(United States Department of Energy (DOE))	This work was supported in part by the US National Science Foundation (NSF) Grants IIS-0534929 and IIS-0812118, and by the US Department of Energy (DOE) Grant DE-FG52-06NA27508. The authors would like to thank Guy Shechter for providing his KDTREE software in Matlab and for extending it by a KDNN function, which was extremely useful for their experimental results.	Beal M. J., 2003, BAYESIAN STAT, V7; BREIMAN L, 1977, TECHNOMETRICS, V19, P135, DOI 10.2307/1268623; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; HEALY M., 1956, Applied Statistics, V5, P203, DOI 10.2307/2985421; Lakaemper R., 2006, P IEEE INT C ROB AUT; LATECKI LJ, 2006, P ACM SIGKDD INT C K; LOFTSGAARDEN DO, 1965, ANN MATH STAT, V36, P1049, DOI 10.1214/aoms/1177700079; Neal R. M., 1998, LEARNING GRAPHICAL M; Rosin PL, 1997, IEEE T PATTERN ANAL, V19, P659, DOI 10.1109/34.601253; Ryan T. P., 1997, MODERN REGRESSION ME; TERRELL GR, 1992, ANN STAT, V20, P1236, DOI 10.1214/aos/1176348768; Ueda N, 2002, NEURAL NETWORKS, V15, P1223, DOI 10.1016/S0893-6080(02)00040-0; Ueda N, 2000, NEURAL COMPUT, V12, P2109, DOI 10.1162/089976600300015088	14	2	2	1	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	2009	31	8					1525	1531		10.1109/TPAMI.2009.13	http://dx.doi.org/10.1109/TPAMI.2009.13			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	458UN	19542585				2022-12-18	WOS:000267050600015
J	Varley, PAC				Varley, Peter A. C.			Comments on "What the Back of the Object Looks Like: 3D Reconstruction from Line Drawings without Hidden Lines"	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material						3D reconstruction; hidden topology; line drawing interpretation; visual perception	OPTIMIZATION	I comment on a paper describing a method for deducing the hidden topology of an object portrayed in a 2D natural line drawing. The principal problem with this paper is that it cannot be considered an advance on (or even an equal of) the state of the art as the approach it describes makes the same limiting assumptions as approaches proposed 10 years ago. There are also important omissions in the review of related work.	Univ Jaume 1, Dept Mech Engn & Construct, E-12071 Castellon de La Plana, Spain	Universitat Jaume I	Varley, PAC (corresponding author), Univ Jaume 1, Dept Mech Engn & Construct, E-12071 Castellon de La Plana, Spain.	varley@emc.uji.es		Varley, Peter/0000-0003-4181-9234				Cao LL, 2008, IEEE T PATTERN ANAL, V30, P507, DOI 10.1109/TPAMI.2007.1185; Company P, 2004, COMPUT GRAPH-UK, V28, P955, DOI 10.1016/j.cag.2004.08.007; GRIMSTEAD I, 1997, THESIS CARDIFF U; Grimstead I. J., 1995, Proceedings. Third Symposium on Solid Modeling and Applications, P323, DOI 10.1145/218013.218082; Huffman D. A., 1971, Machine Intelligence Volume 6, P295; LECLERC YG, 1992, INT J COMPUT VISION, V9, P113, DOI 10.1007/BF00129683; Liu JZ, 2008, IEEE T PATTERN ANAL, V30, P315, DOI 10.1109/TPAMI.2007.1172; MARILL T, 1991, INT J COMPUT VISION, V6, P147, DOI 10.1007/BF00128154; Parodi P, 1998, ARTIF INTELL, V105, P47, DOI 10.1016/S0004-3702(98)00077-0; Varley PAC, 2005, COMPUT AIDED DESIGN, V37, P1285, DOI 10.1016/j.cad.2005.01.002; VARLEY PAC, 2000, P 1 KOR UK JOINT WOR, P129; VARLEY PAC, 2008, IEEE T PATTERN ANAL, V31; VARLEY PAC, 2002, P 7 ACM S SOL MOD AP, P180; Varley PAC., 2003, THESIS CARDIFF U DEP	14	2	2	0	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	2009	31	8					1532	1534		10.1109/TPAMI.2008.159	http://dx.doi.org/10.1109/TPAMI.2008.159			3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	458UN	19542586	Green Published, Green Submitted			2022-12-18	WOS:000267050600016
J	Moses, Y; Shimshoni, I				Moses, Yael; Shimshoni, Ilan			3D Shape Recovery of Smooth Surfaces: Dropping the Fixed-Viewpoint Assumption	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						3D shape reconstruction; featureless objects	MULTIPLE IMAGES; LIGHT-SOURCE; RECONSTRUCTION; ILLUMINATION; DIRECTION; MODELS; REFLECTANCE; OBJECTS; ALBEDO	We present a new method for recovering the 3D shape of a featureless smooth surface from three or more calibrated images illuminated by different light sources (three of them are independent). This method is unique in its ability to handle images taken from unconstrained perspective viewpoints and unconstrained illumination directions. The correspondence between such images is hard to compute and no other known method can handle this problem locally from a small number of images. Our method combines geometric and photometric information in order to recover dense correspondence between the images and accurately computes the 3D shape. Only a single pass starting at one point and local computation are used. This is in contrast to methods that use the occluding contours recovered from many images to initialize and constrain an optimization process. The output of our method can be used to initialize such processes. In the special case of fixed viewpoint, the proposed method becomes a new perspective photometric stereo algorithm. Nevertheless, the introduction of the multiview setup, self-occlusions, and regions close to the occluding boundaries are better handled, and the method is more robust to noise than photometric stereo. Experimental results are presented for simulated and real images.	[Moses, Yael] Interdisciplinary Ctr, Dept Comp Sci, Efi Arazi Sch Comp Sci, IL-46150 Herzliyya, Israel; [Shimshoni, Ilan] Univ Haifa, Dept Management Informat Syst, IL-31905 Haifa, Israel	Reichman University; University of Haifa	Moses, Y (corresponding author), Interdisciplinary Ctr, Dept Comp Sci, Efi Arazi Sch Comp Sci, IL-46150 Herzliyya, Israel.	yael@idc.ac.il; ishimshoni@mis.haifa.ac.il			Israel Science Foundation [133/0-125]	Israel Science Foundation(Israel Science Foundation)	This research was supported by the Israel Science Foundation (Grant 133/0-125). The authors would like to thank Avi Barliya, Gil Ben-Artzi, and Benjamin Neeman for working on the implementation of the method.	Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; BIRKBECK N, 2006, P 9 EUR C COMP VIS M, V1, P536; Bouguet J. Y., 2008, CAMERA CALIBRATION T; BRUCKSTEIN AM, 1988, COMPUT VISION GRAPH, V44, P139, DOI 10.1016/S0734-189X(88)80002-1; Drew MS, 1996, COMPUT VIS IMAGE UND, V64, P286, DOI 10.1006/cviu.1996.0059; Dupuis P., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P453, DOI 10.1109/CVPR.1992.223151; DUROU JD, 2007, P INT C COMP VIS WOR; Durou JD, 2008, COMPUT VIS IMAGE UND, V109, P22, DOI 10.1016/j.cviu.2007.09.003; Fan J, 1997, COMPUT VIS IMAGE UND, V65, P347, DOI 10.1006/cviu.1996.0581; FAUGERAS O, 1998, P 5 EUR C COMP VIS, P379; Freeman WT, 1997, PROC CVPR IEEE, P554, DOI 10.1109/CVPR.1997.609380; FUA P, 1993, P DARPA IM UND WORKS, P1097; Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547; HAYAKAWA H, 1994, J OPT SOC AM A, V11, P3079, DOI 10.1364/JOSAA.11.003079; Hernandez C, 2008, IEEE T PATTERN ANAL, V30, P548, DOI 10.1109/TPAMI.2007.70820; Horn B.K.P., 1989, SHAPE SHADING; Horovitz I, 2004, IMAGE VISION COMPUT, V22, P681, DOI 10.1016/j.imavis.2004.01.005; Hougen D. R., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P148, DOI 10.1109/ICCV.1993.378225; IKEUCHI K, 1981, ARTIF INTELL, V17, P141, DOI 10.1016/0004-3702(81)90023-0; Jin HL, 2008, INT J COMPUT VISION, V76, P245, DOI 10.1007/s11263-007-0055-y; KIMMEL R, 1995, COMPUT VIS IMAGE UND, V62, P47, DOI 10.1006/cviu.1995.1040; Kimmel R, 2003, SIAM J SCI COMPUT, V24, P1218, DOI 10.1137/S1064827501389229; KIMMEL R, 1995, COMPUT VIS IMAGE UND, V62, P360, DOI 10.1006/cviu.1995.1060; KIMMEL R, 2000, P AS C COMP VIS; KLETTE R, 1998, CITRTR20 U AUCKL; KOLEV K, 2007, P 6 INT C EN MIN MET; KONTSEVICH LL, 1994, J OPT SOC AM A, V11, P1047, DOI 10.1364/JOSAA.11.001047; Lim J, 2005, IEEE I CONF COMP VIS, P1635; Maki A, 2002, INT J COMPUT VISION, V48, P75, DOI 10.1023/A:1016057422703; MOSES Y, 2006, P 7 AS C COMP VIS, P429; ONN R, 1990, INT J COMPUT VISION, V5, P105, DOI 10.1007/BF00056773; Prados E, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P826; PRADOS E, 2005, P 3 INT WORKSH VAR G, P320; Samaras D, 2003, IEEE T PATTERN ANAL, V25, P247, DOI 10.1109/TPAMI.2003.1177155; Shimshoni I, 2000, INT J COMPUT VISION, V39, P97, DOI 10.1023/A:1008118909580; Simakov D, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1202; Tankus A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P862; Tankus A, 2005, IEEE I CONF COMP VIS, P611; WEBER M, 2002, P 13 BRIT MACH VIS C, P83; Woodham R. J., 1978, Proceedings of the Society of Photo-Optical Instrumentation Engineers, vol.155. Image Understanding Systems and Industrial Applications, P136; Yu TL, 2007, INT J COMPUT VISION, V73, P123, DOI 10.1007/s11263-006-9373-8; Yuille AL, 1999, INT J COMPUT VISION, V35, P203, DOI 10.1023/A:1008180726317; Zhang L, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P618, DOI 10.1109/ICCV.2003.1238405; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284; ZHENG QF, 1991, IEEE T PATTERN ANAL, V13, P680, DOI 10.1109/34.85658	45	2	2	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2009	31	7					1310	1324		10.1109/TPAMI.2008.147	http://dx.doi.org/10.1109/TPAMI.2008.147			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	447KB	19443927				2022-12-18	WOS:000266188900013
J	Nagy, G; Seth, S; Viswanathan, M				Nagy, George; Seth, Sharad; Viswanathan, Mahesh			Comment: Projection Methods Require Black Border Removal	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						X-Y tree; page segmentation; layout analysis	SEGMENTATION ALGORITHMS; PERFORMANCE EVALUATION; TECHNICAL JOURNALS	A persistent flaw in the evaluation of page segmentation algorithms is examined.	[Nagy, George] Rensselaer Polytech Inst, Dept Elect Comp & Syst Engn, Troy, NY 12180 USA; [Seth, Sharad] Univ Nebraska, Dept Comp Sci & Engn, Lincoln, NE 68588 USA; [Viswanathan, Mahesh] IBM Corp, TJ Watson Res Ctr, Somers, NY 10589 USA	Rensselaer Polytechnic Institute; University of Nebraska System; University of Nebraska Lincoln; International Business Machines (IBM)	Nagy, G (corresponding author), Rensselaer Polytech Inst, Dept Elect Comp & Syst Engn, JEC 6020,110 8th St, Troy, NY 12180 USA.	nagy@ecse.rpi.edu; seth@cse.unl.edu; maheshv@us.ibm.com	SETH, SHARAD/F-9880-2010	Nagy, George/0000-0002-0521-1443				Guyon Isabelle, 1997, HDB CHARACTER RECOGN, P779; KRISHNAMOORTHY M, 1993, IEEE T PATTERN ANAL, V15, P737, DOI 10.1109/34.221173; Mao S, 2001, IEEE T PATTERN ANAL, V23, P242, DOI 10.1109/34.910877; NAGY G, 1992, COMPUTER, V25, P10, DOI 10.1109/2.144436; Shafait F, 2008, IEEE T PATTERN ANAL, V30, P941, DOI 10.1109/TPAMI.2007.70837	5	2	2	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	2009	31	4					762	762		10.1109/TPAMI.2008.192	http://dx.doi.org/10.1109/TPAMI.2008.192			1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	407WX	19358365				2022-12-18	WOS:000263396100016
J	Shahrokni, A; Drummond, T; Fleuret, F; Fua, P				Shahrokni, Ali; Drummond, Tom; Fleuret, Francois; Fua, Pascal			Classification-Based Probabilistic Modeling of Texture Transition for Fast Line Search Tracking and Delineation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Computer vision; texture; tracking; classification; segmentation		We introduce a classification-based approach to finding occluding texture boundaries. The classifier is composed of a set of weak learners, which operate on image intensity discriminative features that are defined on small patches and are fast to compute. A database that is designed to simulate digitized occluding contours of textured objects in natural images is used to train the weak learners. The trained classifier score is then used to obtain a probabilistic model for the presence of texture transitions, which can readily be used for line search texture boundary detection in the direction normal to an initial boundary estimate. This method is fast and therefore suitable for real-time and interactive applications. It works as a robust estimator, which requires a ribbon-like search region and can handle complex texture structures without requiring a large number of observations. We demonstrate results both in the context of interactive 2D delineation and of fast 3D tracking and compare its performance with other existing methods for line search boundary detection.	[Shahrokni, Ali] Univ Reading, Sch Syst Engn, Computat Vis Grp, Reading RG6 6AY, Berks, England; [Drummond, Tom] Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England; [Fleuret, Francois] IDIAP Res Inst, Ctr Parc, CH-1920 Martigny, Switzerland; [Fua, Pascal] Ecole Polytech Fed Lausanne, Dept Comp Sci, Comp Vis Lab, IC CVLab, CH-1015 Lausanne, Switzerland	University of Reading; University of Cambridge; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Shahrokni, A (corresponding author), Univ Reading, Sch Syst Engn, Computat Vis Grp, POB 225, Reading RG6 6AY, Berks, England.	ashahrokni@reading.ac.uk; twd20@eng.cam.ac.uk; fleuret@idiap.ch; pascal.fua@epfl.ch	Drummond, Tom/A-4696-2011; Fua, Pascal/H-3928-2011	Drummond, Tom/0000-0001-8204-5904; Fua, Pascal/0000-0002-6702-9970				BLAKE A, 2004, P ECCV, V1, P428; Boykov Y.Y., 2001, ICCV, V1, P105, DOI DOI 10.1109/ICCV.2001.937505; Dollar P., 2006, P IEEE CS C COMP VIS; Drummond T, 2002, IEEE T PATTERN ANAL, V24, P932, DOI 10.1109/TPAMI.2002.1017620; Freund Y, 1996, P 13 INT C MACH LEAR, P148, DOI DOI 10.5555/3091696.3091715; Harris C., 1992, TRACKING RIGID OBJEC; Ilic S, 2006, IEEE T PATTERN ANAL, V28, P328, DOI 10.1109/TPAMI.2006.37; Kaucic R, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P370, DOI 10.1109/ICCV.1998.710745; LAMPERT CH, 2008, IEEE T PATT IN PRESS; LANGLEY P, 1992, AAAI-92 PROCEEDINGS : TENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, P223; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; RATSCH G, 1998, NEURAL INFORM PROCES, P564; RIVERA M, 2004, P BRIT MACH VIS C, P809; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Shahrokni A, 2004, LECT NOTES COMPUT SC, V3022, P566; SHAHROKNI A, 2005, P BRIT MACH VIS C; SHAHROKNI A, 2005, THESIS LAUSANNE; Simard P., 1999, ADV NEURAL INFORM PR, V11	18	2	2	0	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	2009	31	3					570	576		10.1109/TPAMI.2008.236	http://dx.doi.org/10.1109/TPAMI.2008.236			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	394VO	19147883	Green Submitted			2022-12-18	WOS:000262480200014
J	Shinagawa, Y				Shinagawa, Yoshihisa			Homotopic image pseudo-invariants for openset object recognition and image retrieval	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						face recognition; content-based image retrieval; open set recognition; homotopic image pseudo-invariants	FACE-RECOGNITION; ILLUMINATION; PERFORMANCE; PCA; REPRESENTATION; EXPRESSION; TRACKING; ROBUST; POSE	This paper presents novel homotopic image pseudo-invariants for face recognition based on pixelwise analysis. An exemplar face and test images are matched and the most similar image is determined first. The homotopic image pseudo-invariants are calculated next to judge whether the most similar image contains the same person as the exemplar. The proposed method can be applied to openset recognition. The recognition task can be performed with or without face databases, while the recognition rate is higher when a database is available. This fact facilitates the recognition of faces and various other objects on the Internet. We benchmark the method using FERET and PIE, as well as images downloaded from the Internet.	Siemens Med Solut USA Inc, Malvern, PA 19355 USA	Siemens AG	Shinagawa, Y (corresponding author), Siemens Med Solut USA Inc, 51 Valley Stream Pkwy, Malvern, PA 19355 USA.	sinagawa@uiuc.edu						Alvarez L, 2000, INT J COMPUT VISION, V39, P41, DOI 10.1023/A:1008170101536; Armstrong M.A., 1983, BASIC TOPOLOGY; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; CHELLAPPA R, 1995, P IEEE, V83, P705, DOI 10.1109/5.381842; DATTA R, 2007, ACM COMPUT SURV, V39, P65; Draper BA, 2003, COMPUT VIS IMAGE UND, V91, P115, DOI 10.1016/S1077-3142(03)00077-8; Habuka K, 2004, INT J COMPUT VISION, V58, P19, DOI 10.1023/B:VISI.0000016145.44583.5f; Kim J, 2005, IEEE T PATTERN ANAL, V27, P1977, DOI 10.1109/TPAMI.2005.242; Lee KC, 2005, COMPUT VIS IMAGE UND, V99, P303, DOI 10.1016/j.cviu.2005.02.002; Levine MD, 2006, COMPUT VIS IMAGE UND, V104, P1, DOI 10.1016/j.cviu.2006.06.004; Li F, 2005, IEEE T PATTERN ANAL, V27, P1686, DOI 10.1109/TPAMI.2005.224; LI Y, 2007, P IEEE INT C COMP VI; Lin SH, 1997, IEEE T NEURAL NETWOR, V8, P114, DOI 10.1109/72.554196; Liu CJ, 2004, IEEE T PATTERN ANAL, V26, P572, DOI 10.1109/TPAMI.2004.1273927; Liu DH, 2005, PATTERN RECOGN, V38, P1705, DOI 10.1016/j.patcog.2005.03.009; Liu LY, 2006, INT J REMOTE SENS, V27, P737, DOI 10.1080/01431160500296867; Liu XM, 2003, PATTERN RECOGN, V36, P313, DOI 10.1016/S0031-3203(02)00033-X; McCane B, 2001, COMPUT VIS IMAGE UND, V84, P126, DOI 10.1006/cviu.2001.0930; OKADA K, 1998, FACE RECOGNITION THE, P186; PARK U, 2007, P IEEE INT C COMP VI; Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790; Rodriguez Y, 2006, IMAGE VISION COMPUT, V24, P882, DOI 10.1016/j.imavis.2006.02.012; Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701; Schneiderman H., 2000, P IEEE INT C COMP VI; Shinagawa Y, 1998, IEEE T PATTERN ANAL, V20, P994, DOI 10.1109/34.713364; Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154; Sim T., 2001, CMURIT0102; SIVIC J, 2003, P IEEE INT C COMP VI, P399; Tefas A, 2001, IEEE T PATTERN ANAL, V23, P735, DOI 10.1109/34.935847; TURK M, 1991, P IEEE C COMP VIS PA, P586, DOI DOI 10.1109/CVPR.1991.139758; Vasilescu M Alex O, 2002, P EUR C COMP VIS; Verma RC, 2003, IEEE T PATTERN ANAL, V25, P1215, DOI 10.1109/TPAMI.2003.1233896; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; WANG H, 2003, P IEEE INT C COMP VI; Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109; Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097; Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883; Yang MH, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P215; YU H, 2006, PATTERN RECOGNITION; Zhang L, 2006, IEEE T PATTERN ANAL, V28, P351, DOI 10.1109/TPAMI.2006.53; Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342	43	2	3	2	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2008	30	11					1891	1901		10.1109/TPAMI.2008.143	http://dx.doi.org/10.1109/TPAMI.2008.143			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	347AC	18787238				2022-12-18	WOS:000259110000003
J	Bishnu, A; Bhattacharya, BB				Bishnu, Arijit; Bhattacharya, Bhargab B.			Stacked Euler Vector (SERVE): A gray-tone image feature based on bit-plane augmentation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Euler number; bit-plane graph; combinatorial feature; CBIR	MOMENT INVARIANTS; BINARY IMAGE; RETRIEVAL	A new combinatorial feature called Stacked Euler Vector (SERVE) is introduced to characterize a gray- tone image. SERVE comprises a four-tuple, where each element is an integer representing the Euler number of the partial binary image formed by certain pixel overlap relations among the four most significant bit planes of the gray- tone image. Computation of SERVE is simple, fast, and does not involve any floating point operation. SERVE can be used to augment other features to improve the performance of image retrieval significantly. Experimental results on the COIL database are reported to demonstrate its performance.	Indian Inst Technol, Dept Comp Sci & Engn, Kharagpur 721302, W Bengal, India; Indian Stat Inst, Kolkata 700108, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Kharagpur; Indian Statistical Institute; Indian Statistical Institute Kolkata	Bishnu, A (corresponding author), Indian Inst Technol, Dept Comp Sci & Engn, Kharagpur 721302, W Bengal, India.	arijit.bishnu@iitkgp.ac.in; bhargab@isical.ac.in	Bhattacharya, Bhargab/AAE-6130-2020					Adler R. J., 1981, GEOMETRY RANDOM FIEL; Bishnu A, 2005, J SYST ARCHITECT, V51, P470, DOI 10.1016/j.sysarc.2004.12.001; Bishnu A, 2005, IEEE T SYST MAN CY B, V35, P801, DOI 10.1109/TSMCB.2005.846642; DiZenzo S, 1996, IEEE T PATTERN ANAL, V18, P83, DOI 10.1109/34.476016; GRAY SB, 1971, IEEE T COMPUT, VC 20, P551, DOI 10.1109/T-C.1971.223289; Harary Frank, 1972, GRAPH THEORY; HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692; Jain A. K., 1990, FUNDAMENTALS DIGITAL; Nene A. S., 1996, CUCS00696; OBDRZALEK S, 2002, P BRIT MACH VIS C, P113; RABBANI M, 2001, INT C IM PROC ICIP O; REISS TH, 1991, IEEE T PATTERN ANAL, V13, P830, DOI 10.1109/34.85675; Rosenfeld A., 1982, DIGITAL PICTURE PROC; Schmid C, 1997, IEEE T PATTERN ANAL, V19, P530, DOI 10.1109/34.589215; Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972; VELTKAMP R, 2000, UUCS200034 U UTR	16	2	5	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2007	29	2					350	355		10.1109/TPAMI.2007.43	http://dx.doi.org/10.1109/TPAMI.2007.43			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	116TV	17170486				2022-12-18	WOS:000242826900014
J	Wang, LW; Feng, JF				Wang, LW; Feng, JF			Comments on "Fundamental limits of reconstruction-based superresolution algorithms under local translation"	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						superresolution; perturbation theory; fundamental limit; magnification factor		Fundamental limits of reconstruction-based superresolution were proposed in [1]. In this note, we point out that their proof is incomplete.	Peking Univ, Ctr Informat Sci, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China	Peking University	Wang, LW (corresponding author), Peking Univ, Ctr Informat Sci, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.	wanglw@cis.pku.edu.cn; fif@cis.edu.cn						Lin ZC, 2004, IEEE T PATTERN ANAL, V26, P83, DOI 10.1109/TPAMI.2004.1261081	1	2	2	0	18	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	2006	28	5					846	846		10.1109/TPAMI.2006.91	http://dx.doi.org/10.1109/TPAMI.2006.91			1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	020CO	16640271				2022-12-18	WOS:000235885700017
J	Martina, M; Masera, G				Martina, M; Masera, G			Mumford and Shah functional: VLSI analysis and implementation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						image segmentation; Mumford and Shah; performance evaluation; VLSI implementation	SEGMENTATION	This paper describes the analysis of the Mumford and Shah functional from the implementation point of view. Our goal is to show results in terms of complexity for real-time applications, such as motion estimation based on segmentation techniques, of the Mumford and Shah functional. Moreover, the sensitivity to finite precision representation is addressed, a fast VLSI architecture is described, and results obtained for its complete implementation on a 0.13 mu m standard cells technology are presented.	Politecn Torino, Dipartimento Elettr, CERCOM, I-10129 Turin, Italy	Polytechnic University of Turin	Martina, M (corresponding author), Politecn Torino, Dipartimento Elettr, CERCOM, Corso Duca Abruzzi 24, I-10129 Turin, Italy.	maurizio.martina@polito.it; guido.masera@polito.it	Martina, Maurizio/D-1495-2010	Martina, Maurizio/0000-0002-3069-0319; Masera, Guido/0000-0003-2238-9443				ADAMS MF, 2001, ACM IEEE P SC2001 HI, P1; AMBROSIO L, 1990, COMMUN PUR APPL MATH, V43, P999, DOI 10.1002/cpa.3160430805; Brook A, 2003, J MATH IMAGING VIS, V18, P247, DOI 10.1023/A:1022895410391; Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291; Cremers D, 2005, INT J COMPUT VISION, V62, P249, DOI 10.1007/s11263-005-4882-4; Cremers D, 2002, INT J COMPUT VISION, V50, P295, DOI 10.1023/A:1020826424915; Gibou F, 2005, MATH BIOSCI ENG, V2, P209, DOI 10.3934/mbe.2005.2.209; Hadjam FZ, 2001, ACS/IEEE INTERNATIONAL CONFERENCE ON COMPUTER SYSTEMS AND APPLICATIONS, PROCEEDINGS, P283, DOI 10.1109/AICCSA.2001.933994; KOEPFLER G, 1994, SIAM J NUMER ANAL, V31, P282, DOI 10.1137/0731015; MARCH R, 1992, IMAGE VISION COMPUT, V10, P30, DOI 10.1016/0262-8856(92)90081-D; MARTINA M, 2004, P IEEE INT S SIGN PR; MARTINA M, 2004, MUMFORD SHAH C MODEL; MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503; Robertson J. E., 1958, IRE T ELECT COMPUTER, V7, P218; Sroubek F, 2003, IEEE T IMAGE PROCESS, V12, P1094, DOI 10.1109/TIP.2003.815260; STROUT MM, 2002, P 15 WORKSH LANG COM, P1; Vanzella W, 2004, IEEE T PATTERN ANAL, V26, P804, DOI 10.1109/TPAMI.2004.15; VOGL TP, 1988, BIOL CYBERN, V59, P257, DOI 10.1007/BF00332914; Yang J. A., 1992, Proceedings. Sixth International Parallel Processing Symposium (Cat. No.92TH0419-2), P204, DOI 10.1109/IPPS.1992.223046; 2004, MEGAWAVE2	20	2	2	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	2006	28	3					487	494		10.1109/TPAMI.2006.59	http://dx.doi.org/10.1109/TPAMI.2006.59			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	001FB	16526435	Green Submitted			2022-12-18	WOS:000234517900015
J	Jiang, G; Wei, YC; Quan, L; Tsui, HT; Shum, HY				Jiang, G; Wei, YC; Quan, L; Tsui, HT; Shum, HY			Outward-looking circular motion analysis of large image sequences	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						structure from motion; circular motion; single axis motion; concentric mosaic	EPIPOLAR GEOMETRY	This paper presents a novel and simple method of analyzing the motion of a large image sequence captured by a calibrated outward-looking video camera moving on a circular trajectory for large-scale environment applications. Previous circular motion algorithms mainly focus on inward-looking turntable-like setups. They are not suitable for outward-looking motion where the conic trajectory of corresponding points degenerates to straight lines. The circular motion of a calibrated camera essentially has only one unknown rotation angle for each frame. The motion recovery for the entire sequence computes only one fundamental matrix of a pair of frames to extract the angular motion of the pair using Laguerre's formula and then propagates the computation of the unknown rotation angles to the other frames by tracking one point over at least three frames. Finally, a maximum-likelihood estimation is developed for the optimization of the whole sequence. Extensive experiments demonstrate the validity of the method and the feasibility of the application in image-based rendering.	Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China; Xidian Univ, Natl Key Lab ISN, Xian 710071, Peoples R China; Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China; Microsoft Res Asia, Beijing, Peoples R China	Hong Kong University of Science & Technology; Xidian University; Chinese University of Hong Kong; Microsoft; Microsoft Research Asia	Jiang, G (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.	gjiang@cs.ust.hk; yichenw@cs.ust.hk; quan@cs.ust.hk; httsui@ee.cuhk.edu.hk; hshum@microsoft.com						ARMSTRONG M, 1996, P 4 EUR C COMP VIS, V1064, P3; CHAI JX, 2000, P SMILE WORKSH STRUC; FAUGERAS O, 1995, J OPT SOC AM A, V12, P465, DOI 10.1364/JOSAA.12.000465; FAUGERAS O, 1998, P 5 EUR C COMP VIS F, P36; FITZGIBBON AW, 1998, P ECCV 98 WORKSH 3D, P154; Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; Jiang G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P221, DOI 10.1109/ICCV.2003.1238345; Jiang G, 2003, IEEE T PATTERN ANAL, V25, P1343, DOI 10.1109/TPAMI.2003.1233910; JIANG G, 2004, P AS C COMP VIS; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; LIEBOWITZ D, 1998, P C COMP VIS PATT RE; Luong QT, 1996, INT J COMPUT VISION, V17, P43, DOI 10.1007/BF00127818; McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398; Mendonca PRS, 2001, IEEE T PATTERN ANAL, V23, P604, DOI 10.1109/34.927461; MENDONCA PRS, 2000, P 6 EUR C COMP VIS, V2, P864; Peleg S, 1997, PROC CVPR IEEE, P338, DOI 10.1109/CVPR.1997.609346; PELEG S, 1999, P C COMP VIS PATT RE; Semple J.G, 1952, ALGEBRAIC PROJECTIVE; SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794; Shum HY, 1999, COMP GRAPH, P299, DOI 10.1145/311535.311573; STURM P, 1999, P C COMP VIS PATT RE; Tomasi C, 1991, CMUCS91132; ZHANG Z, 1999, P 7 INT C COMP VIS S; Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561	26	2	3	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2005	27	2					271	277		10.1109/TPAMI.2005.34	http://dx.doi.org/10.1109/TPAMI.2005.34			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	879AR	15688564				2022-12-18	WOS:000225689300010
J	Rehg, JM; Pavlovic, V; Huang, TS; Freeman, WT				Rehg, JM; Pavlovic, V; Huang, TS; Freeman, WT			Guest editors' introduction to the special section on graphical models in computer vision	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									Georgia Inst Technol, Atlanta, GA 30332 USA; Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA; Beckman Inst Adv Sci & Technol, Dept Informat, Dept Elect & Comp Engn, Urbana, IL 61801 USA; MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA	University System of Georgia; Georgia Institute of Technology; Rutgers State University New Brunswick; Massachusetts Institute of Technology (MIT)	Rehg, JM (corresponding author), Georgia Inst Technol, 801 Atlantic Dr, Atlanta, GA 30332 USA.		Rehg, James/AAM-6888-2020	Rehg, James/0000-0003-1793-5462				BINFORD T, 1987, P 3 ANN C UNC ART IN, P73; JORDAN M, 1998, LEARNING GRAPHICAL M; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4	4	2	2	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2003	25	7					785	786		10.1109/TPAMI.2003.1206508	http://dx.doi.org/10.1109/TPAMI.2003.1206508			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	692NN					2022-12-18	WOS:000183667300001
J	Mitra, P; Murthy, CA; Pal, SK				Mitra, P; Murthy, CA; Pal, SK			Unsupervised feature selection using feature similarity (vol 24, pg 301, 2002)	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Correction																		Mitra P, 2002, IEEE T PATTERN ANAL, V24, P301, DOI 10.1109/34.990133	1	2	3	0	17	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	2002	24	6					721	721		10.1109/TPAMI.2002.1008379	http://dx.doi.org/10.1109/TPAMI.2002.1008379			1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	556JU					2022-12-18	WOS:000175846300001
J	Darrell, T; Covell, M				Darrell, T; Covell, M			Correspondence with cumulative similarity transforms	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						image correspondence; stereo; motion; contour tracking	VISION; WINDOW	A local image transform based on cumulative similarity measures is defined and is shown to enable efficient correspondence and tracking near occluding boundaries. Unlike traditional methods, this transform allows correspondences to be found when the only contrast present is the occluding boundary itself and when the sign of contrast along the boundary is possibly reversed. The transform is based on the idea of a cumulative similarity measure which characterizes the shape of local image homogeneity; both the value of an image at a particular point and the shape of the region with locally similar and connected values is captured. This representation is insensitive to structure beyond an occluding boundary but is sensitive to the shape of the boundary itself. which is often an important cue. We show results comparing this method to traditional least-squares and robust correspondence matching.	MIT, AI Lab, Cambridge, MA 02139 USA; YesVideo Com, San Jose, CA 95131 USA	Massachusetts Institute of Technology (MIT)	Darrell, T (corresponding author), MIT, AI Lab, Room NE43-829,545 Technol Sq, Cambridge, MA 02139 USA.	trevou@ai.mif.edu; covell@ieee.org						AMINI AA, 1990, IEEE T PATTERN ANAL, V12, P855, DOI 10.1109/34.57681; AYER S, 1995, P INT C COMP VIS; BERGEN JR, 1991, ARTIFICIAL INTELLIGE; BHAR D, 1994, P C COMP VIS PATT RE; BLACK M, 1997, P C COMP VIS PATT RE; BLACK M, 1993, P INT C COMP VIS, P263; BLACK M, 1997, P 3 INT WORKSH VIS F; BLACK MJ, 1995, P INT C COMP VIS; BLAKE A, 1994, P ACM SIGGRAPH, P185; Boykov Y, 1998, IEEE T PATTERN ANAL, V20, P1283, DOI 10.1109/34.735802; BREGLER C, 1995, 1995017 IRCTR; COOTES TF, 1993, P 4 INT C COMP VIS, P242; COVELL M, 1999, P IEEE C COMP VIS PA; COVELL M, 1996, P INT C IM PROC; COVELL M, 1994, P INT C AC SPEECH SI; DARRELL T, 1993, P COMP VIS PATT REC; Darrell Trevor, 1991, P IEEE WORKSH VIS MO, P2; Galvin B., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P294, DOI 10.1109/CVPR.1999.784647; HAGER G, 1996, P COMP VIS PATT REC; HUTTENLOCHER DP, 1993, P 4 INT C COMP VIS, P93; Irani M., 1994, INT J COMPUTER VISIO, V12; ISARD M, 1996, P EUR C COMP VIS, P343; JONES DG, 1992, P EUR C COMP VIS, P395; KANADE T, 1994, IEEE T PATTERN ANAL, V16, P920, DOI 10.1109/34.310690; Kass M., 1987, International Journal of Computer Vision, V1, P321, DOI 10.1007/BF00133570; LITWINOWICZ P, 1994, P SIGGRAPH 94, P409; Peterfreund N, 1997, IEEE NONRIGID AND ARTICULATED MOTION WORKSHOP, PROCEEDINGS, P70, DOI 10.1109/NAMW.1997.609855; SHIZAWA M, 1990, P COMP VIS PATT REC; SIMONCELLI EP, 1991, P COMP VIS PATT REC; Slaney M, 1997, P SIGGRAPH; Terzopoulos D., 1992, ACTIVE VISION, P3; WANG J, 1993, P C COMP VIS PATT RE; WISKOTT L, 1995, P INT WORKSH AUT FAC, P92; YU W, 1999, P COMP VIS PATT REC, P171; Yuille A. L., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P104, DOI 10.1109/CVPR.1989.37836; Zabih R., 1994, P 3 EUR C COMP VIS; [No title captured]	37	2	2	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2001	23	2					222	227		10.1109/34.908973	http://dx.doi.org/10.1109/34.908973			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	401NJ					2022-12-18	WOS:000166933500012
J	Sanchiz, JM; Fisher, RB				Sanchiz, JM; Fisher, RB			Viewpoint estimation in three-dimensional images taken with perspective range sensors	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						range images; viewpoint		We present a method for estimating the viewpoint from which a 3D image has been taken using a central-projection range sensor. We assume we have the 3D coordinates of the points, organized with a known topology, but considerable noise is present in the data. At points in the scene where there are surface discontinuities, we estimate step rays through a linear interpolation. The viewpoint is found as the point of minimum distance to the set of step rays. To cope with noise, we define an unbiased distance measure. The minimization of the sum of distances provides the viewpoint. We present results of several experiments carried out with 3D images of an old church.	Univ Jaume 1, Dept Informat, E-12071 Castellon de La Plana, Spain; Univ Edinburgh, IPAB, Div Informat, Edinburgh EH1 2QL, Midlothian, Scotland	Universitat Jaume I; University of Edinburgh	Sanchiz, JM (corresponding author), Univ Jaume 1, Dept Informat, Campus Riu Sec, E-12071 Castellon de La Plana, Spain.	sanchiz@inf.uji.es; rbf@dai.ed.ac.uk						Bar-Shalom Y., 1988, TRACKING DATA ASS; Dalton G., 1998, Sensor Review, V18, P92, DOI 10.1108/02602289810209867; Danuser G, 1998, IEEE T PATTERN ANAL, V20, P263, DOI 10.1109/34.667884; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; HEBERT M, 1992, IMAGE VISION COMPUT, V10, P170, DOI 10.1016/0262-8856(92)90068-E; Huber P., 1981, ROBUST STAT; MEER P, 1991, INT J COMPUT VISION, V6, P59, DOI 10.1007/BF00127126; NOE G, 1999, 3 DIMENSIONAL LASER; Rousseeuw P.J., 1987, ROBUST REGRESSION OU	9	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2000	22	11					1324	1329						6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	374QE					2022-12-18	WOS:000165355200010
J	Gu, HZ; Takahashi, H				Gu, HZ; Takahashi, H			How bad may learning curves be?	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						generalization; concept learning; generalization error; learning curves; sample complexity; PAC learning; worst-case learning; interpolation dimension	BOUNDS	In this paper, we motivate the need for estimating bounds on learning curves of average-case learning algorithms when they perform the worst on training samples. We then apply the method of reducing learning problems to hypothesis testing ones to investigate the learning curves of a so-called ill-disposed learning algorithm in terms of a system complexity, the Boolean interpolation dimension. Since the ill-disposed algorithm behaves worse than ordinal ones, and the Boolean interpolation dimension is generally bounded by the number of system weights, the results can apply to interpreting or to bounding the worst-case learning curve in real learning situations. This study leads to a new understanding of the worst-case generalization in real learning situations, which differs significantly from that in the uniform learnable setting via Vapnik-Chervonenkis (VC) dimension analysis. We illustrate the results with some numerical simulations.	Kawasaki Steel Syst R&D, Kawasaki, Kanagawa, Japan; Univ Electrocommun, Dept Informat & Commun Engn, Chofu, Tokyo 1828585, Japan	Kawasaki Heavy Industries; University of Electro-Communications - Japan	Gu, HZ (corresponding author), Kawasaki Steel Syst R&D, Kawasaki, Kanagawa, Japan.	guhan@ma4.justnet.ne.jp; takahashi@ice.uec.ac.jp						AMARI S, 1992, NEURAL COMPUT, V4, P605, DOI 10.1162/neco.1992.4.4.605; AMARI SI, 1993, NEURAL NETWORKS, V6, P161, DOI 10.1016/0893-6080(93)90013-M; BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371; COHN D, 1992, NEURAL COMPUT, V4, P249, DOI 10.1162/neco.1992.4.2.249; GU H, 1997, THESIS U ELECTROCOMM; Gu HZ, 1997, IEICE T INF SYST, VE80D, P78; Gu HZ, 1997, NEURAL NETWORKS, V10, P1089, DOI 10.1016/S0893-6080(97)00047-6; Gu HZ, 1996, IEEE T NEURAL NETWOR, V7, P953, DOI 10.1109/72.508938; Gu HZ, 2000, NEURAL COMPUT, V12, P795, DOI 10.1162/089976600300015592; HAUSSLER D, 1991, PROCEEDINGS OF THE FOURTH ANNUAL WORKSHOP ON COMPUTATIONAL LEARNING THEORY, P61; HAUSSLER D, 1988, ARTIF INTELL, V36, P177, DOI 10.1016/0004-3702(88)90002-1; HAUSSLER D, 1994, RIGOROUS LEARNING CU; HOLDEN SB, 1994, PRACTICAL APPL VC DI; Macintyre A., 1993, Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, P325, DOI 10.1145/167088.167192; Ruck D W, 1990, IEEE Trans Neural Netw, V1, P296, DOI 10.1109/72.80266; Schwartz DB, 1990, NEURAL COMPUT, V2, P374, DOI 10.1162/neco.1990.2.3.374; Takahashi H, 1998, IEEE T NEURAL NETWOR, V9, P1191, DOI 10.1109/72.728362; TISHBY N, 1989, P INT JOINT C NEURAL, V2, P403; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Vapnik V., 1982, ESTIMATION DEPENDENC; WANG E, 1990, IEEE T NEURAL NETWOR, V1, P303	21	2	3	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2000	22	10					1155	1167		10.1109/34.879795	http://dx.doi.org/10.1109/34.879795			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	369LQ					2022-12-18	WOS:000165067100008
J	Kabuka, MR				Kabuka, MR			Reply to: Comments on "Design of supervised classifiers using Boolean neural networks"	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material								In his comments, Guy Smith discussed three points he believes are either not correct or unclear. These points are: 1)The uniqueness of thermometer coding in the algorithm is not supported in the paper. 2) The BKNN algorithm is not equivalent to the k-nearest neighbor algorithm. 3) The relationship between exemplars and classes is not clear. Following is a reply to each of the above three points:	Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33124 USA	University of Miami	Kabuka, MR (corresponding author), Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33124 USA.							AHA DW, 1991, MACH LEARN, V6, P37, DOI 10.1023/A:1022689900470; BHIDE S, 1994, THESIS U MIAMI; CHANG CL, 1974, IEEE T COMPUT, VC 23, P1179, DOI 10.1109/T-C.1974.223827; GAZULA S, 1995, IEEE T PATTERN ANAL, V17, P1239, DOI 10.1109/34.476519; RITTER GL, 1975, IEEE T INFORM THEORY, V21, P665, DOI 10.1109/TIT.1975.1055464; Schalkoff R, 1992, PATTERN RECOGN; SWONGER CW, 1972, FRONTIERS PATTERN RE, P511; Tou JT, 1974, PATTERN RECOGN; WILSON DL, 1972, IEEE T SYST MAN CYB, VSMC2, P408, DOI 10.1109/TSMC.1972.4309137	9	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1999	21	9					957	958		10.1109/TPAMI.1999.790439	http://dx.doi.org/10.1109/TPAMI.1999.790439			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	234TZ					2022-12-18	WOS:000082501600015
J	Smith, G				Smith, G			Comments on "Design of supervised classifiers using Boolean neural neworks"	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material								Gazula and Kabuka [1] describe a binary neural network which implements a nonparametric statistical classifier. However, they implement a kernel-based classifier rather than k-nearest-neighbors, as stated in their paper. Some other aspects of their paper are not clear.	Univ Queensland, Dept Comp Sci, St Lucia, Qld 4072, Australia	University of Queensland	Smith, G (corresponding author), Univ Queensland, Dept Comp Sci, St Lucia, Qld 4072, Australia.							GAZULA S, 1995, IEEE T PATTERN ANAL, V17, P1239, DOI 10.1109/34.476519; HAND DJ, 1981, DISCRIMINATION CLASS, P16; STANFORD PH, 1994, P INT C ART NEUR NET, P1432	3	2	2	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1999	21	9					956	956		10.1109/34.790438	http://dx.doi.org/10.1109/34.790438			1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	234TZ					2022-12-18	WOS:000082501600014
J	Havaldar, P; Medioni, G				Havaldar, P; Medioni, G			Full volumetric descriptions from three intensity images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						shape description; grouping; stereo; generalized cylinders	HOMOGENEOUS GENERALIZED CYLINDERS; CONTOURS; SHAPE; RECOGNITION; RECOVERY; OBJECT; STEREO; SURFACES; VIEW	We address the problem of recovering high-level, volumetric and segmented (or part-based) descriptions of objects from intensity images. As input we use three closely spaced images of an object and recover descriptions based on Generalized Cylinders (GCCs). We start by extracting a hierarchy of groups from contour images in the three views. Grouping is based on proximity, parallelism, and symmetry. The groups in the three views are matched and their contours are labeled as "true" edges, which correspond to surface normal/reflectance discontinuities and "limb" edges, which are viewpoint dependent edges. We then infer the GC axis, its cross-section, and the scaling function. The properties of straight and curved axis generalized cylinders are used locally on the visible surfaces to obtain the GC axis. The cross-section is recovered if seen in the images, else it is inferred using the visible surfaces and GC properties. We consider groups with true edges, limb edges, or a combination of both. The final descriptions are volumetric and in terms of parts. The coarse volumetric descriptions obtained are then refined to include surface details as seen in the intensity images. We demonstrate results on real images of moderately complex objects with texture and shadows.	Nichimen Graph, Los Angeles, CA 90066 USA; Univ So Calif, Inst Robot & Intelligent Syst, Los Angeles, CA 90089 USA	University of Southern California	Havaldar, P (corresponding author), Nichimen Graph, 12555 W Jefferson Blvd, Los Angeles, CA 90066 USA.	havaldar@iris.usc.edu; medioni@iris.usc.edu						BERGEVIN R, 1993, IEEE T PATTERN ANAL, V15, P19, DOI 10.1109/34.184772; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; CIPOLLA R, 1992, INT J COMPUT VISION, V9, P83, DOI 10.1007/BF00129682; COCHRAN SD, 1992, IEEE T PATTERN ANAL, V14, P981, DOI 10.1109/34.159902; DHOND UR, 1989, IEEE T SYST MAN CYB, V19, P1489, DOI 10.1109/21.44067; Gross AD, 1996, IEEE T PATTERN ANAL, V18, P161, DOI 10.1109/34.481541; Havaldar P, 1996, INT J COMPUT VISION, V20, P59, DOI 10.1007/BF00144117; Havaldar P, 1996, PROC CVPR IEEE, P278, DOI 10.1109/CVPR.1996.517086; HAVALDAR P, 1994, P EUR C COMP VIS, V1, P251; HAVALDAR P, 1995, INT C COMP VIS, P102; HOFF W, 1989, IEEE T PATTERN ANAL, V11, P121, DOI 10.1109/34.16709; JOSHI T, 1995, INT C COMP VIS, P290; KANADE T, 1981, ARTIF INTELL, V17, P409, DOI 10.1016/0004-3702(81)90031-X; KRIEGMAN DJ, 1990, IEEE T PATTERN ANAL, V12, P1127, DOI 10.1109/34.62602; MACKWORTH AK, 1973, ARTIF INTELL, V4, P121, DOI 10.1016/0004-3702(73)90003-9; NEVATIA R, 1977, ARTIF INTELL, V8, P77, DOI 10.1016/0004-3702(77)90006-6; PONCE J, 1989, IEEE T PATTERN ANAL, V11, P951, DOI 10.1109/34.35498; SATO H, 1992, P IM UND WORKSH SAN; TOMASI C, 1991, IEEE WORKSH VIS MOT, P21; ULUPINAR F, 1995, IEEE T PATTERN ANAL, V17, P120, DOI 10.1109/34.368175; ULUPINAR F, 1993, IEEE T PATTERN ANAL, V15, P3, DOI 10.1109/34.184771; VAILLANT R, 1992, IEEE T PATTERN ANAL, V14, P157, DOI 10.1109/34.121787; Zerroug M., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P96, DOI 10.1109/CVPR.1993.340973; ZERROUG M, 1994, P INT C PATT REC, VA, P678; ZERROUG M, 1994, P EUR C COMP VIS STO, P319; ZHANG ZY, 1995, ARTIF INTELL, V78, P87, DOI 10.1016/0004-3702(95)00022-4	26	2	2	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	1998	20	5					540	545		10.1109/34.682183	http://dx.doi.org/10.1109/34.682183			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	ZR253					2022-12-18	WOS:000073955600009
J	Rosin, PL; West, GAW				Rosin, PL; West, GAW			Nonparametric segmentation of curves into various representations - Response	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						curve segmentation; model selection; line; ellipse		We discuss the advantages and disadvantages of two approaches to model selection: (1) the information theoretic method suggested by Kanatani and others and (2) our heuristic sequential selection method.	CURTIN UNIV TECHNOL, DEPT COMP SCI, PERTH, WA 6001, AUSTRALIA	Curtin University	Rosin, PL (corresponding author), BRUNEL UNIV, DEPT COMP SCI & INFORMAT SYST, UXBRIDGE UB8 3PH, MIDDX, ENGLAND.							AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; BESL PJ, 1988, IEEE T PATTERN ANAL, V10, P167, DOI 10.1109/34.3881; Kanatani K, 1997, IEEE T PATTERN ANAL, V19, P1391, DOI 10.1109/34.643901; LI M, 1992, CVAP114 ROYAL I TECH; Lindeberg T, 1997, COMPUT VIS IMAGE UND, V67, P88, DOI 10.1006/cviu.1996.0510; Rissanen Jorma, 1989, STOCHASTIC COMPLEXIT; ROSIN PL, 1995, IEEE T PATTERN ANAL, V17, P1140, DOI 10.1109/34.476507; ROSIN PL, 1994, ASPECTS VISUAL FORM, P465; THACKER N, 1996, P BRIT MACHINE VISIO, P283; WEST GAW, 1991, PATTERN RECOGN, V24, P643, DOI 10.1016/0031-3203(91)90031-Y; 1987, PSYCHOMETRIKA, V52	11	2	2	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	1997	19	12					1393	1394		10.1109/TPAMI.1997.643902	http://dx.doi.org/10.1109/TPAMI.1997.643902			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YK781					2022-12-18	WOS:A1997YK78100011
J	Modayur, BR; Shapiro, LG				Modayur, BR; Shapiro, LG			PERFORM: A fast object recognition method using intersection of projection error regions	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						object recognition; bounded error; uncertainty regions; parallel processing; algorithm complexity	IMAGE	This paper describes a new formulation of the problem of object recognition under a bounded-error noise model and an object recognition methodology called PERFORM that finds matches by establishing correspondences between model and image features using this formulation. PERFORM evaluates correspondences by intersecting error regions in the image space. The algorithm is analyzed with respect to theoretical complexity as well as actual running times. When a single solution to the matching problem is sought, the time complexity of the sequential matching algorithm for 2D-2D matching using point features is of the order O(l(3) N-2), where N is the number of model features and I is the number of image features. When line features are used, the sequential complexity is of the order O(l(2) N-2). When a single solution is sought, PERFORM runs faster than the fastest known algorithm [8] to solve the bounded-error matching problem. The PERFORM method, which was developed with parallelizability as an important requirement, is shown to be easily realizable on both SIMD and MIMD architectures. The parallel versions of PERFORM are scalable, achieving linear speedups on both the MasPar and the KSR machines. When implemented in parallel, PERFORM does not require a targe number of processors or memory, needs minimal to no inter-processor communication, requires no load balancing, and can produce all or just one solution to the matching problem. The communication-efficient version of PERFORM described in this paper has minimal memory requirements, since it only needs to store the model and image features and computes everything else on the fly.			Modayur, BR (corresponding author), UNIV WASHINGTON, DEPT ELECT ENGN, INTELLIGENT SYST LAB, FT-10, SEATTLE, WA 98195 USA.							AYACHE N, 1986, IEEE T PATTERN ANAL, V8, P44, DOI 10.1109/TPAMI.1986.4767751; BAIRD HS, 1985, MODEL BASED IMAGE MA; BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1; BENARIE J, 1990, IEEE T PATTERN ANAL, V12, P760, DOI 10.1109/34.57667; Bolles R. C., 1982, INT J ROBOT RES, V1, P57; BOLLES RC, 1986, INT J ROBOT RES, V5, P3, DOI 10.1177/027836498600500301; Breuel T. M., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P445, DOI 10.1109/CVPR.1992.223152; BREUEL TM, 1992, THESIS MIT; Camps O. I., 1991, P IEEE WORKSHOP DIRE, P11; CASS TA, 1988, P IM UND WORKSH BOST; COSTA MS, 1990, P 10 ICPR, V1, P233; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; GRIMSON WEL, 1987, IEEE T PATTERN ANAL, V9, P469, DOI 10.1109/TPAMI.1987.4767935; GRIMSON WEL, 1992, LECT NOTES COMPUT SC, V588, P291; HUTTENLOCHER DP, 1990, INT J COMPUT VISION, V5, P195, DOI 10.1007/BF00054921; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; MODAYUR B, 1994, P INT C PATT REC JER, VD, P284; MODAYUR B, 1994, ISL01081995 U WASH D; MODAYUR B, 1994, ISL01011994 U WASH D; MODAYUR BR, 1995, THESIS U WASHINGTON; MODAYUR BR, 1996, P ICPR96, V1, P238; MODAYUR BR, 1995, CAMP 95 COMPUTER ARC, P313; Olson C. F., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P387, DOI 10.1109/CVPR.1993.341101; OLSON CF, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P251, DOI 10.1109/CVPR.1994.323837; THOMPSON DW, 1987, IEEE J ROBOTIC AUTOM, P208	25	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	1997	19	5					499	506		10.1109/34.589210	http://dx.doi.org/10.1109/34.589210			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XB163					2022-12-18	WOS:A1997XB16300009
J	Chatterjee, C; Roychowdhury, VP				Chatterjee, C; Roychowdhury, VP			An adaptive stochastic approximation algorithm for simultaneous diagonalization of matrix sequences with applications	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						adaptive generalized eigen-decomposition; interference cancellation	NEURAL NETWORKS	We describe an adaptive algorithm based on stochastic approximation theory for the simultaneous diagonalization of the expectations of two random matrix sequences. Although there are several conventional approaches to solving this problem, there are many applications in pattern analysis and signal detection that require an online (i.e., real-time) procedure for this computation. In these applications, we are given two sequences of random matrices {A(k)} and {B-k} as online observations, with lim(k-->infinity)E[A(k)] = A and lim(k-->infinity)E[B-k] = B, where A and B are real, symmetric and positive definite. For every sample (A(k),B-k), we need the current estimates Phi(k) and Lambda(k) respectively of the eigenvectors Phi and eigenvalues Lambda of A(-1) B. We have described such an algorithm where Phi(k) and Lambda(k) converge provably with probability one to Phi and Lambda respectively. A novel computational procedure used in the algorithm is the adaptive computation of A(-1/2). Besides its use in the generalized eigen-decomposition problem, this procedure can be used on its own in several feature extraction problems. The performance of the algorithm is demonstrated with an example of detecting a high-dimensional signal in the presence of interference and noise, in a digital mobile communications problem. Experiments comparing computational complexity and performance demonstrate the effectiveness of the algorithm in this real-time application.	UNIV CALIF LOS ANGELES,DEPT ELECT ENGN,LOS ANGELES,CA 90095	University of California System; University of California Los Angeles	Chatterjee, C (corresponding author), NEWPORT CORP,1791 DEERE DR,IRVINE,CA 92606, USA.							BALDI PF, 1995, IEEE T NEURAL NETWOR, V6, P837, DOI 10.1109/72.392248; CHATTERJEE C, 1996, THESIS PURDUE U W LA; FUKUNAGA K, 1990, INTRO STATISTICAL PA; Golub G.H., 2013, MATRIX COMPUTATIONS, P357; LJUNG L, 1977, IEEE T AUTOMAT CONTR, V22, P551, DOI 10.1109/TAC.1977.1101561; MAO JC, 1995, IEEE T NEURAL NETWOR, V6, P296, DOI 10.1109/72.363467; SANGER TD, 1989, NEURAL NETWORKS, V2, P459, DOI 10.1016/0893-6080(89)90044-0; ZOLTOWSKI MD, UNPUB BLIND ADPATIVE	8	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	1997	19	3					282	287		10.1109/34.584108	http://dx.doi.org/10.1109/34.584108			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WR582					2022-12-18	WOS:A1997WR58200012
J	Burlina, P; Chellappa, R				Burlina, P; Chellappa, R			Analyzing looming motion components from their spatiotemporal spectral signature	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						motion analysis; frequency domain analysis; time-to-collision; Mellin transforms; spectral structure	TIME	This paper addresses the use of spatiotemporal transform methods applied to the analysis of dynamic image sequences and the characterization of image motion. Image motion including a divergent component (resulting from a looming camera component) is analyzed in the spatiotemporal Merlin Transform (MT) domain, resulting in the separation of the spectrum into two parts: a structural term corresponding to the spatial MT of the static image, and a kinematic term depending on Time-to-Collision (a motion support). We examine potential applications of this property for the recovery of image motion from integral image brightness measurements and the computation of Time-To-Collision using spatiotemporal MT analysis.	UNIV MARYLAND,DEPT ELECT ENGN,COLLEGE PK,MD 20742	University System of Maryland; University of Maryland College Park	Burlina, P (corresponding author), UNIV MARYLAND,CTR AUTOMAT RES,AV WILLIAMS BLDG,ROOM 4417,COLLEGE PK,MD 20742, USA.		Chellappa, Rama/AAJ-1504-2020; Chellappa, Rama/B-6573-2012; Chellappa, Rama/AAV-8690-2020					ABUMOSTAFA YS, 1984, IEEE T PATTERN ANAL, V6, P698, DOI 10.1109/TPAMI.1984.4767594; BURT PJ, 1991, OCT IEEE WORKSH VIS, P187; CHEN W, 1994, P INT C IM PROC AUST, P232; Colombo S., 1972, TRANSFORMATIONS LAPL; CORTELAZZO G, 1993, IEEE T PATTERN ANAL, V15, P411, DOI 10.1109/34.206960; ECKERT MP, 1993, PHILOS T ROY SOC B, V339, P385, DOI 10.1098/rstb.1993.0038; FLEET DJ, 1993, IEEE T PATTERN ANAL, V15, P1253, DOI 10.1109/34.250844; GOH S, 1985, THESIS PURDUE U; Heeger D. J., 1988, INT J COMPUT VISION, V1, P279; Iwasaki K., 1991, GAUSS PAINLEVE; JASINSCHI RS, 1992, IEEE T PATTERN ANAL, V14, P353, DOI 10.1109/34.120330; KONFORTI N, 1990, J OPT SOC AM A, V7, P225, DOI 10.1364/JOSAA.7.000225; NELSON R, 1989, IEEE T PATTERN ANAL, V2, P1102; SEGMAN J, 1992, IEEE T PATTERN ANAL, V14, P1171, DOI 10.1109/34.177382; SHENG Y, 1986, J OPT SOC AM A, V3, P771, DOI 10.1364/JOSAA.3.000771; TISTARELLI M, 1993, IEEE T PATTERN ANAL, V15, P401, DOI 10.1109/34.206959; Yao Y. S., 1995, Proceedings. International Conference on Image Processing (Cat. No.95CB35819), P191, DOI 10.1109/ICIP.1995.529578	17	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	1996	18	10					1029	1033		10.1109/34.541412	http://dx.doi.org/10.1109/34.541412			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	VP691					2022-12-18	WOS:A1996VP69100007
J	CANNING, J				CANNING, J			A MINIMUM DESCRIPTION LENGTH MODEL FOR RECOGNIZING OBJECTS WITH VARIABLE APPEARANCES (THE VAPOR MODEL)	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						VARIABLE APPEARANCE OBJECTS; OBJECT MODELS; MINIMUM DESCRIPTION LENGTH; OBJECT RECOGNITION; SHAPE RECOGNITION; KNOWLEDGE REPRESENTATION	CONTOURS	Most object recognition systems can only model objects composed of rigid pieces whose appearance depends only on lighting and viewpoint. Many real world objects, however, have variable appearances because they are flexible and/or have a variable number of parts. These objects cannot be easily modeled using current techniques. We propose the use of a knowledge representation called the VAPOR (Variable APpearance Object Representation) model to represent objects with these kinds of variable appearances. The VAPOR model is an idealization of the object; all instances of the model in an image are variations from the ideal appearance. The variations are evaluated by the description length of the data given the model, i.e., the number of information-theoretic bits needed to represent the model and the deviations of the data from the ideal appearance. The shortest length model is chosen as the best description. We demonstrate how the VAPOR model performs in a simple domain of circles and polygons and in the complex domain of finding cloverleaf interchanges in aerial images of roads.	UNIV MARYLAND, CTR AUTOMAT RES, COLLEGE PK, MD 20742 USA	University System of Maryland; University of Maryland College Park	CANNING, J (corresponding author), UNIV OKLAHOMA, SCH COMP SCI, 200 FELGAR ST, ROOM 114, NORMAN, OK 73019 USA.		Canning, John/R-7421-2019					BODDY M, 1989, 11TH P INT JOINT C A, P979; BROOKS RA, 1981, ARTIF INTELL, V17, P285, DOI 10.1016/0004-3702(81)90028-X; CANNING J, 1991, CARTR563 CTR AUT RES; FUA P, 1989, P 11 INT JOINT C ART, P1596; GRIMSON WEL, 1987, 1ST P INT C COMP VIS, P93; KANIZSA G, 1976, SCI AM, V234, P48, DOI 10.1038/scientificamerican0476-48; KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570; KRIEGMAN DJ, 1990, IEEE T PATTERN ANAL, V12, P1127, DOI 10.1109/34.62602; LOWE DG, 1987, INT J COMPUT VISION, V1, P57, DOI 10.1007/BF00128526; PEDNAULT EPD, 1989, P INT JOINT C ART IN, P1603; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; RISSANEN J, 1983, ANN STAT, V11, P416, DOI 10.1214/aos/1176346150; Segen J., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P597, DOI 10.1109/CVPR.1989.37907; TERZOPOULOS D, 1987, INT J COMPUT VISION, V1, P211, DOI 10.1007/BF00127821	14	2	2	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	1994	16	10					1032	1036		10.1109/34.329006	http://dx.doi.org/10.1109/34.329006			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PM827					2022-12-18	WOS:A1994PM82700007
J	CHANDRAN, PS				CHANDRAN, PS			COMPARATIVE-ANALYSIS OF BACKPROPAGATION AND THE EXTENDED KALMAN FILTER FOR TRAINING MULTILAYER PERCEPTRONS - COMMENTS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note						BACKPROPAGATION; EXTENDED KALMAN FILTER; NEURAL NETWORKS		In this note, the connection between the backpropagation algorithm and the extended Kalman fiber is analyzed using an alternate form of representation for the Kalman gain term and shown to be much simpler than that reported by Ruck et al.			CHANDRAN, PS (corresponding author), NANYANG TECHNOL UNIV,SCH ELECT & ELECTR ENGN,CTR SIGNAL PROC,NANYANG AVE,SINGAPORE 2263,SINGAPORE.							Jacobs O. L. R., 1974, INTRO CONTROL THEORY; RUCK DW, 1992, IEEE T PATTERN ANAL, V14, P686, DOI 10.1109/34.141559; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Singhal S., 1989, ADV NEURAL INFORM PR, P133; Stengel Robert F, 1986, STOCHASTIC OPTIMAL C	5	2	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	1994	16	8					862	863						2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PB475					2022-12-18	WOS:A1994PB47500015
J	TARATORIN, AM; SIDEMAN, S				TARATORIN, AM; SIDEMAN, S			CONSTRAINED REGULARIZED DIFFERENTIATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						EDGE DETECTION; REGULARIZATION; A PRIORI CONSTRAINTS; PROJECTION ON CONVEX SETS; CONTOUR MODIFICATION	EDGE-DETECTION; BOUNDARY DETECTION; SCALE-SPACE	Numerical differentiation is an iii-posed problem. This correspondence demonstrates that the application of the regularization theory together with the methods of projections on convex sets of constraints improve the accuracy of the derivatives calculation. Simulation results are presented and the applications of the proposed method to edge detection are discussed.			TARATORIN, AM (corresponding author), TECHNION ISRAEL INST TECHNOL, JULIUS SLIVER INST, DEPT BIOMED ENGN, HEART SYST RES CTR, IL-32000 HAIFA, ISRAEL.							BERTERO M, 1988, P IEEE, V76, P869, DOI 10.1109/5.5962; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; FENG J, 1991, IEEE T MED IMAGING, V10, P187, DOI 10.1109/42.79477; FONG YS, 1989, OPT ENG, V28, P749, DOI 10.1117/12.7977031; FRIEDLAND N, 1989, IEEE T MED IMAGING, V8, P344, DOI 10.1109/42.41487; GERMAN D, 1990, IEEE T PATTERN ANAL, V12, P609; Hamming R. W., 1977, DIGITAL FILTERS; HANCOCK ER, 1990, IEEE T PATTERN ANAL, V12, P165, DOI 10.1109/34.44403; Huang T.S, 1981, 2 DIMENSIONAL DIGITA, V43; Hurt N., 1989, PHASE RETRIEVAL ZERO; KATSAGGELOS AK, 1988, P ICASSP, V88, P1048; LU Y, 1992, IEEE T PATTERN ANAL, V14, P450, DOI 10.1109/34.126806; PAVLIDIS T, 1990, IEEE T PATTERN ANAL, V12, P225, DOI 10.1109/34.49050; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; PHILIP K, 1991, COMPUT CARDIOL, P443; Poggio T., 1988, Journal of Complexity, V4, P106, DOI 10.1016/0885-064X(88)90024-6; TAGARE HD, 1990, IEEE T PATTERN ANAL, V12, P1186, DOI [10.1109/34.62607, 10.1117/12.19530]; TARATORIN AM, 1991, UNPUB IEEE T SIGNAL; Tikhonov A.N., 1977, SOLUTION ILL POSED P; YUILLE AL, 1986, IEEE T PATTERN ANAL, V8, P15, DOI [10.1109/34.41383, 10.1109/TPAMI.1986.4767748]	20	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	1994	16	1					88	92		10.1109/34.273713	http://dx.doi.org/10.1109/34.273713			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MV733					2022-12-18	WOS:A1994MV73300009
J	BERKEMEIER, MD; FEARING, RS				BERKEMEIER, MD; FEARING, RS			DETERMINING THE AXIS OF A SURFACE OF REVOLUTION USING TACTILE SENSING	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						CLASS CONSTRAINT; POSE IDENTIFICATION; SURFACE OF REVOLUTION; TACTILE SENSING	3-D	Dextrous robot hands need to be able to determine the pose of objects to reliably grasp and manipulate them. The first few contacts with an object can be used to provide an initial estimate of this information if we constrain the object to be of a particular class. This paper considers a simple example of exploiting class constraints: finding the axis of an unknown surface of revolution. Three tactile curvature measurements on a surface of revolution with twice-differentiable sweeping rule function are shown to be sufficient for determining the axis except for certain singular configurations. Position and orientation error uncertainties and experimental results are presented for a cylindrical tactile sensor.	UNIV CALIF BERKELEY,DEPT ELECT ENGN & COMP SCI,ELECTR RES LAB,BERKELEY,CA 94720	University of California System; University of California Berkeley				Fearing, Ronald/0000-0001-6242-5379				Allen P. K., 1986, Proceedings 1986 IEEE International Conference on Robotics and Automation (Cat. No.86CH2282-2), P126; ALLEN PK, 1990, IEEE T ROBOTIC AUTOM, V6, P397, DOI 10.1109/70.59353; BERKEMEIER MD, 1989, UCBERL M89117 UC BER; BERKEMEIER MD, 1990, P IEEE INT C ROB AUT, P974; BRADY M, 1985, 2ND P INT S ROB RES, P5; COLE R, 1987, J ALGORITHM, V8, P19, DOI 10.1016/0196-6774(87)90025-3; ELLIS RE, 1989, IEEE INT C ROB AUT, P348; ELLIS RE, 1987, TACTILE SENSING STRA; FAUGERAS OD, 1986, INT J ROBOT RES, V5, P27, DOI 10.1177/027836498600500302; FEARING RS, 1991, IEEE T ROBOTIC AUTOM, V7, P826; FEARING RS, 1990, DEXTROUS ROBOT HANDS; GASTON PC, 1983, TACTILE RECOGNITON L; GRIMSON WEL, 1988, ROBOTIC RES, P245; Lipschutz M., 1969, DIFFERENTIAL GEOMETR; PRINTZ H, 1987, P IEEE COMPUT SOC WO; RAO K, 1988, INT J COMPUT VISION, V2, P33, DOI 10.1007/BF00836280; 1987, IEEE INT C ROBOTICS	17	2	4	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	1993	15	10					1079	1087		10.1109/34.254065	http://dx.doi.org/10.1109/34.254065			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MD775					2022-12-18	WOS:A1993MD77500010
J	GREGOR, J; THOMASON, MG				GREGOR, J; THOMASON, MG			HYBRID PATTERN-RECOGNITION USING MARKOV NETWORKS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						BANDED HUMAN CHROMOSOMES; DATA-DRIVEN INFERENCE; DECISION-THEORETIC PATTERN CLASSIFICATION; MARKOV NETWORK; STRING SEGMENTATION; STRUCTURAL PATTERN ANALYSIS		Markov networks are inferred automatically for different classes of learning strings. In subsequent string-to-network alignments for test samples, the networks are used to deduce structural characteristics and to provide similarity measures. By processing the similarity measures as numerical-valued features, standard nonparametric decision-theoretic pattern classifiers may be applied to determine class membership. This correspondence addresses nearest-neighbor rule and linear discriminant function classifiers and compares their performance with a maximum likelihood classifier. The hybrid system's ability to determine string orientation correctly is also investigated. Experiments with several thousand human banded chromosomes are reported.	UNIV AALBORG,INST ELECTR SYST,AALBORG,DENMARK	Aalborg University	GREGOR, J (corresponding author), UNIV TENNESSEE,DEPT COMP SCI,KNOXVILLE,TN 37996, USA.							Bunke H., 1990, SYNTACTIC STRUCTURAL; BUNKE H, 1990, SYNTACTIC STRUCTURAL, P307; Devijver PA, 1982, PATTERN RECOGNITION; Duda R.O., 1972, PATTERN CLASSIFICATI; FILIPSKI AJ, 1980, IEEE T PATTERN ANAL, V2, P252, DOI 10.1109/TPAMI.1980.4767012; FU KS, 1979, IEEE T SYST MAN CYB, V9, P55; FU KS, 1983, IEEE T PATTERN ANAL, V5, P200; GONZALEZ RG, 1978, SYNTACTIC PATTERN R; GRANUM E, 1990, CYTOMETRY, V11, P26, DOI 10.1002/cyto.990110105; GRANUM E, 1989, AUTOMATION CYTOGENET, P233; GRANUM E, 1980, THESIS TU DENMARK; Gregor J., 1991, International Journal of Pattern Recognition and Artificial Intelligence, V5, P413, DOI 10.1142/S0218001491000235; Kruskal J.B., 1983, TIME WARPS STRING ED; LUNDSTEEN C, 1980, CLIN GENET, V18, P355; RAUDYS SJ, 1990, 10TH INT C PATTERN R, V1, P417; STRANG G, 1980, LINEAR ALGEBRA ITS A; THOMASON MG, 1986, IEEE T PATTERN ANAL, V8, P491, DOI 10.1109/TPAMI.1986.4767813; Tou JT, 1974, PATTERN RECOGN; Vidal E., 1989, International Journal of Pattern Recognition and Artificial Intelligence, V3, P181, DOI 10.1142/S0218001489000152	19	2	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	1993	15	6					651	656		10.1109/34.216736	http://dx.doi.org/10.1109/34.216736			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	LF257					2022-12-18	WOS:A1993LF25700015
J	LEE, WT; TENORIO, MF				LEE, WT; TENORIO, MF			ON AN ASYMPTOTICALLY OPTIMAL ADAPTIVE CLASSIFIER DESIGN CRITERION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note						CLASSIFIER COMPLEXITY; CLASSIFIER DESIGN CRITERION; CLASSIFIER METRICS; HIDDEN LAYER OPTIMAL SIZING; MINIMUM DESCRIPTION LENGTH; NEURAL NETWORKS; VAPNIK-CHERVONEKIS DIMENSION (VCDIM)	SEGMENTATION	One of the central problems in classifier design is the estimation of classification error. The difficulty in estimating the error probability of a classifier is due to the unknown sample distribution and small number of training samples available. In this correspondence, we will present a new approach for solving this problem. In our model, there are two types of classification error: empirical and generalization error. The first is the error observed over the training samples, and the second is the discrepancy between the error probability and empirical error. In general, the error probability of a classifier can be bounded from above by the sum of empirical and generalization errors. Since both terms depend on classifier complexity, we need a proper measure for classifier complexity. In this research, we adopted the Vapnik and Chervonenkis dimension (VCdim) as such a measure. Based on this complexity measure, we have developed an estimate for generalization error. An optimal classifier design criterion (the generalized minimum empirical error criterion (GMEE)) has been proposed. The GMEE criterion consists of two terms: the empirical and the estimate of generalization error. This criterion is useful for optimal classifier design since the classifier that minimizes the criterion is the one with the smallest error probability. Furthermore, we prove that the GMEE criterion is GAMMA optimal. This means that the criterion can select the best classifier from GAMMA, which is a collection of classifiers with finite VCdim. As an application, the criterion is used to design the optimal neural network classifier. A corollary to the GAMMA optimality of neural network-based classifiers has been proven. Thus, our approach provides a theoretic foundation for the connectionist approach to optimal classifier design. Experimental results are given to validate this approach, followed by discussions and suggestions for future research.			LEE, WT (corresponding author), PURDUE UNIV,SCH ELECT ENGN,W LAFAYETTE,IN 47907, USA.							BARRON AR, 1989, P DECISION CONTR; BAUM EB, 1988, ADV NEURAL INFO PROC; BLUMER A, 1987, UCSCCRL8720 UC SANTZ; COHN D, 1990, TR910304 U WASH; CRAVEN P, 1979, NUMER MATH, V31, P377, DOI 10.1007/BF01437407; CYBENKO G, 1989, CSRD856 U ILL REP; DEVROYE L, 1988, IEEE T PATTERN ANAL, V10, P530, DOI 10.1109/34.3915; Duda R.O., 1973, J ROYAL STAT SOC SER; DUDLEY RM, 1978, ANN PROBAB, V6, P899, DOI 10.1214/aop/1176995384; FOLEY DH, 1972, IEEE T INFORM THEORY, V18, P618, DOI 10.1109/TIT.1972.1054863; FUKUNAGA K, 1978, INTRO STATISTICAL PA; GLICK N, 1976, IEEE T INFORM THEORY, V22, P454, DOI 10.1109/TIT.1976.1055581; Grenander U., 1981, ABSTRACT INFERENCE; HARALICK RM, 1985, COMPUT VISION GRAPH, V29, P100, DOI 10.1016/S0734-189X(85)90153-7; HSIAO JY, 1989, IEEE T PATTERN ANAL, V11, P1279, DOI 10.1109/34.41366; LEE WT, 1991, IN PRESS OPTIMAL ADA; LEE WT, 1991, IN PRESS COMPUTATION; LIPPMANN RP, 1989, IEEE COMMUNICATI NOV, P47; MORGAN N, 1990, ADV NEURAL INFORMATI, P630; Rabiner L. R., 1986, IEEE ASSP MAGAZI JAN, P4; Ruck D W, 1990, IEEE Trans Neural Netw, V1, P296, DOI 10.1109/72.80266; RUMELHART M, 1987, PARALLEL DISTRIBUTIO, V1; STONE M, 1974, J R STAT SOC B, V36, P111, DOI 10.1111/j.2517-6161.1974.tb00994.x; Tikhonov A., 1977, SOLUTIONS ILL POSED; TOUSSAINT GT, 1974, IEEE T INFORM THEORY, V20, P422; Vapnik V., 1982, ESTIMATION DEPENDENC; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; WEIGEND AS, 1990, TRPARCSSL9020 STANF	28	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	1993	15	3					312	318		10.1109/34.204915	http://dx.doi.org/10.1109/34.204915			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	KT658					2022-12-18	WOS:A1993KT65800014
J	NURRE, JH; HALL, EL				NURRE, JH; HALL, EL			ENCODED MOIRE INSPECTION BASED ON A COMPUTER SOLID MODEL	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						COMPUTER SOLID MODELING; MODEL-BASED VISION; PROJECTION MOIRE; STEREO VISION; STRUCTURED LIGHT	SURFACE MEASUREMENT; PROJECTION; TOPOGRAPHY	The results of an investigation into a system linking a computer solid modeler to an active stereo imaging inspection station are presented. Given the computer solid model of a surface, a projected pattern can be encoded so that the reflections from the actual object's surface will exhibit a uniformly simple pattern when the object's surface and modeled surface are identical. Research concerning the method or generating projection patterns, as well as the sensitivity of the system, is discussed. A prototype system consisting of a computer graphics workstation, a flexible projector, and a machine vision system was used to inspect two different manufactured parts. The results indicate the feasibility of this high-speed inspection process.	UNIV CINCINNATI,CTR ROBOT RES,CINCINNATI,OH 45221	University System of Ohio; University of Cincinnati	NURRE, JH (corresponding author), OHIO UNIV,ATHENS,OH 45701, USA.			Hall, Ernest/0000-0003-4361-8647				AGNEW KL, 1971, Patent No. 3619065; CHIN RT, 1982, IEEE T PATTERN ANAL, V4, P557, DOI 10.1109/TPAMI.1982.4767309; HARDING KG, 1983, APPL OPTICS, V22, P856, DOI 10.1364/AO.22.000856; HARDING KG, 1986, P VISION 86 DETROIT, P2; LOHMANN AW, 1980, OPT COMMUN, V34, P167, DOI 10.1016/0030-4018(80)90007-3; MICHALSKI M, 1986, APPL OPTICS, V25, P4338, DOI 10.1364/AO.25.004338; NURRE JH, 1992, CVGIP-IMAG UNDERSTAN, V56, P131, DOI 10.1016/1049-9660(92)90035-2; NURRE JH, 1991, IEEE T PATTERN ANAL, V13, P491, DOI 10.1109/34.134048; NURRE JH, 1988, NOV P SPIE ADV INT R; PERRIN JC, 1979, APPL OPTICS, V18, P563, DOI 10.1364/AO.18.000563; PIRODDA L, 1982, OPT ENG, V21, P640, DOI 10.1117/12.7972959; POSDAMER JL, 1982, COMPUT VISION GRAPH, V18, P1, DOI 10.1016/0146-664X(82)90096-X; SATO K, 1985, J ROBOTIC SYST, V2, P27; SCIAMMARELLA CA, 1982, EXP MECH, V22, P418, DOI 10.1007/BF02326823; SRINIVASAN R, 1982, OPT ENG, V21, P655, DOI 10.1117/12.7972961; WHITTED T, 1980, COMMUN ACM, V23, P343, DOI 10.1145/358876.358882; YATAGAI T, 1977, OPT COMMUN, V20, P243, DOI 10.1016/0030-4018(77)90342-X; 1988, IDEAS TM OPEN ARCHIT	18	2	2	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	1992	14	12					1214	1218		10.1109/34.177388	http://dx.doi.org/10.1109/34.177388			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	KC573					2022-12-18	WOS:A1992KC57300009
J	GREENSPAN, H; PORAT, M; ZEEVI, YY				GREENSPAN, H; PORAT, M; ZEEVI, YY			PROJECTION-BASED APPROACH TO IMAGE-ANALYSIS - PATTERN-RECOGNITION AND REPRESENTATION IN THE POSITION-ORIENTATION SPACE	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter							GLASS PATTERNS; SEGMENTATION; SELECTION	A method for image analysis, based on the formalism of functional analysis, is proposed. This approach, which is employed for pattern recognition in the combined position-orientation space, is motivated by a variety of neurophysiological findings that emphasize the importance of the orientation feature in visual image processing and analysis. The approach is applied to the so-called 'Glass patterns," which are special cases of images characterized uniquely by their orientational feature of local correlations between pairs of corresponding dots. The approach can be of interest in the analysis of optical flow.	TECHNION ISRAEL INST TECHNOL,DEPT ELECT ENGN,HAIFA,ISRAEL	Technion Israel Institute of Technology	GREENSPAN, H (corresponding author), AT&T BELL LABS,DEPT SIGNAL PROC RES,MURRAY HILL,NJ 07974, USA.							ADIV G, 1985, IEEE T PATT ANAL MAC, V7; BUXTON BF, 1985, IMAGE VISION COMPUT, V3, P163, DOI 10.1016/0262-8856(85)90003-4; FOLEY JD, 1982, FUNDAMENTAL COMPUTER; GIUSTO DD, 1989, SIGNAL PROCESS, V16, P41, DOI 10.1016/0165-1684(89)90112-6; GLASS L, 1969, NATURE, V243, P578; GREENSPAN H, 1989, 16TH P CONV IEEE ISR; GREENSPAN H, 1990, EE PUB TECHNION ISRA, V765; GROSS A, 1985, PATTERN RECOGN LETT, V3, P263, DOI 10.1016/0167-8655(85)90006-6; Higgins JR., 2004, COMPLETENESS BASIS P; HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455; PHILLIPS TH, 1986, PATTERN RECOGN LETT, V4, P213, DOI 10.1016/0167-8655(86)90022-X; PORAT M, 1989, IEEE T BIO-MED ENG, V36, P115, DOI 10.1109/10.16457; PRAZDNY K, 1986, BIOL CYBERN, V53, P153, DOI 10.1007/BF00342883; STEVENS KA, 1978, BIOL CYBERN, V29, P19, DOI 10.1007/BF00365232; TOMITA F, 1990, COMPUTER ANAL VISUAL, pCH3; WALTERS D, 1987, COMPUT VISION GRAPH, V37, P261, DOI 10.1016/S0734-189X(87)80005-1; ZUCKER SW, 1985, COMPUT VISION GRAPH, V32, P74, DOI 10.1016/0734-189X(85)90003-9; ZUCKER SW, 1983, PHYSICAL BIOL PROCES, P326	18	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	1992	14	11					1105	1110		10.1109/34.166626	http://dx.doi.org/10.1109/34.166626			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JX370					2022-12-18	WOS:A1992JX37000006
J	GRIMSON, WEL; HUTTENLOCHER, DP				GRIMSON, WEL; HUTTENLOCHER, DP			INTRODUCTION TO THE SPECIAL ISSUE ON INTERPRETATION OF 3-D SCENES .1.	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									CORNELL UNIV, ITHACA, NY 14853 USA	Cornell University	GRIMSON, WEL (corresponding author), MIT, CAMBRIDGE, MA 02139 USA.								0	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	1991	13	10					969	970						2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GM763					2022-12-18	WOS:A1991GM76300001
J	LEUNG, MK; HUANG, TS				LEUNG, MK; HUANG, TS			AN INTEGRATED APPROACH TO 3-D MOTION ANALYSIS AND OBJECT RECOGNITION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						FEATURE CORRESPONDENCES; FEATURE EXTRACTION; MATCHING; MOTION ESTIMATION; OBJECT RECOGNITION		In this paper, an integrated system for three-dimensional (3-D) motion estimation and object recognition with outdoor stereo image sequences as inputs is presented. The scene contains stationary background with a moving vehicle. The goals are to obtain the 3-D motion description and the identification of the vehicle from the input stereo images. In order to accomplish the desired goals, the system consists of four stages: 1) motion estimation, 2) distinctive feature extraction, 3) model database, and 4) object recognition. The results of 3-D motion estimation are used to considerably narrow down the search space in the stage of object recognition. The system is applied to eight sets of stereo image pairs. Experimental results demonstrate that the system can successfully provide the 3-D motion description and the identification of the vehicle in all the stereo image pairs.	UNIV ILLINOIS,BECKMAN INST,URBANA,IL 61801	University of Illinois System; University of Illinois Urbana-Champaign	LEUNG, MK (corresponding author), UNIV ILLINOIS,COORDINATED SCI LAB,URBANA,IL 61801, USA.							BOLLES RC, 1986, INT J ROBOT RES, V5, P3, DOI 10.1177/027836498600500301; FAN TJ, 1989, IEEE T PATTERN ANAL, V11, P1140, DOI 10.1109/34.42853; FAUGERAS OD, 1983, AUG P INT JOINT C AR; GRIMSON WEL, 1984, INT J ROBOT RES, V3, P3, DOI 10.1177/027836498400300301; GRIMSON WEL, 1987, IEEE T PATT ANAL MAC, V9; HEURTAS A, 1986, IEEE T PATTERN ANAL, V8, P651; HORAUD R, 1984, MAR P INT C ROB ATL, P78; HUANG TS, 1986, JUN P IEEE C COMP VI; KIM YC, 1985, JUN P IEEE C COMP VI; KIMME C, 1975, COMMUN ACM, V18, P120, DOI 10.1145/360666.360677; LEUNG MK, 1991, THESIS U ILLINOIS UR; LEUNG MK, 1990, 10TH P INT C PATT RE; MIKHAIL EM, 1988, PHOTOGRAMMETRIC SERI; MUUSS M, 1987, JUN SECAD VLD COMP C	14	2	3	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	1991	13	10					1075	1084		10.1109/34.99240	http://dx.doi.org/10.1109/34.99240			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GM763					2022-12-18	WOS:A1991GM76300009
J	NURRE, JH; HALL, EL				NURRE, JH; HALL, EL			POSITIONING QUADRIC SURFACES IN AN ACTIVE STEREO IMAGING-SYSTEM	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						COMPUTER VISION; CONIC SECTIONS; PINHOLE MODEL; PROJECTION MOIRE; RONCHI GRATING; STEREO IMAGING; TRIANGULATION	MOIRE TOPOGRAPHY; PROJECTION	The location of surfaces using stereo imaging techniques is an important area of research for robot guidance and machine inspection applications. This correspondence will investigate the underlying geometry of finite focal length stereo pinhole cameras. This is the model used in both active and passive stereo imaging systems. It is shown that the points of intersecting views from the pinhole models result in conic sections. This information is used to locate quadric surfaces in the inspection space. This work provides a basis for understanding the underlying geometry of stereo vision and uses that understanding for solving practical positioning problems.	UNIV CINCINNATI,CTR ROBOT RES,CINCINNATI,OH 45221	University System of Ohio; University of Cincinnati				Hall, Ernest/0000-0003-4361-8647				HALL EL, 1979, COMPUTER IMAGE PROCE, pCH3; HARDING KG, 1986, P VISION 86 DETROIT, P2; LOHMANN AW, 1980, OPT COMMUN, V34, P167, DOI 10.1016/0030-4018(80)90007-3; MICHALSKI M, 1986, APPL OPTICS, V25, P4338, DOI 10.1364/AO.25.004338; NURRE JH, 1988, P SPIE ADV INTELLIGE, V1004, P48; PERRIN JC, 1979, APPL OPTICS, V18, P563, DOI 10.1364/AO.18.000563; PIRODDA L, 1982, OPT ENG, V21, P640, DOI 10.1117/12.7972959; SCIAMMARELLA CA, 1982, EXP MECH, V22, P418, DOI 10.1007/BF02326823; YATAGAI T, 1977, OPT COMMUN, V20, P243, DOI 10.1016/0030-4018(77)90342-X	9	2	2	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	1991	13	5					491	495		10.1109/34.134048	http://dx.doi.org/10.1109/34.134048			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	FQ207					2022-12-18	WOS:A1991FQ20700009
J	CHIU, DKY; WONG, AKC; CHAN, KCC				CHIU, DKY; WONG, AKC; CHAN, KCC			SYNTHESIS OF STATISTICAL KNOWLEDGE FROM TIME-DEPENDENT DATA	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						DISCRETE-VALUED MULTIVARIATE TIME SERIES; EVENT-COVERING; FORECASTING; INDUCTIVE PREDICTION; MACHINE LEARNING; MIXED-MODE MULTIVARIATE TIME SERIES		This paper presents a general approach to analyzing multivariate time-dependent system processes with discrete-valued (both nominal and ordinal) and/or continuous-valued outcomes. The approach is based on an event-covering approach which selects (or covers) a subspace from the outcome space of an n-tuple of variables for estimation purposes. From the covered subspace, statistically interdependent events are selected as statistical knowledge for forecasting unknown events. The event-covering method presented here is based on the use of restricted variables with only a subset of the outcomes considered. Extension to event-covering method based on the selection of joint outcomes is also discussed. We have tested this method using climatic data and simulated data which model situations in real life. The experiments successfully demonstrate both its flexibility and efficacy.	UNIV WATERLOO,PATTERN ANAL & MACHINE INTELLIGENCE LAB,WATERLOO N2L 3G1,ONTARIO,CANADA; IBM CORP,CANADA LAB,N YORK,ONTARIO,CANADA	University of Waterloo; International Business Machines (IBM)	CHIU, DKY (corresponding author), UNIV GUELPH,DEPT COMP & INFORMAT SCI,GUELPH N1G 2W1,ONTARIO,CANADA.		Chiu, David/C-4922-2013	CHAN, Keith C.C./0000-0001-7962-6564; Chan, Keith C C/0000-0003-1709-2637; Keith, Chan/0000-0001-7296-873X				Abraham B, 1983, STAT METHODS FORECAS; Bishop YM., 2007, DISCRETE MULTIVARIAT; BOX GEP, 1970, TIME SERIES ANAL; CHAN KCC, 1988, LECT NOTES COMPUT SC, V301, P507; CHAN KCC, 1989, THESIS U WATERLOO CA; Chiu D. K. Y., 1990, Journal of Experimental and Theoretical Artificial Intelligence, V2, P117, DOI 10.1080/09528139008953718; CHIU DKY, 1986, IEEE T SYST MAN CYB, V16, P251, DOI 10.1109/TSMC.1986.4308945; CHRISTENSEN R, 1973, P IEEE INT C CYBERNE, P321; CRAMER H, 1946, MATH METHODS STATIST; DITTERICH TG, 1985, ARTIF INTELL, V25, P187; DRAPER NR, 1966, APPLIED REGRESSION A; Fienberg S.E., 1980, ANAL CROSS CLASSIFIE, V2nd ed.; Fingleton B, 1984, MODELS CATEGORY COUN; HABERMAN SJ, 1973, BIOMETRICS, V29, P205, DOI 10.2307/2529686; KALBFLEISCH JG, 1985, PROBABILITY STATISTI; LASCURAIN M, 1983, THESIS U WATERLOO ON; MAKRIDAKIS S, 1979, STUDIES MANAGEMENT S, V12; MICHALSKI RS, 1986, EXPERT SYSTEMS; MITCHELL BR, 1980, EUROPEAN HIST STATIS; MORTIMER E, 1979, MATH INTRO ITS SPIRI; RAO CR, 1952, ADV STATISTICAL METH; SANDERSON AC, 1980, IEEE T SYST MAN CYB, V10, P384, DOI 10.1109/TSMC.1980.4308519; UNNY TE, 1978, ADV HYDROSCI, V12, P195; WANG DCC, 1979, IEEE T AUTOMAT CONTR, V24, P434, DOI 10.1109/TAC.1979.1102039; Wheelwright, 1982, HDB FORECASTING MANA; WONG AKC, 1987, PATTERN RECOGN, V20, P245, DOI 10.1016/0031-3203(87)90058-6; WONG AKC, 1975, IEEE T COMPUT, VC 24, P158, DOI 10.1109/T-C.1975.224183; WONG AKC, 1987, IEEE T PATTERN ANAL, V9, P796, DOI 10.1109/TPAMI.1987.4767986; WONG AKC, 1979, IEEE T PATTERN ANAL, V1, P342, DOI 10.1109/TPAMI.1979.4766942; WONG AKC, 1982, NATO ADV STUDY I SER, P157; Wrigley N, 1985, CATEGORICAL DATA ANA	31	2	2	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	1991	13	3					265	271		10.1109/34.75513	http://dx.doi.org/10.1109/34.75513			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	FG360					2022-12-18	WOS:A1991FG36000005
J	GONG, YF; HATON, JP				GONG, YF; HATON, JP			SIGNAL-TO-STRING CONVERSION BASED ON HIGH LIKELIHOOD REGIONS USING EMBEDDED DYNAMIC-PROGRAMMING	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note						DYNAMIC PROGRAMMING; LIKELIHOOD FUNCTION; SPEECH RECOGNITION; STRING RECOGNITION		Many pattern recognition problems involve signal-to-string conversion. The string recognition problem can be formulated as the maximization of a constrained time integral of a sequence of likelihood functions. Such likelihood functions are time series of likelihood ratios between image of component symbols and input data. We propose in this correspondence a new method of conversion based on embedded dynamic programming which can adapt its search to the variation of the input signal. The optimizing process is guided by high valued portions of likelihood function of symbols composing the string and is solved by two embedded dynamic programming processes. Applied to continuous speech recognition using phoneme as basic recognition unit on a 100-word vocabulary, the method achieved 4% improvement of recognition rate in a 1/20 time compared to a classical DP-based method.	SE UNIV NANJING,NANJING,PEOPLES R CHINA	Southeast University - China	GONG, YF (corresponding author), INRIA LORRAINE,CRIN,BP 239,F-54506 VANDOEUVRE NANCY,FRANCE.							BAHL LR, 1983, IEEE T PATTERN ANAL, V5, P179, DOI 10.1109/TPAMI.1983.4767370; Bellman RE, 1957, DYNAMIC PROGRAMMING; CARBONELL N, 1986, P IEEE INT C ACOUST; GONG Y, 1988, THESIS U NANCY 1 NAN; GONG Y, 1987, SEP P EUR C SPEECH T, P121; GONG Y, 1989, 7EME ACT C REC FORM, P1191; GONG Y, 1986, 2ND P INT C ART INT, P521; HATON JP, 1974, CR ACAD SCI A MATH, V278, P1527; MARI JF, 1979, THESIS U NANCY 1 NAN; Nocerino N., 1985, ICASSP 85. Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No. 85CH2118-8), P25; RABINER LR, 1977, IEEE T ACOUST SPEECH, V27, P336; ROBINSON AJ, 1988, JUN P NEURO 88 PAR; WHITE GM, 1978, IEEE T ACOUST SPEECH, P413	13	2	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	1991	13	3					297	302		10.1109/34.75518	http://dx.doi.org/10.1109/34.75518			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	FG360					2022-12-18	WOS:A1991FG36000010
J	XU, J; YANG, YH				XU, J; YANG, YH			GENERALIZED MULTIDIMENSIONAL ORTHOGONAL POLYNOMIALS WITH APPLICATIONS TO SHAPE-ANALYSIS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note											XU, J (corresponding author), UNIV SASKATCHEWAN,DEPT COMP SCI,SASKATOON S7N 0W0,SASKATCHEWAN,CANADA.		Rohlf, F J/A-8710-2008					ABUMOSTAFA YS, 1985, IEEE T PATTERN ANAL, V7, P46, DOI 10.1109/TPAMI.1985.4767617; ABUMOSTAFA YS, 1984, IEEE T PATTERN ANAL, V6, P698, DOI 10.1109/TPAMI.1984.4767594; ALT FL, 1962, OPTICAL CHARACTER RE, P153; Askey R., 1975, ORTHOGONAL POLYNOMIA, DOI DOI 10.1137/1.9781611970470; BOREL R, 1965, MATH PATTERN RECOGNI; BRILL EL, 1968, 25 WESCON SESS QUAL; Cosgriff R.L., 1960, IDENTIFICATION SHAPE; DUBOIS SR, 1986, IEEE T PATTERN ANAL, V8, P55, DOI 10.1109/TPAMI.1986.4767752; DUDANI SA, 1977, IEEE T COMPUT, V26, P39, DOI 10.1109/TC.1977.5009272; DUDANI SA, 1971, THESIS OHIO STATE U; DUDANI SA, 1973, THESIS OHIO STATE U; Duff G., 1966, DIFFERENTIAL EQUATIO; Fritzsche, 1961, SYSTEMATIC METHOD CH; HALL EL, 1975, IEEE T BIO-MED ENG, V22, P518, DOI 10.1109/TBME.1975.324475; HARALICK RM, 1984, IEEE T PATTERN ANAL, V6, P58, DOI 10.1109/TPAMI.1984.4767475; HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692; JEFFREYS H, 1972, METHODS MATH PHYSICS; KASHYAP RL, 1981, IEEE T INFORM THEORY, V27, P627, DOI 10.1109/TIT.1981.1056390; LIN CC, 1987, IEEE T PATTERN ANAL, V9, P686, DOI 10.1109/TPAMI.1987.4767963; MALHATHAPPA KT, 1969, MATH METHODS THEORET; PERSOON E, 1977, IEEE T SYST MAN CYB, V7, P170, DOI 10.1109/TSMC.1977.4309681; Raudseps J. G., 1965, SOME ASPECTS TANGENT; RICHARD CW, 1974, IEEE T SYST MAN CYB, VSMC4, P371, DOI 10.1109/TSMC.1974.5408458; SADJADI FA, 1980, IEEE T PATTERN ANAL, V2, P127, DOI 10.1109/TPAMI.1980.4766990; Sadjadi Firooz A, 1978, P IEEE C PATTERN REC, P127; Szego, 1959, ORTHOGONAL POLYNOMIA; TEH CH, 1988, IEEE T PATTERN ANAL, V10, P496, DOI 10.1109/34.3913; XU J, 1988, PATTERN RECOGN LETT, V7, P191, DOI 10.1016/0167-8655(88)90064-5; ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949	29	2	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1990	12	9					906	913		10.1109/34.57684	http://dx.doi.org/10.1109/34.57684			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DV778					2022-12-18	WOS:A1990DV77800005
J	CHEN, MH; PAVLIDIS, T				CHEN, MH; PAVLIDIS, T			IMAGE SEAMING FOR SEGMENTATION ON PARALLEL ARCHITECTURE	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									SUNY STONY BROOK,DEPT COMP SCI,STONY BROOK,NY 11794	State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook	CHEN, MH (corresponding author), SUNY STONY BROOK,DEPT ELECT ENGN,STONY BROOK,NY 11794, USA.							BOUTHEMY P, 1987, 1ST P INT C COMP VIS, P463; BROWNING JD, 1982, PATTERN RECOGN, V14, P1; CHEN PC, 1979, COMPUT VISION GRAPH, V10, P172, DOI 10.1016/0146-664X(79)90049-2; DOHERTY MF, P CVPR 85 SAN FRANCI, P659; GERSHON R, 1987, 1ST P INT C COMP VIS, P161; HARALICK RM, 1985, COMPUT VISION GRAPHI, V29; HOROWITZ SL, 1976, J ACM, V23, P368, DOI 10.1145/321941.321956; Laprade R. H., 1987, Proceedings of the SPIE - The International Society for Optical Engineering, V758, P74; PAVLIDIS T, 1982, COMPUT VISION GRAPH, V20, P133, DOI 10.1016/0146-664X(82)90041-7; Pavlidis T., 1977, STRUCTURAL PATTERN R; ROSENFELD A, 1982, PATTERN RECOGN, V15, P41, DOI 10.1016/0031-3203(82)90059-0; Serra J, 1982, IMAGE ANAL MATH MORP	12	2	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	1990	12	6					588	594		10.1109/34.56195	http://dx.doi.org/10.1109/34.56195			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	DE003					2022-12-18	WOS:A1990DE00300008
J	HARRISON, DD; WEIR, MP				HARRISON, DD; WEIR, MP			HIGH-SPEED TRIANGULATION-BASED 3-D IMAGING WITH ORTHONORMAL DATA PROJECTIONS AND ERROR-DETECTION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											HARRISON, DD (corresponding author), GE,RES & DEV,SCHENECTADY,NY 12301, USA.							BOLLE RM, 1986, IEEE T PATTERN ANAL, V8; BURRY JM, 1982, OPT COMMUN, V41, P243, DOI 10.1016/0030-4018(82)90423-0; IKEUCHI K, 1981, ARTIF INTELL, V17, P141, DOI 10.1016/0004-3702(81)90023-0; JARVIS RA, 1983, IEEE T PATTERN ANAL, V5; MICHAELSON AA, 1903, LIGHT WAVES THEIR US; MICHON G, 1980, TOPICS APPLIED PHYSI, V38; NIMROD N, 1982, 2ND P INT C ROB VIS; NITZAN D, 1977, P IEEE, V65; PAGE CJ, 1980, 1ST P INT C SYST ENG; PARR EA, 1984, LOGIC DESIGNERS GUID, P170; RIOUX M, 1984, APPL OPT, V23; SATO K, 1985, J ROBOTIC SYST, V2; Van Trees H., 2013, DETECTION ESTIMATION; WILLIAMS CC, 1986, J APPL PHYS, V60; WITKIN AP, 1981, ARTIF INTELL, V17, P17, DOI 10.1016/0004-3702(81)90019-9; WITKIN AP, 1983, 7TH P INT JOINT C AR, P1019; 1979, ENCY PRACTICAL PHOTO, P2193	17	2	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	1990	12	4					409	416		10.1109/34.50627	http://dx.doi.org/10.1109/34.50627			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CV929					2022-12-18	WOS:A1990CV92900008
J	CONSTANT, P; MATWIN, S; OPPACHER, F				CONSTANT, P; MATWIN, S; OPPACHER, F			LEW - LEARNING BY WATCHING	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									CARLETON UNIV,SCH COMP SCI,OTTAWA K1S 5B6,ONTARIO,CANADA	Carleton University	CONSTANT, P (corresponding author), UNIV OTTAWA,DEPT COMP SCI,OTTAWA K1N 9B4,ONTARIO,CANADA.			Matwin, Stan/0000-0001-6629-8434				AKAMA K, 1979, 6TH P INT JOINT C AR, P4; BOOSE JH, 1986, P KNOWLEDGE ACQUISIT, P241; BOOSE JH, 1986, KNOWLEDGE ACQUISITIO; BUNDY A, 1985, ARTIF INTELL, V27, P137, DOI 10.1016/0004-3702(85)90052-9; CONSTANT P, 1987, 3RD P C AI APPL ORL, P29; CONSTANT P, 1988, 8TH P C CAN SOC COMP, P242; Dejong G., 1986, Machine Learning, V1, P145, DOI 10.1007/BF00114116; DEJONG GF, 1985, TR64 U ILL URB CHAMP; DIETTERICH TG, 1983, MACHINE LEARNING, V1; Fisher D. H., 1987, Machine Learning, V2, P139, DOI 10.1007/BF00114265; KAHN G, 1985, IEEE T PATTERN ANAL, V7, P511, DOI 10.1109/TPAMI.1985.4767699; Klahr D., 1987, PRODUCTION SYSTEM MO; KOLODNER JL, 1984, RETRIEVAL ORG STRATE; LANGLEY P, 1986, TR8612 U CAL TECH RE; Lebowitz M., 1987, Machine Learning, V2, P103, DOI 10.1023/A:1022800624210; LEBOWITZ M, 1985, 9TH P INT JOINT C AR, P858; MATWIN S, 1989, COMPUT INTELL; Michalski R, 1983, MACH LEARN, P183; MICHALSKI RS, 1988, CONSTRUCTIVE CLOSED; MICHALSKI RS, 1986, MACHINE LEARNING, V2; MICHALSKI RS, 1980, INT J POLICY ANAL IN, V4; MITCHELL T, 1987, MACHINE LEARNING GUI; Mitchell T. M., 1986, Machine Learning, V1, P47, DOI 10.1023/A:1022691120807; Mitchell T. M., 1983, MACHINE LEARNING ART, P163; POLITAKIS P, 1985, EMPIRICAL ANAL EXPER; QUINLAN JR, 1986, MACHINE LEARNING, V2; Schank RC, 1982, DYNAMIC MEMORY; SIKLOSSY L, 1972, REPRESENTATION MEANI; VERE SA, 1977, 5TH P INT JOINT C AR, P349; Winston P. H., 1984, ARTIFICIAL INTELLIGE; Winston P. H., 1986, MACHINE LEARNING ART, V2, P45; WOLFF JG, 1982, LANG COMMUN, V2, P57, DOI 10.1016/0271-5309(82)90035-0; [No title captured]	33	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	1990	12	3					294	308		10.1109/34.49054	http://dx.doi.org/10.1109/34.49054			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CP943					2022-12-18	WOS:A1990CP94300005
J	GOUTSIAS, J; MENDEL, JM				GOUTSIAS, J; MENDEL, JM			SIMULTANEOUS OPTIMAL SEGMENTATION AND MODEL ESTIMATION OF NONSTATIONARY NOISY IMAGES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									UNIV SO CALIF, INST SIGNAL & IMAGE PROC, DEPT ELECT ENGN SYST, LOS ANGELES, CA 90089 USA	University of Southern California	GOUTSIAS, J (corresponding author), JOHNS HOPKINS UNIV, DEPT ELECT & COMP ENGN, IMAGE ANAL & COMMUN LAB, BALTIMORE, MD 21218 USA.		Goutsias, John/A-3274-2010					BESAG J, 1974, J ROY STAT SOC B MET, V36, P192; CHELLAPPA R, 1985, IEEE T ACOUST SPEECH, V33, P194, DOI 10.1109/TASSP.1985.1164507; DERIN H, 1984, IEEE T PATTERN ANAL, V6, P707, DOI 10.1109/TPAMI.1984.4767595; DERIN H, 1987, IEEE T PATTERN ANAL, V9, P39, DOI 10.1109/TPAMI.1987.4767871; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GOUTSIAS J, 1988, IEEE T INFORM THEORY, V34, P551, DOI 10.1109/18.6036; GOUTSIAS J, 1987, JHUECE8708 J HOPK U; GOUTSIAS J, 1986, USCSPIRI102 U SO CAL; HUNT BR, 1980, COMPUT VISION GRAPH, V12, P173, DOI 10.1016/0146-664X(80)90010-6; HUNT BR, 1976, IEEE T SYST MAN CYB, V6, P876; JAIN AK, 1981, P IEEE, V69, P502, DOI 10.1109/PROC.1981.12021; JIANG SS, 1986, USCSIPI100 U SO CAL; KASHYAP RL, 1983, IEEE T INFORM THEORY, V29, P60, DOI 10.1109/TIT.1983.1056610; Kindermann R., 1980, CONTEMP MATH, V1; KUAN DT, 1985, IEEE T PATTERN ANAL, V7, P165, DOI 10.1109/TPAMI.1985.4767641; LEE JS, 1981, COMPUT VISION GRAPH, V15, P380, DOI 10.1016/S0146-664X(81)80018-4; Luenberger D.G, 2016, LINEAR NONLINEAR PRO, DOI 10.1007/978-3-319-18842-3; NAHI NE, 1975, IEEE T CIRCUITS SYST, VCA22, P286, DOI 10.1109/TCS.1975.1084029; Papoulis A., 2002, PROBABILITY RANDOM V; PEYROVIAN MJ, 1977, APPL OPTICS, V16, P3147, DOI 10.1364/AO.16.003147	20	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1989	11	9					990	998		10.1109/34.35503	http://dx.doi.org/10.1109/34.35503			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AM008					2022-12-18	WOS:A1989AM00800010
J	TANIMOTO, SL				TANIMOTO, SL			INTRODUCTION TO THE PAPERS ON MULTIRESOLUTION REPRESENTATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material																			0	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1989	11	7					673	673						1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AB815					2022-12-18	WOS:A1989AB81500001
J	JEONG, DS; LAPSA, PM				JEONG, DS; LAPSA, PM			UNIFIED APPROACH FOR EARLY-PHASE IMAGE UNDERSTANDING USING A GENERAL DECISION CRITERION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									PENN STATE UNIV,DEPT ELECT ENGN,UNIVERSITY PK,PA 16802	Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park	JEONG, DS (corresponding author), INHA UNIV,DEPT ELECT ENGN,INCHON 402751,SOUTH KOREA.							BESAG J, 1974, J ROY STAT SOC B MET, V36, P192; BEVINGTON JE, 1984, P C ASSP; BOLLE RM, 1984, IEEE T PATTERN ANAL, V6; Brodatz P., 1966, TEXTURES PHOTOGRAPHI; CHELLAPPA R, 1985, IEEE T ACOUST SPEECH, V33, P194, DOI 10.1109/TASSP.1985.1164507; CHELLAPPA R, 1982, JUN P C PRIP, P77; CHELLAPPA R, 1981, P IEEE CS C PATT REC, P577; CHEN PC, 1980, COMPUT VISION GRAPH, V12, P153, DOI 10.1016/0146-664X(80)90009-X; FU KS, 1981, PATTERN RECOGN, V13, P3, DOI 10.1016/0031-3203(81)90028-5; HARALICK RM, 1980, COMPUT VISION GRAPH, V12, P60, DOI 10.1016/0146-664X(80)90004-0; HARALICK RM, 1981, COMPUT VISION GRAPH, V15, P113, DOI 10.1016/0146-664X(81)90073-3; HOROWITZ SL, 1976, J ACM, V23, P368, DOI 10.1145/321941.321956; JEONG D, 1985, THESIS VIRGINIA POLY; KAHSYAP RL, 1980, 5TH P INT C PATT REC; KASHYAP RL, 1981, COMPUT VISION GRAPH, V15, P301, DOI 10.1016/S0146-664X(81)80014-7; KASHYAP RL, 1983, IEEE T INFORM THEORY, V29, P60, DOI 10.1109/TIT.1983.1056610; KASHYAP RL, 1984, IEEE T PATTERN ANAL, V6, P800, DOI 10.1109/TPAMI.1984.4767604; KASHYAP RL, 1982, IEEE T PATTERN ANAL, V4, P99, DOI 10.1109/TPAMI.1982.4767213; KASHYAP RL, 1983, IMAGE VISION COMPUT, V1, P189; LARIMORE WE, 1977, P IEEE, V65, P961, DOI 10.1109/PROC.1977.10593; MILGRAM DL, 1979, COMPUT VISION GRAPH, V11, P1, DOI 10.1016/0146-664X(79)90073-X; ROSENFELD A, 1979, P IEEE, V67, P764, DOI 10.1109/PROC.1979.11326; ROSENFELD A, 1984, PATTERN RECOGN, V17, P3, DOI 10.1016/0031-3203(84)90031-1; ROSENFELD A, 1985, COMPUT GRAPHICS IMAG, V17, P189; THERRIEN CW, 1983, COMPUT VISION GRAPH, V22, P313, DOI 10.1016/0734-189X(83)90079-8; THERRIEN CW, 1986, P IEEE, V74; WHITTLE P, 1954, BIOMETRIKA, V41, P434	27	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	1989	11	4					357	371		10.1109/34.19033	http://dx.doi.org/10.1109/34.19033			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	T9100					2022-12-18	WOS:A1989T910000002
J	DYER, CR				DYER, CR			INTRODUCTION TO THE SPECIAL SECTION ON COMPUTER ARCHITECTURES AND PARALLEL ALGORITHMS FOR PAMI	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material											DYER, CR (corresponding author), UNIV WISCONSIN,COMP SCI,MADISON,WI 53706, USA.							Weems C., 1988, Proceedings CVPR '88: The Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.88CH2605-4), P673, DOI 10.1109/CVPR.1988.196308; 1987, P WORKSHOP COMPUTER	2	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	1989	11	3					225	226						2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	T3840					2022-12-18	WOS:A1989T384000001
J	DRAKE, KC; MCVEY, ES; INIGO, RM				DRAKE, KC; MCVEY, ES; INIGO, RM			SENSOR ROLL ANGLE ERROR FOR A MOBILE ROBOT USING A NAVIGATION LINE	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									UNIV VIRGINIA,SCH ENGN & APPL SCI,CHARLOTTESVILLE,VA 22901	University of Virginia	DRAKE, KC (corresponding author), USAF,FLIGHT DYNAM,WRIGHT PATTERSON AFB,OH 45433, USA.							DRAKE KC, 1985, IEEE T PATTERN ANAL, V7; DRAKE KC, 1986, IN PRESS IEEE J  JUL; DUDA RO, 1973, PATTERN CLASSIFICATI, P386; MCVEY ES, 1986, IEEE T PATTERN ANAL, V8; SCHMIDT RA, 1971, THESIS STANFORD U ST	5	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1988	10	5					727	731		10.1109/34.6783	http://dx.doi.org/10.1109/34.6783			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	Q4255					2022-12-18	WOS:A1988Q425500014
J	MCFEE, JE; DAS, Y				MCFEE, JE; DAS, Y			A CLASSIFIER FOR FEATURE VECTORS WHOSE PROTOTYPES ARE A FUNCTION OF MULTIPLE CONTINUOUS PARAMETERS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											MCFEE, JE (corresponding author), DEF RES ESTAB,ORDNANCE DETECT GRP,RALSTON AB,SUFFIELD T0J 2N0,ALBERTA,CANADA.							Arfken G, 1970, MATH METHODS PHYS; CHESNEY RH, 1984, IEEE T PATTERN ANAL, V19, P809; DAS Y, 1981, 283 DEF RES EST SUFF; DUIN RPW, 1976, IEEE T COMPUT, V25, P1175, DOI 10.1109/TC.1976.1674577; KOVALEVSKY VA, 1968, CHARACTER READERS PA, P38; MCFEE JE, 1985, J PHYS E SCI INSTRUM, V18, P54, DOI 10.1088/0022-3735/18/1/015; MCFEE JE, 1986, IEEE T GEOSCI REMOTE, V24; RICE JR, 1966, MATH COMPUT, V29, P325; SAMMON JW, 1970, IEEE T COMPUT, VC 19, P594, DOI 10.1109/T-C.1970.222993; SHANMUGAM KS, 1977, PATTERN RECOGN, V9, P167, DOI 10.1016/0031-3203(77)90014-0; SHLESINGER MI, 1968, CHARACTER READERS PA, P53; Stratton JA., 1941, ELECTROMAGNETIC THEO; Tou JT, 1974, PATTERN RECOGN; [No title captured]	14	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1988	10	4					599	606		10.1109/34.3922	http://dx.doi.org/10.1109/34.3922			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	P1493					2022-12-18	WOS:A1988P149300015
J	GOTOH, T; TORIU, T; SASAKI, S; YOSHIDA, M				GOTOH, T; TORIU, T; SASAKI, S; YOSHIDA, M			A FLEXIBLE VISION-BASED ALGORITHM FOR A BOOK SORTING SYSTEM	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											GOTOH, T (corresponding author), FUJITSU LABS LTD,PATTERN INFORMAT PROC LAB,1015 KAMIKODANAKA,NAKAHARA KU,KAWASAKI,KANAGAWA 211,JAPAN.		Gotoh, Toshiyuki/B-8537-2015	Gotoh, Toshiyuki/0000-0003-2209-854X				ARGENTIERO P, 1982, IEEE T PATTERN ANAL, V4, P51, DOI 10.1109/TPAMI.1982.4767195; BAIRD ML, 1978, IEEE T SYST MAN CYB, V8, P133; Duda R.O., 1973, J ROYAL STAT SOC SER; Gotoh T., 1984, Seventh International Conference on Pattern Recognition (Cat. No. 84CH2046-1), P1098; Hai T., 1985, Transactions of the Institute of Electronics and Communication Engineers of Japan, Part D, VJ68D, P773; HARADA H, 1985, P IEEE COMPUTER SOC, P146; Iwase H., 1986, Second International Conference on Image Processing and its Applications (Conf. Publ. No.265), P25; Kulkarni A. V., 1976, 3rd International Joint Conference on Pattern Recognition, P459; OZAKI T, 1986, P SPIE CAMBRIDGE S A, P730; Tanimoto S., 1975, COMPUTER GRAPHICS IM, V4, P104; Toriu T., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P222; TOU JT, 1969, METHODOLOGIES PATTER	12	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	1988	10	3					393	399		10.1109/34.3903	http://dx.doi.org/10.1109/34.3903			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	N5337					2022-12-18	WOS:A1988N533700010
J	XIE, SE; CALVERT, TW				XIE, SE; CALVERT, TW			CSG-EESI - A NEW SOLID REPRESENTATION SCHEME AND A CONVERSION EXPERT SYSTEM	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									SIMON FRASER UNIV,SCH COMP SCI,COMP & COMMUN RES LAB,BURNABY V5A 1S6,BC,CANADA	Simon Fraser University								BHANU B, 1984, IEEE T PATTERN ANAL, V6, P340, DOI 10.1109/TPAMI.1984.4767527; BINFORD TO, 1971, P IEEE C SYST CONT M; BROU P, 1984, INT J ROBOT RES, V3, P89, DOI 10.1177/027836498400300406; CHANDRASEKARAN B, 1979, P 6 INT JOINT C ART, P134; HENDERSON TC, 1983, IEEE T PATTERN ANAL, V5, P609, DOI 10.1109/TPAMI.1983.4767450; HORN BKP, 1984, P IEEE, V72, P1671, DOI 10.1109/PROC.1984.13073; IKEUCHI K, 1981, AUG P IJCAI 81 VANC, P595; Little JJ, 1983, AUG P NAT C ART INT, P247; NAU DS, 1982, TR1201 U MAR TECH RE; PERKINS WA, 1977, 5TH P INT JOINT C AR, P678; POGORELOV AV, 1973, T MATH MONOGRAPHS, V35, P133; POGORELOV AV, 1978, MINKOWSKY MULTIDIMEN; Preiss K., 1981, Computers in Industry, V2, P133, DOI 10.1016/0166-3615(81)90006-3; Requicha A. G., 1980, ACM COMPUT SURV, P437; REQUICHA AAG, 1983, IEEE COMPUT GRAPH, V3, P25, DOI 10.1109/MCG.1983.263271; REQUICHA AAG, 1983, INT J ROBOT RES, V2, P45, DOI 10.1177/027836498300200403; REQUICHA AAG, 1986, IEEE T ROBOTIC AUTOM, V2, P156, DOI 10.1109/JRA.1986.1087053; REQUICHA AAG, 1980, LECTURE NOTES COMPUT, V89, P2; ROACH JW, 1979, IEEE T PATTERN ANAL, V1, P127, DOI 10.1109/TPAMI.1979.4766898; SAKURAI H, 1983, ACM COMPUT GRAPHICS, V17, P243; SHAPIRO L, 1980, P AM ASS ARTIF INTEL, P28; SHAPIRO LG, 1981, 1981 P IEEE PATT REC, P534; SMITH DA, 1979, MIT530 AI MEM; UNDERWOOD SA, 1975, IEEE T COMPUT, VC 24, P651, DOI 10.1109/T-C.1975.224277; VOSSLER DL, 1985, IEEE COMPUT GRAPH, V5, P61, DOI 10.1109/MCG.1985.276215; WESLEY MA, 1984, SOLID MODELING COMPU, P23; XIE S, 1986, 8TH P INT C PATT REC; XIE S, 1986, SPIE P, V726; XIE S, 1986, MAY P GRAPH VIS INT, P300	29	2	2	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	1988	10	2					221	234		10.1109/34.3884	http://dx.doi.org/10.1109/34.3884			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	M2974					2022-12-18	WOS:A1988M297400006
J	KATSINIS, C; POULARIKAS, AD				KATSINIS, C; POULARIKAS, AD			ANALYSIS OF A SAMPLING TECHNIQUE APPLIED TO BIOLOGICAL IMAGES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note											KATSINIS, C (corresponding author), UNIV ALABAMA,DEPT ELECT & COMP ENGN,HUNTSVILLE,AL 35899, USA.							HEIDEMAN GHL, 1983, SIGNAL PROCESSING, V2; JEFFRIES HP, 1980, 2ND P INT WORKSH IN; JEFFRIES HP, 1984, MARINE BIOL, V78; JEFFRIES HP, 1980, ESTUARINE PERSPECTIV; KATSINIS C, 1986, 18TH P SE S SYST THE; KATSINIS C, 1982, MULTICOMPUTER SYSTEM; KATSINIS C, 1984, 28TH P SPIE ANN INT; KATSINIS C, 1985, 2ND P INT C OPT EL S; KATSINIS C, 1985, 29TH P SPIE ANN INT	9	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	1987	9	6					832	835		10.1109/TPAMI.1987.4767989	http://dx.doi.org/10.1109/TPAMI.1987.4767989			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	K6735	21869444				2022-12-18	WOS:A1987K673500010
J	DON, HS; FU, KS				DON, HS; FU, KS			A PARALLEL ALGORITHM FOR STOCHASTIC IMAGE SEGMENTATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									PURDUE UNIV,SCH ELECT ENGN,W LAFAYETTE,IN 47907	Purdue University System; Purdue University; Purdue University West Lafayette Campus	DON, HS (corresponding author), SUNY STONY BROOK,DEPT ELECT ENGN,STONY BROOK,NY 11794, USA.							BOOTH TL, 1969, 10TH IEEE ANN S SWIT; CHIANG YT, 1984, IEEE T PATTERN ANAL, V6; DON HS, 1985, PATTER RECOGNITION, V18; FU KS, 1973, IEEE T COMPUT, VC 22, P1087, DOI 10.1109/T-C.1973.223654; FU KS, 1973, IEEE T COMPUT, V22; FU KS, 1982, SYNTACTIC PATTERN RE; HARALICK RM, 1983, COMPUT VISION GRAPH, V22, P388, DOI 10.1016/0734-189X(83)90083-X; Harris T. E., 1963, THEORY BRANCHING PRO; LU SY, 1978, COMPUT VISION GRAPH, V7, P303, DOI 10.1016/S0146-664X(78)80001-X; LU SY, 1979, COMPUT VISION GRAPH, V9, P234, DOI 10.1016/0146-664X(79)90039-X; MOAYER B, 1976, IEEE T COMPUT, V25; PELEG S, 1980, IEEE T PATTERN ANAL, V2; PERKINS WA, 1973, J COMPUT GRAPHICS IM, V2, P355; Rosenfeld A, 1976, IEEE T SYST MAN CYBE, VSMC-6	14	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1986	8	5					594	603		10.1109/TPAMI.1986.4767834	http://dx.doi.org/10.1109/TPAMI.1986.4767834			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	D7584	21869358				2022-12-18	WOS:A1986D758400002
J	FLICK, TE; JONES, LK				FLICK, TE; JONES, LK			A COMBINATORIAL APPROACH FOR CLASSIFICATION OF PATTERNS WITH MISSING INFORMATION AND RANDOM ORIENTATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									CATHOLIC UNIV AMER,DEPT MATH,WASHINGTON,DC 20064	Catholic University of America	FLICK, TE (corresponding author), USN,RES LAB,WASHINGTON,DC 20375, USA.							Aho A.V., 1972, THEORY PARSING TRANS; BUNKE H, 1983, PATTERN RECOGNIT MAY, P245; BURR D, 1980, 5TH P INT C PATT REC, P223; CLARK CS, 1980, 5TH P INT C PATT REC, P217; DUERR B, 1980, PATTERN RECOGN, V12, P189, DOI 10.1016/0031-3203(80)90043-6; Fu K.S., 1974, MATH SCI ENG; FU KS, 1983, IEEE T PATTERN ANAL, V5, P200; FUKUNAGA K, 1972, INTRO STATISTICAL PA; Hestenes M., 1975, OPTIMIZATION THEORY; KNUTH DE, 1981, ART COMPUTER PROGRAM, V2, P480; LAVINE D, 1983, PATTERN RECOGN, V16, P289, DOI 10.1016/0031-3203(83)90034-1; NARENDRA PM, 1981, AUG P IEEE COMP SOC, P481; Olshen R., 1984, CLASSIFICATION REGRE; SHAPIRO LG, 1981, IEEE T PATTERN ANAL, V3, P504, DOI 10.1109/TPAMI.1981.4767144; TSAI WH, 1983, IEEE T SYST MAN CYB, V13, P48, DOI 10.1109/TSMC.1983.6313029; WIDROW B, 1973, PATTERN RECOGN, V5, P175, DOI 10.1016/0031-3203(73)90042-3	16	2	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1986	8	4					482	490		10.1109/TPAMI.1986.4767812	http://dx.doi.org/10.1109/TPAMI.1986.4767812			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	C7400					2022-12-18	WOS:A1986C740000007
J	VANGRONINGEN, WDH; VERMEIJ, GF; BOSMAN, D; TRAAS, CR				VANGRONINGEN, WDH; VERMEIJ, GF; BOSMAN, D; TRAAS, CR			GEOMETRIC RECONSTRUCTION OF BURIED HEAT-SOURCES FROM A SURFACE THERMOGRAM	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									TWENTE UNIV TECHNOL,DEPT ELECT ENGN,MEASUREMENT SCI & INSTRUMENTAT GRP,ENSCHEDE,NETHERLANDS; TWENTE UNIV TECHNOL,DEPT MATH,APPL ANAL GRP,ENSCHEDE,NETHERLANDS	University of Twente; University of Twente								Andrews H.C., 1977, DIGITAL IMAGE RESTOR; BECK JV, 1970, INT J HEAT MASS TRAN, V13, P703, DOI 10.1016/0017-9310(70)90044-X; Carlslaw H.S., 1959, CONDUCTION HEAT SOLI; CHEN MM, 1977, T ASME           MAY, P58; DRAPER JW, 1971, PHYS MED BIOL, V16, P201, DOI 10.1088/0031-9155/16/2/301; GROBER H, 1963, GRUNDSETZE WARME UBE; HAMERLY JR, 1981, J OPT SOC AM, V71, P448, DOI 10.1364/JOSA.71.000448; Pratt W. K., 1978, DIGITAL IMAGE PROCES; VANGRONINGEN WDH, TM80013 TWENT U TECH; VERMEIJ GF, 1979, J MED ENG TECHNOL, V3; VERMEY GF, 1975, PHYS MED BIOL, V20, P384, DOI 10.1088/0031-9155/20/3/003; WEINBERGER HF, 1965, 1ST COURSE PARTIAL D	12	2	2	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1985	7	5					610	616		10.1109/TPAMI.1985.4767708	http://dx.doi.org/10.1109/TPAMI.1985.4767708			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AQH94	21869298				2022-12-18	WOS:A1985AQH9400012
J	PAU, LF				PAU, LF			GAME THEORETICAL PATTERN-RECOGNITION - APPLICATION TO IMPERFECT NONCOOPERATIVE LEARNING AND TO MULTICLASS CLASSIFICATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note											PAU, LF (corresponding author), BATTELLE MEM INST,7 ROUTE DE DRIZE,CH-1227 CAROUGE,SWITZERLAND.		Pau, L.F./A-1262-2010	Pau, L.F./0000-0003-1503-1486				Case, 1979, EC COMPETITIVE PROCE; Dresher M, 1961, GAMES STRATEGY THEOR; Fu K. S., 1968, SEQUENTIAL METHODS P, V240, P241; FUKUNAGA K, 1972, INTRO STATISTICAL PA; LEMKE CE, 1971, EQUILIBRIUM POINTS B; Owen G, 1968, GAME THEORY; PARTHASARATHY, 1971, SOME TOPICS TWO PERS; PAU LF, 1975, CNRS A011446 REP; WILSON R, 1972, MANAGE SCI, V18, P448, DOI 10.1287/mnsc.18.7.448; Young T. Y., 1974, CLASSIFICATION ESTIM; [No title captured]	11	2	2	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1984	6	1					118	122		10.1109/TPAMI.1984.4767486	http://dx.doi.org/10.1109/TPAMI.1984.4767486			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	SB213	21869176				2022-12-18	WOS:A1984SB21300017
J	SCLOVE, SL				SCLOVE, SL			APPLICATION OF THE CONDITIONAL POPULATION-MIXTURE MODEL TO IMAGE SEGMENTATION - REPLY	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note											SCLOVE, SL (corresponding author), UNIV ILLINOIS,DEPT QUANTITAT METHODS,CHICAGO,IL 60680, USA.							SCLOVE SL, 1977, COMMUN STAT A-THEOR, V6, P417, DOI 10.1080/03610927708827502; SCLOVE SL, UNPUB P ARMY RES OFF	2	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1984	6	5					657	658						2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TM813					2022-12-18	WOS:A1984TM81300014
J	THERRIEN, CW; FUKUNAGA, K				THERRIEN, CW; FUKUNAGA, K			PROPERTIES OF SEPARABLE COVARIANCE MATRICES AND THEIR ASSOCIATED GAUSSIAN RANDOM-PROCESSES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note									PURDUE UNIV,SCH ELECT ENGN,W LAFAYETTE,IN 47907	Purdue University System; Purdue University; Purdue University West Lafayette Campus	THERRIEN, CW (corresponding author), MIT,LINCOLN LAB,LEXINGTON,MA 02173, USA.							BURG JP, 1972, GEOPHYSICS, V37, P375, DOI 10.1190/1.1440265; BURG JP, 1967, 37TH ANN INT SEG M O; CAPON J, 1969, P IEEE, V57, P1408, DOI 10.1109/PROC.1969.7278; FISHMAN PM, 1983, IEEE T INFORM THEORY, V29, P245, DOI 10.1109/TIT.1983.1056640; FISHMAN PM, 1981, MIT DTIC A103062 LIN; FUKUNAGA K, 1972, INTRO STATISTICAL PA; KULLBACK S, 1959, INFORMATION THEORY S, P189; PEARSON C, 1983, HDB APPLIED MATH; THERRIEN CW, 1983, P IEEE, V71, P1459, DOI 10.1109/PROC.1983.12800; [No title captured]	10	2	2	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1984	6	5					652	656		10.1109/TPAMI.1984.4767580	http://dx.doi.org/10.1109/TPAMI.1984.4767580			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TM813	21869235				2022-12-18	WOS:A1984TM81300012
J	DELLARICCIA, G; SHAPIRO, A				DELLARICCIA, G; SHAPIRO, A			FISHER DISCRIMINANT-ANALYSIS AND FACTOR-ANALYSIS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									BEN GURION UNIV NEGEV,IL-84120 BEERSHEBA,ISRAEL	Ben Gurion University								ANDERSON TW, 1956, 3RD P BERK S MATH ST, V5, P111; CHIEN YT, 1967, IEEE T INFORMATION T, P518; DELLARICCIA G, 1980, IEEE T PATTERN ANAL, V2, P60; Duda R.O., 1973, J ROYAL STAT SOC SER; FOLEY DH, 1976, IEEE T COMPUT C, V24, P281; FUKUNAGA K, 1970, IEEE T COMPUT, VC 19, P311, DOI 10.1109/T-C.1970.222918; Harman H.H., 1976, MODERN FACTOR ANAL; MORRISON DF, 1976, MULTIVARIATE STATIST; WATANABE S, 1967, 4TH T PRAG C	9	2	3	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1983	5	1					99	104		10.1109/TPAMI.1983.4767352	http://dx.doi.org/10.1109/TPAMI.1983.4767352			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PZ844	21869091				2022-12-18	WOS:A1983PZ84400015
J	AYALA, IL; ORTON, DA; LARSON, JB; ELLIOTT, DF				AYALA, IL; ORTON, DA; LARSON, JB; ELLIOTT, DF			MOVING TARGET TRACKING USING SYMBOLIC REGISTRATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									ROCKWELL INT CORP,ANAHEIM,CA 92803	Rockwell Collins								AGGARWAL JK, 1975, IEEE T COMPUT, V24, P966, DOI 10.1109/T-C.1975.224102; Anuta PE., 1970, IEEE T GEOSCI ELECTR, V8, P353, DOI [10.1109/tge.1970.271435, DOI 10.1109/TGE.1970.271435]; AYALA IL, 1980, 14TH P ANN AS C CIRC; BOURGEOIS F, 1971, COMMUN ASS COMPUT MA, P802; CHOW WK, 1977, IEEE T COMPUT, V26, P179, DOI 10.1109/TC.1977.5009299; COLEMAN GB, 1979, P IEEE, V67, P773, DOI 10.1109/PROC.1979.11327; HOLBEN RD, 1980, 1980 P IEEE NAT AER, V1, P114; LARSON JB, 1976, 6TH P INT S MULT LOG; LARSON JB, 1977, THESIS U ILLINOIS UR; LO TK, 1979, MAY P DIG PROC AER I, V186; MICHALSKI RS, 1980, IEEE T PATTERN ANAL, V2, P349, DOI 10.1109/TPAMI.1980.4767034; MORAVEX HP, 1977, 1977 P IJCAI CAMBR, P584; MOSTAFAVI H, 1978, IEEE T AERO ELEC SYS, V14, P487, DOI 10.1109/TAES.1978.308610; MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003; OHLANDER R, 1978, COMPUT VISION GRAPH, V8, P313, DOI 10.1016/0146-664X(78)90060-6; ORTON DA, 1979, NOV P OP SESS WORKSH, P175; PRICE K, 1979, IEEE T PATTERN ANAL, V1, P110, DOI 10.1109/TPAMI.1979.4766884; RATKOVIC JA, 1979, HYBRID CORRELATION A, P648; ROUNDS EM, 1979, P SPIE C, V205, P143; SILVER R, 1960, COMMUN ACM, V3, P605, DOI 10.1145/367436.367476; TENEBAUM JN, 1974, 87 STANF RES I ART I; THOMPSON WB, 1980, IEEE T PATTERN ANAL, V2, P543, DOI 10.1109/TPAMI.1980.6447701; WILLIAMS TD, 1980, IEEE T PATTERN ANAL, V2, P511, DOI 10.1109/TPAMI.1980.6447697	23	2	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	5					515	520		10.1109/TPAMI.1982.4767296	http://dx.doi.org/10.1109/TPAMI.1982.4767296			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PG894	21869071				2022-12-18	WOS:A1982PG89400008
J	BHATTACHARYA, BK; TOUSSAINT, GT				BHATTACHARYA, BK; TOUSSAINT, GT			A COUNTEREXAMPLE TO A DIAMETER ALGORITHM FOR CONVEX POLYGONS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											BHATTACHARYA, BK (corresponding author), MCGILL UNIV,SCH COMP SCI,MONTREAL H3C 3G1,QUEBEC,CANADA.							AVIS D, UNPUB COMPUT MATH AP; Dobkin D. P., 1979, 20th Annual Symposium of Foundations of Computer Science, P9, DOI 10.1109/SFCS.1979.28; FISCHLER MA, 1980, PATTERN RECOGN, V12, P35, DOI 10.1016/0031-3203(80)90052-7; JACOBSEN J, 1964, 17TH P ANN C ENG MED, P117; MCCALLUM D, 1979, INFORM PROCESS LETT, V9, P201, DOI 10.1016/0020-0190(79)90069-3; Shamos MI, 1975, P 7 ANN ACM S THEOR, P224; TOUSSAINT GT, 1980, 5TH P INT C PATT REC, P1324	8	2	2	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	3					306	309		10.1109/TPAMI.1982.4767248	http://dx.doi.org/10.1109/TPAMI.1982.4767248			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NN069	21869038				2022-12-18	WOS:A1982NN06900009
J	BOUCHER, RE; NOONAN, JP				BOUCHER, RE; NOONAN, JP			ADAPTIVE DETECTION AND REMOVAL OF NON-GAUSSIAN SPIKES FROM GAUSSIAN DATA	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note											BOUCHER, RE (corresponding author), BEDFORD RES ASSOCIATES,BEDFORD,MA 01730, USA.							DAVIES DL, 1979, IEEE T PATTERN ANAL, V1, P224, DOI 10.1109/TPAMI.1979.4766909; DAY NE, 1969, BIOMETRIKA, V56, P463, DOI 10.1093/biomet/56.3.463; DUDA RO, 1973, PATTERN CLASSIFICATI, P190; FU KS, 1970, ADAPTIVE LEARNING PA, P68; HASSELBLAD V, 1966, TECHNOMETRICS, V8, P431, DOI 10.2307/1266689; PATRICK EA, 1966, IEEE T INFORM THEORY, V12, P362, DOI 10.1109/TIT.1966.1053901; VanTrees H. L, 1968, DETECTION ESTIMATI 1, P74	7	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	2					132	136		10.1109/TPAMI.1982.4767218	http://dx.doi.org/10.1109/TPAMI.1982.4767218			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NE957	21869017				2022-12-18	WOS:A1982NE95700007
J	CHEN, CH				CHEN, CH			ADAPTIVE AND LEARNING ALGORITHMS FOR SEISMIC DETECTION OF PERSONNEL	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note											CHEN, CH (corresponding author), SOUTHEASTERN MASSACHUSETTS UNIV,DEPT ELECT ENGN,N DARTMOUTH,MA 02747, USA.							AHMED N, 1977, DEC P DIG SIGN PROC, P326; CHEN CH, 1980, DACW3980M0944 ARM WA; DONOHOE GW, 1977, DEC P DIG SIGN PROC, P388; ELLIOTT GR, 1981, 2ND P INT S COMP AID, P92; MEHRA RK, 1969, P IEEE S ADAPTIVE PR; PAU LF, 1979, IEEE T CIRCUITS SYST, V26, P565, DOI 10.1109/TCS.1979.1084674; WIDROW B, 1975, P IEEE, V63, P1692, DOI 10.1109/PROC.1975.10036	7	2	2	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	2					129	132		10.1109/TPAMI.1982.4767217	http://dx.doi.org/10.1109/TPAMI.1982.4767217			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	NE957	21869016				2022-12-18	WOS:A1982NE95700006
J	GRISWOLD, NC; SAYOOD, K				GRISWOLD, NC; SAYOOD, K			UNSUPERVISED LEARNING APPROACH TO ADAPTIVE DIFFERENTIAL PULSE CODE MODULATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											GRISWOLD, NC (corresponding author), TEXAS A&M UNIV,DEPT ELECT ENGN,COLLEGE STN,TX 77843, USA.							ADAMS WC, 1978, IEEE T COMMUN, V26, P1295, DOI 10.1109/TCOM.1978.1094224; AHMED N, 1975, ORTHOGONAL TRANSFORM, P169; CASTELLINO P, 1974, 1974 P INT ZUR SEM D, pB6; COHN DL, 1974, 8TH C REC AS C CIRC; CUMMISKEY P, 1973, BELL SYST TECH J, V52, P1105, DOI 10.1002/j.1538-7305.1973.tb02007.x; Duda R.O., 1973, J ROYAL STAT SOC SER; ESSMAN JE, 1967, AUG P SOC PHOT OPT I, V67; GIBSON JD, 1980, P IEEE, V68, P488, DOI 10.1109/PROC.1980.11676; GOLDING LS, 1967, PR INST ELECTR ELECT, V55, P293, DOI 10.1109/PROC.1967.5485; Gonzalez R.C., 1977, DIGITAL IMAGE PROCES; GOODMAN DJ, 1974, IEEE T COMMUN, VCO22, P1037, DOI 10.1109/TCOM.1974.1092334; HABIBI A, 1974, IEEE T COMMUN    MAY; HABIBI A, 1974, IEEE T COMMUN, V22; HUHNS MN, 1975, USC IPI600 U SO CAL; JAIN AK, 1977, STOCHASTIC IMAGE MOD; JAIN AK, 1977, AUG INT PICT COD S T; JAYANT NS, 1973, AT&T TECH J, V52, P1119, DOI 10.1002/j.1538-7305.1973.tb02008.x; MUSSMAN HG, 1974, P INT ZURICH SEMIN D, pC1; NETRAVALI AN, 1977, P IEEE, V65, P1177, DOI 10.1109/PROC.1977.10663; Patrick EA., 1972, FUNDAMENTALS PATTERN; PRATT WK, 1978, DIGITAL IMAGE PROCES, P134; PRATT WK, 1979, IMAGE TRANSMISSION T, P86; ROESE JA, 1977, IEEE T COMMUN    NOV; SCHLINK W, 1972, MAR P INT ZUR SEM IN; STROH RW, 1970, THESIS POLYTECHNIC I	25	2	2	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	4					380	391		10.1109/TPAMI.1982.4767269	http://dx.doi.org/10.1109/TPAMI.1982.4767269			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NT735	21869052				2022-12-18	WOS:A1982NT73500004
J	LEMONE, KA				LEMONE, KA			SIMILARITY MEASURES BETWEEN STRINGS EXTENDED TO SETS OF STRINGS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											LEMONE, KA (corresponding author), WORCESTER POLYTECH INST,DEPT COMP SCI,WORCESTER,MA 01609, USA.							Andrews H. C., 1972, INTRO MATH TECHNIQUE; BLACKWELL FW, 1974, 2ND P INT JOINT C PA, P534; Cover T. M., 1976, 3rd International Joint Conference on Pattern Recognition, P245; Diday E., 1976, Digital pattern recognition, P47; DIDAY E, 1974, 2 INT JOINT C PATT R, P534; FINDLER NV, 1979, IEEE T PATTERN ANAL, V1, P116, DOI 10.1109/TPAMI.1979.4766885; Fu K.S., 1974, MATH SCI ENG; FU KS, 1977, IEEE T SYST MAN CYB, V7, P734, DOI 10.1109/TSMC.1977.4309608; KALISKI M, 1979, 17TH IEEE C DEC CONT; LANGRIDGE D, 1962, FRONTIERS PATTERN RE, P347; LEMONE K, 1979, THESIS NE U BOSTON; PAVLIDIS T, 1980, IEEE T PATTERN ANAL, V2, P301, DOI 10.1109/TPAMI.1980.4767029; SHAPIRO LG, 1980, IEEE T PATTERN ANAL, V2, P111, DOI 10.1109/TPAMI.1980.4766989; SIMON JC, 1974, 2ND P INT JOINT C PA, P489	14	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	3					345	347		10.1109/TPAMI.1982.4767257	http://dx.doi.org/10.1109/TPAMI.1982.4767257			3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NN069	21869047				2022-12-18	WOS:A1982NN06900018
J	WOODS, RE; GONZALEZ, RC				WOODS, RE; GONZALEZ, RC			SAMPLING CONSIDERATIONS FOR MULTILEVEL CROSSING ANALYSIS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											WOODS, RE (corresponding author), UNIV TENNESSEE,DEPT ELECT ENGN,KNOXVILLE,TN 37919, USA.							BARBE A, 1976, IEEE T INFORM THEORY, V22, P96, DOI 10.1109/TIT.1976.1055506; BENDAT JS, 1958, PRINCIPLES APPLICATI; BLAKE IF, 1973, IEEE T INFORM THEORY, V19, P295, DOI 10.1109/TIT.1973.1055016; CRAMER H, 1965, ANN MATH STAT, V36, P1656, DOI 10.1214/aoms/1177699794; MALEVICH TL, 1969, THEOR PROBAB APPL+, V14, P287, DOI 10.1137/1114035; MCFADDEN JA, 1958, IRE T INFORM THEOR, V4, P14, DOI 10.1109/TIT.1958.1057438; MCFADDEN JA, 1956, IRE T INFORM THEOR, V2, P146, DOI 10.1109/TIT.1956.1056822; Mitchell R. J., 1978, Proceedings of the 1978 IEEE International Conference on Acoustics, Speech and Signal Processing, P218; Papoulis A., 1962, FOURIER INTEGRAL ITS; Rice SO, 1944, BELL SYST TECH J, V23, P282, DOI 10.1002/j.1538-7305.1944.tb00874.x; RICE SO, 1945, AT&T TECH J, V24, P46, DOI 10.1002/j.1538-7305.1945.tb00453.x; Suthasinekul S., 1976, 3rd International Joint Conference on Pattern Recognition, P524; SUTHASINEKUL S, 1976, THESIS U MISSOURI CO; YLVISAKER ND, 1965, ANN MATH STAT, V36, P1043, DOI 10.1214/aoms/1177700077	14	2	2	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	2					117	123		10.1109/TPAMI.1982.4767215	http://dx.doi.org/10.1109/TPAMI.1982.4767215			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NE957	21869014				2022-12-18	WOS:A1982NE95700004
J	YABLON, M; CHU, JT				YABLON, M; CHU, JT			APPROXIMATIONS OF BAYES AND MINIMAX RISKS AND THE LEAST FAVORABLE DISTRIBUTION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									POLYTECH INST NEW YORK,DIV MANAGEMENT,BROOKLYN,NY 11201	New York University; New York University Tandon School of Engineering	YABLON, M (corresponding author), CUNY,JOHN JAY COLL CRIMINAL JUSTICE,DEPT MATH,NEW YORK,NY 10019, USA.							Anderson T.W, 1958, INTRO MULTIVARIATE S; BLACKWELL D, 1958, THEORY GAMES STATIST; Chernoff H., 1959, ELEMENTARY DECISION; CHU JT, 1974, IEEE T COMPUT, VC 23, P194, DOI 10.1109/T-C.1974.223887; KULLBACK S, 1968, INFORMATION THEORY S; [No title captured]	7	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	1					35	40		10.1109/TPAMI.1982.4767192	http://dx.doi.org/10.1109/TPAMI.1982.4767192			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MY534	21869000				2022-12-18	WOS:A1982MY53400006
J	MULLIN, JK				MULLIN, JK			RELIABLE INDEXING USING UNRELIABLE RECOGNITION DEVICES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											MULLIN, JK (corresponding author), UNIV WESTERN ONTARIO, DEPT COMP SCI, LONDON N6A 3K7, ONTARIO, CANADA.							DAVIDSON L, 1962, COMMUN ACM, V5, P169, DOI 10.1145/366862.366913; FISHER EG, 1976, THESIS U MASSACHUSET; Lorge Irving, 1944, TEACHERS WORDBOOK 30; RAVIV J, 1967, IEEE T INFORM THEORY, V13, P536, DOI 10.1109/TIT.1967.1054060; SALTON G, 1974, 1974 P IFIP AMST	5	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1981	3	3					347	350		10.1109/TPAMI.1981.4767108	http://dx.doi.org/10.1109/TPAMI.1981.4767108			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MN969	21868956				2022-12-18	WOS:A1981MN96900012
J	TAMBURELLI, G				TAMBURELLI, G			SOME RESULTS IN THE PROCESSING OF THE HOLY SHROUD OF TURIN	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											TAMBURELLI, G (corresponding author), CTR STUD & LAB TELECOMMUN SPA,TORINO,ITALY.							ANDREWS HC, 1972, IEEE SPECTRUM, V9, P20, DOI 10.1109/MSPEC.1972.5218964; FREI W, 1977, COMPUT VISION GRAPH, V6, P286, DOI 10.1016/S0146-664X(77)80030-0; GARIBOTTO G, 1977, JUN P INT COMM C, P202; GARIBOTTO G, 1977, CSELT RAPPORTI TECNI, P47; GARIBOTTO G, 1980, DIGITAL SIGNAL PROCE, P47; GARIBOTTO G, UNPUBLISHED; GERMAN JJ, 1977, MAR P US C SHROUD TU, P234; HALL EL, 1974, IEEE T COMPUT, VC 23, P207, DOI 10.1109/T-C.1974.223892; JACKSON JP, 1977, MAR P US C RES SHROU, P74; JANNEY DH, 1977, MAR P US C SHROUD TU, P146; JUMPER E, 1977, MAR P US C SHROUD TU, P197; LAMBARELLI L, 1979, ELECTRON LETT, V15, P24; LORRE JJ, 1977, MAR P US C SHROUD TU, P154; MARIA GA, 1974, IEEE T ACOUST SPEECH, VAS22, P15, DOI 10.1109/TASSP.1974.1162535; MCCOWN TM, 1977, MAR P US C SHROUD TU, P95; MERSEREAU RM, 1975, P IEEE, V63, P610, DOI 10.1109/PROC.1975.9795; RABINER LR, 1975, IEEE T ACOUST SPEECH, V23, P552, DOI 10.1109/TASSP.1975.1162749; RICCIOTTI G, 1974, VITA GESU CRISTO SEP; TAMBURELLI G, 1979, OSSERVATORE ROM 0711, P6; TAMBURELLI G, 1978, OCT ATT C INT SIND T, P173; Tukey J. W., 1974, EASCON 74, P673	21	2	2	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1981	3	6					670	676		10.1109/TPAMI.1981.4767168	http://dx.doi.org/10.1109/TPAMI.1981.4767168			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MR996	21868987				2022-12-18	WOS:A1981MR99600007
J	BROOKS, R; HEISER, J				BROOKS, R; HEISER, J			SOME EXPERIENCE WITH TRANSFERRING THE MYCIN SYSTEM TO A NEW DOMAIN	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											BROOKS, R (corresponding author), UNIV TEXAS,MED BRANCH,DEPT PSYCHIAT & BEHAV SCI,GALVESTON,TX 77550, USA.							Shortliffe E.H., 2012, COMPUTER BASED MED C; VANMELLE W, 1979, 6TH P INT JOINT C AR, P923	2	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1980	2	5					477	478		10.1109/TPAMI.1980.6592369	http://dx.doi.org/10.1109/TPAMI.1980.6592369			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	KW185					2022-12-18	WOS:A1980KW18500011
J	DELLARICCIA, G				DELLARICCIA, G			FEATURE PROCESSING BY OPTIMAL FACTOR-ANALYSIS TECHNIQUES IN STATISTICAL PATTERN-RECOGNITION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note											DELLARICCIA, G (corresponding author), BEN GURION UNIV NEGEV,DEPT MATH,BEER SHEVA 84120,ISRAEL.							COMREY AL, 1973, FIRST COURSE FACTOR; DELLARICCIA G, 1978, 1978 P INT C CYB SOC; FUKUNAGA K, 1970, IEEE T COMPUT, VC 19, P311, DOI 10.1109/T-C.1970.222918; Harman H.H., 1976, MODERN FACTOR ANAL; KITTLER J, 1973, PATTERN RECOGN, V5, P335, DOI 10.1016/0031-3203(73)90025-3; Mendel J. M., 1970, ADAPTIVE LEARNING PA; WATANABE S, 1969, KNOWING GUESSING, P547; WATANABE S, 1967, 4TH T PRAG C	8	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1980	2	1					60	61						2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JD568					2022-12-18	WOS:A1980JD56800008
J	HECHT, SE; VIDAL, JJ				HECHT, SE; VIDAL, JJ			GENERATION OF ECG PROTOTYPE WAVEFORMS BY PIECEWISE CORRELATIONAL AVERAGING	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											HECHT, SE (corresponding author), UNIV CALIF LOS ANGELES,DEPT COMP SCI,LOS ANGELES,CA 90024, USA.							ALMASI JJ, 1974, IEEE T BIO-MED ENG, VBM21, P264, DOI 10.1109/TBME.1974.324312; BERNI AJ, 1973, SCIENCE, V179, P1338, DOI 10.1126/science.179.4080.1338; BRODY DA, 1971, CLIN ELECTROCARDIOGR; CACERES CA, 1976, AM J CARDIOL, V37, P1052; FISCHMANN E, 1968, AM HEART J, V75, P465, DOI 10.1016/0002-8703(68)90005-7; GROEBEN J, 1973, COMPUTER APPLICATION; GROEBEN J, 1973, COMPUTER APPLICATION, P467; ISHIKAWA K, 1971, AM HEART J, V81, P236, DOI 10.1016/0002-8703(71)90134-7; JOHN E, 1978, EVENT RELATED POTENT; KATZ AM, 1977, PHYSL HEART; PIPBERGER HV, 1975, ANNU REV BIOPHYS BIO, V4, P15, DOI 10.1146/annurev.bb.04.060175.000311; SANTOPIETRO RF, 1977, P IEEE, V65, P707, DOI 10.1109/PROC.1977.10551; SZUREK JL, 1975, P SAN DIEGO BIOMED S, V14, P57; WATANABE Y, 1977, CARDIAC ARRHYTHMIAS; WOODY CD, 1967, MED BIOL ENG, V5, P539, DOI 10.1007/BF02474247	15	2	3	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1980	2	5					415	420		10.1109/TPAMI.1980.6592362	http://dx.doi.org/10.1109/TPAMI.1980.6592362			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	KW185					2022-12-18	WOS:A1980KW18500004
J	NEUMANN, B				NEUMANN, B			EXPLOITING IMAGE-FORMATION KNOWLEDGE FOR MOTION ANALYSIS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											NEUMANN, B (corresponding author), UNIV HAMBURG,FACHBEREICH INFORMAT,D-2000 HAMBURG 13,FED REP GER.							BANARD ST, 1979, TR791 U MINN DEP COM; Barrow H., 1978, COMPUT VIS SYST, V2, P2; BONDE T, 1979, DERIVING 3D DESCRIPT; Horn Berthold K. P., 1975, PSYCHOL COMPUTER VIS, P115; HORN BKP, 1978, AI490 MASS I TECHN M; JAIN R, 1979, IEEE T PATTERN ANAL, V1, P206, DOI 10.1109/TPAMI.1979.4766907; POTTER J, 1975, P INT JOINT C ARTIFI, P803; Radig B., 1978, Proceedings of the 4th International Joint Conference on Pattern Recognition, P723; Ullman S., 1979, PROC R SOC SER B-BIO, DOI 10.7551/mitpress/3877.003.0009; Waltz D., 1975, PSYCHOL COMPUTER VIS, P19	10	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1980	2	6					550	554		10.1109/TPAMI.1980.6447702	http://dx.doi.org/10.1109/TPAMI.1980.6447702			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	KS962					2022-12-18	WOS:A1980KS96200008
J	YABLON, M; CHU, JT				YABLON, M; CHU, JT			RELATIONSHIP OF THE BAYES RISK TO CERTAIN SEPARABILITY MEASURES IN NORMAL CLASSIFICATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											YABLON, M (corresponding author), CUNY,JOHN JAY COLL CRIMINAL JUSTICE,DEPT MATH,NEW YORK,NY 10019, USA.							Anderson T.W, 1958, INTRO MULTIVARIATE S; BRADT RN, 1956, ANN MATH STAT, V27, P390, DOI 10.1214/aoms/1177728265; CHU JT, 1974, IEEE T COMPUT, VC 23, P194, DOI 10.1109/T-C.1974.223887; CHU JT, 1967, J ACM, V14, P273, DOI 10.1145/321386.321390; COCHRAN WG, 1964, TECHNOMETRICS, V6, P179, DOI 10.2307/1266150; FU KS, 1970, IEEE T SYST SCI CYB, VSSC6, P33, DOI 10.1109/TSSC.1970.300326; FUKUNAGA K, 1972, INTRO STATISTICAL PA; JOHNSON NL, 1949, BIOMETRIKA, V36, P149, DOI 10.2307/2332539; KULLBACK S, 1968, INFORMATION THEORY S; LISSACK TSVI, 1976, IEEE T INFORM THEORY, V22, P34, DOI 10.1109/TIT.1976.1055512; MARILL T, 1963, IEEE T INFORM THEORY, V9, P11, DOI 10.1109/TIT.1963.1057810; Mathai A., 1975, BASIC CONCEPTS INFOR; MATUSITA K, 1966, MULTIVARIATE ANAL, P187; MCLACHLAN GJ, 1976, BIOMETRICS, V32, P529, DOI 10.2307/2529742; SHUBIN H, 1968, CARDIOVASC RES, V4, P329	16	2	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1980	2	2					97	100		10.1109/TPAMI.1980.4766987	http://dx.doi.org/10.1109/TPAMI.1980.4766987			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JH803	21868880				2022-12-18	WOS:A1980JH80300001
J	COULON, D; KAYSER, D				COULON, D; KAYSER, D			CONSTRUCTION OF NATURAL-LANGUAGE SENTENCE ACCEPTORS BY A SUPERVISED-LEARNING TECHNIQUE	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									UNIV PARIS 11,RECH INFORMAT LAB,F-91405 ORSAY,FRANCE	UDICE-French Research Universities; Universite Paris Saclay	COULON, D (corresponding author), ECOLE MINES,CTR RECH INFORMAT NANCY,NANCY,FRANCE.							BIERMANN AW, 1971, JAN INT C FRONT PATT; COOK CM, 1974, AUG PREPR NATOASI WO, P157; COULON D, 1977, RAIRO-INF-COMPUT SCI, V11, P75; COULON D, 1975, THESIS U PARIS 6; COULON D, 1976, NOV P AFCET C PAR, P113; FU KS, 1975, IEEE T SYST MAN CYB, VSMC5, P95, DOI 10.1109/TSMC.1975.5409159; FU KS, 1975, IEEE T SYST MAN CYBE, V5, P411; GAINES BR, 1976, INT J MAN MACH STUD, V8, P337, DOI 10.1016/S0020-7373(76)80005-3; HORNING J, 1969, CS139 STANF U COMP S; KAYSER D, 1975, THESIS PARIS; KLEIN S, 1973, AUG INT C COMP LING; SIMON JC, 1974, AUG NATO ASI WORKSH; SOLOMONOFF RJ, 1964, INFORM CONTROL, V7, P224, DOI 10.1016/S0019-9958(64)90131-7; SOLOMONOFF RJ, 1964, INFORM CONTROL, V7, P1, DOI 10.1016/S0019-9958(64)90223-2; WATANABE S, 1960, IBM J RES DEV, V4, P208, DOI 10.1147/rd.42.0208; WHARTON RM, 1973, 51 U TOR DEP COMP SC	16	2	2	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1979	1	1					94	99		10.1109/TPAMI.1979.4766882	http://dx.doi.org/10.1109/TPAMI.1979.4766882			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	HA303	21868837				2022-12-18	WOS:A1979HA30300013
J	JOHNSTON, B; BAILEY, T; DUBES, R				JOHNSTON, B; BAILEY, T; DUBES, R			VARIATION ON A NONPARAMETRIC CLUSTERING METHOD	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									MICHIGAN STATE UNIV,DEPT COMP SCI,E LANSING,MI 48824	Michigan State University	JOHNSTON, B (corresponding author), MICHIGAN STATE UNIV,DEPT RADIOL,E LANSING,MI 48824, USA.							ANDERBERG MR, 1973, CLUSTER ANAL APPLICA; BLASHFIELD RK, 1976, PSYCHOL BULL, V83, P377, DOI 10.1037/0033-2909.83.3.377; DUBES R, 1976, PATTERN RECOGN, V8, P247, DOI 10.1016/0031-3203(76)90045-5; GOWER JC, 1969, APPL STATIST, V18, P54, DOI DOI 10.2307/2346439; JOHNSON SC, 1967, PSYCHOMETRIKA, V32, P241, DOI 10.1007/BF02289588; KATZ JO, 1973, SYST ZOOL, V22, P295, DOI 10.2307/2412309; KITTLER J, 1976, PATTERN RECOGN, V8, P23, DOI 10.1016/0031-3203(76)90026-1; KOONTZ WL, 1976, IEEE T COMPUT, V25, P939; Wishart D., 1969, NUMERICAL TAXONOMY, P282; ZAHN CT, 1971, IEEE T COMPUT, VC 20, P68, DOI 10.1109/T-C.1971.223083	10	2	2	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1979	1	4					400	408		10.1109/TPAMI.1979.4766948	http://dx.doi.org/10.1109/TPAMI.1979.4766948			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HV227	21868874				2022-12-18	WOS:A1979HV22700008
J	MACLEOD, IDG				MACLEOD, IDG			QUANTITATIVE STUDY OF THE ORIENTATION BIAS OF SOME EDGE DETECTOR SCHEMES - COMMENT	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											MACLEOD, IDG (corresponding author), AUSTRALIAN NATL UNIV,DEPT ENGN PHYS,CANBERRA 2600,ACT,AUSTRALIA.							MACLEOD IDG, 1974, VISION RES, V14, P909, DOI 10.1016/0042-6989(74)90157-6; MACLEOD IDG, 1970, PICTURE LANGUAGE MAC, P231; MACLEOD IDG, 1970, EPT9 AUSTR NAT U DEP	4	2	2	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1979	1	4					408	409		10.1109/TPAMI.1979.4766949	http://dx.doi.org/10.1109/TPAMI.1979.4766949			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HV227	21868875				2022-12-18	WOS:A1979HV22700009
J	Aberdam, A; Golts, A; Elad, M				Aberdam, Aviad; Golts, Alona; Elad, Michael			Ada-LISTA: Learned Solvers Adaptive to Varying Models	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Dictionaries; Adaptation models; Training; Convergence; Encoding; Sparse matrices; Numerical models; Sparse coding; learned solvers; lista; deep learning modeling	THRESHOLDING ALGORITHM; SPARSE; REGULARIZATION; SHRINKAGE	Neural networks that are based on the unfolding of iterative solvers as LISTA (Learned Iterative Soft Shrinkage), are widely used due to their accelerated performance. These networks, trained with a fixed dictionary, are inapplicable in varying model scenarios, as opposed to their flexible non-learned counterparts. We introduce, Ada-LISTA, an adaptive learned solver which receives as input both the signal and its corresponding dictionary, and learns a universal architecture to serve them all. This scheme allows solving sparse coding in linear rate, under varying models, including permutations and perturbations of the dictionary. We provide an extensive theoretical and numerical study, demonstrating the adaptation capabilities of our approach, and its application to the task of natural image inpainting.	[Aberdam, Aviad] Technion Israel Inst Technol, Dept Elect Engn, IL-3200003 Haifa, Israel; [Golts, Alona; Elad, Michael] Technion Israel Inst Technol, Dept Comp Sci, IL-3200003 Haifa, Israel	Technion Israel Institute of Technology; Technion Israel Institute of Technology	Aberdam, A (corresponding author), Technion Israel Inst Technol, Dept Elect Engn, IL-3200003 Haifa, Israel.	aaberdam@cs.technion.ac.il; salonaz@cs.technion.ac.il; elad@cs.technion.ac.il		Elad, Michael/0000-0001-8131-6928; Golts, Alona/0000-0001-7103-8047; Aberdam, Aviad/0000-0003-0084-5022	Israel Science Foundation (ISF) [335/18]; Technion Hiroshi Fujiwara Cyber Security Research Center; Israel Cyber Bureau	Israel Science Foundation (ISF)(Israel Science Foundation); Technion Hiroshi Fujiwara Cyber Security Research Center; Israel Cyber Bureau	This work was supported in part by the Israel Science Foundation (ISF) under Grant 335/18 and in part by the Technion Hiroshi Fujiwara Cyber Security Research Center and the Israel Cyber Bureau.	Alexandre Gramfort, 2019, Arxiv, DOI arXiv:1905.11071; Andrychowicz M, 2016, ADV NEUR IN, V29; [Anonymous], 2010, P INT C MACH LEARN; Beck A, 2017, MOS-SIAM SER OPTIMIZ, P1, DOI 10.1137/1.9781611974997; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bin Zhao, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3313, DOI 10.1109/CVPR.2011.5995524; Borgerding M, 2017, IEEE T SIGNAL PROCES, V65, P4293, DOI 10.1109/TSP.2017.2708040; Boyd S, 2004, CONVEX OPTIMIZATION; Bruckstein AM, 2009, SIAM REV, V51, P34, DOI 10.1137/060657704; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S1064827596304010; Chen XH, 2018, ADV NEUR IN, V31; Combettes PL, 2005, MULTISCALE MODEL SIM, V4, P1168, DOI 10.1137/050626090; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; Felix Weninger, 2014, Arxiv, DOI arXiv:1409.2574; Giryes R, 2018, IEEE T SIGNAL PROCES, V66, P1676, DOI 10.1109/TSP.2018.2791945; Golts A, 2021, IEEE J-STSP, V15, P324, DOI 10.1109/JSTSP.2021.3049634; Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55; Liu J., 2019, P INT C LEARN REPR; Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Mazumder R, 2010, J MACH LEARN RES, V11, P2287; Metzler CA, 2017, ADV NEUR IN, V30; Protter M, 2009, IEEE T IMAGE PROCESS, V18, P27, DOI 10.1109/TIP.2008.2008065; Romano Y, 2017, SIAM J IMAGING SCI, V10, P1804, DOI 10.1137/16M1102884; Sprechmann P, 2015, IEEE T PATTERN ANAL, V37, P1821, DOI 10.1109/TPAMI.2015.2392779; Sulam J, 2020, IEEE T PATTERN ANAL, V42, P1968, DOI 10.1109/TPAMI.2019.2904255; Tang S, 2014, SIGNAL PROCESS, V94, P339, DOI 10.1016/j.sigpro.2013.07.005; Tibshirani R, 2011, J R STAT SOC B, V73, P273, DOI 10.1111/j.1467-9868.2011.00771.x; Venkatakrishnan S, 2013, IEEE GLOB CONF SIG, P945, DOI 10.1109/GlobalSIP.2013.6737048; Wang Z., 2016, PROC AAAI C ARTIF IN, V30; Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50; Wu K., 2020, PROC INT C LEARN REP; Xin B, 2016, ADV NEUR IN, V29; Yang Y, 2016, ADV NEUR IN, V29; Zarka J., 2019, ARXIV; Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196	37	1	1	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9222	9235		10.1109/TPAMI.2021.3125041	http://dx.doi.org/10.1109/TPAMI.2021.3125041			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34735338	Green Submitted			2022-12-18	WOS:000880661400049
J	Chen, Z; Nobuhara, S; Nishino, K				Chen, Zhe; Nobuhara, Shohei; Nishino, Ko			Invertible Neural BRDF for Object Inverse Rendering	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Lighting; Estimation; Rendering (computer graphics); Geometry; Computational modeling; Analytical models; Neural networks; Reflectance; BRDF; inverse rendering; illumination estimation	DENSITY-ESTIMATION; REFLECTANCE; ILLUMINATION; MODEL	We introduce a novel neural network-based BRDF model and a Bayesian framework for object inverse rendering, i.e., joint estimation of reflectance and natural illumination from a single image of an object of known geometry. The BRDF is expressed with an invertible neural network, namely, normalizing flow, which provides the expressive power of a high-dimensional representation, computational simplicity of a compact analytical model, and physical plausibility of a real-world BRDF. We extract the latent space of real-world reflectance by conditioning this model, which directly results in a strong reflectance prior. We refer to this model as the invertible neural BRDF model (iBRDF). We also devise a deep illumination prior by leveraging the structural bias of deep neural networks. By integrating this novel BRDF model and reflectance and illumination priors in a MAP estimation formulation, we show that this joint estimation can be computed efficiently with stochastic gradient descent. We experimentally validate the accuracy of the invertible neural BRDF model on a large number of measured data and demonstrate its use in object inverse rendering on a number of synthetic and real images. The results show new ways in which deep neural networks can help solve challenging radiometric inverse problems.	[Chen, Zhe; Nobuhara, Shohei; Nishino, Ko] Kyoto Univ, Dept Intelligence Sci & Technol, Kyoto 6068501, Japan	Kyoto University	Chen, Z (corresponding author), Kyoto Univ, Dept Intelligence Sci & Technol, Kyoto 6068501, Japan.	zchen@vision.ist.i.kyoto-u.ac.jp; nob@i.kyoto-u.ac.jp; kon@i.kyoto-u.ac.jp		Nishino, Ko/0000-0002-3534-3447; Chen, Zhe/0000-0002-5153-4593; Nobuhara, Shohei/0000-0002-3204-8696	JSPS KAKENHI [17K20143, 20H05951, 21H04893]; JST [JPMJCR20G7]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); JST(Japan Science & Technology Agency (JST))	This work was supported in part by JSPS KAKENHI under Grants 17K20143, 20H05951, 21H04893, and JST JPMJCR20G7.	Ardizzone L., 2019, PROC INT C LEARN REP; Ashikhmin M., 2007, DISTRIBUTION B UNPUB, V2; Azinovic D, 2019, PROC CVPR IEEE, P2442, DOI 10.1109/CVPR.2019.00255; Basri R, 2001, PROC CVPR IEEE, P374; Blinn James F., 1977, COMPUT GRAPHICS-US, V11, P192, DOI [DOI 10.1145/965141.563893, 10.1145/965141, DOI 10.1145/965141]; Boss M, 2020, PROC CVPR IEEE, P3981, DOI 10.1109/CVPR42600.2020.00404; BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7; Burley Brent, 2012, PHYSICALLY BASED SHA, V2012, P1; Cook R., 1982, ACM T GRAPHIC, V1, P7, DOI DOI 10.1145/357290.357293; Debevec Paul., 2008, ACM SIGGRAPH 2008 CL, P1; Dinh L, 2015, PROC 3 INT C LEARN R; Dupuy J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275059; Gao D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323042; Gardner MA, 2019, IEEE I CONF COMP VIS, P7174, DOI 10.1109/ICCV.2019.00727; Garon M, 2019, PROC CVPR IEEE, P6901, DOI 10.1109/CVPR.2019.00707; Georgoulis S, 2018, IEEE T PATTERN ANAL, V40, P1932, DOI 10.1109/TPAMI.2017.2742999; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Kang KZ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201279; Kingma D.P., 2014, P 2 INT C LEARN REPR, DOI DOI 10.1093/BIOINFORMATICS/BTAA169; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Koenderink J. J., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P28; Lattas A, 2020, PROC CVPR IEEE, P757, DOI 10.1109/CVPR42600.2020.00084; Lee H.-C., 1992, ILLUMINANT COLOR SHA, P340; LeGendre C, 2019, PROC CVPR IEEE, P5911, DOI 10.1109/CVPR.2019.00607; Li ZQ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275055; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Lombardi S, 2016, INT CONF 3D VISION, P305, DOI 10.1109/3DV.2016.39; Lombardi S, 2016, IEEE T PATTERN ANAL, V38, P129, DOI 10.1109/TPAMI.2015.2430318; Lombardi S, 2012, LECT NOTES COMPUT SC, V7577, P582, DOI 10.1007/978-3-642-33783-3_42; Lombardi S, 2012, PROC CVPR IEEE, P238, DOI 10.1109/CVPR.2012.6247681; Marschner SR, 1997, FIFTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS, AND APPLICATIONS, P262; Matusik W, 2003, ACM T GRAPHIC, V22, P759, DOI 10.1145/882262.882343; Meka A, 2018, PROC CVPR IEEE, P6315, DOI 10.1109/CVPR.2018.00661; Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24; Muller T, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3341156; Nicodemus F. E., 1977, NAT BUREAU STANDARDS, DOI [10.6028/NBS.MONO.160, DOI 10.6028/NBS.MONO.160]; Nielsen JB, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818085; Nishino K, 2009, IEEE I CONF COMP VIS, P476, DOI 10.1109/ICCV.2009.5459255; Nishino K, 2011, J OPT SOC AM A, V28, P8, DOI 10.1364/JOSAA.28.000008; Oxholm G, 2016, IEEE T PATTERN ANAL, V38, P376, DOI 10.1109/TPAMI.2015.2450734; Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; Ramamoorthi R, 2001, COMP GRAPH, P117, DOI 10.1145/383259.383271; Rematas K, 2016, PROC CVPR IEEE, P4508, DOI 10.1109/CVPR.2016.488; Romeiro F, 2008, LECT NOTES COMPUT SC, V5305, P859, DOI 10.1007/978-3-540-88693-8_63; Romeiro F, 2010, LECT NOTES COMPUT SC, V6311, P45, DOI 10.1007/978-3-642-15549-9_4; Rusinkiewicz S. M., 1998, Rendering Techniques '98. Proceedings of the Eurographics Workshop, P11; Sato Y., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P379, DOI 10.1145/258734.258885; Sengupta S, 2019, IEEE I CONF COMP VIS, P8597, DOI 10.1109/ICCV.2019.00869; Serrano A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980242; Tabak EG, 2013, COMMUN PUR APPL MATH, V66, P145, DOI 10.1002/cpa.21423; Tabak EG, 2010, COMMUN MATH SCI, V8, P217; TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Vaswani A, 2017, ADV NEUR IN, V30; Wang TFY, 2018, INT CONF 3D VISION, P22, DOI 10.1109/3DV.2018.00014; Wilcox R.R, 2011, INTRO ROBUST ESTIMAT, V2nd ed.; Yamaguchi S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201364; Yu Y, 2019, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2019.00327	59	1	1	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9380	9395		10.1109/TPAMI.2021.3129537	http://dx.doi.org/10.1109/TPAMI.2021.3129537			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34807819	Green Submitted			2022-12-18	WOS:000880661400059
J	Das, S; Dai, R; Yang, D; Bremond, F				Das, Srijan; Dai, Rui; Yang, Di; Bremond, Francois			VPN plus plus : Rethinking Video-Pose Embeddings for Understanding Activities of Daily Living	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Videos; Virtual private networks; Visualization; Skeleton; Topology; Task analysis; Trimmed videos; pose; activities of daily living; embedding; attention		Many attempts have been made towards combining RGB and 3D poses for the recognition of Activities of Daily Living (ADL). ADL may look very similar and often necessitate to model fine-grained details to distinguish them. Because the recent 3D ConvNets are too rigid to capture the subtle visual patterns across an action, this research direction is dominated by methods combining RGB and 3D Poses. But the cost of computing 3D poses from RGB stream is high in the absence of appropriate sensors. This limits the usage of aforementioned approaches in real-world applications requiring low latency. Then, how to best take advantage of 3D Poses for recognizing ADL? To this end, we propose an extension of a pose driven attention mechanism: Video-Pose Network (VPN), exploring two distinct directions. One is to transfer the Pose knowledge into RGB through a feature-level distillation and the other towards mimicking pose driven attention through an attention-level distillation. Finally, these two approaches are integrated into a single model, we call VPN++. It is worth noting that VPN++ exploits the pose embeddings at training via distillation but not at inference. We show that VPN++ is not only effective but also provides a high speed up and high resilience to noisy Poses. VPN++, with or without 3D Poses, outperforms the representative baselines on 4 public datasets. Code is available at https://github.com/srijandas07/vpnplusplus.	[Das, Srijan] SUNY Stony Brook, Stony Brook, NY 11794 USA; [Dai, Rui; Yang, Di; Bremond, Francois] INRIA, F-06902 Valbonne, France; [Dai, Rui; Yang, Di; Bremond, Francois] Univ Cote dAzur, F-06902 Valbonne, France	State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook; Inria; UDICE-French Research Universities; Universite Cote d'Azur	Das, S (corresponding author), SUNY Stony Brook, Stony Brook, NY 11794 USA.	srijan.das@stonybrook.edu; rui.dai@inria.fr; di.yang@inria.fr; francois.bremondj@inria.fr		Yang, Di/0000-0002-8124-532X; Dai, Rui/0000-0002-3698-4086	French Government, through the 3IA Cote d'Azur Investments in the Future Project [ANR-19-P3IA-0002]	French Government, through the 3IA Cote d'Azur Investments in the Future Project	This work was supported by the French Government, through the 3IA Cote d'Azur Investments in the Future Project managed by the National Research Agency (ANR) with the reference number ANR-19-P3IA-0002.	Amir Roshan Zamir, 2012, Arxiv, DOI arXiv:1212.0402; Andrew Zisserman, 2017, Arxiv, DOI arXiv:1705.06950; Antoine Miech, 2020, Arxiv, DOI arXiv:1804.02516; Arnold W.M. Smeulders, 2019, Arxiv, DOI arXiv:1812.01289; Baradel F., 2018, BMVC, P1; Baradel F, 2018, PROC CVPR IEEE, P469, DOI 10.1109/CVPR.2018.00056; Baradel F, 2017, IEEE INT CONF COMP V, P604, DOI 10.1109/ICCVW.2017.77; Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Crasto N, 2019, PROC CVPR IEEE, P7874, DOI 10.1109/CVPR.2019.00807; Das Srijan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P72, DOI 10.1007/978-3-030-58545-7_5; Das S, 2019, IEEE I CONF COMP VIS, P833, DOI 10.1109/ICCV.2019.00092; Das S, 2020, IEEE WINT CONF APPL, P487, DOI 10.1109/WACV45572.2020.9093575; Das S, 2019, LECT NOTES COMPUT SC, V11296, P493, DOI 10.1007/978-3-030-05716-9_40; Das S, 2019, IEEE WINT CONF APPL, P71, DOI 10.1109/WACV.2019.00015; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Du Tran, 2020, Arxiv, DOI arXiv:1905.12681; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Du Y, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P579, DOI 10.1109/ACPR.2015.7486569; Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Garcia NC, 2018, LECT NOTES COMPUT SC, V11212, P106, DOI 10.1007/978-3-030-01237-3_7; Girdhar R., 2018, ARXIV; Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Hoffman J, 2016, PROC CVPR IEEE, P826, DOI 10.1109/CVPR.2016.96; Hoffman J, 2016, IEEE INT CONF ROBOT, P5032, DOI 10.1109/ICRA.2016.7487708; Ke QH, 2018, IEEE T IMAGE PROCESS, V27, P2842, DOI 10.1109/TIP.2018.2812099; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115; Liu GY, 2019, IEEE INT C INT ROBOT, P258, DOI 10.1109/IROS40897.2019.8967570; Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873; Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391; Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50; Liu MY, 2018, PROC CVPR IEEE, P1159, DOI 10.1109/CVPR.2018.00127; Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030; Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022; Luo ZL, 2018, LECT NOTES COMPUT SC, V11218, P174, DOI 10.1007/978-3-030-01264-9_11; Mahasseni B, 2016, PROC CVPR IEEE, P3054, DOI 10.1109/CVPR.2016.333; Miech Antoine, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9876, DOI 10.1109/CVPR42600.2020.00990; Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39; Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497; Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794; Perez-Rua JM, 2019, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2019.00713; Piergiovanni AJ, 2020, PROC CVPR IEEE, P130, DOI 10.1109/CVPR42600.2020.00021; Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590; Qiushan Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11017, DOI 10.1109/CVPR42600.2020.01103; Rahmani H, 2017, IEEE I CONF COMP VIS, P5833, DOI 10.1109/ICCV.2017.621; Rahmani H, 2016, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2016.167; Rahmani H, 2015, PROC CVPR IEEE, P2458, DOI 10.1109/CVPR.2015.7298860; Rogez G, 2020, IEEE T PATTERN ANAL, V42, P1146, DOI 10.1109/TPAMI.2019.2892985; Ryoo Michael S., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P654, DOI 10.1007/978-3-030-58565-5_39; Ryoo M. S., 2020, PROC INT C LEARN REP; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115; Shahroudy A, 2014, 2014 6TH INTERNATIONAL SYMPOSIUM ON COMMUNICATIONS, CONTROL AND SIGNAL PROCESSING (ISCCSP), P73, DOI 10.1109/ISCCSP.2014.6877819; Shi L, 2020, IEEE T IMAGE PROCESS, V29, P9532, DOI 10.1109/TIP.2020.3028207; Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810; Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Simonyan K, 2014, ADV NEUR IN, V27; Song SJ, 2017, AAAI CONF ARTIF INTE, P4263; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Tian Yonglong, 2020, INT C LEARN REPR, V1; Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444; Yang D, 2021, IEEE WINT CONF APPL, P2362, DOI 10.1109/WACV48630.2021.00241; Zhang PF, 2020, PROC CVPR IEEE, P1109, DOI 10.1109/CVPR42600.2020.00119; Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI 10.1109/ICCV.2017.233; Zhang SY, 2017, IEEE WINT CONF APPL, P148, DOI 10.1109/WACV.2017.24	76	1	1	8	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9703	9717		10.1109/TPAMI.2021.3127885	http://dx.doi.org/10.1109/TPAMI.2021.3127885			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34767506	Green Submitted			2022-12-18	WOS:000880661400081
J	Eom, C; Lee, W; Lee, G; Ham, B				Eom, Chanho; Lee, Wonkyung; Lee, Geon; Ham, Bumsub			Disentangled Representations for Short-Term and Long-Term Person Re-Identification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature extraction; Task analysis; Clutter; Interpolation; Image color analysis; Generative adversarial networks; Benchmark testing; Person re-identification; disentanglement; generative adversarial learning	NETWORK; SET	We address the problem of person re-identification (reID), that is, retrieving person images from a large dataset, given a query image of the person of interest. A key challenge is to learn person representations robust to intra-class variations, as different persons could have the same attribute, and persons' appearances look different, e.g., with viewpoint changes. Recent reID methods focus on learning person features discriminative only for a particular factor of variations (e.g., human pose), which also requires corresponding supervisory signals (e.g., pose annotations). To tackle this problem, we propose to factorize person images into identity-related and -unrelated features. Identity-related features contain information useful for specifying a particular person (e.g., clothing), while identity-unrelated ones hold other factors (e.g., human pose). To this end, we propose a new generative adversarial network, dubbed identity shuffle GAN (IS-GAN). It disentangles identity-related and -unrelated features from person images through an identity-shuffling technique that exploits identification labels alone without any auxiliary supervisory signals. We restrict the distribution of identity-unrelated features, or encourage the identity-related and -unrelated features to be uncorrelated, facilitating the disentanglement process. Experimental results validate the effectiveness of IS-GAN, showing state-of-the-art performance on standard reID benchmarks, including Market-1501, CUHK03 and DukeMTMC-reID. We further demonstrate the advantages of disentangling person representations on a long-term reID task, setting a new state of the art on a Celeb-reID dataset. Our code and models are available online: https://cvlab-yonsei.github.io/projects/ISGAN/.	[Eom, Chanho; Lee, Wonkyung; Lee, Geon; Ham, Bumsub] Yonsei Univ, Sch Elect & Elect Engn, Seoul 03722, South Korea	Yonsei University	Ham, B (corresponding author), Yonsei Univ, Sch Elect & Elect Engn, Seoul 03722, South Korea.	cheom@yonsei.ac.kr; wonkyung.lee@yonsei.ac.kr; geon.lee@yonsei.ac.kr; bumsub.ham@yonsei.ac.kr		HAM, BUMSUB/0000-0002-3443-8161; Eom, Chanho/0000-0002-0623-8074	R&D program for Advanced Integratedintelligence for Identification (AIID) through the National Research Foundation of KOREA(NRF) - Ministry of Science and ICT [NRF-2018M3E3A1057289]; Yonsei University [2021-22-0001]	R&D program for Advanced Integratedintelligence for Identification (AIID) through the National Research Foundation of KOREA(NRF) - Ministry of Science and ICT(National Research Foundation of Korea); Yonsei University	This research was supported by R&D program for Advanced Integratedintelligence for Identification (AIID) through the National Research Foundation of KOREA(NRF) funded by Ministry of Science and ICT under Grant NRF-2018M3E3A1057289, and Yonsei University Research Fund of 2021 (2021-22-0001).	Alexander G. Hauptmann, 2016, Arxiv, DOI arXiv:1610.02984; Andrea Vedaldi, 2017, Arxiv, DOI arXiv:1607.08022; Bao JM, 2018, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2018.00702; Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225; Chen GY, 2019, IEEE I CONF COMP VIS, P9636, DOI 10.1109/ICCV.2019.00973; Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844; Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145; Cheung B., 2014, P INT C LEARN REPR W; Chi Zhang, 2018, Arxiv, DOI arXiv:1711.08184; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Eom C, 2019, ADV NEUR IN, V32; Fu Y, 2019, AAAI CONF ARTIF INTE, P8295; Ge YX, 2018, ADV NEUR IN, V31; Gel'fand I. M., 1959, AM MATH SOC TRANSL 2, V12, P199; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo JY, 2019, IEEE I CONF COMP VIS, P3641, DOI 10.1109/ICCV.2019.00374; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Huang Y, 2020, IEEE T CIRC SYST VID, V30, P3459, DOI 10.1109/TCSVT.2019.2948093; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975; Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117; Kingma D.P., 2014, P 2 INT C LEARN REPR, DOI DOI 10.1093/BIOINFORMATICS/BTAA169; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Kostinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939; Kris M. Kitani, 2020, Arxiv, DOI arXiv:2003.07340; Kristoufek L, 2014, PHYSICA A, V402, P291, DOI 10.1016/j.physa.2014.01.058; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3; Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782; Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Li YY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P163, DOI 10.1145/3240508.3240573; Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063; Liao SC, 2015, IEEE I CONF COMP VIS, P3685, DOI 10.1109/ICCV.2015.420; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006; Liu JL, 2019, IEEE ACCESS, V7, P114021, DOI 10.1109/ACCESS.2019.2933910; Liu XH, 2017, IEEE I CONF COMP VIS, P350, DOI 10.1109/ICCV.2017.46; Liu Y, 2018, PROC CVPR IEEE, P2080, DOI 10.1109/CVPR.2018.00222; Lu BY, 2019, PROC CVPR IEEE, P10217, DOI 10.1109/CVPR.2019.01047; Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; Martinel N, 2019, IEEE COMPUT SOC CONF, P1544, DOI 10.1109/CVPRW.2019.00196; Mathieu M, 2016, ADV NEUR IN, V29; Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063; Odena A, 2017, PR MACH LEARN RES, V70; Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40; Quispe R, 2021, INT C PATT RECOG, P2980, DOI 10.1109/ICPR48806.2021.9412017; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Sabour S, 2017, ADV NEUR IN, V30; Shen YT, 2018, PROC CVPR IEEE, P6886, DOI 10.1109/CVPR.2018.00720; Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30; Shizhen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P647, DOI 10.1007/978-3-030-58539-6_39; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427; Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25; Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643; Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30; Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410; Tay CP, 2019, PROC CVPR IEEE, P7127, DOI 10.1109/CVPR.2019.00730; Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552; Wang Y, 2018, PROC CVPR IEEE, P8042, DOI 10.1109/CVPR.2018.00839; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Xingyang Ni, 2020, 2020 25th International Conference on Pattern Recognition (ICPR), P9601, DOI 10.1109/ICPR48806.2021.9412481; Yang Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P87, DOI 10.1007/978-3-030-58536-5_6; Yu SJ, 2020, PROC CVPR IEEE, P3397, DOI 10.1109/CVPR42600.2020.00346; Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103; Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349; Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133; Zheng M, 2019, PROC CVPR IEEE, P5728, DOI 10.1109/CVPR.2019.00588; Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI [10.1109/CVPR.2019.00224, 10.1109/CVPR.2019.01247]; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389; Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	85	1	1	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					8975	8991		10.1109/TPAMI.2021.3122444	http://dx.doi.org/10.1109/TPAMI.2021.3122444			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34699350				2022-12-18	WOS:000880661400032
J	Hu, RW; Xiang, SJ				Hu, Runwen; Xiang, Shijun			Reversible Data Hiding By Using CNN Prediction and Adaptive Embedding	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature extraction; Convolution; Convolutional neural networks; Histograms; Iron; Training; Kernel; Adaptive embedding; convolutional neural network; prediction error ordering; reversible data hiding	EXPANSION	In the field of reversible data hiding (RDH), how to predict an image and embed a message into the image with smaller distortion are two important aspects. In this paper, we propose a novel and efficient RDH method by innovating an intelligent predictor and an adaptive embedding way. In the prediction stage, we first constructed a convolutional neural network (CNN) based predictor by reasonably dividing an image into four parts. In such a way, each part can be predicted by using the other three parts as the context for the improvement of the prediction performance. Compared with existing predictors, the proposed CNN predictor can use more neighboring pixels for the prediction by exploiting its multi-receptive fields and global optimization capacities. In the embedding stage, we also developed a prediction-error-ordering (PEO) based adaptive embedding strategy, which can better adapt image content and thus efficiently reduce the embedding distortion by elaborately and luminously applying background complexity to select and pair those smaller prediction errors for data hiding. With the proposed CNN prediction and embedding ways, the RDH method presented in this paper provides satisfactory results in improving the visual quality of data hidden images, e.g., the average PSNR value for the Kodak benchmark dataset can reach as high as 63.59 dB with an embedding capacity of 10,000 bits. Extensive experimental results have shown that the RDH method proposed in this paper is superior to those existing state-of-the-art works.	[Hu, Runwen; Xiang, Shijun] Jinan Univ, Coll Informat Sci & Technol, Coll Cyber Secur, Guangzhou 510632, Peoples R China	Jinan University	Xiang, SJ (corresponding author), Jinan Univ, Coll Informat Sci & Technol, Coll Cyber Secur, Guangzhou 510632, Peoples R China.	runwen_hu@qq.com; Shijun_Xiang@qq.com			National Natural Science Foundation of China [61772234, 61871201]; Special Funds for the Cultivation of Guangdong College Students' Scientific and Technological Innovation [pdjh2020a0060]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Special Funds for the Cultivation of Guangdong College Students' Scientific and Technological Innovation	This work was supported in part by the National Natural Science Foundation of China under Grants 61772234 and 61871201 and in part by the Special Funds for the Cultivation of Guangdong College Students' Scientific and Technological Innovation under Grant pdjh2020a0060.	Baluja S, 2020, IEEE T PATTERN ANAL, V42, P1685, DOI 10.1109/TPAMI.2019.2901877; Chan CK, 2004, PATTERN RECOGN, V37, P469, DOI 10.1016/j.patcog.2003.08.007; Chen HS, 2017, IEEE SIGNAL PROC LET, V24, P574, DOI 10.1109/LSP.2017.2679043; Chen ZP, 2019, AAAI CONF ARTIF INTE, P6276; Coltuc D, 2012, IEEE T IMAGE PROCESS, V21, P412, DOI 10.1109/TIP.2011.2162424; Coltuc D, 2011, IEEE T INF FOREN SEC, V6, P873, DOI 10.1109/TIFS.2011.2145372; Cox I., 2002, DIGITAL WATERMARKING; Dacrema MF, 2021, ACM T INFORM SYST, V39, DOI 10.1145/3434185; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dragoi IC, 2018, IEEE IMAGE PROC, P1668, DOI 10.1109/ICIP.2018.8451299; Dragoi IC, 2015, IEEE T IMAGE PROCESS, V24, P1244, DOI 10.1109/TIP.2015.2395724; Fan LX, 2022, IEEE T PATTERN ANAL, V44, P6122, DOI 10.1109/TPAMI.2021.3088846; Franzen Rich, 1999, KODAK LOSSLESS TRUE; Fridrich J, 2001, P SOC PHOTO-OPT INS, V4314, P197, DOI 10.1117/12.435400; Hong W, 2012, OPT COMMUN, V285, P101, DOI 10.1016/j.optcom.2011.09.005; Hu RW, 2021, IEEE SIGNAL PROC LET, V28, P464, DOI 10.1109/LSP.2021.3059202; Hu RW, 2021, IEEE T IMAGE PROCESS, V30, P318, DOI 10.1109/TIP.2020.3036727; Hu XC, 2015, IEEE T INF FOREN SEC, V10, P653, DOI 10.1109/TIFS.2015.2392556; Jafar IF, 2016, COMPUT J, V59, P423, DOI 10.1093/comjnl/bxv067; Jianqiang Qin, 2020, Pattern Recognition and Computer Vision. Third Chinese Conference, PRCV 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12305), P527, DOI 10.1007/978-3-030-60633-6_44; Kim S, 2019, IEEE T CIRC SYST VID, V29, P3236, DOI 10.1109/TCSVT.2018.2878932; Kingma D.P, P 3 INT C LEARNING R; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li XL, 2013, SIGNAL PROCESS, V93, P198, DOI 10.1016/j.sigpro.2012.07.025; Li XL, 2011, IEEE T IMAGE PROCESS, V20, P3524, DOI 10.1109/TIP.2011.2150233; Luo LX, 2010, IEEE T INF FOREN SEC, V5, P187, DOI 10.1109/TIFS.2009.2035975; Luo T, 2019, J VIS COMMUN IMAGE R, V61, P61, DOI 10.1016/j.jvcir.2019.03.017; Ma B, 2016, IEEE T INF FOREN SEC, V11, P1914, DOI 10.1109/TIFS.2016.2566261; Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964; Ou B, 2019, IEEE T CIRC SYST VID, V29, P2176, DOI 10.1109/TCSVT.2018.2859792; Ou B, 2016, J VIS COMMUN IMAGE R, V39, P12, DOI 10.1016/j.jvcir.2016.05.005; Ou B, 2014, SIGNAL PROCESS-IMAGE, V29, P760, DOI 10.1016/j.image.2014.05.003; Ou B, 2013, IEEE T IMAGE PROCESS, V22, P5010, DOI 10.1109/TIP.2013.2281422; Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031; Sachnev V, 2009, IEEE T CIRC SYST VID, V19, P989, DOI 10.1109/TCSVT.2009.2020257; Souibgui MA, 2022, IEEE T PATTERN ANAL, V44, P1180, DOI 10.1109/TPAMI.2020.3022406; Thodi DM, 2007, IEEE T IMAGE PROCESS, V16, P721, DOI 10.1109/TIP.2006.891046; Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962; Wang X, 2015, INFORM SCIENCES, V310, P16, DOI 10.1016/j.ins.2015.03.022; Weng SW, 2019, INFORM SCIENCES, V489, P136, DOI 10.1016/j.ins.2019.03.032; WITTEN IH, 1987, COMMUN ACM, V30, P520, DOI 10.1145/214762.214771; Xiang SJ, 2022, IEEE T CIRC SYST VID, V32, P2868, DOI 10.1109/TCSVT.2021.3103215; Yurtsever E, 2018, IEEE T INTELL VEHICL, V3, P242, DOI 10.1109/TIV.2018.2843171; Zhang J, 2021, IEEE T PATTERN ANAL, V44, P4005, DOI 10.1109/TPAMI.2021.3064850; Zhang T, 2020, IEEE T INF FOREN SEC, V15, P2306, DOI 10.1109/TIFS.2019.2963766; Zhang T, 2020, LECT NOTES COMPUT SC, V11961, P317, DOI 10.1007/978-3-030-37731-1_26	47	1	1	9	24	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					10196	10208		10.1109/TPAMI.2021.3131250	http://dx.doi.org/10.1109/TPAMI.2021.3131250			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34847020				2022-12-18	WOS:000880661400115
J	Joseph, KJ; Rajasegaran, J; Khan, S; Khan, FS; Balasubramanian, VN				Joseph, K. J.; Rajasegaran, Jathushan; Khan, Salman; Khan, Fahad Shahbaz; Balasubramanian, Vineeth N.			Incremental Object Detection via Meta-Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Detectors; Object detection; Training; Proposals; Standards; Feature extraction; Object detection; incremental learning; deep neural networks; meta-learning; gradient preconditioning		In a real-world setting, object instances from new classes can be continuously encountered by object detectors. When existing object detectors are applied to such scenarios, their performance on old classes deteriorates significantly. A few efforts have been reported to address this limitation, all of which apply variants of knowledge distillation to avoid catastrophic forgetting. We note that although distillation helps to retain previous learning, it obstructs fast adaptability to new tasks, which is a critical requirement for incremental learning. In this pursuit, we propose a meta-learning approach that learns to reshape model gradients, such that information across incremental tasks is optimally shared. This ensures a seamless information transfer via a meta-learned gradient preconditioning that minimizes forgetting and maximizes knowledge transfer. In comparison to existing meta-learning methods, our approach is task-agnostic, allows incremental addition of new-classes and scales to high-capacity models for object detection. We evaluate our approach on a variety of incremental learning settings defined on PASCAL-VOC and MS COCO datasets, where our approach performs favourably well against state-of-the-art methods. Code and trained models: https://github.com/JosephKJ/iOD.	[Joseph, K. J.; Balasubramanian, Vineeth N.] Indian Inst Technol Hyderabad, Dept Comp Sci & Engn, Kandi 502285, Telangana, India; [Rajasegaran, Jathushan] Univ Calfornia Berkeley, Berkeley, CA 94720 USA; [Khan, Salman; Khan, Fahad Shahbaz] MBZ Univ AI, Abu Dhabi, U Arab Emirates; [Khan, Salman] Australian Natl Univ, Canberra, ACT 0200, Australia; [Khan, Fahad Shahbaz] Linkoping Univ, CVL, S-58183 Linkoping, Sweden	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Hyderabad; Australian National University; Linkoping University	Joseph, KJ (corresponding author), Indian Inst Technol Hyderabad, Dept Comp Sci & Engn, Kandi 502285, Telangana, India.	cs17m18p100001@iith.ac.in; brjathu@gmail.com; khan@anu.edu.au; fahad.khan@liu.se; vineethnb@iith.ac.in	; Khan, Salman Hameed/M-4834-2016	N Balasubramanian, Vineeth/0000-0003-2656-0375; Khan, Salman Hameed/0000-0002-9502-1749; Rajasegaran, Jathushan/0000-0003-1081-4254; K J, Joseph/0000-0003-1168-1609	DST through the IMPRINT program [IMP/2019/000250]; TCS PhD Fellowship; VR [2016-05543]	DST through the IMPRINT program; TCS PhD Fellowship; VR(Swedish Research Council)	This work has been partly supported by the funding received from DST through the IMPRINT program IMP/2019/000250, in part by TCS PhD Fellowship and VR starting under Grant 2016-05543.	Acharya M., 2020, P BRIT MACH VIS C; Alex Nichol, 2018, Arxiv, DOI arXiv:1803.02999; Andrei A. Rusu, 2019, Arxiv, DOI arXiv:1807.05960; Castro FM, 2018, LECT NOTES COMPUT SC, V11216, P241, DOI 10.1007/978-3-030-01258-8_15; Cermelli Fabio, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9230, DOI 10.1109/CVPR42600.2020.00925; Chaudhry Arslan, 2019, P ICLR; Chen L., 2019, PROC INT JOINT C NEU, P1, DOI DOI 10.3969/J.ISSN.1006-6535; Desjardins G., 2015, P 28 C NEUR PROC SYS, P2071; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Finn C, 2017, PR MACH LEARN RES, V70; Flennerhag Sebastian, 2020, INT C LEARN REPR; French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2; Geoffrey Hinton, 2015, Arxiv, DOI arXiv:1503.02531; Girshick R., 2014, PROC IEEE C COMPUT V; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Hao Y, 2019, IEEE INT CON MULTI, P1, DOI 10.1109/ICME.2019.00009; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Joseph KJ, 2021, PROC CVPR IEEE, P5826, DOI 10.1109/CVPR46437.2021.00577; Kang BY, 2019, IEEE I CONF COMP VIS, P8419, DOI 10.1109/ICCV.2019.00851; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Lee Y, 2017, PROC INT C MACH LEAR, P2933; Li DW, 2019, SEC'19: PROCEEDINGS OF THE 4TH ACM/IEEE SYMPOSIUM ON EDGE COMPUTING, P113, DOI 10.1145/3318216.3363317; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lopez-Paz D, 2017, ADV NEUR IN, V30; Michieli U, 2019, IEEE INT CONF COMP V, P3205, DOI 10.1109/ICCVW.2019.00400; Mostafa Rohaninejad, 2018, Arxiv, DOI arXiv:1707.03141; Nallure Balasubramanian V, 2020, PROC 32 C NEURAL INF, P14374; Park E, 2019, P ANN C NEUR INF PRO, P3309; Peng C, 2020, PATTERN RECOGN LETT, V140, P109, DOI 10.1016/j.patrec.2020.09.030; Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Santoro A, 2016, PR MACH LEARN RES, V48; Seungjin Choi, 2018, Arxiv, DOI arXiv:1801.05558; Shmelkov K, 2017, IEEE I CONF COMP VIS, P3420, DOI 10.1109/ICCV.2017.368; Snell J, 2017, ADV NEUR IN, V30; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Wang YX, 2019, IEEE I CONF COMP VIS, P9924, DOI 10.1109/ICCV.2019.01002; Wu Y., 2019, DETECTRON2; Yan XP, 2019, IEEE I CONF COMP VIS, P9576, DOI 10.1109/ICCV.2019.00967; Zhang JT, 2020, IEEE WINT CONF APPL, P1120, DOI 10.1109/WACV45572.2020.9093365; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	46	1	1	3	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9209	9216		10.1109/TPAMI.2021.3124133	http://dx.doi.org/10.1109/TPAMI.2021.3124133			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34727027	Green Accepted, Green Submitted			2022-12-18	WOS:000880661400047
J	Katircioglu, I; Rhodin, H; Constantin, V; Sporri, J; Salzmann, M; Fua, P				Katircioglu, Isinsu; Rhodin, Helge; Constantin, Victor; Sporri, Jorg; Salzmann, Mathieu; Fua, Pascal			Self-Supervised Human Detection and Segmentation via Background Inpainting	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image segmentation; Training; Cameras; Optical imaging; Proposals; Optical sensors; Object detection; Self-supervised training; importance sampling; proposal-based detection and segmentation; image inpainting		While supervised object detection and segmentation methods achieve impressive accuracy, they generalize poorly to images whose appearance significantly differs from the data they have been trained on. To address this when annotating data is prohibitively expensive, we introduce a self-supervised detection and segmentation approach that can work with single images captured by a potentially moving camera. At the heart of our approach lies the observation that object segmentation and background reconstruction are linked tasks, and that, for structured scenes, background regions can be re-synthesized from their surroundings, whereas regions depicting the moving object cannot. We encode this intuition into a self-supervised loss function that we exploit to train a proposal-based segmentation network. To account for the discrete nature of the proposals, we develop a Monte Carlo-based training strategy that allows the algorithm to explore the large space of object proposals. We apply our method to human detection and segmentation in images that visually depart from those of standard benchmarks and outperform existing self-supervised methods.	[Katircioglu, Isinsu; Constantin, Victor; Salzmann, Mathieu; Fua, Pascal] Ecole Polytech Fed Lausanne, Comp Vis Lab, CH-1015 Lausanne, Switzerland; [Rhodin, Helge] Univ British Columbia, Comp Vis Lab, Vancouver, BC V6T 1Z4, Canada; [Rhodin, Helge] Univ British Columbia, Imager Lab, Vancouver, BC V6T 1Z4, Canada; [Sporri, Jorg] Univ Zurich, Balgrist Univ Hosp, Dept Orthopaed, CH-8006 Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; University of British Columbia; University of British Columbia; University of Zurich	Katircioglu, I (corresponding author), Ecole Polytech Fed Lausanne, Comp Vis Lab, CH-1015 Lausanne, Switzerland.	isinsu.katircioglu@epfl.ch; rhodin@cs.ubc.ca; victor.constantin@epfl.ch; Joerg.Spoerri@balgrist.ch; mathieu.salzmann@epfl.ch; pascal.fua@epfl.ch		Rhodin, Helge/0000-0003-2692-0801; Sporri, Jorg/0000-0002-0353-1021; Salzmann, Mathieu/0000-0002-8347-8637	Swiss National Science Foundation (SNSF)	Swiss National Science Foundation (SNSF)(Swiss National Science Foundation (SNSF))	This work was supported in part by the Swiss National Science Foundation (SNSF).	Arandjelovic R., 2019, ARXIV; Baque P, 2017, IEEE I CONF COMP VIS, P271, DOI 10.1109/ICCV.2017.38; Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613; Benny Yaniv, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P514, DOI 10.1007/978-3-030-58574-7_31; Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P777, DOI 10.1007/978-3-030-58536-5_46; Bielski A, 2019, ADV NEUR IN, V32; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Chen M, 2019, ADV NEUR IN, V32; Cheng JC, 2017, IEEE I CONF COMP VIS, P686, DOI 10.1109/ICCV.2017.81; Cho M, 2015, PROC CVPR IEEE, P1201, DOI 10.1109/CVPR.2015.7298724; Crawford E, 2019, AAAI CONF ARTIF INTE, P3412; Croitoru I, 2019, INT J COMPUT VISION, V127, P1279, DOI 10.1007/s11263-019-01183-3; Eslami SM, 2016, NEURIPS, V1; Faktor A., 2014, PROC BRIT MACH VIS C, DOI [10.5244/C.28.21, DOI 10.5244/C.28.21]; GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552; Haller E, 2017, IEEE I CONF COMP VIS, P5095, DOI 10.1109/ICCV.2017.544; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu YT, 2018, LECT NOTES COMPUT SC, V11205, P813, DOI 10.1007/978-3-030-01246-5_48; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Jaderberg M, 2015, ADV NEUR IN, V28; Jain SD, 2017, PROC CVPR IEEE, P2117, DOI 10.1109/CVPR.2017.228; Jang E., 2017, ICLR; KAHN H, 1953, J OPER RES SOC AM, V1, P263, DOI 10.1287/opre.1.5.263; Keuper M, 2015, IEEE I CONF COMP VIS, P3271, DOI 10.1109/ICCV.2015.374; Koh YJ, 2017, PROC CVPR IEEE, P7417, DOI 10.1109/CVPR.2017.784; Koller D., 2009, PROBABILISTC GRAPHIC; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Kosiorek A., 2018, PROC INT C NEURAL IN, P8615; Lee YJ, 2011, IEEE I CONF COMP VIS, P1995, DOI 10.1109/ICCV.2011.6126471; Leordeanu M., 2020, UNSUPERVISED LEARNIN; Li SY, 2018, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2018.00683; Li SY, 2018, LECT NOTES COMPUT SC, V11207, P215, DOI 10.1007/978-3-030-01219-9_13; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin Z., 2020, PROC INT C LEARN REP; Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374; Lu Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P490, DOI 10.1007/978-3-030-58568-6_29; Mingmin Zhen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P445, DOI 10.1007/978-3-030-58583-9_27; Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223; Pathak D, 2017, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2017.638; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85; Perazzi F, 2015, IEEE I CONF COMP VIS, P3227, DOI 10.1109/ICCV.2015.369; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Rhodin H, 2018, LECT NOTES COMPUT SC, V11214, P765, DOI 10.1007/978-3-030-01249-6_46; Rhodin H, 2019, PROC CVPR IEEE, P7695, DOI 10.1109/CVPR.2019.00789; Rhodin H, 2018, PROC CVPR IEEE, P8437, DOI 10.1109/CVPR.2018.00880; Ronneberger O., 2015, P MED IM COMP ASS IN, P234, DOI DOI 10.1007/978-3-319-24574-4_28; Rubinstein RY, 2016, SIMULATION MONTE CAR, DOI DOI 10.1002/9781118631980; Russell C, 2014, LECT NOTES COMPUT SC, V8695, P583, DOI 10.1007/978-3-319-10584-0_38; Seong Hongje, 2020, ECCV, P629, DOI DOI 10.1007/978-3-030-58542-6_38; Seonguk Seo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P208, DOI 10.1007/978-3-030-58555-6_13; Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44; Stretcu O., 2015, PROC BRIT MACH VIS C; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Tokmakov P, 2017, IEEE I CONF COMP VIS, P4491, DOI 10.1109/ICCV.2017.480; Tokmakov P, 2017, PROC CVPR IEEE, P531, DOI 10.1109/CVPR.2017.64; Wang C, 2019, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR.2019.00346; Wang WG, 2019, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2019.00318; Wang XG, 2019, PROC CVPR IEEE, P8868, DOI [10.1109/CVPR.2019.00908, 10.1109/CVPR.2019.00267]; Wei XS, 2019, PATTERN RECOGN, V88, P113, DOI 10.1016/j.patcog.2018.10.022; Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393; Xiankai Lu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P661, DOI 10.1007/978-3-030-58580-8_39; Xiankai Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8957, DOI 10.1109/CVPR42600.2020.00898; Yang YC, 2019, PROC CVPR IEEE, P879, DOI 10.1109/CVPR.2019.00097; Yang Z, 2019, IEEE I CONF COMP VIS, P931, DOI 10.1109/ICCV.2019.00102; Yu Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P735, DOI 10.1007/978-3-030-58607-2_43; Zongxin Yang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P332, DOI 10.1007/978-3-030-58558-7_20	72	1	1	1	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9574	9588		10.1109/TPAMI.2021.3123902	http://dx.doi.org/10.1109/TPAMI.2021.3123902			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34714741	Green Published			2022-12-18	WOS:000880661400072
J	Kobler, E; Effland, A; Kunisch, K; Pock, T				Kobler, Erich; Effland, Alexander; Kunisch, Karl; Pock, Thomas			Total Deep Variation: A Stable Regularization Method for Inverse Problems	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Inverse problems; Optimal control; Training; Noise reduction; Task analysis; Stability analysis; Trajectory; Convolutional neural networks; gradient flow; image restoration; inverse problems; mean-field optimal control problem; medical imaging; variational methods	IMAGE; PROJECTION; FRAMEWORK; CT	Various problems in computer vision and medical imaging can be cast as inverse problems. A frequent method for solving inverse problems is the variational approach, which amounts to minimizing an energy composed of a data fidelity term and a regularizer. Classically, handcrafted regularizers are used, which are commonly outperformed by state-of-the-art deep learning approaches. In this work, we combine the variational formulation of inverse problems with deep learning by introducing the data-driven general-purpose total deep variation regularizer. In its core, a convolutional neural network extracts local features on multiple scales and in successive blocks. This combination allows for a rigorous mathematical analysis including an optimal control formulation of the training problem in a mean-field setting and a stability analysis with respect to the initial values and the parameters of the regularizer. In addition, we experimentally verify the robustness against adversarial attacks and numerically derive upper bounds for the generalization error. Finally, we achieve state-of-the-art results for several imaging tasks.	[Kobler, Erich] Univ Linz, Inst Comp Graph, A-4040 Linz, Austria; [Effland, Alexander] Univ Bonn, Inst Appl Math, D-53113 Bonn, Germany; [Kunisch, Karl] Karl Franzens Univ Graz, Inst Math & Sci Comp, A-8010 Graz, Austria; [Pock, Thomas] Graz Univ Technol, Inst Comp Graph & Vis, A-8010 Graz, Austria	Johannes Kepler University Linz; University of Bonn; University of Graz; Graz University of Technology	Kobler, E (corresponding author), Univ Linz, Inst Comp Graph, A-4040 Linz, Austria.	erich.kobler@jku.at; effland@iam.uni-bonn.de; karl.kunischC@unigraz.at; pock@icg.tugraz.at			ERC [OCLOC 668998, HOMOVIS 640156]; German Research Foundation under Germany's Excellence Strategy [EXC-2047/1 -390685813, EXC2151 -390873048]	ERC(European Research Council (ERC)European Commission); German Research Foundation under Germany's Excellence Strategy	This work was supported in part by the ERC under Grant HOMOVIS 640156 and in part by ERC advanced Grant OCLOC 668998. Alexander Effland was funded by the German Research Foundation under Germany's Excellence Strategy under Grants-EXC-2047/1 -390685813 and -EXC2151 -390873048.	Aaron Defazio, 2019, Arxiv, DOI arXiv:1811.08839; Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150; Alexander Effland, 2020, Arxiv, DOI arXiv:2011.06539; Ambrosio L, 2008, LECT MATH, P1; Antun V, 2020, P NATL ACAD SCI USA, V117, P30088, DOI 10.1073/pnas.1907377117; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Bredies K, 2010, SIAM J IMAGING SCI, V3, P492, DOI 10.1137/090769521; Calypso Herrera, 2020, Arxiv, DOI arXiv:2004.13135; Chambolle A, 1997, NUMER MATH, V76, P167, DOI 10.1007/s002110050258; Chambolle A, 2019, NUMER MATH, V142, P611, DOI 10.1007/s00211-019-01026-w; Chambolle A, 2016, ACTA NUMER, V25, P161, DOI 10.1017/S096249291600009X; Chan TF, 2003, SIAM J APPL MATH, V63, P564; Chang JHR, 2017, IEEE I CONF COMP VIS, P5889, DOI 10.1109/ICCV.2017.627; Chen GH, 2008, MED PHYS, V35, P660, DOI 10.1118/1.2836423; Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Domke Justin, 2012, INT C ARTIFICIAL INT; Effland A, 2020, J MATH IMAGING VIS, V62, P396, DOI 10.1007/s10851-019-00926-8; Fazlyab M, 2019, ADV NEUR IN, V32; Gilboa G., 2018, ADV COMPUTER VISION; Griswold MA, 2002, MAGN RESON MED, V47, P1202, DOI 10.1002/mrm.10171; Ha S, 2018, IEEE T MED IMAGING, V37, P361, DOI 10.1109/TMI.2017.2741781; Hammernik K, 2018, MAGN RESON MED, V79, P3055, DOI 10.1002/mrm.26977; He XY, 2019, PROC CVPR IEEE, P1732, DOI 10.1109/CVPR.2019.00183; Jia XX, 2019, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2019.00621; Jinggang Huang, 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P541, DOI 10.1109/CVPR.1999.786990; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kobler E., 2020, PROC 6 CT M, P332; Kobler E., 2020, PROC IEEE C COMPUT V, DOI [DOI 10.1109/CVPR42600.2020.00757, 10.1109/CVPR42600.2020.00757]; LeCun Y., 1988, P 1988 CONN MOD SUMM, P21; Lefkimmiatis S, 2017, PROC CVPR IEEE, P5882, DOI 10.1109/CVPR.2017.623; Levin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2833, DOI 10.1109/CVPR.2011.5995309; Li HS, 2020, INVERSE PROBL, V36, DOI 10.1088/1361-6420/ab6d57; Liu XH, 2013, IEEE T IMAGE PROCESS, V22, P5226, DOI 10.1109/TIP.2013.2283400; Lunz S, 2018, ADV NEUR IN, V31; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; McCollough CH, 2017, MED PHYS, V44, pe339, DOI 10.1002/mp.12345; Nesterov Yu. E., 1983, Doklady Akademii Nauk SSSR, V269, P543; Nitzberg M., 1993, LECT NOTES COMPUTER, V662; Pinetz T., 2021, PROC INT C SCALE SPA, P491; Plotz T, 2018, ADV NEUR IN, V31; Romano Y, 2017, SIAM J IMAGING SCI, V10, P1804, DOI 10.1137/16M1102884; Ronneberger O., 2015, P INT C MED IMAG COM, P234, DOI [DOI 10.1007/978-3-319-24574-4_28, DOI 10.48550/ARXIV.1505.04597]; Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Samuel KGG, 2009, PROC CVPR IEEE, P477, DOI 10.1109/CVPRW.2009.5206774; Scaman K, 2018, ADV NEUR IN, V31; Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486; Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Venkatakrishnan S, 2013, IEEE GLOB CONF SIG, P945, DOI 10.1109/GlobalSIP.2013.6737048; Zhang K, 2020, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR42600.2020.00328; Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891; Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zhang R, 2019, PR MACH LEARN RES, V97; Zhao NN, 2016, IEEE T IMAGE PROCESS, V25, P3683, DOI 10.1109/TIP.2016.2567075; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420; Zhuang JT, 2020, ADV NEUR IN, V33	64	1	1	2	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9163	9180		10.1109/TPAMI.2021.3124086	http://dx.doi.org/10.1109/TPAMI.2021.3124086			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34727026				2022-12-18	WOS:000880661400044
J	Lei, JB; Jia, K; Ma, Y				Lei, Jiabao; Jia, Kui; Ma, Yi			Learning and Meshing From Deep Implicit Surface Networks Using an Efficient Implementation of Analytic Marching	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Surface reconstruction; Shape; Faces; Isosurfaces; Surface treatment; Partitioning algorithms; Three-dimensional displays; Generative shape modeling; implicit surface representation; polygon mesh; deep learning; multi-layer perceptron		Reconstruction of object or scene surfaces has tremendous applications in computer vision, computer graphics, and robotics. The topic attracts increased attention with the emerging pipeline of deep learning surface reconstruction, where implicit field functions constructed from deep networks (e.g., multi-layer perceptrons or MLPs) are proposed for generative shape modeling. In this paper, we study a fundamental problem in this context about recovering a surface mesh from an implicit field function whose zero-level set captures the underlying surface. To achieve the goal, existing methods rely on traditional meshing algorithms (e.g., the de-facto standard marching cubes); while promising, they suffer from loss of precision learned in the implicit surface networks, due to the use of discrete space sampling in marching cubes. Given that an MLP with activations of Rectified Linear Unit (ReLU) partitions its input space into a number of linear regions, we are motivated to connect this local linearity with a same property owned by the desired result of polygon mesh. More specifically, we identify from the linear regions, partitioned by an MLP based implicit function, the analytic cells and analytic facesthat are associated with the function's zero-level isosurface. We prove that under mild conditions, the identified analytic faces are guaranteed to connect and form a closed, piecewise planar surface. Based on the theorem, we propose an algorithm of analytic marching, which marches among analytic cells to exactly recover the mesh captured by an implicit surface network. We also show that our theory and algorithm are equally applicable to advanced MLPs with shortcut connections and max pooling. Given the parallel nature of analytic marching, we contribute AnalyticMesh, a software package that supports efficient meshing of implicit surface networks via CUDA parallel computing, and mesh simplification for efficient downstream processing. We apply our method to different settings of generative shape modeling using implicit surface networks. Extensive experiments demonstrate our advantages over existing methods in terms of both meshing accuracy and efficiency. Codes are at https://github.com/Karbo123/AnalyticMesh.	[Lei, Jiabao; Jia, Kui] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Guangdong, Peoples R China; [Ma, Yi] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA	South China University of Technology; University of California System; University of California Berkeley	Jia, K (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Guangdong, Peoples R China.	eejblei@mail.scut.edu.cn; kuijia@scut.edu.cn; yima@eecs.berkeley.edu		Lei, Jiabao/0000-0002-3508-2945	National Natural Science Foundation of China [61771201]; Program for Guangdong Introducing Innovative and Enterpreneurial Teams [2017ZT07X183]; Guangdong R&DKey Project of China [2019B010155001]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Program for Guangdong Introducing Innovative and Enterpreneurial Teams; Guangdong R&DKey Project of China	This work was supported in part by the National Natural Science Foundation of China under Grant 61771201, in part by the Program for Guangdong Introducing Innovative and Enterpreneurial Teams under Grant 2017ZT07X183, and in part by the Guangdong R&DKey Project of China under Grant 2019B010155001.	Angel X. Chang, 2015, Arxiv, DOI arXiv:1512.03012; [Anonymous], STANFORD 3D SCANNING; Artec3d, US; Atzmon M, 2019, ADV NEUR IN, V32; Avis D, 1991, PROC ANN S COMPUT GE, P98; Blinn J. F., 1982, Computer Graphics, V16, DOI 10.1145/965145.801290; Botsch Mario, 2010, POLYGON MESH PROCESS, P3; Carr JC, 1997, IEEE T MED IMAGING, V16, P96, DOI 10.1109/42.552059; Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266; Chen ZQ, 2019, IEEE I CONF COMP VIS, P8489, DOI 10.1109/ICCV.2019.00858; Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609; Chu LY, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1244, DOI 10.1145/3219819.3220063; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Djork-Arn, ICLR 2016; DOI A, 1991, IEICE TRANS COMMUN, V74, P214; Douze M, 2015, QUICKCSG ARBITRARY F, P1; Free3d, US; Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849; Glorot X., 2011, P 14 INT C ART INT S, P315; Gorban AN, 2018, PHILOS T R SOC A, V376, DOI 10.1098/rsta.2017.0237; Gropp A, 2020, PR MACH LEARN RES, V119; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Guennebaud G, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276406, 10.1145/1239451.1239474]; Hart JC, 1996, VISUAL COMPUT, V12, P527, DOI 10.1007/s003710050084; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hinton G., 2015, NIPS WORKSH, P1; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Hu Q, 2022, IEEE T NEUR NET LEAR, V33, P644, DOI 10.1109/TNNLS.2020.3028431; Ju T, 2002, ACM T GRAPHIC, V21, P339; Kolluri R, 2008, ACM T ALGORITHMS, V4, DOI 10.1145/1361192.1361195; Lange M., 2014, PROC EUR S ARTIF NEU, P271; Lei J, 2020, PROC INT C MACH LEAR, P5789; Li S, 2021, IEEE T PATTERN ANAL, V43, P1352, DOI 10.1109/TPAMI.2019.2948352; Liepa P., 2003, Symposium on Geometry Processing, P200; Littwin G, 2019, IEEE I CONF COMP VIS, P1824, DOI 10.1109/ICCV.2019.00191; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459; Montufar G, 2014, ADV NEUR IN, V27; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Nishimura H., 1985, Transactions of the Institute of Electronics and Communication Engineers of Japan, Part D, VJ68D, P718; Orlik P., 1992, ARRANGEMENTS HYPERPL; Pan JY, 2019, IEEE I CONF COMP VIS, P9963, DOI 10.1109/ICCV.2019.01006; Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025; Pascanu R., 2014, ICLR; Ramachandran Prajit, 2018, INT C LEARN REPR ICL, P2; Russ S., 1980, HIST MATH, V7, P156, DOI DOI 10.1016/0315-0860(80)90036-1; Sin FS, 2013, COMPUT GRAPH FORUM, V32, P36, DOI 10.1111/j.1467-8659.2012.03230.x; Sitzmann Vincent, 2020, ARXIV200609661, V33, P7462; Tang JP, 2019, PROC CVPR IEEE, P4536, DOI 10.1109/CVPR.2019.00467; Davies T, 2020, Arxiv, DOI arXiv:2009.09808; Tretschk Edgar, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P293, DOI 10.1007/978-3-030-58517-4_18; Turk G, 1999, COMP GRAPH, P335, DOI 10.1145/311535.311580; Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4; Wang WY, 2019, ADV NEUR IN, V32; Wyvill G., 1986, Visual Computer, V2, P227, DOI 10.1007/BF01900346; Xu H., 2014, PROC GRAPH INT, P35; ZASLAVSKY T, 1975, MEM AM MATH SOC, V1, P1	61	1	1	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					10068	10086		10.1109/TPAMI.2021.3135007	http://dx.doi.org/10.1109/TPAMI.2021.3135007			19	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34905490	Green Submitted			2022-12-18	WOS:000880661400106
J	Li, MY; Lin, J; Ding, YY; Liu, ZJ; Zhu, JY; Han, S				Li, Muyang; Lin, Ji; Ding, Yaoyao; Liu, Zhijian; Zhu, Jun-Yan; Han, Song			GAN Compression: Efficient Architectures for Interactive Conditional GANs	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Training; Computational modeling; Generative adversarial networks; Computer architecture; Generators; Image coding; Image edge detection; GAN compression; GAN; compression; image-to-image translation; distillation; neural architecture search		Conditional Generative Adversarial Networks (cGANs) have enabled controllable image synthesis for many vision and graphics applications. However, recent cGANs are 1-2 orders of magnitude more compute-intensive than modern recognition CNNs. For example, GauGAN consumes 281G MACs per image, compared to 0.44G MACs for MobileNet-v3, making it difficult for interactive deployment. In this work, we propose a general-purpose compression framework for reducing the inference time and model size of the generator in cGANs. Directly applying existing compression methods yields poor performance due to the difficulty of GAN training and the differences in generator architectures. We address these challenges in two ways. First, to stabilize GAN training, we transfer knowledge of multiple intermediate representations of the original model to its compressed model and unify unpaired and paired learning. Second, instead of reusing existing CNN designs, our method finds efficient architectures via neural architecture search. To accelerate the search process, we decouple the model training and search via weight sharing. Experiments demonstrate the effectiveness of our method across different supervision settings, network architectures, and learning methods. Without losing image quality, we reduce the computation of CycleGAN by 21x, Pix2pix by 12x, MUNIT by 29x, and GauGAN by 9x, paving the way for interactive image synthesis.	[Li, Muyang; Zhu, Jun-Yan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Lin, Ji; Liu, Zhijian; Han, Song] MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA; [Ding, Yaoyao] Univ Toronto, Toronto, ON M5S 1A1, Canada	Carnegie Mellon University; Massachusetts Institute of Technology (MIT); University of Toronto	Li, MY (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	muyangli@cs.cmu.edu; jilin@mit.edu; yaoyao.ding@mail.utoronto.ca; zhijian@mit.edu; junyanz@cs.cmu.edu; songhan@mit.edu			National Science Foundation; MIT-IBM Watson AI Lab; Adobe; Samsung	National Science Foundation(National Science Foundation (NSF)); MIT-IBM Watson AI Lab(International Business Machines (IBM)); Adobe; Samsung(Samsung)	This work was supported in part by National Science Foundation, MIT-IBM Watson AI Lab, Adobe, and Samsung, for supporting this research.	Aberman K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322999; Alex Gain, 2019, Arxiv, DOI arXiv:1902.00159; Andrew G. Howard, 2017, Arxiv, DOI arXiv:1704.04861; Azadi S., 2019, PROC INT C LEARN REP; Bender G, 2018, PR MACH LEARN RES, V80; Brock A., 2019, PROC INT C LEARN REP; Cai H., 2020, ICLR, P1; Cai H, 2020, IEEE MICRO, V40, P75, DOI 10.1109/MM.2019.2953153; Cai Han, 2019, INT C LEARN REPR; Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603; Chen GB, 2017, ADV NEUR IN, V30; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen YT, 2018, AAAI CONF ARTIF INTE, P2852; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Fu YG, 2020, PR MACH LEARN RES, V119; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Han S., 2016, P 4 INT C LEARN REPR, P1; Han S, 2015, ADV NEUR IN, V28; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Hensel M, 2017, ADV NEUR IN, V30; Hinton G., 2015, NIPS WORKSH, P1; Hou L, 2021, AAAI CONF ARTIF INTE, V35, P7746; Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140; Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jin Q., 2021, PROC IEEECVF C COMPU, p13 600; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kim T, 2017, PR MACH LEARN RES, V70; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3; Lin J, 2021, PROC CVPR IEEE, P14981, DOI 10.1109/CVPR46437.2021.01474; Lin J, 2017, ADV NEUR IN, V30; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Liu H., 2019, PROC INT C LEARN REP; Liu Hanxiao, 2018, ARXIV180609055; Liu MY, 2017, ADV NEUR IN, V30; Liu YC, 2021, PROC CVPR IEEE, P12151, DOI 10.1109/CVPR46437.2021.01198; Liu ZC, 2019, IEEE I CONF COMP VIS, P3295, DOI [10.1109/ICCV.2019.00339, 10.1109/ICCV.2019.00339D\]; Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Luo P, 2016, AAAI CONF ARTIF INTE, P3560; Ma RX, 2021, SCI PROGRAMMING-NETH, V2021, DOI 10.1155/2021/5573751; Mehdi Mirza, 2014, Arxiv, DOI arXiv:1411.1784; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Polino Antonio, 2018, P 6 INT C LEARN REPR; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Reed S, 2016, PR MACH LEARN RES, V48; Ronneberger O., 2015, P MED IM COMP ASS IN, P234, DOI DOI 10.1007/978-3-319-24574-4_28; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Sangkloy P, 2017, PROC CVPR IEEE, P6836, DOI 10.1109/CVPR.2017.723; Shaham TR, 2021, PROC CVPR IEEE, P14877, DOI 10.1109/CVPR46437.2021.01464; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Shu H, 2019, IEEE I CONF COMP VIS, P3234, DOI 10.1109/ICCV.2019.00333; Simonyan K., 2015, P INT C LEARN REPR, P1, DOI DOI 10.48550/ARXIV.1409.1556; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Taigman Yaniv, 2017, 5 INT C LEARN REPR I; Tianhong Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14627, DOI 10.1109/CVPR42600.2020.01465; Wang Haoyang, 2020, ECCV; Wang K, 2019, PROC CVPR IEEE, P8604, DOI 10.1109/CVPR.2019.00881; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang TC, 2018, ADV NEUR IN, V31; Wei SE, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323030; Wen W, 2016, ADV NEUR IN, V29; Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754; Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32; Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75; Zagoruyko S., 2017, P INT C LEARN REPR, P1; Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhu Chenzhuo, 2017, ICLR; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhuang ZW, 2018, ADV NEUR IN, V31; Zoph B., 2017, P1	85	1	1	6	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9331	9346		10.1109/TPAMI.2021.3126742	http://dx.doi.org/10.1109/TPAMI.2021.3126742			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34752389	Green Submitted, Green Published			2022-12-18	WOS:000880661400056
J	Monfort, M; Pan, BW; Ramakrishnan, K; Andonian, A; McNamara, BA; Lascelles, A; Fan, QF; Gutfreund, D; Feris, RS; Oliva, A				Monfort, Mathew; Pan, Bowen; Ramakrishnan, Kandan; Andonian, Alex; McNamara, Barry A.; Lascelles, Alex; Fan, Quanfu; Gutfreund, Dan; Feris, Rogerio Schmidt; Oliva, Aude			Multi-Moments in Time: Learning and Interpreting Models for Multi-Action Video Understanding	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Visualization; Annotations; Training; Analytical models; Three-dimensional displays; Semantics; Convolutional neural networks; Computer vision; machine learning; video; vision and scene understanding; benchmarking; multi-modal recognition; modeling from video; methods of data collection; neural nets		Videos capture events that typically contain multiple sequential, and simultaneous, actions even in the span of only a few seconds. However, most large-scale datasets built to train models for action recognition in video only provide a single label per video. Consequently, models can be incorrectly penalized for classifying actions that exist in the videos but are not explicitly labeled and do not learn the full spectrum of information present in each video in training. Towards this goal, we present the Multi-Moments in Time dataset (M-MiT) which includes over two million action labels for over one million three second videos. This multi-label dataset introduces novel challenges on how to train and analyze models for multi-action detection. Here, we present baseline results for multi-action recognition using loss functions adapted for long tail multi-label learning, provide improved methods for visualizing and interpreting models trained for multi-label action detection and show the strength of transferring models trained on M-MiT to smaller datasets.	[Monfort, Mathew; Pan, Bowen; Andonian, Alex; McNamara, Barry A.; Lascelles, Alex; Oliva, Aude] MIT, CSAIL, Cambridge, MA 02139 USA; [Monfort, Mathew; Ramakrishnan, Kandan; Fan, Quanfu; Gutfreund, Dan; Feris, Rogerio Schmidt; Oliva, Aude] MIT, IBM Watson AI Lab, Cambridge, MA 02142 USA; [Fan, Quanfu; Gutfreund, Dan; Feris, Rogerio Schmidt] IBM Res, Cambridge, MA 02142 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); International Business Machines (IBM)	Monfort, M (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	mmonfort@mit.edu; bpan@csail.mit.edu; krama@ibm.com; alexlasc@mit.edu; barryam3@mit.edu; andonian@mit.edu; qfan@us.ibm.com; dgutfre@us.ibm.com; rsferis@us.ibm.com; oliva@mit.edu			MIT-IBM Watson AI Lab; Nexplore; Woodside; Google; SystemsThatLearn@CSAIL award; Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) [D17PC00341]	MIT-IBM Watson AI Lab(International Business Machines (IBM)); Nexplore; Woodside; Google(Google Incorporated); SystemsThatLearn@CSAIL award; Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC)	This work was supported by the MIT-IBM Watson AI Lab and its member companies, Nexplore and Woodside, Google faculty award and SystemsThatLearn@CSAIL award (to A.O), as well as the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) under Grant D17PC00341.	Abu-El-Haija S., 2016, ARXIV; Andrew Zisserman, 2017, Arxiv, DOI arXiv:1705.06950; Bau D., 2019, PROC INT C LEARN REP; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Bell S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462002; Boopathy A, 2020, PR MACH LEARN RES, V119; Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254; Damen D, 2018, LECT NOTES COMPUT SC, V11208, P753, DOI 10.1007/978-3-030-01225-0_44; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Elisseeff A, 2002, ADV NEUR IN, V14, P681; Esser Patrick, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9220, DOI 10.1109/CVPR42600.2020.00924; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622; Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633; Jiang Y.-G., 2014, THUMOS CHALLENGE ACT; Kuehne H, 2013, HIGH PERFORMANCE COMPUTING IN SCIENCE AND ENGINEERING '12: TRANSACTIONS OF THE HIGH PERFORMANCE COMPUTING CENTER, STUTTGART (HLRS) 2012, P571, DOI 10.1007/978-3-642-33374-3_41; Li YC, 2017, PROC CVPR IEEE, P1837, DOI 10.1109/CVPR.2017.199; Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Mikolov T., 2013, ARXIV; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Monfort M, 2020, IEEE T PATTERN ANAL, V42, P502, DOI 10.1109/TPAMI.2019.2901464; Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31; Simonyan K, 2014, ADV NEUR IN, V27; Soomro K., 2012, CRCVTR1201; Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251; Weston Jason, 2011, 22 INT JOINT C ART I; Yang H, 2016, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2016.37; Yeung S, 2018, INT J COMPUT VISION, V126, P375, DOI 10.1007/s11263-017-1013-y; Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605; Zhang ML, 2006, IEEE T KNOWL DATA EN, V18, P1338, DOI 10.1109/TKDE.2006.162; Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0; Zhou BL, 2019, IEEE T PATTERN ANAL, V41, P2131, DOI 10.1109/TPAMI.2018.2858759; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319	40	1	1	3	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9434	9445		10.1109/TPAMI.2021.3126682	http://dx.doi.org/10.1109/TPAMI.2021.3126682			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34752386	Green Submitted			2022-12-18	WOS:000880661400062
J	Nie, FP; Zhao, XW; Wang, R; Li, XL				Nie, Feiping; Zhao, Xiaowei; Wang, Rong; Li, Xuelong			Fast Locality Discriminant Analysis With Adaptive Manifold Embedding	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Dimensionality reduction; Principal component analysis; Feature extraction; Manifolds; Null space; Covariance matrices; Task analysis; Locality discriminant analysis; Time cost; Anchor-based strategy; Manifold structure of data	DIMENSIONALITY REDUCTION; RECOGNITION; PCA; FRAMEWORK; LDA	Linear discriminant analysis (LDA) has been proven to be effective in dimensionality reduction. However, the performance of LDA depends on the consistency assumption of the global structure and the local structure. Some work extended LDA along this line of research and proposed local formulations of LDA. Unfortunately, the learning scheme of these algorithms is suboptimal in that the intrinsic relationship between data points is pre-learned in the original space, which is usually affected by the noise and redundant features. Besides, the time cost is relatively high. To alleviate these drawbacks, we propose a Fast Locality Discriminant Analysis framework (FLDA), which has three advantages: (1) It can divide a non-Gaussian distribution class into many sub-blocks that obey Gaussian distributions by using the anchor-based strategy. (2) It captures the manifold structure of data by learning the fuzzy membership relationship between data points and the corresponding anchor points, which can reduce computation time. (3) The weights between data points and anchor points are adaptively updated in the subspace where the irrelevant information and the noise in high-dimensional space have been effectively suppressed. Extensive experiments on toy data sets, UCI benchmark data sets and imbalanced data sets demonstrate the efficiency and effectiveness of the proposed method.	[Nie, Feiping] Northwestern Polytech Univ, Sch Comp Sci, Sch Artificial Intelligence Opt & Elect iOPEN, Key Lab Intelligent Interact & Applicat,Minist In, Xian 710072, Shaanxi, Peoples R China; [Zhao, Xiaowei] Northwestern Polytech Univ, Sch Comp Sci, Sch Artificial Intelligence Opt & Elect iOPEN, Xian 710072, Shaanxi, Peoples R China; [Wang, Rong; Li, Xuelong] Northwestern Polytech Univ, Minist Ind & Informat Technol, Sch Artificial Intelligence Opt & Elect iOPEN, Key Lab Intelligent Interact & Applicat, Xian 710072, Peoples R China	Northwestern Polytechnical University; Northwestern Polytechnical University; Northwestern Polytechnical University	Nie, FP (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Sch Artificial Intelligence Opt & Elect iOPEN, Key Lab Intelligent Interact & Applicat,Minist In, Xian 710072, Shaanxi, Peoples R China.	feipingnie@gmail.com; xiaoweizhao4@gmail.com; wangrong07@tsinghua.org.cn; li@nwpu.edu.cn		Nie, Feiping/0000-0002-0871-6519; Wang, Rong/0000-0001-9240-6726	National Key Research and Development Program of China [2018AAA0101902]; Natural Science Basic Research Program of Shaanxi [2021JM-071]; National Natural Science Foundation of China [62176212, 61936014, 61772427]; Fundamental Research Funds for the Central Universities [G2019KY0501]	National Key Research and Development Program of China; Natural Science Basic Research Program of Shaanxi; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	This work was supported in part by the National Key Research and Development Program of China under Grant 2018AAA0101902, in part by the Natural Science Basic Research Program of Shaanxi under Grant 2021JM-071, in part by the National Natural Science Foundation of China under Grants 62176212, 61936014 and 61772427, and in part by the Fundamental Research Funds for the Central Universities under Grant G2019KY0501.	ABRAMSON N, 1963, IEEE T INFORM THEORY, V9, P257, DOI 10.1109/TIT.1963.1057854; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; Cai D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P714; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chen LF, 2000, PATTERN RECOGN, V33, P1713, DOI 10.1016/S0031-3203(99)00139-9; Duin, 1995, P 9 SCAND C IM AN, V2, P957; Fan ZZ, 2011, IEEE T NEURAL NETWOR, V22, P1119, DOI 10.1109/TNN.2011.2152852; Gui J, 2012, PATTERN RECOGN, V45, P2884, DOI 10.1016/j.patcog.2012.02.005; Han H, 2005, LECT NOTES COMPUT SC, V3644, P878, DOI 10.1007/11538059_91; He XF, 2004, ADV NEUR IN, V16, P153; Hua JL, 2016, NEUROCOMPUTING, V193, P1, DOI 10.1016/j.neucom.2016.01.060; Huang P, 2018, DIGIT SIGNAL PROCESS, V76, P84, DOI 10.1016/j.dsp.2018.02.009; Ji SW, 2008, IEEE T NEURAL NETWOR, V19, P1768, DOI 10.1109/TNN.2008.2002078; Jia YQ, 2009, IEEE T NEURAL NETWOR, V20, P729, DOI 10.1109/TNN.2009.2015760; Kubat M, 1998, MACH LEARN, V30, P195, DOI 10.1023/A:1007452223027; Lai ZH, 2018, IEEE T CYBERNETICS, V48, P2472, DOI 10.1109/TCYB.2017.2740949; Larose C. D, 2014, DISCOVERING KNOWLEDG, V4; Li XL, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2201; Li ZH, 2018, IEEE T NEUR NET LEAR, V29, P6073, DOI 10.1109/TNNLS.2018.2817538; Liu W., 2010, P 27 INT C MACH LEAR, P679; Luo MN, 2017, NEURAL COMPUT, V29, P1124, DOI 10.1162/NECO_a_00937; Martinez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974; Nie FP, 2020, IEEE T CYBERNETICS, V50, P3682, DOI 10.1109/TCYB.2019.2910751; Nie FP, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P977, DOI 10.1145/2623330.2623726; Pang YW, 2019, IEEE T NEUR NET LEAR, V30, P2779, DOI 10.1109/TNNLS.2018.2886317; Pourhabib A, 2015, J MACH LEARN RES, V16, P2695; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Sandhan T, 2014, INT C PATT RECOG, P1449, DOI 10.1109/ICPR.2014.258; Sugiyama M, 2007, J MACH LEARN RES, V8, P1027; Wang XD, 2018, PATTERN RECOGN LETT, V102, P89, DOI 10.1016/j.patrec.2017.12.022; Yan CX, 2021, ACM T INTEL SYST TEC, V12, DOI 10.1145/3418284; Yang H, 2003, PATTERN RECOGN, V36, P563, DOI 10.1016/S0031-3203(02)00048-1; Yang XJ, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3073035; Yang XJ, 2021, J FRANKLIN I, V358, P6462, DOI 10.1016/j.jfranklin.2021.06.009; Yang XJ, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3035677; Ye J., 2005, ADV NEURAL INFORM PR, P1569, DOI DOI 10.5555/2976040.2976237; Ye JP, 2006, J MACH LEARN RES, V7, P1183; Ye JP, 2005, J MACH LEARN RES, V6, P483; Yuan MD, 2019, NEUROCOMPUTING, V356, P228, DOI 10.1016/j.neucom.2019.05.014; Zhao XW, 2020, IEEE T NEUR NET LEAR, V31, P433, DOI 10.1109/TNNLS.2019.2904701; Zhao XW, 2017, NEURAL COMPUT, V29, P1352, DOI 10.1162/NECO_a_00950; Zhu L, 2009, INT CONF BIOMED, P1486	43	1	1	1	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9315	9330		10.1109/TPAMI.2022.3162498	http://dx.doi.org/10.1109/TPAMI.2022.3162498			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	35333712				2022-12-18	WOS:000880661400055
J	Nunes, UM; Demiris, Y				Nunes, Urbano Miguel; Demiris, Yiannis			Robust Event-Based Vision Model Estimation by Dispersion Minimisation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Dispersion; Estimation; Cameras; Optical imaging; Optimization; Computational modeling; Motion estimation; Event-based vision; optimisation framework; optical flow and high-speed motion estimation; dispersion minimisation; real-time motion estimation		We propose a novel Dispersion Minimisation framework for event-based vision model estimation, with applications to optical flow and high-speed motion estimation. The framework extends previous event-based motion compensation algorithms by avoiding computing an optimisation score based on an explicit image-based representation, which provides three main benefits: i) The framework can be extended to perform incremental estimation, i.e., on an event-by-event basis. ii) Besides purely visual transformations in 2D, the framework can readily use additional information, e.g., by augmenting the events with depth, to estimate the parameters of motion models in higher dimensional spaces. iii) The optimisation complexity only depends on the number of events. We achieve this by modelling the event alignment according to candidate parameters and minimising the resultant dispersion, which is computed by a family of suitable entropy-based measures. Data whitening is also proposed as a simple and effective pre-processing step to make the framework's accuracy performance more robust, as well as other event-based motion-compensation methods. The framework is evaluated on several challenging motion estimation problems, including 6-DOF transformation, rotational motion, and optical flow estimation, achieving state-of-the-art performance.	[Nunes, Urbano Miguel; Demiris, Yiannis] Imperial Coll London, Dept Elect & Elect Engn, Personal Robot Lab, London SW7 2AZ, England	Imperial College London	Nunes, UM (corresponding author), Imperial Coll London, Dept Elect & Elect Engn, Personal Robot Lab, London SW7 2AZ, England.	um.nunes@imperial.ac.uk; y.demiris@imperial.ac.uk		Nunes, Urbano Miguel/0000-0002-3083-8301; Demiris, Yiannis/0000-0003-4917-3343	Portuguese Foundation for Science and Technology under Doctoral Grant [SFRH/BD/130732/2017]; Royal Academy of Engineering Chair in Emerging Technologies	Portuguese Foundation for Science and Technology under Doctoral Grant; Royal Academy of Engineering Chair in Emerging Technologies(Royal Academy of Engineering - UK)	Urbano Miguel Nunes was supported by the Portuguese Foundation for Science and Technology under Doctoral Grant with reference SFRH/BD/130732/2017. Yiannis Demiris was supported by a Royal Academy of Engineering Chair in Emerging Technologies. Research presented in this paper was a continuation of Nunes and Demiris [1] and includes results from [1]	Adams A, 2010, COMPUT GRAPH FORUM, V29, P753, DOI 10.1111/j.1467-8659.2009.01645.x; Akolkar H, 2022, IEEE T PATTERN ANAL, V44, P361, DOI 10.1109/TPAMI.2020.3010468; Almatrafi M, 2020, IEEE T PATTERN ANAL, V42, P1547, DOI 10.1109/TPAMI.2020.2986748; Andreopoulos A, 2018, PROC CVPR IEEE, P7532, DOI 10.1109/CVPR.2018.00786; Bardow P, 2016, PROC CVPR IEEE, P884, DOI 10.1109/CVPR.2016.102; Benosman R, 2014, IEEE T NEUR NET LEAR, V25, P407, DOI 10.1109/TNNLS.2013.2273537; Gallego G, 2019, PROC CVPR IEEE, P12272, DOI 10.1109/CVPR.2019.01256; Gallego G, 2018, PROC CVPR IEEE, P3867, DOI 10.1109/CVPR.2018.00407; Gallego G, 2018, IEEE T PATTERN ANAL, V40, P2402, DOI 10.1109/TPAMI.2017.2769655; Gallego G, 2017, IEEE ROBOT AUTOM LET, V2, P632, DOI 10.1109/LRA.2016.2647639; Gehrig D, 2019, IEEE I CONF COMP VIS, P5632, DOI 10.1109/ICCV.2019.00573; Glover A, 2018, IEEE INT CONF ROBOT, P2178; Glover A, 2017, IEEE INT C INT ROBOT, P3769; Haessig G, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-40064-0; Kim H, 2016, LECT NOTES COMPUT SC, V9910, P349, DOI 10.1007/978-3-319-46466-4_21; Lafferty John, 2001, CONDITIONAL RANDOM F, P282; Manderscheid J, 2019, PROC CVPR IEEE, P10237, DOI 10.1109/CVPR.2019.01049; Mitrolchin A, 2018, IEEE INT C INT ROBOT, P6895, DOI 10.1109/IROS.2018.8593805; Mueggler E, 2017, INT J ROBOT RES, V36, P142, DOI 10.1177/0278364917691115; Nunes Urbano Miguel, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P161, DOI 10.1007/978-3-030-58558-7_10; Rebecq H, 2018, INT J COMPUT VISION, V126, P1394, DOI 10.1007/s11263-017-1050-6; Rebecq H, 2017, IEEE ROBOT AUTOM LET, V2, P593, DOI 10.1109/LRA.2016.2645143; Scheerlinck C, 2019, IEEE ROBOT AUTOM LET, V4, P816, DOI 10.1109/LRA.2019.2893427; Sharma B.D., 1975, J MATH SCI-U TOKYO, V10, P28, DOI 10.6092/issn.1973-2201/6621; Sironi A, 2018, PROC CVPR IEEE, P1731, DOI 10.1109/CVPR.2018.00186; Valeiras DR, 2015, IEEE T NEUR NET LEAR, V26, P3045, DOI 10.1109/TNNLS.2015.2401834; Vidal AR, 2018, IEEE ROBOT AUTOM LET, V3, P994, DOI 10.1109/LRA.2018.2793357; Xie Z, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00535; Ye CX, 2020, IEEE INT C INT ROBOT, P5831, DOI 10.1109/IROS45743.2020.9341224; Zhu Alex Zihao, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4465, DOI 10.1109/ICRA.2017.7989517; Zhu A. Z., 2018, IEEE ROBOT AUTOM LET, V3, P2032, DOI [DOI 10.1109/LRA.2018.2800793, 10.1109/lra.2018.2800793]; Zhu AZ, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Zhu AZ, 2019, PROC CVPR IEEE, P989, DOI 10.1109/CVPR.2019.00108; Zhu ALZH, 2018, LECT NOTES COMPUT SC, V11210, P438, DOI 10.1007/978-3-030-01231-1_27	34	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9561	9573		10.1109/TPAMI.2021.3130049	http://dx.doi.org/10.1109/TPAMI.2021.3130049			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34813470				2022-12-18	WOS:000880661400071
J	Pan, JH; Gao, JB; Zheng, WS				Pan, Jia-Hui; Gao, Jibin; Zheng, Wei-Shi			Adaptive Action Assessment	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Adaptation models; Computer architecture; Task analysis; Visualization; Training; Videos; Image quality; Action assessment; action modelling	IMAGE QUALITY ASSESSMENT; ACTION RECOGNITION; SURGICAL SKILLS; VIDEO	Action assessment, the process of evaluating how well an action is performed, is an important task in human action analysis. Action assessment has experienced considerable development based on visual cues; however, existing methods neglect to adaptively learn different architectures for varied types of actions and are therefore limited in achieving high-performance assessment for each type of action. In fact, every type of action has specific evaluation criteria, and human experts are trained for years to correctly evaluate a single type of action. Therefore, it is difficult for a single assessment architecture to achieve high performance for all types of actions. However, manually designing an assessment architecture for each specific type of action is very difficult and impracticable. This work addresses this problem by adaptively designing different assessment architectures for different types of actions, and the proposed approach is therefore called the adaptive action assessment. In order to facilitate our adaptive action assessment by exploiting the specific joint interactions for each type of action, a set of graph-based joint relations is learned for each type of action by means of trainable joint relation graphs built according to the human skeleton structure, and the learned joint relation graphs can visually interpret the assessment process. In addition, we introduce using a normalized mean squared error loss (N-MSE loss) and a Pearson loss that perform automatic score normalization to operate adaptive assessment training. The experiments on four benchmarks for action assessment demonstrate the effectiveness and feasibility of the proposed method. We also demonstrate the visual interpretability of our model by visualizing the details of the assessment process.	[Pan, Jia-Hui; Gao, Jibin] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Peoples R China; [Zheng, Wei-Shi] Sun Yat Sen Univ, Sch Comp Sci & Engn, Key Lab Machine Intelligence & Adv Comp, Minist Educ, Guangzhou 510275, Peoples R China; [Zheng, Wei-Shi] Peng Cheng Lab, Shenzhen 518005, Peoples R China	Sun Yat Sen University; Sun Yat Sen University; Peng Cheng Laboratory	Zheng, WS (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Key Lab Machine Intelligence & Adv Comp, Minist Educ, Guangzhou 510275, Peoples R China.	panjh7@mail2.sysu.edu.cn; gaojb5@mail2.sysu.edu.cn; wszheng@ieee.org		Gao, Jibin/0000-0002-5312-1813	National Key Research and Development Program of China [2018YFB1004903]; NSFC [U21A20471, U1911401, U1811461]; Guangdong NSF Project [2018B030312002]; Guangzhou Research Project [201902010037]; Research Projects of Zhejiang Lab [2019KD0AB03]	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Guangdong NSF Project; Guangzhou Research Project; Research Projects of Zhejiang Lab	This work was supported in part by the National Key Research and Development Program of China under Grant 2018YFB1004903, in part by NSFC under Grants U21A20471, U1911401, and U1811461, in part by Guangdong NSF Project under Grant 2018B030312002, in part by Guangzhou Research Project under Grant 201902010037, and in part by the Research Projects of Zhejiang Lab under Grant 2019KD0AB03.	Bertasius G, 2017, IEEE I CONF COMP VIS, P2196, DOI 10.1109/ICCV.2017.239; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen WJ, 2019, PROC CVPR IEEE, P7234, DOI 10.1109/CVPR.2019.00741; Doughty H., 2021, SKILL DETERMINATION; Doughty H, 2019, PROC CVPR IEEE, P7854, DOI 10.1109/CVPR.2019.00805; Doughty H, 2018, PROC CVPR IEEE, P6057, DOI 10.1109/CVPR.2018.00634; Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256; Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373; Fawaz HI, 2018, LECT NOTES COMPUT SC, V11073, P214, DOI 10.1007/978-3-030-00937-3_25; Forestier G, 2017, LECT NOTES ARTIF INT, V10259, P136, DOI 10.1007/978-3-319-59758-4_15; Gao XB, 2009, IEEE T IMAGE PROCESS, V18, P1409, DOI 10.1109/TIP.2009.2018014; Gao Y, 2014, MICCAI WORKSH M2CAI, V3, P3, DOI DOI 10.1109/MWSYM.2014.6848587; Gattupalli S, 2017, IUI'17: PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P577, DOI 10.1145/3025171.3025213; Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685; He LH, 2014, INT J COMPUT MATH, V91, P2374, DOI 10.1080/00207160.2013.816415; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Hou WL, 2015, IEEE T NEUR NET LEAR, V26, P1275, DOI 10.1109/TNNLS.2014.2336852; Ilg W, 2003, LECT NOTES COMPUT SC, V2781, P523; International Gymnastics Federation (FIG), 2017, CODE POINTS WOMENS A; International Ski Federation (FIS), 2019, JUDG HDB SNOW FREESK; International Swimming Federation (FINA), 2017, FIN DIV RUL; Jain H, 2021, IEEE T CIRC SYST VID, V31, P2260, DOI 10.1109/TCSVT.2020.3017727; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jibin Gao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P222, DOI 10.1007/978-3-030-58577-8_14; Joachims T, 2006, PROC 22 ACM SIGKDD I, P217, DOI DOI 10.1145/1150402.1150429; Jug M, 2003, LECT NOTES COMPUT SC, V2626, P534; Kaiming He, 2020, IEEE Transactions on Pattern Analysis and Machine Intelligence, V42, P386, DOI 10.1109/TPAMI.2018.2844175; Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224; Kristan M., 2007, PROC COMPUT VIS WINT, P11; Lei Q, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9040568; Li ZQ, 2019, IEEE INT CONF COMP V, P4385, DOI 10.1109/ICCVW.2019.00539; Lingling Tao, 2012, Information Processing in Computer-Assisted Interventions. Proceedings Third International Conference, IPCAI 2012, P167, DOI 10.1007/978-3-642-30618-1_17; Liu H., 2019, INT C LEARN REPRESEN; Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306; Malpani Anand, 2014, Information Processing in Computer-Assisted Interventions. 5th International Conference, IPCAI 2014. Proceedings: LNCS 8498, P138, DOI 10.1007/978-3-319-07521-1_15; Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444; Martin JA, 1997, BRIT J SURG, V84, P273, DOI 10.1002/bjs.1800840237; Materzynska J, 2020, PROC CVPR IEEE, P1046, DOI 10.1109/CVPR42600.2020.00113; Monfort M, 2020, IEEE T PATTERN ANAL, V42, P502, DOI 10.1109/TPAMI.2019.2901464; Nekoui M, 2020, IEEE COMPUT SOC CONF, P3941, DOI 10.1109/CVPRW50498.2020.00458; Nekoui M, 2021, IEEE WINT CONF APPL, P394, DOI 10.1109/WACV48630.2021.00044; Paiement A., 2014, PROC BRIT MACH VIS C, P153; Pan JH, 2019, IEEE I CONF COMP VIS, P6340, DOI 10.1109/ICCV.2019.00643; Parmar P, 2019, IEEE WINT CONF APPL, P1468, DOI 10.1109/WACV.2019.00161; Parmar P, 2017, IEEE COMPUT SOC CONF, P76, DOI 10.1109/CVPRW.2017.16; Pirsiavash H, 2014, LECT NOTES COMPUT SC, V8694, P556, DOI 10.1007/978-3-319-10599-4_36; Sanchez J, 2013, IMAGE PROCESS ON LIN, V3, P137, DOI 10.5201/ipol.2013.26; Sharma Y, 2014, PROC WORKSHOP MODEL, V3, P1; Simonyan K, 2014, ADV NEUR IN, V27; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang Z, 2002, INT CONF ACOUST SPEE, P3313; Wang ZH, 2018, INT J COMPUT ASS RAD, V13, P1959, DOI 10.1007/s11548-018-1860-1; Xu CM, 2020, IEEE T CIRC SYST VID, V30, P4578, DOI 10.1109/TCSVT.2019.2927118; Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444; Yang CY, 2020, PROC CVPR IEEE, P588, DOI 10.1109/CVPR42600.2020.00067; Yang X., 2021, PROC BRIT MACH VIS C; Yansong Tang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9836, DOI 10.1109/CVPR42600.2020.00986; Yao T, 2016, PROC CVPR IEEE, P982, DOI 10.1109/CVPR.2016.112; Zhang P, 2015, PROC CVPR IEEE, P2394, DOI 10.1109/CVPR.2015.7298853; Zhang PF, 2020, PROC CVPR IEEE, P1109, DOI 10.1109/CVPR42600.2020.00119; Zhang Q., 2011, PROC INT ACM WORKSHO, P19, DOI DOI 10.1145/1964114.1964119; Zia A, 2018, INT J COMPUT ASS RAD, V13, P731, DOI 10.1007/s11548-018-1735-5; Zia A, 2018, INT J COMPUT ASS RAD, V13, P443, DOI 10.1007/s11548-018-1704-z; Zia A, 2016, INT J COMPUT ASS RAD, V11, P1623, DOI 10.1007/s11548-016-1468-2; Zia A, 2015, LECT NOTES COMPUT SC, V9349, P430, DOI 10.1007/978-3-319-24553-9_53	69	1	1	3	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					8779	8795		10.1109/TPAMI.2021.3126534	http://dx.doi.org/10.1109/TPAMI.2021.3126534			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34752383				2022-12-18	WOS:000880661400020
J	Peng, W; Varanka, T; Mostafa, A; Shi, HL; Zhao, GY				Peng, Wei; Varanka, Tuomas; Mostafa, Abdelrahman; Shi, Henglin; Zhao, Guoying			Hyperbolic Deep Neural Networks: A Survey	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Mathematical models; Manifolds; Numerical models; Deep learning; Task analysis; Geometry; Computational modeling; Neural networks on Riemannian manifold; hyperbolic neural networks; Poincare model; Lorentz model		Recently, hyperbolic deep neural networks (HDNNs) have been gaining momentum as the deep representations in the hyperbolic space provide high fidelity embeddings with few dimensions, especially for data possessing hierarchical structure. Such a hyperbolic neural architecture is quickly extended to different scientific fields, including natural language processing, single-cell RNA-sequence analysis, graph embedding, financial analysis, and computer vision. The promising results demonstrate its superior capability, significant compactness of the model, and a substantially better physical interpretability than its counterpart in the euclidean space. To stimulate future research, this paper presents a comprehensive review of the literature around the neural components in the construction of HDNN, as well as the generalization of the leading deep approaches to the hyperbolic space. It also presents current applications of various tasks, together with insightful observations and identifying open questions and promising future directions.	[Peng, Wei; Varanka, Tuomas; Mostafa, Abdelrahman; Shi, Henglin; Zhao, Guoying] Univ Oulu, Ctr Machine Vis & Signal Anal, FIN-90570 Oulu, Finland	University of Oulu	Zhao, GY (corresponding author), Univ Oulu, Ctr Machine Vis & Signal Anal, FIN-90570 Oulu, Finland.	wei.peng@oulu.fi; Tuomas.Varanka@oulu.fi; Abdelrahman.Mostafa@oulu.fi; henglin.shi@oulu.fi; guoying.zhao@oulu.fi		Zhao, Guoying/0000-0003-3694-206X	Academy of Finland [328115]; Academy Professor Project EmotionAI [336116, 345122]; Project MiGA [316765]; Infotech Oulu	Academy of Finland(Academy of Finland); Academy Professor Project EmotionAI; Project MiGA; Infotech Oulu	This work was supported in part by the Academy of Finland for ICT 2023 Project under Grant 328115, in part by Academy Professor Project EmotionAI under Grants 336116 and 345122, in part by Project MiGA under Grant 316765, and in part by Infotech Oulu.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Adam Santoro, 2018, Arxiv, DOI arXiv:1805.09786; Adcock AB, 2013, IEEE DATA MINING, P1, DOI 10.1109/ICDM.2013.77; Alex Graves, 2013, Arxiv, DOI arXiv:1312.5602; Alexander G. Hauptmann, 2016, Arxiv, DOI arXiv:1610.02984; Alexandros Kalousis, 2019, Arxiv, DOI arXiv:1908.04895; Aly R, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4811; Andrej Cvetkovski, 2016, Arxiv, DOI arXiv:1105.5332; Andrew M. Dai, 2018, Arxiv, DOI arXiv:1806.04313; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Anoop Cherian, 2016, Arxiv, DOI arXiv:1607.05447; Arthur Szlam, 2014, Arxiv, DOI arXiv:1312.6203; Bachmann G, 2020, PR MACH LEARN RES, V119; Balazevic I, 2019, ADV NEUR IN, V32; Barker Roger G, 1955, MIDWEST ITS CHILDREN, P2; Bart van Merrienboer, 2014, Arxiv, DOI arXiv:1406.1078; Becigneul G., 2018, ARXIV; Begelfor E., 2005, SCH ENG COMPUT SCI H, V3, P8; Beltrami E., 1868, ANN MAT PUR APPL, V2, P232; Benjamin Paul Chamberlain, 2017, Arxiv, DOI arXiv:1705.10359; Bernhard Schoelkopf, 2018, Arxiv, DOI arXiv:1802.03761; Bianconi G, 2017, SCI REP-UK, V7, DOI 10.1038/srep41974; Bing Xu, 2015, Arxiv, DOI arXiv:1505.00853; Blasius T, 2018, IEEE ACM T NETWORK, V26, P920, DOI 10.1109/TNET.2018.2810186; Blondel M, 2016, ADV NEUR IN, V29; Bogatskiy A, 2020, PR MACH LEARN RES, V119; Boguna M, 2010, NAT COMMUN, V1, DOI 10.1038/ncomms1063; Bonk M, 2000, GEOM FUNCT ANAL, V10, P266, DOI 10.1007/s000390050009; Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619; Bordes A., 2013, ADV NEURAL INFORM PR, P2787, DOI DOI 10.5555/2999792.2999923; Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242; Bose A. J., 2020, ARXIV; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Brehmer J., 2020, P ADV NEUR INF PROC; Bridson M. R., 2013, GRUNDLEHREN MATH WIS, V319; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Cannon J.W., 1997, FLAVORS GEOMETRY, P59; Chamberlain B. P., 2019, ARXIV; Chami I., 2020, ANN M ASS COMPUTATIO, P6901; Chami I., 2020, P ADV NEUR INF PROC, P714; Chami I, 2019, ADV NEUR IN, V32; Cho H, 2019, PR MACH LEARN RES, V89; Christian Szegedy, 2015, Arxiv, DOI arXiv:1502.03167; Cohen T.S., 2018, INT C LEARN REPR; Cohen-Addad V, 2019, J ACM, V66, DOI 10.1145/3321386; COLLINS AM, 1969, J VERB LEARN VERB BE, V8, P240, DOI 10.1016/S0022-5371(69)80069-1; Conway J. H., 2016, SYMMETRIES THINGS; Coolidge J. L., 1909, ELEMENTS NONEUCLIDEA; Cvetkovski A, 2009, IEEE INFOCOM SER, P1647, DOI 10.1109/INFCOM.2009.5062083; Dai Bo, 2018, P ADV NEUR INF PROC; Danilo Rezende, 2016, Arxiv, DOI arXiv:1611.02304; Dasgupta S, 2016, ACM S THEORY COMPUT, P118, DOI 10.1145/2897518.2897527; Davidson TR, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P856; Defferrard M., 2020, P INT C LEARN REPR; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Devlin J., 2019, P 2019 C N AM CHAPTE, P4171, DOI [10.18653/v1/n19-1423, DOI 10.18653/V1/N19-1423]; Diederik P Kingma, 2014, Arxiv, DOI arXiv:1312.6114; Ding JR, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-22851-4; do Carmo M. P., 1992, RIEMANNIAN GEOMETRY; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Dyubina A., 2001, B LOND MATH SOC; Esteves C, 2018, LECT NOTES COMPUT SC, V11217, P54, DOI 10.1007/978-3-030-01261-8_4; Eugeny Burnaev, 2020, Arxiv, DOI arXiv:2007.07698; Falorsi L, 2019, PR MACH LEARN RES, V89; Fang L, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3946; Fr~echet M. R., 1948, ANN LINSTITUT HENRI, V10, P215; Frank Hutter, 2019, Arxiv, DOI arXiv:1808.05377; Gallot S., 1990, RIEMANNIAN GEOMETRY; Ganea OE, 2018, ADV NEUR IN, V31; Ganea OE, 2018, PR MACH LEARN RES, V80; Gao Cong, 2019, Arxiv, DOI arXiv:1809.01703; Garcia-Perez G, 2016, SCI REP-UK, V6, DOI 10.1038/srep33441; Grattarola D, 2019, APPL SOFT COMPUT, V81, DOI 10.1016/j.asoc.2019.105511; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Gromov Mikhael, 1987, MATH SCI RES I PUBL, V8, P75, DOI 10.1007/978-1-4613-9586-7_3; Gu A., 2018, PROC INT C LEARN REP; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hsieh CK, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P193, DOI 10.1145/3038912.3052639; Huang ZW, 2018, IEEE T PATTERN ANAL, V40, P2827, DOI 10.1109/TPAMI.2017.2776154; Huerta M., 2000, NIH WORKING DEFINITI, P1; Hugo Chu, 2018, Arxiv, DOI arXiv:1812.10408; James Melville, 2020, Arxiv, DOI arXiv:1802.03426; Jascha Sohl-Dickstein, 2017, Arxiv, DOI arXiv:1605.08803; Joan Bruna, 2015, Arxiv, DOI arXiv:1506.05163; Joulin A., 2017, P 15 C EUR CHAPT ASS, P427, DOI DOI 10.18653/V1/E17-2068; Katayama K., 2015, J INF PROCESS, V22, P210; Keil F. C., 2013, SEMANTIC CONCEPTUAL; Keller-Ressel M, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-83328-4; Khrulkov V, 2020, PROC CVPR IEEE, P6417, DOI 10.1109/CVPR42600.2020.00645; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Kleinberg R, 2007, IEEE INFOCOM SER, P1902, DOI 10.1109/INFCOM.2007.221; Klimovskaia A, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-16822-4; Kochurov M., 2020, ARXIV; Krauthgamer R, 2006, ANN IEEE SYMP FOUND, P119; Krioukov D, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.036106; Krioukov D, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.035101; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Law M. T., 2020, P ADV NEUR INF PROC, P1668; Law MT, 2019, PR MACH LEARN RES, V97; Lebanon G., 2004, PROC 21 INT C MACH L; LeCun Y., 1998, CONVOLUTIONAL NETWOR, V3361, P255, DOI DOI 10.1109/IJCNN.2004.1381049; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lee J. M., 2006, RIEMANNIAN MANIFOLDS, DOI 10.1007/b98852; Lee J, 2019, PR MACH LEARN RES, V97; Lin HW, 2017, ENTROPY-SWITZ, V19, DOI 10.3390/e19070299; Liu Q, 2019, ADV NEUR IN, V32; Lou A., 2020, PROC 37 INT C MACH L, P6393; Manolio TA, 2009, NATURE, V461, P747, DOI 10.1038/nature08494; Mardia K. V., 2014, STAT DIRECTIONAL DAT; Mathieu E., 2020, ADV NEURAL INF PROCE, V33, P2503; Mathieu E, 2019, ADV NEUR IN, V32; Max Welling, 2017, Arxiv, DOI arXiv:1609.02907; Monath N, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P714, DOI 10.1145/3292500.3330997; Muscoloni A, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01825-5; Nagano Y, 2019, PR MACH LEARN RES, V97; Nair V, 2010, P 27 INT C MACHINE L, P807; Newman MEJ, 2005, CONTEMP PHYS, V46, P323, DOI 10.1080/00107510500052444; Nickel M, 2018, PR MACH LEARN RES, V80; Nickel M, 2017, ADV NEUR IN, V30; Novembre J, 2008, NATURE, V456, P98, DOI 10.1038/nature07331; O'Neill B., 1983, SEMIRIEMANNIAN GEOME; Ovinnikov I., 2019, ARXIV; Paszke A, 2019, ADV NEUR IN, V32; Peng W, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1432, DOI 10.1145/3394171.3413910; Peng W, 2020, AAAI CONF ARTIF INTE, V34, P2669; Pennec X, 2006, J MATH IMAGING VIS, V25, P127, DOI 10.1007/s10851-006-6228-4; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Perraudin N, 2019, ASTRON COMPUT, V27, P130, DOI 10.1016/j.ascom.2019.03.004; Petar Velickovic, 2018, ARXIV, DOI DOI 10.48550/ARXIV.1710.10903; Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127; Rendle S., 2009, BPR BAYESIAN PERSONA, P452; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Rezende Danilo Jimenez, 2020, ARXIV200202428, P8083; Ringner M, 2008, NAT BIOTECHNOL, V26, P303, DOI 10.1038/nbt0308-303; ROCKAFELLAR RT, 1966, DUKE MATH J, V33, P81, DOI 10.1215/S0012-7094-66-03312-6; Roy D. M., 2007, P ADV NEUR INF PROC, P1185; Said S, 2014, ENTROPY-SWITZ, V16, P4015, DOI 10.3390/e16074015; Sala F, 2018, PR MACH LEARN RES, V80; Sarkar R, 2012, LECT NOTES COMPUT SC, V7034, P355; Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093; Shavitt Y, 2008, IEEE ACM T NETWORK, V16, P25, DOI 10.1109/TNET.2007.899021; Shimizu R., 2021, PROC INT C LEARN REP; Dai SY, 2020, Arxiv, DOI arXiv:2005.00054; Skopek O., 2020, PROC INT C LEARN REP; Snell J, 2017, ADV NEUR IN, V30; Sonthalia R., 2020, P ADV NEUR INF PROC, P845; Suzuki Atsushi, 2018, RIEMANNIAN TRANSE MU; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tadic B, 2018, FRONT PHYS-LAUSANNE, V6, DOI 10.3389/fphy.2018.00007; Tay Y, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P583, DOI 10.1145/3159652.3159664; Tifrea A., 2019, PROC INT C LEARN REP; Tolstikhin I., 2018, P INT C LEARN REPR; Tomczak JM, 2018, PR MACH LEARN RES, V84; Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75; Ungar A. A., 2008, SYNTH LECT MATH STAT; Ungar AA, 2001, COMPUT MATH APPL, V41, P135, DOI 10.1016/S0898-1221(01)85012-4; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vaswani A, 2017, ADV NEUR IN, V30; Vendrov I., 2016, P INT C LEARN REPR; Verbeek K., 2014, PROC 13 ANN S COMPUT, P501; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Vulic I, 2017, COMPUT LINGUIST, V43, P781, DOI 10.1162/COLI_a_00301; Walter JA, 2004, INFORM SYST, V29, P273, DOI 10.1016/j.is.2003.10.002; Weber M., 2020, P ADV NEUR INF PROC; Wiki, NON SPAC; Xu CR, 2020, AAAI CONF ARTIF INTE, V34, P6470; Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444; Yang B., 2015, P INT C LEARN REPR; Yim O, 2015, QUANT METH PSYCHOL, V11, P8, DOI 10.20982/tqmp.11.1.p008; Yu T, 2019, ADV NEUR IN, V32; Zhang Hongyi, 2016, PROC C LEARN THEORY; Zhang Y., 2020, PROC AAAI C ARTIF IN, P4375; Zhou YS, 2021, ISCIENCE, V24, DOI 10.1016/j.isci.2021.102225; Zhou YS, 2018, SCI ADV, V4, DOI 10.1126/sciadv.aaq1458; Zhu S., 2020, P ADV NEUR INF PROC, P7548; Zhu Yudong, 2020, FINDINGS ASS COMPUTA, P1166, DOI DOI 10.18653/V1/2020.FINDINGSEMNLP.104	183	1	1	9	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					10023	10044		10.1109/TPAMI.2021.3136921	http://dx.doi.org/10.1109/TPAMI.2021.3136921			22	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34932472	Green Published, Green Submitted, hybrid			2022-12-18	WOS:000880661400104
J	Shi, WJ; Huang, G; Song, SJ; Wu, C				Shi, Wenjie; Huang, Gao; Song, Shiji; Wu, Cheng			Temporal-Spatial Causal Interpretations for Vision-Based Reinforcement Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Adaptation models; Reliability; Decision making; Perturbation methods; Visualization; Task analysis; Feature extraction; Reinforcement learning; markov decision process; interpretability; attention map; temporal causality		Deep reinforcement learning (RL) agents are becoming increasingly proficient in a range of complex control tasks. However, the agent's behavior is usually difficult to interpret due to the introduction of black-box function, making it difficult to acquire the trust of users. Although there have been some interesting interpretation methods for vision-based RL, most of them cannot uncover temporal causal information, raising questions about their reliability. To address this problem, we present a temporal-spatial causal interpretation (TSCI) model to understand the agent's long-term behavior, which is essential for sequential decision-making. TSCI model builds on the formulation of temporal causality, which reflects the temporal causal relations between sequential observations and decisions of RL agent. Then a separate causal discovery network is employed to identify temporal-spatial causal features, which are constrained to satisfy the temporal causality. TSCI model is applicable to recurrent agents and can be used to discover causal features with high efficiency once trained. The empirical results show that TSCI model can produce high-resolution and sharp attention masks to highlight task-relevant temporal-spatial information that constitutes most evidence about how vision-based RL agents make sequential decisions. In addition, we further demonstrate that our method is able to provide valuable causal interpretations for vision-based RL agents from the temporal perspective.	[Shi, Wenjie; Huang, Gao; Song, Shiji; Wu, Cheng] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Dept Automat, Beijing 100084, Peoples R China; [Huang, Gao] Beijing Acad Artificial Intelligence BAAI, Beijing 100084, Peoples R China	Tsinghua University	Song, SJ (corresponding author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Dept Automat, Beijing 100084, Peoples R China.	shiwj16@mails.tsinghua.edu.cn; gaohuang@tsinghua.edu.cn; shijis@tsinghua.edu.cn; wuc@tsinghua.edu.cn		Huang, Gao/0000-0002-7251-0988; Shi, Wenjie/0000-0001-8677-0011	National Science and Technology Major Project of the Ministry of Science and Technology of China [2018AAA0100701]; Major Research and Development Project of Guangdong Province [2020B1111500002]; National Natural Science Foundation of China [61906106, 62022048]	National Science and Technology Major Project of the Ministry of Science and Technology of China; Major Research and Development Project of Guangdong Province; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by the National Science and Technology Major Project of the Ministry of Science and Technology of China under Grant 2018AAA0100701, in part by the Major Research and Development Project of Guangdong Province under Grant 2020B1111500002, in part by the National Natural Science Foundation of China under Grants 61906106 and 62022048.	Alec Radford, 2017, Arxiv, DOI arXiv:1707.06347; Aleksandr Fedorov, 2015, Arxiv, DOI arXiv:1512.01693; Alharin A, 2020, IEEE ACCESS, V8, P171058, DOI 10.1109/ACCESS.2020.3023394; Ancona M, 2019, PR MACH LEARN RES, V97; Annasamy RM, 2019, AAAI CONF ARTIF INTE, P4561; Arjovsky M, 2017, PR MACH LEARN RES, V70; Arnold A, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P66; Atrey A., 2020, PROC INT C LEARN REP; Bargal SA, 2018, PROC CVPR IEEE, P1440, DOI 10.1109/CVPR.2018.00156; Bastani O, 2018, ADV NEUR IN, V31; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Binder A, 2016, LECT NOTES COMPUT SC, V9887, P63, DOI 10.1007/978-3-319-44781-0_8; Cao QX, 2021, IEEE T PATTERN ANAL, V43, P887, DOI 10.1109/TPAMI.2019.2943456; Chia H, 2019, LAW TECH HUM, V1, P129; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Choi J., 2017, PROC WORKSHOPS 31 AA; Dabkowski P., 2017, P ADV NEUR INF PROC, P6967; Drton M, 2008, J STAT PLAN INFER, V138, P1179, DOI 10.1016/j.jspi.2007.05.035; Eichler M., 2006, PREPRINT; Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371; Fortunato M, 2019, ADV NEUR IN, V32; Ghorbani A, 2019, AAAI CONF ARTIF INTE, P3681; Gong MM, 2015, PR MACH LEARN RES, V37, P1898; GRANGER CWJ, 1969, ECONOMETRICA, V37, P424, DOI 10.2307/1912791; Greydanus S, 2018, PR MACH LEARN RES, V80; Iyer R, 2018, PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18), P144, DOI 10.1145/3278721.3278776; Karpathy A., 2016, WORKSHOP INT C LEARN; Lampinen A. K., 2021, P ADV NEUR INF PROC; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Liu HM, 2021, IEEE T PATTERN ANAL, V43, P1791, DOI 10.1109/TPAMI.2019.2954501; Lundberg SM, 2017, ADV NEUR IN, V30; Madumal P, 2020, AAAI CONF ARTIF INTE, V34, P2493; Manchin A, 2019, COMM COM INF SC, V1143, P223, DOI 10.1007/978-3-030-36802-9_25; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Monfort M, 2020, IEEE T PATTERN ANAL, V42, P502, DOI 10.1109/TPAMI.2019.2901464; Mott A, 2019, ADV NEUR IN, V32; Nikulin D, 2019, IEEE INT CONF COMP V, P4240, DOI 10.1109/ICCVW.2019.00522; Opgen-Rhein R, 2007, BMC BIOINFORMATICS, V8, DOI 10.1186/1471-2105-8-S2-S3; Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88; Puri N., 2020, PROC INT C LEARN REP; Raghu A, 2017, P MACH LEARN HEALTHC, P147; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Ronneberger O., 2015, P INT C MED IMAG COM, P234, DOI [DOI 10.1007/978-3-319-24574-4_28, DOI 10.48550/ARXIV.1505.04597]; Schwab P, 2019, ADV NEUR IN, V32; Schwab P, 2019, AAAI CONF ARTIF INTE, P4846; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shapley L. S., 1953, CONTRIBUTIONS THEORY, V28, P307, DOI [10.3390/atmos10110723, DOI 10.1515/9781400881970-018]; Shi W., 2020, IEEE T PATTERN ANAL; Shrikumar A, 2017, PR MACH LEARN RES, V70; Simonyan K., 2014, WORKSH INT C LEARN R, P1; Stergiou A, 2019, IEEE IMAGE PROC, P1830, DOI 10.1109/ICIP.2019.8803153; Sundararajan M, 2017, PR MACH LEARN RES, V70; Szegedy C., 2014, P INT C LEARN REPR; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Valdes-Sosa PA, 2005, PHILOS T R SOC B, V360, P969, DOI 10.1098/rstb.2005.1654; Wang WG, 2021, IEEE T PATTERN ANAL, V43, P2413, DOI 10.1109/TPAMI.2020.2966453; Wang ZY, 2016, PR MACH LEARN RES, V48; Wiering M, 2012, ADAPT LEARN OPTIM, V12, P1, DOI 10.1007/978-3-642-27645-3; Yu J, 2005, PLOS BIOL, V3, P266, DOI 10.1371/journal.pbio.0030038; Zahavy T, 2016, PR MACH LEARN RES, V48; Zeiler M. D., 2014, EUR C COMP VIS, P818; Zhang J., 2020, PROC INT C MACH LEAR, P11012; Zhang JM, 2018, INT J COMPUT VISION, V126, P1084, DOI 10.1007/s11263-017-1059-x; Zhang RH, 2020, AAAI CONF ARTIF INTE, V34, P6811, DOI 10.1609/aaai.v34i04.6161; Zhou BL, 2019, IEEE T PATTERN ANAL, V41, P2131, DOI 10.1109/TPAMI.2018.2858759	69	1	1	2	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					10222	10235		10.1109/TPAMI.2021.3133717	http://dx.doi.org/10.1109/TPAMI.2021.3133717			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34882545	Green Submitted			2022-12-18	WOS:000880661400117
J	Shi, YJ; Campbell, D; Yu, X; Li, HD				Shi, Yujiao; Campbell, Dylan; Yu, Xin; Li, Hongdong			Geometry-Guided Street-View Panorama Synthesis From Satellite Imagery	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Satellites; Cameras; Image synthesis; Task analysis; Semantics; Generators; Visualization; Novel view synthesis; satellite imagery; street-view imagery		This paper presents a new approach for synthesizing a novel street-view panorama given a satellite image, as if captured from the geographical location at the center of the satellite image. Existing works approach this as an image generation problem, adopting generative adversarial networks to implicitly learn the cross-view transformations, but ignore the geometric constraints. In this paper, we make the geometric correspondences between the satellite and street-view images explicit so as to facilitate the transfer of information between domains. Specifically, we observe that when a 3D point is visible in both views, and the height of the point relative to the camera is known, there is a deterministic mapping between the projected points in the images. Motivated by this, we develop a novel satellite to street-view projection (S2SP) module which learns the height map and projects the satellite image to the ground-level viewpoint, explicitly connecting corresponding pixels. With these projected satellite images as input, we next employ a generator to synthesize realistic street-view panoramas that are geometrically consistent with the satellite images. Our S2SP module is differentiable and the whole framework is trained in an end-to-end manner. Extensive experimental results on two cross-view benchmark datasets demonstrate that our method generates more accurate and consistent images than existing approaches.	[Shi, Yujiao; Li, Hongdong] Australian Natl Univ, Canberra, ACT 0200, Australia; [Campbell, Dylan] Univ Oxford, Oxford OX1 2JD, England; [Yu, Xin] Univ Technol Sydney, Ultimo, NSW 2007, Australia	Australian National University; University of Oxford; University of Technology Sydney	Shi, YJ (corresponding author), Australian Natl Univ, Canberra, ACT 0200, Australia.	Yujiao.Shi@anu.edu.au; dylan@robots.ox.ac.uk; xin.yu@uts.edu.au; Hongdong.Li@anu.edu.au		li, hongdong/0000-0003-4125-1554; Yu, Xin/0000-0002-0269-5649	ARC [190102261]; China Scholarship Council (CSC); Continental AG	ARC(Australian Research Council); China Scholarship Council (CSC)(China Scholarship Council); Continental AG	This work was supported in part by ARC-Discovery under Grant 190102261. Yujiao Shi is a China Scholarship Council (CSC)-funded PhD student to ANU and Dylan Campbell is grateful for support from Continental AG. (Corresponding author: Yujiao Shi.)	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bin Sun, 2019, Arxiv, DOI arXiv:1904.06281; Cai SD, 2019, IEEE I CONF COMP VIS, P8390, DOI 10.1109/ICCV.2019.00848; Chunhua Shen, 2018, Arxiv, DOI arXiv:1810.03272; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dylan Campbell, 2020, Arxiv, DOI arXiv:2005.03860; Flynn J, 2019, PROC CVPR IEEE, P2362, DOI 10.1109/CVPR.2019.00247; Forrest N. Iandola, 2016, Arxiv, DOI arXiv:1602.07360; Hu SX, 2018, PROC CVPR IEEE, P7258, DOI 10.1109/CVPR.2018.00758; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Liu L, 2019, PROC CVPR IEEE, P5607, DOI 10.1109/CVPR.2019.00577; Liu MM, 2018, PROC CVPR IEEE, P4616, DOI 10.1109/CVPR.2018.00485; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu XH, 2020, PROC CVPR IEEE, P856, DOI 10.1109/CVPR42600.2020.00094; Regmi K, 2018, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR.2018.00369; Regmi K, 2019, IEEE I CONF COMP VIS, P470, DOI 10.1109/ICCV.2019.00056; Regmi K, 2019, COMPUT VIS IMAGE UND, V187, DOI 10.1016/j.cviu.2019.07.008; Shi YJ, 2020, AAAI CONF ARTIF INTE, V34, P11990; Tang H, 2019, PROC CVPR IEEE, P2412, DOI 10.1109/CVPR.2019.00252; Toker A., 2021, ARXIV; Tucker R, 2020, PROC CVPR IEEE, P548, DOI 10.1109/CVPR42600.2020.00063; Vo NN, 2016, LECT NOTES COMPUT SC, V9905, P494, DOI 10.1007/978-3-319-46448-0_30; Workman S, 2015, IEEE COMPUT SOC CONF; Workman S, 2015, IEEE I CONF COMP VIS, P3961, DOI 10.1109/ICCV.2015.451; Zhai M, 2017, PROC CVPR IEEE, P4132, DOI 10.1109/CVPR.2017.440; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhou TH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201323	31	1	1	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					10009	10022		10.1109/TPAMI.2022.3140750	http://dx.doi.org/10.1109/TPAMI.2022.3140750			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34995180	Green Submitted			2022-12-18	WOS:000880661400103
J	Sun, T; Shen, H; Chen, TY; Li, DS				Sun, Tao; Shen, Han; Chen, Tianyi; Li, Dongsheng			Adaptive Temporal Difference Learning With Linear Function Approximation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Temporal difference; linear function approximation; adaptive step size; MDP; finite-time convergence	CONVERGENCE	This paper revisits the temporal difference (TD) learning algorithm for the policy evaluation tasks in reinforcement learning. Typically, the performance of TD(0) and TD(lambda) is very sensitive to the choice of stepsizes. Oftentimes, TD(0) suffers from slow convergence. Motivated by the tight link between the TD(0) learning algorithm and the stochastic gradient methods, we develop a provably convergent adaptive projected variant of the TD(0) learning algorithm with linear function approximation that we term AdaTD (0). In contrast to the TD(0), AdaTD(0) is robust or less sensitive to the choice of stepsizes. Analytically, we establish that to reach an epsilon accuracy, the number of iterations needed is (O) over tilde(epsilon(-2) ln(4) 1/epsilon/ln(4) 1/rho) in the general case, where p represents the speed of the underlying Markov chain converges to the stationary distribution. This implies that the iteration complexity of AdaTD(0) is no worse than that of TD (0) in the worst case. When the stochastic semi-gradients are sparse, we provide theoretical acceleration of AdaTD(0). Going beyond TD(0), we develop an adaptive variant of TD(lambda), which is referred to as AdaTD(lambda). Empirically, we evaluate the performance of AdaTD (0) and AdaTD(lambda) on several standard reinforcement learning tasks, which demonstrate the effectiveness of our new approaches.	[Sun, Tao; Li, Dongsheng] Natl Univ Def Technol, Coll Comp, Changsha 410073, Hunan, Peoples R China; [Shen, Han; Chen, Tianyi] Rensselaer Polytech Inst, Dept ECSE, Troy, NY 12180 USA	National University of Defense Technology - China; Rensselaer Polytechnic Institute	Li, DS (corresponding author), Natl Univ Def Technol, Coll Comp, Changsha 410073, Hunan, Peoples R China.	nudtsuntao@163.com; shenh5@rpi.edu; chent18@rpi.edu; dsli@nudt.edu.cn		tao, sun/0000-0001-5024-1900	National Key R&D Program of China [2018YFB0204300]; National Natural Science Foundation of China [61932001, 61906200]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	The work of Tao Sun and Dongsheng Li is sponsored in part by the National Key R&D Program of China under Grant 2018YFB0204300 and the National Natural Science Foundation of China under Grants 61932001 and 61906200.	Abel D, 2016, PR MACH LEARN RES, V48; Baird L., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P30; Bertsekas D. P., 1995, DYNAMIC PROGRAMMING; BERTSEKAS DP, 1989, IEEE T AUTOMAT CONTR, V34, P589, DOI 10.1109/9.24227; Bhandari J., 2018, ARXIV180602450, P1691; Chen Xiaoran, 2018, P INT C LEARN REPR; Chen Z, 2018, I SYMPOS LOW POWER E, P7, DOI [10.1109/TII.2018.2869843, 10.1145/3218603.3218637]; Dalal G, 2018, AAAI CONF ARTIF INTE, P6144; Devraj AM, 2017, ADV NEUR IN, V30; Dozat T., 2016, INCORPORATING NESTER; Duan Y., 2019, P ADV NEUR INF PROC, P4486; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Greg Brockman, 2016, Arxiv, DOI arXiv:1606.01540; Hu B, 2019, ADV NEUR IN, V32; Kingma D.P, P 3 INT C LEARNING R; Lakshminarayanan C., 2018, PROC INT C ARTIF INT, P1347; Levin D.A., 2017, MARKOV CHAINS MIXING, V107, DOI DOI 10.1090/MBK/107; Li L., 2006, ISAIM; Liu B, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P504; Liu MX, 2020, INT J ENVIRON HEAL R, V30, P284, DOI 10.1080/09603123.2019.1593329; Lowe R, 2017, ADV NEUR IN, V30; Liao LF, 2021, Arxiv, DOI arXiv:2106.10022; MENDELSSOHN R, 1982, OPER RES, V30, P62, DOI 10.1287/opre.30.1.62; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Reddi Sashank J., 2018, INT C LEARN REPR; Shani L, 2020, AAAI CONF ARTIF INTE, V34, P5668; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton R. S., 2009, PROC ADV NEURAL INF, P1609; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Sutton RS, 1996, ADV NEUR IN, V8, P1038; Tieleman T., 2012, 6A U TOR; Tsitsiklis JN, 1997, IEEE T AUTOMAT CONTR, V42, P674, DOI 10.1109/9.580874; Van Roy B., 1998, THESIS MIT CAMBRIDGE; Van Roy B, 2006, MATH OPER RES, V31, P234, DOI 10.1287/moor.1060.0188; Vieillard N., 2020, PROC INT C ARTIF INT, P2529; Ward R, 2019, PR MACH LEARN RES, V97; Zou FY, 2019, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2019.01138	46	1	1	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					8812	8824		10.1109/TPAMI.2021.3119645	http://dx.doi.org/10.1109/TPAMI.2021.3119645			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34648431	Green Submitted			2022-12-18	WOS:000880661400022
J	Theagarajan, R; Bhanu, B				Theagarajan, Rajkumar; Bhanu, Bir			Privacy Preserving Defense For Black Box Classifiers Against On-Line Adversarial Attacks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Training; Perturbation methods; Bayes methods; Uncertainty; Deep learning; Privacy; Data models; Adversarial defense; Bayesian uncertainties; black box defense; ensemble of defenses; image purifiers; knowledge distillation; privacy preserving defense		Deep learning models have been shown to be vulnerable to adversarial attacks. Adversarial attacks are imperceptible perturbations added to an image such that the deep learning model misclassifies the image with a high confidence. Existing adversarial defenses validate their performance using only the classification accuracy. However, classification accuracy by itself is not a reliable metric to determine if the resulting image is "adversarial-free". This is a foundational problem for online image recognition applications where the ground-truth of the incoming image is not known and hence we cannot compute the accuracy of the classifier or validate if the image is "adversarial-free" or not. This paper proposes a novel privacy preserving framework for defending Black box classifiers from adversarial attacks using an ensemble of iterative adversarial image purifiers whose performance is continuously validated in a loop using Bayesian uncertainties. The proposed approach can convert a single-step black box adversarial defense into an iterative defense and proposes three novel privacy preserving Knowledge Distillation (KD) approaches that use prior meta-information from various datasets to mimic the performance of the Black box classifier. Additionally, this paper proves the existence of an optimal distribution for the purified images that can reach a theoretical lower bound, beyond which the image can no longer be purified. Experimental results on six public benchmark datasets namely: 1) Fashion-MNIST, 2) CIFAR-10, 3) GTSRB, 4) MIO-TCD, 5) Tiny-ImageNet, and 6) MS-Celeb show that the proposed approach can consistently detect adversarial examples and purify or reject them against a variety of adversarial attacks.	[Theagarajan, Rajkumar; Bhanu, Bir] Univ Calif Riverside, Ctr Res Intelligent Syst, Riverside, CA 92521 USA	University of California System; University of California Riverside	Theagarajan, R (corresponding author), Univ Calif Riverside, Ctr Res Intelligent Syst, Riverside, CA 92521 USA.	rthea001@ucr.edu; bhanu@vislab.ucr.edu			NSF [1911197]; Bourns endowment funds	NSF(National Science Foundation (NSF)); Bourns endowment funds	This work was supported in part by NSF under Grant 1911197 and in part by Bourns endowment funds. The contents of the information do not reflect the position or the policy of the U.S. Government.	Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Ahmed Salem, 2018, Arxiv, DOI arXiv:1806.01246; Alec Radford, 2016, Arxiv, DOI arXiv:1511.06434; Alexander G. Ororbia II, 2017, Arxiv, DOI arXiv:1612.01401; Alexey Kurakin, 2017, Arxiv, DOI arXiv:1611.01236; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ba LJ, 2014, ADV NEUR IN, V27; Baosen Zhang, 2017, Arxiv, DOI arXiv:1703.04318; Bhanu B., 2010, MULTIBIOMETRICS HUMA; Bin Dong, 2019, Arxiv, DOI arXiv:1905.00877; Bing Xu, 2016, Arxiv, DOI arXiv:1511.03034; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Brendel Wieland, 2017, DECISION BASED ADVER; Buckman J., 2018, INT C LEARN REPR; Carlini Nicholas, 2017, P 10 ACM WORKSHOP AR, P3, DOI [10.1145/3128572.3140444, DOI 10.1145/3128572.3140444]; Chen GB, 2017, ADV NEUR IN, V30; Chen P.-Y., 2017, P AISEC CCS 2017, P15, DOI DOI 10.1145/3128572.3140448; Chen ST, 2019, LECT NOTES ARTIF INT, V11051, P52, DOI 10.1007/978-3-030-10925-7_4; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; D.Liu, 2020, Arxiv, DOI arXiv:2005.10486; Das Nilaksh, 2017, ARXIV; David Evans, 2017, Arxiv, DOI arXiv:1704.01155; David Wagner, 2017, Arxiv, DOI arXiv:1608.04644; Deng JK, 2017, IEEE COMPUT SOC CONF, P2006, DOI 10.1109/CVPRW.2017.251; Dhillon G. S., 2018, PROC INT C LEARN REP; Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957; Duo Wang, 2020, Arxiv, DOI arXiv:1710.09282; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Felix Laumann, 2019, Arxiv, DOI arXiv:1806.05978; Fellbaum C., 2012, ENCY APPL LINGUISTIC, DOI 10.1002/9781405198431.wbeal1285/abstract; Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002; Gal Y, 2016, PR MACH LEARN RES, V48; Geoffrey Hinton, 2017, Arxiv, DOI arXiv:1711.09784; Goswami G, 2019, INT J COMPUT VISION, V127, P719, DOI 10.1007/s11263-019-01160-w; Grigorescu S, 2020, J FIELD ROBOT, V37, P362, DOI 10.1002/rob.21918; Guo C., 2019, ARXIV; Guo C., 2018, PROC INT C LEARN REP; Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6; Han Xiao, 2017, Arxiv, DOI arXiv:1708.07747; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Ilyas A, 2018, PR MACH LEARN RES, V80; Jenni S, 2018, LECT NOTES COMPUT SC, V11214, P632, DOI 10.1007/978-3-030-01249-6_38; Jia JY, 2019, PROCEEDINGS OF THE 2019 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'19), P259, DOI 10.1145/3319535.3363201; Jia-Bin Huang, 2017, Arxiv, DOI arXiv:1710.00814; Jin C, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/4512473; Joseph Keshet, 2017, Arxiv, DOI arXiv:1707.05373; K_ohler J. M., 2019, PROC IEEE C COMPUT V, P33; Kaidi Xu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P665, DOI 10.1007/978-3-030-58558-7_39; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kathrin Grosse, 2017, Arxiv, DOI arXiv:1702.06280; Korshunov P, 2019, INT CONF BIOMETR; Krizhevsky A, 2009, LEARNING MULTIPLE LA; KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Li YD, 2019, PR MACH LEARN RES, V97; Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191; Liu K, 2018, LECT NOTES COMPUT SC, V11050, P273, DOI 10.1007/978-3-030-00470-5_13; Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713; Liu Y., 2017, PROC INT C LEARN REP; Lu JJ, 2017, IEEE I CONF COMP VIS, P446, DOI 10.1109/ICCV.2017.56; Luo ZM, 2018, IEEE T IMAGE PROCESS, V27, P5129, DOI 10.1109/TIP.2018.2848705; Madry Aleksander, 2017, ARXIV; Meng DY, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P135, DOI 10.1145/3133956.3134057; Mishra A., 2018, PROC INT C LEARN REP; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mustafa A, 2019, IEEE I CONF COMP VIS, P3384, DOI 10.1109/ICCV.2019.00348; Narodytska N, 2017, IEEE COMPUT SOC CONF, P1310, DOI 10.1109/CVPRW.2017.172; Phan N, 2016, AAAI CONF ARTIF INTE, P1309; Orekondy T, 2019, PROC CVPR IEEE, P4949, DOI 10.1109/CVPR.2019.00509; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Rawat A., 2017, ARXIV; Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587; Ren MY, 2018, PR MACH LEARN RES, V80; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Samangouei P., 2018, P INT C LEARN REPR; Shin H, 2017, ADV NEUR IN, V30; Shokri R, 2017, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2017.41; Song XM, 2018, ACM/SIGIR PROCEEDINGS 2018, P5, DOI 10.1145/3209978.3209996; Song Y, 2019, P COMBUST INST, V37, P667, DOI 10.1016/j.proci.2018.06.115; Stallkamp J, 2012, NEURAL NETWORKS, V32, P323, DOI 10.1016/j.neunet.2012.02.016; Su JW, 2019, IEEE T EVOLUT COMPUT, V23, P828, DOI 10.1109/TEVC.2019.2890858; Tan S, 2018, PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18), P303, DOI 10.1145/3278721.3278725; Tessler C, 2017, AAAI CONF ARTIF INTE, P1553; Theagarajan R, 2020, IEEE COMPUT SOC CONF, P3537, DOI 10.1109/CVPRW50498.2020.00414; Theagarajan R, 2019, PROC CVPR IEEE, P6981, DOI 10.1109/CVPR.2019.00715; Theagarajan R, 2017, IEEE COMPUT SOC CONF, P906, DOI 10.1109/CVPRW.2017.125; Thys S, 2019, IEEE COMPUT SOC CONF, P49, DOI 10.1109/CVPRW.2019.00012; Tramer F., 2018, PROC INT C LEARN REP; Uesato J, 2018, PR MACH LEARN RES, V80; Vaccari C, 2020, SOC MEDIA SOC, V6, DOI 10.1177/2056305120903408; van Oorschot PC, 2003, LECT NOTES COMPUT SC, V2851, P1; Wang F, 2018, LECT NOTES COMPUT SC, V11213, P780, DOI 10.1007/978-3-030-01240-3_47; Wang J, 2019, AAAI CONF ARTIF INTE, P1190; Wang JP, 2019, IEEE T VIS COMPUT GR, V25, P2168, DOI 10.1109/TVCG.2019.2903943; washingtonpost, CALIFORNIA COULD BEC; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Xie CH, 2019, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2019.00059; Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153; Ye SK, 2019, IEEE I CONF COMP VIS, P111, DOI 10.1109/ICCV.2019.00020; Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754; Yu XY, 2019, IEEE T NEUR NET LEAR, V30, P2805, DOI 10.1109/TNNLS.2018.2886017; Zhu XX, 2017, IEEE GEOSC REM SEN M, V5, P8, DOI 10.1109/MGRS.2017.2762307	110	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9503	9520		10.1109/TPAMI.2021.3125931	http://dx.doi.org/10.1109/TPAMI.2021.3125931			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34748482				2022-12-18	WOS:000880661400067
J	Tulsiani, S; Zhou, TH; Efros, AA; Malik, J				Tulsiani, Shubham; Zhou, Tinghui; Efros, Alexei A.; Malik, Jitendra			Multi-View Supervision for Single-View Reconstruction via Differentiable Ray Consistency	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Shape; Image reconstruction; Cameras; Solid modeling; Color; Training data; 3D reconstruction; multi-view supervision; ray consistency		We study the notion of consistency between a 3D shape and a 2D observation and propose a differentiable formulation which allows computing gradients of the 3D shape given an observation from an arbitrary view. We do so by reformulating view consistency using a differentiable ray consistency (DRC) term. We show that this formulation can be incorporated in a learning framework to leverage different types of multi-view observations e.g., foreground masks, depth, color images, semantics etc. as supervision for learning single-view 3D prediction. We present empirical analysis of our technique in a controlled setting. We also show that this approach allows us to improve over existing techniques for single-view reconstruction of objects from the PASCAL VOC dataset.	[Tulsiani, Shubham; Zhou, Tinghui; Efros, Alexei A.; Malik, Jitendra] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Tulsiani, S (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	shubhtuls@eecs.berkeley.edu; tinghuiz@eecs.berkeley.edu; efros@eecs.berkeley.edu; malik@eecs.berkeley.edu			Intel/NSF VEC award [IIS-1539099]; NSF [IIS-1212798]; Berkeley Fellowship	Intel/NSF VEC award; NSF(National Science Foundation (NSF)); Berkeley Fellowship	We thank Saurabh Gupta and David Fouhey for insightful discussions, and the anonymous reviewers for the helpful comments. This work was supported in part by Intel/NSF VEC award IIS-1539099, NSF Award IIS-1212798, and the Berkeley Fellowship to ST. We gratefully acknowledge NVIDIA corporation for the donation of Tesla GPUs used for this research.	Andrey Kurenkov, 2017, Arxiv, DOI arXiv:1708.04672; Angel X. Chang, 2015, Arxiv, DOI arXiv:1512.03012; Bane C, 2017, INT CONF 3D VISION, P412, DOI 10.1109/3DV.2017.00054; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Broadhurst A, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P388, DOI 10.1109/ICCV.2001.937544; Cashman TJ, 2013, IEEE T PATTERN ANAL, V35, P232, DOI 10.1109/TPAMI.2012.68; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; De Bonet J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P418, DOI 10.1109/ICCV.1999.791251; ebay, US; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Fouhey DF, 2015, IEEE I CONF COMP VIS, P1053, DOI 10.1109/ICCV.2015.126; Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45; Gargallo P, 2007, LECT NOTES COMPUT SC, V4844, P373; GIBSON JJ, 1978, LEONARDO, V11, P227, DOI 10.2307/1574154; Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Kar A, 2017, ADV NEUR IN, V30; Kundu A, 2014, LECT NOTES COMPUT SC, V8694, P703, DOI 10.1007/978-3-319-10599-4_45; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; Laine S, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099581; LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735; Liu SB, 2010, PROC CVPR IEEE, P1530, DOI 10.1109/CVPR.2010.5539790; Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951; Rezende DJ, 2016, ADV NEUR IN, V29; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Savinov N, 2016, PROC CVPR IEEE, P5460, DOI 10.1109/CVPR.2016.589; Sayinov N, 2015, PROC CVPR IEEE, P5511, DOI 10.1109/CVPR.2015.7299190; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230; Tulsiani S, 2017, IEEE T PATTERN ANAL, V39, P719, DOI 10.1109/TPAMI.2016.2574713; Tulsiani S, 2015, PROC CVPR IEEE, P1510, DOI 10.1109/CVPR.2015.7298758; Ulusoy AO, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P10, DOI 10.1109/3DV.2015.9; Vicente S, 2014, PROC CVPR IEEE, P41, DOI 10.1109/CVPR.2014.13; Woodford OJ, 2012, LECT NOTES COMPUT SC, V7576, P144, DOI 10.1007/978-3-642-33715-4_11; Wu JJ, 2017, ADV NEUR IN, V30; Wu JJ, 2016, LECT NOTES COMPUT SC, V9910, P365, DOI 10.1007/978-3-319-46466-4_22; Xiang Y, 2014, IEEE WINT CONF APPL, P75, DOI 10.1109/WACV.2014.6836101; Yu F., 2016, P ICLR 2016; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700; Zhu R, 2017, IEEE I CONF COMP VIS, P57, DOI 10.1109/ICCV.2017.16	51	1	1	1	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					8754	8765		10.1109/TPAMI.2019.2898859	http://dx.doi.org/10.1109/TPAMI.2019.2898859			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	30762530	Green Submitted			2022-12-18	WOS:000880661400018
J	Tutsoy, O				Tutsoy, Onder			Pharmacological, Non-Pharmacological Policies and Mutation: An Artificial Intelligence Based Multi-Dimensional Policy Making Algorithm for Controlling the Casualties of the Pandemic Diseases	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Artificial intelligence; Pandemics; Vaccines; Parametric statistics; COVID-19; Computational modeling; Optimization; Artificial intelligence; pandemic; priority and age specific vaccination policy; parametric model; prediction; non-pharmacological policies; mutant virus		Fighting against the pandemic diseases with unique characters requires new sophisticated approaches like the artificial intelligence. This paper develops an artificial intelligence algorithm to produce multi-dimensional policies for controlling and minimizing the pandemic casualties under the limited pharmacological resources. In this respect, a comprehensive parametric model with a priority and age-specific vaccination policy and a variety of non-pharmacological policies are introduced. This parametric model is utilized for constructing an artificial intelligence algorithm by following the exact analogy of the model-based solution. Also, this parametric model is manipulated by the artificial intelligence algorithm to seek for the best multi-dimensional non-pharmacological policies that minimize the future pandemic casualties as desired. The role of the pharmacological and non-pharmacological policies on the uncertain future casualties are extensively addressed on the real data. It is shown that the developed artificial intelligence algorithm is able to produce efficient policies which satisfy the particular optimization targets such as focusing on minimization of the death casualties more than the infected casualties or considering the curfews on the people age over 65 rather than the other non-pharmacological policies. The paper finally analyses a variety of the mutant virus cases and the corresponding non-pharmacological policies aiming to reduce the morbidity and mortality rates.	[Tutsoy, Onder] Adana Alparslan Turkes Sci & Technol Univ, TR-01250 Adana, Turkey	Adana Alparslan Turkes Science & Technology University	Tutsoy, O (corresponding author), Adana Alparslan Turkes Sci & Technol Univ, TR-01250 Adana, Turkey.	otutsoy@atu.edu.tr			TUBITAK [120M793]	TUBITAK(Turkiye Bilimsel ve Teknolojik Arastirma Kurumu (TUBITAK))	This work was supported by TUBITAK under Grant 120M793.	[Anonymous], 2020, DIGITAL TRANSFORMATI; Baba IA, 2021, RESULTS PHYS, V20, DOI 10.1016/j.rinp.2020.103716; Cdr S., 2021, J MAR MED SOC, V22, P9, DOI [10.4103/jmms.jmms, DOI 10.4103/JMMS.JMMS]; Cooper I, 2020, CHAOS SOLITON FRACT, V139, DOI 10.1016/j.chaos.2020.110057; Giordano G., 2020, ARXIV; Gupta R, 2020, DIABETES METAB SYND, V14, P211, DOI 10.1016/j.dsx.2020.03.002; Hsiang S, 2020, NATURE, V585, pE7, DOI [10.1038/s41586-020-2691-0, 10.1038/s41586-020-2404-8]; Iwata K, 2020, J CLIN MED, V9, DOI 10.3390/jcm9040944; Kathakali Biswas, 2020, Arxiv, DOI arXiv:2003.03149; Knipl D. H., 2009, ARXIV; Lalwani S, 2020, CHAOS SOLITON FRACT, V138, DOI 10.1016/j.chaos.2020.109939; Lee S, 2012, B MATH BIOL, V74, P958, DOI 10.1007/s11538-011-9704-y; Meltzer MI, 1999, EMERG INFECT DIS, V5, P659, DOI 10.3201/eid0505.990507; Ren C, 2021, IEEE T CYBERNETICS, V51, P3535, DOI 10.1109/TCYB.2019.2933257; Sahu P, 2020, CUREUS, V12, DOI 10.7759/cureus.7541; Song J., 2020, CYBERPSYCHOL BEHAV, V20, P122; Sooppy K., 2020, RESULTS PHYS, V21; Sun KY, 2020, LANCET DIGIT HEALTH, V2, pE201, DOI 10.1016/S2589-7500(20)30026-1; Tian Y, 2021, IEEE T CYBERNETICS, V51, P3115, DOI 10.1109/TCYB.2020.2979930; Tutsoy O., 2021, BMC MED INFORM DECIS; Tutsoy O, 2020, IEEE ACCESS, V8, P225272, DOI 10.1109/ACCESS.2020.3044929; Wang F, 2018, IEEE T CYBERNETICS, V48, P1839, DOI 10.1109/TCYB.2017.2715980; Yuan Y, 2021, IEEE T CYBERNETICS, V51, P3562, DOI 10.1109/TCYB.2019.2931735; Zhou X., 2020, FORECASTING WORLDWID, DOI [10.1101/2020.03.26.20044289, DOI 10.1101/2020.03.26.20044289]	24	1	1	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9477	9488		10.1109/TPAMI.2021.3127674	http://dx.doi.org/10.1109/TPAMI.2021.3127674			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34767503				2022-12-18	WOS:000880661400065
J	Wang, DB; Zhang, ML; Li, L				Wang, Deng-Bao; Zhang, Min-Ling; Li, Li			Adaptive Graph Guided Disambiguation for Partial Label Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Training; Labeling; Predictive models; Task analysis; Manifolds; Faces; Tagging; Machine learning; weakly supervised learning; partial label learning; label disambiguation		In partial label learning, a multi-class classifier is learned from the ambiguous supervision where each training example is associated with a set of candidate labels among which only one is valid. An intuitive way to deal with this problem is label disambiguation, i.e., differentiating the labeling confidences of different candidate labels so as to try to recover ground-truth labeling information. Recently, feature-aware label disambiguation has been proposed which utilizes the graph structure of feature space to generate labeling confidences over candidate labels. Nevertheless, the existence of noises and outliers in training data makes the graph structure derived from original feature space less reliable. In this paper, a novel partial label learning approach based on adaptive graph guided disambiguation is proposed, which is shown to be more effective in revealing the intrinsic manifold structure among training examples. Other than the sequential disambiguation-then-induction learning strategy, the proposed approach jointly performs adaptive graph construction, candidate label disambiguation and predictive model induction with alternating optimization. Furthermore, we consider the particular human-in-the-loop framework in which a learner is allowed to actively query some ambiguously labeled examples for manual disambiguation. Extensive experiments clearly validate the effectiveness of adaptive graph guided disambiguation for learning from partial label examples.	[Wang, Deng-Bao; Zhang, Min-Ling] Southeast Univ, Sch Comp Sci & Engn, Nanjing 210096, Peoples R China; [Wang, Deng-Bao; Zhang, Min-Ling] Southeast Univ, Minist Educ, Key Lab Comp Network & Informat Integrat, Nanjing, Peoples R China; [Li, Li] Southwest Univ, Coll Comp & Informat Sci, Chongqing 400715, Peoples R China	Southeast University - China; Southeast University - China; Southwest University - China	Zhang, ML (corresponding author), Southeast Univ, Sch Comp Sci & Engn, Nanjing 210096, Peoples R China.	wangdb@seu.edu.cn; zhangml@seu.edu.cn; lily@swu.edu.cn		, lily/0000-0003-4818-8770	National Science Foundation of China [62176055, 61877051]; Postgraduate Research & Practice Innovation Program of Jiangsu Province [KYCX21_0151]; China University S&T Innovation Plan Guided by theMinistry of Education	National Science Foundation of China(National Natural Science Foundation of China (NSFC)); Postgraduate Research & Practice Innovation Program of Jiangsu Province; China University S&T Innovation Plan Guided by theMinistry of Education	This work was supported by the National Science Foundation of China under Grants 62176055 and 61877051, the Postgraduate Research & Practice Innovation Program of Jiangsu Province under Grant KYCX21_0151, and the China University S&T Innovation Plan Guided by theMinistry of Education. (Corresponding author: Min-Ling Zhang.)	Amores J, 2013, ARTIF INTELL, V201, P81, DOI 10.1016/j.artint.2013.06.003; Andrews S., 2002, SUPPORT VECTOR MACHI, P561; Baldridge J., 2004, PROC C EMPIRICAL MET, P9; Bouguelia MR, 2018, INT J MACH LEARN CYB, V9, P1307, DOI 10.1007/s13042-017-0645-0; Briggs F., 2012, P 18 ACM SIGKDD INT, P534, DOI DOI 10.1145/2339530.2339616; Chai J, 2020, IEEE T NEUR NET LEAR, V31, P2594, DOI 10.1109/TNNLS.2019.2933530; Chen CH, 2018, IEEE T PATTERN ANAL, V40, P1653, DOI 10.1109/TPAMI.2017.2723401; Chen YC, 2014, IEEE T INF FOREN SEC, V9, P2076, DOI 10.1109/TIFS.2014.2359642; Cour T, 2011, J MACH LEARN RES, V12, P1501; Cour T, 2009, PROC CVPR IEEE, P919, DOI 10.1109/CVPRW.2009.5206667; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Fang M, 2014, AAAI CONF ARTIF INTE, P1809; Feng L, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2107; Feng L, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2294; Feng L, 2019, AAAI CONF ARTIF INTE, P3542; Garrette Dan, 2013, P 2013 C N AM CHAPT, P138; Gibaja E, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2716262; Gong C, 2018, IEEE T CYBERNETICS, V48, P967, DOI 10.1109/TCYB.2017.2669639; Gonsior Julius, 2020, Discovery Science. 23rd International Conference, DS 2020. Proceedings. Lecture Notes in Artificial Intelligence. Subseries of Lecture Notes in Computer Science (LNAI 12323), P34, DOI 10.1007/978-3-030-61527-7_3; Guillaumin M, 2010, LECT NOTES COMPUT SC, V6311, P634, DOI 10.1007/978-3-642-15549-9_46; Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944; Hu P., 2019, PROC 7 INT C LEARN R; Huang SJ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1886; Hullermeier E, 2006, INTELL DATA ANAL, V10, P419, DOI 10.3233/IDA-2006-10503; Huiskes Mark J, 2008, P 1 ACM INT C MULTIM, P39, DOI DOI 10.1145/1460096.1460104; Ishida T, 2017, ADV NEUR IN, V30; Jin R., 2003, PROC NEURAL INF PROC, P921; Johan B., 2009, P 8 INT WORKSH TREEB, P27; Li YF, 2021, IEEE T PATTERN ANAL, V43, P334, DOI 10.1109/TPAMI.2019.2922396; Li YF, 2019, FRONT COMPUT SCI-CHI, V13, P669, DOI 10.1007/s11704-019-8452-2; Lin CH, 2016, AAAI CONF ARTIF INTE, P1845; Liu L., 2012, ADV NEURAL INFORM PR, P548; Liu WW, 2015, AAAI CONF ARTIF INTE, P2800; Luo J., 2010, PROC NEURAL INF PROC, P1504; Lyu GY, 2021, IEEE T KNOWL DATA EN, V33, P521, DOI 10.1109/TKDE.2019.2933837; Ma YL, 2019, SCI CHINA INFORM SCI, V62, DOI 10.1007/s11432-017-9371-y; Nguyen N., 2008, P 14 ACM SIGKDD INT, P551, DOI 10.1145/1401890.1401958; Panis G, 2016, IET BIOMETRICS, V5, P37, DOI 10.1049/iet-bmt.2014.0053; Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203; Ren MY, 2018, PR MACH LEARN RES, V80; Song JH, 2018, KNOWL-BASED SYST, V159, P244, DOI 10.1016/j.knosys.2018.07.010; Sun LJ, 2019, AAAI CONF ARTIF INTE, P5016; Tang CZ, 2017, AAAI CONF ARTIF INTE, P2611; Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34; Wang DB, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P83, DOI 10.1145/3292500.3330840; Wang H., 2020, PROC EUR C MACH LEAR, P455; Wang HB, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2943; Wang HB, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3691; Wang J, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2427, DOI 10.1145/3219819.3220008; Wang LC, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2798; Wang QW, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3755; Liu WW, 2020, Arxiv, DOI arXiv:2011.11197; Wu JH, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P416, DOI 10.1145/3292500.3330901; Wu X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2868; Xie MK, 2018, AAAI CONF ARTIF INTE, P4302; Xu N, 2019, AAAI CONF ARTIF INTE, P5557; Yan S., 2016, PROC NEURAL INF PROC, P2128; Yan SB, 2018, PR MACH LEARN RES, V80; Yan SB, 2015, ANN ALLERTON CONF, P1352, DOI 10.1109/ALLERTON.2015.7447165; Yang J, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P23, DOI 10.1145/3178876.3186033; YE YY, 1989, MATH PROGRAM, V44, P157, DOI 10.1007/BF01587086; Yu F, 2017, MACH LEARN, V106, P573, DOI 10.1007/s10994-016-5606-4; Yu GX, 2018, IEEE DATA MINING, P1398, DOI 10.1109/ICDM.2018.00192; Zeng ZN, 2013, PROC CVPR IEEE, P708, DOI 10.1109/CVPR.2013.97; Zhang LF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3399; Zhang ML, 2017, IEEE T KNOWL DATA EN, V29, P2155, DOI 10.1109/TKDE.2017.2721942; Zhang ML, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1335, DOI 10.1145/2939672.2939788; Zhang ML, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4048; Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39; Zhang X, 2015, ADV NEUR IN, V28; Zhou DY, 2018, ACM T ASIAN LOW-RESO, V17, DOI 10.1145/3214707; Zhou ZH, 2018, NATL SCI REV, V5, P44, DOI 10.1093/nsr/nwx106; Zhou ZH, 2012, ARTIF INTELL, V176, P2291, DOI 10.1016/j.artint.2011.10.002; Zhu X., 2009, SYNTHESIS LECT ARTIF, V3, P1, DOI [10.2200/S00196ED1V01Y200906AIM006, DOI 10.2200/S00196ED1V01Y200906AIM006]	75	1	1	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					8796	8811		10.1109/TPAMI.2021.3120012	http://dx.doi.org/10.1109/TPAMI.2021.3120012			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34648433				2022-12-18	WOS:000880661400021
J	Wang, L; Yoon, KJ				Wang, Lin; Yoon, Kuk-Jin			Deep Learning for HDR Imaging: State-of-the-Art and Future Trends	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Imaging; Image reconstruction; Loss measurement; Cameras; Deep learning; Visualization; Dynamic range; High-dynamic-range (HDR) imaging; deep learning (DL); convolutional neural networks (CNNs)	HIGH-DYNAMIC-RANGE; FUSION; NETWORK; RECONSTRUCTION; EXPANSION; IMAGES; MODEL; VIDEO	High dynamic range (HDR) imaging is a technique that allows an extensive dynamic range of exposures, which is important in image processing, computer graphics, and computer vision. In recent years, there has been a significant advancement in HDR imaging using deep learning (DL). This study conducts a comprehensive and insightful survey and analysis of recent developments in deep HDR imaging methodologies. We hierarchically and structurally group existing deep HDR imaging methods into five categories based on (1) number/domain of input exposures, (2) number of learning tasks, (3) novel sensor data, (4) novel learning strategies, and (5) applications. Importantly, we provide a constructive discussion on each category regarding its potential and challenges. Moreover, we review some crucial aspects of deep HDR imaging, such as datasets and evaluation metrics. Finally, we highlight some open problems and point out future research directions.	[Wang, Lin; Yoon, Kuk-Jin] Korea Adv Inst Sci & Technol, Dept Mech Engn, Visual Intelligence Lab, Daejeon 34141, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Yoon, KJ (corresponding author), Korea Adv Inst Sci & Technol, Dept Mech Engn, Visual Intelligence Lab, Daejeon 34141, South Korea.	wanglin@kaist.ac.kr; kjyoon@kaist.ac.kr		yun, gugjin/0000-0002-1634-2756	National Research Foundation of Korea (NRF) - Korea government (MSIT) [NRF2018R1A2B3008640]	National Research Foundation of Korea (NRF) - Korea government (MSIT)(National Research Foundation of KoreaMinistry of Science, ICT & Future Planning, Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea)	This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) under Grant NRF2018R1A2B3008640.	Aittala M, 2018, LECT NOTES COMPUT SC, V11212, P748, DOI 10.1007/978-3-030-01237-3_45; Akyuz AG, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239489, 10.1145/1276377.1276425]; Alex Zihao Zhu, 2018, Arxiv, DOI arXiv:1802.06898; Alghamdi M. M., 2019, RECONFIGURABLE SNAPS; Alghamdi M, 2021, COMPUT GRAPH FORUM, V40, P90, DOI 10.1111/cgf.14205; An VG, 2017, ASIAPAC SIGN INFO PR, P1768; Artusi A, 2020, IEEE T IMAGE PROCESS, V29, P1843, DOI 10.1109/TIP.2019.2944079; Banterle Francesco, 2017, ADV HIGH DYNAMIC RAN; Brandli C, 2014, IEEE J SOLID-ST CIRC, V49, P2333, DOI 10.1109/JSSC.2014.2342715; Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218; Chang J., US; Chaudhari P., 2019, ARXIV; Chen G., 2021, ARXIV; Chen HT, 2019, IEEE I CONF COMP VIS, P3513, DOI 10.1109/ICCV.2019.00361; Chen SY, 2020, INT CONF ACOUST SPEE, P1464, DOI 10.1109/ICASSP40776.2020.9053765; Chen T, 2020, PR MACH LEARN RES, V119; Chen XY, 2021, IEEE COMPUT SOC CONF, P354, DOI 10.1109/CVPRW53098.2021.00045; Chen YY, 2020, IEEE T COMPUT IMAG, V6, P1044, DOI 10.1109/TCI.2020.3001398; Chen YY, 2019, IEEE IMAGE PROC, P3502, DOI 10.1109/ICIP.2019.8803656; Chen Z., 2021, PROC C COMPUT VIS PA, p147 60; Choi S, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20185102; Choudhury A, 2018, IEEE GLOB CONF SIG, P91, DOI 10.1109/GlobalSIP.2018.8646579; Cogalan U., 2020, ARXIV; Cogalan U, 2020, IEEE T IMAGE PROCESS, V29, P7511, DOI 10.1109/TIP.2020.3004014; Deng X, 2021, IEEE T IMAGE PROCESS, V30, P3098, DOI 10.1109/TIP.2021.3058764; Deng YP, 2020, PROC SPIE, V11519, DOI 10.1117/12.2572977; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Pan ED, 2021, Arxiv, DOI arXiv:2103.12545; Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816; Endo Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130834; Finn C, 2017, PR MACH LEARN RES, V70; Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413; Gallo O., 2016, HIGH DYNAMIC RANGE V, P85; Gao Huang, 2019, Arxiv, DOI arXiv:1810.05270; Gardner MA, 2019, IEEE I CONF COMP VIS, P7174, DOI 10.1109/ICCV.2019.00727; Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592; Gkitsas V, 2020, IEEE COMPUT SOC CONF, P2719, DOI 10.1109/CVPRW50498.2020.00328; Godard C, 2018, LECT NOTES COMPUT SC, V11219, P560, DOI 10.1007/978-3-030-01267-0_33; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Han J, 2020, PROC CVPR IEEE, P1727, DOI 10.1109/CVPR42600.2020.00180; Hanhart P, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0091-4; He Q, 2018, IEEE INT CONF MULTI; Hitoshi Kiya, 2019, Arxiv, DOI arXiv:1903.01277; Hold-Geoffroy Y, 2019, PROC CVPR IEEE, P6920, DOI 10.1109/CVPR.2019.00709; Hold-Geoffroy Y, 2017, PROC CVPR IEEE, P2373, DOI 10.1109/CVPR.2017.255; Hu J, 2013, PROC CVPR IEEE, P1163, DOI 10.1109/CVPR.2013.154; Huang YF, 2021, IEEE T MULTIMEDIA, V23, P176, DOI 10.1109/TMM.2020.2981994; Huo YQ, 2014, VISUAL COMPUT, V30, P507, DOI 10.1007/s00371-013-0875-4; Im S, 2019, IEEE T IMAGE PROCESS, V28, P2451, DOI 10.1109/TIP.2018.2886777; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jagatap G, 2020, INT CONF ACOUST SPEE, P9289, DOI 10.1109/ICASSP40776.2020.9054218; Jaiswal A, 2021, TECHNOLOGIES, V9, DOI 10.3390/technologies9010002; Jia S, 2017, IEEE IMAGE PROC, P765; Jiang Y., 2021, ARXIV; Johnson A. K., 2015, INT J IMAGE PROCESS, V9; Jolicoeur-Martineau Alexia, 2019, P INT C LEARN REPR I; Jung H, 2020, IEEE T IMAGE PROCESS, V29, P3845, DOI 10.1109/TIP.2020.2966075; Kalantari NK, 2019, COMPUT GRAPH FORUM, V38, P193, DOI 10.1111/cgf.13630; Kalantari NK, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073609; Kaur H, 2021, ARCH COMPUT METHOD E, V28, P4425, DOI 10.1007/s11831-021-09540-7; Kenta Moriwaki, 2018, Arxiv, DOI arXiv:1812.07134; Kim J. H., 2021, ASS ADV ARTIF INTELL; Kim SY, 2020, AAAI CONF ARTIF INTE, V34, P11287; Kim SY, 2019, IEEE I CONF COMP VIS, P3116, DOI 10.1109/ICCV.2019.00321; Kim Soo Ye, 2018, P 14 AS C COMP VIS, P379; Kinoshita Y, 2019, IEEE ACCESS, V7, P73555, DOI 10.1109/ACCESS.2019.2919296; Kniaz VV, 2019, LECT NOTES COMPUT SC, V11134, P606, DOI 10.1007/978-3-030-11024-6_46; Koh J, 2021, COMPUT VIS IMAGE UND, V203, DOI 10.1016/j.cviu.2020.103134; Kovaleski RP, 2014, 2014 27TH SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND IMAGES (SIBGRAPI), P49, DOI 10.1109/SIBGRAPI.2014.29; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kuang XD, 2020, INFRARED PHYS TECHN, V107, DOI 10.1016/j.infrared.2020.103338; Kuk-Jin Yoon, 2020, Arxiv, DOI arXiv:1907.03107; Kumar VA, 2017, INT WORK QUAL MULTIM; Lee S.-H., 2020, IEEE ACCESS, V8, p117 428; Lee S, 2021, IEEE T MULTIMEDIA, V23, P2561, DOI 10.1109/TMM.2020.3013378; Lee S, 2018, LECT NOTES COMPUT SC, V11206, P613, DOI 10.1007/978-3-030-01216-8_37; Lee S, 2018, IEEE ACCESS, V6, P49913, DOI 10.1109/ACCESS.2018.2868246; Lehtinen J, 2018, PR MACH LEARN RES, V80; Li H, 2018, INT C PATT RECOG, P2705, DOI 10.1109/ICPR.2018.8546006; Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342; Li J., 2019, PROC 4 INT C MULTIME, P119; Lin HY, 2009, IEEE IMAGE PROC, P4305, DOI 10.1109/ICIP.2009.5413665; Liu C., 2009, THESIS MIT BOSTON MA; Liu S, 2021, ACM T INTEL SYST TEC, V12, DOI 10.1145/3426239; Liu YL, 2020, PROC CVPR IEEE, P1648, DOI 10.1109/CVPR42600.2020.00172; Liu Z, 2021, IEEE COMPUT SOC CONF, P463, DOI 10.1109/CVPRW53098.2021.00057; Ma JY, 2020, INFORM FUSION, V54, P85, DOI 10.1016/j.inffus.2019.07.005; Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004; Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004; Ma KD, 2020, IEEE T IMAGE PROCESS, V29, P2808, DOI 10.1109/TIP.2019.2952716; Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920; Mantiuk R, 2005, PROC SPIE, V5666, P204, DOI 10.1117/12.586757; Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935; Marcel Santana Santos, 2020, Arxiv, DOI arXiv:2005.07335; Marnerides D, 2018, COMPUT GRAPH FORUM, V37, P37, DOI 10.1111/cgf.13340; Martel JNP, 2020, IEEE T PATTERN ANAL, V42, P1642, DOI 10.1109/TPAMI.2020.2986944; Masia B, 2017, MULTIMED TOOLS APPL, V76, P631, DOI 10.1007/s11042-015-3036-0; Mehdi Mirza, 2014, Arxiv, DOI arXiv:1411.1784; Metwaly K, 2020, INT CONF ACOUST SPEE, P2623, DOI 10.1109/ICASSP40776.2020.9053180; Metzler CA, 2020, PROC CVPR IEEE, P1372, DOI 10.1109/CVPR42600.2020.00145; Mostafavi M, 2021, INT J COMPUT VISION, V129, P900, DOI 10.1007/s11263-020-01410-2; Mostafavi ISM, 2020, PROC CVPR IEEE, P2765, DOI 10.1109/CVPR42600.2020.00284; Mukherjee R, 2020, IEEE ACCESS, V8, P142736, DOI 10.1109/ACCESS.2020.3010340; Mukul Khanna, 2019, Arxiv, DOI arXiv:1912.11463; Nafchi H. Ziaei, 2018, THESIS ECOLE TECHNOL; Narwaria M, 2015, SIGNAL PROCESS-IMAGE, V35, P46, DOI 10.1016/j.image.2015.04.009; Nemoto H., 2015, P 9 INT WORKSH VID P; Ning SY, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1383; Niu YZ, 2021, IEEE T IMAGE PROCESS, V30, P3885, DOI 10.1109/TIP.2021.3064433; Oh TH, 2015, IEEE T PATTERN ANAL, V37, P1219, DOI 10.1109/TPAMI.2014.2361338; Onzon E, 2021, P IEEECVF C COMPUTER, P7710; Paredes-Vall ~es F., 2021, PROC IEEECVF C COMPU, P3446; Park JS, 2018, IEEE ACCESS, V6, P10966, DOI 10.1109/ACCESS.2018.2797197; Park SW, 2019, PROC CVPR IEEE, P4287, DOI 10.1109/CVPR.2019.00442; Patel Vaibhav Amit, 2017, PROC NAT C COMPUT VI, P220; Peng FY, 2018, 2018 IEEE 3RD INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC), P347, DOI 10.1109/ICIVC.2018.8492856; Perez-Pellitero E, 2021, IEEE COMPUT SOC CONF, P691, DOI 10.1109/CVPRW53098.2021.00078; Prabhakar K. Ram, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P497, DOI 10.1007/978-3-030-58589-1_30; Prabhakar KR, 2019, IEEE INT CONF COMPUT; Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505; Pu Z., 2020, PROC ASIAN C COMPUT, P134; Rana A, 2020, IEEE T IMAGE PROCESS, V29, P1285, DOI 10.1109/TIP.2019.2936649; Rebecq H, 2021, IEEE T PATTERN ANAL, V43, P1964, DOI 10.1109/TPAMI.2019.2963386; Ronneberger O., 2015, P MED IM COMP ASS IN, P234, DOI DOI 10.1007/978-3-319-24574-4_28; Rosh KSG, 2019, IEEE IMAGE PROC, P4714, DOI 10.1109/ICIP.2019.8803582; Li R, 2021, Arxiv, DOI arXiv:2102.01850; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Satilmis P, 2020, SIGNAL PROCESS-IMAGE, V88, DOI 10.1016/j.image.2020.115950; Scheerlinck C, 2020, IEEE WINT CONF APPL, P156, DOI 10.1109/WACV45572.2020.9093366; Sen P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366222; Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329; Simonyan K., 2014, 3 INT C LEARN REPR I; Soh JW, 2020, PROC CVPR IEEE, P3513, DOI 10.1109/CVPR42600.2020.00357; Soh JW, 2019, IEEE ACCESS, V7, P177427, DOI 10.1109/ACCESS.2019.2957775; Song SR, 2019, PROC CVPR IEEE, P6911, DOI 10.1109/CVPR.2019.00708; Song Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P666, DOI 10.1007/978-3-030-58523-5_39; Srikantha A, 2012, SIGNAL PROCESS-IMAGE, V27, P650, DOI 10.1016/j.image.2012.02.001; Stoffregen Timo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P534, DOI 10.1007/978-3-030-58583-9_32; Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049; Goswami S, 2021, Arxiv, DOI arXiv:2101.06910; Tiwari G., 2015, INT J SIGNAL PROCESS, V8, P93; Tursun OT, 2016, COMPUT GRAPH FORUM, V35, P139, DOI 10.1111/cgf.12818; Tursun OT, 2015, COMPUT GRAPH FORUM, V34, P683, DOI 10.1111/cgf.12593; Vaezi Joze Hamid Reza, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13286, DOI 10.1109/CVPR42600.2020.01330; van Engelen JE, 2020, MACH LEARN, V109, P373, DOI 10.1007/s10994-019-05855-6; Vaswani A., 2017, ARXIV, DOI DOI 10.48550/ARXIV.1706.03762; Wang Bishan, 2020, ECCV, P155, DOI DOI 10.1007/978-3-030-58601-0_10; Wang J.-G., 2020, DEEP LEARNING ALGORI, P157; Wang JG, 2019, IEEE T INTELL TRANSP, V20, P1341, DOI 10.1109/TITS.2018.2849505; Wang JY, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9122053; Wang JY, 2020, PROC CVPR IEEE, P469, DOI 10.1109/CVPR42600.2020.00055; Wang L., 2021, PROC INT C COMPUT VI, P2135; Wang L, 2022, IEEE T PATTERN ANAL, V44, P7657, DOI 10.1109/TPAMI.2021.3113352; Wang L, 2021, PROC CVPR IEEE, P608, DOI 10.1109/CVPR46437.2021.00067; Wang L, 2022, IEEE T PATTERN ANAL, V44, P3048, DOI 10.1109/TPAMI.2021.3055564; Wang L, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108206; Wang L, 2019, PROC CVPR IEEE, P10073, DOI 10.1109/CVPR.2019.01032; Wang L, 2020, IEEE ROBOT AUTOM LET, V5, P1421, DOI 10.1109/LRA.2020.2967289; Wang Lizhi, 2020, P IEEE CVF C COMP VI; Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701; Wang X, 2020, IEEE T CIRC SYST VID, V30, P4409, DOI 10.1109/TCSVT.2020.2985427; Weiher M, 2019, DOMAIN ADAPTATION HD; Wieschollek P, 2017, IEEE I CONF COMP VIS, P231, DOI 10.1109/ICCV.2017.34; Wikipedia, HIGH DYNAMIC RANGE I; Wu SZ, 2018, LECT NOTES COMPUT SC, V11206, P120, DOI 10.1007/978-3-030-01216-8_8; Zhang X, 2021, Arxiv, DOI arXiv:2103.02376; Xu H, 2020, AAAI CONF ARTIF INTE, V34, P12484; Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548; Xu H, 2020, IEEE T IMAGE PROCESS, V29, P7203, DOI 10.1109/TIP.2020.2999855; Xu YC, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P142, DOI [10.1109/BigMM.2019.00-32, 10.1109/BigMM.2019.00030]; Yan QS, 2021, NEUROCOMPUTING, V428, P79, DOI 10.1016/j.neucom.2020.11.056; Yan QS, 2020, IEEE T IMAGE PROCESS, V29, P4308, DOI 10.1109/TIP.2020.2971346; Yan QS, 2019, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2019.00185; Yan QS, 2019, IEEE WINT CONF APPL, P41, DOI 10.1109/WACV.2019.00012; Yang JH, 2020, AAAI CONF ARTIF INTE, V34, P12613; Yang X, 2018, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2018.00193; Yang ZG, 2021, NEURAL COMPUT APPL, V33, P6133, DOI 10.1007/s00521-020-05387-4; Yin JL, 2020, IEEE INT CON MULTI; Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104; Yu RS, 2018, ADV NEUR IN, V31; Zamir Amir R., 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11194, DOI 10.1109/CVPR42600.2020.01121; Zeng H, 2022, IEEE T PATTERN ANAL, V44, P2058, DOI 10.1109/TPAMI.2020.3026740; Zeng HM, 2020, IEEE ACCESS, V8, P182815, DOI 10.1109/ACCESS.2020.3028584; Zhang JS, 2019, PROC CVPR IEEE, P10150, DOI 10.1109/CVPR.2019.01040; Zhang JS, 2017, IEEE I CONF COMP VIS, P4529, DOI 10.1109/ICCV.2017.484; Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454; Zhang Yulun, 2018, P EUROPEAN C COMPUTE, P286; Zhao H, 2015, IEEE INT CONF COMPUT; Zhe J, 2019, I C VIRTUAL REALITY, P113, DOI 10.1109/ICVRV47840.2019.00027; Pan ZY, 2020, NEUROCOMPUTING, V386, P147, DOI 10.1016/j.neucom.2019.12.093; Zhou C., 2020, PROC NEURAL INF PROC, V33, P1; Zhou S., 2020, PROC 34 C NEURAL INF, P1; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zou YH, 2021, PROC CVPR IEEE, P2024, DOI 10.1109/CVPR46437.2021.00206	195	1	1	12	21	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					8874	8895		10.1109/TPAMI.2021.3123686	http://dx.doi.org/10.1109/TPAMI.2021.3123686			22	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34714739	Green Submitted			2022-12-18	WOS:000880661400026
J	Xian, YQ; Korbar, B; Douze, M; Torresani, L; Schiele, B; Akata, Z				Xian, Yongqin; Korbar, Bruno; Douze, Matthijs; Torresani, Lorenzo; Schiele, Bernt; Akata, Zeynep			Generalized Few-Shot Video Classification With Video Retrieval and Feature Generation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Training; Benchmark testing; Feature extraction; Training data; Three-dimensional displays; Semantics; Task analysis; Few-shot learning; video classification		Few-shot learning aims to recognize novel classes from a few examples. Although significant progress has been made in the image domain, few-shot video classification is relatively unexplored. We argue that previous methods underestimate the importance of video feature learning and propose to learn spatiotemporal features using a 3D CNN. Proposing a two-stage approach that learns video features on base classes followed by fine-tuning the classifiers on novel classes, we show that this simple baseline approach outperforms prior few-shot video classification methods by over 20 points on existing benchmarks. To circumvent the need of labeled examples, we present two novel approaches that yield further improvement. First, we leverage tag-labeled videos from a large dataset using tag retrieval followed by selecting the best clips with visual similarities. Second, we learn generative adversarial networks that generate video features of novel classes from their semantic embeddings. Moreover, we find existing benchmarks are limited because they only focus on 5 novel classes in each testing episode and introduce more realistic benchmarks by involving more novel classes, i.e., few-shot learning, as well as a mixture of novel and base classes, i.e., generalized few-shot learning. The experimental results show that our retrieval and feature generation approach significantly outperform the baseline approach on the new benchmarks.	[Xian, Yongqin] Swiss Fed Inst Technol, CH-8092 Zurich, Switzerland; [Korbar, Bruno] Univ Oxford, Oxford OX1 2JD, England; [Douze, Matthijs; Torresani, Lorenzo] Facebook, Menlo Pk, CA 94025 USA; [Torresani, Lorenzo] Dartmouth Coll, Hanover, NH 03755 USA; [Schiele, Bernt; Akata, Zeynep] Max Planck Inst Informat, D-66123 Saarbrucken, Germany; [Akata, Zeynep] Univ Tubingen, D-72074 Tubingen, Germany; [Akata, Zeynep] Max Planck Inst Intelligent Syst, D-72076 Tubingen, Germany	Swiss Federal Institutes of Technology Domain; ETH Zurich; University of Oxford; Facebook Inc; Dartmouth College; Max Planck Society; Eberhard Karls University of Tubingen; Max Planck Society	Xian, YQ (corresponding author), Swiss Fed Inst Technol, CH-8092 Zurich, Switzerland.	yongqin.xian@vision.ee.ethz.ch; bruno@kor.bar; matthijs@fb.com; LT@dartmouth.edu; schiele@mpi-inf.mpg.de; zeynep.akata@uni-tuebingen.de			ERC [853489 - DEXIM]; DFG - EXC [2064/1, 390727645]	ERC(European Research Council (ERC)European Commission); DFG - EXC	This work was supported by the ERC 853489 - DEXIM and DFG - EXC number 2064/1 - Project No. 390727645.	Agharwal A., 2016, PROC IEEE WINTER C A, P1; Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; Andrew Zisserman, 2017, Arxiv, DOI arXiv:1705.06950; Arjovsky M., 2017, P 34 INT C MACH LEAR, P214; Arjovsky Mart<prime>in, 2017, P 5 INT C LEARN REPR; Baraldi L, 2018, PROC CVPR IEEE, P7804, DOI 10.1109/CVPR.2018.00814; Bishay M., 2019, PROC BRIT MACH VIS C; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen X, 2016, ADV NEUR IN, V29; Devlin J., 2019, P 2019 C N AM CHAPTE, P4171, DOI [10.18653/v1/n19-1423, DOI 10.18653/V1/N19-1423]; Douze M, 2018, PROC CVPR IEEE, P3349, DOI 10.1109/CVPR.2018.00353; Douze M, 2016, INT J COMPUT VISION, V119, P291, DOI 10.1007/s11263-015-0875-0; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Elhoseiny M, 2013, IEEE I CONF COMP VIS, P2584, DOI 10.1109/ICCV.2013.321; Feichtenhofer C, 2016, ADV NEUR IN, V29; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Frome Andrea, 2013, NEURIPS; Girdhar R, 2017, PROC CVPR IEEE, P3165, DOI 10.1109/CVPR.2017.337; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622; Han-Jia Ye, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8805, DOI 10.1109/CVPR42600.2020.00883; Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Gulrajani I, 2017, ADV NEUR IN, V30; Johnson J, 2021, IEEE T BIG DATA, V7, P535, DOI 10.1109/TBDATA.2019.2921572; Joulin A., 2016, ARXIV; Joulin A, 2016, LECT NOTES COMPUT SC, V9911, P67, DOI 10.1007/978-3-319-46478-7_5; Kaidi Cao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10615, DOI 10.1109/CVPR42600.2020.01063; Kaiser O., 2017, P INT C LEARN REPR; Kang B., 2020, INT C LEARN REPR; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kilian Q. Weinberger, 2019, Arxiv, DOI arXiv:1911.04623; Larsen ABL, 2016, PR MACH LEARN RES, V48; Li XZ, 2019, ADV NEUR IN, V32; Liu Y., 2019, PROC INT C LEARN REP; Mazloom M., 2014, PROC INT C MULTIMEDI, P459; Mehdi Mirza, 2014, Arxiv, DOI arXiv:1411.1784; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Norouzi M., 2014, PROC INT C LEARN REP; Qi H, 2018, PROC CVPR IEEE, P5822, DOI 10.1109/CVPR.2018.00610; Radford A., 2016, 4 INT C LEARN REPR I; Ravi Sachin, 2017, INT C LEARN REPR, V2, P5; Reed S, 2016, PR MACH LEARN RES, V48; Reimers N, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3982; Ren M., 2018, P INT C LEARN REPR; Simonyan K, 2014, ADV NEUR IN, V27; Snell J, 2017, ADV NEUR IN, V30; Soomro K., 2012, ARXIV; Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; Vaswani A, 2017, ADV NEUR IN, V30; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768; Yikai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12833, DOI 10.1109/CVPR42600.2020.01285; Zhu LC, 2018, LECT NOTES COMPUT SC, V11211, P782, DOI 10.1007/978-3-030-01234-2_46; Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111	65	1	1	3	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					8949	8961		10.1109/TPAMI.2021.3120550	http://dx.doi.org/10.1109/TPAMI.2021.3120550			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34652997	Green Submitted			2022-12-18	WOS:000880661400030
J	Yang, S; Wu, SH; Liu, TL; Xu, M				Yang, Shuo; Wu, Songhua; Liu, Tongliang; Xu, Min			Bridging the Gap Between Few-Shot and Many-Shot Learning via Distribution Calibration	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Data models; Task analysis; Training; Mathematical models; Calibration; Adaptation models; Gaussian distribution; Few-shot learning; image classification; transfer learning; generalization error		A major gap between few-shot and many-shot learning is the data distribution empirically oserved by the model during training. In few-shot learning, the learned model can easily become over-fitted based on the biased distribution formed by only a few training examples, while the ground-truth data distribution is more accurately uncovered in many-shot learning to learn a well-generalized model. In this paper, we propose to calibrate the distribution of these few-sample classes to be more unbiased to alleviate such an over-fitting problem. The distribution calibration is achieved by transferring statistics from the classes with sufficient examples to those few-sample classes. After calibration, an adequate number of examples can be sampled from the calibrated distribution to expand the inputs to the classifier. Specifically, we assume every dimension in the feature representation from the same class follows a Gaussian distribution so that the mean and the variance of the distribution can borrow from that of similar classes whose statistics are better estimated with an adequate number of samples. Extensive experiments on three datasets, miniImageNet, tieredImageNet, and CUB, show that a simple linear classifier trained using the features sampled from our calibrated distribution can outperform the state-of-the-art accuracy by a large margin. Besides the favorable performance, the proposed method also exhibits high flexibility by showing consistent accuracy improvement when it is built on top of any off-the-shelf pretrained feature extractors and classification models without extra learnable parameters. The visualization of these generated features demonstrates that our calibrated distribution is an accurate estimation thus the generalization ability gain is convincing. We also establish a generalization error bound for the proposed distribution-calibration-based few-shot learning, which consists of the distribution assumption error, the distribution approximation error, and the estimation error. This generalization error bound theoretically justifies the effectiveness of the proposed method.	[Yang, Shuo; Xu, Min] Univ Technol Sydney, Fac Engn & Informat Technol, Sch Elect & Data Engn, Ultimo, NSW 2007, Australia; [Wu, Songhua; Liu, Tongliang] Univ Sydney, Sch Comp Sci, Trustworthy Machine Learning Lab, Darlington, NSW 2008, Australia	University of Technology Sydney; University of Sydney	Xu, M (corresponding author), Univ Technol Sydney, Fac Engn & Informat Technol, Sch Elect & Data Engn, Ultimo, NSW 2007, Australia.	shuo.yang@student.uts.edu.au; songhua.wu@syd-ney.edu.au; tongliang.liu@syd-ney.edu.au; min.xu@uts.edu.au						Aditya Khosla, 2015, Arxiv, DOI arXiv:1409.0575; Amos Storkey, 2019, Arxiv, DOI arXiv:1902.09884; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Beckenbach E. F., 2012, INEQUALITIES; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Bertinetto Luca, 2018, INT C LEARN REPR; Bin Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P438, DOI 10.1007/978-3-030-58548-8_26; Blitzer J., 2007, ADV NEURAL INFORM PR, V20, P129; Bousquet O, 2004, LECT NOTES ARTIF INT, V3176, P169; Chen ZT, 2019, IEEE T IMAGE PROCESS, V28, P4594, DOI 10.1109/TIP.2019.2910052; Chi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12200, DOI 10.1109/CVPR42600.2020.01222; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Fei Chen, 2017, Arxiv, DOI arXiv:1707.09835; Finn C, 2017, PR MACH LEARN RES, V70; Franceschi L, 2018, PR MACH LEARN RES, V80; Gao Hang, 2018, NEURIPS; Golowich N., 2018, PROC C LEARN THEORY, P297; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grant E., 2018, PROC INT C LEARN REP; Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Kim J, 2019, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2019.00010; Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091; Li HY, 2019, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2019.00009; Liu Y., 2019, PROC INT C LEARN REP; Liu Y. -C., 2019, PROC INT C LEARN REP; Mangla P, 2020, IEEE WINT CONF APPL, P2207, DOI 10.1109/WACV45572.2020.9093338; Mohri M., 2018, FDN MACHINE LEARNING; Munkhdalai T, 2018, PR MACH LEARN RES, V80; Park S.-J., 2020, PROC 37 INT C MACH L, P7510; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Qin T., 2020, ARXIV; Ravi S., 2017, INT C LEARN REPR, P12; Ren M., 2018, ICLR; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Rusu Andrei A, 2019, ICLR; Salakhutdinov R., 2012, PROC INT C MACH LEAR; Satorras Victor Garcia, 2018, INT C LEARN REPR; Schwartz E, 2018, ADV NEUR IN, V31; Yang S, 2021, Arxiv, DOI arXiv:2105.13001; Snell J, 2017, ADV NEUR IN, V30; Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Tukey JW., 1977, EXPLORATORY DATA ANA; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760; Welinder P., 2010, CNSTR2010001 CALT UC; Wu Songhua, 2021, PROC 38 INT C MACH L, P11285; Xia X., 2019, PROC INT C NEURAL IN; Xia X., 2021, ARXIV; Xia X., 2020, ARXIV; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Yang S., 2021, ARXIV; Yang S., 2021, PROC INT C LEARN REP; Yang S, 2021, PROC CVPR IEEE, P3151, DOI 10.1109/CVPR46437.2021.00317; Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2097, DOI 10.1145/3343031.3350995; Yaoyao Liu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P404, DOI 10.1007/978-3-030-58517-4_24; Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064; Zhang Chao, 2012, Adv Neural Inf Process Syst, V4, P3320; Zhang J, 2019, IEEE I CONF COMP VIS, P1685, DOI 10.1109/ICCV.2019.00177; Zhang RX, 2018, ADV NEUR IN, V31	62	1	1	4	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9830	9843		10.1109/TPAMI.2021.3132021	http://dx.doi.org/10.1109/TPAMI.2021.3132021			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34860647	Green Submitted			2022-12-18	WOS:000880661400091
J	Zhong, G; Pun, CM				Zhong, Guo; Pun, Chi-Man			Improved Normalized Cut for Multi-View Clustering	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Laplace equations; Clustering algorithms; Optimization; Clustering methods; Matrix decomposition; Linear programming; Fuses; Clustering; multi-view data; normalized cut	LOW-RANK	Spectral clustering (SC) algorithms have been successful in discovering meaningful patterns since they can group arbitrarily shaped data structures. Traditional SC approaches typically consist of two sequential stages, i.e., performing spectral decomposition of an affinity matrix and then rounding the relaxed continuous clustering result into a binary indicator matrix. However, such a two-stage process could make the obtained binary indicator matrix severely deviate from the ground true one. This is because the former step is not devoted to achieving an optimal clustering result. To alleviate this issue, this paper presents a general joint framework to simultaneously learn the optimal continuous and binary indicator matrices for multi-view clustering, which also has the ability to tackle the conventional single-view case. Specially, we provide theoretical proof for the proposed method. Furthermore, an effective alternate updating algorithm is developed to optimize the corresponding complex objective. A number of empirical results on different benchmark datasets demonstrate that the proposed method outperforms several state-of-the-arts in terms of six clustering metrics.	[Zhong, Guo; Pun, Chi-Man] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China; [Zhong, Guo] Guangdong Univ Foreign Studies, Sch Informat Sci & Technol, Guangzhou 510006, Peoples R China	University of Macau; Guangdong University of Foreign Studies	Pun, CM (corresponding author), Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.	yb77410@um.edu.mo; cmpun@umac.mo		Pun, Chi-Man/0000-0003-1788-3746	Science and Technology Development Fund, Macau SAR [0034/2019/AMJ, 0087/2020/A2, 0049/2021/A]	Science and Technology Development Fund, Macau SAR	This work was supported by Science and Technology Development Fund, Macau SAR underGrants 0034/2019/AMJ, 0087/2020/A2, and 0049/2021/A.	Andrew G., 2013, INT C MACH LEARN, p1247?1255; Bezdek J. C., 2003, Neural, Parallel & Scientific Computations, V11, P351; Brbic M, 2018, PATTERN RECOGN, V73, P247, DOI 10.1016/j.patcog.2017.08.024; Chen MS, 2019, LECT NOTES COMPUT SC, V11446, P175, DOI 10.1007/978-3-030-18576-3_11; Chen XJ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1518; Dua D., 2017, UCI MACHINE LEARNING, DOI DOI 10.1002/JCC.23219; Fahad A, 2014, IEEE T EMERG TOP COM, V2, P267, DOI 10.1109/TETC.2014.2330519; Greene D., 2006, P 23 INT C MACHINE L, P377, DOI DOI 10.1145/1143844.1143892; Honglak Lee, 2017, Arxiv, DOI arXiv:1610.03454; Kang Z, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2701; Kumar Abhishek, 2011, NEURIPS, P2, DOI DOI 10.5555/2986459.2986617; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lu CY, 2016, IEEE T IMAGE PROCESS, V25, P2833, DOI 10.1109/TIP.2016.2553459; Nie F., 2016, IJCAI, P1881; Pang YW, 2020, IEEE T CYBERNETICS, V50, P247, DOI 10.1109/TCYB.2018.2868742; Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Tao H, 2018, AAAI CONF ARTIF INTE, P4123; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wang WR, 2015, PR MACH LEARN RES, V37, P1083; Wang XB, 2019, PATTERN RECOGN, V88, P50, DOI 10.1016/j.patcog.2018.09.009; Wang ZX, 2020, INT J PROD RES, V58, P7094, DOI [10.1109/TCYB.2020.2977677, 10.1080/00207543.2020.1764656]; Wen ZW, 2013, MATH PROGRAM, V142, P397, DOI 10.1007/s10107-012-0584-1; Winn J, 2005, IEEE I CONF COMP VIS, P756; Xia RK, 2014, AAAI CONF ARTIF INTE, P2149; Xia TA, 2010, IEEE T SYST MAN CY B, V40, P1438, DOI 10.1109/TSMCB.2009.2039566; Yang Y, 2017, IEEE T KNOWL DATA EN, V29, P1834, DOI 10.1109/TKDE.2017.2701825; Yin M, 2020, AAAI CONF ARTIF INTE, V34, P6688; Zhao J, 2017, INFORM FUSION, V38, P43, DOI 10.1016/j.inffus.2017.02.007	30	1	1	2	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					10244	10251		10.1109/TPAMI.2021.3136965	http://dx.doi.org/10.1109/TPAMI.2021.3136965			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34932473				2022-12-18	WOS:000880661400119
J	Bieker, K; Gebken, B; Peitz, S				Bieker, Katharina; Gebken, Bennet; Peitz, Sebastian			On the Treatment of Optimization Problems With L1 Penalty Terms via Multiobjective Continuation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Multiobjective optimization; nonsmooth optimization; machine learning; sparsity	ALGORITHM; SELECTION	We present a novel algorithm that allows us to gain detailed insight into the effects of sparsity in linear and nonlinear optimization. Sparsity is of great importance in many scientific areas such as image and signal processing, medical imaging, compressed sensing, and machine learning, as it ensures robustness against noisy data and yields models that are easier to interpret due to the small number of relevant terms. It is common practice to enforce sparsity by adding the `1-norm as a penalty term. In order to gain a better understanding and to allow for an informed model selection, we directly solve the corresponding multiobjective optimization problem (MOP) that arises when minimizing the main objective and the `1-norm simultaneously. As this MOP is in general non-convex for nonlinear objectives, the penalty method will fail to provide all optimal compromises. To avoid this issue, we present a continuation method specifically tailored to MOPs with two objective functions one of which is the l(1)-norm. Our method can be seen as a generalization of homotopy methods for linear regression problems to the nonlinear case. Several numerical examples - including neural network training - demonstrate our theoretical findings and the additional insight gained by this multiobjective approach.	[Bieker, Katharina; Gebken, Bennet] Paderborn Univ, Dept Math, D-33098 Paderborn, Germany; [Peitz, Sebastian] Paderborn Univ, Dept Comp Sci, D-33098 Paderborn, Germany	University of Paderborn; University of Paderborn	Bieker, K (corresponding author), Paderborn Univ, Dept Math, D-33098 Paderborn, Germany.	bicker@math.upb.de; bgebken@math.upb.de; sebastian.peitz@upb.de	Peitz, Sebastian/AAC-2075-2020	Peitz, Sebastian/0000-0002-3389-793X; Bieker, Katharina/0000-0002-6762-9730	European Union; German Federal State of North Rhine-Westphalia within the EFRE.NRW project "SET CPS"; DFG Priority Programme 1962 "Non-smooth and Complementarity-based Distributed Parameter Systems"	European Union(European Commission); German Federal State of North Rhine-Westphalia within the EFRE.NRW project "SET CPS"; DFG Priority Programme 1962 "Non-smooth and Complementarity-based Distributed Parameter Systems"(German Research Foundation (DFG))	This work was supported by the European Union and the German Federal State of North Rhine-Westphalia within the EFRE.NRW project "SET CPS", and by the DFG Priority Programme 1962 "Non-smooth and Complementarity-based Distributed Parameter Systems."	Allgower E. L., 1990, NUMERICAL CONTINUATI; [Anonymous], US; Askan A, 2014, ANN OPER RES, V216, P191, DOI 10.1007/s10479-012-1300-5; Aytug H, 2012, EUR J OPER RES, V218, P667, DOI 10.1016/j.ejor.2011.11.037; Baraniuk RG, 2010, P IEEE, V98, P906, DOI 10.1109/JPROC.2010.2047424; Bringmann B, 2018, MATH COMPUT, V87, P2343, DOI 10.1090/mcom/3287; Brunton SL, 2016, P NATL ACAD SCI USA, V113, P3932, DOI 10.1073/pnas.1517384113; CHEN SB, 1994, CONF REC ASILOMAR C, P41, DOI 10.1109/ACSSC.1994.471413; Clarke F.H., 1983, OPTIMIZATION NONSMOO; Deb K, 2002, IEEE T EVOLUT COMPUT, V6, P182, DOI 10.1109/4235.996017; Deb K., 2001, MULTIOBJECTIVE OPTIM, DOI DOI 10.1109/TEVC.2002.804322; Donoho DL, 2008, IEEE T INFORM THEORY, V54, P4789, DOI 10.1109/TIT.2008.929958; Ehrgott M., 2005, MULTICRITERIA OPTIMI; Elad M, 2010, P IEEE, V98, P972, DOI 10.1109/JPROC.2009.2037655; Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x; Gebken B, 2021, J OPTIMIZ THEORY APP, V188, P696, DOI 10.1007/s10957-020-01803-w; Gebken B, 2019, J GLOBAL OPTIM, V73, P891, DOI 10.1007/s10898-019-00737-6; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Hillermeier C, 2001, J OPTIMIZ THEORY APP, V110, P557, DOI 10.1023/A:1017536311488; Hui Li, 2012, Parallel Problem Solving from Nature - PPSN XII. Proceedings of the 12th International Conference, P93, DOI 10.1007/978-3-642-32964-7_10; Kingma D.P, P 3 INT C LEARNING R; LeCun Y., 2010, MNIST HANDWRITTEN DI; Loiseau JC, 2018, J FLUID MECH, V838, P42, DOI 10.1017/jfm.2017.823; M?kel?, 2014, INTRO NONSMOOTH OPTI, DOI [10.1007/978-3-319-08114-4, DOI 10.1007/978-3-319-08114-4]; Makela M. M., 2014, OPTIMIZATION SCI ENG; Malioutov DM, 2005, INT CONF ACOUST SPEE, P733; Martin A, 2018, ENG OPTIMIZ, V50, P516, DOI 10.1080/0305215X.2017.1327579; Miettinen K., 2004, NONLINEAR MULTIOBJEC, V4th; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Park MY, 2007, J ROY STAT SOC B, V69, P659, DOI 10.1111/j.1467-9868.2007.00607.x; Plumbley MD, 2010, P IEEE, V98, P995, DOI 10.1109/JPROC.2009.2030345; Roman V., 2012, COMPRESSED SENSING T; Rosset S., 2005, PROC 17 INT C NEURAL, P1153; Rosset S, 2004, PROC INT C NEURAL IN, P1153; Schutze O, 2003, LECT NOTES COMPUT SC, V2632, P509; Schutze O, 2020, ENG OPTIMIZ, V52, P832, DOI 10.1080/0305215X.2019.1617286; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Vidaurre D, 2013, INT STAT REV, V81, P361, DOI 10.1111/insr.12023; Zhao P., 2004, BOOSTED LASSO	41	1	1	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7797	7808		10.1109/TPAMI.2021.3114962	http://dx.doi.org/10.1109/TPAMI.2021.3114962			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34559634	Green Submitted, hybrid			2022-12-18	WOS:000864325900039
J	Dong, JX; Pan, JS; Ren, JS; Lin, L; Tang, JH; Yang, MH				Dong, Jiangxin; Pan, Jinshan; Ren, Jimmy S.; Lin, Liang; Tang, Jinhui; Yang, Ming-Hsuan			Learning Spatially Variant Linear Representation Models for Joint Filtering	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Convolutional neural networks; Image restoration; Computational modeling; Linear programming; Optimization; Kernel; Spatially variant linear representation model; convolutional neural network; joint filtering	DEPTH ENHANCEMENT; IMAGE	Joint filtering mainly uses an additional guidance image as a prior and transfers its structures to the target image in the filtering process. Different from existing approaches that rely on local linear models or hand-designed objective functions to extract the structural information from the guidance image, we propose a new joint filtering method based on a spatially variant linear representation model (SVLRM), where the target image is linearly represented by the guidance image. However, learning SVLRMs for vision tasks is a highly ill-posed problem. To estimate the spatially variant linear representation coefficients, we develop an effective approach based on a deep convolutional neural network (CNN). As such, the proposed deep CNN (constrained by the SVLRM) is able to model the structural information of both the guidance and input images. We show that the proposed approach can be effectively applied to a variety of applications, including depth/RGB image upsampling and restoration, flash deblurring, natural image denoising, and scale-aware filtering. In addition, we show that the linear representation model can be extended to high-order representation models (e.g., quadratic and cubic polynomial representations). Extensive experimental results demonstrate that the proposed method performs favorably against the state-of-the-art methods that have been specifically designed for each task.	[Dong, Jiangxin; Pan, Jinshan; Tang, Jinhui] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China; [Ren, Jimmy S.] SenseTime Res, Hong Kong, Peoples R China; [Ren, Jimmy S.] Shanghai Jiao Tong Univ, Qing Yuan Res Inst, Shanghai 200240, Peoples R China; [Lin, Liang] Sun Yat Sen Univ, Guangzhou 510275, Peoples R China; [Yang, Ming-Hsuan] Univ Calif Merced, Merced, CA 95344 USA; [Yang, Ming-Hsuan] Yonsei Univ, Seoul 03722, South Korea; [Yang, Ming-Hsuan] Google, Mountain View, CA 94043 USA	Nanjing University of Science & Technology; Shanghai Jiao Tong University; Sun Yat Sen University; University of California System; University of California Merced; Yonsei University; Google Incorporated	Pan, JS (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.	dongjxjx@gmail.com; sdluran@gmail.com; jimmy.sj.ren@gmail.com; linliang@ieee.org; jinhuitang@njust.edu.cn; mhyang@ucmerced.edu			National Key R&D Program of China [2018AAA0102001]; National Natural Science Foundation of China [61922043, 61872421, 61732007]; Natural Science Foundation of Jiangsu Province [BK20180471]; Fundamental Research Funds for the Central Universities [30920041109]; National Science Foundation CAREER Award [1149783]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Jiangsu Province(Natural Science Foundation of Jiangsu Province); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); National Science Foundation CAREER Award(National Science Foundation (NSF))	This work was supported in part by the National Key R&D Program of China under Grant 2018AAA0102001, in part by the National Natural Science Foundation of China under Grants 61922043, 61872421, and 61732007, in part by the Natural Science Foundation of Jiangsu Province under Grant BK20180471, and in part by the Fundamental Research Funds for the Central Universities under Grant 30920041109). The work of M.-H. Yang's was supported in part by the National Science Foundation CAREER Award under Grant 1149783.	Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182; Anwar S., 2020, PROC IEEE C COMPUT V, P520; Anwar S, 2019, IEEE I CONF COMP VIS, P3155, DOI 10.1109/ICCV.2019.00325; Barron JT, 2016, LECT NOTES COMPUT SC, V9907, P617, DOI 10.1007/978-3-319-46487-9_38; Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952; Chang M, 2020, P EUR C COMP VIS, P171; Chen J, 2007, ACM T GRAPHIC, V26, DOI [10.1109/SARNOF.2007.4567317, 10.1145/1239451.1239554, 10.1145/1276377.1276506]; Chen QF, 2017, IEEE I CONF COMP VIS, P2516, DOI 10.1109/ICCV.2017.273; Criminisi A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857910; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Diebel James, 2005, NEURAL INF PROCESS S, P291; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Dong JX, 2018, LECT NOTES COMPUT SC, V11215, P777, DOI 10.1007/978-3-030-01252-6_46; Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574; Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666; Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127; Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964; Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592; Gu SH, 2017, PROC CVPR IEEE, P712, DOI 10.1109/CVPR.2017.83; Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181; Guo XJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1283, DOI 10.1145/3123266.3123378; Ham B, 2018, IEEE T PATTERN ANAL, V40, P192, DOI 10.1109/TPAMI.2017.2669034; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22; Ignatov A., 2018, PROC EUR C COMPUT VI, P315; Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355; Jampani V, 2016, PROC CVPR IEEE, P4452, DOI 10.1109/CVPR.2016.482; Jevnisek RJ, 2017, PROC CVPR IEEE, P3816, DOI 10.1109/CVPR.2017.406; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kim Y, 2020, PROC CVPR IEEE, P3479, DOI 10.1109/CVPR42600.2020.00354; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239547, 10.1145/1276377.1276497]; Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521; Lecouat Bruno, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P238, DOI 10.1007/978-3-030-58542-6_15; Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177; Li YJ, 2016, LECT NOTES COMPUT SC, V9908, P154, DOI 10.1007/978-3-319-46493-0_10; Liu SF, 2016, LECT NOTES COMPUT SC, V9908, P560, DOI 10.1007/978-3-319-46493-0_34; Liu Y., 2020, PROC IEEECVF C COMPU, P508; Lu S, 2014, PROC CVPR IEEE, P3390, DOI 10.1109/CVPR.2014.433; Ma ZY, 2013, IEEE I CONF COMP VIS, P49, DOI 10.1109/ICCV.2013.13; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Min DB, 2012, IEEE T IMAGE PROCESS, V21, P1176, DOI 10.1109/TIP.2011.2163164; Pan JS, 2019, PROC CVPR IEEE, P1702, DOI 10.1109/CVPR.2019.00180; Pan JS, 2018, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2018.00324; Pan JS, 2018, IEEE T PATTERN ANAL, V40, P2315, DOI 10.1109/TPAMI.2017.2753804; Pan JS, 2019, IEEE T PATTERN ANAL, V41, P1412, DOI 10.1109/TPAMI.2018.2832125; Pan JS, 2017, IEEE T PATTERN ANAL, V39, P342, DOI 10.1109/TPAMI.2016.2551244; Pan JS, 2016, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR.2016.180; Park J, 2011, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2011.6126423; Rhemann C, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995372; Riegler G., 2016, PROC BRIT MACH VIS C; Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349; Shen XY, 2017, INT J COMPUT VISION, V125, P19, DOI 10.1007/s11263-017-1021-y; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939; Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; Wang ZH, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107274; Wu HY, 2007, IEEE I CONF COMP VIS, P628, DOI 10.1109/cvpr.2007.383211; Wu HK, 2018, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2018.00197; Xiao JJ, 2006, LECT NOTES COMPUT SC, V3951, P211; Xiaohe Wu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P352, DOI 10.1007/978-3-030-58548-8_21; Xu L, 2015, PR MACH LEARN RES, V37, P1669; Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158; Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208; Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157; Yan Q, 2013, IEEE I CONF COMP VIS, P1537, DOI 10.1109/ICCV.2013.194; Yan ZC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2790296; Ye XC, 2020, IEEE T IMAGE PROCESS, V29, P7427, DOI 10.1109/TIP.2020.3002664; Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891; Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zhang Q, 2014, PROC CVPR IEEE, P2830, DOI 10.1109/CVPR.2014.362; Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53; Zhu J, 2017, IEEE IMAGE PROC, P4068; Zhuo SJ, 2010, PROC CVPR IEEE, P2440, DOI 10.1109/CVPR.2010.5539941; Zongsheng Yue, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P41, DOI 10.1007/978-3-030-58607-2_3; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	79	1	1	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8355	8370		10.1109/TPAMI.2021.3102575	http://dx.doi.org/10.1109/TPAMI.2021.3102575			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34357863				2022-12-18	WOS:000864325900076
J	Duan, PQ; Wang, ZHW; Shi, BX; Cossairt, O; Huang, TJ; Katsaggelos, AK				Duan, Peiqi; Wang, Zihao W.; Shi, Boxin; Cossairt, Oliver; Huang, Tiejun; Katsaggelos, Aggelos K.			Guided Event Filtering: Synergy Between Intensity Images and Neuromorphic Events for High Performance Imaging	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cameras; Sensors; Optical sensors; Spatial resolution; Optical imaging; High-speed optical techniques; Image reconstruction; Computational hybrid cameras; event-based imaging and vision; joint filtering	VISION	Many visual and robotics tasks in real-world scenarios rely on robust handling of high speed motion and high dynamic range (HDR) with effectively high spatial resolution and low noise. Such stringent requirements, however, cannot be directly satisfied by a single imager or imaging modality, rather by multi-modal sensors with complementary advantages. In this paper, we address high performance imaging by exploring the synergy between traditional frame-based sensors with high spatial resolution and low sensor noise, and emerging event-based sensors with high speed and high dynamic range. We introduce a novel computational framework, termed Guided Event Filtering (GEF), to process these two streams of input data and output a stream of super-resolved yet noise-reduced events. To generate high quality events, GEF first registers the captured noisy events onto the guidance image plane according to our flow model. it then performs joint image filtering that inherits the mutual structure from both inputs. Lastly, GEF re-distributes the filtered event frame in the space-time volume while preserving the statistical characteristics of the original events. When the guidance images under-perform, GEF incorporates an event self-guiding mechanism that resorts to neighbor events for guidance. We demonstrate the benefits of GEF by applying the output high quality events to existing event-based algorithms across diverse application categories, including high speed object tracking, depth estimation, high frame-rate video synthesis, and super resolution/HDR/color image restoration.	[Duan, Peiqi] Peking Univ, Natl Engn Lab Video Technol, Dept Comp Sci & Technol, Beijing 100000, Peoples R China; [Wang, Zihao W.; Cossairt, Oliver; Katsaggelos, Aggelos K.] Northwestern Univ, Evanston, IL 60208 USA; [Shi, Boxin; Huang, Tiejun] Peking Univ, Inst Artificial Intelligence, Dept Comp Sci & Technol, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China; [Shi, Boxin; Huang, Tiejun] Beijing Acad Artificial Intelligence, Beijing, Peoples R China	Peking University; Northwestern University; Peking University	Shi, BX (corresponding author), Peking Univ, Inst Artificial Intelligence, Dept Comp Sci & Technol, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.	duanqi0001@pku.edu.cn; zwinswang@gmail.com; boxin.shi@gmail.com; olivercossairt@gmail.com; tjhuang@pku.edu.cn; aggk@eecs.northwestern.edu			National Key R&D Program of China [2020AAA0105200]; National Natural Science Foundation of China [62136001, 62088102, 61872012]; Defense Advanced Research Projects Agency (DARPA) [GrantHR0011-172-0044]; NSF CAREER [IIS-1453192]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Defense Advanced Research Projects Agency (DARPA)(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work was supported in part by the National Key R&D Program of China under Grant 2020AAA0105200, in part by the National Natural Science Foundation of China under Grants 62136001, 62088102, and 61872012, in part by the Defense Advanced Research Projects Agency (DARPA) under GrantHR0011-172-0044, and in part by the NSF CAREER under Grant IIS-1453192. Peiqi Duan and Zihao W. Wang are equal contribution to this work.	Aggelos Katsaggelos, 2020, Arxiv, DOI arXiv:2005.00974; Alex Zihao Zhu, 2018, Arxiv, DOI arXiv:1802.06898; Almatrafi M, 2020, IEEE T PATTERN ANAL, V42, P1547, DOI 10.1109/TPAMI.2020.2986748; Andrea Censi, 2020, Arxiv, DOI arXiv:1904.08405; Baldwin RW, 2020, PROC CVPR IEEE, P1698, DOI 10.1109/CVPR42600.2020.00177; Barranco F, 2018, IEEE INT C INT ROBOT, P5764, DOI 10.1109/IROS.2018.8593380; Benosman R, 2014, IEEE T NEUR NET LEAR, V25, P407, DOI 10.1109/TNNLS.2013.2273537; Bhat P., 2007, P EUROGRAPHICS S REN, P327, DOI 10.2312; Chen SS, 2019, IEEE COMPUT SOC CONF, P1682, DOI 10.1109/CVPRW.2019.00214; Cheng WS, 2019, IEEE COMPUT SOC CONF, P1666, DOI 10.1109/CVPRW.2019.00210; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Duan P., 2021, PROC IEEE C COMPUT V, P12824; Gallego G, 2018, PROC CVPR IEEE, P3867, DOI 10.1109/CVPR.2018.00407; Gallego G, 2017, IEEE ROBOT AUTOM LET, V2, P632, DOI 10.1109/LRA.2016.2647639; Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2017.128; Gehrig D, 2018, LECT NOTES COMPUT SC, V11216, P766, DOI 10.1007/978-3-030-01258-8_46; Gehrig D, 2020, INT J COMPUT VISION, V128, P601, DOI 10.1007/s11263-019-01209-w; Guo XJ, 2020, IEEE T PATTERN ANAL, V42, P694, DOI 10.1109/TPAMI.2018.2883553; Gupta M, 2010, LECT NOTES COMPUT SC, V6311, P100, DOI 10.1007/978-3-642-15549-9_8; Han J, 2020, PROC CVPR IEEE, P1727, DOI 10.1109/CVPR42600.2020.00180; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; He K, 2020, OPT EXPRESS, V28, P12108, DOI 10.1364/OE.390719; Iliadis M, 2018, DIGIT SIGNAL PROCESS, V72, P9, DOI 10.1016/j.dsp.2017.09.010; Jiang Z, 2020, PROC CVPR IEEE, P3317, DOI 10.1109/CVPR42600.2020.00338; Li YJ, 2016, LECT NOTES COMPUT SC, V9908, P154, DOI 10.1007/978-3-319-46493-0_10; Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399; Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P120, DOI 10.1109/TIP.2014.2371234; Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337; Lin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8312, DOI 10.1109/CVPR42600.2020.00834; Liu D., 2020, ARXIV; Liu HJ, 2015, IEEE INT SYMP CIRC S, P722, DOI 10.1109/ISCAS.2015.7168735; Liu YL, 2020, PROC CVPR IEEE, P1648, DOI 10.1109/CVPR42600.2020.00172; Malvar HS, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P485; Maqueda AI, 2018, PROC CVPR IEEE, P5419, DOI 10.1109/CVPR.2018.00568; Miyatani Y, 2016, IEEE WINT CONF APPL; Mostafavi ISM, 2020, PROC CVPR IEEE, P2765, DOI 10.1109/CVPR42600.2020.00284; Mueggler E, 2017, INT J ROBOT RES, V36, P142, DOI 10.1177/0278364917691115; Paliwal A, 2020, IEEE T PATTERN ANAL, V42, P1557, DOI 10.1109/TPAMI.2020.2987316; Pan LY, 2020, PROC CVPR IEEE, P1669, DOI 10.1109/CVPR42600.2020.00174; Pan LY, 2019, PROC CVPR IEEE, P6813, DOI 10.1109/CVPR.2019.00698; Park J, 2011, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2011.6126423; Qu JH, 2017, IEEE GEOSCI REMOTE S, V14, P2152, DOI 10.1109/LGRS.2017.2755679; Rebecq H, 2019, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2019.00398; Reinbacher C., 2016, P BRIT MACH VIS C; Rueckauer B, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00176; Scheerlinck C, 2019, IEEE COMPUT SOC CONF, P1684, DOI 10.1109/CVPRW.2019.00215; Scheerlinck C, 2019, LECT NOTES COMPUT SC, V11365, P308, DOI 10.1007/978-3-030-20873-8_20; Schubert F., 2008, PROC BRIT MACH VIS C, P1; Seifozzakerini S., 2016, PROC BRIT MACH VIS C, P1; Sekikawa Y, 2019, PROC CVPR IEEE, P3882, DOI 10.1109/CVPR.2019.00401; Shen XY, 2015, IEEE I CONF COMP VIS, P3406, DOI 10.1109/ICCV.2015.389; Stankovic Vladimir, 2008, PROC 16 EUR SIGNAL P, P1; Stoffregen T, 2019, IEEE I CONF COMP VIS, P7243, DOI 10.1109/ICCV.2019.00734; Su H, 2019, PROC CVPR IEEE, P11158, DOI 10.1109/CVPR.2019.01142; Vasco V, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4144, DOI 10.1109/IROS.2016.7759610; Vidal AR, 2018, IEEE ROBOT AUTOM LET, V3, P994, DOI 10.1109/LRA.2018.2793357; Wang L, 2019, PROC CVPR IEEE, P10073, DOI 10.1109/CVPR.2019.01032; Wang YX, 2019, PROC CVPR IEEE, P6351, DOI 10.1109/CVPR.2019.00652; Wang ZH, 2017, OPT EXPRESS, V25, P250, DOI 10.1364/OE.25.000250; Wang ZHW, 2019, IEEE INT CONF COMP V, P4320, DOI 10.1109/ICCVW.2019.00532; Wang ZHW, 2020, PROC CVPR IEEE, P1606, DOI 10.1109/CVPR42600.2020.00168; Xu J, 2020, IEEE T COMPUT IMAG, V6, P604, DOI 10.1109/TCI.2020.2964255; Xu L, 2020, PROC CVPR IEEE, P4967, DOI 10.1109/CVPR42600.2020.00502; Yan Q, 2013, IEEE I CONF COMP VIS, P1537, DOI 10.1109/ICCV.2013.194; Yang Y, 2020, IEEE T PATTERN ANAL, V42, P521, DOI 10.1109/TPAMI.2018.2883941; Zabrodsky H, 1990, J VIS COMMUN IMAGE R, V1, P189; Zhu AZ, 2019, PROC CVPR IEEE, P989, DOI 10.1109/CVPR.2019.00108	67	1	1	6	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8261	8275		10.1109/TPAMI.2021.3113344	http://dx.doi.org/10.1109/TPAMI.2021.3113344			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34543190				2022-12-18	WOS:000864325900070
J	Han, K; Wang, YH; Xu, C; Xu, CJ; Wu, EH; Tao, DC				Han, Kai; Wang, Yunhe; Xu, Chang; Xu, Chunjing; Wu, Enhua; Tao, Dacheng			Learning Versatile Convolution Filters for Efficient Visual Recognition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Convolution; Neural networks; Quantization (signal); Computational modeling; Convolutional neural networks; Visualization; Redundancy; CNN compression; CNN speed-up; versatile filters		This paper introduces versatile filters to construct efficient convolutional neural networks that are widely used in various visual recognition tasks. Considering the demands of efficient deep learning techniques running on cost-effective hardware, a number of methods have been developed to learn compact neural networks. Most of these works aim to slim down filters in different ways, e.g., investigating small, sparse or quantized filters. In contrast, we treat filters from an additive perspective. A series of secondary filters can be derived from a primary filter with the help of binary masks. These secondary filters all inherit in the primary filter without occupying more storage, but once been unfolded in computation they could significantly enhance the capability of the filter by integrating information extracted from different receptive fields. Besides spatial versatile filters, we additionally investigate versatile filters from the channel perspective. Binary masks can be further customized for different primary filters under orthogonal constraints. We conduct theoretical analysis on network complexity and an efficient convolution scheme is introduced. Experimental results on benchmark datasets and neural networks demonstrate that our versatile filters are able to achieve comparable accuracy as that of original filters, but require less memory and computation cost.	[Han, Kai; Wu, Enhua] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China; [Han, Kai; Wu, Enhua] Univ Chinese Acad Sci, Beijing 100049, Peoples R China; [Han, Kai; Wang, Yunhe; Xu, Chunjing] Huawei Technol, Noahs Ark Lab, Shenzhen 518129, Guangdong, Peoples R China; [Xu, Chang; Tao, Dacheng] Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia; [Wu, Enhua] Univ Macau, FST, Macau 999078, Macao, Peoples R China	Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Huawei Technologies; University of Sydney; University of Macau	Wu, EH (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.; Wu, EH (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.; Xu, C (corresponding author), Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia.	hankai@ios.ac.cn; yunhe.wang@huawei.com; c.xu@sydney.edu.au; xuchunjing@huawei.com; ehwu@umac.mo; dacheng.tao@sydney.edu.au		Han, Kai/0000-0002-9761-2702; wu, en hua/0000-0002-2174-1428; Xu, Chang/0000-0002-4756-0609	NSFC [61632003, 62072449]; Macau FDCT [0018/2019/AKP, SKL-IOTSC-2021-2023]; Australian Research Council [DE180101438, DP210101859]	NSFC(National Natural Science Foundation of China (NSFC)); Macau FDCT; Australian Research Council(Australian Research Council)	This work was supported in part by NSFC under Grants 61632003 and 62072449, and in part by Macau FDCT under Grants 0018/2019/AKP and SKL-IOTSC-2021-2023, and in part by the Australian Research Council under Grants DE180101438 and DP210101859.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Andrew G. Howard, 2017, Arxiv, DOI arXiv:1704.04861; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arora S, 2014, PR MACH LEARN RES, V32; Bengio Y., 2013, ARXIV; Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135; Bichen Wu, 2018, Arxiv, DOI arXiv:1812.00090; Cai H., 2019, 2019 IEEE 27 INT C N, P1; Chang Xu, 2020, Arxiv, DOI arXiv:2009.08695; Chen HT, 2019, IEEE I CONF COMP VIS, P3513, DOI 10.1109/ICCV.2019.00361; Chen YP, 2019, IEEE I CONF COMP VIS, P3434, DOI 10.1109/ICCV.2019.00353; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Courbariaux M, 2015, ADV NEUR IN, V28; Daniel Soudry, 2016, Arxiv, DOI arXiv:1602.02830; Denton E, 2014, ADV NEUR IN, V27; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Figurnov Mikhail, 2016, NEURIPS; Forrest N. Iandola, 2016, Arxiv, DOI arXiv:1602.07360; Gao HY, 2018, ADV NEUR IN, V31; Geoffrey Hinton, 2015, Arxiv, DOI arXiv:1503.02531; Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165; Han K, 2020, PR MACH LEARN RES, V119; Han S., 2016, P 4 INT C LEARN REPR, P1; Han S, 2015, ADV NEUR IN, V28; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140; Huang ZH, 2018, LECT NOTES COMPUT SC, V11220, P317, DOI 10.1007/978-3-030-01270-0_19; Hubara I, 2016, ADV NEUR IN, V29; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; Jeon Y, 2018, ADV NEUR IN, V31; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kim Y., 2016, ICLR; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lavin A, 2016, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2016.435; Lebedev Vadim, 2015, INT C LEARN REPR; Li H., 2017, P INT C LEARN REPR I, P1; Lin MB, 2020, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR42600.2020.00160; Lin SH, 2019, PROC CVPR IEEE, P2785, DOI 10.1109/CVPR.2019.00290; Lin T.-Y., 2014, P EUROPEAN C COMPUTE, P740; Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44; Liu ZC, 2019, IEEE I CONF COMP VIS, P3295, DOI [10.1109/ICCV.2019.00339, 10.1109/ICCV.2019.00339D\]; Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; Molchanov P, 2019, PROC CVPR IEEE, P11256, DOI 10.1109/CVPR.2019.01152; nihui, 2017, NCNN; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Romero Adriana, 2015, ICLR; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Savarese P., 2019, PROC INT C LEARN REP, P1; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293; Tan MX, 2019, PR MACH LEARN RES, V97; Tan Mingxing, 2019, BRIT MACH VIS C BMVC; Vanhoucke V., 2011, P ADV NEUR INF PROC, P1; Wan Alvin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12962, DOI 10.1109/CVPR42600.2020.01298; Wang YH, 2016, ADV NEUR IN, V29, P253; Wang YH, 2017, PR MACH LEARN RES, V70; Wang YH, 2018, ADV NEUR IN, V31; Wen W, 2016, ADV NEUR IN, V29; Wu BC, 2018, PROC CVPR IEEE, P9127, DOI 10.1109/CVPR.2018.00951; Wu JX, 2016, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2016.521; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yang JW, 2019, PROC CVPR IEEE, P7300, DOI 10.1109/CVPR.2019.00748; Yang Y, 2022, PHYSIOTHER THEOR PR, V38, P847, DOI 10.1080/09593985.2020.1805834; Yu, 2019, ICLR, P1; Yu RC, 2018, PROC CVPR IEEE, P9194, DOI 10.1109/CVPR.2018.00958; Zhang DQ, 2018, LECT NOTES COMPUT SC, V11212, P373, DOI 10.1007/978-3-030-01237-3_23; Zhang T, 2017, IEEE I CONF COMP VIS, P4383, DOI 10.1109/ICCV.2017.469; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zoph B., 2017, P1	82	1	1	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7731	7746		10.1109/TPAMI.2021.3114368	http://dx.doi.org/10.1109/TPAMI.2021.3114368			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34546919	Green Submitted			2022-12-18	WOS:000864325900035
J	Jiang, DY; Liu, XC; Luo, JW; Liao, ZP; Velten, A; Lou, X				Jiang, Deyang; Liu, Xiaochun; Luo, Jianwen; Liao, Zhengpeng; Velten, Andreas; Lou, Xin			Ring and Radius Sampling Based Phasor Field Diffraction Algorithm for Non-Line-of-Sight Reconstruction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Non-line-of-sight (NLOS) imaging; rayleigh-sommerfeld diffraction (RSD); ring sampling; radius sampling	WAVES	Non-Line-of-Sight (NLOS) imaging reconstructs occluded scenes based on indirect diffuse reflections. The computational complexity and memory consumption of existing NLOS reconstruction algorithms make them challenging to be implemented in real-time. This paper presents a fast and memory-efficient phasor field-diffraction-based NLOS reconstruction algorithm. In the proposed algorithm, the radial property of the Rayleigh Sommerfeld diffraction (RSD) kernels along with the linear property of Fourier transformare utilized to reconstruct the Fourier domain representations of RSD kernels using a set of kernel bases. Moreover, memory consumption is further reduced by sampling the kernel bases in a radius direction and constructing them during the run-time. According to the analysis, the memory efficiency can be improved by as much as 220 x. Experimental results show that compared with the original RSD algorithm, the reconstruction time of the proposed algorithm is significantly reduced with little impact on the final imaging quality.	[Jiang, Deyang; Luo, Jianwen; Liao, Zhengpeng; Lou, Xin] ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China; [Jiang, Deyang] Royal Inst Technol, Sch Elect Engn & Comp Sci, S-10044 Stockholm, Sweden; [Liu, Xiaochun; Velten, Andreas] Univ Wisconsin, Dept Elect & Comp Engn, Madison, WI 53705 USA; [Liu, Xiaochun; Velten, Andreas] Univ Wisconsin, Dept Biostat & Med Informat, Madison, WI 53705 USA	ShanghaiTech University; Royal Institute of Technology; University of Wisconsin System; University of Wisconsin Madison; University of Wisconsin System; University of Wisconsin Madison	Lou, X (corresponding author), ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China.	jiangdy98@gmail.com; xliu669@wisc.edu; luojw@shanghaitech.edu.cn; liaozhp@shanghaitech.edu.cn; velten@gmail.com; louxin@shanghaitech.edu.cn		Jiang, Deyang/0000-0002-2726-9320; Zhengpeng, Liao/0000-0002-1725-9719; LUO, Jianwen/0000-0002-0186-6207; Liu, Xiaochun/0000-0002-3536-511X	Natural Science Foundation of China [61801292]; Shanghai Rising-Star Program [21QC1401400]	Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shanghai Rising-Star Program	This work was supported in part by the Natural Science Foundation of China under Grant 61801292 and in part by the Shanghai Rising-Star Program under Grant 21QC1401400.	Ahn B, 2019, IEEE I CONF COMP VIS, P7888, DOI 10.1109/ICCV.2019.00798; Altmann Y, 2018, SCIENCE, V361, P660, DOI 10.1126/science.aat2298; Arellano V, 2017, OPT EXPRESS, V25, P11574, DOI 10.1364/OE.25.011574; Baddour N, 2011, ADV IMAG ELECT PHYS, V165, P1, DOI 10.1016/B978-0-12-385861-0.00001-4; Batarseh M, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-05985-w; Bouman KL, 2017, IEEE I CONF COMP VIS, P2289, DOI 10.1109/ICCV.2017.249; Bracewell R, 2004, FOURIER ANAL IMAGING, P128; Buttafava M, 2015, OPT EXPRESS, V23, P20997, DOI 10.1364/OE.23.020997; Chen WZ, 2019, PROC CVPR IEEE, P3783, DOI 10.1109/CVPR.2019.00695; Dove J, 2019, OPT EXPRESS, V27, P18016, DOI 10.1364/OE.27.018016; Guillen I, 2020, INT CONF ACOUST SPEE, P9269, DOI 10.1109/ICASSP40776.2020.9052985; Gupta M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2735702; Gupta O, 2012, OPT EXPRESS, V20, P19096, DOI 10.1364/OE.20.019096; Heide F, 2014, PROC CVPR IEEE, P3222, DOI 10.1109/CVPR.2014.418; Iseringhausen J, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3368314; Isogawa Mariko, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P193, DOI 10.1007/978-3-030-58571-6_12; Katz O, 2014, NAT PHOTONICS, V8, P784, DOI [10.1038/nphoton.2014.189, 10.1038/NPHOTON.2014.189]; Kirmani A, 2009, IEEE I CONF COMP VIS, P159, DOI 10.1109/ICCV.2009.5459160; Klein J, 2016, SCI REP-UK, V6, DOI 10.1038/srep32491; La Manna M, 2019, IEEE T PATTERN ANAL, V41, P1615, DOI 10.1109/TPAMI.2018.2843363; Laurenzis M, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.6.063003; Lindell DB, 2019, PROC CVPR IEEE, P3773, DOI 10.1109/CVPR.2019.00694; Lindell DB, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322937; Liu X, 2020, 2020 IEEE WORKSHOP ON WIDE BANDGAP POWER DEVICES AND APPLICATIONS IN ASIA (WIPDA ASIA), DOI 10.1109/WiPDAAsia49671.2020.9360278; Liu XC, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-15157-4; Liu XC, 2019, NATURE, V572, P620, DOI 10.1038/s41586-019-1461-3; Marhenke T, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19040863; O'Toole M, 2018, NATURE, V555, P338, DOI 10.1038/nature25489; Panda PR, 2000, ACM T DES AUTOMAT EL, V5, P682, DOI 10.1145/348019.348570; Pediredla AK, 2017, IEEE INT CONF COMPUT, P1; Ramesh R., 2008, 5D TIME LIGHT TRANSP; Renna Marco, 2020, Instruments, V4, DOI 10.3390/instruments4020014; Reza SA, 2019, OPT EXPRESS, V27, P32587, DOI 10.1364/OE.27.032587; Saunders C, 2019, NATURE, V565, P472, DOI 10.1038/s41586-018-0868-6; Tawk T., 2018, PROC INT SOC OPT PHO; Teichman JA, 2019, OPT EXPRESS, V27, P27500, DOI 10.1364/OE.27.027500; Tsai CY, 2019, PROC CVPR IEEE, P1545, DOI 10.1109/CVPR.2019.00164; Tsai CY, 2017, PROC CVPR IEEE, P2336, DOI 10.1109/CVPR.2017.251; Veerman JAC, 2005, J OPT SOC AM A, V22, P636, DOI 10.1364/JOSAA.22.000636; Velten A, 2012, NAT COMMUN, V3, DOI 10.1038/ncomms1747; Xin SM, 2019, PROC CVPR IEEE, P6793, DOI 10.1109/CVPR.2019.00696; Xu FH, 2018, OPT EXPRESS, V26, P9945, DOI 10.1364/OE.26.009945	42	1	1	13	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7841	7853		10.1109/TPAMI.2021.3117962	http://dx.doi.org/10.1109/TPAMI.2021.3117962			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34613909				2022-12-18	WOS:000864325900042
J	Li, M; Ma, L				Li, Meng; Ma, Li			Learning Asymmetric and Local Features in Multi-Dimensional Data Through Wavelets With Recursive Partitioning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Wavelet analysis; Bayes methods; Computational modeling; Data models; Discrete wavelet transforms; Analytical models; Indexes; Bayesian inference; hierarchical models; multi-resolution analysis; image processing; sparse coding; wavelets	REPRESENTATIONS; SELECTION; SPARSE; INNER	Effective learning of asymmetric and local features in images and other data observed on multi-dimensional grids is a challenging objective critical for a wide range of image processing applications involving biomedical and natural images. It requires methods that are sensitive to local details while fast enough to handle massive numbers of images of ever increasing sizes. We introduce a probabilistic model-based framework that achieves these objectives by incorporating adaptivity into discrete wavelet transforms (DWT) through Bayesian hierarchical modeling, thereby allowing wavelet bases to adapt to the geometric structure of the data while maintaining the high computational scalability of wavelet methods-linear in the sample size (e.g., the resolution of an image). We derive a recursive representation of the Bayesian posterior model which leads to an exact message passing algorithm to complete learning and inference. While our framework is applicable to a range of problems including multi-dimensional signal processing, compression, and structural learning, we illustrate its work and evaluate its performance in the context of image reconstruction using real images from the ImageNet database, two widely used benchmark datasets, and a dataset from retinal optical coherence tomography and compare its performance to state-of-the-art methods based on basis transforms and deep learning.	[Li, Meng] Rice Univ, Dept Stat, Houston, TX 77025 USA; [Ma, Li] Duke Univ, Dept Stat Sci, Durham, NC 27708 USA	Rice University; Duke University	Ma, L (corresponding author), Duke Univ, Dept Stat Sci, Durham, NC 27708 USA.	meng@rice.edu; li.ma@duke.edu		Ma, Li/0000-0002-0159-3296; Li, Meng/0000-0003-2123-2444	NSF [DMS-1749789, DMS-2013930]; ORAU Ralph E. Powe Junior Faculty Enhancement Award	NSF(National Science Foundation (NSF)); ORAU Ralph E. Powe Junior Faculty Enhancement Award	The authors are very grateful to an AE and three reviewers for providing extremely helpful comments and suggestions. They also thank Daniel Bourgeois for his help in porting our C++ code to R. The work of Meng Li's was supported in part by NSF under Grant DMS-2015569 and an ORAU Ralph E. Powe Junior Faculty Enhancement Award. The work of Li Ma's was supported in part by NSF under Grants DMS-1749789 and DMS-2013930.	Abramovich F, 1998, J ROY STAT SOC B, V60, P725, DOI 10.1111/1467-9868.00151; Aho AV., 1973, FIBONACCI QUART, V11, P429; Alasil T, 2010, OPHTHALMOLOGY, V117, P2379, DOI 10.1016/j.ophtha.2010.03.051; ALI ST, 2000, GRAD TEXT C, P1; Brown PJ, 2001, J AM STAT ASSOC, V96, P398, DOI 10.1198/016214501753168118; Bussel II, 2014, BRIT J OPHTHALMOL, V98, P15, DOI 10.1136/bjophthalmol-2013-304326; Cai TT, 1999, ANN STAT, V27, P898, DOI 10.1214/aos/1018031262; Castro R, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P992; Chipman HA, 1998, J AM STAT ASSOC, V93, P935, DOI 10.2307/2669832; Chipman HA, 1997, J AM STAT ASSOC, V92, P1413, DOI 10.2307/2965411; Clyde M, 2000, J ROY STAT SOC B, V62, P681, DOI 10.1111/1467-9868.00257; Coifman R. R., 1995, TRANSLATION INVARIAN, P125, DOI [DOI 10.1007/978-1-4612-2544-7_9, 10.1007/978-1-4612-2544-7]; Coupe P, 2008, IEEE T MED IMAGING, V27, P425, DOI 10.1109/TMI.2007.906087; Crouse MS, 1998, IEEE T SIGNAL PROCES, V46, P886, DOI 10.1109/78.668544; Cuenca N, 2018, OPHTHALMOLOGY, V125, P407, DOI 10.1016/j.ophtha.2017.09.016; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Denison DGT, 1998, BIOMETRIKA, V85, P363; Donoho DL, 1999, ANN STAT, V27, P859, DOI 10.1214/aos/1018031261; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.1080/01621459.1995.10476626; Duker JS, 2013, OPHTHALMOLOGY, V120, P2611, DOI 10.1016/j.ophtha.2013.07.042; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Fang LY, 2017, IEEE T MED IMAGING, V36, P407, DOI 10.1109/TMI.2016.2611503; Fang LY, 2012, BIOMED OPT EXPRESS, V3, P927, DOI 10.1364/BOE.3.000927; Fryzlewicz P, 2016, J COMPUT GRAPH STAT, V25, P879, DOI 10.1080/10618600.2015.1048345; Grewal DS, 2013, CURR OPIN OPHTHALMOL, V24, P150, DOI 10.1097/ICU.0b013e32835d9e27; Hoeting JA, 1999, STAT SCI, V14, P382, DOI 10.1214/ss/1009212519; HUANG D, 1991, SCIENCE, V254, P1178, DOI 10.1126/science.1957169; Huang WC, 2014, INVEST OPHTH VIS SCI, V55, P1810, DOI 10.1167/iovs.13-13768; Jacques L, 2011, SIGNAL PROCESS, V91, P2699, DOI 10.1016/j.sigpro.2011.04.025; Johnstone IM, 2005, ANN STAT, V33, P1700, DOI 10.1214/009053605000000345; Johnstone IM, 1997, J R STAT SOC B, V59, P319, DOI 10.1111/1467-9868.00071; Kafieh R, 2015, J OPHTHALMOL, V2015, DOI 10.1155/2015/259123; Keane PA, 2012, SURV OPHTHALMOL, V57, P389, DOI 10.1016/j.survophthal.2012.01.006; Li M, 2015, IEEE T IMAGE PROCESS, V24, P4876, DOI 10.1109/TIP.2015.2470601; Li M, 2014, BAYESIAN ANAL, V9, P733, DOI 10.1214/14-BA871; Liu R., 2020, PROC IEEECVF C COMPU, p14 294; Ma L, 2018, J AM STAT ASSOC, V113, P802, DOI 10.1080/01621459.2017.1286241; Ma L, 2013, J AM STAT ASSOC, V108, P1493, DOI 10.1080/01621459.2013.838899; Mallat S, 2009, WAVELET TOUR OF SIGNAL PROCESSING: THE SPARSE WAY, P1; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Moghaddam B, 2007, IEEE I CONF COMP VIS, P2073, DOI 10.1109/cvpr.2007.383092; Morris JS, 2006, J R STAT SOC B, V68, P179, DOI 10.1111/j.1467-9868.2006.00539.x; Mukherjee PS, 2011, TECHNOMETRICS, V53, P196, DOI 10.1198/TECH.2011.10070; Oishi A, 2018, RETINA-J RET VIT DIS, V38, P1331, DOI 10.1097/IAE.0000000000001688; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Polzehl J, 2000, J R STAT SOC B, V62, P335, DOI 10.1111/1467-9868.00235; Raftery AE, 1995, SOCIOL METHODOL, V25, P111, DOI 10.2307/271063; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Shi F, 2015, IEEE T MED IMAGING, V34, P441, DOI 10.1109/TMI.2014.2359980; Sun JK, 2014, JAMA OPHTHALMOL, V132, P1309, DOI 10.1001/jamaophthalmol.2014.2350; Virgili G, 2015, COCHRANE DB SYST REV, DOI 10.1002/14651858.CD008081.pub3; Willett RM, 2004, 2004 2ND IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING: MACRO TO NANO, VOLS 1 and 2, P1192; Wong WH, 2010, ANN STAT, V38, P1433, DOI 10.1214/09-AOS755; Xu J, 2015, IEEE I CONF COMP VIS, P244, DOI 10.1109/ICCV.2015.36; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zhou MY, 2012, IEEE T IMAGE PROCESS, V21, P130, DOI 10.1109/TIP.2011.2160072	58	1	1	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7674	7687		10.1109/TPAMI.2021.3110403	http://dx.doi.org/10.1109/TPAMI.2021.3110403			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34495829	Green Submitted			2022-12-18	WOS:000864325900031
J	Liu, ZJ; Tang, HT; Zhao, SY; Shao, K; Han, S				Liu, Zhijian; Tang, Haotian; Zhao, Shengyu; Shao, Kevin; Han, Song			PVNAS: 3D Neural Architecture Search With Point-Voxel Convolution	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Convolution; Solid modeling; Random access memory; Computational modeling; Memory management; Neural networks; 3D point cloud; neural architecture search; efficient deep learning; autonomous driving	NETWORKS	3D neural networks are widely used in real-world applications (e.g., AR/VR headsets, self-driving cars). They are required to be fast and accurate; however, limited hardware resources on edge devices make these requirements rather challenging. Previous work processes 3D data using either voxel-based or point-based neural networks, but both types of 3D models are not hardware-efficient due to the large memory footprint and random memory access. In this paper, we study 3D deep learning from the efficiency perspective. We first systematically analyze the bottlenecks of previous 3D methods. We then combine the best from point-based and voxel-based models together and propose a novel hardware-efficient 3D primitive, Point-Voxel Convolution (PVConv). We further enhance this primitive with the sparse convolution to make it more effective in processing large (outdoor) scenes. Based on our designed 3D primitive, we introduce 3D Neural Architecture Search (3D-NAS) to explore the best 3D network architecture given a resource constraint. We evaluate our proposed method on six representative benchmark datasets, achieving state-of-the-art performance with 1.8-23.7x measured speedup. Furthermore, our method has been deployed to the autonomous racing vehicle of MIT Driverless, achieving larger detection range, higher accuracy and lower latency.	[Liu, Zhijian; Tang, Haotian; Zhao, Shengyu; Shao, Kevin; Han, Song] MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Liu, ZJ (corresponding author), MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA.	zhijian@mit.edu; kentang@mit.edu; shengyuz@mit.edu; kshao23@mit.edu; songhan@mit.edu			NSF CAREER Award [1943349]; MIT Quest for Intelligence; MIT-IBM Watson AI Lab; Samsung; Hyundai; SONY	NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); MIT Quest for Intelligence; MIT-IBM Watson AI Lab(International Business Machines (IBM)); Samsung(Samsung); Hyundai; SONY	The authors would like to thank AWS Machine Learning Research awards for providing the computational resource. This work was supported in part by the NSF CAREER Award under Grant 1943349, in part by the MIT Quest for Intelligence, in part by the MIT-IBM Watson AI Lab, in part by Samsung, in part by Hyundai, and in part by SONY. Zhijian Liu and Haotian Tang contributed equally to this work.	Ali Thabet, 2020, Arxiv, DOI arXiv:2008.10309; Alonso I, 2020, IEEE ROBOT AUTOM LET, V5, P5432, DOI 10.1109/LRA.2020.3007440; Andrew G. Howard, 2017, Arxiv, DOI arXiv:1704.04861; Angel X. Chang, 2015, Arxiv, DOI arXiv:1512.03012; Armeni I., 2017, ARXIV; Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170; Bae W, 2019, LECT NOTES COMPUT SC, V11765, P228, DOI 10.1007/978-3-030-32245-8_26; Behley J, 2019, IEEE I CONF COMP VIS, P9296, DOI 10.1109/ICCV.2019.00939; Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164; Cai H, 2020, IEEE MICRO, V40, P75, DOI 10.1109/MM.2019.2953153; Cai Han, 2019, INT C LEARN REPR; Cai Han, 2020, ICLR; Chen YK, 2019, ADV NEUR IN, V32; Chenfeng Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P1, DOI 10.1007/978-3-030-58604-1_1; Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319; Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Eren Erdal Aksoy, 2020, Arxiv, DOI arXiv:2003.03653; Forrest N. Iandola, 2016, Arxiv, DOI arXiv:1602.07360; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961; Han S., 2016, PROC INT C LEARN REP; Han S, 2015, ADV NEUR IN, V28; He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48; Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140; Huang QG, 2018, PROC CVPR IEEE, P2626, DOI 10.1109/CVPR.2018.00278; Ioffe S., 2015, P 32 INT C MACH LEAR; Kim S, 2019, LECT NOTES COMPUT SC, V11766, P220, DOI 10.1007/978-3-030-32248-9_25; Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99; Lan SY, 2019, PROC CVPR IEEE, P998, DOI 10.1109/CVPR.2019.00109; Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479; Le Eric-Tuan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9500, DOI 10.1109/CVPR42600.2020.00952; Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959; Lei H, 2019, PROC CVPR IEEE, P9623, DOI 10.1109/CVPR.2019.00986; Li GH, 2020, PROC CVPR IEEE, P1617, DOI 10.1109/CVPR42600.2020.00169; Li GH, 2019, IEEE I CONF COMP VIS, P9266, DOI 10.1109/ICCV.2019.00936; Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979; Li MY, 2020, PROC CVPR IEEE, P5283, DOI 10.1109/CVPR42600.2020.00533; Li YY, 2018, ADV NEUR IN, V31; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Liu H., 2019, PROC INT C LEARN REP; Liu ZC, 2019, IEEE I CONF COMP VIS, P3295, DOI [10.1109/ICCV.2019.00339, 10.1109/ICCV.2019.00339D\]; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; Ma ZJ, 2020, IEEE ACCESS, V8, P12942, DOI 10.1109/ACCESS.2019.2961715; Maas A.L., 2013, ICML WORKSHOP DEEP L; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100; Qi CR, 2017, ADV NEUR IN, V30; Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102; Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112; Radosavovic I, 2019, IEEE I CONF COMP VIS, P1882, DOI 10.1109/ICCV.2019.00197; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086; Stamoulis D, 2020, LECT NOTES ARTIF INT, V11907, P481, DOI 10.1007/978-3-030-46147-8_29; Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268; Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293; Tan MX, 2019, PR MACH LEARN RES, V97; Tatarchenko M, 2018, PROC CVPR IEEE, P3887, DOI 10.1109/CVPR.2018.00409; Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067; Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651; Wang Hanrui, 2020, ARXIV200514187, P1; Wang K, 2019, PROC CVPR IEEE, P8604, DOI 10.1109/CVPR.2019.00881; Wang K, 2020, INT J COMPUT VISION, V128, P2035, DOI 10.1007/s11263-020-01339-6; Wang PS, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275050; Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Wang TZ, 2020, PROC CVPR IEEE, P2075, DOI 10.1109/CVPR42600.2020.00215; Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Wang ZJ, 2020, IEEE T VIS COMPUT GR, V26, P2919, DOI 10.1109/TVCG.2019.2896310; Wong KCL, 2019, LECT NOTES COMPUT SC, V11766, P393, DOI 10.1007/978-3-030-32248-9_44; Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6; Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337; Yang D, 2019, LECT NOTES COMPUT SC, V11765, P3, DOI 10.1007/978-3-030-32245-8_1; Yang TJ, 2018, LECT NOTES COMPUT SC, V11214, P289, DOI 10.1007/978-3-030-01249-6_18; Yang Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9598, DOI 10.1109/CVPR42600.2020.00962; Yu QH, 2020, PROC CVPR IEEE, P4125, DOI 10.1109/CVPR42600.2020.00418; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhou A., 2017, PROC INT C LEARN REP; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472; Zhu ZT, 2019, INT CONF 3D VISION, P240, DOI 10.1109/3DV.2019.00035; Zoph B., 2017, P1; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	92	1	1	7	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8552	8568		10.1109/TPAMI.2021.3109025	http://dx.doi.org/10.1109/TPAMI.2021.3109025			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34469291	Green Submitted			2022-12-18	WOS:000864325900089
J	Nayak, GK; Mopuri, KR; Jain, S; Chakraborty, A				Nayak, Gaurav Kumar; Mopuri, Konda Reddy; Jain, Saksham; Chakraborty, Anirban			Mining Data Impressions From Deep Models as Substitute for the Unavailable Training Data	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Data models; Adaptation models; Training data; Data mining; Training; Task analysis; Computational modeling; Data impressions; proxy data; synthetic transfer set; surrogate data; absence of training data; knowledge distillation; universal adversarial perturbations; continual learning; unsupervised domain adaptation		Pretrained deep models hold their learnt knowledge in the form of model parameters. These parameters act as "memory" for the trained models and help them generalize well on unseen data. However, in absence of training data, the utility of a trained model is merely limited to either inference or better initialization towards a target task. In this paper, we go further and extract synthetic data by leveraging the learnt model parameters. We dub them Data Impressions, which act as proxy to the training data and can be used to realize a variety of tasks. These are useful in scenarios where only the pretrained models are available and the training data is not shared (e.g., due to privacy or sensitivity concerns). We show the applicability of data impressions in solving several computer vision tasks such as unsupervised domain adaptation, continual learning as well as knowledge distillation. We also study the adversarial robustness of lightweight models trained via knowledge distillation using these data impressions. Further, we demonstrate the efficacy of data impressions in generating data-free Universal Adversarial Perturbations (UAPs) with better fooling rates. Extensive experiments performed on benchmark datasets demonstrate competitive performance achieved using data impressions in absence of original training data.	[Nayak, Gaurav Kumar; Chakraborty, Anirban] Indian Inst Sci, Dept Computat & Data Sci, Bangalore 560012, Karnataka, India; [Mopuri, Konda Reddy] Indian Inst Technol Tirupati, Dept Comp Sci & Engn, Tirupati 517506, Andhra Pradesh, India; [Jain, Saksham] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA	Indian Institute of Science (IISC) - Bangalore; Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Tirupati; Duke University	Chakraborty, A (corresponding author), Indian Inst Sci, Dept Computat & Data Sci, Bangalore 560012, Karnataka, India.	gauravnayak@iisc.ac.in; kmopuri@iittp.ac.in; saksham.jain@duke.edu; anirban@iisc.ac.in			Start-up Research Grant (SRG), SERB, DST, India [SRG/2019/001938]; DAE-BRNS, India [59/20/11/2020-BRNS]	Start-up Research Grant (SRG), SERB, DST, India; DAE-BRNS, India(Department of Atomic Energy (DAE)Board of Research in Nuclear Sciences (BRNS))	This work was supported in part by the Start-up Research Grant (SRG), SERB, DST, India, under Project SRG/2019/001938 and in part by the Young Scientist Research Award, DAE-BRNS, India, under Grant 59/20/11/2020-BRNS. (Corresponding author: Anirban Chakraborty.)	Addepalli S, 2020, AAAI CONF ARTIF INTE, V34, P3130; Alexey Kurakin, 2017, Arxiv, DOI arXiv:1611.01236; Balakrishnan N., 2004, PRIMER STAT DISTRIBU; Bucilua Cristian, 2006, P 12 ACM SIGKDD INT, P535, DOI [10.1145/1150402.1150464, DOI 10.1145/1150402.1150464]; Castro FM, 2018, LECT NOTES COMPUT SC, V11216, P241, DOI 10.1007/978-3-030-01258-8_15; Chen HT, 2019, IEEE I CONF COMP VIS, P3513, DOI 10.1109/ICCV.2019.00361; Eric Tzeng, 2014, Arxiv, DOI arXiv:1412.3474; Ganin Y, 2017, ADV COMPUT VIS PATT, P189, DOI 10.1007/978-3-319-58347-1_10; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Geoffrey Hinton, 2015, Arxiv, DOI arXiv:1503.02531; Goodfellow I. J., 2014, ARXIV14126572; Han Xiao, 2017, Arxiv, DOI arXiv:1708.07747; Haroush Matan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8491, DOI 10.1109/CVPR42600.2020.00852; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hongxu Yin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8712, DOI 10.1109/CVPR42600.2020.00874; Ioffe S, 2015, BATCH NORMALIZATION, P1097; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kimura A., 2018, PROC BRIT MACH VIS C; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Krizhevsky A., 2012, PROC NEURAL INF PROC; Kundu JN, 2020, PROC CVPR IEEE, P4543, DOI 10.1109/CVPR42600.2020.00460; Kurmi VK, 2021, IEEE WINT CONF APPL, P615, DOI 10.1109/WACV48630.2021.00066; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee S, 2019, IEEE I CONF COMP VIS, P91, DOI 10.1109/ICCV.2019.00018; Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081; Liang Jian, 2020, ICML; Lin J., 2016, THESIS QUEENS U KING; Liu MY, 2016, ADV NEUR IN, V29; Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274; Lopes R. G., 2017, PROC LLD WORKSHOP NE, P1; Madry Aleksander, 2017, ARXIV; Maji S., 2009, UCBEECS2009159; Micaelli P, 2019, ADV NEUR IN, V32; Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17; Mopuri K. R., 2017, PROC BRIT MACH VIS C; Mopuri KR, 2018, LECT NOTES COMPUT SC, V11213, P20, DOI 10.1007/978-3-030-01240-3_2; Mordvintsev A., 2015, GOOGLE DEEP DREAM; Mordvintsev A, 2015, INCEPTIONISM GOING D; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Olah C., 2017, FEATURE VISUALIZATIO, P1; Ratner A. J., ARXIV; Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T, 2016, ADV NEUR IN, V29; Shin H, 2017, ADV NEUR IN, V30; Shoukai Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P1, DOI 10.1007/978-3-030-58610-2_1; Smola, 2007, ADV NEURAL INFORM PR, P513, DOI DOI 10.5555/2188385.2188410; Springenberg Jost Tobias, 2015, ARXIV151106390; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; van de Ven G. M., 2019, 3 SCENARIOS CONTINUA, P469; Yoo J, 2019, ADV NEUR IN, V32; You KC, 2019, PROC CVPR IEEE, P2715, DOI 10.1109/CVPR.2019.00283; Zhang JT, 2020, IEEE WINT CONF APPL, P1120, DOI 10.1109/WACV45572.2020.9093365	57	1	1	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8465	8481		10.1109/TPAMI.2021.3112816	http://dx.doi.org/10.1109/TPAMI.2021.3112816			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34529560	Green Submitted			2022-12-18	WOS:000864325900083
J	Pan, WW; Yin, YL; Wang, XC; Jing, YC; Song, ML				Pan, Wenwen; Yin, Yanling; Wang, Xinchao; Jing, Yongcheng; Song, Mingli			Seek-and-Hide: Adversarial Steganography via Deep Reinforcement Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Steganography; Containers; Reinforcement learning; Receivers; Convolutional neural networks; Task analysis; Location awareness; Steganography; deep learning; adversarial examples; reinforcement learning	IMAGE	The goal of image steganography is to hide a full-sized image, termed secret, into another, termed cover. Prior image steganography algorithms can conceal only one secret within one cover. In this paper, we propose an adaptive local image steganography (AdaSteg) system that allows for scale- and location-adaptive image steganography. By adaptively hiding the secret on a local scale, the proposed system makes the steganography more secured, and further enables multi-secret steganography within one single cover. Specifically, this is achieved via two stages, namely the adaptive patch selection stage and secret encryption stage. Given a pair of secret and cover, first, the optimal local patch for concealment is determined adaptively by exploiting deep reinforcement learning with the proposed steganography quality function and policy network. The secret image is then converted into a patch of encrypted noises, resembling the process of generating adversarial examples, which are further encoded to a local region of the cover to realize a more secured steganography. Furthermore, we propose a novel criterion for the assessment of local steganography, and also collect a challenging dataset that is specialized for the task of image steganography, thus contributing to a standardized benchmark for the area. Experimental results demonstrate that the proposed model yields results superior to the state of the art in both security and capacity.	[Pan, Wenwen; Yin, Yanling; Jing, Yongcheng; Song, Mingli] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China; [Wang, Xinchao] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore; [Song, Mingli] Zhejiang Univ, Alibaba Zhejiang Univ Joint Res Inst Frontier Tec, Hangzhou 310027, Peoples R China	Zhejiang University; National University of Singapore; Zhejiang University	Song, ML (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.; Song, ML (corresponding author), Zhejiang Univ, Alibaba Zhejiang Univ Joint Res Inst Frontier Tec, Hangzhou 310027, Peoples R China.	wenwenpan@zju.edu.cn; yanlingyin@zju.edu.cn; xinchao@nus.edu.sg; ycjing@zju.edu.cn; brooksong@zju.edu.cn	Jing, Yongcheng/HHN-4906-2022	Wang, Xinchao/0000-0003-0057-1404	National Natural Science Foundation of China [U20B2066, 61976186]; Key Research and Development Program of Zhejiang Province [2020C01023]; Major Scientific Research Project of Zhejiang Lab [2019KD0AC01]; Fundamental Research Funds for the Central Universities; AI Singapore 100 Experiments Programme [AISG2100E-2021-077]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Key Research and Development Program of Zhejiang Province; Major Scientific Research Project of Zhejiang Lab; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); AI Singapore 100 Experiments Programme	This work was supported in part by the National Natural Science Foundation of China under Grants U20B2066 and 61976186, in part by the Key Research and Development Program of Zhejiang Province under Grant 2020C01023, in part by the Major Scientific Research Project of Zhejiang Lab under Grant 2019KD0AC01, and in part by the Fundamental Research Funds for the Central Universities, and AI Singapore 100 Experiments Programme (AISG2100E-2021-077).	Abadi M., 2016, ARXIV, DOI DOI 10.48550/ARXIV.1603.04467; Amit R, 2020, PR MACH LEARN RES, V119; Arnab A, 2018, PROC CVPR IEEE, P888, DOI 10.1109/CVPR.2018.00099; Azza AA, 2020, MULTIMED TOOLS APPL, V79, P21241, DOI 10.1007/s11042-020-08823-8; Baluja S, 2020, IEEE T PATTERN ANAL, V42, P1685, DOI 10.1109/TPAMI.2019.2901877; Baluja S, 2017, ADV NEUR IN, V30; Bellver M., 2016, ABS161103718 CORR; Benedikt Boehm, 2014, Arxiv, DOI arXiv:1410.6656; Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286; Cheddad A, 2010, SIGNAL PROCESS, V90, P727, DOI 10.1016/j.sigpro.2009.08.010; Chen XY, 2018, LECT NOTES COMPUT SC, V11206, P167, DOI 10.1007/978-3-030-01216-8_11; Chen YY, 2020, COMPUT NETW, V181, DOI 10.1016/j.comnet.2020.107432; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fridrich J., 2001, IEEE Multimedia, V8, P22, DOI 10.1109/93.959097; Goel V, 2018, ADV NEUR IN, V31; Goodfellow I. J., 2015, PROC INT C LEARNING; Holub V, 2012, IEEE INT WORKS INFOR, P234, DOI 10.1109/WIFS.2012.6412655; Huang SZ, 2020, COMPLEXITY, V2020, DOI 10.1155/2020/6814263; Hung WC, 2018, LECT NOTES COMPUT SC, V11211, P72, DOI 10.1007/978-3-030-01234-2_5; Ilyas A, 2019, ADV NEUR IN, V32; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336; Kingma DP, 2015, INT C LEARN REPR ICL; Kos J, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P36, DOI 10.1109/SPW.2018.00014; Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z; Li B, 2018, IEEE T INF FOREN SEC, V13, P1242, DOI 10.1109/TIFS.2017.2780805; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Meng RH, 2018, CMC-COMPUT MATER CON, V55, P1, DOI 10.3970/cmc.2018.055.001; Mielikainen J, 2006, IEEE SIGNAL PROC LET, V13, P285, DOI 10.1109/LSP.2006.870357; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Papernot N, 2016, IEEE MILIT COMMUN C, P49, DOI 10.1109/MILCOM.2016.7795300; Pevny T, 2010, LECT NOTES COMPUT SC, V6387, P161, DOI 10.1007/978-3-642-16435-4_13; Pirinen A, 2018, PROC CVPR IEEE, P6945, DOI 10.1109/CVPR.2018.00726; Qazanfari K., 2011, PROC 19 IRANISAN C E, P1; Qian YG, 2020, COMPUT SECUR, V95, DOI 10.1016/j.cose.2020.101826; Qiu XF, 2020, NEUROCOMPUTING, V394, P1, DOI 10.1016/j.neucom.2020.01.040; Sharifzadeh M, 2020, IEEE T INF FOREN SEC, V15, P867, DOI 10.1109/TIFS.2019.2929441; Shen B, 2009, INT CONF ACOUST SPEE, P697, DOI 10.1109/ICASSP.2009.4959679; Simonyan K., 2015, INT C LEARN REPR ICL; Szegedy C., 2014, ICLR 2014; Tabacof P., 2016, WORKSHOP ADVERSARIAL; Taheri S, 2020, BIG DATA COGN COMPUT, V4, DOI 10.3390/bdcc4020011; Tamimi AA, 2013, INT J ADV COMPUT SC, V4, P18; Tang WX, 2021, IEEE T INF FOREN SEC, V16, P952, DOI 10.1109/TIFS.2020.3025438; Tang WX, 2019, IEEE T INF FOREN SEC, V14, P2074, DOI 10.1109/TIFS.2019.2891237; Van Seijen H., 2019, PROC INT C NEURAL IN, p14 134; Wang C, 2018, LECT NOTES COMPUT SC, V11205, P796, DOI 10.1007/978-3-030-01246-5_47; Wang CY, 2018, IEEE T IMAGE PROCESS, V27, P4066, DOI 10.1109/TIP.2018.2836316; Wang CY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2901; Wang YF, 2020, IEEE T INF FOREN SEC, V15, P2081, DOI 10.1109/TIFS.2019.2956590; Weng XY, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P87, DOI 10.1145/3323873.3325011; Wu P, 2018, FUTURE INTERNET, V10, DOI 10.3390/fi10060054; Zhang BW, 2020, COMPUT VIS IMAGE UND, V197, DOI 10.1016/j.cviu.2020.102988; Zhang HW, 2021, IEEE T INF FOREN SEC, V16, P701, DOI 10.1109/TIFS.2020.3021899; Zhu XG, 2018, INT CONF 3D VISION, P454, DOI 10.1109/3DV.2018.00059	57	1	1	15	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7871	7884		10.1109/TPAMI.2021.3114555	http://dx.doi.org/10.1109/TPAMI.2021.3114555			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34550880				2022-12-18	WOS:000864325900044
J	Wang, L; Kim, TK; Yoon, KJ				Wang, Lin; Kim, Tae-Kyun; Yoon, Kuk-Jin			Joint Framework for Single Image Reconstruction and Super-Resolution With an Event Camera	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image reconstruction; Cameras; Superresolution; Spatial resolution; Streaming media; Task analysis; Training; Event-based vision; image reconstruction; image super-resolution; adversarial learning		Event cameras sense brightness changes in each pixel and yield asynchronous event streams instead of producing intensity images. They have distinct advantages over conventional cameras, such as a high dynamic range (HDR) and no motion blur. To take advantage of event cameras with existing image-based algorithms, a few methods have been proposed to reconstruct images from event streams. However, the output images have a low resolution (LR) and are unrealistic. Low-quality outputs stem from broader applications of event cameras, where high-quality and high-resolution (HR) images are needed. In this work, we consider the problem of reconstructing and super-resolving images from LR events when no ground truth (GT) HR images and degradation models are available. We propose a novel end-to-end joint framework for single image reconstruction and super-resolution from LR event data. Our method is primarily unsupervised to handle the absence of real inputs from GT and deploys adversarial learning. To train our framework, we constructed an open dataset, including simulated events and real-world images. The use of the dataset boosts the network performance, and the network architectures and various loss functions in each phase help improve the quality of the resulting image. Various experiments showed that our method surpasses the state-of-the-art LR image reconstruction methods for real-world and synthetic datasets. The experiments for super-resolution (SR) image reconstruction also substantiate the effectiveness of the proposed method. We further extended our method to more challenging problems of HDR, sharp image reconstruction, and color events. In addition, we demonstrate that the reconstruction and super-resolution results serve as intermediate representations of events for high-level tasks, such as semantic segmentation, object recognition, and detection. We further examined how events affect the outputs of the three phases and analyze our method's efficacy through an ablation study.	[Wang, Lin; Yoon, Kuk-Jin] Korea Adv Inst Sci & Technol KAIST, Visual Intelligence Lab, Dept Mech Engn, Daejeon 34141, South Korea; [Kim, Tae-Kyun] Imperial Coll London, Imperial Comp Vis & Learning Lab, Dept Elect & Elect Engn, London SW7 2BX, England; [Kim, Tae-Kyun] Korea Adv Inst Sci & Technol, Sch Comp, Daejeon 34141, South Korea	Korea Advanced Institute of Science & Technology (KAIST); Imperial College London; Korea Advanced Institute of Science & Technology (KAIST)	Yoon, KJ (corresponding author), Korea Adv Inst Sci & Technol KAIST, Visual Intelligence Lab, Dept Mech Engn, Daejeon 34141, South Korea.	wanglin@kaist.ac.kr; tk.kim@imperial.ac.uk; kjyoon@kaist.ac.kr		yun, gugjin/0000-0002-1634-2756	National Research Foundation of Korea (NRF) - Korea government(MSIT) [NRF2018R1A2B3008640]; Institute of Information & Communications Technology Planning & Evaluation (IITP) - Korea government(MSIT) [2014-3-00123]	National Research Foundation of Korea (NRF) - Korea government(MSIT)(National Research Foundation of KoreaMinistry of Science, ICT & Future Planning, Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); Institute of Information & Communications Technology Planning & Evaluation (IITP) - Korea government(MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea)	This work was supported in part by the National Research Foundation of Korea (NRF) grant funded by the Korea government(MSIT) under Grant NRF2018R1A2B3008640, and in part by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) under Grant 2014-3-00123, Development of High Performance Visual BigData Discovery Platform for Large-Scale Realtime Data Analysis.	Alexia Jolicoeur-Martineau, 2018, Arxiv, DOI arXiv:1807.00734; Alonso I, 2019, IEEE COMPUT SOC CONF, P1624, DOI 10.1109/CVPRW.2019.00205; Aly HA, 2005, IEEE T IMAGE PROCESS, V14, P1647, DOI 10.1109/TIP.2005.851684; Antonio Loquercio, 2019, Arxiv, DOI arXiv:1904.08245; Bardow P. A., 2018, THESIS IMPERIAL COLL; Bardow P, 2016, PROC CVPR IEEE, P884, DOI 10.1109/CVPR.2016.102; Binas J., 2017, ARXIV; Brandli C, 2014, IEEE J SOLID-ST CIRC, V49, P2333, DOI 10.1109/JSSC.2014.2342715; Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952; Cannici M, 2019, IEEE COMPUT SOC CONF, P1656, DOI 10.1109/CVPRW.2019.00209; Cook M, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P770, DOI 10.1109/IJCNN.2011.6033299; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413; Gallego G, 2019, PROC CVPR IEEE, P12272, DOI 10.1109/CVPR.2019.01256; Gehrig D, 2018, LECT NOTES COMPUT SC, V11216, P766, DOI 10.1007/978-3-030-01258-8_46; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Jenni S, 2019, PROC CVPR IEEE, P12137, DOI 10.1109/CVPR.2019.01242; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Jonghyun Choi, 2020, Arxiv, DOI arXiv:1912.01196; Kim H, 2016, LECT NOTES COMPUT SC, V9910, P349, DOI 10.1007/978-3-319-46466-4_21; Kim Hanme, 2014, BRIT MACH VIS C BMVC, DOI [10.5244/C.28.26, DOI 10.5244/C.28.26]; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kingma D.P, P 3 INT C LEARNING R; Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854; Lagorce X, 2017, IEEE T PATTERN ANAL, V39, P1346, DOI 10.1109/TPAMI.2016.2574707; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8312, DOI 10.1109/CVPR42600.2020.00834; Mostafavi M, 2021, INT J COMPUT VISION, V129, P900, DOI 10.1007/s11263-020-01410-2; Mueggler E, 2017, INT J ROBOT RES, V36, P142, DOI 10.1177/0278364917691115; Munda G, 2018, INT J COMPUT VISION, V126, P1381, DOI 10.1007/s11263-018-1106-2; Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35; Nick Barnes, 2020, Arxiv, DOI arXiv:1904.07523; Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]; Pan LY, 2019, PROC CVPR IEEE, P6813, DOI 10.1109/CVPR.2019.00698; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Rebecq H, 2021, IEEE T PATTERN ANAL, V43, P1964, DOI 10.1109/TPAMI.2019.2963386; Rebecq Henri, 2018, C ROB LEARN, P2; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Reeves Stanley J, 2014, ACAD PRESS LIB SIGNA, V4, P165; Reinbacher C, 2017, IEEE INT CONF COMPUT, P106; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Scheerlinck C, 2019, IEEE COMPUT SOC CONF, P1684, DOI 10.1109/CVPRW.2019.00215; Scheerlinck C, 2019, LECT NOTES COMPUT SC, V11365, P308, DOI 10.1007/978-3-030-20873-8_20; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Sironi A, 2018, PROC CVPR IEEE, P1731, DOI 10.1109/CVPR.2018.00186; Taverni G, 2018, IEEE T CIRCUITS-II, V65, P677, DOI 10.1109/TCSII.2018.2824899; Timofte R, 2018, IEEE COMPUT SOC CONF, P965, DOI 10.1109/CVPRW.2018.00130; Tulyakov S, 2019, IEEE I CONF COMP VIS, P1527, DOI 10.1109/ICCV.2019.00161; Wang L, 2022, IEEE T PATTERN ANAL, V44, P3048, DOI 10.1109/TPAMI.2021.3055564; Wang L, 2019, PROC CVPR IEEE, P10073, DOI 10.1109/CVPR.2019.01032; Wang L, 2020, IEEE ROBOT AUTOM LET, V5, P1421, DOI 10.1109/LRA.2020.2967289; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Wang YX, 2019, PROC CVPR IEEE, P6351, DOI 10.1109/CVPR.2019.00652; Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166; Yuan Y, 2018, IEEE COMPUT SOC CONF, P814, DOI 10.1109/CVPRW.2018.00113; Zeyde Roman, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47; Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262; Zhu AZ, 2019, PROC CVPR IEEE, P989, DOI 10.1109/CVPR.2019.00108; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	67	1	1	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7657	7673		10.1109/TPAMI.2021.3113352	http://dx.doi.org/10.1109/TPAMI.2021.3113352			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34543191				2022-12-18	WOS:000864325900030
J	Wang, M; Zhang, YB; Deng, WH				Wang, Mei; Zhang, Yaobin; Deng, Weihong			Meta Balanced Network for Fair Face Recognition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Skin; Face recognition; Training; Training data; Metadata; Databases; Adaptation models; Fairness with respect to skin tone; meta learning; adaptive margin; face recognition	RACE	Although deep face recognition has achieved impressive progress in recent years, controversy has arisen regarding discrimination based on skin tone, questioning their deployment into real-world scenarios. In this paper, we aim to systematically and scientifically study this bias from both data and algorithm aspects. First, using the dermatologist approved Fitzpatrick Skin Type classification system and Individual Typology Angle, we contribute a benchmark called Identity Shades (IDS) database, which effectively quantifies the degree of the bias with respect to skin tone in existing face recognition algorithms and commercial APIs. Further, we provide two skin-tone aware training datasets, called BUPT-Globalface dataset and BUPT-Balancedface dataset, to remove bias in training data. Finally, to mitigate the algorithmic bias, we propose a novel meta-learning algorithm, called Meta Balanced Network (MBN), which learns adaptive margins in large margin loss such that the model optimized by this loss can perform fairly across people with different skin tones. To determine the margins, our method optimizes a meta skewness loss on a clean and unbiased meta set and utilizes backward-on-backward automatic differentiation to perform a second order gradient descent step on the current margins. Extensive experiments show that MBN successfully mitigates bias and learns more balanced performance for people with different skin tones in face recognition. The proposed datasets are available at http://www.whdeng.cn/RFW/index.html.	[Wang, Mei; Zhang, Yaobin; Deng, Weihong] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Pattern Recognit & Intelligent Syst Lab, Beijing 100876, Peoples R China; [Deng, Weihong] Beijing Univ Posts & Telecommun, Minist Educ, Key Lab Trustworthy Distributed Comp & Serv, Beijing 100876, Peoples R China	Beijing University of Posts & Telecommunications; Beijing University of Posts & Telecommunications	Deng, WH (corresponding author), Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Pattern Recognit & Intelligent Syst Lab, Beijing 100876, Peoples R China.	wangmei1@bupt.edu.cn; zhangyaobin@bupt.edu.cn; whdeng@bupt.edu.cn		Deng, Weihong/0000-0001-5952-6996	National Natural Science Foundation of China [61871052]; BUPT Excellent Ph.D.; Students Foundation [CX2020207]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); BUPT Excellent Ph.D.; Students Foundation	This work was supported in part by the National Natural Science Foundation of China under Grant 61871052 and BUPT Excellent Ph.D. Students Foundation CX2020207.	Alex Nichol, 2018, Arxiv, DOI arXiv:1803.02999; Alvi M, 2019, LECT NOTES COMPUT SC, V11129, P556, DOI 10.1007/978-3-030-11009-3_34; amazon, AMAZONS REKOGNITION; Amini A, 2019, AIES '19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, P289, DOI 10.1145/3306618.3314243; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Angwin J., 2016, MACHINE BIAS; baidu, BAIDU CLOUD VISION A; Beveridge J. R., 2008, P INT C AUTOMATIC FA, P1; Buolamwini J., 2018, C FAIRN ACC TRANSP, P77; Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020; CHARDON A, 1991, INT J COSMETIC SCI, V13, P191, DOI 10.1111/j.1467-2494.1991.tb00561.x; Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482; Donahue J, 2014, PR MACH LEARN RES, V32; Dong Yi, 2014, Arxiv, DOI arXiv:1411.7923; faceplusplus, FACE RES TOOLKIT; Finn C, 2017, PR MACH LEARN RES, V70; FITZPATRICK TB, 1988, ARCH DERMATOL, V124, P869, DOI 10.1001/archderm.124.6.869; Franceschi L, 2018, PR MACH LEARN RES, V80; Friedler Sorelle A., 2016, ARXIV; Furl N, 2002, COGNITIVE SCI, V26, P797, DOI 10.1016/S0364-0213(02)00084-8; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Garvie Clare, 2016, PERPETUAL LINE UP UN; Google, 2015, FREEB DAT DUMPS; Grother P., 2019, FACE RECOGNITION V 3, DOI [DOI 10.6028/NIST.IR.8280, 10.6028/NIST.IR.8280]; Grother P. J., 2010, NIST INTERAGENCY REP, V7709; Guo JZ, 2020, PROC CVPR IEEE, P6162, DOI 10.1109/CVPR42600.2020.00620; Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang Gary B., 2007, 0749 U MASS, P7; John R. Smith, 2019, Arxiv, DOI arXiv:1901.10436; Kan MN, 2015, IEEE I CONF COMP VIS, P3846, DOI 10.1109/ICCV.2015.438; Kan MN, 2014, INT J COMPUT VISION, V109, P94, DOI 10.1007/s11263-013-0693-1; Kemelmacher-Shlizerman I, 2016, PROC CVPR IEEE, P4873, DOI 10.1109/CVPR.2016.527; Klare BF, 2012, IEEE T INF FOREN SEC, V7, P1789, DOI 10.1109/TIFS.2012.2214212; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kush R. Varshney, 2018, Arxiv, DOI arXiv:1805.09910; Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713; Microsoft azure, US; Mirjalili V, 2018, INT CONF BIOMETR THE; Mirjalili V, 2018, INT CONF BIOMETR, P82, DOI 10.1109/ICB2018.2018.00023; Morales A, 2021, IEEE T PATTERN ANAL, V43, P2158, DOI 10.1109/TPAMI.2020.3015420; Othman A, 2015, LECT NOTES COMPUT SC, V8926, P682, DOI 10.1007/978-3-319-16181-5_52; Phillips P. J., 2003, 2003 IEEE International Workshop on Analysis and Modeling of Faces and Gestures; Phillips PJ, 2012, IMAGE VISION COMPUT, V30, P177, DOI 10.1016/j.imavis.2012.01.004; Phillips PJ, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/1870076.1870082; Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X; Rajeswaran A, 2019, ADV NEUR IN, V32; Raji ID, 2019, AIES '19: PROCEEDINGS OF THE 2019 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY, P429, DOI 10.1145/3306618.3314244; Ravi Sachin, 2017, INT C LEARN REPR, V2, P5; Ren MY, 2018, PR MACH LEARN RES, V80; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shu J, 2019, ADV NEUR IN, V32; Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8; Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049; Sun Y., 2014, ADV NEURAL INFORM PR, P1988; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; technologyreview, ARE FACE RECOGNITION; Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359; Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552; Wang M., 2020, P IEEE C COMP VIS PA, P9322; Wang M, 2021, NEUROCOMPUTING, V429, P215, DOI 10.1016/j.neucom.2020.10.081; Wang M, 2019, IEEE I CONF COMP VIS, P692, DOI 10.1109/ICCV.2019.00078; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Yudell M, 2016, SCIENCE, V351, P564, DOI 10.1126/science.aac4951; Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463	67	1	1	3	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8433	8448		10.1109/TPAMI.2021.3103191	http://dx.doi.org/10.1109/TPAMI.2021.3103191			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34383643				2022-12-18	WOS:000864325900081
J	Wang, XG; Ang, MH; Lee, GH				Wang, Xiaogang; Ang, Marcelo H. Jr Jr; Lee, Gim Hee			Cascaded Refinement Network for Point Cloud Completion With Self-Supervision	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Feature extraction; Shape; Training; Task analysis; Supervised learning; Kernel; 3D learning; point cloud completion; self-supervised learning; coarse-to-fine		Point clouds are often sparse and incomplete, which imposes difficulties for real-world applications. Existing shape completion methods tend to generate rough shapes without fine-grained details. Considering this, we introduce a two-branch network for shape completion. The first branch is a cascaded shape completion sub-network to synthesize complete objects, where we propose to use the partial input together with the coarse output to preserve the object details during the dense point reconstruction. The second branch is an auto-encoder to reconstruct the original partial input. The two branches share a same feature extractor to learn an accurate global feature for shape completion. Furthermore, we propose two strategies to enable the training of our network when ground truth data are not available. This is to mitigate the dependence of existing approaches on large amounts of ground truth training data that are often difficult to obtain in real-world applications. Additionally, our proposed strategies are also able to improve the reconstruction quality for fully supervised learning. We verify our approach in self-supervised, semi-supervised and fully supervised settings with superior performances. Quantitative and qualitative results on different datasets demonstrate that our method achieves more realistic outputs than state-of-the-art approaches on the point cloud completion task.	[Wang, Xiaogang] Natl Univ Singapore, Dept Mech Engn, Singapore 117575, Singapore; [Ang, Marcelo H. Jr Jr] Natl Univ Singapore, Adv Robot Ctr, Dept Mech Engn, Singapore 117608, Singapore; [Lee, Gim Hee] Natl Univ Singapore, Dept Comp Sci, Singapore 117417, Singapore	National University of Singapore; National University of Singapore; National University of Singapore	Wang, XG (corresponding author), Natl Univ Singapore, Dept Mech Engn, Singapore 117575, Singapore.	xiaogangw@u.nus.edu; mpeangh@nus.edu.sg; gimhee.lee@nus.edu.sg			Singapore Ministry of Education (MOE) Tier 1 grant [R-252-000-A65-114]; National University of Singapore Scholarship Funds; National Research Foundation, Prime Ministers Office, Singapore, under its CREATE programme; Singapore-MIT Alliance for Research and Technology (SMART) Future Urban Mobility (FM) IRG; Singapore Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme [A18A2b0046]	Singapore Ministry of Education (MOE) Tier 1 grant(Ministry of Education, Singapore); National University of Singapore Scholarship Funds; National Research Foundation, Prime Ministers Office, Singapore, under its CREATE programme(National Research Foundation, Singapore); Singapore-MIT Alliance for Research and Technology (SMART) Future Urban Mobility (FM) IRG(Singapore-MIT Alliance for Research & Technology Centre (SMART)); Singapore Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme(Agency for Science Technology & Research (A*STAR))	This research was supported in part by the Singapore Ministry of Education (MOE) Tier 1 grant R-252-000-A65-114, National University of Singapore Scholarship Funds, the National Research Foundation, Prime Ministers Office, Singapore, under its CREATE programme, Singapore-MIT Alliance for Research and Technology (SMART) Future Urban Mobility (FM) IRG, and the Singapore Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme (Project #A18A2b0046).	Achituve I, 2021, IEEE WINT CONF APPL, P123, DOI 10.1109/WACV48630.2021.00017; Boud A. C., 1999, 1999 IEEE International Conference on Information Visualization (Cat. No. PR00210), P32, DOI 10.1109/IV.1999.781532; Brock Andrew, 2016, ARXIV160804236; Chang A, 2017, INT CONF 3D VISION, P667, DOI 10.1109/3DV.2017.00081; Chen X., 2020, PROC INT C LEARN REP; Dai A, 2020, PROC CVPR IEEE, P846, DOI 10.1109/CVPR42600.2020.00093; Dai A, 2018, PROC CVPR IEEE, P4578, DOI 10.1109/CVPR.2018.00481; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Duan YQ, 2019, PROC CVPR IEEE, P949, DOI 10.1109/CVPR.2019.00104; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Giancola S, 2019, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2019.00145; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434; Han XG, 2017, IEEE I CONF COMP VIS, P85, DOI 10.1109/ICCV.2017.19; Hensel M, 2017, ADV NEUR IN, V30; Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Kingma DP, 2015, INT C LEARN REPR ICL; Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298; Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959; Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979; Li YY, 2018, ADV NEUR IN, V31; Liang M, 2018, LECT NOTES COMPUT SC, V11220, P663, DOI 10.1007/978-3-030-01270-0_39; Litany O, 2018, PROC CVPR IEEE, P1886, DOI 10.1109/CVPR.2018.00202; Liu MH, 2020, AAAI CONF ARTIF INTE, V34, P11596; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459; Michalkiewicz Mateusz, 2019, ARXIV190106802; Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025; Qi CR, 2017, ADV NEUR IN, V30; Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16; Qin C, 2019, ADV NEUR IN, V32; Richard A, 2020, INT CONF 3D VISION, P101, DOI 10.1109/3DV50981.2020.00020; Sarmad M, 2019, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR.2019.00605; Shen YR, 2018, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2018.00478; Shu DW, 2019, IEEE I CONF COMP VIS, P3858, DOI 10.1109/ICCV.2019.00396; Sinha A, 2017, PROC CVPR IEEE, P791, DOI 10.1109/CVPR.2017.91; Smith Edward J., 2017, ABS170709557 CORR; Stutz D, 2018, PROC CVPR IEEE, P1955, DOI 10.1109/CVPR.2018.00209; Tatarchenko M, 2019, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2019.00352; Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047; Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651; Vakalopoulou M, 2018, LECT NOTES COMPUT SC, V11073, P658, DOI 10.1007/978-3-030-00937-3_75; Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4; Wang PS, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275050; Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608; Wang WY, 2019, PROC CVPR IEEE, P1038, DOI 10.1109/CVPR.2019.00113; Wang XG, 2020, PROC CVPR IEEE, P787, DOI 10.1109/CVPR42600.2020.00087; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Webster A, 1996, COMPUTING IN CIVIL ENGINEERING, P913; Wen X, 2021, PROC CVPR IEEE, P7439, DOI 10.1109/CVPR46437.2021.00736; Wen X, 2020, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR42600.2020.00201; Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Xie H., 2020, ECCV, P365; Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563; Yang JC, 2019, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2019.00344; Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029; Yida Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P70, DOI 10.1007/978-3-030-58580-8_5; Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088; Zhang H., 2018, 6 INT C LEARNING REP, DOI 10.48550/arXiv.1710.09412; Zhang JM, 2021, IEEE ROBOT AUTOM LET, V6, P596, DOI 10.1109/LRA.2020.3048658; Zhang W., 2020, P EUR C COMP VIS ECC, P2; Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571; Zitian Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7659, DOI 10.1109/CVPR42600.2020.00768	68	1	1	7	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8139	8150		10.1109/TPAMI.2021.3108410	http://dx.doi.org/10.1109/TPAMI.2021.3108410			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34460366	Green Submitted			2022-12-18	WOS:000864325900062
J	Yao, ZY; Wang, YB; Wang, JM; Yu, PS; Long, MS				Yao, Zhiyu; Wang, Yunbo; Wang, Jianmin; Yu, Philip S.; Long, Mingsheng			VideoDG: Generalizing Temporal Relations in Videos to Novel Domains	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Videos; Training; Adaptation models; Transfer learning; Spatiotemporal phenomena; Benchmark testing; Robustness; Deep learning; transfer learning; domain generalization; adversarial pyramid network; video action recognition		This paper introduces video domain generalization where most video classification networks degenerate due to the lack of exposure to the target domains of divergent distributions. We observe that the global temporal features are less generalizable, due to the temporal domain shift that videos from other unseen domains may have an unexpected absence or misalignment of the temporal relations. This finding has motivated us to solve video domain generalization by effectively learning the local-relation features of different timescales that are more generalizable, and exploiting them along with the global-relation features to maintain the discriminability. This paper presents the VideoDG framework with two technical contributions. The first is a new deep architecture named the Adversarial Pyramid Network, which improves the generalizability of video features by capturing the local-relation, global-relation, and cross-relation features progressively. On the basis of pyramid features, the second contribution is a new and robust approach of adversarial data augmentation that can bridge different video domains by improving the diversity and quality of augmented data. We construct three video domain generalization benchmarks in which domains are divided according to different datasets, different consequences of actions, or different camera views, respectively. VideoDG consistently outperforms the combinations of previous video classification models and existing domain generalization methods on all benchmarks.	[Yao, Zhiyu; Wang, Yunbo; Wang, Jianmin; Yu, Philip S.; Long, Mingsheng] Tsinghua Univ, Sch Software, BNRist, Beijing 100084, Peoples R China; [Wang, Yunbo] Shanghai Liao Tong Univ, AI Inst, MoE Key Lab AI, Shanghai 200240, Peoples R China	Tsinghua University; Shanghai Jiao Tong University	Long, MS (corresponding author), Tsinghua Univ, Sch Software, BNRist, Beijing 100084, Peoples R China.	yaozy15@gmail.com; yunbo.thu@gmail.com; jimwang@tsinghua.edu.cn; psyu@tsinghua.edu.cn; mingsheng@tsinghua.edu.cn		Yu, Philip/0000-0002-3491-5968	National Key R&D Program of China [2020AAA0109201]; NSFC [62022050, 62021002, 61772299]; Beijing Nova Program [Z201100006820041]; MOE Innovation Plan; BNRist Innovation Fund; Shanghai Sailing Program; CAAI-Huawei MindSpore Open Fund	National Key R&D Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Beijing Nova Program(Beijing Municipal Science & Technology Commission); MOE Innovation Plan; BNRist Innovation Fund; Shanghai Sailing Program; CAAI-Huawei MindSpore Open Fund	This work was supported in part by the National Key R&D Program of China under Grant 2020AAA0109201, in part by the NSFC under Grants 62022050, 62021002, and 61772299, in part by the Beijing Nova Program under Grant Z201100006820041, in part by the MOE Innovation Plan, and in part by the BNRist Innovation Fund. The work of Yunbo Wang was supported in part by Shanghai Sailing Program and in part by the CAAI-Huawei MindSpore Open Fund. Zhiyu Yao and Yunbo Wang contributed equally to this work.	Ahsan U, 2019, IEEE WINT CONF APPL, P179, DOI 10.1109/WACV.2019.00025; Aman Sinha, 2020, Arxiv, DOI arXiv:1710.10571; Amir Roshan Zamir, 2012, Arxiv, DOI arXiv:1212.0402; Andrew Zisserman, 2017, Arxiv, DOI arXiv:1705.06950; Ba J.L., 2016, ARXIV, DOI DOI 10.48550/ARXIV.1607.06450; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen MH, 2019, IEEE I CONF COMP VIS, P6330, DOI [10.1109/ICCV.2019.00642, 10.1109/ICCVW.2019.00374]; David Lopez-Paz, 2020, Arxiv, DOI arXiv:2007.01434; Diba A, 2019, IEEE I CONF COMP VIS, P6191, DOI 10.1109/ICCV.2019.00629; Dou Q, 2019, ADV NEUR IN, V32; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Feichtenhofer C, 2016, ADV NEUR IN, V29; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Fengchun Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12553, DOI 10.1109/CVPR42600.2020.01257; Ghifary M, 2015, IEEE I CONF COMP VIS, P2551, DOI 10.1109/ICCV.2015.293; Girdhar R, 2017, ADV NEUR IN, V30; Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Jackson P. T., 2019, P IEEE C COMP VIS PA, P83; Jamal Arshad, 2018, BMVC; Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209; Zhou KY, 2022, Arxiv, DOI arXiv:2103.02503; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Li D, 2019, IEEE I CONF COMP VIS, P1446, DOI 10.1109/ICCV.2019.00153; Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566; Li Y, 2018, LECT NOTES COMPUT SC, V11219, P647, DOI 10.1007/978-3-030-01267-0_38; Li YJ, 2019, PR MACH LEARN RES, V97; Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718; Long MS, 2018, ADV NEUR IN, V31; Martinez B, 2019, IEEE I CONF COMP VIS, P5481, DOI 10.1109/ICCV.2019.00558; Munro J, 2020, PROC CVPR IEEE, P119, DOI 10.1109/CVPR42600.2020.00020; Pan BX, 2020, AAAI CONF ARTIF INTE, V34, P11815; Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590; Scholkopf B, 2000, ADV NEUR IN, V12, P582; Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115; Shankar Shiv, 2018, P INT C LEARN REPR I; Shu Y, 2021, PROC CVPR IEEE, P9619, DOI 10.1109/CVPR46437.2021.00950; Simonyan K, 2014, ADV NEUR IN, V27; Sun SY, 2018, PROC CVPR IEEE, P1390, DOI 10.1109/CVPR.2018.00151; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Vaswani A, 2017, ADV NEUR IN, V30; Volpi R, 2018, ADV NEUR IN, V31; Volpi R, 2019, IEEE I CONF COMP VIS, P7979, DOI 10.1109/ICCV.2019.00807; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wang Y., 2019, PROC INT C LEARN REP; Wang YB, 2017, PROC CVPR IEEE, P2097, DOI 10.1109/CVPR.2017.226; Wu CY, 2019, PROC CVPR IEEE, P284, DOI 10.1109/CVPR.2019.00037; Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19; Yang CY, 2020, PROC CVPR IEEE, P588, DOI 10.1109/CVPR42600.2020.00067; Yosinski J, 2014, ADV NEUR IN, V27; Zhang HY, 2019, PR MACH LEARN RES, V97; Zhang L, 2018, ADV NEUR IN, V31; Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49	58	1	1	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7989	8004		10.1109/TPAMI.2021.3116945	http://dx.doi.org/10.1109/TPAMI.2021.3116945			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34596532	Green Submitted			2022-12-18	WOS:000864325900052
J	Yu, KC; Ranftl, R; Salzmann, M				Yu, Kaicheng; Ranftl, Rene; Salzmann, Mathieu			An Analysis of Super-Net Heuristics in Weight-Sharing NAS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Training; Protocols; Computer architecture; Task analysis; Measurement; Benchmark testing; Encoding; AutoML; neural architecture search; weight-sharing; super-net		Weight sharing promises to make neural architecture search (NAS) tractable even on commodity hardware. Existing methods in this space rely on a diverse set of heuristics to design and train the shared-weight backbone network, a.k.a. the super-net. Since heuristics substantially vary across different methods and have not been carefully studied, it is unclear to which extent they impact super-net training and hence the weight-sharing NAS algorithms. In this paper, we disentangle super-net training from the search algorithm, isolate 14 frequently-used training heuristics, and evaluate them over three benchmark search spaces. Our analysis uncovers that several commonly-used heuristics negatively impact the correlation between super-net and stand-alone performance, whereas simple, but often overlooked factors, such as proper hyper-parameter settings, are key to achieve strong performance. Equipped with this knowledge, we show that simple random search achieves competitive performance to complex state-of-the-art NAS algorithms when the super-net is properly trained.	[Yu, Kaicheng] Ecole Polytech Fed Lausanne, CVLab, CH-1015 Lausanne, Switzerland; [Yu, Kaicheng] Abacus AI, 1099 Folsom St, San Francisco, CA USA; [Ranftl, Rene] Intel, Intelligent Syst Lab, D-85579 Neubiberg, Germany; [Salzmann, Mathieu] Ecole Polytech Fed Lausanne, Sch Comp & Commun Sci, CVLab, CH-1015 Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Intel Corporation; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Yu, KC (corresponding author), Ecole Polytech Fed Lausanne, CVLab, CH-1015 Lausanne, Switzerland.	kaicheng.yu.yt@gmail.com; rene.ranftl@intel.com; mathieu.salzmann@epfl.ch			Swiss National Science Foundation	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission)	This work was supported in part by the Swiss National Science Foundation.	Bender Gabriel, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14311, DOI 10.1109/CVPR42600.2020.01433; Cai H., 2020, ICLR, P1; Cai Han, 2019, INT C LEARN REPR; Chen X, 2019, IEEE I CONF COMP VIS, P1294, DOI 10.1109/ICCV.2019.00138; Chu X., 2019, ARXIV; Dong X., 2020, PROC INT C LEARN REP; Dong XY, 2019, ADV NEUR IN, V32; Dong XY, 2019, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2019.00186; Elsken T, 2019, J MACH LEARN RES, V20; Erik Goodman, 2019, Arxiv, DOI arXiv:1810.03522; Gaofeng Meng, 2019, Arxiv, DOI arXiv:1903.10979; Jiahui Yu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P702, DOI 10.1007/978-3-030-58571-6_41; Jin HF, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1946, DOI 10.1145/3292500.3330648; Kandasamy K, 2018, ADV NEUR IN, V31; Krizhevsky A., CIFAR 10 CANADIAN I; Kubernetes, KUB; Li H, 2018, ADV NEUR IN, V31; Li L, 2020, PR MACH LEARN RES, V115, P367; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Liu H., 2019, PROC INT C LEARN REP; Luo RQ, 2018, ADV NEUR IN, V31; Miikkulainen R, 2019, ARTIFICIAL INTELLIGENCE IN THE AGE OF NEURAL NETWORKS AND BRAIN COMPUTING, P293, DOI 10.1016/B978-0-12-815480-9.00015-3; Nayman N, 2019, ADV NEUR IN, V32; Paszke A, 2019, ADV NEUR IN, V32; Pham H, 2018, PR MACH LEARN RES, V80; Radosavovic I, 2019, IEEE I CONF COMP VIS, P1882, DOI 10.1109/ICCV.2019.00197; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Real E, 2017, PR MACH LEARN RES, V70; Ryoo M. S., 2020, P INT C LEARN REPR; Slurm, SLURM WORKL MAN; So DR, 2019, PR MACH LEARN RES, V97; Wang LN, 2020, AAAI CONF ARTIF INTE, V34, P9983; Xiang Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13833, DOI 10.1109/CVPR42600.2020.01385; Xie Sirui, 2019, INT C LEARN REPR; Xu Y, 2020, PROC INT C LEARN REP; Yang A., 2020, PROC INT C LEARN REP; Ying C, 2019, PR MACH LEARN RES, V97; Yu J., 2019, CORR; Yu K., 2020, C LEARN REPR ICLR; Zela A., 2020, PROC ICLR 2020; Zhang GF, 2012, NATURE, V490, P49, DOI 10.1038/nature11413; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhou HP, 2019, PR MACH LEARN RES, V97; Zoph B., 2017, P1; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	48	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8110	8124		10.1109/TPAMI.2021.3108480	http://dx.doi.org/10.1109/TPAMI.2021.3108480			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34460367	Green Submitted			2022-12-18	WOS:000864325900060
J	Akrour, R; Tateo, D; Peters, J				Akrour, Riad; Tateo, Davide; Peters, Jan			Continuous Action Reinforcement Learning From a Mixture of Interpretable Experts	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Complexity theory; Approximation algorithms; Neural networks; Trajectory; Reinforcement learning; Clustering algorithms; Reinforcement learning; mixture of experts; interpretability; robotics	ABSTRACTION; GAME	Reinforcement learning (RL) has demonstrated its ability to solve high dimensional tasks by leveraging non-linear function approximators. However, these successes are mostly achieved by 'black-box' policies in simulated domains. When deploying RL to the real world, several concerns regarding the use of a 'black-box' policy might be raised. In order to make the learned policies more transparent, we propose in this paper a policy iteration scheme that retains a complex function approximator for its internal value predictions but constrains the policy to have a concise, hierarchical, and human-readable structure, based on a mixture of interpretable experts. Each expert selects a primitive action according to a distance to a prototypical state. A key design decision to keep such experts interpretable is to select the prototypical states from trajectory data. The main technical contribution of the paper is to address the challenges introduced by this non-differentiable prototypical state selection procedure. Experimentally, we show that our proposed algorithm can learn compelling policies on continuous action deep RL benchmarks, matching the performance of neural network based policies, but returning policies that are more amenable to human inspection than neural network or linear-in-feature policies.	[Akrour, Riad] Aalto Univ, Espoo 02150, Finland; [Tateo, Davide; Peters, Jan] Tech Univ Darmstadt, D-64289 Darmstadt, Germany	Aalto University; Technical University of Darmstadt	Akrour, R (corresponding author), Aalto Univ, Espoo 02150, Finland.	riad.akrour@aalto.fi; davide@robot-learning.de; mail@jan-peters.net						Abel D, 2016, PR MACH LEARN RES, V48; Abhishek Gupta, 2018, Arxiv, DOI arXiv:1806.02813; Abhishek Gupta, 2018, Arxiv, DOI arXiv:1802.06070; Akrour R., 2019, PROC INT C MACH LEAR, P181; Akrour R, 2018, IEEE INT C INT ROBOT, P534, DOI 10.1109/IROS.2018.8594201; Alec Radford, 2017, Arxiv, DOI arXiv:1707.06347; Anderson C. W., 2000, TECH REP CS 00 101; Andrychowicz M., 2017, ADV NEURAL INFORM PR; [Anonymous], 2009, ADV NEURAL INFORM PR; Barto AG, 2003, DISCRETE EVENT DYN S, V13, P41, DOI 10.1023/A:1025696116075; Bastani O, 2018, ADV NEUR IN, V31; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Bresina JL, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P271; Coppens Youri, 2019, CEX WORKSHOP, P1; Coumans E., 2017, PYBULLET PYTHON MODU; Craven MW, 1996, ADV NEUR IN, V8, P24; da Silva B., 2012, PROC INT C MACH LEAR, P1443; DEramo C., 2020, MUSHROOMRL SIMPLIFYI; Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639; Dietterich TG, 2000, ADV NEUR IN, V12, P994; Ernst D, 2005, J MACH LEARN RES, V6, P503; Florensa C., 2017, PROC INT C MACH LEAR, P1; Gentile C, 2014, PR MACH LEARN RES, V32, P757; Hayes B, 2017, ACMIEEE INT CONF HUM, P303, DOI 10.1145/2909824.3020233; Hesse C., 2017, OPENAI BASELINES; Hinton, 1993, ADV NEURAL INFORM PR, V5, P271; Kakade S, 2002, ADV NEUR IN, V14, P1531; Korda N, 2016, PR MACH LEARN RES, V48; Lagoudakis MG, 2004, J MACH LEARN RES, V4, P1107, DOI 10.1162/1532443041827907; Letham B, 2015, ANN APPL STAT, V9, P1350, DOI 10.1214/15-AOAS848; Levy A., 2018, PROC INT C LEARN REP; Li L., 2006, ISAIM; Lillicrap T. P, 2015, ARXIV, DOI DOI 10.48550/ARXIV.1509.02971; Madumal P, 2020, AAAI CONF ARTIF INTE, V34, P2493; Mahadik K., 2020, P 34 ACM INT C SUP, P1; Marjaninejad A, 2019, NAT MACH INTELL, V1, P144, DOI 10.1038/s42256-019-0029-0; Masoudnia S, 2014, ARTIF INTELL REV, V42, P275, DOI 10.1007/s10462-012-9338-y; Matthew Hausknecht, 2016, Arxiv, DOI arXiv:1511.04143; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nachum O, 2018, ADV NEUR IN, V31; Nie XK, 2020, J AM STAT ASSOC, V116, P392, DOI 10.1080/01621459.2020.1831925; Nived Rajaraman, 2018, Arxiv, DOI arXiv:1810.12861; Parr R, 1998, ADV NEUR IN, V10, P1043; Precup D., 1998, Machine Learning: ECML-98. 10th European Conference on Machine Learning. Proceedings, P382, DOI 10.1007/BFb0026709; Rajeswaran A., 2017, ADV NEURAL INFORM PR, P6550; Rexakis I, 2008, SEVENTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS, P91, DOI 10.1109/ICMLA.2008.31; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Riemer M, 2020, AAAI CONF ARTIF INTE, V34, P5519; Ross S., 2010, PROC 13 INT C ARTIF, V9, P661; Scherrer B, 2014, PR MACH LEARN RES, V32, P1314; Schulman J., 2016, P INT C LEARNING REP; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Strumbelj E, 2010, J MACH LEARN RES, V11, P1; Sundaresan P, 2020, IEEE INT CONF ROBOT, P9411, DOI 10.1109/ICRA40945.2020.9197121; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Szepesvari C, 2010, ALGORITHMS REINFORCE, V4; Tateo D, 2019, IEEE INT C INT ROBOT, P1003, DOI 10.1109/IROS40897.2019.8968252; Tessler C, 2017, AAAI CONF ARTIF INTE, P1553; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Ustun B, 2016, MACH LEARN, V102, P349, DOI 10.1007/s10994-015-5528-6; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; Vaswani A, 2017, ADV NEUR IN, V30; Verma A, 2018, PR MACH LEARN RES, V80; Xu X, 2007, IEEE T NEURAL NETWOR, V18, P973, DOI 10.1109/TNN.2007.899161; ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X	75	1	1	6	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6795	6806		10.1109/TPAMI.2021.3103132	http://dx.doi.org/10.1109/TPAMI.2021.3103132			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34375280	Green Submitted			2022-12-18	WOS:000853875300069
J	Dong, JT; Fang, Q; Jiang, W; Yang, YR; Huang, QX; Bao, HJ; Zhou, XW				Dong, Junting; Fang, Qi; Jiang, Wen; Yang, Yurou; Huang, Qixing; Bao, Hujun; Zhou, Xiaowei			Fast and Robust Multi-Person 3D Pose Estimation and Tracking From Multiple Views	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Image reconstruction; Pose estimation; Cameras; Solid modeling; Noise measurement; Detectors; 3D human pose estimation; motion capture; multi-view reconstruction		This paper addresses the problem of reconstructing 3D poses of multiple people from a few calibrated camera views. The main challenge of this problem is to find the cross-view correspondences among noisy and incomplete 2D pose predictions. Most previous methods address this challenge by directly reasoning in 3D using a pictorial structure model, which is inefficient due to the huge state space. We propose a fast and robust approach to solve this problem. Our key idea is to use a multi-way matching algorithm to cluster the detected 2D poses in all views. Each resulting cluster encodes 2D poses of the same person across different views and consistent correspondences across the keypoints, from which the 3D pose of each person can be effectively inferred. The proposed convex optimization based multi-way matching algorithm is efficient and robust against missing and false detections, without knowing the number of people in the scene. Moreover, we propose to combine geometric and appearance cues for cross-view matching. Finally, an efficient tracking method is proposed to track the detected 3D poses across the multi-view video. The proposed approach achieves the state-of-the-art performance on the Campus and Shelf datasets, while being efficient for real-time applications.	[Dong, Junting; Fang, Qi; Jiang, Wen; Bao, Hujun; Zhou, Xiaowei] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China; [Yang, Yurou] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA; [Huang, Qixing] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	Zhejiang University; Carnegie Mellon University; University of Texas System; University of Texas Austin	Zhou, XW (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.	jtdong@zju.edu.cn; fangqi19@zju.edu.cn; wenjiangpr@gmail.com; yurouy@andrew.cmu.edu; huangqx@cs.utexas.edu; bao@cad.zju.edu.cn; xwzhoul@zju.edu.cn			National Key Research and Development Program of China [2020AAA0108901]; NSFC [61806176]; ZJU-SenseTime Joint Lab of 3D Vision; NSFHDR [TRIPODS-1934932]	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); ZJU-SenseTime Joint Lab of 3D Vision; NSFHDR	The authors from Zhejiang University would like to acknowledge support from the National Key Research and Development Program of China under Grant 2020AAA0108901, NSFC under Grant 61806176, and ZJU-SenseTime Joint Lab of 3D Vision. Q. Huang would like to acknowledge support from NSFHDR TRIPODS-1934932.	Alexey Bochkovskiy, 2020, Arxiv, DOI arXiv:2004.10934; Belagiannis V, 2016, IEEE T PATTERN ANAL, V38, P1929, DOI 10.1109/TPAMI.2015.2509986; Belagiannis V, 2014, PROC CVPR IEEE, P1669, DOI 10.1109/CVPR.2014.216; Belagiannis Vasileios, 2014, PROC EUR C COMPUT VI, P742; Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Bridgeman L, 2019, IEEE COMPUT SOC CONF, P2487, DOI [10.1109/CVERW.2019.00304, 10.1109/CVPRW.2019.00304]; Burenius M, 2013, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2013.464; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Carmo M. P. D., 1992, RIEMANNIAN GEOMETRY; Chen CH, 2017, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR.2017.610; Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742; Dong JT, 2019, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2019.00798; Elhayek A, 2015, PROC CVPR IEEE, P3810, DOI 10.1109/CVPR.2015.7299005; Ershadi-Nasab S, 2018, MULTIMED TOOLS APPL, V77, P15573, DOI 10.1007/s11042-017-5133-8; Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256; Fazel M., 2002, MATRIX RANK MINIMIZA; Gallier Jean, 2012, NOTES DIFFERENTIAL G, V4, P4; Habermann M, 2020, PROC CVPR IEEE, P5051, DOI 10.1109/CVPR42600.2020.00510; Habermann M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3311970; He K., 2017, IEEE INT C COMP VIS, P2961; Huang QX, 2013, COMPUT GRAPH FORUM, V32, P177, DOI 10.1111/cgf.12184; Huang SL, 2017, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2017.329; Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Joo H, 2018, PROC CVPR IEEE, P8320, DOI 10.1109/CVPR.2018.00868; Joo H, 2019, IEEE T PATTERN ANAL, V41, P190, DOI 10.1109/TPAMI.2017.2782743; Joo H, 2015, IEEE I CONF COMP VIS, P3334, DOI 10.1109/ICCV.2015.381; Kadkhodamohammadi A., 2018, MACH VISION APPL, V32, P1; Kocabas M, 2018, LECT NOTES COMPUT SC, V11215, P437, DOI 10.1007/978-3-030-01252-6_26; Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234; Lee J.M., 2001, INTRO SMOOTH MANIFOL; Leonardos S, 2016, IEEE INT CONF ROBOT, P587, DOI 10.1109/ICRA.2016.7487183; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288; Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170; Newell A, 2017, ADV NEUR IN, V30; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Pavlakos G, 2018, PROC CVPR IEEE, P7307, DOI 10.1109/CVPR.2018.00763; Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055; Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138; Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222; Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533; Sigal L, 2012, INT J COMPUT VISION, V98, P15, DOI 10.1007/s11263-011-0493-4; Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584; Sun X, 2017, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2017.284; Taylor GW, 2010, PROC CVPR IEEE, P631, DOI 10.1109/CVPR.2010.5540157; Tekin B, 2017, IEEE I CONF COMP VIS, P3961, DOI 10.1109/ICCV.2017.425; Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Yao A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.67; Zhang YX, 2020, PROC CVPR IEEE, P1321, DOI 10.1109/CVPR42600.2020.00140; Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541; Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537; Zhou XW, 2015, IEEE I CONF COMP VIS, P4032, DOI 10.1109/ICCV.2015.459; Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51	61	1	1	8	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6981	6992		10.1109/TPAMI.2021.3098052	http://dx.doi.org/10.1109/TPAMI.2021.3098052			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34283712				2022-12-18	WOS:000853875300080
J	Fan, LX; Ng, KW; Chan, CS; Yang, Q				Fan, Lixin; Ng, Kam Woh; Chan, Chee Seng; Yang, Qiang			DeepIPR: Deep Neural Network Ownership Verification With Passports	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Watermarking; Data models; Computational modeling; Analytical models; Training; Task analysis; Neural networks; Deep model protection; model ownership verification; intellectual property protection; model security; deep learning	ATTACKS	With substantial amount of time, resources and human (team) efforts invested to explore and develop successful deep neural networks (DNN), there emerges an urgent need to protect these inventions from being illegally copied, redistributed, or abused without respecting the intellectual properties of legitimate owners. Following recent progresses along this line, we investigate a number of watermark-based DNN ownership verification methods in the face of ambiguity attacks, which aim to cast doubts on the ownership verification by forging counterfeit watermarks. It is shown that ambiguity attacks pose serious threats to existing DNN watermarking methods. As remedies to the above-mentioned loophole, this paper proposes novel passport-based DNN ownership verification schemes which are both robust to network modifications and resilient to ambiguity attacks. The gist of embedding digital passports is to design and train DNN models in a way such that, the DNN inference performance of an original task will be significantly deteriorated due to forged passports. In other words, genuine passports are not only verified by looking for the predefined signatures, but also reasserted by the unyielding DNN model inference performances. Extensive experimental results justify the effectiveness of the proposed passport-based DNN ownership verification schemes. Code is available at https://github.com/kamwoh/DeepIPR.	[Fan, Lixin; Yang, Qiang] WeBank AI Lab, Shenzhen 518052, Peoples R China; [Ng, Kam Woh] Univ Surrey, Guildford GU2 7XH, Surrey, England; [Chan, Chee Seng] Univ Malaya, Fac Comp Sci & Informat Technol, Ctr Image & Signal Proc CISiP, Kuala Lumpur 50603, Malaysia; [Yang, Qiang] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China	University of Surrey; Universiti Malaya; Hong Kong University of Science & Technology	Chan, CS (corresponding author), Univ Malaya, Fac Comp Sci & Informat Technol, Ctr Image & Signal Proc CISiP, Kuala Lumpur 50603, Malaysia.	lixinfan@webank.com; kamwoh@gmail.com; cs.chan@um.edu.my; qyang@cse.ust.hk	yang, qiang/GYJ-0971-2022		National Key Research and Development Program of China [2020YFB1805501]; Fundamental Research Grant Scheme (FRGS) MoHE from the Ministry of Education Malaysia [FP021-2018A]; NVIDIA Corporation	National Key Research and Development Program of China; Fundamental Research Grant Scheme (FRGS) MoHE from the Ministry of Education Malaysia; NVIDIA Corporation	The authors would like to thank support of NVIDIA Corporation with the donation of Titan V GPU used for this research. This work was supported in part by the National Key Research and Development Program of China under Grant 2020YFB1805501 and in part by Fundamental Research Grant Scheme (FRGS) MoHE under Grant FP0212018A from the Ministry of Education Malaysia.	Adi Y, 2018, PROCEEDINGS OF THE 27TH USENIX SECURITY SYMPOSIUM, P1615; Allen-Zhu Z, 2019, PR MACH LEARN RES, V97; Benjamin Recht, 2017, Arxiv, DOI arXiv:1611.03530; Bita Darvish Rohani, 2018, Arxiv, DOI arXiv:1804.03648; Craver S, 1998, IEEE J SEL AREA COMM, V16, P573, DOI 10.1109/49.668979; Fan LX, 2019, ADV NEUR IN, V32; Boenisch F, 2020, Arxiv, DOI arXiv:2009.12153; Guo J, 2018, ICCAD-IEEE ACM INT, DOI 10.1145/3240765.3240862; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jebreel NM, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11030999; Le Merrer E., 2017, ARXIV; Li Q, 2006, PROC 8 WORKSHOP MULT, P158; Pyone April, 2020, 2020 IEEE 9th Global Conference on Consumer Electronics (GCCE), P818, DOI 10.1109/GCCE50665.2020.9291813; Rouhani BD, 2019, TWENTY-FOURTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXIV), P485, DOI 10.1145/3297858.3304051; See Abigail, 2016, P 20 SIGNLL C COMP N, P291, DOI DOI 10.18653/V1/K16-1029; Sencar HT, 2007, IEEE T INF FOREN SEC, V2, P664, DOI 10.1109/TIFS.2007.908211; Uchida Y, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P274, DOI 10.1145/3078971.3078974; Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI 10.1007/s11263-019-01198-w; Xu XR, 2020, IEEE ACCESS, V8, P102065, DOI 10.1109/ACCESS.2020.2998784; Zhang J., 2020, ADV NEURAL INFORM PR, V33, P22619; Zhang JL, 2018, PROCEEDINGS OF THE 2018 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIACCS'18), P159, DOI 10.1145/3196494.3196550	21	1	1	6	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6122	6139		10.1109/TPAMI.2021.3088846	http://dx.doi.org/10.1109/TPAMI.2021.3088846			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34125666				2022-12-18	WOS:000853875300022
J	Ferrari, C; Berretti, S; Pala, P; Del Bimbo, A				Ferrari, Claudio; Berretti, Stefano; Pala, Pietro; Del Bimbo, Alberto			A Sparse and Locally Coherent Morphable Face Model for Dense Semantic Correspondence Across Heterogeneous 3D Faces	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Faces; Three-dimensional displays; Strain; Solid modeling; Shape; Training; Training data; 3D morphable model; sparse components learning; dense correspondence	REGISTRATION; DATABASE; DEEP	The 3D Morphable Model (3DMM) is a powerful statistical tool for representing 3D face shapes. To build a 3DMM, a training set of face scans in full point-to-point correspondence is required, and its modeling capabilities directly depend on the variability contained in the training data. Thus, to increase the descriptive power of the 3DMM, establishing a dense correspondence across heterogeneous scans with sufficient diversity in terms of identities, ethnicities, or expressions becomes essential. In this manuscript, we present a fully automatic approach that leverages a 3DMM to transfer its dense semantic annotation across raw 3D faces, establishing a dense correspondence between them. We propose a novel formulation to learn a set of sparse deformation components with local support on the face that, together with an original non-rigid deformation algorithm, allow the 3DMM to precisely fit unseen faces and transfer its semantic annotation. We extensively experimented our approach, showing it can effectively generalize to highly diverse samples and accurately establish a dense correspondence even in presence of complex facial expressions. The accuracy of the dense registration is demonstrated by building a heterogeneous, large-scale 3DMM from more than 9,000 fully registered scans obtained by joining three large datasets together.	[Ferrari, Claudio; Berretti, Stefano; Pala, Pietro; Del Bimbo, Alberto] Univ Florence, Dept Informat Engn, I-50139 Florence, Italy	University of Florence	Ferrari, C (corresponding author), Univ Florence, Dept Informat Engn, I-50139 Florence, Italy.	claudio.ferrari@unifi.it; stefano.berretti@unifi.it; pietro.pala@unifi.it; alberto.delbimbo@unifi.it	Ferrari, Claudio/AEQ-4611-2022		Italian MIUR [20172BH297: I-MALL]	Italian MIUR(Ministry of Education, Universities and Research (MIUR))	This work was partially supported by the Italian MIUR within PRIN 2017, Project Grant 20172BH297: I-MALL.	Amberg B, 2007, IEEE I CONF COMP VIS, P1326; Amberg B, 2008, IEEE INT CONF AUTOMA, P667; Tran AT, 2018, PROC CVPR IEEE, P3935, DOI 10.1109/CVPR.2018.00414; Bagautdinov T, 2018, PROC CVPR IEEE, P3877, DOI 10.1109/CVPR.2018.00408; Berretti S, 2010, IEEE T PATTERN ANAL, V32, P2162, DOI 10.1109/TPAMI.2010.43; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; Booth J, 2018, INT J COMPUT VISION, V126, P233, DOI 10.1007/s11263-017-1009-7; Booth J, 2017, PROC CVPR IEEE, P5464, DOI 10.1109/CVPR.2017.580; Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598; Bouritsas G, 2019, IEEE I CONF COMP VIS, P7212, DOI 10.1109/ICCV.2019.00731; Brunton A, 2014, COMPUT VIS IMAGE UND, V128, P1, DOI 10.1016/j.cviu.2014.05.005; Brunton A, 2014, LECT NOTES COMPUT SC, V8689, P297, DOI 10.1007/978-3-319-10590-1_20; Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249; Cao X, 2018, PROC CVPR IEEE, P4635, DOI 10.1109/CVPR.2018.00487; Creusot C, 2013, INT J COMPUT VISION, V102, P146, DOI 10.1007/s11263-012-0605-9; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dai H, 2017, IEEE I CONF COMP VIS, P3104, DOI 10.1109/ICCV.2017.335; Davies R, 2008, STAT MODELS SHAPE OP; Egger B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3395208; Fan ZF, 2018, LECT NOTES COMPUT SC, V11220, P541, DOI 10.1007/978-3-030-01270-0_32; Fan ZF, 2019, PROC CVPR IEEE, P10936, DOI 10.1109/CVPR.2019.01120; Ferrari C, 2021, INT C PATT RECOG, P8833, DOI 10.1109/ICPR48806.2021.9412015; Ferrari C, 2019, LECT NOTES COMPUT SC, V11751, P532, DOI 10.1007/978-3-030-30642-7_48; Ferrari C, 2019, LECT NOTES COMPUT SC, V11130, P441, DOI 10.1007/978-3-030-11012-3_34; Ferrari C, 2016, INT C PATT RECOG, P1047, DOI 10.1109/ICPR.2016.7899774; Ferrari C, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P509, DOI 10.1109/3DV.2015.63; Galteri L., 2019, PROC IEEECVF C COMPU, P25; Galteri L, 2019, COMPUT VIS IMAGE UND, V185, P31, DOI 10.1016/j.cviu.2019.05.002; Gecer B, 2019, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2019.00125; Gilani SZ, 2018, IEEE T PATTERN ANAL, V40, P1584, DOI 10.1109/TPAMI.2017.2725279; Gilani SZ, 2017, PATTERN RECOGN, V69, P238, DOI 10.1016/j.patcog.2017.04.013; Gilani SZ, 2015, PROC CVPR IEEE, P4639, DOI 10.1109/CVPR.2015.7299095; Jiang ZH, 2019, PROC CVPR IEEE, P11949, DOI 10.1109/CVPR.2019.01223; Koppen P, 2018, PATTERN RECOGN, V74, P617, DOI 10.1016/j.patcog.2017.09.006; Lee H, 2016, ADV NEURAL INFORM PR, V19; Li H, 2008, COMPUT GRAPH FORUM, V27, P1421, DOI 10.1111/j.1467-8659.2008.01282.x; Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813; Liu F, 2019, IEEE I CONF COMP VIS, P9407, DOI 10.1109/ICCV.2019.00950; Lu XG, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P585; Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767; Luthi M, 2018, IEEE T PATTERN ANAL, V40, P1860, DOI 10.1109/TPAMI.2017.2739743; MAIRAL J., 2009, P 26 ANN INT C MACH, P689, DOI [10.1145/1553374.1553463, DOI 10.1145/1553374.1553463]; Mairal J, 2010, J MACH LEARN RES, V11, P19; Maiseli B, 2017, J VIS COMMUN IMAGE R, V46, P95, DOI 10.1016/j.jvcir.2017.03.012; Masi I, 2014, INT C PATT RECOG, P4477, DOI 10.1109/ICPR.2014.766; Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46; Neumann T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508417; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Perakis P, 2014, PATTERN RECOGN, V47, P2783, DOI 10.1016/j.patcog.2014.03.007; Perakis P, 2013, IEEE T PATTERN ANAL, V35, P1552, DOI 10.1109/TPAMI.2012.247; Phillips PJ, 2005, PROC CVPR IEEE, P947; Ploumpis S, 2021, IEEE T PATTERN ANAL, V43, P4142, DOI 10.1109/TPAMI.2020.2991150; Ploumpis S, 2019, PROC CVPR IEEE, P10926, DOI 10.1109/CVPR.2019.01119; Kassim SRA, 2006, IEEE IMAGE PROC, P661; Ranjan A, 2018, LECT NOTES COMPUT SC, V11207, P725, DOI 10.1007/978-3-030-01219-9_43; Salazar A, 2014, MACH VISION APPL, V25, P859, DOI 10.1007/s00138-013-0579-9; Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6; Segundo MP, 2010, IEEE T SYST MAN CY B, V40, P1319, DOI 10.1109/TSMCB.2009.2038233; Sengupta S, 2018, PROC CVPR IEEE, P6296, DOI 10.1109/CVPR.2018.00659; Sukno FM, 2015, IEEE T CYBERNETICS, V45, P1717, DOI 10.1109/TCYB.2014.2359056; Sun Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P263, DOI 10.1109/ICCV.2001.937634; Tam GKL, 2013, IEEE T VIS COMPUT GR, V19, P1199, DOI 10.1109/TVCG.2012.310; Tu XG, 2022, IEEE T CIRC SYST VID, V32, P1285, DOI 10.1109/TCSVT.2021.3078517; Tu XG, 2021, IEEE T MULTIMEDIA, V23, P1160, DOI 10.1109/TMM.2020.2993962; Tu Xiaoguang, 2019, ARXIV PREPRINT ARXIV; Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405; Zeng Y, 2016, IEEE T PATTERN ANAL, V38, P2416, DOI 10.1109/TPAMI.2016.2528240; Zeng Y, 2010, PROC CVPR IEEE, P382, DOI 10.1109/CVPR.2010.5540189; Zhao J., 2017, ADV NEURAL INFORM PR, P65; Zhao J, 2020, INT J COMPUT VISION, V128, P460, DOI 10.1007/s11263-019-01252-7; Zhao J, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1184; Zhao J, 2018, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2018.00235; Zhao J, 2019, IEEE T PATTERN ANAL, V41, P2380, DOI 10.1109/TPAMI.2018.2858819; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	76	1	1	2	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6667	6682		10.1109/TPAMI.2021.3090942	http://dx.doi.org/10.1109/TPAMI.2021.3090942			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34156937	Green Submitted			2022-12-18	WOS:000853875300061
J	Li, X; Chen, SC				Li, Xiang; Chen, Songcan			A Concise Yet Effective Model for Non-Aligned Incomplete Multi-View and Missing Multi-Label Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Learning systems; Fish; Data privacy; Video surveillance; Transforms; Training; Time complexity; Non-aligned incomplete multi-view; missing multi-label; global and local structures; model selection	CLASSIFIERS	In reality, learning from multi-view multi-label data inevitably confronts three challenges: missing labels, incomplete views, and non-aligned views. Existing methods mainly concern the first two and commonly need multiple assumptions to attack them, making even state-of-the-arts involve at least two explicit hyper-parameters such that model selection is quite difficult. More toughly, they will fail in handling the third challenge, let alone addressing the three jointly. In this paper, we aim at meeting these under the least assumption by building a concise yet effective model with just one hyper-parameter. To ease insufficiency of available labels, we exploit not only the consensus of multiple views but also the global and local structures hidden among multiple labels. Specifically, we introduce an indicator matrix to tackle the first two challenges in a regression form while aligning the same individual labels and all labels of different views in a common label space to battle the third challenge. In aligning, we characterize the global and local structures of multiple labels to be high-rank and low-rank, respectively. Subsequently, an efficient algorithm with linear time complexity in the number of samples is established. Finally, even without view-alignment, our method substantially outperforms state-of-the-arts with view-alignment on five real datasets.	[Li, Xiang; Chen, Songcan] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China; [Li, Xiang; Chen, Songcan] Nanjing Univ Aeronaut & Astronaut, Coll Artificial Intelligence, Nanjing 211106, Peoples R China; [Li, Xiang; Chen, Songcan] MIIT Key Lab Pattern Anal & Machine Intelligence, Nanjing 211106, Peoples R China	Nanjing University of Aeronautics & Astronautics; Nanjing University of Aeronautics & Astronautics	Chen, SC (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.	lx90@nuaa.edu.cn; s.chen@nuaa.edu.cn		Li, Xiang/0000-0003-1273-4610	Key Program of NSFC [61732006]; NSFC [61672281]	Key Program of NSFC(National Natural Science Foundation of China (NSFC)); NSFC(National Natural Science Foundation of China (NSFC))	The authors would like to thank Dr. Huan Li and Zhenghao Tan for their generous help and beneficial discussions. This work was supported in part by the Key Program of NSFC under Grant 61732006 and in part by the NSFC under Grant 61672281.	Asif U, 2018, IEEE T PATTERN ANAL, V40, P2051, DOI 10.1109/TPAMI.2017.2747134; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Bucak S. S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2801, DOI 10.1109/CVPR.2011.5995734; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Changming Zhu, 2019, 2019 International Conference on Data Mining Workshops (ICDMW). Proceedings, P689, DOI 10.1109/ICDMW.2019.00104; Da'u A, 2020, ARTIF INTELL REV, V53, P2709, DOI 10.1007/s10462-019-09744-1; Demsar J, 2006, J MACH LEARN RES, V7, P1; Du CD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P108, DOI 10.1145/3240508.3240528; Fu Y, 2018, ACM T INTEL SYST TEC, V9, P1; Gu XL, 2019, IEEE T MULTIMEDIA, V21, P1524, DOI 10.1109/TMM.2018.2876822; Guillaumin M, 2010, PROC CVPR IEEE, P902, DOI 10.1109/CVPR.2010.5540120; Houthuys L., 2017, PROC IEEE S SER COMP, P1; Hu ML, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2262; Kan MN, 2016, IEEE T PATTERN ANAL, V38, P188, DOI 10.1109/TPAMI.2015.2435740; Kim S, 2017, IEEE T PATTERN ANAL, V39, P1712, DOI 10.1109/TPAMI.2016.2615619; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Li SY, 2014, AAAI CONF ARTIF INTE, P1968; Li YM, 2019, IEEE T KNOWL DATA EN, V31, P1863, DOI 10.1109/TKDE.2018.2872063; Liu M, 2015, AAAI CONF ARTIF INTE, P2778; Liu XW, 2019, IEEE T PATTERN ANAL, V41, P2410, DOI 10.1109/TPAMI.2018.2879108; Luo Y, 2015, IEEE T IMAGE PROCESS, V24, P2355, DOI 10.1109/TIP.2015.2421309; Moyano JM, 2018, INFORM FUSION, V44, P33, DOI 10.1016/j.inffus.2017.12.001; NEMENYI P, 1962, BIOMETRICS, V18, P263; Neverova N, 2016, IEEE T PATTERN ANAL, V38, P1692, DOI 10.1109/TPAMI.2015.2461544; Nie FP, 2018, IEEE T IMAGE PROCESS, V27, P1501, DOI 10.1109/TIP.2017.2754939; Nie FP, 2017, AAAI CONF ARTIF INTE, P2408; Nie FP, 2017, IEEE T IMAGE PROCESS, V26, P5718, DOI 10.1109/TIP.2017.2746270; Qiu Q, 2015, J MACH LEARN RES, V16, P187; Seber G. A., 2012, LINEAR REGRESSION AN, V329; Sun SL, 2021, IEEE T PATTERN ANAL, V43, P2682, DOI 10.1109/TPAMI.2020.2974203; Tan QY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2703; Tingkai Sun, 2009, 2009 WRI World Congress on Computer Science and Information Engineering, CSIE, P95, DOI 10.1109/CSIE.2009.794; Tsoumakas G., 2007, INT J DATA WAREHOUSI, V3, P1; Wang XG, 2013, PATTERN RECOGN LETT, V34, P3, DOI 10.1016/j.patrec.2012.07.005; WATSON GA, 1992, LINEAR ALGEBRA APPL, V170, P33, DOI 10.1016/0024-3795(92)90407-2; Wu J, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3379504; Wu PC, 2016, IEEE T KNOWL DATA EN, V28, P454, DOI 10.1109/TKDE.2015.2477296; Wu X, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3884; Xu C, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2490539; Xu C, 2014, IEEE T PATTERN ANAL, V36, P1559, DOI 10.1109/TPAMI.2013.2296528; Xu XX, 2016, IEEE T PATTERN ANAL, V38, P1113, DOI 10.1109/TPAMI.2015.2476813; Xue Z, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4026; You XG, 2019, PATTERN RECOGN, V92, P37, DOI 10.1016/j.patcog.2019.03.008; Yuan Lei, 2012, KDD, P1149; Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958; Zhang CQ, 2019, ADV NEUR IN, V32; Zhang CQ, 2018, AAAI CONF ARTIF INTE, P4414; Zhang J, 2019, PATTERN RECOGN, V95, P136, DOI 10.1016/j.patcog.2019.06.003; Zhang MY, 2019, IEEE IMAGE PROC, P2134, DOI 10.1109/ICIP.2019.8803160; Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39; Zhang W., 2013, PROC INT JOINT C ART, P1910; Zhao H., 2016, PROC INT JOINT C ART, P2392; Zhao J, 2017, INFORM FUSION, V38, P43, DOI 10.1016/j.inffus.2017.02.007; Ma ZC, 2022, Arxiv, DOI arXiv:2004.03951; Zhu CM, 2020, NEUROCOMPUTING, V371, P67, DOI 10.1016/j.neucom.2019.09.009; Zhu Y, 2018, IEEE T KNOWL DATA EN, V30, P1081, DOI 10.1109/TKDE.2017.2785795; Zhuge WZ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4482	58	1	1	6	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					5918	5932		10.1109/TPAMI.2021.3086895	http://dx.doi.org/10.1109/TPAMI.2021.3086895			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34097605	Green Submitted			2022-12-18	WOS:000853875300009
J	Li, YM; Cui, BY; Zhang, ZM				Li, Yingming; Cui, Baiyun; Zhang, Zhongfei Mark			Efficient Relational Sentence Ordering Network	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Hidden Markov models; Bit error rate; Task analysis; Coherence; Computational modeling; Predictive models; Decoding; Sentence ordering; divide-and-fuse; cross-attention; sentence relationship; pointer network	LOCAL COHERENCE	In this paper, we propose a novel deep Efficient Relational Sentence Ordering Network (referred to as ERSON) by leveraging pre-trained language model in both encoder and decoder architectures to strengthen the coherence modeling of the entire model. Specifically, we first introduce a divide-and-fuse BERT (referred to as DF-BERT), a new refactor of BERT network, where lower layers in the improved model encode each sentence in the paragraph independently, which are shared by different sentence pairs, and the higher layers learn the cross-attention between sentence pairs jointly. It enables us to capture the semantic concepts and contextual information between the sentences of the paragraph, while significantly reducing the runtime and memory consumption without sacrificing the model performance. Besides, a Relational Pointer Decoder (referred to as RPD) is developed, which utilizes the pre-trained Next Sentence Prediction (NSP) task of BERT to capture the useful relative ordering information between sentences to enhance the order predictions. In addition, a variety of knowledge distillation based losses are added as auxiliary supervision to further improve the ordering performance. The extensive evaluations on Sentence Ordering, Order Discrimination, and Multi-Document Summarization tasks show the superiority of ERSON to the state-of-the-art ordering methods.	[Li, Yingming; Cui, Baiyun] Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou 310027, Zhejiang, Peoples R China; [Zhang, Zhongfei Mark] Binghamton Univ, Comp Sci Dept, Binghamton, NY 13902 USA	Zhejiang University; State University of New York (SUNY) System; State University of New York (SUNY) Binghamton	Li, YM (corresponding author), Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou 310027, Zhejiang, Peoples R China.	yingming@zju.edu.cn; baiyunc@yahoo.com; zzhang@binghamton.edu		Zhang, Zhongfei/0000-0001-5098-2506	NSFC [61702448, 61672456, U19B2043]; Artificial Intelligence Research Foundation of Baidu Inc.; Horizon Robotics; HIKVision	NSFC(National Natural Science Foundation of China (NSFC)); Artificial Intelligence Research Foundation of Baidu Inc.; Horizon Robotics; HIKVision	This work was supported in part by the NSFC under Grants 61702448, 61672456, and U19B2043, in part by the Artificial Intelligence Research Foundation of Baidu Inc., and in part by the HIKVision and Horizon Robotics.	Adhikari Ashutosh, 2019, ARXIV190408398; Adriana Romero, 2015, Arxiv, DOI arXiv:1412.6550; Alan W Black, 2020, Arxiv, DOI arXiv:2005.00432; Aruna Balasubramanian, 2020, Arxiv, DOI arXiv:2005.00697; Avishek Joey Bose, 2019, Arxiv, DOI arXiv:1905.11912; Ba J.L., 2016, ARXIV, DOI DOI 10.48550/ARXIV.1607.06450; Barzilay R, 2004, HLT-NAACL 2004: HUMAN LANGUAGE TECHNOLOGY CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE MAIN CONFERENCE, P113; Barzilay R, 2002, J ARTIF INTELL RES, V17, P35, DOI 10.1613/jair.991; Barzilay R, 2008, COMPUT LINGUIST, V34, P1, DOI 10.1162/coli.2008.34.1.1; Barzilay Regina, 2005, P 43 ANN M ASS COMP, P141, DOI DOI 10.3115/1219840.1219858; Bucilua Cristian, 2006, P 12 ACM SIGKDD INT, P535, DOI [10.1145/1150402.1150464, DOI 10.1145/1150402.1150464]; Burstein Jill, 2010, HUMAN LANGUAGE TECHN, P681; Chulun Zhou, 2019, Arxiv, DOI arXiv:1912.07225; Clark K, 2019, BLACKBOXNLP WORKSHOP ON ANALYZING AND INTERPRETING NEURAL NETWORKS FOR NLP AT ACL 2019, P276, DOI 10.18653/v1/w19-4828; Cui BY, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P6310; Cui BY, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2027, DOI 10.1145/3132847.3133047; Cui Baiyun, 2018, P 2018 C EMP METH NA, P4340, DOI DOI 10.18653/V1/D18-1465; Danqi Chen, 2019, Arxiv, DOI arXiv:1907.11692; Nguyen DT, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1320, DOI 10.18653/v1/P17-1121; Devlin Jacob, 2019, P 2019 C N AM CHAPT, Patent No. [ArXiv181004805Cs, 181004805]; Dhanajit Brahma, 2019, Arxiv, DOI arXiv:2001.00056; Elsner Micha, 2011, P 49 ANN M ASS COMP, V2, P125; Erkan G, 2004, J ARTIF INTELL RES, V22, P457, DOI 10.1613/jair.1523; Fabbri AR, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1074; Farag Y, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P629; Geoffrey Hinton, 2015, Arxiv, DOI arXiv:1503.02531; GROSZ BJ, 1995, COMPUT LINGUIST, V21, P203; Hao Y., 2019, P C EMP METH NAT LAN, P4134; Huang Ting-Hao Kenneth, 2016, P 2016 C N AM CHAPT, P1233, DOI DOI 10.18653/V1/N16-1147; Jaime Carbonell, 2020, Arxiv, DOI arXiv:1906.08237; Jawahar G, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3651; Jiao Xiaoqi, 2020, TINYBERT DISTILLING; Jingjing Gong, 2016, Arxiv, DOI arXiv:1611.04953; Julien Chaumond, 2020, Arxiv, DOI arXiv:1910.01108; Lai A, 2018, 19TH ANNUAL MEETING OF THE SPECIAL INTEREST GROUP ON DISCOURSE AND DIALOGUE (SIGDIAL 2018), P214; Lan Z., 2020, ARXIV; Lapata M, 2003, 41ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P545; Lapata M, 2006, COMPUT LINGUIST, V32, P471, DOI 10.1162/coli.2006.32.4.471; Lee H, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1551; Li J., 2014, P 2014 C EMPIRICAL M, P2039; Li JQ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P3009; Li Jiwei, 2017, P EMNLP; Li Yanran, 2017, ARXIV171003957; Lin C.-Y., 2002, P ACL 02 WORKSH AUT, V4, P45, DOI DOI 10.3115/1118162.1118168; Liu SN, 2020, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2020), P1695; Logeswaran L, 2018, AAAI CONF ARTIF INTE, P5285; Manjunath Kudlur, 2016, Arxiv, DOI arXiv:1511.06391; Mesgar M, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P1439; Michel Paul, 2019, ARXIV; Mihalcea R, 2004, P 2004 C EMP METH NA, P404, DOI 10.5555/1613715; Mostafazadeh N., 2016, P 2016 C N AM CHAPT, P839, DOI DOI 10.18653/V1/N16-1098; Nayeem M. T., 2017, PROC TEXTGRAPHS 11 W, P51; Zafrir O, 2019, Arxiv, DOI arXiv:1910.06188; Oh B, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2273; Over Paul, 2004, INTRO DUC 2004; Shen S, 2020, AAAI CONF ARTIF INTE, V34, P8815; Sun SQ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4323; Tenney I, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4593; Vaswani A, 2017, ADV NEUR IN, V30; Vinyals O., 2015, ADV NEURAL INFORM PR, P2692; Wang D., 2020, ACL, P6209, DOI [DOI 10.18653/V1/2020.ACL-MAIN.553, 10.18653/v1/2020.acl-main.553]; Wang TM, 2019, AAAI CONF ARTIF INTE, P7184; Xinchi Chen, 2016, Arxiv, DOI arXiv:1607.06952; Yin YJ, 2020, AAAI CONF ARTIF INTE, V34, P9482; Zeng Xingshan, 2018, P C N AM CHAPT ASS C, P375, DOI DOI 10.18653/V1/N18-1035; Zhang W, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P509	69	1	1	2	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6169	6183		10.1109/TPAMI.2021.3085738	http://dx.doi.org/10.1109/TPAMI.2021.3085738			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34061736				2022-12-18	WOS:000853875300025
J	Lin, WY; Liu, SY; Ren, CH; Cheung, NM; Li, HD; Matsushita, Y				Lin, Wen-Yan; Liu, Siying; Ren, Changhao; Cheung, Ngai-Man; Li, Hongdong; Matsushita, Yasuyuki			Shell Theory: A Statistical Model of Reality	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Semantics; Mathematical model; Machine learning; Random variables; Manifolds; Machine learning algorithms; Prediction algorithms; High dimension; statistics; semantic manifold; anomaly detection; one-class learning; life-long learning; incremental learning; hierarchical models; generative models	POINT	The foundational assumption of machine learning is that the data under consideration is separable into classes; while intuitively reasonable, separability constraints have proven remarkably difficult to formulate mathematically. We believe this problem is rooted in the mismatch between existing statistical techniques and commonly encountered data; object representations are typically high dimensional but statistical techniques tend to treat high dimensions a degenerate case. To address this problem, we develop a dedicated statistical framework for machine learning in high dimensions. The framework derives from the observation that object relations form a natural hierarchy; this leads us to model objects as instances of a high dimensional, hierarchal generative processes. Using a distance based statistical technique, also developed in this paper, we show that in such generative processes, instances of each process in the hierarchy, are almost-always encapsulated by a distinctive-shell that excludes almost-all other instances. The result is shell theory, a statistical machine learning framework in which separability constraints (distinctive-shells) are formally derived from the assumed generative process.	[Lin, Wen-Yan] Singapore Management Univ, Singapore 188065, Singapore; [Liu, Siying] Inst Infocomm Res I2R, Singapore 138632, Singapore; [Ren, Changhao; Cheung, Ngai-Man] Singapore Univ Technol & Design, Singapore 487372, Singapore; [Li, Hongdong] Australian Natl Univ, Canberra, ACT 0200, Australia; [Matsushita, Yasuyuki] Osaka Univ, Osaka 5650871, Japan	Singapore Management University; Agency for Science Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R); Singapore University of Technology & Design; Australian National University; Osaka University	Lin, WY (corresponding author), Singapore Management Univ, Singapore 188065, Singapore.	daniellin@smu.edu.sg; liusy1@i2r.a-star.edu.sg; changhao_ren@sutd.edu.sg; ngaiman_cheung@sutd.edu.sg; hongdong.li@anu.edu.au; yasumat@ist.osaka-u.ac.jp		Matsushita, Yasuyui/0000-0002-1935-4752	Singapore Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 1 Grant	Singapore Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 1 Grant(Ministry of Education, Singapore)	The authors would like to thank Lam Hoi Sze's for his contribution toward making the paper possible. This work was supported by the Singapore Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 1 Grant.	Aggarwal CC, 2001, LECT NOTES COMPUT SC, V1973, P420; Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16; An J., 2015, SPEC LECT, V2, P1, DOI DOI 10.1007/BF00758335; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; [Anonymous], US; Blei DM, 2002, ADV NEUR IN, V14, P601; BOX GEP, 1965, ANN MATH STAT, V36, P1468, DOI 10.1214/aoms/1177699906; Chen YQ, 2001, IEEE IMAGE PROC, P34, DOI 10.1109/ICIP.2001.958946; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Deecke L, 2019, LECT NOTES ARTIF INT, V11051, P3, DOI 10.1007/978-3-030-10925-7_1; Dellaert F, 2002, EXPECTATION MAXIMIZA; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Dinh L., 2017, ARXIV; Elson J, 2007, CCS'07: PROCEEDINGS OF THE 14TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P366; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Gelman A., 2014, BAYESIAN DATA ANAL, DOI [DOI 10.1201/B16018, 10.1201/b16018]; Golan I, 2018, ADV NEUR IN, V31; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gorban AN, 2018, PHILOS T R SOC A, V376, DOI 10.1098/rsta.2017.0237; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428; Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7; Holster A, 2016, L OPEZ CORREDOIRAS T; Kawaguchi K., 2017, ARXIV; Kenneth O. Stanley, 2018, Arxiv, DOI arXiv:1609.02226; Khalid H, 2020, IEEE COMPUT SOC CONF, P2794, DOI 10.1109/CVPRW50498.2020.00336; Kuo YH, 2011, PROC CVPR IEEE, P905, DOI 10.1109/CVPR.2011.5995639; Li XP, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P305, DOI 10.1145/3097983.3098077; Li Y., 2020, ARXIV; Lin HW, 2017, J STAT PHYS, V168, P1223, DOI 10.1007/s10955-017-1836-5; Lin WY, 2018, PROC CVPR IEEE, P5784, DOI 10.1109/CVPR.2018.00606; Mitchell T, 2018, COMMUN ACM, V61, P103, DOI 10.1145/3191513; Moon TK, 1996, IEEE SIGNAL PROC MAG, V13, P47, DOI 10.1109/79.543975; Parisi GI, 2019, NEURAL NETWORKS, V113, P54, DOI 10.1016/j.neunet.2019.01.012; Parkhi OM, 2011, IEEE I CONF COMP VIS, P1427, DOI 10.1109/ICCV.2011.6126398; Platt JC, 2000, ADV NEUR IN, P61; Radovanovic M, 2015, IEEE T KNOWL DATA EN, V27, P1369, DOI 10.1109/TKDE.2014.2365790; Russell B. C., 2006, P IEEE C COMP VIS PA, V2, P1605; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sivic J, 2008, PROC CVPR IEEE, P2182; Tomasev N, 2014, IEEE T KNOWL DATA EN, V26, P739, DOI 10.1109/TKDE.2013.25; WELLS RO, 1979, B AM MATH SOC, V1, P296, DOI 10.1090/S0273-0979-1979-14596-8; Wen-Yan Lin, 2013, 2013 IEEE International Conference on Computer Vision (ICCV), P2376, DOI 10.1109/ICCV.2013.295; Xiao H., 2017, FASHION MNIST NOVEL; Xuan GR, 2001, IEEE IMAGE PROC, P145, DOI 10.1109/ICIP.2001.958974; Yang MX, 2021, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR46437.2021.00119; Zhai S., 2016, ARXIV; Zhang C., 2016, ARXIV; Zhou BL, 2014, ADV NEUR IN, V27; Zhu JK, 2009, PROC CVPR IEEE, P1319, DOI 10.1109/CVPRW.2009.5206512; Zhu PF, 2013, IEEE I CONF COMP VIS, P2664, DOI 10.1109/ICCV.2013.331; Zong B, 2018, INT C LEARN REPR	58	1	1	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6438	6453		10.1109/TPAMI.2021.3084598	http://dx.doi.org/10.1109/TPAMI.2021.3084598			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34048335	Green Accepted			2022-12-18	WOS:000853875300043
J	Liu, FH; Huang, XL; Chen, YD; Suykens, JAK				Liu, Fanghui; Huang, Xiaolin; Chen, Yudong; Suykens, Johan A. K.			Random Features for Kernel Approximation: A Survey on Algorithms, Theory, and Beyond	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Kernel; Approximation algorithms; Taxonomy; Scalability; Risk management; Prediction algorithms; Loss measurement; Random features; kernel approximation; generalization properties; over-parameterized models	MATRIX	The class of random features is one of the most popular techniques to speed up kernel methods in large-scale problems. Related works have been recognized by the NeurIPS Test-of-Time award in 2017 and the ICML Best Paper Finalist in 2019. The body of work on random features has grown rapidly, and hence it is desirable to have a comprehensive overview on this topic explaining the connections among various algorithms and theoretical results. In this survey, we systematically review the work on random features from the past ten years. First, the motivations, characteristics and contributions of representative random features based algorithms are summarized according to their sampling schemes, learning procedures, variance reduction properties and how they exploit training data. Second, we review theoretical results that center around the following key question: how many random features are needed to ensure a high approximation quality or no loss in the empirical/expected risks of the learned estimator. Third, we provide a comprehensive evaluation of popular random features based algorithms on several large-scale benchmark datasets and discuss their approximation quality and prediction performance for classification. Last, we discuss the relationship between random features and modern over-parameterized deep neural networks (DNNs), including the use of high dimensional random features in the analysis of DNNs as well as the gaps between current theoretical and empirical results. This survey may serve as a gentle introduction to this topic, and as a users' guide for practitioners interested in applying the representative algorithms and understanding theoretical results under various technical assumptions. We hope that this survey will facilitate discussion on the open problems in this topic, and more importantly, shed light on future research directions. Due to the page limit, we suggest the readers refer to the full version of this survey https://arxiv.org/abs/2004.11154.	[Liu, Fanghui; Suykens, Johan A. K.] Katholieke Univ Leuven, Dept Elect Engn ESAT STADIUS, B-3001 Leuven, Belgium; [Huang, Xiaolin] Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai 200240, Peoples R China; [Huang, Xiaolin] Shanghai Jiao Tong Univ, Inst Med Robot, Shanghai 200240, Peoples R China; [Chen, Yudong] Cornell Univ, Sch Operat Res & Informat Engn, Ithaca, NY 14850 USA	KU Leuven; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Cornell University	Liu, FH (corresponding author), Katholieke Univ Leuven, Dept Elect Engn ESAT STADIUS, B-3001 Leuven, Belgium.; Huang, XL (corresponding author), Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai 200240, Peoples R China.; Huang, XL (corresponding author), Shanghai Jiao Tong Univ, Inst Med Robot, Shanghai 200240, Peoples R China.	fanghui.liu@esat.kuleuven.be; xiaolinhuang@sjtu.edu.cn; yudong.chen@cornell.edu; johan.suykens@esat.kuleuven.be	Suykens, Johan/C-9781-2014	Suykens, Johan/0000-0002-8846-6352	European Research Council through the European Union's Horizon 2020 research and innovation program/ERC Advanced Grant EDUALITY [787960]; Research Council KU Leuven: Through Optimization frameworks for deep kernel machines [C14/18/068]; Flemish Government through FWO [GOA4917N]; Flemish Government (AI Research Program); Ford KU Leuven Research Alliance Project [KUL0076]; EU H2020 ICT-48 Network TAILOR (Foundations of Trustworthy AI -Integrating Reasoning, Learning and Optimization), Leuven. AI Institute; National Natural Science Foundation of China [61977046]; National Science Foundation [CCF-1657420, CCF-1704828]; SJTU Global Strategic Partnership Fund (2020 SJTUCORNELL); Shanghai Municipal Science and Technology Major Project [2021SHZDZX0102]	European Research Council through the European Union's Horizon 2020 research and innovation program/ERC Advanced Grant EDUALITY; Research Council KU Leuven: Through Optimization frameworks for deep kernel machines; Flemish Government through FWO(FWO); Flemish Government (AI Research Program); Ford KU Leuven Research Alliance Project; EU H2020 ICT-48 Network TAILOR (Foundations of Trustworthy AI -Integrating Reasoning, Learning and Optimization), Leuven. AI Institute; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Science Foundation(National Science Foundation (NSF)); SJTU Global Strategic Partnership Fund (2020 SJTUCORNELL); Shanghai Municipal Science and Technology Major Project	This work was supported in part by European Research Council through the European Union's Horizon 2020 research and innovation program/ERC Advanced Grant EDUALITY under Grant 787960, in part by Research Council KU Leuven: Through Optimization frameworks for deep kernel machines under Grant C14/18/068, in part by Flemish Government through FWO Projects: GOA4917N (Deep Restricted Kernel Machines: Methods and Foundations), in part by the PhD/Postdoc grant, in part by the Flemish Government (AI Research Program), in part by Ford KU Leuven Research Alliance Project under Grant KUL0076 (Stability analysis and performance improvement of deep reinforcement learning algorithms), in part by EU H2020 ICT-48 Network TAILOR (Foundations of Trustworthy AI -Integrating Reasoning, Learning and Optimization), Leuven. AI Institute, in part by the National Natural Science Foundation of China under Grant 61977046, in part by National Science Foundation under Grants CCF-1657420 and CCF-1704828, in part by SJTU Global Strategic Partnership Fund (2020 SJTUCORNELL), and in part by Shanghai Municipal Science and Technology Major Project under Grant 2021SHZDZX0102.	Adlam B., 2020, ADV NEURAL INFORM PR, V33, P11022; Agrawal R, 2019, PR MACH LEARN RES, V89; Allen-Zhu Z, 2019, ADV NEUR IN, V32; Andrea Montanari, 2020, Arxiv, DOI arXiv:1908.05355; Andrea Montanari, 2020, Arxiv, DOI arXiv:1904.12191; Andrea Montanari, 2020, Arxiv, DOI arXiv:1911.01544; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Arora Sonu, 2020, 2020 IEEE Hot Chips 32 Symposium (HCS), DOI 10.1109/HCS49909.2020.9220414; Arora S, 2019, ADV NEUR IN, V32; Arora S, 2019, PR MACH LEARN RES, V97; Avron H, 2014, ADV NEUR IN, V27; Avron H, 2017, PR MACH LEARN RES, V70; Avron H, 2016, J MACH LEARN RES, V17; Ba J., 2020, PROC INT C LEARN REP, P1; Bach Francis, 2013, C LEARNING THEORY, P185; Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282; Belhadji A, 2019, ADV NEUR IN, V32; Belkin M, 2019, P NATL ACAD SCI USA, V116, P15849, DOI 10.1073/pnas.1903070116; Benjamin Recht, 2017, Arxiv, DOI arXiv:1611.03530; Bietti A, 2019, ADV NEUR IN, V32; Bietti A, 2019, J MACH LEARN RES, V20; Blanchard G., 2010, ADV NEURAL INFORM PR, V23, P226; Bochner S, 1934, B AM MATH SOC, P271, DOI DOI 10.1090/S0002-9904-1934-05843-9; Bojarski M, 2017, PR MACH LEARN RES, V54, P1020; Brabanter J.D., 2002, LEAST SQUARES SUPPOR, DOI DOI 10.1142/9789812776655; Brauchart JS, 2015, J COMPLEXITY, V31, P293, DOI 10.1016/j.jco.2015.02.003; Bullins B., 2018, PROC INT C LEARN REP, P1; Calandriello D, 2017, PR MACH LEARN RES, V54, P1421; Cao Y., 2019, PROC INT C MACH LEAR, P10835; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Carleo G, 2019, REV MOD PHYS, V91, DOI 10.1103/RevModPhys.91.045002; Carratino L., 2018, ADV NEURAL INFORM PR, P10212; Chang WC, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1497; Chi-Keung Tang, 2016, Arxiv, DOI arXiv:1607.03250; Chizat L., 2020, C LEARNING THEORY, P1305; Chizat L, 2019, ADV NEUR IN, V32; Cho Youngmin, 2009, P ADV NEUR INF PROC, V22, P342; Choromanski K, 2021, PROC INT C LEARN REP, P1; Choromanski K. M., 2017, PROC 31 INT C NEURAL, P219; Cucker F, 2002, B AM MATH SOC, V39, P1; Dai B, 2014, ADV NEUR IN, V27; Dan Hendrycks, 2020, Arxiv, DOI arXiv:1606.08415; Dao Tri, 2017, Adv Neural Inf Process Syst, V30, P6109; dAscoli S., 2020, ARXIV; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dhifallah O., 2020, ARXIV; Du Simon S., 2019, 7 INT C LEARNING REP, P1; El Alaoui A, 2015, ADV NEUR IN, V28; Eran Malach, 2020, Arxiv, DOI arXiv:2002.00585; Erd ~elyi T., 2020, PROC INT C NEURAL IN, P109; Evans G., 1993, PRACTICAL NUMERICAL; Felix X. Yu, 2015, Arxiv, DOI arXiv:1503.03893; Feng C, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3490; Frankle J., 2018, LOTTERY TICKET HYPOT, P1; Gerace F, 2020, PR MACH LEARN RES, V119; Ghashami M, 2016, JMLR WORKSH CONF PRO, V51, P1365; Ghorbani B., 2009, PROC INT C NEURAL IN, P1; Guo ZC, 2019, APPL COMPUT HARMON A, V47, P662, DOI 10.1016/j.acha.2017.11.005; Hastie T., 2019, ARXIV PREPRINT ARXIV; Heiss F, 2008, J ECONOMETRICS, V144, P62, DOI 10.1016/j.jeconom.2007.12.004; Hu H, 2020, Arxiv, DOI arXiv:2009.07669; Honorio J., 2017, ARXIV; Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Javanmard A., 2020, PROC C LEARN THEORY; Ji ZL, 2020, IEEE GLOB COMM CONF, DOI 10.1109/GLOBECOM42002.2020.9322411; Kafai M, 2019, IEEE T PATTERN ANAL, V41, P34, DOI 10.1109/TPAMI.2017.2785313; Kar P., 2012, ARTIF INTELL, P583; Kobak D, 2020, J MACH LEARN RES, V21; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Kumar, 2015, ADV NEURAL INFORM PR, P1846; Le Q.V., 2013, JMLR W CP, P244; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee Jooyoung, 2018, INT C LEARN REPR; Li CL, 2019, PR MACH LEARN RES, V89; Li P, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P315, DOI 10.1145/3097983.3098081; Li X., 2021, PROC INT C MACH LEAR, P6369; Li X, 2017, IEEE INFOCOM SER; Li Z., 2019, P MACHINE LEARNING R, P3905; Li Z, 2021, J MACH LEARN RES, V22; Liang T, 2019, AM J ROENTGENOL, V212, P958, DOI 10.2214/AJR.18.20696; Liao Z., 2020, ADV NEURAL INF PROCE, V33, P13939; Liao ZY, 2018, PR MACH LEARN RES, V80; Lin LC, 2020, Arxiv, DOI arXiv:2010.05170; Lin SB, 2017, J MACH LEARN RES, V18; Liu F., 2021, J MACH LEARN RES, V22, P1; Liu F., 2020, ARXIV; Liu F., 2021, PERS UBIQUIT COMPUT, P1; Liu FH, 2021, PR MACH LEARN RES, V130, P388; Liu FH, 2020, AAAI CONF ARTIF INTE, V34, P4844; Liu FH, 2020, IEEE T NEUR NET LEAR, V31, P2965, DOI 10.1109/TNNLS.2019.2934729; Liu J. S., 2008, MONTE CARLO STRATEGI; Loosli Gaelle, 2007, LARGE SCALE KERNEL M, V6; Lopez-Paz D, 2014, PR MACH LEARN RES, V32, P1359; LYU Y., 2020, PROC INT C NEURAL IN, P6269; Lyu YM, 2017, PR MACH LEARN RES, V70; Meanti G., 2020, ADV NEURAL INFORM PR, P11022; Mei S., ARXIV 210110588, V2021; Meister M, 2019, ADV NEUR IN, V32; Mezard M., 1987, SPIN GLASS THEORY IN; Munkhoeva M, 2018, ADV NEUR IN, V31; Nakkiran P, 2021, J STAT MECH-THEORY E, V2021, DOI 10.1088/1742-5468/ac3a74; Niederreiter H., 1992, RANDOM NUMBER GENERA, V63; Pham N, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P239, DOI 10.1145/2487575.2487591; Ohana R., 2019, ARXIV; Oliva JB, 2016, JMLR WORKSH CONF PRO, V51, P1078; Peng H, 2022, INT J ADV MANUF TECH, V119, P99, DOI 10.1007/s00170-021-08284-9; Pennington J, 2017, ADV NEUR IN, V30; Peters J, 2017, ADAPT COMPUT MACH LE; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rahimi A, 2008, ANN ALLERTON CONF, P555, DOI 10.1109/ALLERTON.2008.4797607; Refinetti M., 2021, ARXIV; Rocks JW., 2020, ARXIV; Rudi A, 2018, ADV NEUR IN, V31; Rudi A, 2017, ADV NEUR IN, V30; Saade A, 2016, INT CONF ACOUST SPEE, P6215, DOI 10.1109/ICASSP.2016.7472872; Saitoh S, 2016, DEV MATH, V44, P1, DOI 10.1007/978-981-10-0530-5; Schoenberg I., 1942, DUKE MATH J, V9, P96, DOI DOI 10.1215/S0012-7094-42-00908-6; Shahrampour S, 2018, AAAI CONF ARTIF INTE, P4026; Shawe-Taylor J, 2002, LECT NOTES ARTIF INT, V2533, P23; Shen WW, 2017, AAAI CONF ARTIF INTE, P2520; Shen Z., 2019, PROC INT C ARTIF INT; Sinha A., 2016, ADV NEURAL INFORM PR, P1298; Smale S, 2007, CONSTR APPROX, V26, P153, DOI 10.1007/s00365-006-0659-y; Smola AJ, 2001, ADV NEUR IN, V13, P308; Sriperumbudur B., 2017, ARXIV; Sriperumbudur Bharath, 2015, ADV NEURAL INFORM PR, P1144; Sun YT, 2018, ADV NEUR IN, V31; Sutherland DJ, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P862; Tao T., 2012, GRADUATE STUDIES MAT, V132; Liang TY, 2021, Arxiv, DOI arXiv:2002.01586; Thrampoulidis C., 2015, PROC C LEARN THEORY, P1683; Ullah E., 2018, ADV NEURAL INFORM PR, P7311; uller C. M, 2006, SPHERICAL HARMONICS; Vapnik V., 2013, STAT LEARNING THEORY, DOI DOI 10.1007/978-1-4757-2440-0; Wainwright MJ, 2019, CA ST PR MA, P1, DOI 10.1017/9781108627771; Wang S., 2019, ARXIV; Wendland H., 2005, SCATTERED DATA APPRO; Wilson A., 2013, INT C MACH LEARN, P1067; Woodruff D., 2020, PROC INT C MACH LEAR, P10324; Wu DD, 2021, POLYM BULL, V78, P5567, DOI 10.1007/s00289-020-03379-x; Xie B., 2015, ADV NEURAL INFORM PR, P2341; Yamada H., 2020, P 2020 INT WORKSHOP, P1; Yang JY, 2014, PR MACH LEARN RES, V32; Yang T., 2012, ADV NEURAL INFORM PR, P476; Yang ZC, 2015, JMLR WORKSH CONF PRO, V38, P1098; Yehudai G, 2019, ADV NEUR IN, V32; Wang YS, 2019, Arxiv, DOI arXiv:1910.05384; Yu Felix X, 2016, ADV NEURAL INFORM PR, V29, P1975; Zambon D., 2020, PROC INT C MACH LEAR, P10968; Zandieh A., 2021, ARXIV; Zhang Jian, 2019, Proc Mach Learn Res, V89, P1264; Zhang T, 2005, NEURAL COMPUT, V17, P2077, DOI 10.1162/0899766054323008	171	1	1	2	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					7128	7148		10.1109/TPAMI.2021.3097011	http://dx.doi.org/10.1109/TPAMI.2021.3097011			21	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34310285	Green Submitted, Green Accepted			2022-12-18	WOS:000853875300089
J	Ma, K; Xu, QQ; Zeng, JS; Cao, XC; Huang, QM				Ma, Ke; Xu, Qianqian; Zeng, Jinshan; Cao, Xiaochun; Huang, Qingming			Poisoning Attack Against Estimating From Pairwise Comparisons	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Optimization; Heuristic algorithms; Sports; Voting; Uncertainty; Games; Data models; Adversarial learning; poisoning attack; pairwise comparison; rank aggregation; robust game; distributionally robust optimization	BAYESIAN PLAYERS; ROBUST; GAMES; EQUILIBRIUM; RANKING	As pairwise ranking becomes broadly employed for elections, sports competitions, recommendation, information retrieval and so on, attackers have strong motivation and incentives to manipulate or disrupt the ranking list. They could inject malicious comparisons into the training data to fool the target ranking algorithm. Such a technique is called "poisoning attack" in regression and classification tasks. In this paper, to the best of our knowledge, we initiate the first systematic investigation of data poisoning attack on the pairwise ranking algorithms, which can be generally formalized as the dynamic and static games between the ranker and the attacker, and can be modeled as certain kinds of integer programming problems mathematically. To break the computational hurdle of the underlying integer programming problems, we reformulate them into the distributionally robust optimization (DRO) problems, which are computational tractable. Based on such DRO formulations, we propose two efficient poisoning attack algorithms and establish the associated theoretical guarantees including the existence of Nash equilibrium and the generalization ability bounds. The effectiveness of the suggested poisoning attack strategies is demonstrated by a series of toy simulations and several real data experiments. These experimental results show that the proposed methods can significantly reduce the performance of the ranker in the sense that the correlation between the true ranking list and the aggregated results with toxic data can be decreased dramatically.	[Ma, Ke; Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 100049, Peoples R China; [Ma, Ke; Huang, Qingming] Peng Cheng Lab, Artificial Intelligence Res Ctr, Shenzhen 518055, Guangdong, Peoples R China; [Xu, Qianqian; Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China; [Zeng, Jinshan] Jiangxi Normal Univ, Sch Comp & Informat Engn, Nanchang 330022, Jiangxi, Peoples R China; [Cao, Xiaochun] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur SKLOIS, Beijing 100093, Peoples R China; [Cao, Xiaochun] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China; [Huang, Qingming] Univ Chinese Acad Sci, Sch Econ & Management, Key Lab Big Data Min & Knowledge Management, Beijing 100049, Peoples R China	Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Peng Cheng Laboratory; Chinese Academy of Sciences; Institute of Computing Technology, CAS; Jiangxi Normal University; Chinese Academy of Sciences; Institute of Information Engineering, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Xu, QQ; Huang, QM (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.	make@ucas.ac.cn; xuqianqian@ict.ac.cn; jinshanzeng@jxnu.edu.cn; caoxiaochun@iie.ac.cn; qmhuang@ucas.ac.cn	Ma, Ke/AAG-1467-2020	Ma, Ke/0000-0003-4178-0907	National Key R&D Program of China [2018AAA0102003]; National Natural Science Foundation of China [61931008, 62025604, U1936208, 61620106009, 61836002, 61977038, 61976202, 62006217]; Thousand Talents Plan of Jiangxi Province [jxsq2019201124]; Youth Innovation Promotion Association CAS; Strategic Priority Research Program of Chinese Academy of Sciences [XDB28000000]; China Postdoctoral Science Foundation [2021T140653, 2020M680651]; Fundamental Research Funds for Central Universities	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Thousand Talents Plan of Jiangxi Province; Youth Innovation Promotion Association CAS; Strategic Priority Research Program of Chinese Academy of Sciences(Chinese Academy of Sciences); China Postdoctoral Science Foundation(China Postdoctoral Science Foundation); Fundamental Research Funds for Central Universities(Fundamental Research Funds for the Central Universities)	This work was supported in part by the National Key R&D Program of China under Grant 2018AAA0102003, in part by the National Natural Science Foundation of China under Grants 61931008, 62025604, U1936208, 61620106009, 61836002, 61977038, 61976202, and 62006217, in part by the Thousand Talents Plan of Jiangxi Province under Grant jxsq2019201124, in part by the Youth Innovation Promotion Association CAS, in part by the Strategic Priority Research Program of Chinese Academy of Sciences under Grant XDB28000000, in part by China Postdoctoral Science Foundation under Grant 2021T140653 and 2020M680651, and in part by the Fundamental Research Funds for Central Universities.	Aghassi M, 2006, MATH PROGRAM, V107, P231, DOI 10.1007/s10107-005-0686-0; Ambrosio L, 2008, LECT MATH, P1; Arrow K. J., 2012, SOCIAL CHOICE INDIVI, V3rd; Bank B., 1982, NONLINEAR PARAMETRIC, DOI [10.1007/978-3-0348-6328-5, DOI 10.1007/978-3-0348-6328-5]; Bard J.F., 2013, PRACTICAL BILEVEL OP, V30, DOI [10.1007/978-1-4757-2836-1, DOI 10.1007/978-1-4757-2836-1]; BARD JF, 1991, J OPTIMIZ THEORY APP, V68, P371, DOI 10.1007/BF00941574; Bayraksan G., 2010, TUTORIALS OPERATIONS, P1; Ben-Tal A, 2013, MANAGE SCI, V59, P341, DOI 10.1287/mnsc.1120.1641; Biggio B., 2012, 29 INT C MACH LEARN, P1807; Blanchet J, 2019, MATH OPER RES, V44, P565, DOI 10.1287/moor.2018.0936; Boyd S, 2004, CONVEX OPTIMIZATION; BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029; Chen YD, 2020, AAAI CONF ARTIF INTE, V34, P3545; CRITCHLOW DE, 1991, J MATH PSYCHOL, V35, P294, DOI 10.1016/0022-2496(91)90050-4; David H.A, 1963, METHOD PAIRED COMP; DEBREU G, 1960, AM ECON REV, V50, P186; Duchi J., 2008, PROC 25 INT C MACH L, P272; Duchi J, 2019, J MACH LEARN RES, V20; DUDLEY R.M, 1967, J FUNCT ANAL, V1, P290, DOI DOI 10.1016/0022-1236(67)90017-1; Erdogan E, 2006, MATH PROGRAM, V107, P37, DOI 10.1007/s10107-005-0678-0; Esfahani PM, 2018, MATH PROGRAM, V171, P115, DOI 10.1007/s10107-017-1172-1; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Grasmair M, 2011, COMMUN PUR APPL MATH, V64, P161, DOI 10.1002/cpa.20350; HARSANYI JC, 1968, MANAGE SCI, V14, P486, DOI 10.1287/mnsc.14.7.486; HARSANYI JC, 1968, MANAGE SCI, V14, P320, DOI 10.1287/mnsc.14.5.320; Harsanyi John, 1967, MANAGE SCI, V50, P159, DOI [10.1287/mnsc.14.3.159, DOI 10.1287/MNSC.14.3.159]; Hiriart-Urruty J-B., 2013, CONVEX ANAL MINIMIZA; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Jagielski M, 2018, P IEEE S SECUR PRIV, P19, DOI 10.1109/SP.2018.00057; Jiang RW, 2016, MATH PROGRAM, V158, P291, DOI 10.1007/s10107-015-0929-7; Jiang Weiwen, 2019, P 56 ANN DES AUT C, P5, DOI DOI 10.1109/ICC.2019.8761422; Jiang XY, 2011, MATH PROGRAM, V127, P203, DOI 10.1007/s10107-010-0419-x; Kakutani S., 1941, DUKE MATH J, V8, P457, DOI [10.1215/s0012-7094-41-00838-4, DOI 10.1215/S0012-7094-41-00838-4]; Karp RM., 1972, COMPLEXITY COMPUTER, P85; Kolde R, 2012, BIOINFORMATICS, V28, P573, DOI 10.1093/bioinformatics/btr709; Koltchinskii V, 2002, ANN STAT, V30, P1; Korba A, 2017, PR MACH LEARN RES, V54, P1001; Lee Jaeho, 2018, ADV NEURAL INFORM PR, P2692; Liese F, 2006, IEEE T INFORM THEORY, V52, P4394, DOI 10.1109/TIT.2006.881731; Liu Fang, 2019, INT C MACH LEARN, P4042; Liu XQ, 2019, ADV NEUR IN, V32; Liu YC, 2018, EUR J OPER RES, V265, P631, DOI 10.1016/j.ejor.2017.07.050; Loizou N., 2016, P INT C OP RES ENT S, P186; Ma YZ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4732; Ma YZ, 2019, ADV NEUR IN, V32; Mattei Nicholas, 2013, Algorithmic Decision Theory. Third International Conference, ADT 2013. Proceedings: LNCS 8176, P259, DOI 10.1007/978-3-642-41575-3_20; MONGE G., 1781, HIST ACAD ROYALE SCI; Namkoong H, 2017, ADV NEURAL INFORM PR, V30, P2971; Namkoong H, 2016, ADV NEUR IN, V29; NASH J, 1951, ANN MATH, V54, P286, DOI 10.2307/1969529; NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48; Negahban S, 2018, J MACH LEARN RES, V19; Negahban S, 2017, OPER RES, V65, P266, DOI 10.1287/opre.2016.1534; Olsder T., 1999, DYNAMIC NONCOOPERATI; Pananjady A, 2020, ANN STAT, V48, P1072, DOI 10.1214/19-AOS1838; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Rajkumar A, 2015, PR MACH LEARN RES, V37, P665; ROSEN JB, 1965, ECONOMETRICA, V33, P520, DOI 10.2307/1911749; Saeed Mahloujifar, 2019, INT C MACHINE LEARNI, P4274; Shah N., 2016, INT C MACHINE LEARNI, P11; Shah NB, 2018, J MACH LEARN RES, V18; Shah NB, 2016, J MACH LEARN RES, V17, P1; Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Vaish R, 2016, AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P358; Van der Vaart A.W., 1996, WEAK CONVERGENCE EMP; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Von Neuman J., 1944, THEORY GAMES EC BEHA; Walsh T, 2011, ANN MATH ARTIF INTEL, V62, P7, DOI 10.1007/s10472-011-9255-9; Wang ZZ, 2016, COMPUT MANAG SCI, V13, P241, DOI 10.1007/s10287-015-0240-3; Wauthier Fabian, 2013, INT C MACH LEARN, P109; Wozabal D, 2014, OPER RES, V62, P1302, DOI 10.1287/opre.2014.1323; Xu QQ, 2019, IEEE T PATTERN ANAL, V41, P844, DOI 10.1109/TPAMI.2018.2817205; Zhang X., 2020, P 37 INT C MACHINE L, P11225	79	1	1	2	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6393	6408		10.1109/TPAMI.2021.3087514	http://dx.doi.org/10.1109/TPAMI.2021.3087514			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34101586	Green Submitted			2022-12-18	WOS:000853875300040
J	Meng, JK; Zheng, WS; Lai, JH; Wang, L				Meng, Jingke; Zheng, Wei-Shi; Lai, Jian-Huang; Wang, Liang			Deep Graph Metric Learning for Weakly Supervised Person Re-Identification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Training; Cameras; Labeling; Probes; Visualization; Annotations; Loss measurement; Person re-identification; weakly supervised person re-identification; visual surveillance		In conventional person re-identification (re-id), the images used for model training in the training probe set and training gallery set are all assumed to be instance-level samples that are manually labeled from raw surveillance video (likely with the assistance of detection) in a frame-by-frame manner. This labeling across multiple non-overlapping camera views from raw video surveillance is expensive and time consuming. To overcome these issues, we consider a weakly supervised person re-id modeling that aims to find the raw video clips where a given target person appears. In our weakly supervised setting, during training, given a sample of a person captured in one camera view, our weakly supervised approach aims to train a re-id model without further instance-level labeling for this person in another camera view. The weak setting refers to matching a target person with an untrimmed gallery video where we only know that the identity appears in the video without the requirement of annotating the identity in any frame of the video during the training procedure. The weakly supervised person re-id is challenging since it not only suffers from the difficulties occurring in conventional person re-id (e.g., visual ambiguity and appearance variations caused by occlusions, pose variations, background clutter, etc.), but more importantly, is also challenged by weakly supervised information because the instance-level labels and the ground-truth locations for person instances (i.e., the ground-truth bounding boxes of person instances) are absent. To solve the weakly supervised person re-id problem, we develop deep graph metric learning (DGML). On the one hand, DGML measures the consistency between intra-video spatial graphs of consecutive frames, where the spatial graph captures neighborhood relationship about the detected person instances in each frame. On the other hand, DGML distinguishes the inter-video spatial graphs captured from different camera views at different sites simultaneously. To further explicitly embed weak supervision into the DGML and solve the weakly supervised person re-id problem, we introduce weakly supervised regularization (WSR), which utilizes multiple weak video-level labels to learn discriminative features by means of a weak identity loss and a cross-video alignment loss. We conduct extensive experiments to demonstrate the feasibility of the weakly supervised person re-id approach and its special cases (e.g., its bag-to-bag extension) and show that the proposed DGML is effective.	[Meng, Jingke; Lai, Jian-Huang] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 519082, Peoples R China; [Meng, Jingke] Pazhou Lab, Guangzhou 519082, Peoples R China; [Zheng, Wei-Shi] Sun Yat Sen Univ, Sch Comp Sci & Engn, Key Lab Machine Intelligence & Adv Comp, Minist Educ, Guangzhou 519082, Peoples R China; [Zheng, Wei-Shi] Peng Cheng Lab, Shenzhen 518066, Peoples R China; [Wang, Liang] Chinese Acad Sci, Inst Automat, Beijing 100049, Peoples R China	Sun Yat Sen University; Pazhou Lab; Sun Yat Sen University; Peng Cheng Laboratory; Chinese Academy of Sciences; Institute of Automation, CAS	Zheng, WS (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Key Lab Machine Intelligence & Adv Comp, Minist Educ, Guangzhou 519082, Peoples R China.	mengjke@mail2.sysu.edu.cn; wszheng@ieee.org; stsljh@mail.sysu.edu.cn; wangliang@nlpr.ia.ac.cn			National Key Research andDevelopment Program of China [2016YFB1001002]; NSFC [U1911401, U1811461, U1803120]; Guangdong NSF Project [2020B1515120085, 2018B030312002]; Research Projects of Zhejiang Lab [2019KD0AB03]; Key-Area Research and Development Program of Guangzhou [202007030004]; Guangzhou Research Project [201902010037]	National Key Research andDevelopment Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Guangdong NSF Project; Research Projects of Zhejiang Lab; Key-Area Research and Development Program of Guangzhou; Guangzhou Research Project	This work was supported in part by theNational Key Research andDevelopment Program of China (2016YFB1001002), in part by the NSFC (U1911401, U1811461, U1803120), Guangdong NSF Project (No. 2020B1515120085, 2018B030312002), Guangzhou Research Project (201902010037), and Research Projects of Zhejiang Lab (No. 2019KD0AB03), and the Key-Area Research and Development Program of Guangzhou (202007030004). The principal investigator for this paper isWei-Shi Zheng.	Chen DP, 2018, PROC CVPR IEEE, P8649, DOI 10.1109/CVPR.2018.00902; Chen GY, 2019, IEEE I CONF COMP VIS, P9546, DOI 10.1109/ICCV.2019.00964; Chen GY, 2019, IEEE T IMAGE PROCESS, V28, P4192, DOI 10.1109/TIP.2019.2908062; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110; Di Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12612, DOI 10.1109/CVPR42600.2020.01263; Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316; Feng J, 2017, AAAI CONF ARTIF INTE, P1884; Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621; Geng X, 2019, AAAI CONF ARTIF INTE, P3656; Guo HF, 2018, INT J PATTERN RECOGN, V32, DOI 10.1142/S021800141859005X; He K., 2017, IEEE INT C COMP VIS, P2961; Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9; Hou RB, 2019, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR.2019.00735; Hu JL, 2018, IEEE T CIRC SYST VID, V28, P1875, DOI 10.1109/TCSVT.2017.2691801; Huang SJ, 2019, IEEE T PATTERN ANAL, V41, P2614, DOI 10.1109/TPAMI.2018.2861732; Huang SJ, 2014, AAAI CONF ARTIF INTE, P1868; Hung-Min Hsu, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P964, DOI 10.1145/3394171.3413863; Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975; Kim J, 2019, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2019.00010; Li S, 2018, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2018.00046; Li X, 2018, LECT NOTES COMPUT SC, V11206, P287, DOI [10.1007/978-3-030-01216-8_18, 10.1007/978-3-030-01267-0_22]; Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345; Lin YT, 2019, AAAI CONF ARTIF INTE, P8738; Liu Y., 2019, PROC 7 INT C LEARN R; Lv JM, 2018, PROC CVPR IEEE, P7948, DOI 10.1109/CVPR.2018.00829; Meng JK, 2019, PROC CVPR IEEE, P760, DOI 10.1109/CVPR.2019.00085; Meng JK, 2019, PATTERN RECOGN, V93, P430, DOI 10.1016/j.patcog.2019.04.008; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Ruibing Hou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P388, DOI 10.1007/978-3-030-58595-2_24; Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30; Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810; Song LC, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107173; Song LY, 2018, IEEE T IMAGE PROCESS, V27, P6025, DOI 10.1109/TIP.2018.2864920; Subramaniam A, 2019, IEEE I CONF COMP VIS, P562, DOI 10.1109/ICCV.2019.00065; Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25; Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048; Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30; Wang GC, 2019, AAAI CONF ARTIF INTE, P8933; Wang GR, 2021, IEEE T NEUR NET LEAR, V32, P2142, DOI 10.1109/TNNLS.2020.2999517; Wang ML, 2021, AAAI CONF ARTIF INTE, V35, P2764; Wang R, 2021, IEEE T MULTIMEDIA, V23, P228, DOI 10.1109/TMM.2020.2981189; Wang TQ, 2016, IEEE T PATTERN ANAL, V38, P2501, DOI 10.1109/TPAMI.2016.2522418; Wang TQ, 2014, LECT NOTES COMPUT SC, V8692, P688, DOI 10.1007/978-3-319-10593-2_45; Wang XP, 2021, IEEE T CIRC SYST VID, V31, P4020, DOI 10.1109/TCSVT.2020.3043444; Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1437, DOI 10.1145/3343031.3351034; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Wu GL, 2020, AAAI CONF ARTIF INTE, V34, P12362; Wu JL, 2019, IEEE I CONF COMP VIS, P8320, DOI 10.1109/ICCV.2019.00841; Wu Y, 2018, AAAI CONF ARTIF INTE, P7412; Wu Y, 2018, PROC CVPR IEEE, P5177, DOI 10.1109/CVPR.2018.00543; Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393; Xiao T., 2016, ARXIV; Xing YY, 2019, AAAI CONF ARTIF INTE, P5508; Xinqian Gu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P228, DOI 10.1007/978-3-030-58536-5_14; Xu SJ, 2017, IEEE I CONF COMP VIS, P4743, DOI 10.1109/ICCV.2017.507; Yan YC, 2020, PROC CVPR IEEE, P2896, DOI 10.1109/CVPR42600.2020.00297; Yan YC, 2019, PROC CVPR IEEE, P2153, DOI 10.1109/CVPR.2019.00226; Yang C, 2020, INT CONF ACOUST SPEE, P5530, DOI 10.1109/ICASSP40776.2020.9053552; Yang QZ, 2019, PROC CVPR IEEE, P3628, DOI 10.1109/CVPR.2019.00375; Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775; Ye M, 2020, PROC CVPR IEEE, P5456, DOI 10.1109/CVPR42600.2020.00550; Ye M, 2020, IEEE T INF FOREN SEC, V15, P2655, DOI 10.1109/TIFS.2020.2970590; Ye M, 2019, IEEE T IMAGE PROCESS, V28, P2976, DOI 10.1109/TIP.2019.2893066; Ye M, 2018, LECT NOTES COMPUT SC, V11211, P176, DOI 10.1007/978-3-030-01234-2_11; Ye M, 2017, IEEE I CONF COMP VIS, P5152, DOI 10.1109/ICCV.2017.550; Yoshida T., MACH LEARN, P2021; You JJ, 2016, PROC CVPR IEEE, P1345, DOI 10.1109/CVPR.2016.150; Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225; Yu HX, 2020, IEEE T PATTERN ANAL, V42, P956, DOI 10.1109/TPAMI.2018.2886878; Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113; Zhang RM, 2019, IEEE T IMAGE PROCESS, V28, P4870, DOI 10.1109/TIP.2019.2911488; Zhang XK, 2020, IEEE T NEUR NET LEAR, V31, P3047, DOI 10.1109/TNNLS.2019.2935173; Zhao YR, 2019, PROC CVPR IEEE, P4908, DOI 10.1109/CVPR.2019.00505; Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414; Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52; Zheng WS, 2016, IEEE T PATTERN ANAL, V38, P591, DOI 10.1109/TPAMI.2015.2453984; Zheng WZ, 2019, PROC CVPR IEEE, P72, DOI 10.1109/CVPR.2019.00016; Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI [10.1109/CVPR.2019.00224, 10.1109/CVPR.2019.01247]; Zhou ZH, 2012, ARTIF INTELL, V176, P2291, DOI 10.1016/j.artint.2011.10.002; Zhu X, 2017, ARXIV PREPRINT ARXIV, P1	85	1	1	20	20	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6074	6093		10.1109/TPAMI.2021.3084613	http://dx.doi.org/10.1109/TPAMI.2021.3084613			20	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34048336				2022-12-18	WOS:000853875300019
J	Qi, L; Wang, Y; Chen, YK; Chen, YC; Zhang, XY; Sun, J; Jia, JY				Qi, Lu; Wang, Yi; Chen, Yukang; Chen, Ying-Cong; Zhang, Xiangyu; Sun, Jian; Jia, Jiaya			PointINS: Point-Based Instance Segmentation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature extraction; Detectors; Convolution; Image segmentation; Semantics; Training; Object detection; Instance segmentation; single-point feature		In this paper, we explore the mask representation in instance segmentation with Point-of-Interest (PoI) features. Differentiating multiple potential instances within a single PoI feature is challenging, because learning a high-dimensional mask feature for each instance using vanilla convolution demands a heavy computing burden. To address this challenge, we propose an instance-aware convolution. It decomposes this mask representation learning task into two tractable modules as instance-aware weights and instance-agnostic features. The former is to parametrize convolution for producing mask features corresponding to different instances, improving mask learning efficiency by avoiding employing several independent convolutions. Meanwhile, the latter serves as mask templates in a single point. Together, instance-aware mask features are computed by convolving the template with dynamic weights, used for the mask prediction. Along with instance-aware convolution, we propose PointINS, a simple and practical instance segmentation approach, building upon dense one-stage detectors. Through extensive experiments, we evaluated the effectiveness of our framework built upon RetinaNet and FCOS. PointINS in ResNet101 backbone achieves a 38.3 mask mean average precision (mAP) on COCO dataset, outperforming existing point-based methods by a large margin. It gives a comparable performance to the region-based Mask R-CNN K. He, G. Gkioxari, P. Dollar, and R. Girshick, "Mask R-CNN," in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 2980-2988 with faster inference.	[Qi, Lu; Wang, Yi; Chen, Yukang; Jia, Jiaya] Chinese Univ Hong, Dept Comp Sci & Engn, Hong Kong, Peoples R China; [Chen, Ying-Cong] MIT, Dept Comp Sci, Cambridge, MA 02139 USA; [Chen, Ying-Cong] MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA; [Zhang, Xiangyu; Sun, Jian] MEGVII Technol, Beijing 100191, Peoples R China	Chinese University of Hong Kong; Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Zhang, XY (corresponding author), MEGVII Technol, Beijing 100191, Peoples R China.	luqi@cse.cnhk.edu.hk; yiwang@cse.cnhk.edu.hk; yukangchen@cse.cnhk.edu.hk; yingcong.ian.chen@gmail.com; xiangyu.zhang@megvii.com; sunjian@megvii.com; leojia@cse.cnhk.edu.hk	Chen, Ying-Cong/ABE-3123-2020	Chen, Ying-Cong/0000-0002-9565-8205	National Key Research and Development Program of China [2020AAA0105200]; Beijing Academy of Artificial Intelligence (BAAI)	National Key Research and Development Program of China; Beijing Academy of Artificial Intelligence (BAAI)	This work was supported by the National Key Research and Development Program of China (No. 2020AAA0105200) and Beijing Academy of Artificial Intelligence (BAAI). A part of this work was done during the internship of the authors Lu Qi, Yi Wang, and Yukang Chen in MEGVII Technology and they contributed equally to thiswork.	Alexander C. Berg, 2019, Arxiv, DOI arXiv:1901.03353; Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Andrychowicz M, 2016, ADV NEUR IN, V29; Bai M, 2017, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2017.305; Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314; Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925; Cai Q, 2018, PROC CVPR IEEE, P4080, DOI 10.1109/CVPR.2018.00429; Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644; Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22; Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen XL, 2019, IEEE I CONF COMP VIS, P2061, DOI 10.1109/ICCV.2019.00215; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai JF, 2016, ADV NEUR IN, V29; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343; Dai JF, 2016, LECT NOTES COMPUT SC, V9910, P534, DOI 10.1007/978-3-319-46466-4_32; Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221; Girshick R., 2015, ICCV; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He YH, 2019, PROC CVPR IEEE, P2883, DOI 10.1109/CVPR.2019.00300; Hu R, 2018, PROC CVPR IEEE, P4233, DOI 10.1109/CVPR.2018.00445; Hu XC, 2019, PROC CVPR IEEE, P1575, DOI 10.1109/CVPR.2019.00167; Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657; Hui Ying, 2019, Arxiv, DOI arXiv:1912.01954; Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494; Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656; Kong T, 2018, LECT NOTES COMPUT SC, V11209, P172, DOI 10.1007/978-3-030-01228-1_11; Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98; Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z; Lee H, 2020, IEEE T IMAGE PROCESS, V29, P1030, DOI 10.1109/TIP.2019.2938879; Lee Y., 2020, P IEEECVF C COMP VIS, P13906; Lemke C, 2015, ARTIF INTELL REV, V44, P117, DOI 10.1007/s10462-013-9406-y; Li YH, 2019, IEEE I CONF COMP VIS, P6053, DOI 10.1109/ICCV.2019.00615; Li YW, 2019, PROC CVPR IEEE, P7019, DOI 10.1109/CVPR.2019.00719; Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913; Liu S, 2017, IEEE I CONF COMP VIS, P3516, DOI 10.1109/ICCV.2017.378; Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2; Liu Y, 2018, PROC CVPR IEEE, P6985, DOI 10.1109/CVPR.2018.00730; Long J., 2015, P IEEE C COMPUTER VI, P3431, DOI DOI 10.1109/CVPR.2015.7298965; Morrison D, 2018, IEEE INT CONF ROBOT, P7757; Najibi M, 2019, IEEE I CONF COMP VIS, P9744, DOI 10.1109/ICCV.2019.00984; Najibi M, 2016, PROC CVPR IEEE, P2369, DOI 10.1109/CVPR.2016.260; Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091; Qi L, 2019, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR.2019.00313; Qin Z, 2019, IEEE I CONF COMP VIS, P6717, DOI 10.1109/ICCV.2019.00682; Ravi Sachin, 2017, INT C LEARN REPR, V2, P5; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rufeng Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10223, DOI 10.1109/CVPR42600.2020.01024; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Shrivastava A, 2016, LECT NOTES COMPUT SC, V9905, P330, DOI 10.1007/978-3-319-46448-0_20; Shu G, 2014, HUMAN DETECTION TRAC; Sida Peng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8530, DOI 10.1109/CVPR42600.2020.00856; Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377; Tan ZY, 2019, IEEE I CONF COMP VIS, P8272, DOI 10.1109/ICCV.2019.00836; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Wang XL, 2017, PROC CVPR IEEE, P3039, DOI 10.1109/CVPR.2017.324; Wang YX, 2016, LECT NOTES COMPUT SC, V9910, P616, DOI 10.1007/978-3-319-46466-4_37; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xinlong Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P649, DOI 10.1007/978-3-030-58523-5_38; Yang B, 2019, ADV NEUR IN, V32; Yang T, 2018, ADV NEUR IN, V31; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094; Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953	73	1	1	5	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6377	6392		10.1109/TPAMI.2021.3085295	http://dx.doi.org/10.1109/TPAMI.2021.3085295			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34061733	Green Submitted			2022-12-18	WOS:000853875300039
J	Shang, P; Kong, LC				Shang, Pan; Kong, Lingchen			l(1)-Norm Quantile Regression Screening Rule via the Dual Circumscribed Sphere	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						l(1)-Norm quantile regression; screening rule; dual circumscribed sphere; computational efficiency	COORDINATE DESCENT ALGORITHM; GENE-EXPRESSION; LASSO; PREDICTION; TUMOR	l(1)-norm quantile regression is a common choice if there exists outlier or heavy-tailed error in high-dimensional data sets. However, it is computationally expensive to solve this problem when the feature size of data is ultra high. As far as we know, existing screening rules can not speed up the computation of the l(1)-norm quantile regression, which dues to the non-differentiability of the quantile function/pinball loss. In this paper, we introduce the dual circumscribed sphere technique and propose a novel l(1)-norm quantile regression screening rule. Our rule is expressed as the closed-form function of given data and eliminates inactive features with a low computational cost. Numerical experiments on some simulation and real data sets show that this screening rule can be used to eliminate almost all inactive features. Moreover, this rule can help to reduce up to 23 times of computational time, compared with the computation without our screening rule.	[Shang, Pan; Kong, Lingchen] Beijing Jiaotong Univ, Dept Appl Math, Beijing 100044, Peoples R China	Beijing Jiaotong University	Shang, P (corresponding author), Beijing Jiaotong Univ, Dept Appl Math, Beijing 100044, Peoples R China.	18118019@bjtu.edu.cn; konglchen@126.com			National Natural Science Foundation of China [12071022]; Beijing Natural Science Foundation [Z190002]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation)	The authors would like to sincerely thank the referees as well as the associate editor for their constructive comments, which have significantly improved the quality of the paper. This work was supported by the National Natural Science Foundation of China (12071022) and Beijing Natural Science Foundation (Z190002).	Alizadeh AA, 2000, NATURE, V403, P503, DOI 10.1038/35000501; Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; Beck A., 2017, MOS SIAM SER OPTIM, DOI DOI 10.1137/1.9781611974997; Belloni A, 2011, ANN STAT, V39, P82, DOI 10.1214/10-AOS827; Bradley E., 2010, LARGE SCALE INFERENC; Chen HY, 2020, APPL MATH COMPUT, V386, DOI 10.1016/j.amc.2020.125500; Christmann A., 2007, ADV NEURAL INFORM PR, P305; El Ghaoui L, 2012, PAC J OPTIM, V8, P667; Fan JQ, 2008, J R STAT SOC B, V70, P849, DOI 10.1111/j.1467-9868.2008.00674.x; Fan JQ, 2017, J R STAT SOC B, V79, P247, DOI 10.1111/rssb.12166; Fan JQ, 2014, ANN STAT, V42, P324, DOI 10.1214/13-AOS1191; Fazel M, 2013, SIAM J MATRIX ANAL A, V34, P946, DOI 10.1137/110853996; Gu YW, 2018, TECHNOMETRICS, V60, P319, DOI 10.1080/00401706.2017.1345703; Hiriart-Urruty J-B, 1993, CONVEX ANAL MINIMIZA, VII; Huang J, 2008, STAT SINICA, V18, P1603; Huang XL, 2014, IEEE T PATTERN ANAL, V36, P984, DOI 10.1109/TPAMI.2013.178; HUBER PJ, 1973, ANN STAT, V1, P799, DOI 10.1214/aos/1176342503; Jumutc V, 2013, IEEE IJCNN; Khan J, 2001, NAT MED, V7, P673, DOI 10.1038/89044; KOENKER R, 1978, ECONOMETRICA, V46, P33, DOI 10.2307/1913643; Koenker R, 2016, QUANTREG QUANTILE RE; Kuang Zhaobin, 2017, Adv Neural Inf Process Syst, V30, P720; Lee S, 2018, IEEE T PATTERN ANAL, V40, P2841, DOI 10.1109/TPAMI.2017.2765321; Li YJ, 2008, J COMPUT GRAPH STAT, V17, P163, DOI 10.1198/106186008X289155; Michael G., 2013, CVX MATLAB SOFTWARE; Mkhadri A, 2017, STAT COMPUT, V27, P865, DOI 10.1007/s11222-016-9659-9; Ndiaye E, 2017, J MACH LEARN RES, V18; Pan XL, 2022, IEEE T PATTERN ANAL, V44, P4544, DOI 10.1109/TPAMI.2021.3071138; Pan XL, 2019, IEEE T NEUR NET LEAR, V30, P2263, DOI 10.1109/TNNLS.2018.2879800; Peng B, 2015, J COMPUT GRAPH STAT, V24, P676, DOI 10.1080/10618600.2014.913516; Pomeroy SL, 2002, NATURE, V415, P436, DOI 10.1038/415436a; Ren SG, 2018, IEEE T PATTERN ANAL, V40, P2992, DOI 10.1109/TPAMI.2017.2776267; Rockafellar R. T., 1970, CONVEX ANAL; Roger Koenker, 2005, [Acta Mathematicae Applicatae Sinica, Ying yung shu hseh hseh pao], V21, P225; Scheetz TE, 2006, P NATL ACAD SCI USA, V103, P14429, DOI 10.1073/pnas.0602562103; Steinwart I, 2011, BERNOULLI, V17, P211, DOI 10.3150/10-BEJ267; Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x; Wang J, 2015, J MACH LEARN RES, V16, P1063; Wang J, 2015, IEEE T PATTERN ANAL, V37, P1806, DOI 10.1109/TPAMI.2014.2388203; Wu TT, 2008, ANN APPL STAT, V2, P224, DOI 10.1214/07-AOAS147; Xiang ZJ, 2017, IEEE T PATTERN ANAL, V39, P1008, DOI 10.1109/TPAMI.2016.2568185; Yi CR, 2017, J COMPUT GRAPH STAT, V26, P547, DOI 10.1080/10618600.2016.1256816	42	1	1	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6254	6263		10.1109/TPAMI.2021.3087160	http://dx.doi.org/10.1109/TPAMI.2021.3087160			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34101582				2022-12-18	WOS:000853875300031
J	Singh, M; Nagpal, S; Singh, R; Vatsa, M				Singh, Maneet; Nagpal, Shruti; Singh, Richa; Vatsa, Mayank			DeriveNet for (Very) Low Resolution Image Classification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Face recognition; Image resolution; Feature extraction; Image reconstruction; Training; Task analysis; Image recognition; Very low resolution classification; face recognition; digit classification	FACE RECOGNITION	Images captured from a distance often result in (very) low resolution (VLR/LR) region of interest, requiring automated identification. VLR/LR images (or regions of interest) often contain less information content, rendering ineffective feature extraction and classification. To this effect, this research proposes a novel DeriveNet model for VLR/LR classification, which focuses on learning effective class boundaries by utilizing the class-specific domain knowledge. DeriveNet model is jointly trained via two losses: (i) proposed Derived-Margin softmax loss and (ii) the proposed Reconstruction-Center (ReCent) loss. The Derived-Margin softmax loss focuses on learning an effective VLR classifier while explicitly modeling the inter-class variations. The ReCent loss incorporates domain information by learning a HR reconstruction space for approximating the class variations for the VLR/LR samples. It is utilized to derive inter-class margins for the Derived-Margin softmax loss. The DeriveNet model has been trained with a novel Multi-resolution Pyramid based data augmentation which enables the model to learn from varying resolutions during training. Experiments and analysis have been performed on multiple datasets for (i) VLR/LR face recognition, (ii) VLR digit classification, and (iii) VLR/LR face recognition from drone-shot videos. The DeriveNet model achieves state-of-the-art performance across different datasets, thus promoting its utility for several VLR/LR classification tasks.	[Singh, Maneet; Nagpal, Shruti] IIT Delhi, Dept Comp Sci & Engn, Delhi 110020, India; [Singh, Richa; Vatsa, Mayank] ITT Jodhpur, Dept Comp Sci & Engn, Jodhpur 342037, Rajasthan, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Delhi	Vatsa, M (corresponding author), ITT Jodhpur, Dept Comp Sci & Engn, Jodhpur 342037, Rajasthan, India.	maneets@iiitd.ac.in; shrutin@iiitd.ac.in; richa@iitj.ac.in; mvatsa@iitj.ac.in			TCS PhD Fellowship; Government of India	TCS PhD Fellowship; Government of India	The work of Shruti Nagpal was supported in part by TCS PhD Fellowship and the work of Mayank Vatsa was supported in part by the SwarnaJayanti Fellowship by the Government of India.	Aghdam OA, 2019, IEEE COMPUT SOC CONF, P2363, DOI 10.1109/CVPRW.2019.00290; Amato Giuseppe, 2020, SSIP 2020: 3rd International Conference on Sensors, Signal and Image Processing, P13, DOI 10.1145/3441233.3441237; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020; Carlos D. Castillo, 2017, Arxiv, DOI arXiv:1703.09507; Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482; Du DW, 2018, LECT NOTES COMPUT SC, V11214, P375, DOI 10.1007/978-3-030-01249-6_23; Ge SM, 2020, AAAI CONF ARTIF INTE, V34, P10845; Ge SM, 2020, IEEE T IMAGE PROCESS, V29, P6898, DOI 10.1109/TIP.2020.2995049; Ge SM, 2019, IEEE T IMAGE PROCESS, V28, P2051, DOI 10.1109/TIP.2018.2883743; Grgic M, 2011, MULTIMED TOOLS APPL, V51, P863, DOI 10.1007/s11042-009-0417-2; Grm K, 2020, IEEE T IMAGE PROCESS, V29, P2150, DOI 10.1109/TIP.2019.2945835; Hsu CC, 2019, IEEE T IMAGE PROCESS, V28, P6225, DOI 10.1109/TIP.2019.2924554; Huang G. B, 2014, TECH REP UM CS 2014; Kalra I, 2019, IEEE INT CONF AUTOMA, P207; Kazemi H., 2019, P IEEE 10 INT C BIOM, P1; Kumar SVA, 2021, IEEE T INF FOREN SEC, V16, P1696, DOI 10.1109/TIFS.2020.3040881; Li P, 2019, IEEE T INF FOREN SEC, V14, P2000, DOI 10.1109/TIFS.2018.2890812; Liu WY, 2016, PR MACH LEARN RES, V48; Lu Z, 2018, IEEE SIGNAL PROC LET, V25, P526, DOI 10.1109/LSP.2018.2810121; Massoli FV, 2020, IMAGE VISION COMPUT, V99, DOI 10.1016/j.imavis.2020.103927; McNemar Q, 1947, PSYCHOMETRIKA, V12, P153, DOI 10.1007/BF02295996; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Ouyang SX, 2016, IMAGE VISION COMPUT, V56, P28, DOI 10.1016/j.imavis.2016.09.001; Paszke A, 2017, PROC C NEURAL INF PR; Ryoo MS, 2018, AAAI CONF ARTIF INTE, P7315; Ryoo MS, 2017, AAAI CONF ARTIF INTE, P4255; Sapkota A., 2013, PROC IEEE INT C BIOM, P1; Shaogang Gong, 2018, Arxiv, DOI arXiv:1804.09691; Shi JG, 2015, IEEE SIGNAL PROC LET, V22, P554, DOI 10.1109/LSP.2014.2364262; Singh M, 2019, IEEE I CONF COMP VIS, P340, DOI 10.1109/ICCV.2019.00043; Singh M, 2018, IEEE COMPUT SOC CONF, P592, DOI 10.1109/CVPRW.2018.00089; Talreja V., 2019, 10 IEEE INT C BIOM T, P1; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552; Wang ZY, 2016, PROC CVPR IEEE, P4792, DOI 10.1109/CVPR.2016.518; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032; Yu X, 2020, IEEE T PATTERN ANAL, V42, P2926, DOI 10.1109/TPAMI.2019.2916881; Yue LW, 2016, SIGNAL PROCESS, V128, P389, DOI 10.1016/j.sigpro.2016.05.002; Zhang KP, 2018, LECT NOTES COMPUT SC, V11215, P196, DOI 10.1007/978-3-030-01252-6_12; Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139; Zhao SW, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P148, DOI [10.1109/BigMM.2019.00031, 10.1109/BigMM.2019.00-31]; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhu Pengfei, 2020, ARXIV; Zou WWW, 2012, IEEE T IMAGE PROCESS, V21, P327, DOI 10.1109/TIP.2011.2162423	47	1	1	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6569	6577		10.1109/TPAMI.2021.3088756	http://dx.doi.org/10.1109/TPAMI.2021.3088756			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34115585				2022-12-18	WOS:000853875300051
J	Song, HJ; Chang, LB; Chen, ZW; Ren, P				Song, Huajun; Chang, Laibin; Chen, Ziwei; Ren, Peng			Enhancement-Registration-Homogenization (ERH): A Comprehensive Underwater Visual Reconstruction Paradigm	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image registration; multi-scale composition; underwater image reconstruction	IMAGE; MOSAICKING	This paper presents a comprehensive underwater visual reconstruction paradigm that comprises three procedures, i.e., the E-procedure, the R-procedure, and the H-procedure. The E-procedure enhances original underwater images based on color compensation balance and weighted image fusion, yielding restored color, sharpened edges, and global contrast. The R-procedure registers multiple enhanced underwater images by exploiting global similarity and local deformation. The H-procedure homogenizes the registered underwater images by multi-scale composition strategy, which eliminates the inhomogeneous transition and brightness difference across overlapping regions, resulting in a reconstructed wide-field underwater image with comfortable and natural visibility. The three procedures operate in a cascade where the former procedure processes underwater images in a way that facilitates the latter one. We refer to the overall three procedures as the Enhancement-Registration-Homogenization (ERH) paradigm. Comprehensive qualitative and quantitative empirical evaluations reveal that our ERH paradigm outperforms state-of-the-art visual reconstruction methods, including the AutoStitch, APAP, SPHP, APNAP, and REW.	[Song, Huajun; Chang, Laibin; Chen, Ziwei; Ren, Peng] China Univ Petr East China, Coll Oceanog & Space Informat, Qingdao 266580, Shandong, Peoples R China	China University of Petroleum	Ren, P (corresponding author), China Univ Petr East China, Coll Oceanog & Space Informat, Qingdao 266580, Shandong, Peoples R China.	huajun.song@upc.edu.cn; changlb666@163.com; ziwenchen_upc@163.com; pengren@upc.edu.cn			National Natural Science Foundation of China [61971444]; Natural Science Foundation of Shandong Province [ZR2020MD034]; Key Program of Marine Economy Development Special Foundation of Department of Natural Resources of Guangdong Province [GDNRC [2020]012]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Shandong Province(Natural Science Foundation of Shandong Province); Key Program of Marine Economy Development Special Foundation of Department of Natural Resources of Guangdong Province	The authors would like to thank the reviewers and the associate editor for numerous suggestions and comments that largely improved our manuscript. This work was supported by the National Natural Science Foundation of China under Grant 61971444, and the Natural Science Foundation of Shandong Province under Grant ZR2020MD034, and the Key Program of Marine Economy Development Special Foundation of Department of Natural Resources of Guangdong Province under Grant GDNRC [2020]012.	Abu A, 2019, IEEE J OCEANIC ENG, V44, P1179, DOI 10.1109/JOE.2018.2863961; Ahn J, 2020, IEEE J OCEANIC ENG, V45, P350, DOI 10.1109/JOE.2018.2872500; Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252; Ancuti CO, 2013, IEEE T IMAGE PROCESS, V22, P3271, DOI 10.1109/TIP.2013.2262284; Berman D, 2021, IEEE T PATTERN ANAL, V43, P2822, DOI 10.1109/TPAMI.2020.2977624; Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3; Chang CH, 2014, PROC CVPR IEEE, P3254, DOI 10.1109/CVPR.2014.422; Chang HH, 2019, IEEE J OCEANIC ENG, V44, P1130, DOI 10.1109/JOE.2018.2865045; Chen MM, 2015, OCEANS 2015 - GENOVA, DOI 10.1109/OCEANS-Genova.2015.7271744; Codruta AO, 2020, IEEE T IMAGE PROCESS, V29, P2653, DOI 10.1109/TIP.2019.2951304; Drews P, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P825, DOI 10.1109/ICCVW.2013.113; Fang FM, 2019, IEEE GEOSCI REMOTE S, V16, P1115, DOI 10.1109/LGRS.2019.2893210; Fang X., 2019, IEEE ACCESS, V7, p125 300; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006; Gao Y, 2017, PROCEEDINGS OF 2017 3RD IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS (ICCC), P1873; He DM, 2004, OPT LASER ENG, V41, P217, DOI 10.1016/S0143-8166(02)00138-0; He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168; Igarashi Takeo, 2009, Journal of Graphics Tools, V14, P17; Khan A, 2016, 2016 IEEE 6TH INTERNATIONAL CONFERENCE ON UNDERWATER SYSTEM TECHNOLOGY: THEORY AND APPLICATIONS, P83, DOI 10.1109/USYS.2016.7893927; Kuo MYJ, 2021, IEEE T PATTERN ANAL, V43, P2220, DOI 10.1109/TPAMI.2021.3075450; Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241; Li J, 2018, IEEE T MULTIMEDIA, V20, P1672, DOI 10.1109/TMM.2017.2777461; Li Mingjie, 2015, 2015 International Conference on Intelligent Transportation, Big Data and Smart City (ICITBS). Proceedings, P959, DOI 10.1109/ICITBS.2015.243; Lin CC, 2015, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2015.7298719; Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Ma XM, 2019, J ELECTRON IMAGING, V28, DOI 10.1117/1.JEI.28.5.053033; Madhusudana PC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2921858; Mahmood A, 2019, IEEE J OCEANIC ENG, V44, P121, DOI 10.1109/JOE.2017.2786878; MARKS RL, 1995, IEEE J OCEANIC ENG, V20, P229, DOI 10.1109/48.393078; Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915; Qu Z, 2018, IET IMAGE PROCESS, V12, P1361, DOI 10.1049/iet-ipr.2017.1064; Rajendran R., 2017, PROC IEEE INT S TECH, P1; Raut S, 2017, 2017 2ND IEEE INTERNATIONAL CONFERENCE ON RECENT TRENDS IN ELECTRONICS, INFORMATION & COMMUNICATION TECHNOLOGY (RTEICT), P1237; Richmond K, 2006, OCEANS-IEEE, P1245; Shuang D, 2015, 2015 12TH INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY (FSKD), P1883, DOI 10.1109/FSKD.2015.7382234; Wang Y, 2018, COMPUT ELECTR ENG, V70, P904, DOI 10.1016/j.compeleceng.2017.12.006; Wang Y, 2019, OCEANS-IEEE; Wencheng Wang, 2011, Journal of Computers, V6, P2559, DOI 10.4304/jcp.6.12.2559-2566; Xu C., 2014, PROC OCEANS 2014 TAI, P1; Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020; Yuan YT, 2021, IEEE T GEOSCI REMOTE, V59, P1565, DOI 10.1109/TGRS.2020.2999404; Zaragoza J, 2014, IEEE T PATTERN ANAL, V36, P1285, DOI 10.1109/TPAMI.2013.247; Zhang F, 2014, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2014.423	45	1	1	13	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6953	6967		10.1109/TPAMI.2021.3097804	http://dx.doi.org/10.1109/TPAMI.2021.3097804			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34280089				2022-12-18	WOS:000853875300078
J	Tian, YS; Shen, L; Su, GN; Li, ZF; Liu, W				Tian, Yuesong; Shen, Li; Su, Guinan; Li, Zhifeng; Liu, Wei			AlphaGAN: Fully Differentiable Architecture Search for Generative Adversarial Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Generative adversarial networks; neural architecture search; generative models		Generative Adversarial Networks (GANs) are formulated as minimax game problems that generative networks attempt to approach real data distributions by adversarial learning against discriminators which learn to distinguish generated samples from real ones, of which the intrinsic problem complexity poses challenges to performance and robustness. In this work, we aim to boost model learning from the perspective of network architectures, by incorporating recent progress on automated architecture search into GANs. Specially we propose a fully differentiable search framework, dubbed alphaGAN, where the searching process is formalized as solving a bi-level minimax optimization problem. The outer-level objective aims for seeking an optimal network architecture towards pure Nash Equilibrium conditioned on the network parameters of generators and discriminators optimized with a traditional adversarial loss within inner level. The entire optimization performs a first-order approach by alternately minimizing the two-level objective in a fully differentiable manner that enables obtaining a suitable architecture efficiently from an enormous search space. Extensive experiments on CIFAR-10 and STL-10 datasets show that our algorithm can obtain high-performing architectures only with 3-GPU hours on a single GPU in the search space comprised of approximate 2 x 10(11) possible configurations. We further validate the method on the state-of-the-art network StyleGAN2, and push the score of Frechet Inception Distance (FID) further, i.e., achieving 1.94 on CelebA, 2.86 on LSUN-church and 2.75 on FFHQ, with relative improvements 3%similar to 26% over the baseline architecture. We also provide a comprehensive analysis of the behavior of the searching process and the properties of searched architectures, which would benefit further research on architectures for generative models. Codes and models are available at https://github.com/yuesongtian/AlphaGAN.	[Tian, Yuesong] Zhejiang Univ, Coll Biomed Engn & Instrument Sci, Hangzhou 310027, Zhejiang, Peoples R China; [Shen, Li] JD Explore Acad, Beijing 100101, Peoples R China; [Shen, Li] Tencent AI Lab, Shenzhen 518000, Guangdong, Peoples R China; [Su, Guinan] Microsoft, Beijing 100102, Peoples R China; [Li, Zhifeng; Liu, Wei] Tencent Data Platform, Shenzhen 518000, Guangdong, Peoples R China	Zhejiang University; Tencent	Shen, L (corresponding author), JD Explore Acad, Beijing 100101, Peoples R China.; Liu, W (corresponding author), Tencent Data Platform, Shenzhen 518000, Guangdong, Peoples R China.	tianys163@gmail.com; mathshenli@gmail.com; guinansu33@gmail.com; michaelzfli@tencent.com; wl2223@columbia.edu						Alec Radford, 2016, Arxiv, DOI arXiv:1511.06434; Ameet Talwalkar, 2019, Arxiv, DOI arXiv:1902.07638; Andrew Brock, 2017, Arxiv, DOI arXiv:1708.05344; Andrew Brock, 2019, Arxiv, DOI arXiv:1809.11096; Anirudh Goyal, 2019, Arxiv, DOI arXiv:1910.13540; Arber Zela, 2020, Arxiv, DOI arXiv:1909.09656; Augustus Odena, 2020, Arxiv, DOI arXiv:2002.04724; Augustus Odena, 2020, Arxiv, DOI arXiv:1910.12027; Augustus Odena, 2019, Arxiv, DOI arXiv:1805.08318; Barret Zoph, 2018, Arxiv, DOI arXiv:1802.03268; Barret Zoph, 2017, Arxiv, DOI arXiv:1611.01578; Chen Gao, 2020, Arxiv, DOI arXiv:1912.02037; Chen YK, 2019, ADV NEUR IN, V32; Chintala S., 2017, ARXIV, DOI DOI 10.48550/ARXIV.1701.07875; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Dengxin Dai, 2020, Arxiv, DOI arXiv:2007.09180; Donahue J, 2019, ADV NEUR IN, V32; Du D.-Z., 2013, MINIMAX APPL; Edwin D. de Jong, 2017, Arxiv, DOI arXiv:1712.00679; Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grnarova P, 2019, ADV NEUR IN, V32; Hanchao Wang, 2019, Arxiv, DOI arXiv:1906.11080; Hanxiao Liu, 2019, Arxiv, DOI arXiv:1806.09055; He H., 2018, PROC INT C LEARN REP; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hensel M, 2017, ADV NEUR IN, V30; Ho J, 2016, ADV NEUR IN, V29; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Gulrajani I, 2017, ADV NEUR IN, V30; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jaakko Lehtinen, 2020, Arxiv, DOI arXiv:2006.06676; Jaakko Lehtinen, 2018, Arxiv, DOI arXiv:1710.10196; Ji Lin, 2020, Arxiv, DOI arXiv:2006.10738; Jian Tang, 2019, Arxiv, DOI arXiv:1903.09769; Jin C, 2020, PR MACH LEARN RES, V119; Karnewar Animesh, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7796, DOI 10.1109/CVPR42600.2020.00782; Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kingma D.P, P 3 INT C LEARNING R; Lee R., 2020, ARXIV; Li J., 2017, ARXIV; Lin CH, 2019, IEEE I CONF COMP VIS, P4511, DOI 10.1109/ICCV.2019.00461; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Liu Hanxiao, 2017, ARXIV; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Masanori Koyama, 2018, Arxiv, DOI arXiv:1802.05957; NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48; Osborne MJ, 1994, COURSE GAME THEORY; Park Taesung, 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-7_19, DOI 10.48550/ARXIV.2007.15651]; Peng C., 2020, DG GAN GAN DUALITY G; Pinto L, 2017, PR MACH LEARN RES, V70; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Salimans T, 2016, ADV NEUR IN, V29; Lin TY, 2021, Arxiv, DOI arXiv:1906.00331; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	66	1	1	6	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6752	6766		10.1109/TPAMI.2021.3099829	http://dx.doi.org/10.1109/TPAMI.2021.3099829			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34310290	Green Submitted			2022-12-18	WOS:000853875300066
J	Wang, HY; Qin, ZQ; Li, SY; Li, X				Wang, Huanyu; Qin, Zequn; Li, Songyuan; Li, Xi			CoDiNet: Path Distribution Modeling With Consistency and Diversity for Dynamic Routing	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Routing; Computational modeling; Computational efficiency; Training; Image color analysis; Recurrent neural networks; Predictive models; Routing space mapping; distribution of routing paths; the consistency regularization; the diversity regularization; dynamic routing	CONSTRUCTION	Dynamic routing networks, aimed at finding the best routing paths in the networks, have achieved significant improvements to neural networks in terms of accuracy and efficiency. In this paper, we see dynamic routing networks in a fresh light, formulating a routing method as a mapping from a sample space to a routing space. From the perspective of space mapping, prevalent methods of dynamic routing did not take into account how routing paths would be distributed in the routing space. Thus, we propose a novel method, termed CoDiNet, to model the relationship between a sample space and a routing space by regularizing the distribution of routing paths with the properties of consistency and diversity. In principle, the routing paths for the self-supervised similar samples should be closely distributed in the routing space.Moreover, we design a customizable dynamic routing module, which can strike a balance between accuracy and efficiency. When deployed upon ResNet models, our method achieves higher performance and effectively reduces average computational cost on four widely used datasets.	[Wang, Huanyu; Qin, Zequn; Li, Songyuan; Li, Xi] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310007, Peoples R China	Zhejiang University	Li, X (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310007, Peoples R China.	huanyuhello@zju.edu.cn; qinzequn@zju.edu.cn; leizungjyun@zju.edu.cn; xilizju@zju.edu.cn	Qin, Zequn/HGU-7660-2022		National Key Research and Development Program of China [2020AAA0107400]; National Natural Science Foundation of China [U20A20222]; Zhejiang Provincial Natural Science Foundation of China [LR19F020004]; key scientific technological innovation research project by Ministry of Education	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Zhejiang Provincial Natural Science Foundation of China(Natural Science Foundation of Zhejiang Province); key scientific technological innovation research project by Ministry of Education	This work was supported in part by National Key Research and Development Program of China under Grant 2020AAA0107400, National Natural Science Foundation of China under Grant U20A20222, Zhejiang Provincial Natural Science Foundation of China under Grant LR19F020004, and key scientific technological innovation research project by Ministry of Education	Alex Graves, 2017, Arxiv, DOI arXiv:1603.08983; Almahairi A, 2016, PR MACH LEARN RES, V48; Babak Ehteshami Bejnordi, 2020, Arxiv, DOI arXiv:2004.01808; Cai Han, 2019, INT C LEARN REPR; Chen J., 2020, ARXIV; Chen X, 2019, IEEE I CONF COMP VIS, P1294, DOI 10.1109/ICCV.2019.00138; Chen ZH, 2018, IEEE T IND INFORM, V14, P4334, DOI 10.1109/TII.2018.2789925; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dong XY, 2017, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR.2017.205; Du XC, 2019, IEEE J EM SEL TOP C, V9, P453, DOI 10.1109/JETCAS.2019.2933233; Fei Sun, 2020, Arxiv, DOI arXiv:2004.11946; Figurnov M., 2016, P ADV NEUR INF PROC, P1; Figurnov M, 2017, PROC CVPR IEEE, P1790, DOI 10.1109/CVPR.2017.194; Gao Xitong, 2019, INT C LEARN REPR; Guo QS, 2019, PROC CVPR IEEE, P5142, DOI 10.1109/CVPR.2019.00529; Han S., 2016, P 4 INT C LEARN REPR, P1; Hanwen Liang, 2020, Arxiv, DOI arXiv:1909.06035; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He Kaiming, 2017, CVPR; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Hinton G., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/TPAMI.2012.59; Hu H., 2014, PROC 32 C UNCER ARTI, P279; Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang Gao, 2018, ICLR; Ioannou Y., 2016, PROC INT C LEARN REP, P1, DOI [10.17863/CAM.13730, DOI 10.17863/CAM.13730]; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; Jang E., 2017, INT C LEARN REPR; Jordao A, 2020, IEEE J-STSP, V14, P828, DOI 10.1109/JSTSP.2020.2975987; Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Leroux Sam, 2018, ICLR WORKSH; Li H., 2017, P INT C LEARN REPR I, P1; Li H, 2019, IEEE I CONF COMP VIS, P1891, DOI 10.1109/ICCV.2019.00198; McIntosh L., 2016, PROC INT C LEARN REP, P1; McIntosh L, 2018, IEEE COMPUT SOC CONF, P1729, DOI 10.1109/CVPRW.2018.00216; Mullapudi RT, 2018, PROC CVPR IEEE, P8080, DOI 10.1109/CVPR.2018.00843; Netzer Yuval, 2011, NEURIPS WORKSH, V2, P6; Polino Antonio, 2018, P 6 INT C LEARN REPR; Rao YM, 2019, IEEE T PATTERN ANAL, V41, P2291, DOI 10.1109/TPAMI.2018.2878258; Reyzin L., 2011, P INT C MACH LEARN, P529; Shafiee Mohammad Saeed, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P667, DOI 10.1109/CVPRW.2019.00093; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Su ZZ, 2021, PSYCHOL HEALTH MED, V26, P991, DOI 10.1080/13548506.2020.1774626; Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293; Tan MX, 2019, PR MACH LEARN RES, V97; Teerapittayanon S, 2016, INT C PATT RECOG, P2464, DOI 10.1109/ICPR.2016.7900006; Veit A, 2018, LECT NOTES COMPUT SC, V11205, P3, DOI 10.1007/978-3-030-01246-5_1; Veit A, 2016, ADV NEUR IN, V29; Verelst T, 2020, PROC CVPR IEEE, P2317, DOI 10.1109/CVPR42600.2020.00239; Wan L., 2013, P INT C MACHINE LEAR, P1058; Wang X, 2018, LECT NOTES COMPUT SC, V11217, P420, DOI 10.1007/978-3-030-01261-8_25; Wang Y, 2020, IEEE J-STSP, V14, P623, DOI 10.1109/JSTSP.2020.2979669; Wen W, 2016, ADV NEUR IN, V29; Wu JX, 2016, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2016.521; Wu ZX, 2018, PROC CVPR IEEE, P8817, DOI 10.1109/CVPR.2018.00919; Xia WH, 2022, IEEE T EMERG TOP COM, V10, P962, DOI 10.1109/TETC.2021.3056031; Yang L, 2020, PROC CVPR IEEE, P2366, DOI 10.1109/CVPR42600.2020.00244; Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104; Yu JJQ, 2018, IEEE T IND INFORM, V14, P3271, DOI 10.1109/TII.2018.2825243; Yu JH, 2019, IEEE I CONF COMP VIS, P1803, DOI 10.1109/ICCV.2019.00189; Yu RC, 2018, PROC CVPR IEEE, P9194, DOI 10.1109/CVPR.2018.00958; Yue Meng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P86, DOI 10.1007/978-3-030-58571-6_6; Zhang PY, 2019, IEEE INT CONF COMP V, P37, DOI 10.1109/ICCVW.2019.00011; Zhenda Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P531, DOI 10.1007/978-3-030-58452-8_31	67	1	1	4	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6011	6023		10.1109/TPAMI.2021.3084680	http://dx.doi.org/10.1109/TPAMI.2021.3084680			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34048337	Green Submitted			2022-12-18	WOS:000853875300015
J	Wang, P; Li, H; Shen, CH				Wang, Peng; Li, Hui; Shen, Chunhua			Towards End-to-End Text Spotting in Natural Scenes	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Text recognition; Feature extraction; Proposals; Image recognition; Task analysis; Training; Shape; End-to-end scene text spotting; deep neural network; attention model	NEURAL-NETWORK; RECOGNITION	Text spotting in natural scene images is of great importance for many image understanding tasks. It includes two sub-tasks: text detection and recognition. In this work, we propose a unified network that simultaneously localizes and recognizes text with a single forward pass, avoiding intermediate processes such as image cropping and feature re-calculation, word separation, and character grouping. The overall framework is trained end-to-end and is able to spot text of arbitrary shapes. The convolutional features are calculated only once and shared by both the detection and recognition modules. Through multi-task training, the learned features become more discriminative and improve the overall performance. By employing a 2D attention model in word recognition, the issue of text irregularity is robustly addressed. The attention model provides the spatial location for each character, which not only helps local feature extraction in word recognition, but also indicates an orientation angle to refine text localization. Experiments demonstrate that our proposed method can achieve state-of-the-art performance on several commonly used text spotting benchmarks, including both regular and irregular datasets. Extensive ablation experiments are performed to verify the effectiveness of each module design.	[Wang, Peng] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China; [Wang, Peng] Northwestern Polytech Univ, Ningbo Inst, Xian 710072, Shaanxi, Peoples R China; [Wang, Peng] Natl Engn Lab Integrated AeroSp Ground Ocean Big, Xian 710072, Shaanxi, Peoples R China; [Li, Hui] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia; [Shen, Chunhua] Monash Univ, Clayton, Vic 3800, Australia	Northwestern Polytechnical University; Northwestern Polytechnical University; University of Adelaide; Monash University	Shen, CH (corresponding author), Monash Univ, Clayton, Vic 3800, Australia.	peng.wang@nwpu.edu.cn; huili03855@gmail.com; chunhua@me.com			National Natural Science Foundation of China [U19B2037, 61876152]; National Key R&D Program of China [2020AA A0106900]; Ningbo Natural Science Foundation [202003N4369]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key R&D Program of China; Ningbo Natural Science Foundation	This work was supported by the National Natural Science Foundation of China under Grants U19B2037, 61876152, National Key R&D Program of China under Grant 2020AA A0106900, and Ningbo Natural Science Foundation under Grant 202003N4369. Peng Wang and Hui Li equally contributed to thiswork.	Andreas Veit, 2016, Arxiv, DOI arXiv:1601.07140; Bai F, 2018, PROC CVPR IEEE, P1508, DOI 10.1109/CVPR.2018.00163; Busta M, 2017, IEEE I CONF COMP VIS, P2223, DOI 10.1109/ICCV.2017.242; Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157; Cheng ZZ, 2018, PROC CVPR IEEE, P5571, DOI 10.1109/CVPR.2018.00584; Cheng ZZ, 2017, IEEE I CONF COMP VIS, P5086, DOI 10.1109/ICCV.2017.543; Cong Yao, 2016, Arxiv, DOI arXiv:1606.09002; Feng W, 2019, IEEE I CONF COMP VIS, P9075, DOI 10.1109/ICCV.2019.00917; Gomez L, 2017, PATTERN RECOGN, V70, P60, DOI 10.1016/j.patcog.2017.04.027; Graves A., 2006, P INT C MACH LEARN I; Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331; He P, 2016, AAAI CONF ARTIF INTE, P3501; He T, 2018, PROC CVPR IEEE, P5020, DOI 10.1109/CVPR.2018.00527; Huang WL, 2014, LECT NOTES COMPUT SC, V8692, P497, DOI 10.1007/978-3-319-10593-2_33; Jaderberg M, 2014, ARXIV; Jaderberg M, 2015, ADV NEUR IN, V28; Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z; Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34; Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942; Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221; Lee CY, 2016, PROC CVPR IEEE, P2231, DOI 10.1109/CVPR.2016.245; Li H, 2019, AAAI CONF ARTIF INTE, P8610; Li H, 2017, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2017.560; Liao MH, 2021, IEEE T PATTERN ANAL, V43, P532, DOI 10.1109/TPAMI.2019.2937086; Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107; Liao MH, 2017, AAAI CONF ARTIF INTE, P4161; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu W, 2018, AAAI CONF ARTIF INTE, P7154; Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2; Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595; Liu YL, 2017, PROC CVPR IEEE, P3454, DOI 10.1109/CVPR.2017.368; Long SB, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01369-0; Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788; Lyu PY, 2018, LECT NOTES COMPUT SC, V11218, P71, DOI 10.1007/978-3-030-01264-9_5; Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020; Nayef N, 2017, PROC INT CONF DOC, P1454, DOI 10.1109/ICDAR.2017.237; Neumann L, 2016, IEEE T PATTERN ANAL, V38, P1872, DOI 10.1109/TPAMI.2015.2496234; Prasad S, 2018, LECT NOTES COMPUT SC, V11220, P559, DOI 10.1007/978-3-030-01270-0_33; Qin SY, 2019, IEEE I CONF COMP VIS, P4703, DOI 10.1109/ICCV.2019.00480; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939; Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371; Shi BG, 2016, PROC CVPR IEEE, P4168, DOI 10.1109/CVPR.2016.452; Sun YP, 2019, LECT NOTES COMPUT SC, V11363, P83, DOI 10.1007/978-3-030-20893-6_6; Sutskever I, 2014, ADV NEUR IN, V27; Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4; Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436; Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402; Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956; Xing LJ, 2019, IEEE I CONF COMP VIS, P9125, DOI 10.1109/ICCV.2019.00922; Ye QX, 2015, IEEE T PATTERN ANAL, V37, P1480, DOI 10.1109/TPAMI.2014.2366765; Yin XC, 2016, IEEE T IMAGE PROCESS, V25, P2752, DOI 10.1109/TIP.2016.2554321; Yin XC, 2014, IEEE T PATTERN ANAL, V36, P970, DOI 10.1109/TPAMI.2013.182; Yuliang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9806, DOI 10.1109/CVPR42600.2020.00983; Zhan FN, 2019, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2019.00216; Zhang CQ, 2019, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR.2019.01080; Zhang Z, 2016, PROC CVPR IEEE, P4159, DOI 10.1109/CVPR.2016.451; Zhang Z, 2015, PROC CVPR IEEE, P2558, DOI 10.1109/CVPR.2015.7298871; Zhong ZY, 2017, INT CONF ACOUST SPEE, P1208, DOI 10.1109/ICASSP.2017.7952348; Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283; Zhu YY, 2016, FRONT COMPUT SCI-CHI, V10, P19, DOI 10.1007/s11704-015-4488-0	66	1	1	13	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					7266	7281		10.1109/TPAMI.2021.3095916	http://dx.doi.org/10.1109/TPAMI.2021.3095916			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34242162	Green Submitted			2022-12-18	WOS:000853875300097
J	Xu, YY; Zhang, ZH; Gao, SH				Xu, Yanyu; Zhang, Ziheng; Gao, Shenghua			Spherical DNNs and Their Applications in 360 degrees Images and Videos	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Spherical deep neural networks; saliency detection; gaze prediction; 360 degrees videos	SALIENCY; GAZE; PREDICTION; PERCEPTION; EYE	Spherical images or videos, as typical non-euclidean data, are usually stored in the form of 2D panoramas obtained through an equirectangular projection, which is neither equal area nor conformal. The distortion caused by the projection limits the performance of vanilla Deep Neural Networks (DNNs) designed for traditional euclidean data. In this paper, we design a novel Spherical Deep Neural Network (DNN) to deal with the distortion caused by the equirectangular projection. Specifically, we customize a set of components, including a spherical convolution, a spherical pooling, a spherical ConvLSTM cell and a spherical MSE loss, as the replacements of their counterparts in vanilla DNNs for spherical data. The core idea is to change the identical behavior of the conventional operations in vanilla DNNs across different feature patches so that they will be adjusted to the distortion caused by the variance of sampling rate among different feature patches. We demonstrate the effectiveness of our Spherical DNNs for saliency detection and gaze estimation in 360 degrees videos. For saliency detection, we take the temporal coherence of an observer's viewing process into consideration and propose to use a Spherical U-Net and a Spherical ConvLSTM to predict the saliency maps for each frame sequentially. As for gaze prediction, we propose to leverage a Spherical Encoder Module to extract spatial panoramic features, then we combine them with the gaze trajectory feature extracted by an LSTM for future gaze prediction. To facilitate the study of the 360 degrees videos saliency detection, we further construct a large-scale 360 degrees video saliency detection dataset that consists of 104 360 degrees videos viewed by 20+ human subjects. Comprehensive experiments validate the effectiveness of our proposed Spherical DNNs for 360 degrees handwritten digit classification and sport classification, saliency detection and gaze tracking in 360 degrees videos. We also visualize the regions contributing to the classification decisions in our proposed Spherical DNNs via the Grad-CAM technique in the classification task, and the results show that our Spherical DNNs constantly leverage reasonable and important regions for decision making, regardless the large distortions. All codes and dataset are available on https://github.com/svip-lab/SphericalDNNs.	[Xu, Yanyu] ASTAR, Inst High Performance Comp IHPC, Singapore 138632, Singapore; [Zhang, Ziheng] AI Prime Co Ltd, Shanghai 200090, Peoples R China; [Gao, Shenghua] ShanghaiTech Univ, Shanghai Engn Res Ctr Intelligent Vis & Imaging, Shanghai 201210, Peoples R China; [Gao, Shenghua] ShanghaiTech Univ, Shanghai Engn Res Ctr Energy Efficient & Custom A, Shanghai 201210, Peoples R China	Agency for Science Technology & Research (A*STAR); A*STAR - Institute of High Performance Computing (IHPC); ShanghaiTech University; ShanghaiTech University	Gao, SH (corresponding author), ShanghaiTech Univ, Shanghai Engn Res Ctr Intelligent Vis & Imaging, Shanghai 201210, Peoples R China.; Gao, SH (corresponding author), ShanghaiTech Univ, Shanghai Engn Res Ctr Energy Efficient & Custom A, Shanghai 201210, Peoples R China.	xuyy2@shanghaitech.edu.cn; zhangzh@shanghaitech.edu.cn; gaoshh@shanghaitech.edu.cn		Xu, Yanyu/0000-0001-8926-7833	National Key R&D Program of China [2018AAA0100704]; NSFC [61932020]; Science and Technology Commission of Shanghai Municipality [20ZR1436000]; Shuguang Program' by Shanghai Education Development Foundation; Shanghai Municipal Education Commission	National Key R&D Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Science and Technology Commission of Shanghai Municipality(Science & Technology Commission of Shanghai Municipality (STCSM)); Shuguang Program' by Shanghai Education Development Foundation; Shanghai Municipal Education Commission(Shanghai Municipal Education Commission (SHMEC))	The work was supported by National Key R&D Program of China (2018AAA0100704), NSFC #61932020, Science and Technology Commission of Shanghai Municipality under Grant 20ZR1436000, and Shuguang Program' supported by Shanghai Education Development Foundation and Shanghai Municipal Education Commission. Yanyu Xu and Ziheng Zhang have contributed equally to this work.	Assens M, 2017, IEEE INT CONF COMP V, P2331, DOI 10.1109/ICCVW.2017.275; Bak C ., 2016, ARXIV; Bazzani L., 2017, PROC INT C LEARN REP; Boomsma W., 2017, ADV NEURAL INFORM PR, V30, P3433; Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512; Chaabouni S, 2016, IEEE IMAGE PROC, P1604, DOI 10.1109/ICIP.2016.7532629; Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154; Cohen T.S., 2018, INT C LEARN REPR; Coors B, 2018, LECT NOTES COMPUT SC, V11213, P525, DOI 10.1007/978-3-030-01240-3_32; de La Garanderie GP, 2018, LECT NOTES COMPUT SC, V11217, P812, DOI 10.1007/978-3-030-01261-8_48; DRISCOLL JR, 1994, ADV APPL MATH, V15, P202, DOI 10.1006/aama.1994.1008; Eder Marc, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12423, DOI 10.1109/CVPR42600.2020.01244; Esteves C, 2018, LECT NOTES COMPUT SC, V11217, P54, DOI 10.1007/978-3-030-01261-8_4; Fang S, 2017, IEEE T NEUR NET LEAR, V28, P1095, DOI 10.1109/TNNLS.2016.2522440; GREENE N, 1986, IEEE COMPUT GRAPH, V6, P21, DOI 10.1109/MCG.1986.276658; Gutierrez J, 2018, SIGNAL PROCESS-IMAGE, V69, P35, DOI 10.1016/j.image.2018.05.003; Harel J., 2006, PAPER PRESENTED INT, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073; Hu HN, 2017, PROC CVPR IEEE, P1396, DOI 10.1109/CVPR.2017.153; Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38; Itti L, 2003, PROC SPIE, V5200, P64, DOI 10.1117/12.512618; Jaderberg M, 2015, ADV NEUR IN, V28; Jiang C.M., 2019, PROC INT C LEARN REP, P1; Judd T., 2012, MIT TECHNICAL REPORT; Khasanova R., 2019, PROC 36 INT C MACH L, P3351; Kruthiventi SSS, 2016, PROC CVPR IEEE, P5781, DOI 10.1109/CVPR.2016.623; Lee Y, 2019, PROC CVPR IEEE, P9173, DOI 10.1109/CVPR.2019.00940; Liu YF, 2017, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2017.343; Luo WX, 2017, IEEE INT CON MULTI, P439, DOI 10.1109/ICME.2017.8019325; Ordonez FJ, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16010115; Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71; Paszke A, 2019, ADV NEUR IN, V32; Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019; Rai Y, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P205, DOI 10.1145/3083187.3083218; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ren ZX, 2013, IEEE T IMAGE PROCESS, V22, P3120, DOI 10.1109/TIP.2013.2259837; Ronneberger O., 2015, P MEDICAL IMAGE COMP, P234; Ruhland K, 2015, COMPUT GRAPH FORUM, V34, P299, DOI 10.1111/cgf.12603; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shi XJ, 2015, ADV NEUR IN, V28; Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599; Snyder JP, 1997, FLATTENING EARTH 200; Su YC, 2019, PROC CVPR IEEE, P9434, DOI 10.1109/CVPR.2019.00967; Su YC, 2017, ADV NEUR IN, V30; Su YC, 2017, LECT NOTES COMPUT SC, V10114, P154, DOI 10.1007/978-3-319-54190-7_10; Su YC, 2017, PROC CVPR IEEE, P1368, DOI 10.1109/CVPR.2017.150; Tateno K, 2018, LECT NOTES COMPUT SC, V11220, P732, DOI 10.1007/978-3-030-01270-0_43; Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941; Wang WG, 2018, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR.2018.00514; Xiong B, 2018, LECT NOTES COMPUT SC, V11209, P3, DOI 10.1007/978-3-030-01228-1_1; Xu M, 2020, IEEE J-STSP, V14, P5, DOI 10.1109/JSTSP.2020.2966864; Xu M, 2020, IEEE J-STSP, V14, P2, DOI 10.1109/JSTSP.2020.2968778; Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783; Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559; Xu YY, 2018, PROC CVPR IEEE, P5275, DOI 10.1109/CVPR.2018.00553; Xu YY, 2019, IEEE T PATTERN ANAL, V41, P2975, DOI 10.1109/TPAMI.2018.2866563; Zelnik-Manor L, 2005, IEEE I CONF COMP VIS, P1292; Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1; Zhang ZH, 2018, LECT NOTES COMPUT SC, V11211, P504, DOI 10.1007/978-3-030-01234-2_30; Zhao Q, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1198; Zhong S.H., 2013, PROC 27 AAAI C ARTIF; Zhou F, 2014, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2014.429	61	1	1	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					7235	7252		10.1109/TPAMI.2021.3100259	http://dx.doi.org/10.1109/TPAMI.2021.3100259			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34314354				2022-12-18	WOS:000853875300095
J	Xu, YL; Wang, WG; Liu, TY; Liu, XB; Xie, JW; Zhu, SC				Xu, Yuanlu; Wang, Wenguan; Liu, Tengyu; Liu, Xiaobai; Xie, Jianwen; Zhu, Song-Chun			Monocular 3D Pose Estimation via Pose Grammar and Data Augmentation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Grammar; Pose estimation; Cameras; Solid modeling; Training; Protocols; 3D pose estimation; dependency grammar; data augmentation; deep neural network; recurrent neural network; evaluation protocol; learning-by-synthesis		In this paper, we propose a pose grammar to tackle the problem of 3D human pose estimation from a monocular RGB image. Our model takes estimated 2D pose as the input and learns a generalized 2D-3D mapping function to leverage into 3D pose. The proposed model consists of a base network which efficiently captures pose-aligned features and a hierarchy of Bi-directional RNNs (BRNNs) on the top to explicitly incorporate a set of knowledge regarding human body configuration (i.e., kinematics, symmetry, motor coordination). The proposed model thus enforces high-level constraints over human poses. In learning, we develop a data augmentation algorithm to further improve model robustness against appearance variations and cross-view generalization ability. We validate our method on public 3D human pose benchmarks and propose a new evaluation protocol working on cross-view setting to verify the generalization capability of different methods. We empirically observe that most state-of-the-art methods encounter difficulty under such setting while our method can well handle such challenges.	[Xu, Yuanlu; Wang, Wenguan; Liu, Tengyu; Zhu, Song-Chun] Univ Calif Los Angeles UCLA, Dept Comp Sci & Stat, Los Angeles, CA 90095 USA; [Liu, Xiaobai] San Diego State Univ SDSU, Dept Comp Sci, San Diego, CA 92182 USA; [Xie, Jianwen] Baidu Res, Sunnyvale, CA 94089 USA	University of California System; University of California Los Angeles; California State University System; San Diego State University; Baidu	Xu, YL (corresponding author), Univ Calif Los Angeles UCLA, Dept Comp Sci & Stat, Los Angeles, CA 90095 USA.	merayxu@gmail.com; wenguanwang.ai@gmail.com; tengyuliu@cs.ucla.edu; xiaobai.liu@mail.sdsu.edu; jianwen@ucla.edu; sczhu@stat.ucla.edu	Yuanlu, Xu/W-6921-2019	Yuanlu, Xu/0000-0002-7095-1018	DARPA XAI Project [N66001-17-2-4029, N66001-17-2-3602]; ONR MURI Project [N00014-16-1-2007]; JUMP CRISP center; NSF [IIS 1423305, 1657600]; ARO [W911NF1810296]	DARPA XAI Project; ONR MURI Project; JUMP CRISP center; NSF(National Science Foundation (NSF)); ARO	Y The authors would like to thank Bruce Xiaohan Nie, Ping Wei and Hao-Shu Fang for their prior efforts and supports. This work was supported in part by DARPA XAI Project under Grants N66001-17-2-4029 and N66001-17-2-3602, in part by ONR MURI Project under Grant N00014-16-1-2007, in part by JUMP CRISP center, in part by NSF under Grants IIS 1423305 and 1657600, and in part by ARO under Grant W911NF1810296.	Akhter I, 2015, PROC CVPR IEEE, P1446, DOI 10.1109/CVPR.2015.7298751; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; [Anonymous], 2016, P INT JOINT C ART IN; Bazzani L, 2013, COMPUT VIS IMAGE UND, V117, P130, DOI 10.1016/j.cviu.2012.10.008; Belagiannis V, 2014, PROC CVPR IEEE, P1669, DOI 10.1109/CVPR.2014.216; Bo LF, 2010, INT J COMPUT VISION, V87, P28, DOI 10.1007/s11263-008-0204-y; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Charles J, 2016, PROC CVPR IEEE, P3063, DOI 10.1109/CVPR.2016.334; Chen CH, 2017, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR.2017.610; Cheng Y, 2019, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2019.00081; Chiovetto E, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0079555; Collins AG, 2001, EVOL DEV, V3, P432, DOI 10.1046/j.1525-142X.2001.01048.x; Du Y, 2016, LECT NOTES COMPUT SC, V9908, P20, DOI 10.1007/978-3-319-46493-0_2; Elhayek A, 2015, PROC CVPR IEEE, P3810, DOI 10.1109/CVPR.2015.7299005; Fang H.-S., 2018, PROC AAAI C ARTIF IN; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Felzenszwalb PF, 2000, PROC CVPR IEEE, P66, DOI 10.1109/CVPR.2000.854739; Fidler S., 2006, PROC IEEE C COMPUT V, P182; FIRSCHEIN O, 1983, P IEEE, V71, P1231, DOI 10.1109/PROC.1983.12755; Flash T, 2005, CURR OPIN NEUROBIOL, V15, P660, DOI 10.1016/j.conb.2005.10.011; Flash T., 2013, J ROBOT AUTON SYST, V61, P330; Funk C, 2017, IEEE I CONF COMP VIS, P793, DOI 10.1109/ICCV.2017.92; GAIFMAN H, 1965, INFORM CONTROL, V8, P304, DOI 10.1016/S0019-9958(65)90232-9; Gazdar G., 1985, GEN PHRASE STRUCTURE; Geman S, 2002, Q APPL MATH, V60, P707, DOI 10.1090/qam/1939008; Habibie I, 2019, PROC CVPR IEEE, P10897, DOI 10.1109/CVPR.2019.01116; Hao Jiang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1674, DOI 10.1109/ICPR.2010.414; HAYS DG, 1964, LANGUAGE, V40, P511, DOI 10.2307/411934; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Ionescu C, 2011, IEEE I CONF COMP VIS, P2220, DOI 10.1109/ICCV.2011.6126500; Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781; Johnson S., 2010, BMVC, DOI [10.5244/C.24.12, DOI 10.5244/C.24.12.CITESEER]; Kocabas M, 2019, PROC CVPR IEEE, P1077, DOI 10.1109/CVPR.2019.00117; Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234; Kostrikov I., 2014, PROC BRIT MACH VIS C; Lan XY, 2005, IEEE I CONF COMP VIS, P470; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; Li C, 2019, PROC CVPR IEEE, P9879, DOI 10.1109/CVPR.2019.01012; Li SJ, 2015, IEEE I CONF COMP VIS, P2848, DOI 10.1109/ICCV.2015.326; Li SJ, 2015, LECT NOTES COMPUT SC, V9004, P332, DOI 10.1007/978-3-319-16808-1_23; Liu TQ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661243; Liu XB, 2018, IEEE T PATTERN ANAL, V40, P710, DOI 10.1109/TPAMI.2017.2689007; Lu YX, 2009, FOUND TRENDS COMPUT, V5, P1, DOI 10.1561/0600000008; Lura D, 2013, IEEE INT CONF ROBOT, P5303, DOI 10.1109/ICRA.2013.6631336; Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539; Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288; Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Nibali A, 2019, IEEE WINT CONF APPL, P1477, DOI 10.1109/WACV.2019.00162; Nie BX, 2017, IEEE I CONF COMP VIS, P3467, DOI 10.1109/ICCV.2017.373; Park S, 2018, IEEE T PATTERN ANAL, V40, P1555, DOI 10.1109/TPAMI.2017.2731842; Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138; Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794; Qi H, 2018, AAAI CONF ARTIF INTE, P7292; Radwan I, 2013, IEEE I CONF COMP VIS, P1888, DOI 10.1109/ICCV.2013.237; Ramakrishna V, 2012, LECT NOTES COMPUT SC, V7575, P573, DOI 10.1007/978-3-642-33765-9_41; Rhodin H, 2018, LECT NOTES COMPUT SC, V11214, P765, DOI 10.1007/978-3-030-01249-6_46; Rhodin H, 2018, PROC CVPR IEEE, P8437, DOI 10.1109/CVPR.2018.00880; Rogez G, 2016, ADV NEUR IN, V29; Rothrock B, 2013, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR.2013.413; Sanzari M, 2016, LECT NOTES COMPUT SC, V9912, P566, DOI 10.1007/978-3-319-46484-8_34; Sarafianos N, 2016, COMPUT VIS IMAGE UND, V152, P1, DOI 10.1016/j.cviu.2016.09.002; Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093; Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750; Sigal L, 2012, INT J COMPUT VISION, V98, P15, DOI 10.1007/s11263-011-0493-4; Simo-Serra E, 2013, PROC CVPR IEEE, P3634, DOI 10.1109/CVPR.2013.466; Simo-Serra E, 2012, PROC CVPR IEEE, P2673, DOI 10.1109/CVPR.2012.6247988; Sun X, 2017, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2017.284; Tai KS, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1556; Taylor GW, 2010, PROC CVPR IEEE, P631, DOI 10.1109/CVPR.2010.5540157; Tekin B, 2017, IEEE I CONF COMP VIS, P3961, DOI 10.1109/ICCV.2017.425; Tekin B, 2016, PROC CVPR IEEE, pCP8, DOI 10.1109/CVPR.2016.113; Tekin Bugra, 2016, BRIT MACH VIS C 2016, DOI DOI 10.5244/C.30.130; Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603; van der Hulst FPJ, 2012, IEEE INT CONF ROBOT, P5123, DOI 10.1109/ICRA.2012.6225350; Veges M, 2019, NEUROCOMPUTING, V339, P194, DOI 10.1016/j.neucom.2019.02.029; Wandt B, 2019, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2019.00797; Wang CY, 2014, PROC CVPR IEEE, P2369, DOI 10.1109/CVPR.2014.303; Wang WG, 2018, PROC CVPR IEEE, P4271, DOI 10.1109/CVPR.2018.00449; Wei P, 2017, IEEE T PATTERN ANAL, V39, P1165, DOI 10.1109/TPAMI.2016.2574712; Xu YL, 2019, IEEE I CONF COMP VIS, P7759, DOI 10.1109/ICCV.2019.00785; Xu YL, 2018, PROC CVPR IEEE, P2178, DOI 10.1109/CVPR.2018.00232; Xu YL, 2017, AAAI CONF ARTIF INTE, P4299; Yamane K, 2010, SPRINGER TRAC ADV RO, V66, P49; Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551; Yang Y, 2013, IEEE T PATTERN ANAL, V35, P2878, DOI 10.1109/TPAMI.2012.261; Yasin H, 2016, PROC CVPR IEEE, P4948, DOI 10.1109/CVPR.2016.535; Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354; Zhou XW, 2017, IEEE T PATTERN ANAL, V39, P1648, DOI 10.1109/TPAMI.2016.2605097; Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537; Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018; Zhu Y., 2018, PROC EUR C COMPUT VI	99	1	1	11	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6327	6344		10.1109/TPAMI.2021.3087695	http://dx.doi.org/10.1109/TPAMI.2021.3087695			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34106844				2022-12-18	WOS:000853875300036
J	Yuan, X; Liu, Y; Suo, JL; Durand, F; Dai, QH				Yuan, Xin; Liu, Yang; Suo, Jinli; Durand, Fredo; Dai, Qionghai			Plug-and-Play Algorithms for Video Snapshot Compressive Imaging	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image reconstruction; Image color analysis; Sensors; Image coding; Gray-scale; Cameras; Noise reduction; Compressive sensing; deep learning; computational imaging; coded aperture; image processing; video processing; coded aperture compressive temporal imaging (CACTI); plug-and-play (PnP) algorithms	SPARSE; MINIMIZATION; RESTORATION; PRIORS; MODEL	We consider the reconstruction problem of video snapshot compressive imaging (SCI), which captures high-speed videos using a low-speed 2D sensor (detector). The underlying principle of SCI is to modulate sequential high-speed frames with different masks and then these encoded frames are integrated into a snapshot on the sensor and thus the sensor can be of low-speed. On one hand, video SCI enjoys the advantages of low-bandwidth, low-power and low-cost. On the other hand, applying SCI to large-scale problems (HD or UHD videos) in our daily life is still challenging and one of the bottlenecks lies in the reconstruction algorithm. Existing algorithms are either too slow (iterative optimization algorithms) or not flexible to the encoding process (deep learning based end-to-end networks). In this paper, we develop fast and flexible algorithms for SCI based on the plug-and-play (PnP) framework. In addition to the PnP-ADMM method, we further propose the PnP-GAP (generalized alternating projection) algorithm with a lower computational workload. We first employ the image deep denoising priors to show that PnP can recover a UHD color video with 30 frames from a snapshot measurement. Since videos have strong temporal correlation, by employing the video deep denoising priors, we achieve a significant improvement in the results. Furthermore, we extend the proposed PnP algorithms to the color SCI system using mosaic sensors, where each pixel only captures the red, green or blue channels. A joint reconstruction and demosaicing paradigm is developed for flexible and high quality reconstruction of color video SCI systems. Extensive results on both simulation and real datasets verify the superiority of our proposed algorithm.	[Yuan, Xin] Westlake Univ, Sch Engn, Hangzhou 310024, Zhejiang, Peoples R China; [Liu, Yang; Durand, Fredo] MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA; [Suo, Jinli; Dai, Qionghai] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China	Westlake University; Massachusetts Institute of Technology (MIT); Tsinghua University	Yuan, X (corresponding author), Westlake Univ, Sch Engn, Hangzhou 310024, Zhejiang, Peoples R China.	xyuan@westlake.edu.cn; yliu12@mit.edu; jlsuo@tsinghua.edu.cn; fredo@mit.edu; qhdai@tsinghua.edu.cn		Liu, Yang/0000-0002-5787-0934; Yuan, Xin/0000-0002-8311-7524	NSFC [61722110, 61931012, 61631009]; Beijing Municipal Science & Technology Commission (BMSTC) [Z181100003118014]	NSFC(National Natural Science Foundation of China (NSFC)); Beijing Municipal Science & Technology Commission (BMSTC)(Beijing Municipal Science & Technology Commission)	The work of Jinli Suo and Qionghai Daiwas supported in part by NSFC under Grants 61722110, 61931012, and 61631009 and in part by Beijing Municipal Science & Technology Commission (BMSTC) under Grant Z181100003118014. X. Yuan and Y. Liu contribute equally to this paper.	Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199; Altmann Y, 2018, SCIENCE, V361, P660, DOI 10.1126/science.aat2298; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bioucas-Dias JM, 2007, IEEE T IMAGE PROCESS, V16, P2992, DOI 10.1109/TIP.2007.909319; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Brady DJ, 2020, ADV OPT PHOTONICS, V12, P787, DOI 10.1364/AOP.398263; Brady DJ, 2018, OPTICA, V5, P127, DOI 10.1364/OPTICA.5.000127; Brady DJ, 2015, ADV OPT PHOTONICS, V7, P756, DOI 10.1364/AOP.7.000756; Chan SH, 2017, IEEE T COMPUT IMAG, V3, P84, DOI 10.1109/TCI.2016.2629286; Cheng ZH, 2021, PROC CVPR IEEE, P16241, DOI 10.1109/CVPR46437.2021.01598; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Deng C, 2021, IEEE T PATTERN ANAL, V43, P1380, DOI 10.1109/TPAMI.2019.2946567; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Duarte MF, 2008, IEEE SIGNAL PROC MAG, V25, P83, DOI 10.1109/MSP.2007.914730; Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366; Hitomi Y, 2011, IEEE I CONF COMP VIS, P287, DOI 10.1109/ICCV.2011.6126254; Huang T, 2021, PROC CVPR IEEE, P16211, DOI 10.1109/CVPR46437.2021.01595; Jalali S, 2019, IEEE T INFORM THEORY, V65, P8005, DOI 10.1109/TIT.2019.2940666; Jalali S, 2018, IEEE INT SYMP INFO, P416; Jin KH, 2017, IEEE T IMAGE PROCESS, V26, P4509, DOI 10.1109/TIP.2017.2713099; Li X, 2008, PROC SPIE, V6822, DOI 10.1117/12.766768; Li YQ, 2020, IEEE INT CONF COMPUT; Liao XJ, 2014, SIAM J IMAGING SCI, V7, P797, DOI 10.1137/130936658; Liu Y, 2019, IEEE T PATTERN ANAL, V41, P2990, DOI 10.1109/TPAMI.2018.2873587; Llull P, 2015, OPTICA, V2, P822, DOI 10.1364/OPTICA.2.000822; Llull P, 2013, OPT EXPRESS, V21, P10526, DOI 10.1364/OE.21.010526; Ma JW, 2019, IEEE I CONF COMP VIS, P10222, DOI 10.1109/ICCV.2019.01032; Maggioni M, 2012, IEEE T IMAGE PROCESS, V21, P3952, DOI 10.1109/TIP.2012.2199324; Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452; Mait JN, 2018, ADV OPT PHOTONICS, V10, P409, DOI 10.1364/AOP.10.000409; Malvar HS, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P485; Meng ZY, 2020, OPT LETT, V45, P3893, DOI 10.1364/OL.393213; Menon D, 2007, IEEE T IMAGE PROCESS, V16, P132, DOI 10.1109/TIP.2006.884928; Miao X, 2019, IEEE I CONF COMP VIS, P4058, DOI 10.1109/ICCV.2019.00416; Qiao M, 2021, OPT LETT, V46, P1888, DOI 10.1364/OL.420139; Qiao M, 2020, OPT LETT, V45, P1659, DOI 10.1364/OL.386238; Qiao M, 2020, APL PHOTONICS, V5, DOI 10.1063/1.5140721; Reddy D, 2011, PROC CVPR IEEE, P329, DOI 10.1109/CVPR.2011.5995542; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Ryu EK, 2019, PR MACH LEARN RES, V97; Sreehari S, 2016, IEEE T COMPUT IMAG, V2, P408, DOI 10.1109/TCI.2016.2599778; Sun YY, 2017, OPT EXPRESS, V25, P18182, DOI 10.1364/OE.25.018182; Tassano M, 2020, PROC CVPR IEEE, P1351, DOI 10.1109/CVPR42600.2020.00143; Tsai TH, 2015, OPT LETT, V40, P4054, DOI 10.1364/OL.40.004054; Tsai TH, 2015, OPT EXPRESS, V23, P11912, DOI 10.1364/OE.23.011912; Venkatakrishnan S, 2013, IEEE GLOB CONF SIG, P945, DOI 10.1109/GlobalSIP.2013.6737048; Wagadarikar AA, 2009, OPT EXPRESS, V17, P6368, DOI 10.1364/OE.17.006368; Wang LZ, 2019, IEEE T PATTERN ANAL, V41, P857, DOI 10.1109/TPAMI.2018.2817496; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xin Yuan, 2020, Arxiv, DOI arXiv:2005.08028; Yang JB, 2015, IEEE T IMAGE PROCESS, V24, P106, DOI 10.1109/TIP.2014.2365720; Yang JB, 2014, IEEE T IMAGE PROCESS, V23, P4863, DOI 10.1109/TIP.2014.2344294; Yang PH, 2020, IEEE T IMAGE PROCESS, V29, P6466, DOI 10.1109/TIP.2020.2989550; Yang S, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P641; Yoshida M, 2018, LECT NOTES COMPUT SC, V11214, P649, DOI 10.1007/978-3-030-01249-6_39; Yuan X, 2021, IEEE SIGNAL PROC MAG, V38, P65, DOI 10.1109/MSP.2020.3023869; Yuan X, 2020, PROC CVPR IEEE, P1444, DOI 10.1109/CVPR42600.2020.00152; Yuan X, 2018, OPT EXPRESS, V26, P1962, DOI 10.1364/OE.26.001962; Yuan X, 2016, IEEE IMAGE PROC, P2539, DOI 10.1109/ICIP.2016.7532817; Yuan X, 2016, APPL OPTICS, V55, P7556, DOI 10.1364/AO.55.007556; Yuan X, 2016, BIOMED OPT EXPRESS, V7, P746, DOI 10.1364/BOE.7.000746; Yuan X, 2015, IEEE J-STSP, V9, P964, DOI 10.1109/JSTSP.2015.2411575; Yuan X, 2013, IEEE IMAGE PROC, P14, DOI 10.1109/ICIP.2013.6738004; Yuan X, 2014, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2014.424; Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P7735, DOI 10.1109/TIP.2020.3005515; Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P5094, DOI 10.1109/TIP.2020.2972109; Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891; Zhang L, 2017, IEEE SIGNAL PROC MAG, V34, P172, DOI 10.1109/MSP.2017.2717489; Zheng SM, 2021, PHOTONICS RES, V9, pB18, DOI 10.1364/PRJ.411745; Ziheng Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P258, DOI 10.1007/978-3-030-58586-0_16; Ziyi Meng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P187, DOI 10.1007/978-3-030-58592-1_12	72	1	1	15	22	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					7093	7111		10.1109/TPAMI.2021.3099035	http://dx.doi.org/10.1109/TPAMI.2021.3099035			19	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34310288	Green Submitted			2022-12-18	WOS:000853875300087
J	Zeng, RH; Huang, WB; Tan, MK; Rong, Y; Zhao, PL; Huang, JZ; Gan, C				Zeng, Runhao; Huang, Wenbing; Tan, Mingkui; Rong, Yu; Zhao, Peilin; Huang, Junzhou; Gan, Chuang			Graph Convolutional Module for Temporal Action Localization in Videos	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Temporal action localization; graph convolutional networks; video analysis		Temporal action localization, which requires a machine to recognize the location as well as the category of action instances in videos, has long been researched in computer vision. The main challenge of temporal action localization lies in that videos are usually long and untrimmed with diverse action contents involved. Existing state-of-the-art action localization methods divide each video into multiple action units (i.e., proposals in two-stage methods and segments in one-stage methods) and then perform action recognition/regression on each of them individually, without explicitly exploiting their relations during learning. In this paper, we claim that the relations between action units play an important role in action localization, and a more powerful action detector should not only capture the local content of each action unit but also allow a wider field of view on the context related to it. To this end, we propose a general graph convolutional module (GCM) that can be easily plugged into existing action localization methods, including two-stage and one-stage paradigms. To be specific, we first construct a graph, where each action unit is represented as a node and their relations between two action units as an edge. Here, we use two types of relations, one for capturing the temporal connections between different action units, and the other one for characterizing their semantic relationship. Particularly for the temporal connections in two-stage methods, we further explore two different kinds of edges, one connecting the overlapping action units and the other one connecting surrounding but disjointed units. Upon the graph we built, we then apply graph convolutional networks (GCNs) to model the relations among different action units, which is able to learn more informative representations to enhance action localization. Experimental results show that our GCM consistently improves the performance of existing action localization methods, including two-stage methods (e.g., CBR [15] and R-C3D [47]) and one-stage methods (e.g., D-SSAD [22]), verifying the generality and effectiveness of our GCM. Moreover, with the aid of GCM, our approach significantly outperforms the state-of-the-art on THUMOS14 (50.9 percent versus 42.8 percent). Augmentation experiments on ActivityNet also verify the efficacy of modeling the relationships between action units. The source code and the pre-trained models are available at https://github.com/Alvin-Zeng/GCM.	[Zeng, Runhao; Tan, Mingkui] South China Univ Technol, Sch Software Engn, Guangzhou 510641, Guangdong, Peoples R China; [Tan, Mingkui] South China Univ Technol, Key Lab Big Data & Intelligent Robot, Minist Educ, Guangzhou 510641, Peoples R China; [Zeng, Runhao] Pazhou Lab, Guangzhou 510335, Guangdong, Peoples R China; [Huang, Wenbing] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China; [Huang, Wenbing] Tsinghua Natl Lab Informat Sci & Technol TNList, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China; [Rong, Yu; Zhao, Peilin; Huang, Junzhou] Tencent AI Lab, Shenzhen, Peoples R China; [Gan, Chuang] MIT IBM Watson AI Lab, Cambridge, MA 02142 USA	South China University of Technology; South China University of Technology; Pazhou Lab; Tsinghua University; Tsinghua University; Tencent	Tan, MK (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou 510641, Guangdong, Peoples R China.	runhaozeng.cs@gmail.com; hwenbing@126.com; mingkuitan@gmail.com; yu.rong@hotmail.com; peilinzhao@hotmail.com; jzhuang@uta.edu; ganchuang1990@gmail.com			Scientific Innovation 2030 Major Project for New Generation of AI [2020AAA0107300]; Ministry of Science and Technology of the People's Republic of China; National Natural Science Foundation of China (NSFC) [62072190]; Key-Area Research and Development Program of Guangdong Province [2018B010107001]; Program for Guangdong Introducing Innovative and Enterpreneurial Teams [2017ZT07X183]	Scientific Innovation 2030 Major Project for New Generation of AI; Ministry of Science and Technology of the People's Republic of China(Ministry of Science and Technology, China); National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Key-Area Research and Development Program of Guangdong Province; Program for Guangdong Introducing Innovative and Enterpreneurial Teams	This work was partially supported by the Scientific Innovation 2030 Major Project for New Generation of AI under Grant 2020AAA0107300, Ministry of Science and Technology of the People's Republic of China, National Natural Science Foundation of China (NSFC) 62072190, Key-Area Research and Development Program of Guangdong Province 2018B010107001, Program for Guangdong Introducing Innovative and Enterpreneurial Teams 2017ZT07X183. Runhao Zeng andWenbing Huang contributed equally to this work.	Alwassel H, 2018, LECT NOTES COMPUT SC, V11213, P253, DOI 10.1007/978-3-030-01240-3_16; Bowen Zhang, 2016, Arxiv, DOI arXiv:1608.00797; Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675; Buch Shyamal, 2017, BMVC, P2; Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124; Chen XL, 2018, PROC CVPR IEEE, P7239, DOI 10.1109/CVPR.2018.00756; Cheng JH, 2018, CHI 2018: EXTENDED ABSTRACTS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3170427.3188467; Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47; Fan LJ, 2018, PROC CVPR IEEE, P6016, DOI 10.1109/CVPR.2018.00630; Gao JY, 2018, LECT NOTES COMPUT SC, V11206, P70, DOI 10.1007/978-3-030-01216-8_5; Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392; Gao Jiyang, 2017, ARXIV170501180; Gleason J, 2019, IEEE WINT CONF APPL, P141, DOI 10.1109/WACV.2019.00021; Hamilton WL, 2017, ADV NEUR IN, V30; Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211; Hou R., 2017, PROC BRIT MACH VIS C; Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378; Huang JJ, 2018, AAAI CONF ARTIF INTE, P6951; Huang WB, 2018, ADV NEUR IN, V31; Huang YP, 2019, IEEE INT CON MULTI, P1288, DOI 10.1109/ICME.2019.00224; Jiang Y.-G., 2014, THUMOS CHALLENGE ACT; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399; Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1; Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343; Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2; Montes Alberto, 2016, ARXIV160808128; Oneata Dan, 2014, LEAR SUBMISSION THUM, P2; Piergiovanni AJ, 2019, PR MACH LEARN RES, V97; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Richard A, 2016, PROC CVPR IEEE, P3131, DOI 10.1109/CVPR.2016.341; Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155; Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119; Simonyan K, 2014, ADV NEUR IN, V27; Singh G., 2016, ACTIVITYNET LARGE SC, P1; Vaswani A, 2017, ADV NEUR IN, V30; Wang L., 2014, ACTION RECOGNITION D; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678; Wang R., 2016, ACTIVITYNET LARGE SC, P1; Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Xu H, 2019, PROC CVPR IEEE, P9290, DOI 10.1109/CVPR.2019.00952; Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617; Xu M., 2020, CVPR, P10156; Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444; Yang K, 2018, AAAI CONF ARTIF INTE, P7477; Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293; Yuan J, 2016, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR.2016.337; Yuan ZH, 2017, PROC CVPR IEEE, P3215, DOI 10.1109/CVPR.2017.342; Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719; Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317	56	1	1	10	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6209	6223		10.1109/TPAMI.2021.3090167	http://dx.doi.org/10.1109/TPAMI.2021.3090167			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34138701	Green Submitted			2022-12-18	WOS:000853875300028
J	Zhou, P; Yuan, XT; Lin, ZC; Hoi, SCH				Zhou, Pan; Yuan, Xiao-Tong; Lin, Zhouchen; Hoi, Steven C. H.			A Hybrid Stochastic-Deterministic Minibatch Proximal Gradient Method for Efficient Optimization and Generalization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Convex optimization; precondition; online convex optimization; stochastic variance-reduced algorithm		Despite the success of stochastic variance-reduced gradient (SVRG) algorithms in solving large-scale problems, their stochastic gradient complexity often scales linearly with data size and is expensive for huge data. Accordingly, we propose a hybrid stochastic-deterministic minibatch proximal gradient (HSDMPG) algorithm for strongly convex problems with linear prediction structure, e.g., least squares and logistic/softmax regression. HSDMPG enjoys improved computational complexity that is data-size-independent for large-scale problems. It iteratively samples an evolving minibatch of individual losses to estimate the original problem, and can efficiently minimize the sampled subproblems. For strongly convex loss of n components, HSDMPG attains an epsilon-optimization-error within O(kappa log(zeta+1) (1/epsilon) 1/epsilon Lambda nlog(zeta) (1/epsilon) stochastic gradient evaluations, where kappa is condition number, zeta - 1 for quadratic loss and zeta - 2 for generic loss. For large- scale problems, our complexity outperforms those of SVRG-type algorithms with/without dependence on data size. Particularly, when epsilon = O(1/ root n) which matches the intrinsic excess error of a learning model and is sufficient for generalization, our complexity for quadratic and generic losses is respectively O(n(0.5) log(2)(n)) and O(n(0.5)log(3)(n)), which for the first time achieves optimal generalization in less than a single pass over data. Besides, we extend HSDMPG to online strongly convex problems and prove its higher efficiency over the prior algorithms. Numerical results demonstrate the computational advantages of HSDMPG.	[Zhou, Pan] Salesforce, Singapore 138522, Singapore; [Zhou, Pan] Sea AI Lab Sea Grp, Singapore 138522, Singapore; [Yuan, Xiao-Tong] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Peoples R China; [Lin, Zhouchen] Peking Univ, Sch EECS, Key Lab Machine Percept MoE, Beijing 100871, Peoples R China; [Lin, Zhouchen] Pazhou Lab, Guangzhou 510330, Peoples R China; [Hoi, Steven C. H.] Salesforce Res, Singapore 210044, Singapore	Salesforce; Nanjing University of Information Science & Technology; Peking University; Pazhou Lab; Salesforce	Yuan, XT (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Peoples R China.	panzhou3@gmail.com; xtyuan@nuist.edu.cn; zlin@pku.edu.cn; shoi@salesforce.com			National Key Research and Development Program of China [2018AAA0100400]; Natural Science Foundation of China (NSFC) [61876090, 61936005]; Key-Area Research and Development Program of Guangdong Province [2019B121204008]; NSF China [61625301, 61731018, 2020BD006]; PKU-Baidu Fund	National Key Research and Development Program of China; Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Key-Area Research and Development Program of Guangdong Province; NSF China(National Natural Science Foundation of China (NSFC)); PKU-Baidu Fund	The authors would like to sincerely thank the anonymous reviewers for their constructive comments on this work. The work of Xiao-Tong Yuan was supported in part by the National Key Research and Development Program of China under Grant 2018AAA0100400 and in part by the Natural Science Foundation of China (NSFC) under Grants 61876090 and 61936005. The work of Zhouchen Lin was supported in part by the Key-Area Research and Development Program of Guangdong Province under Grant 2019B121204008, in part by the NSF China under Grants 61625301 and 61731018, and in part by Project 2020BD006 supported by PKU-Baidu Fund.	Alexander Rakhlin, 2012, Arxiv, DOI arXiv:1109.5647; Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448; [Anonymous], 2019, STAT LEARNING SPARSI; Bach F., 2013, ADV NEURAL INFORM PR, V26, P773; Bottou L., 1991, P NEURONIMES; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Boyarshinov V., 2005, THESIS RENSSELAER PO; Cauchy A. L., 1847, COMP REND SCI PARIS, V25, P536; Defazio A, 2014, ADV NEUR IN, V27; Dieuleveut A, 2017, J MACH LEARN RES, V18; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Feldman V., 2019, C LEARN THEOR, V99, P1270; Friedlander MP, 2012, SIAM J SCI COMPUT, V34, pA1380, DOI 10.1137/110830629; Hardt M, 2016, PR MACH LEARN RES, V48; Hendrikx H., 2019, ARXIV; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Jovanovic M, 2019, ARXIV; Lan GH, 2019, ADV NEUR IN, V32; Lin C.-H., 2017, CONVEX OPTIMIZATION; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Lin Q., 2014, ADV NEURAL INFORM PR, P3059; Mattingley J, 2010, IEEE SIGNAL PROC MAG, V27, P50, DOI 10.1109/MSP.2010.936020; Mokhtari A., 2017, P C NEUR INF PROC SY, P2060; Mokhtari A., 2016, ADV NEURAL INFORM PR, P4062; Monga V., 2017, HDB CONVEX OPTIMIZAT; Nitanda A, 2016, JMLR WORKSH CONF PRO, V51, P195; Palomar D.P., 2010, CONVEX OPTIMIZATION; Pennanen T, 2012, MATH PROGRAM, V134, P157, DOI 10.1007/s10107-012-0573-4; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Shalev-Shwartz S., 2009, P C LEARN THEOR, P113; Shalev-Shwartz S, 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shalev-Shwartz S, 2014, PR MACH LEARN RES, V32; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Shamir O, 2014, PR MACH LEARN RES, V32, P1000; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Vapnik V.N., 2006, ESTIMATION DEPENDENC, VVolume 40; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Yuan XM, 2020, J MACH LEARN RES, V21; Zhang YC, 2015, PR MACH LEARN RES, V37, P353; Zhou P., 2020, PROC INT C MACH LEAR, P11556; Zhou P., 2018, PROC INT C LEARN REP, P1; Zhou P, 2018, PR MACH LEARN RES, V80; Zhou P, 2021, IEEE T PATTERN ANAL, V43, P1718, DOI 10.1109/TPAMI.2019.2954874; Zhou P, 2018, ADV NEUR IN, V31; Zhou P, 2017, PROC CVPR IEEE, P3938, DOI 10.1109/CVPR.2017.419	48	1	1	3	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					5933	5946		10.1109/TPAMI.2021.3087328	http://dx.doi.org/10.1109/TPAMI.2021.3087328			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34101583				2022-12-18	WOS:000853875300010
J	Chien, JT; Wang, CW				Chien, Jen-Tzung; Wang, Chun-Wei			Hierarchical and Self-Attended Sequence Autoencoder	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Decoding; Stochastic processes; Training; Semantics; Recurrent neural networks; Natural languages; Data models; Sequence generation; recurrent neural network; variational autoencoder; hierarchical model; self attention		It is important and challenging to infer stochastic latent semantics for natural language applications. The difficulty in stochastic sequential learning is caused by the posterior collapse in variational inference. The input sequence is disregarded in the estimated latent variables. This paper proposes three components to tackle this difficulty and build the variational sequence autoencoder (VSAE) where sufficient latent information is learned for sophisticated sequence representation. First, the complementary encoders based on a long short-term memory (LSTM) and a pyramid bidirectional LSTM are merged to characterize global and structural dependencies of an input sequence, respectively. Second, a stochastic self attention mechanism is incorporated in a recurrent decoder. The latent information is attended to encourage the interaction between inference and generation in an encoder-decoder training procedure. Third, an autoregressive Gaussian prior of latent variable is used to preserve the information bound. Different variants of VSAE are proposed to mitigate the posterior collapse in sequence modeling. A series of experiments are conducted to demonstrate that the proposed individual and hybrid sequence autoencoders substantially improve the performance for variational sequential learning in language modeling and semantic understanding for document classification and summarization.	[Chien, Jen-Tzung; Wang, Chun-Wei] Natl Chiao Tung Univ, Dept Elect & Comp Engn, Hsinchu 30010, Taiwan	National Yang Ming Chiao Tung University	Chien, JT (corresponding author), Natl Chiao Tung Univ, Dept Elect & Comp Engn, Hsinchu 30010, Taiwan.	jtchien@nctu.edu.tw; wangcw@chien.cm.nctu.edu.tw		Chien, Jen-Tzung/0000-0003-3466-8941	Ministry of Science and Technology, Taiwan [MOST 110-2634-F-009-016]	Ministry of Science and Technology, Taiwan(Ministry of Science and Technology, TaiwanMinistry of Science, ICT & Future Planning, Republic of Korea)	Thiswork was supported by theMinistry of Science and Technology, Taiwan under GrantMOST 110-2634-F-009-016.	Aksan E., 2019, ARXIV190206568; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Burda Yuri, 2016, 4 INT C LEARN REPR I; Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621; Chang YL, 2009, INT CONF ACOUST SPEE, P1689, DOI 10.1109/ICASSP.2009.4959927; Chien J.-T., 2020, IEEE IJCNN, P1; Chien JT, 2019, INTERSPEECH, P1318, DOI 10.21437/Interspeech.2019-1548; Chien JT, 2019, INT CONF ACOUST SPEE, P3202, DOI 10.1109/ICASSP.2019.8683771; Chung J, 2015, ADV NEUR IN, V28; Dieng A. B., 2019, P INT C ART INT STAT, P2397; Gal Y, 2016, ADV NEUR IN, V29; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gui T, 2019, AAAI CONF ARTIF INTE, P6481; Higgins I., 2017, ICLR; Kim Yoon, 2018, P MACH LEARN RES PML, P2683; Kingma DP, 2 INT C LEARN REPR I, P1; Lian RZ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5081; Lin Z., 2017, ARXIV PREPRINT ARXIV; Louizos C., 2016, 4 INT C LEARN REPR I; Merity Stephen, 2018, INT C LEARN REPR; Mikolov T, 2012, IEEE W SP LANG TECH, P234, DOI 10.1109/SLT.2012.6424228; Razavi A., 2019, PROC INT C LEARN REP; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Serdyuk D., 2018, PROC INT C LEARN REP; Shen DH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2079; Sonderby CK, 2016, ADV NEUR IN, V29; Tomczak JM, 2018, PR MACH LEARN RES, V84; van den Oord A, 2017, ADV NEUR IN, V30; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vaswani A, 2017, ADV NEUR IN, V30; Wang Wenlin, 2019, PROC C N AM CHAPTER, P166; Xu Jiacheng, 2018, P EMP METH NAT LANG, P4503, DOI DOI 10.18653/V1/D18-1480; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang Z, 2016, P 2016 C N AM CHAPTE, P1480	37	1	1	3	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4975	4986		10.1109/TPAMI.2021.3068187	http://dx.doi.org/10.1109/TPAMI.2021.3068187			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33755556				2022-12-18	WOS:000836666600037
J	Ke, RH; Schonlieb, CB				Ke, Rihuan; Schonlieb, Carola-Bibiane			Unsupervised Image Restoration Using Partially Linear Denoisers	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Noise measurement; Noise reduction; TV; Image denoising; Training; Image restoration; Neural networks; Image denoising; deep learning; convolutional neural networks; unsupervised learning; partially linear denoiser	PARAMETER-ESTIMATION; NOISE REMOVAL; FRAMEWORK	Deep neural network based methods are the state of the art in various image restoration problems. Standard supervised learning frameworks require a set of noisy measurement and clean image pairs for which a distance between the output of the restoration model and the ground truth, clean images is minimized. The ground truth images, however, are often unavailable or very expensive to acquire in real-world applications. We circumvent this problem by proposing a class of structured denoisers that can be decomposed as the sum of a nonlinear image-dependent mapping, a linear noise-dependent term and a small residual term. We show that these denoisers can be trained with only noisy images under the condition that the noise has zero mean and known variance. The exact distribution of the noise, however, is not assumed to be known. We show the superiority of our approach for image denoising, and demonstrate its extension to solving other restoration problems such as image deblurring where the ground truth is not available. Our method outperforms some recent unsupervised and self-supervised deep denoising models that do not require clean images for their training. For deblurring problems, the method, using only one noisy and blurry observation per image, reaches a quality not far away from its fully supervised counterparts on a benchmark dataset.	[Ke, Rihuan; Schonlieb, Carola-Bibiane] Univ Cambridge, DAMTP, Cambridge CB3 0WA, England	University of Cambridge	Ke, RH (corresponding author), Univ Cambridge, DAMTP, Cambridge CB3 0WA, England.	rk621@cam.ac.uk; cbs31@cam.ac.uk			EPSRC [EP/T003553/1, EP/S026045/1, EP/T017961/1, EP/N014588/1]; EPSRC IAA Partnership Development Awards; Leverhulme Trust project on "Breaking the non-convexity barrier"; Wolfson fellowship from the Royal Society; RISE Project CHiPS; RISE Project NoMADS; Cantab Capital Institute for the Mathematics of Information; Alan Turing Institute; Wellcome Innovator Award [RG98755]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC IAA Partnership Development Awards; Leverhulme Trust project on "Breaking the non-convexity barrier"(Leverhulme Trust); Wolfson fellowship from the Royal Society; RISE Project CHiPS; RISE Project NoMADS; Cantab Capital Institute for the Mathematics of Information; Alan Turing Institute; Wellcome Innovator Award	The work of Rihuan Ke was supported by the EPSRC under Grant EP/T003553/1 and the EPSRC IAA Partnership Development Awards. The work of Carola-Bibiane Sch_onlieb was supported in part by the Leverhulme Trust project on "Breaking the non-convexity barrier," in part by the Philip Leverhulme Prize, in part by the Wolfson fellowship from the Royal Society, in part by the EPSRC under Grant EP/S026045/1, EP/T017961/1, and EP/T003553/1, in part by the EPSRC Centre under Grant EP/N014588/1, in part by the RISE Projects CHiPS and NoMADS, in part by the Cantab Capital Institute for the Mathematics of Information, in part by the Alan Turing Institute, in part by the Wellcome Innovator Award under Grant RG98755.	Acito N, 2011, IEEE T GEOSCI REMOTE, V49, P2957, DOI 10.1109/TGRS.2011.2110657; Alexander Krull, 2019, Arxiv, DOI arXiv:1906.00651; Batson J, 2019, PR MACH LEARN RES, V97; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Chan RH, 2005, IEEE T IMAGE PROCESS, V14, P1479, DOI 10.1109/TIP.2005.852196; Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Ehret T, 2019, PROC CVPR IEEE, P11361, DOI 10.1109/CVPR.2019.01163; Foi A, 2008, IEEE T IMAGE PROCESS, V17, P1737, DOI 10.1109/TIP.2008.2001399; Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186; Han Y, 2018, IEEE T MED IMAGING, V37, P1418, DOI 10.1109/TMI.2018.2823768; Jin KH, 2017, IEEE T IMAGE PROCESS, V26, P4509, DOI 10.1109/TIP.2017.2713099; Kingma D.P, P 3 INT C LEARNING R; Krull A, 2019, PROC CVPR IEEE, P2124, DOI 10.1109/CVPR.2019.00223; Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Lefkimmiatis S, 2018, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR.2018.00338; Lehtinen J, 2018, PR MACH LEARN RES, V80; Liu C., 2006, P IEEE COMP SOC C CV, V1, P901, DOI DOI 10.1109/CVPR.2006.207; Liu D, 2018, ADV NEUR IN, V31; Liu XH, 2014, IEEE T IMAGE PROCESS, V23, P4361, DOI 10.1109/TIP.2014.2347204; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; MALLAT S, 1992, IEEE T INFORM THEORY, V38, P617, DOI 10.1109/18.119727; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Michael Elad, 2019, Arxiv, DOI arXiv:1805.02158; Moran Nick, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12061, DOI 10.1109/CVPR42600.2020.01208; Romano Y, 2017, SIAM J IMAGING SCI, V10, P1804, DOI 10.1137/16M1102884; Roth S, 2005, PROC CVPR IEEE, P860; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Shen ZY, 2018, PROC CVPR IEEE, P8260, DOI 10.1109/CVPR.2018.00862; Soltanayev S, 2018, ADV NEUR IN, V31; Sreehari S, 2016, IEEE T COMPUT IMAG, V2, P408, DOI 10.1109/TCI.2016.2599778; Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Valery D., 2020, ARXIV; Venkatakrishnan S, 2013, IEEE GLOB CONF SIG, P945, DOI 10.1109/GlobalSIP.2013.6737048; Weickert J., 1998, ANISOTROPIC DIFFUSIO, V1; Xia Z., 2019, P ADV NEUR INF PROC, P2439; Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zhang YL, 2021, IEEE T PATTERN ANAL, V43, P2480, DOI 10.1109/TPAMI.2020.2968521; Zhussip M, 2019, ADV NEUR IN, V32; Zuo WM, 2016, IEEE T IMAGE PROCESS, V25, P1751, DOI 10.1109/TIP.2016.2531905	44	1	1	18	18	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5796	5812		10.1109/TPAMI.2021.3070382	http://dx.doi.org/10.1109/TPAMI.2021.3070382			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33819148	Green Submitted			2022-12-18	WOS:000836666600090
J	Kim, J; Yoon, H; Kim, MS				Kim, Jinwook; Yoon, Heeyong; Kim, Min-Soo			Tweaking Deep Neural Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Synapses; Biological neural networks; Neurons; Artificial neural networks; Training; Training data; Arrays; Deep neural networks; synaptic join	CLASSIFICATION	Deep neural networks are trained so as to achieve a kind of the maximum overall accuracy through a learning process using given training data. Therefore, it is difficult to fix them to improve the accuracies of specific problematic classes or classes of interest that may be valuable to some users or applications. To address this issue, we propose the synaptic join method to tweak neural networks by adding certain additional synapses from the intermediate hidden layers to the output layer across layers and additionally training only these synapses, if necessary. To select the most effective synapses, the synaptic join method evaluates the performance of all the possible candidate synapses between the hidden neurons and output neurons based on the distribution of all the possible proper weights. The experimental results show that the proposed method can effectively improve the accuracies of specific classes in a controllable way.	[Kim, Jinwook; Yoon, Heeyong] DGIST, Dept Informat & Commun Engn, Daegu 42988, South Korea; [Kim, Min-Soo] Korea Adv Inst Sci & Technol, Sch Comp, Daejeon 34141, South Korea	Daegu Gyeongbuk Institute of Science & Technology (DGIST); Korea Advanced Institute of Science & Technology (KAIST)	Kim, MS (corresponding author), Korea Adv Inst Sci & Technol, Sch Comp, Daejeon 34141, South Korea.	bm010515@dgist.ac.kr; sunrise2575@dgist.ac.kr; minsoo.k@kaist.ac.kr			National Research Foundation of Korea (NRF) - Korea government (MIST) [2018R1A5A1060031]; Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Science, ICT and Future Planning [2017R1E1A1A01077630]; Samsung Research Funding Center of Samsung Electronics [SRFC-IT1502-10]	National Research Foundation of Korea (NRF) - Korea government (MIST)(National Research Foundation of Korea); Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Science, ICT and Future Planning(National Research Foundation of KoreaMinistry of Science, ICT & Future Planning, Republic of Korea); Samsung Research Funding Center of Samsung Electronics(Samsung)	This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MIST) (No.2018R1A5A1060031), Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT and Future Planning (2017R1E1A1A01077630), and Samsung Research Funding Center of Samsung Electronics under ProjectNumber SRFC-IT1502-10.	[Anonymous], 3 LAYER CNN CIFAR10; [Anonymous], RESNET18 CIFAR 100; Barret Zoph, 2017, Arxiv, DOI arXiv:1611.01578; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Elsken T, 2019, J MACH LEARN RES, V20; Garcia-Molina H., 2008, DATABASE SYSTEMS COM; Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042; Guo YW, 2016, ADV NEUR IN, V29; Han S., 2016, P 4 INT C LEARN REPR, P1; Han S, 2015, ADV NEUR IN, V28; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jozefowicz R, 2015, PR MACH LEARN RES, V37, P2342; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Li H., 2017, P INT C LEARN REPR I, P1; Lin M., 2014, PROC INT C LEARN REP; Liu Hanxiao, 2017, ARXIV; Miikkulainen R, 2019, ARTIFICIAL INTELLIGENCE IN THE AGE OF NEURAL NETWORKS AND BRAIN COMPUTING, P293, DOI 10.1016/B978-0-12-815480-9.00015-3; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Pham H, 2018, PR MACH LEARN RES, V80; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293; Wang YC, 2017, ADV NEUR IN, V30; Wang YX, 2017, PROC CVPR IEEE, P3029, DOI 10.1109/CVPR.2017.323; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Zhuang L., DENSENET 40 MODEL RE; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	35	1	1	5	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5715	5728		10.1109/TPAMI.2021.3079511	http://dx.doi.org/10.1109/TPAMI.2021.3079511			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33979278	hybrid			2022-12-18	WOS:000836666600085
J	Liu, XF; Chao, Y; You, JJ; Kuo, CCJ; Kumar, BVKV				Liu, Xiaofeng; Chao, Yang; You, Jane J.; Kuo, C-C Jay; Kumar, B. V. K. Vijaya			Mutual Information Regularized Feature-Level Frankenstein for Discriminative Recognition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature-level disentanglement; discriminative recognition; mutual information; adversarial learning		Deep learning recognition approaches can potentially perform better if we can extract a discriminative representation that controllably separates nuisance factors. In this paper, we propose a novel approach to explicitly enforce the extracted discriminative representation d, extracted latent variation l (e,g., background, unlabeled nuisance attributes), and semantic variation label vector s (e.g., labeled expressions/pose) to be independent and complementary to each other. We can cast this problem as an adversarial game in the latent space of an auto-encoder. Specifically, with the to-be-disentangled s, we propose to equip an end-to-end conditional adversarial network with the ability to decompose an input sample into d and 1. However, we argue that maximizing the cross-entropy loss of semantic variation prediction from d is not sufficient to remove the impact of s from d, and that the uniform-target and entropy regularization are necessary. A collaborative mutual information regularization framework is further proposed to avoid unstable adversarial training. It is able to minimize the differentiable mutual information between the variables to enforce independence. The proposed discriminative representation inherits the desired tolerance property guided by prior knowledge of the task. Our proposed framework achieves top performance on diverse recognition tasks, including digits classification, large-scale face recognition on LFW and IJB-A datasets, and face recognition tolerant to changes in lighting, makeup, disguise, etc.	[Liu, Xiaofeng; Kumar, B. V. K. Vijaya] Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA; [Liu, Xiaofeng] Harvard Univ, Harvard Med Sch, Fanhan Tech, Cambridge, MA 02138 USA; [Chao, Yang; Kuo, C-C Jay] Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90007 USA; [Chao, Yang] Facebook AI, Menlo Pk, CA 94025 USA; [You, Jane J.] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China; [Kumar, B. V. K. Vijaya] Carnegie Mellon Univ Africa, BP 6150, Kigali, Rwanda	Carnegie Mellon University; Harvard University; University of Southern California; Facebook Inc; Hong Kong Polytechnic University	Kumar, BVKV (corresponding author), Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.	xliu61@mgh.harvard.edu; harryyang.hk@gmail.com; csyjia@comp.polyu.edu.hk; cckuo@sipi.usc.edu; kumar@ece.cmu.edu			Hong Kong GRF [152202/14E]; PolyU Central Research [G-YBJW]; Jiangsu Youth Programme [BK20200238]	Hong Kong GRF(Hong Kong Research Grants Council); PolyU Central Research; Jiangsu Youth Programme	This work was supported in part by the Hong Kong GRF under Grant 152202/14E, in part by the PolyU Central Research under Grant G-YBJW, and in part by the Jiangsu Youth Programme under Grant BK20200238.	Aaron van den Oord, 2019, Arxiv, DOI arXiv:1807.03748; Adam Trischler, 2019, Arxiv, DOI arXiv:1808.06670; Alireza Makhzani, 2016, Arxiv, DOI arXiv:1511.05644; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bao JM, 2018, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2018.00702; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Bengio Y., 2013, ARXIV; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Cao J, 2020, INT J COMPUT VISION, V128, P1485, DOI 10.1007/s11263-019-01229-6; Cao J, 2018, ADV NEUR IN, V31; Choi Jinwoo, 2019, ARXIV191205534; Christos Louizos, 2017, Arxiv, DOI arXiv:1511.00830; Daniel M., 2018, PROC 32 INT C NEURAL; Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482; Dong Yi, 2014, Arxiv, DOI arXiv:1411.7923; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; github, GANS COMP CHERRY PIC; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo GD, 2014, IEEE T CIRC SYST VID, V24, P814, DOI 10.1109/TCSVT.2013.2280076; Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6; Hadad N, 2018, PROC CVPR IEEE, P772, DOI 10.1109/CVPR.2018.00087; He GW, 2020, IEEE COMPUT SOC CONF, P4147, DOI 10.1109/CVPRW50498.2020.00490; He KK, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P733; Hu GS, 2017, IEEE I CONF COMP VIS, P3764, DOI 10.1109/ICCV.2017.404; Hu JL, 2013, INT CONF ACOUST SPEE, P2342, DOI 10.1109/ICASSP.2013.6638073; Hu Q., 2017, ARXIV; Hu YB, 2018, PROC CVPR IEEE, P8398, DOI 10.1109/CVPR.2018.00876; Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267; Hyvarinen A, 1999, NEURAL NETWORKS, V12, P429, DOI 10.1016/S0893-6080(98)00140-3; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jiapeng Hong, 2018, Arxiv, DOI arXiv:1711.05415; Junbo Zhao, 2016, Arxiv, DOI arXiv:1506.02351; Karras Tero, 2018, INT C LEARN REPR; Kingma D.P, P 3 INT C LEARNING R; Kingma DP, 2014, ADV NEUR IN, V27; Kinney JB, 2014, P NATL ACAD SCI USA, V111, P3354, DOI 10.1073/pnas.1309933111; Klare BF, 2015, PROC CVPR IEEE, P1931, DOI 10.1109/CVPR.2015.7298803; Kushwaha V, 2018, IEEE COMPUT SOC CONF, P1, DOI 10.1109/CVPRW.2018.00008; Lample Guillaume, 2017, ARXIV170600409; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Y., 2014, ARXIV; Li Y., 2018, PROC AAAI C ARTIF IN, V32; Li Y, 2018, LECT NOTES COMPUT SC, V11219, P647, DOI 10.1007/978-3-030-01267-0_38; Li Y, 2015, IEEE I CONF COMP VIS, P3819, DOI 10.1109/ICCV.2015.435; Li Y, 2019, PATTERN RECOGN, V90, P99, DOI 10.1016/j.patcog.2019.01.013; LINSKER R, 1988, COMPUTER, V21, P105, DOI 10.1109/2.36; Liu MY, 2017, ADV NEUR IN, V30; Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713; Liu X., 2020, ARXIV; Liu X., 2021, PROC RECOGNIT PERCEP, P143; Liu XF, 2021, AAAI CONF ARTIF INTE, V35, P2189; Liu XF, 2021, INT C PATT RECOG, P7515, DOI 10.1109/ICPR48806.2021.9413284; Liu XF, 2021, INT C PATT RECOG, P7508, DOI 10.1109/ICPR48806.2021.9412820; Liu XF, 2019, IEEE I CONF COMP VIS, P4985, DOI 10.1109/ICCV.2019.00509; Liu XF, 2019, PROC CVPR IEEE, P637, DOI 10.1109/CVPR.2019.00073; Liu XF, 2020, IEEE T INF FOREN SEC, V15, P1501, DOI 10.1109/TIFS.2019.2938418; Liu XF, 2019, PATTERN RECOGN, V88, P1, DOI 10.1016/j.patcog.2018.11.001; Liu XF, 2018, LECT NOTES COMPUT SC, V11215, P573, DOI 10.1007/978-3-030-01252-6_34; Liu XF, 2017, IEEE COMPUT SOC CONF, P522, DOI 10.1109/CVPRW.2017.79; Liu Y, 2018, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR.2018.00394; Liu Y, 2018, PROC CVPR IEEE, P2080, DOI 10.1109/CVPR.2018.00222; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lu YX, 2017, PROC CVPR IEEE, P1131, DOI 10.1109/CVPR.2017.126; Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018; Mathieu M, 2016, ADV NEUR IN, V29; Mehdi Mirza, 2014, Arxiv, DOI arXiv:1411.1784; Belghazi MI, 2018, Arxiv, DOI arXiv:1801.04062; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272; Peng PX, 2018, IEEE T PATTERN ANAL, V40, P1625, DOI 10.1109/TPAMI.2017.2723882; Perarnau G, 2016, ARXIV; Philemon Brakel, 2017, Arxiv, DOI arXiv:1710.05050; Qian YC, 2019, PROC CVPR IEEE, P9843, DOI 10.1109/CVPR.2019.01008; Smirnov E, 2019, IEEE INT CONF COMP V, P551, DOI 10.1109/ICCVW.2019.00068; Sun Y, 2017, PATTERN RECOGN, V66, P153, DOI 10.1016/j.patcog.2017.01.011; Tenenbaum JB, 1997, ADV NEUR IN, V9, P662; Theis L, 2015, ARXIV; Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW); Tran L, 2019, IEEE T PATTERN ANAL, V41, P3007, DOI 10.1109/TPAMI.2018.2868350; Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Velickovic P etal, 2018, ARXIV, DOI DOI 10.48550/ARXIV.1809.10341; Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552; Wang J, 2021, MED IMAGE ANAL, V69, DOI 10.1016/j.media.2020.101942; Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032; Xie Q., 2017, PROC NEURAL INF PROC, P585; Yin X, 2017, IEEE I CONF COMP VIS, P4010, DOI 10.1109/ICCV.2017.430; Zemel R., 2013, P INT C MACH LEARN, P325; Zhang KP, 2018, IEEE COMPUT SOC CONF, P32, DOI 10.1109/CVPRW.2018.00012; Zhao J, 2020, INT J COMPUT VISION, V128, P460, DOI 10.1007/s11263-019-01252-7; Zheng ZZ, 2017, IEEE INT CONF AUTOMA, P918, DOI 10.1109/FG.2017.131; Zou Y, 2019, IEEE I CONF COMP VIS, P5981, DOI 10.1109/ICCV.2019.00608	95	1	1	4	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5243	5260		10.1109/TPAMI.2021.3077397	http://dx.doi.org/10.1109/TPAMI.2021.3077397			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33945470				2022-12-18	WOS:000836666600055
J	Llorente-Vidrio, D; Ballesteros, M; Salgado, I; Chairez, I				Llorente-Vidrio, D.; Ballesteros, M.; Salgado, I; Chairez, I			Deep Learning Adapted to Differential Neural Networks Used as Pattern Classification of Electrophysiological Signals	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Electroencephalography; Biological neural networks; Heuristic algorithms; Complexity theory; Topology; Task analysis; Stability analysis; Deep learning; differential neural networks; Lyapunov stability; EEG classification	FEATURE-SELECTION; RECOGNITION; INTERFACES	This manuscript presents the design of a deep differential neural network (DDNN) for pattern classification. First, we proposed a DDNN topology with three layers, whose learning laws are derived from a Lyapunov analysis, justifying local asymptotic convergence of the classification error and the weights of the DDNN. Then, an extension to include an arbitrary number of hidden layers in the DDNN is analyzed. The learning laws for this general form of the DDNN offer a contribution to the deep learning framework for signal classification with biological nature and dynamic structures. The DDNN is used to classify electroencephalographic signals from volunteers that perform an identification graphical test. The classification results show exponential growth in the signal classification accuracy from 82 percent with one layer to 100 percent with three hidden layers. Working with DDNN instead of static deep neural networks (SDNN) represents a set of advantages, such as processing time and training period reduction up to almost 100 times, and the increment of the classification accuracy while working with less hidden layers than working with SDNN, which are highly dependent on their topology and the number of neurons in each layer. The DDNN employed fewer neurons due to the induced feedback characteristic.	[Llorente-Vidrio, D.; Salgado, I] Inst Politecn Nacl, CIDETEC, Mexico City 07700, DF, Mexico; [Ballesteros, M.; Chairez, I] Inst Politecn Nacl, UPIBI, Lab Med Robot & Biosignals, Mexico City 07340, DF, Mexico; [Ballesteros, M.; Chairez, I] Tecnol Monterrey, Sch Engn & Sci, Guadalajara 44160, Jalisco, Mexico	Instituto Politecnico Nacional - Mexico; Instituto Politecnico Nacional - Mexico; Tecnologico de Monterrey	Ballesteros, M (corresponding author), Inst Politecn Nacl, UPIBI, Lab Med Robot & Biosignals, Mexico City 07340, DF, Mexico.	dusthon@hotmail.com; mballesterose@ipn.mx; ijesusr@gmail.com; isaac_chairez@yahoo.com	Escamilla, Mariana Felisa Ballesteros/U-7900-2019	Escamilla, Mariana Felisa Ballesteros/0000-0003-2879-4474	Instituto Politecnico Nacional [SIP-20210992, SIP-20211863, SIP-20210076]	Instituto Politecnico Nacional	This work was supported by the Instituto Politecnico Nacional through the research grants SIP-20210992, SIP-20211863, and SIP-20210076.	Ableitner T, 2018, PROCEDIA COMPUT SCI, V141, P442, DOI 10.1016/j.procs.2018.10.164; Agand P., 2019, PROC INT JOINT C NEU, P1; Alfaro-Ponce M, 2016, NEURAL NETWORKS, V79, P88, DOI 10.1016/j.neunet.2016.03.004; Alfaro-Ponce M, 2019, INT J MACH LEARN CYB, V10, P2283, DOI 10.1007/s13042-018-0867-9; Alfaro-Ponce M, 2019, NEURAL COMPUT APPL, V31, P363, DOI 10.1007/s00521-017-3051-3; Barham MP, 2017, PSYCHOPHYSIOLOGY, V54, P1393, DOI 10.1111/psyp.12888; Bernstein D. S, 2018, SCALAR VECTOR MATRIX; Bhatti MH, 2019, IEEE T IND INFORM, V15, P5747, DOI 10.1109/TII.2019.2925624; Bi LZ, 2013, IEEE T HUM-MACH SYST, V43, P161, DOI 10.1109/TSMCC.2012.2219046; Bos L., 2011, HDB DIGITAL HOMECARE; Brumberg JS, 2018, IEEE T NEUR SYS REH, V26, P874, DOI 10.1109/TNSRE.2018.2808425; Croce P, 2019, IEEE T BIO-MED ENG, V66, P2372, DOI 10.1109/TBME.2018.2889512; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; da Silva HP, 2014, COMPUT METH PROG BIO, V115, P20, DOI 10.1016/j.cmpb.2014.03.002; Gui K, 2019, IEEE-ASME T MECH, V24, P483, DOI 10.1109/TMECH.2019.2893055; Perez-Cruz JH, 2012, J APPL MATH, DOI 10.1155/2012/529176; Jiang T, 2019, INT J ROBUST NONLIN, V29, P4454, DOI 10.1002/rnc.4636; Joadder MAM, 2019, IRBM, V40, P297, DOI 10.1016/j.irbm.2019.05.004; Khalil HK., 2002, NONLINEAR SYSTEMS, Vthird; Lee TJ, 2013, 2013 13TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS 2013), P237, DOI 10.1109/ICCAS.2013.6703900; Liu B, 2020, IEEE T NEUR NET LEAR, V31, P1771, DOI 10.1109/TNNLS.2019.2921926; Mahmud M, 2018, IEEE T NEUR NET LEAR, V29, P2063, DOI 10.1109/TNNLS.2018.2790388; Martinez-Cagigal V, 2019, EXPERT SYST APPL, V120, P155, DOI 10.1016/j.eswa.2018.11.026; Masood N, 2017, ADV INTELL SYST COMP, V488, P133, DOI 10.1007/978-3-319-41691-5_12; NaitAli A, 2009, ADVANCED BIOSIGNAL PROCESSING, P1; Nourmohammadi A, 2018, IEEE T HUM-MACH SYST, V48, P337, DOI 10.1109/THMS.2018.2830647; Petrosian AA, 2001, CLIN NEUROPHYSIOL, V112, P1378, DOI 10.1016/S1388-2457(01)00579-X; Poznyak AS., 2001, DIFFERENTIAL NEURAL; Rejer I, 2013, ADV INTELL SYST, V226, P579, DOI 10.1007/978-3-319-00969-8_57; Richhariya B, 2018, EXPERT SYST APPL, V106, P169, DOI 10.1016/j.eswa.2018.03.053; Sakhavi S, 2018, IEEE T NEUR NET LEAR, V29, P5619, DOI 10.1109/TNNLS.2018.2789927; Schwartz AB, 2006, NEURON, V52, P205, DOI 10.1016/j.neuron.2006.09.019; Soman S, 2015, APPL SOFT COMPUT, V30, P305, DOI 10.1016/j.asoc.2015.01.018; Srinivasan V, 2007, IEEE T INF TECHNOL B, V11, P288, DOI 10.1109/TITB.2006.884369; Strbac M, 2017, IEEE T NEUR SYS REH, V25, P2133, DOI 10.1109/TNSRE.2017.2712287; Su BY, 2019, IEEE T NEUR SYS REH, V27, P1032, DOI 10.1109/TNSRE.2019.2909585; Nguyen T, 2015, EXPERT SYST APPL, V42, P4370, DOI 10.1016/j.eswa.2015.01.036; Vargas JAR, 2016, PROCEEDINGS OF 2016 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI); Ward C, 2019, J NEUROSCI METH, V311, P338, DOI 10.1016/j.jneumeth.2018.09.015; Wolpaw JR, 2002, CLIN NEUROPHYSIOL, V113, P767, DOI 10.1016/S1388-2457(02)00057-3; Wu YZ, 2018, IEEE T NEUR NET LEAR, V29, P807, DOI 10.1109/TNNLS.2017.2647811; Xu JH, 2018, COMPUT HUM BEHAV, V81, P340, DOI 10.1016/j.chb.2017.12.037; Zhang L, 2019, BIOMED SIGNAL PROCES, V49, P434, DOI 10.1016/j.bspc.2018.12.020; Zhang PB, 2019, IEEE T NEUR SYS REH, V27, P1149, DOI 10.1109/TNSRE.2019.2913400	45	1	1	15	18	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4807	4818		10.1109/TPAMI.2021.3066996	http://dx.doi.org/10.1109/TPAMI.2021.3066996			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33735073				2022-12-18	WOS:000836666600026
J	Mehta, D; Chen, TR; Tang, TT; Hauenstein, JD				Mehta, Dhagash; Chen, Tianran; Tang, Tingting; Hauenstein, Jonathan D.			The Loss Surface of Deep Linear Networks Viewed Through the Algebraic Geometry Lens	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Deep linear network; global optimization; regularization; numerical algebraic geometry	NEURAL-NETWORKS; POLYNOMIAL SYSTEMS; LOCAL MINIMA; HOMOTOPY; ROOTS; NUMBER; DYNAMICS; POINTS; ZEROS	By using the viewpoint of modern computational algebraic geometry, we explore properties of the optimization landscapes of deep linear neural network models. After providing clarification on the various definitions of "flat" minima, we show that the geometrically flat minima, which are merely artifacts of residual continuous symmetries of the deep linear networks, can be straightforwardly removed by a generalized L-2-regularization. Then, we establish upper bounds on the number of isolated stationary points of these networks with the help of algebraic geometry. Combining these upper bounds with a method in numerical algebraic geometry, we find allstationary points for modest depth and matrix size. We demonstrate that, in the presence of the non-zero regularization, deep linear networks can indeed possess local minima which are not global minima. Finally, we show that even though the number of stationary points increases as the number of neurons (regularization parameters) increases (decreases), higher index saddles are surprisingly rare.	[Mehta, Dhagash] Vanguard Grp, Valley Forge, PA 19087 USA; [Chen, Tianran] Auburn Univ, Dept Math, Montgomery, AL 36117 USA; [Tang, Tingting] San Diego State Univ, Dept Math & Stat, Imperial Valley Campus, Calexico, CA 92182 USA; [Hauenstein, Jonathan D.] Univ Notre Dame, Dept Appl & Computat Math & Stat, Notre Dame, IN 46556 USA	The Vanguard Group, Inc.; Auburn University System; Auburn University; California State University System; San Diego State University; University of Notre Dame	Tang, TT (corresponding author), San Diego State Univ, Dept Math & Stat, Imperial Valley Campus, Calexico, CA 92182 USA.	dhagashbmehta@gmail.com; ti@nranchen.org; ttang2@sdsu.edu; hauenstein@nd.edu	Hauenstein, Jonathan/H-9435-2015	Hauenstein, Jonathan/0000-0002-9252-8210	NSF [DMS-1923099, CCF 1812746]; ONR [N00014-16-1-2722]; ARO [W911NF-20-2-0218]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); ARO	Dhagash Mehta was at United Technologies Research Center, USA, when a major part of the work for this paper was completed. The work of Tianran Chen was supported by NSF under Grant DMS-1923099. The work of Tingting Tang was supported by NSF under Grant CCF 1812746. The work of Jonathan D. Hauenstein was supported in part by ONR under Grant N00014-16-1-2722 and in part by ARO under Grant W911NF-20-2-0218.	A.Emin Orhan, 2018, Arxiv, DOI arXiv:1701.09175; ABRAHAM R, 1967, TRANSVERSAL MAPPINGS; Adepu Ravi Sankar, 2017, Arxiv, DOI arXiv:1706.02052; Akira H., 2003, COMPLEX VALUED NEURA, V5; Alex Graves, 2016, Arxiv, DOI arXiv:1602.03032; Ali Jadbabaie, 2018, Arxiv, DOI arXiv:1707.02444; Ali Jadbabaie, 2019, Arxiv, DOI arXiv:1802.03487; Allgower E., 2003, INTRO NUMERICAL CONT, V45; Allgower E. L., 1979, INTRO NUMERICAL CONT; Anandkumar A., 2016, 29 ANN C LEARNING TH, P81; Andoni A, 2014, PR MACH LEARN RES, V32, P1908; Andrew M Saxe, 2018, Arxiv, DOI arXiv:1806.00730; Andrew M. Saxe, 2015, Arxiv, DOI arXiv:1412.6544; Andrew M. Saxe, 2014, Arxiv, DOI arXiv:1312.6120; Anna Choromanska, 2017, Arxiv, DOI arXiv:1611.01838; Arjovsky M, 2016, PR MACH LEARN RES, V48; Armentano D, 2009, BERNOULLI, V15, P249, DOI 10.3150/08-BEJ149; Azais JM, 2005, FOUND COMPUT MATH, V5, P125, DOI 10.1007/s10208-004-0119-0; Baldassi C, 2016, P NATL ACAD SCI USA, V113, pE7655, DOI 10.1073/pnas.1608103113; Baldassi C, 2016, PHYS REV E, V93, DOI 10.1103/PhysRevE.93.052313; Baldassi C, 2016, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2016/02/023301; BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; BALDI PF, 1995, IEEE T NEURAL NETWOR, V6, P837, DOI 10.1109/72.392248; Baldi P, 2012, NEURAL NETWORKS, V33, P136, DOI 10.1016/j.neunet.2012.04.011; Basu S., 2003, ALGORITHMS REAL ALGE; Bates D., 2017, PROC INT C MATH ASPE, P107; Bates DJ, 2013, SOFTW ENVIRON TOOLS; Benjamin Recht, 2017, Arxiv, DOI arXiv:1710.07406; BLUM AL, 1992, NEURAL NETWORKS, V5, P117, DOI 10.1016/S0893-6080(05)80010-3; Blum Lenore, 1998, COMPLEXITY REAL COMP; BROCKETT RW, 1976, IEEE T AUTOMAT CONTR, V21, P449, DOI 10.1109/TAC.1976.1101301; C.Daniel Freeman, 2017, Arxiv, DOI arXiv:1611.01540; CHEN AM, 1993, NEURAL COMPUT, V5, P910, DOI 10.1162/neco.1993.5.6.910; Choromanska A., 2014, EPRINT ARXIV; Chow J. C., 1992, PROC REV PROG QUANTI, P685; CHOW SN, 1978, MATH COMPUT, V32, P887, DOI 10.1090/S0025-5718-1978-0492046-9; Coetzee F. M., 1995, PHD DISSERTATION; Coetzee FM, 1997, ADV NEUR IN, V9, P410; Coetzee FM, 1996, IEEE T NEURAL NETWOR, V7, P307, DOI 10.1109/72.485634; Cox D., 2007, IDEALS VARIETIES ALG; Cox David, 1998, USING ALGEBRAIC GEOM, P1; Daniel Jiwoong Im, 2017, Arxiv, DOI arXiv:1612.04010; Daniel Soudry, 2018, Arxiv, DOI arXiv:1805.04938; Dauphin YN, 2014, ADV NEUR IN, V27; David J. Wales, 2018, Arxiv, DOI arXiv:1804.02411; David P. Reichert, 2014, Arxiv, DOI arXiv:1312.6115; Dedieu JP, 2008, J COMPLEXITY, V24, P89, DOI 10.1016/j.jco.2007.09.003; Dheevatsa Mudigere, 2017, Arxiv, DOI arXiv:1609.04836; Doye JPK, 2002, J CHEM PHYS, V116, P3777, DOI 10.1063/1.1436470; EDELMAN A, 1995, B AM MATH SOC, V32, P1, DOI 10.1090/S0273-0979-1995-00571-9; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Georgios Piliouras, 2016, Arxiv, DOI arXiv:1605.00405; GEORGIOU GM, 1992, IEEE T CIRCUITS-II, V39, P330, DOI 10.1109/82.142037; Gerard Ben Arous, 2015, Arxiv, DOI arXiv:1412.6615; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; GORI M, 1992, IEEE T PATTERN ANAL, V14, P76, DOI 10.1109/34.107014; Haihao Lu, 2017, Arxiv, DOI arXiv:1702.08580; Hirose A, 2012, IEEE T NEUR NET LEAR, V23, P541, DOI 10.1109/TNNLS.2012.2183613; Hochreiter S, 1997, NEURAL COMPUT, V9, P1, DOI 10.1162/neco.1997.9.1.1; Hochreiter S., 1995, Advances in Neural Information Processing Systems 7, P529; Huang DS, 2004, NEURAL COMPUT, V16, P1721, DOI 10.1162/089976604774201668; Huang ZY, 2019, MATH COMPUT SCI, V13, P461, DOI 10.1007/s11786-019-00394-8; HUBER B, 1995, MATH COMPUT, V64, P1541, DOI 10.2307/2153370; Hughes C, 2014, J CHEM PHYS, V140, DOI 10.1063/1.4875697; Jastrzebski S., 2018, PROC 6 INT C LEARN R; Kac M., 1948, P LOND MATH SOC, V50, P390; Kac M., 1943, B AM MATH SOC, V49, P314, DOI [DOI 10.1090/S0002-9904-1943-07912-8, 10.1090/S0002-9904-1943-07912-8]; Kastner M, 2011, PHYS REV LETT, V107, DOI 10.1103/PhysRevLett.107.160602; Kenji Kawaguchi, 2016, Arxiv, DOI arXiv:1605.07110; Kim T, 2003, NEURAL COMPUT, V15, P1641, DOI 10.1162/089976603321891846; Knoll C, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Knoll C, 2018, IEEE T PATTERN ANAL, V40, P2124, DOI 10.1109/TPAMI.2017.2749575; Kostlan E., 2002, PROC FOUND COMPUT MA, P149; KURKOVA V, 1994, NEURAL COMPUT, V6, P543, DOI 10.1162/neco.1994.6.3.543; Laurent Dinh, 2017, Arxiv, DOI arXiv:1703.04933; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lei Wu, 2017, Arxiv, DOI arXiv:1706.10239; Leon Bottou, 2017, Arxiv, DOI arXiv:1611.07476; Leon Bottou, 2018, Arxiv, DOI arXiv:1706.04454; Li T.Y., 2003, HDB NUMERICAL ANAL, VVolume 11, P209, DOI [10.1016/S1570-8659(02)11004-0, DOI 10.1016/S1570-8659(02)11004-0]; Li TY, 1996, MATH COMPUT, V65, P1477, DOI 10.1090/S0025-5718-96-00778-8; Mehta D, 2014, J CHEM PHYS, V140, DOI 10.1063/1.4880417; Mehta D, 2015, CHAOS, V25, DOI 10.1063/1.4919696; Mehta D, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.022133; Mehta D, 2013, PHYS REV E, V87, DOI 10.1103/PhysRevE.87.052143; Mehta D, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.061103; Mehta D, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.025702; Mobahi H, 2015, LECT NOTES COMPUT SC, V8932, P43, DOI 10.1007/978-3-319-14612-6_4; MORGAN A, 1987, APPL MATH COMPUT, V24, P115, DOI 10.1016/0096-3003(87)90064-6; MORGAN AP, 1989, APPL MATH COMPUT, V29, P123, DOI 10.1016/0096-3003(89)90099-4; Moritz Hardt, 2018, Arxiv, DOI arXiv:1611.04231; Mourrain B, 2006, COMPUT MATH APPL, V51, P527, DOI 10.1016/j.camwa.2005.07.012; Nerattini R, 2013, PHYS REV E, V87, DOI 10.1103/PhysRevE.87.032140; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Ninomiya H, 2005, IEEE IJCNN, P978; Nitta T, 1997, NEURAL NETWORKS, V10, P1391, DOI 10.1016/S0893-6080(97)00036-1; Nitta T, 2003, NEURAL NETWORKS, V16, P1101, DOI 10.1016/S0893-6080(03)00168-0; Nitzan Guberman, 2016, Arxiv, DOI arXiv:1602.09046; Perantonis S, 1998, NEURAL PROCESS LETT, V7, P5, DOI 10.1023/A:1009655902122; Pratik Chaudhari, 2017, Arxiv, DOI arXiv:1511.06485; Rojas JM, 1996, J COMPLEXITY, V12, P116, DOI 10.1006/jcom.1996.0009; Sommese AJ, 2005, NUMERICAL SOLUTION OF SYSTEMS OF POLYNOMIALS: ARISING IN ENGINEERING AND SCIENCE, P1, DOI 10.1142/9789812567727; Sontag E. D., 1989, Complex Systems, V3, P91; SUSSMANN HJ, 1992, NEURAL NETWORKS, V5, P589, DOI 10.1016/S0893-6080(05)80037-1; Swirszcz G., 2017, PROC ICLR C, V1050, P17; Taghvaei A., 2017, PROC 31 C ADV NEURAL, P2499; Trabelsi Chiheb, 2017, ARXIV; Wales D. J., 2003, ENERGY LANDSCAPES; Wales DJ, 2004, MOL PHYS, V102, P891, DOI 10.1080/00268970410001703363; Watanabe, 2009, ALGEBRAIC GEOMETRY S, DOI 10.1017/CBO9780511800474; Watanabe S, 2007, 2007 IEEE SYMPOSIUM ON FOUNDATIONS OF COMPUTATIONAL INTELLIGENCE, VOLS 1 AND 2, P383, DOI 10.1109/FOCI.2007.371500; Wisdom S, 2016, ADV NEUR IN, V29; Yan V Fyodorov, 2013, Arxiv, DOI arXiv:1307.2379; Yi Zhou, 2017, Arxiv, DOI arXiv:1710.11205; ZEMEL RS, 1995, NEURAL NETWORKS, V8, P503, DOI 10.1016/0893-6080(94)00094-3; Zhang Y, 2018, MOL PHYS, V116, P3214, DOI 10.1080/00268976.2018.1483535	130	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5664	5680		10.1109/TPAMI.2021.3071289	http://dx.doi.org/10.1109/TPAMI.2021.3071289			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33822722	Green Submitted			2022-12-18	WOS:000836666600082
J	Poggi, M; Tonioni, A; Tosi, F; Mattoccia, S; Di Stefano, L				Poggi, Matteo; Tonioni, Alessio; Tosi, Fabio; Mattoccia, Stefano; Di Stefano, Luigi			Continual Adaptation for Deep Stereo	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Computer architecture; Training; Real-time systems; Estimation; Adaptation models; Three-dimensional displays; Proposals; Stereo matching; deep learning; self-supervision; real-time adaptation; continual learning	ACCURATE; VISION	Depth estimation from stereo images is carried out with unmatched results by convolutional neural networks trained end-to-end to regress dense disparities. Like for most tasks, this is possible if large amounts of labelled samples are available for training, possibly covering the whole data distribution encountered at deployment time. Being such an assumption systematically unmet in real applications, the capacity of adapting to any unseen setting becomes of paramount importance. Purposely, we propose a continual adaptation paradigm for deep stereo networks designed to deal with challenging and ever-changing environments. We design a lightweight and modular architecture, Modularly ADaptive Network (MADNet), and formulate Modular ADaptation algorithms (MAD, MAD++) which permit efficient optimization of independent sub-portions of the entire network. In our paradigm, the learning signals needed to continuously adapt models online can be sourced from self-supervision via right-to-left image warping or from traditional stereo algorithms. With both sources, no other data than the input images being gathered at deployment time are needed. Thus, our network architecture and adaptation algorithms realize the first real-time self-adaptive deep stereo system and pave the way for a new paradigm that can facilitate practical deployment of end-to-end architectures for dense disparity regression.	[Poggi, Matteo; Tosi, Fabio; Mattoccia, Stefano; Di Stefano, Luigi] Univ Bologna, Dept Comp Sci & Engn, I-40126 Bologna, Italy; [Tonioni, Alessio] Google Zurich, CH-8002 Zurich, Switzerland	University of Bologna; Google Incorporated	Poggi, M (corresponding author), Univ Bologna, Dept Comp Sci & Engn, I-40126 Bologna, Italy.	m.poggi@unibo.it; alessiot@google.com; fabio.tosi5@unibo.it; stefano.mattoccia@unibo.it; luigi.distefano@unibo.it	Mattoccia, Stefano/AAV-6931-2021	Mattoccia, Stefano/0000-0002-3681-7704	NVIDIA Corporation	NVIDIA Corporation	The authors gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.	Aleotti Filippo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P614, DOI 10.1007/978-3-030-58621-8_36; Aleotti F, 2020, AAAI CONF ARTIF INTE, V34, P10435; Alismail Hatem Said, 2011, 11 INT C INT AUT SYS, P1; Badki A, 2020, PROC CVPR IEEE, P1597, DOI 10.1109/CVPR42600.2020.00167; Banz Christian, 2010, Proceedings of the 2010 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (IC-SAMOS 2010), P93, DOI 10.1109/ICSAMOS.2010.5642077; Batsos K, 2018, PROC CVPR IEEE, P2060, DOI 10.1109/CVPR.2018.00220; Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567; Chen ZY, 2015, IEEE I CONF COMP VIS, P972, DOI 10.1109/ICCV.2015.117; Cheng X., 2020, P NEURIPS; Cheng XJ, 2020, IEEE T PATTERN ANAL, V42, P2361, DOI [10.1109/TPAMI.2019.2947374, 10.5194/isprs-archives-XLII-3-W7-1-2019]; Dovesi Pier Luigi, 2020, 2020 IEEE International Conference on Robotics and Automation (ICRA), P10780, DOI 10.1109/ICRA40945.2020.9196784; Duggal S, 2019, IEEE I CONF COMP VIS, P4383, DOI 10.1109/ICCV.2019.00448; Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45; Gehrig SK, 2009, LECT NOTES COMPUT SC, V5815, P134, DOI 10.1007/978-3-642-04667-4_14; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Gidaris S, 2017, PROC CVPR IEEE, P7187, DOI 10.1109/CVPR.2017.760; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339; Haeusler R, 2013, PROC CVPR IEEE, P305, DOI 10.1109/CVPR.2013.46; Hirschmuller H, 2009, IEEE T PATTERN ANAL, V31, P1582, DOI 10.1109/TPAMI.2008.221; Hirschmuller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56; Honegger D, 2014, IEEE INT C INT ROBOT, P4930, DOI 10.1109/IROS.2014.6943263; Ilg E, 2018, LECT NOTES COMPUT SC, V11216, P626, DOI 10.1007/978-3-030-01258-8_38; Jiang HZ, 2019, IEEE I CONF COMP VIS, P3194, DOI 10.1109/ICCV.2019.00329; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Khamis S, 2018, LECT NOTES COMPUT SC, V11219, P596, DOI 10.1007/978-3-030-01267-0_35; Kim S, 2019, PROC CVPR IEEE, P205, DOI 10.1109/CVPR.2019.00029; Kuznietsov Y, 2017, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR.2017.238; Lai HY, 2019, PROC CVPR IEEE, P1890, DOI 10.1109/CVPR.2019.00199; Liang ZF, 2018, PROC CVPR IEEE, P2811, DOI 10.1109/CVPR.2018.00297; Luo WJ, 2016, PROC CVPR IEEE, P5695, DOI 10.1109/CVPR.2016.614; Mattoccia S., 2015, P 9 INT C DISTRIBUTE, P146; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925; Nie GY, 2019, PROC CVPR IEEE, P3278, DOI 10.1109/CVPR.2019.00340; Pang JH, 2018, PROC CVPR IEEE, P2070, DOI 10.1109/CVPR.2018.00221; Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108; Park MG, 2015, PROC CVPR IEEE, P101, DOI 10.1109/CVPR.2015.7298605; Pilzer A, 2019, PROC CVPR IEEE, P9760, DOI 10.1109/CVPR.2019.01000; Poggi M, 2019, PROC CVPR IEEE, P979, DOI 10.1109/CVPR.2019.00107; Poggi M, 2020, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR42600.2020.00329; Poggi M, 2018, IEEE INT C INT ROBOT, P5848, DOI 10.1109/IROS.2018.8593814; Poggi M, 2018, INT CONF 3D VISION, P324, DOI 10.1109/3DV.2018.00045; Poggi M, 2017, IEEE I CONF COMP VIS, P5238, DOI 10.1109/ICCV.2017.559; Poggi M, 2016, INT CONF 3D VISION, P509, DOI 10.1109/3DV.2016.61; Poggi Matteo, 2016, BMVC; Pollefeys M, 2016, BRIT MACH VIS C BMVC; Rahnama O, 2018, 2018 INTERNATIONAL CONFERENCE ON FIELD-PROGRAMMABLE TECHNOLOGY (FPT 2018), P105, DOI 10.1109/FPT.2018.00025; Rahnama O, 2019, IEEE T CIRCUITS-II, V66, P773, DOI 10.1109/TCSII.2019.2909169; Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Schmid K, 2013, IEEE INT CONF ROBOT, P4671, DOI 10.1109/ICRA.2013.6631242; Shaked A, 2017, PROC CVPR IEEE, P6901, DOI 10.1109/CVPR.2017.730; Song X, 2019, LECT NOTES COMPUT SC, V11365, P20, DOI 10.1007/978-3-030-20873-8_2; Spyropoulos A, 2014, PROC CVPR IEEE, P1621, DOI 10.1109/CVPR.2014.210; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Tonioni A, 2019, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2019.00028; Tonioni A, 2019, PROC CVPR IEEE, P9653, DOI 10.1109/CVPR.2019.00989; Tonioni A, 2020, IEEE T PATTERN ANAL, V42, P2396, DOI 10.1109/TPAMI.2019.2940948; Tonioni A, 2017, IEEE I CONF COMP VIS, P1614, DOI 10.1109/ICCV.2017.178; Tosi F., 2017, P 28 BRIT MACH VIS C; Tosi F, 2019, PROC CVPR IEEE, P9791, DOI 10.1109/CVPR.2019.01003; Tosi F, 2018, LECT NOTES COMPUT SC, V11210, P323, DOI 10.1007/978-3-030-01231-1_20; Tulyakov S, 2018, ADV NEUR IN, V31; Uhrig J, 2017, INT CONF 3D VISION, P11, DOI 10.1109/3DV.2017.00012; Wang Y, 2019, IEEE INT CONF ROBOT, P5893, DOI 10.1109/ICRA.2019.8794003; Wang Y, 2019, PROC CVPR IEEE, P8063, DOI 10.1109/CVPR.2019.00826; Xu HF, 2020, PROC CVPR IEEE, P1956, DOI 10.1109/CVPR42600.2020.00203; Yang GR, 2019, PROC CVPR IEEE, P899, DOI 10.1109/CVPR.2019.00099; Yang GR, 2018, LECT NOTES COMPUT SC, V11211, P660, DOI 10.1007/978-3-030-01234-2_39; Yin ZC, 2019, PROC CVPR IEEE, P6037, DOI 10.1109/CVPR.2019.00620; Zbontar J, 2016, J MACH LEARN RES, V17; Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027; Zhang YD, 2018, LECT NOTES COMPUT SC, V11212, P802, DOI 10.1007/978-3-030-01237-3_48; Zhang YM, 2020, AAAI CONF ARTIF INTE, V34, P12926; Zhong YR, 2018, LECT NOTES COMPUT SC, V11206, P104, DOI 10.1007/978-3-030-01216-8_7; Zhou C, 2017, IEEE I CONF COMP VIS, P1576, DOI 10.1109/ICCV.2017.174; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700	79	1	1	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4713	4729		10.1109/TPAMI.2021.3075815	http://dx.doi.org/10.1109/TPAMI.2021.3075815			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33909558				2022-12-18	WOS:000836666600020
J	Puerto-Santana, C; Larranaga, P; Bielza, C				Puerto-Santana, Carlos; Larranaga, Pedro; Bielza, Concha			Autoregressive Asymmetric Linear Gaussian Hidden Markov Models	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Hidden Markov models; Markov processes; Graphical models; Bayes methods; Probabilistic logic; Mathematical model; Data models; Hidden markov models; Bayesian networks; model selection; structure learning; time series; information asymmetries; linear gaussian; autoregressive; Yule-Walker equations	TIME-SERIES; INDEPENDENCE; PREDICTION; INFERENCE	In a real life process evolving over time, the relationship between its relevant variables may change. Therefore, it is advantageous to have different inference models for each state of the process. Asymmetric hidden Markov models fulfil this dynamical requirement and provide a framework where the trend of the process can be expressed as a latent variable. In this paper, we modify these recent asymmetric hidden Markov models to have an asymmetric autoregressive component in the case of continuous variables, allowing the model to choose the order of autoregression that maximizes its penalized likelihood for a given training set. Additionally, we show how inference, hidden states decoding and parameter learning must be adapted to fit the proposed model. Finally, we run experiments with synthetic and real data to show the capabilities of this new model.	[Puerto-Santana, Carlos; Larranaga, Pedro; Bielza, Concha] Univ Politecn Madrid, Madrid 28040, Spain; [Puerto-Santana, Carlos] Aingura IIoT, San Sebastian 20009, Spain	Universidad Politecnica de Madrid	Puerto-Santana, C (corresponding author), Univ Politecn Madrid, Madrid 28040, Spain.	ce.puerto@alumnos.upm.es; pedro.larranaga@fi.upm.es; mcbielza@fi.upm.es			Spanish Centre for the Development of Industrial Technology (CDTI) [IDI-20180156]; Spanish Ministry of Science, Innovation and Universities [PID2019-109247GB-I00, RTC2019-006871-7]; project BAYES-CLIMA-NEURO, BBVA Foundation's Grant (2019)	Spanish Centre for the Development of Industrial Technology (CDTI); Spanish Ministry of Science, Innovation and Universities(Spanish Government); project BAYES-CLIMA-NEURO, BBVA Foundation's Grant (2019)	This work was supported by the Spanish Centre for the Development of Industrial Technology (CDTI) through the IDI-20180156 LearnIIoT project, in part by the Spanish Ministry of Science, Innovation and Universities through the PID2019-109247GB-I00 and RTC2019-006871-7 DSTREAMS Project, and from the project BAYES-CLIMA-NEURO, BBVA Foundation's Grant (2019). The authors would like to thank Aingura IIoT for its support related to filtering the datasets to perform the corresponding experiments in the case of ball-bearing degradation case.	Asahara A., 2012, P 20 INT C ADV GEOGR, P414; Barclay LM, 2015, ELECTRON J STAT, V9, P2130, DOI 10.1214/15-EJS1068; Bilmes JA, 2003, COMPUT SPEECH LANG, V17, P213, DOI 10.1016/S0885-2308(03)00010-X; Boutilier C, 1996, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P115; Box G. E. P., 1970, Time series analysis, forecasting and control; Bryan JD, 2015, PROCEDIA COMPUT SCI, V61, P328, DOI 10.1016/j.procs.2015.09.151; Bueno MLP, 2017, INT J APPROX REASON, V88, P169, DOI 10.1016/j.ijar.2017.05.011; Cheng J, 2016, COMMUN STAT-THEOR M, V45, P2785, DOI 10.1080/03610926.2014.894065; Dang S., 2016, PROC 10 INDIAN C COM, P1; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Friedman N., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P129; Geiger D, 1996, ARTIF INTELL, V82, P45, DOI 10.1016/0004-3702(95)00014-3; Glover F., 2013, TABU SEARCH; HAMILTON JD, 1989, ECONOMETRICA, V57, P357, DOI 10.2307/1912559; HAMILTON JD, 1990, J ECONOMETRICS, V45, P39, DOI 10.1016/0304-4076(90)90093-9; HECK LP, 1991, INT CONF ACOUST SPEE, P1697, DOI 10.1109/ICASSP.1991.150631; HECKERMAN D, 1990, NETWORKS, V20, P607, DOI 10.1002/net.3230200508; ITAKURA F, 1975, IEEE T ACOUST SPEECH, VAS23, P67, DOI 10.1109/TASSP.1975.1162641; JUANG BH, 1985, IEEE T ACOUST SPEECH, V33, P1404, DOI 10.1109/TASSP.1985.1164727; KENNY P, 1990, IEEE T ACOUST SPEECH, V38, P220, DOI 10.1109/29.103057; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; Kirshner S., 2004, P 20 C UNC ART INT A, P317; Larra~naga P., 2018, IND APPL MACHINE LEA; Malesevic N, 2018, COMPLEXITY, DOI 10.1155/2018/9728264; Mart~ inez M., 2008, P 21 INT FLOR ART IN, P655; Murphy KP., 2002, DYNAMIC BAYESIAN NET; Nakamura E., 2015, PROC 16 INT SOC MUSI; Nyman H, 2014, BAYESIAN ANAL, V9, P883, DOI 10.1214/14-BA882; Poritz A. B., 1982, Proceedings of ICASSP 82. IEEE International Conference on Acoustics, Speech and Signal Processing, P1291; Puerto-Santana C, 2018, LECT NOTES ARTIF INT, V11160, P98, DOI 10.1007/978-3-030-00374-6_10; Qian YN, 2017, MECH SYST SIGNAL PR, V83, P549, DOI 10.1016/j.ymssp.2016.06.031; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Seifert M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0100295; SHACHTER RD, 1989, MANAGE SCI, V35, P527, DOI 10.1287/mnsc.35.5.527; SHACHTER RD, 1988, OPER RES, V36, P589, DOI 10.1287/opre.36.4.589; Smith JQ, 2008, ARTIF INTELL, V172, P42, DOI 10.1016/j.artint.2007.05.004; Stanculescu I, 2014, IEEE J BIOMED HEALTH, V18, P1560, DOI 10.1109/JBHI.2013.2294692; Stanke M, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-62; Sun W, 2013, SCI TOTAL ENVIRON, V443, P93, DOI 10.1016/j.scitotenv.2012.10.070; Vairo T., 2019, CHEM ENG TRANS, V76, DOI 10.3303/CET1974046; Vitolo C, 2018, EARTH SPACE SCI, V5, P76, DOI 10.1002/2017EA000326; Wang YX, 2011, MECH SYST SIGNAL PR, V25, P1750, DOI 10.1016/j.ymssp.2010.12.008; Yang RJ, 2017, I C COMM SOFTW NET, P1003; Zhang SY, 2017, P ROY SOC A-MATH PHY, V473, DOI 10.1098/rspa.2017.0457	44	1	1	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4642	4658		10.1109/TPAMI.2021.3068799	http://dx.doi.org/10.1109/TPAMI.2021.3068799			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33764873	hybrid, Green Submitted			2022-12-18	WOS:000836666600015
J	Qin, ZY; Wang, JL; Lu, Y				Qin, Zengyi; Wang, Jinglu; Lu, Yan			MonoGRNet: A General Framework for Monocular 3D Object Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Object detection; Estimation; Task analysis; Supervised learning; Proposals; Detectors; 3D object detection; monocular; weakly supervised learning		Detecting and localizing objects in the real 3D space, which plays a crucial role in scene understanding, is particularly challenging given only a monocular image due to the geometric information loss during imagery projection. We propose MonoGRNet for the amodal 3D object detection from a monocular image via geometric reasoning in both the observed 2D projection and the unobserved depth dimension. MonoGRNet decomposes the monocular 3D object detection task into four sub-tasks including 2D object detection, instance-level depth estimation, projected 3D center estimation and local corner regression. The task decomposition significantly facilitates the monocular 3D object detection, allowing the target 3D bounding boxes to be efficiently predicted in a single forward pass, without using object proposals, post-processing or the computationally expensive pixel-level depth estimation utilized by previous methods. In addition, MonoGRNet flexibly adapts to both fully and weakly supervised learning, which improves the feasibility of our framework in diverse settings. Experiments are conducted on KITTI, Cityscapes and MS COCO datasets. Results demonstrate the promising performance of our framework in various scenarios.	[Qin, Zengyi] MIT, Dept Aeronaut & Astronaut, Cambridge, MA 02139 USA; [Wang, Jinglu; Lu, Yan] Microsoft Res Asia MSRA, Beijing 100080, Peoples R China	Massachusetts Institute of Technology (MIT)	Qin, ZY (corresponding author), MIT, Dept Aeronaut & Astronaut, Cambridge, MA 02139 USA.	qinzy@mit.edu; jinglwa@microsoft.com; yanlu@microsoft.com						Abadi M., 2016, ARXIV, DOI DOI 10.48550/ARXIV.1603.04467; Afifi A. J., 2016, 2016 INT C DIG IM CO, P1; Alexey Artemov, 2019, Arxiv, DOI arXiv:1905.05618; Angel X. Chang, 2015, Arxiv, DOI arXiv:1512.03012; Ba J., 2017, P 3 INT C LEARN REPR; Bharath Hariharan, 2020, Arxiv, DOI arXiv:1906.06310; Braun M, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P1546, DOI 10.1109/ITSC.2016.7795763; Chabot F, 2017, PROC CVPR IEEE, P1827, DOI 10.1109/CVPR.2017.198; Chen X., 2015, ADV NEUR IN, V28; Chen XZ, 2018, IEEE T PATTERN ANAL, V40, P1259, DOI 10.1109/TPAMI.2017.2706685; Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691; Chen XZ, 2016, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2016.236; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai JF, 2016, ADV NEUR IN, V29; Ding M., 2019, ARXIV; Feidao Cao, 2020, Arxiv, DOI arXiv:2001.03343; Fu C., 2017, ARXIV, DOI DOI 10.48550/ARXIV.1701.06659; Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393; Guizilini V, 2020, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR42600.2020.00256; Gustafsson F., 2018, MS THESIS; Han JW, 2015, IEEE T GEOSCI REMOTE, V53, P3325, DOI 10.1109/TGRS.2014.2374218; He K., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1703.06870; Hoffman Judy, 2014, NIPS; Hongsheng Li, 2020, Arxiv, DOI arXiv:1907.03670; Hu H.-N., 2018, ARXIV; In So Kweon, 2020, Arxiv, DOI arXiv:1912.09351; Jorgensen E., 2019, ARXIV; Kehl W, 2017, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2017.169; Ku J, 2019, PROC CVPR IEEE, P11859, DOI 10.1109/CVPR.2019.01214; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298; Li PL, 2019, PROC CVPR IEEE, P7636, DOI 10.1109/CVPR.2019.00783; Liang M, 2019, PROC CVPR IEEE, P7337, DOI 10.1109/CVPR.2019.00752; Lim J.J., 2011, ADV NEURAL INFORM PR, P118; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu JB, 2015, IEEE I CONF COMP VIS, P2093, DOI 10.1109/ICCV.2015.242; Liu LJ, 2019, PROC CVPR IEEE, P1057, DOI 10.1109/CVPR.2019.00115; Ma XZ, 2019, IEEE I CONF COMP VIS, P6850, DOI 10.1109/ICCV.2019.00695; Manhardt F, 2019, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2019.00217; Marius Zoellner, 2018, Arxiv, DOI arXiv:1612.07695; Mousavian A, 2017, PROC CVPR IEEE, P5632, DOI 10.1109/CVPR.2017.597; Papadopoulos DP, 2017, IEEE I CONF COMP VIS, pCP38, DOI 10.1109/ICCV.2017.528; Pathak D, 2017, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2017.638; Qin ZY, 2019, PROC CVPR IEEE, P7607, DOI 10.1109/CVPR.2019.00780; Qin ZY, 2019, AAAI CONF ARTIF INTE, P8851; Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren H., 2019, P CVPR WORKSH, P37; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sangineto E, 2019, IEEE T PATTERN ANAL, V41, P712, DOI 10.1109/TPAMI.2018.2804907; Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94; Spinello L, 2012, IEEE INT CONF ROBOT, P4469, DOI 10.1109/ICRA.2012.6225137; Stefano Soatto, 2019, Arxiv, DOI arXiv:1901.03446; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Tang P, 2020, IEEE T PATTERN ANAL, V42, P176, DOI 10.1109/TPAMI.2018.2876304; Torrey Lisa, HDB RES MACHINE LEAR, P242; Van Rossum G., 1995, CENTRUM WISKUNDE INF; Wang Y, 2019, PROC CVPR IEEE, P8063, DOI 10.1109/CVPR.2019.00826; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Xiang Y, 2014, IEEE WINT CONF APPL, P75, DOI 10.1109/WACV.2014.6836101; Xu B, 2018, PROC CVPR IEEE, P2345, DOI 10.1109/CVPR.2018.00249; Yang B, 2018, PROC CVPR IEEE, P7652, DOI 10.1109/CVPR.2018.00798; Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204; Cai YJ, 2021, Arxiv, DOI arXiv:2002.01619; You Y, 2019, ARXIV; Zechen Liu, 2020, Arxiv, DOI arXiv:2002.10111; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144; Zhang XP, 2018, PROC CVPR IEEE, P4262, DOI 10.1109/CVPR.2018.00448; Zhang ZY, 2015, IEEE I CONF COMP VIS, P2614, DOI 10.1109/ICCV.2015.300; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472	77	1	1	16	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5170	5184		10.1109/TPAMI.2021.3074363	http://dx.doi.org/10.1109/TPAMI.2021.3074363			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33877968	Green Submitted			2022-12-18	WOS:000836666600050
J	Wang, JQ; Chen, K; Xu, R; Liu, ZW; Loy, CC; Lin, DH				Wang, Jiaqi; Chen, Kai; Xu, Rui; Liu, Ziwei; Loy, Chen Change; Lin, Dahua			CARAFE plus plus : Unified Content-Aware ReAssembly of FEatures	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Kernel; Task analysis; Image segmentation; Semantics; Interpolation; Convolution; Object detection; Feature reassembly; object detection; instance segmentation; semantic segmentation; image inpainting		Feature reassembly, i.e. feature downsampling and upsampling, is a key operation in a number of modern convolutional network architectures, e.g., residual networks and feature pyramids. Its design is critical for dense prediction tasks such as object detection and semantic/instance segmentation. In this work, we propose unified Content-Aware ReAssembly of FEatures (CARAFE++), a universal, lightweight, and highly effective operator to fulfill this goal. CARAFE++ has several appealing properties: (1) Unlike conventional methods such as pooling and interpolation that only exploit sub-pixel neighborhood, CARAFE++ aggregates contextual information within a large receptive field. (2) Instead of using a fixed kernel for all samples (e.g. convolution and deconvolution), CARAFE++ generates adaptive kernels on-the-fly to enable instance-specific content-aware handling. (3) CARAFE++ introduces little computational overhead and can be readily integrated into modern network architectures. We conduct comprehensive evaluations on standard benchmarks in object detection, instance/semantic segmentation, and image inpainting. CARAFE++ shows consistent and substantial gains on mainstream methods across all the tasks with negligible computational overhead. It shows great potential to serve as a strong building block for modern deep networks.	[Wang, Jiaqi; Chen, Kai; Xu, Rui; Lin, Dahua] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Liu, Ziwei; Loy, Chen Change] Nanyang Technol Univ, S Lab, Singapore 639798, Singapore	Chinese University of Hong Kong; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Wang, JQ (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.	wj017@ie.cuhk.edu.hk; ck015@ie.cuhk.edu.hk; xr018@ie.cuhk.edu.hk; ziwei.liu@ntu.edu.sg; ccloy@ntu.edu.sg; dhlin@ie.cuhk.edu.hk			SenseTime Group, Collaborative Research Grant, CUHK [TS1610626, TS1712093]; General Research Fund of Hong Kong [14236516, 14203518]; NTU SUG; NTU NAP; A*STAR	SenseTime Group, Collaborative Research Grant, CUHK; General Research Fund of Hong Kong; NTU SUG(Nanyang Technological University); NTU NAP; A*STAR(Agency for Science Technology & Research (A*STAR))	This work was supported in part by the SenseTime Group, Collaborative Research Grant, CUHK, under Grants TS1610626 and TS1712093, in part by the General Research Fund of Hong Kong under Grants 14236516 and 14203518, in part by the NTU SUG, in part by the NTU NAP, and in part by the A*STAR through the Industry Alignment Fund, Industry Collaboration Projects.	Chen K., 2019, ARXIV PREPRINT ARXIV; Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Davide Mazzini, 2018, Arxiv, DOI arXiv:1807.07466; De Brabandere B, 2016, ADV NEUR IN, V29; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Gao ZT, 2019, IEEE I CONF COMP VIS, P3354, DOI 10.1109/ICCV.2019.00345; Goyal P., 2017, ARXIV, DOI DOI 10.48550/ARXIV.1706.02677; Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu XC, 2019, PROC CVPR IEEE, P1575, DOI 10.1109/CVPR.2019.00167; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; Jaderberg M, 2015, ADV NEUR IN, V28; Jiahui Yu, 2019, Arxiv, DOI arXiv:1806.03589; Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340; Kong T, 2018, LECT NOTES COMPUT SC, V11209, P172, DOI 10.1007/978-3-030-01228-1_11; Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43; Li XX, 2017, PROC CVPR IEEE, P6459, DOI 10.1109/CVPR.2017.684; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6; Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913; Liu ZW, 2015, IEEE I CONF COMP VIS, P1377, DOI 10.1109/ICCV.2015.162; Mildenhall B, 2018, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2018.00265; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ronneberger O., 2015, P INT C MED IM COMP; Saeedan F, 2018, PROC CVPR IEEE, P9108, DOI 10.1109/CVPR.2018.00949; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Su H, 2019, PROC CVPR IEEE, P11158, DOI 10.1109/CVPR.2019.01142; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308; Wang JQ, 2019, IEEE I CONF COMP VIS, P3007, DOI 10.1109/ICCV.2019.00310; Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26; Xu R, 2019, PROC CVPR IEEE, P3718, DOI 10.1109/CVPR.2019.00384; Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhao QJ, 2019, AAAI CONF ARTIF INTE, P9259; Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0; Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009; Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953	52	1	1	6	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4674	4687		10.1109/TPAMI.2021.3074370	http://dx.doi.org/10.1109/TPAMI.2021.3074370			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33881989	Green Submitted			2022-12-18	WOS:000836666600017
J	Bolelli, F; Allegretti, S; Grana, C				Bolelli, Federico; Allegretti, Stefano; Grana, Costantino			One DAG to Rule Them All	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						GRAPHGEN; directed rooted acyclic graphs; optimal decision trees; decision tables; connected components labeling; thinning; chain-code	FAST PARALLEL ALGORITHM; CONNECTED COMPONENTS; BINARY IMAGES; SEGMENTATION	In this paper, we present novel strategies for optimizing the performance of many binary image processing algorithms. These strategies are collected in an open-source framework, GRAPHGEN, that is able to automatically generate optimized C++ source code implementing the desired optimizations. Simply starting from a set of rules, the algorithms introduced with the GRAPHGEN framework can generate decision trees with minimum average path-length, possibly considering image pattern frequencies, apply state prediction and code compression by the use of directed rooted acyclic graphs (DRAGS). Moreover, the proposed algorithmic solutions allow to combine different optimization techniques and significantly improve performance. Our proposal is showcased on three classical and widely employed algorithms (namely Connected Components Labeling, Thinning, and Contour Tracing). When compared to existing approaches -in 2D and 3D-, implementations using the generated optimal DRAGs perform significantly better than previous state-of-the-art algorithms, both on CPU and GPU.	[Bolelli, Federico; Allegretti, Stefano; Grana, Costantino] Univ Modena & Reggio Emilia, Dipartimento Ingn Enzo Ferrari, I-41121 Modena, Italy	Universita di Modena e Reggio Emilia	Bolelli, F (corresponding author), Univ Modena & Reggio Emilia, Dipartimento Ingn Enzo Ferrari, I-41121 Modena, Italy.	federico.bolelli@unimore.it; stefano.allegretti@unimore.it; costantino.grana@unimore.it	Grana, Costantino/B-4555-2012	Grana, Costantino/0000-0002-4792-2358; Bolelli, Federico/0000-0002-5299-6351				Allegretti S, 2020, IEEE T PARALL DISTR, V31, P423, DOI 10.1109/TPDS.2019.2934683; Allegretti S, 2018, 2018 IEEE THIRD INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, APPLICATIONS AND SYSTEMS (IPAS), P175, DOI 10.1109/IPAS.2018.8708900; [Anonymous], THEBE SOURCE CODE; [Anonymous], GRAPHGEN SOURCE SODE; [Anonymous], BACCA SOURCE CODE; [Anonymous], YACCLAB SOURCE CODE; Baltieri D., 2011, P 2011 JOINT ACMWORK, P59, DOI DOI 10.1145/2072572.2072590; Baraldi L, 2017, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2017.339; Bhunia AK, 2019, PROC CVPR IEEE, P4762, DOI 10.1109/CVPR.2019.00490; Bolelli F, 2020, J REAL-TIME IMAGE PR, V17, P229, DOI 10.1007/s11554-018-0756-1; Bolelli F, 2020, IEEE T IMAGE PROCESS, V29, P1999, DOI 10.1109/TIP.2019.2946979; Bolelli F, 2018, 2018 IEEE THIRD INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, APPLICATIONS AND SYSTEMS (IPAS), P169, DOI 10.1109/IPAS.2018.8708893; Bolelli F, 2018, INT C PATT RECOG, P121, DOI 10.1109/ICPR.2018.8545505; Bolelli F, 2018, COMM COM INF SC, V806, P151, DOI 10.1007/978-3-319-73165-0_15; Bolelli F, 2017, LECT NOTES COMPUT SC, V10485, P48, DOI 10.1007/978-3-319-68548-9_5; Buss S. R., 1997, Computational Logic and Proof Theory. 5th Kurt Godel Colloquium, KGC'97. Proceedings, P18; Cabaret L, 2017, INT CONF IMAG PROC; Canalini L, 2019, LECT NOTES COMPUT SC, V11678, P89, DOI 10.1007/978-3-030-29888-3_8; CEDERBERG RLT, 1979, COMPUT VISION GRAPH, V10, P224, DOI 10.1016/0146-664X(79)90002-9; CHEN YS, 1988, PATTERN RECOGN LETT, V7, P99, DOI 10.1016/0167-8655(88)90124-9; DEUTSCH ES, 1972, COMMUN ACM, V15, P827, DOI 10.1145/361573.361583; Dong F, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0114885; Fabbri M, 2018, LECT NOTES COMPUT SC, V11208, P450, DOI 10.1007/978-3-030-01225-0_27; Falk T, 2019, NAT METHODS, V16, P67, DOI 10.1038/s41592-018-0261-2; Freeman H., 1961, IRE T ELECT COMPUTER, VEC-10, P260, DOI DOI 10.1109/TEC.1961.5219197; Grana Costantino, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2836, DOI 10.1109/ICPR.2010.695; Grana C, 2016, INT C PATT RECOG, P3109, DOI 10.1109/ICPR.2016.7900112; Grana C, 2016, LECT NOTES COMPUT SC, V10016, P431, DOI 10.1007/978-3-319-48680-2_38; Grana C, 2012, PATTERN RECOGN LETT, V33, P2302, DOI 10.1016/j.patrec.2012.08.015; Grana C, 2009, IEEE IMAGE PROC, P4061, DOI 10.1109/ICIP.2009.5413731; Grana C, 2010, IEEE T IMAGE PROCESS, V19, P1596, DOI 10.1109/TIP.2010.2044963; GUO ZC, 1989, COMMUN ACM, V32, P359, DOI 10.1145/62065.62074; Hannuna S, 2019, J REAL-TIME IMAGE PR, V16, P1439, DOI 10.1007/s11554-016-0654-3; He LF, 2017, PATTERN RECOGN, V70, P25, DOI 10.1016/j.patcog.2017.04.018; He LF, 2015, IEEE T IMAGE PROCESS, V24, P2725, DOI 10.1109/TIP.2015.2425540; He LF, 2014, IEEE T IMAGE PROCESS, V23, P943, DOI 10.1109/TIP.2013.2289968; He LF, 2011, IEEE T IMAGE PROCESS, V20, P2122, DOI 10.1109/TIP.2011.2114352; HUANG TS, 1979, IEEE T ACOUST SPEECH, V27, P13, DOI 10.1109/TASSP.1979.1163188; Huiskes Mark J, 2008, P 1 ACM INT C MULTIM, P39, DOI DOI 10.1145/1460096.1460104; Kalentev O, 2011, J PARALLEL DISTR COM, V71, P615, DOI 10.1016/j.jpdc.2010.10.012; Khodadoust J, 2017, PATTERN RECOGN, V67, P110, DOI 10.1016/j.patcog.2017.01.022; Komura Y, 2015, COMPUT PHYS COMMUN, V194, P54, DOI 10.1016/j.cpc.2015.04.015; Laradji IH, 2018, LECT NOTES COMPUT SC, V11206, P560, DOI 10.1007/978-3-030-01216-8_34; Lewis D., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P665, DOI 10.1145/1148170.1148307; Li BY, 2019, PROC CVPR IEEE, P1019, DOI 10.1109/CVPR.2019.00111; Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005; Liu C, 2019, PROC CVPR IEEE, P6057, DOI 10.1109/CVPR.2019.00622; Lucchi A, 2013, PROC CVPR IEEE, P1987, DOI 10.1109/CVPR.2013.259; Maltoni D, 2009, HDB FINGERPRINT RECO, Vfirst; Marcus DS, 2010, J COGNITIVE NEUROSCI, V22, P2677, DOI 10.1162/jocn.2009.21407; Mattyus G, 2017, IEEE I CONF COMP VIS, P3458, DOI 10.1109/ICCV.2017.372; Milletari F, 2017, COMPUT VIS IMAGE UND, V164, P92, DOI 10.1016/j.cviu.2017.04.002; Oliveira VMA, 2010, SIBGRAPI; Palazzi A, 2019, IEEE T PATTERN ANAL, V41, P1720, DOI 10.1109/TPAMI.2018.2845370; Pollastri F, 2020, MULTIMED TOOLS APPL, V79, P15575, DOI 10.1007/s11042-019-7717-y; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; ROSENFEL.A, 1966, J ACM, V13, P471; SCHUMACHER H, 1976, COMMUN ACM, V19, P343, DOI 10.1145/360238.360245; SOBEL I, 1978, COMPUT VISION GRAPH, V8, P127, DOI 10.1016/S0146-664X(78)80020-3; SUZUKI S, 1985, COMPUT VISION GRAPH, V30, P32, DOI 10.1016/0734-189X(85)90016-7; Tschandl P, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.161; Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256; Uslu F, 2019, PATTERN RECOGN, V87, P157, DOI 10.1016/j.patcog.2018.10.017; Wang BR, 2019, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2019.00273; Wang XH, 2019, PATTERN RECOGN, V88, P331, DOI 10.1016/j.patcog.2018.11.030; Wilkinson T., 2017, IEEE I CONF COMP VIS, P4443, DOI [10.1109/ICCV.2017.475, DOI 10.1109/ICCV.2017.475]; Wu CY, 2019, PROC CVPR IEEE, P284, DOI 10.1109/CVPR.2019.00037; Wu KS, 2009, PATTERN ANAL APPL, V12, P117, DOI 10.1007/s10044-008-0109-y; Yang GR, 2019, PROC CVPR IEEE, P899, DOI 10.1109/CVPR.2019.00099; Yang XT, 2019, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2019.00035; Yonehara K, 2015, INT SYMPOS COMPUT NE, P341, DOI 10.1109/CANDAR.2015.78; Zavalishin S, 2016, EI, V2016, P1; ZHANG TY, 1984, COMMUN ACM, V27, P236, DOI 10.1145/357994.358023; Zhou Y, 2019, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2019.00218; Zingaretti P, 1998, IEEE T PATTERN ANAL, V20, P407, DOI 10.1109/34.677272	77	1	1	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3647	3658		10.1109/TPAMI.2021.3055337	http://dx.doi.org/10.1109/TPAMI.2021.3055337			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33507865	Green Published			2022-12-18	WOS:000805820500023
J	Dundar, A; Shih, K; Garg, A; Pottorff, R; Tao, A; Catanzaro, B				Dundar, Aysegul; Shih, Kevin; Garg, Animesh; Pottorff, Robert; Tao, Andrew; Catanzaro, Bryan			Unsupervised Disentanglement of Pose, Appearance and Background from Images and Videos	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image reconstruction; Task analysis; Videos; Pipelines; Training; Image color analysis; Decoding; Unsupervised landmarks; keypoints; foreground-background separation; video prediction		Unsupervised landmark learning is the task of learning semantic keypoint-like representations without the use of expensive input keypoint annotations. A popular approach is to factorize an image into a pose and appearance data stream, then to reconstruct the image from the factorized components. The pose representation should capture a set of consistent and tightly localized landmarks in order to facilitate reconstruction of the input image. Ultimately, we wish for our learned landmarks to focus on the foreground object of interest. However, the reconstruction task of the entire image forces the model to allocate landmarks to model the background. Using a motion-based foreground assumption, this work explores the effects of factorizing the reconstruction task into separate foreground and background reconstructions in an unsupervised way, allowing the model to condition only the foreground reconstruction on the unsupervised landmarks. Our experiments demonstrate that the proposed factorization results in landmarks that are focused on the foreground object of interest when measured against ground-truth foreground masks. Furthermore, the rendered background quality is also improved as ill-suited landmarks are no longer forced to model this content. We demonstrate this improvement via improved image fidelity in a video-prediction task. Code is available at https://github.com/NVIDIA/UnsupervisedLandmarkLearning.	[Dundar, Aysegul] Bilkent Univ, Dept Comp Sci, TR-06800 Ankara, Turkey; [Dundar, Aysegul; Shih, Kevin; Garg, Animesh; Pottorff, Robert; Tao, Andrew; Catanzaro, Bryan] NVIDIA, Santa Clara, CA 95051 USA	Ihsan Dogramaci Bilkent University; Nvidia Corporation	Dundar, A (corresponding author), Bilkent Univ, Dept Comp Sci, TR-06800 Ankara, Turkey.	adundar@cs.bilkent.edu.tr; kshih@nvidia.com; animeshg@nvidia.com; rpottorff@nvidia.com; atao@nvidia.com; bcatanzaro@nvidia.com						Babaeizadeh M, 2017, ARXIV 171011252; Balakrishnan G, 2018, PROC CVPR IEEE, P8340, DOI 10.1109/CVPR.2018.00870; Charles J., 2013, P BRIT MACH VIS C; Denton E. L., 2017, ADV NEURAL INFORM PR, P4414; Denton E, 2018, PR MACH LEARN RES, V80; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Jakab T, 2018, ADV NEUR IN, V31; Kanazawa A, 2016, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2016.354; Kim Y., 2019, ADV NEURAL INFORM PR, P3809; Kumar M., 2019, ARXIV 190301434; Lee A. X., 2018, ARXIV 180401523; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lorenz D, 2019, PROC CVPR IEEE, P10947, DOI 10.1109/CVPR.2019.01121; Ma LQ, 2017, ADV NEUR IN, V30; Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018; Miyato T., 2018, INT C LEARN REPR; Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343; Oh J., 2015, P ADV NEUR INF PROC, P2863; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222; Reda FA, 2019, IEEE I CONF COMP VIS, P892, DOI 10.1109/ICCV.2019.00098; Rhodin H, 2018, LECT NOTES COMPUT SC, V11214, P765, DOI 10.1007/978-3-030-01249-6_46; Rhodin H, 2019, PROC CVPR IEEE, P7695, DOI 10.1109/CVPR.2019.00789; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Saito M, 2017, IEEE I CONF COMP VIS, P2849, DOI 10.1109/ICCV.2017.308; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; Shih K. J., 2019, ARXIV 190902749; Siarohin A, 2019, ADV NEUR IN, V32; Siarohin A, 2019, PROC CVPR IEEE, P2372, DOI 10.1109/CVPR.2019.00248; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Suwajanakorn S, 2018, ADV NEUR IN, V31; Thewlis J, 2017, ADV NEUR IN, V30; Thewlis J, 2017, IEEE I CONF COMP VIS, P3229, DOI 10.1109/ICCV.2017.348; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; Villegas R, 2017, PR MACH LEARN RES, V70; Walker J, 2015, IEEE I CONF COMP VIS, P2443, DOI 10.1109/ICCV.2015.281; Wichers N, 2018, PR MACH LEARN RES, V80; Xue Tianfan, 2016, ADV NEURAL INFORM PR, P2; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang YT, 2018, PROC CVPR IEEE, P2694, DOI 10.1109/CVPR.2018.00285; Zhao L, 2018, LECT NOTES COMPUT SC, V11219, P403, DOI 10.1007/978-3-030-01267-0_24	42	1	1	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3883	3894		10.1109/TPAMI.2021.3055560	http://dx.doi.org/10.1109/TPAMI.2021.3055560			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33513098	Green Submitted, Green Published			2022-12-18	WOS:000805820500039
J	Engelsma, JJ; Deb, D; Cao, K; Bhatnagar, A; Sudhish, PS; Jain, AK				Engelsma, Joshua James; Deb, Debayan; Cao, Kai; Bhatnagar, Anjoo; Sudhish, Prem Sewak; Jain, Anil K.			Infant-ID: Fingerprints for Global Good	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Pediatrics; Fingerprint recognition; Face recognition; Hospitals; Reliability; Iris recognition; Authentication; Infant mortality; infantid; biometrics for global good; high resolution fingerprint reader; high resolution fingerprint matcher	INDIVIDUALITY	In many of the least developed and developing countries, a multitude of infants continue to suffer and die from vaccine-preventable diseases and malnutrition. Lamentably, the lack of official identification documentation makes it exceedingly difficult to track which infants have been vaccinated and which infants have received nutritional supplements. Answering these questions could prevent this infant suffering and premature death around the world. To that end, we propose Infant-Prints, an end-to-end, low-cost, infant fingerprint recognition system. Infant-Prints is comprised of our (i) custom built, compact, low-cost (85 USD), high-resolution (1,900 ppi), ergonomic fingerprint reader, and (ii) high-resolution infant fingerprint matcher. To evaluate the efficacy of Infant-Prints, we collected a longitudinal infant fingerprint database captured in four different sessions over a 12-month time span (December 2018 to January 2020), from 315 infants at the Saran Ashram Hospital, a charitable hospital in Dayalbagh, Agra, India. Our experimental results demonstrate, for the first time, that Infant-Prints can deliver accurate and reliable recognition (over time) of infants enrolled between the ages of 2-3 months, in time for effective delivery of vaccinations, healthcare, and nutritional supplements (TAR = 95.2% @ FAR = 1.0% for infants aged 8-16 weeks at enrollment and authenticated 3 months later).(1) 1. A preliminary version of this paper was present at CVPRW Computer Vision for Global Challenges, Long Beach, CA, 2019.	[Engelsma, Joshua James; Deb, Debayan; Jain, Anil K.] Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48824 USA; [Cao, Kai] Goodix San Diego, San Diego, CA 92121 USA; [Bhatnagar, Anjoo] Saran Ashram Hosp, Dayalbagh 282005, UP, India; [Sudhish, Prem Sewak] Dayalbagh Educ Inst, Dayalbagh 282005, UP, India	Michigan State University; Dayalbagh Educational Institute (DEI)	Engelsma, JJ (corresponding author), Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48824 USA.	engelsm7@cse.msu.edu; debdebay@cse.msu.edu; caokai0505@gmail.com; dranjoo@gmail.com; pss@alumni.stanford.edu; jain@cse.msu.edu	Boothapati, Anil Kumar/HHS-1813-2022					Babler W J, 1991, Birth Defects Orig Artic Ser, V27, P95; Beck H. C., 2008, INVESTIGATIVE OPHTHA, V49, P2265; Cao K, 2020, IEEE T INF FOREN SEC, V15, P880, DOI 10.1109/TIFS.2019.2930487; Centers for Disease Control and Prevention, 2018, POS PAR TIPS; Cummins H, 1961, FINGER PRINTS PALMS, V319; Deb D, 2018, ARXIV 180408122; Nguyen DL, 2018, INT CONF BIOMETR, P9, DOI 10.1109/ICB2018.2018.00013; Dutch Ministry of the Interior and Kingdom Relations, 2004, EV REP BIOM TRIAL 2B; Engelsma J. J, 2019, P IEEECVF C COMPUTER, P67; Engelsma JJ, 2021, IEEE T PATTERN ANAL, V43, P1981, DOI 10.1109/TPAMI.2019.2961349; Engelsma JJ, 2018, INT CONF BIOMETR THE, DOI 10.1109/ICOPS35962.2018.9575371; Engelsma JJ, 2019, IEEE T PATTERN ANAL, V41, P2511, DOI 10.1109/TPAMI.2018.2858764; Galton F., 1899, BRIT ASS ADVANCEMENT, V69, P868; Government of India, 2019, AADHAR UN ID AUTH IN; Index Mundi, 2018, WORLD BIRTH RAT; Index Mundi, 2018, WORLD AG STRUCT; Indovina M. D., 2011, NIST EVALUATION LATE; Jain A. K., 2011, INTRO BIOMETRICS; Jain AK, 2017, IEEE T INF FOREN SEC, V12, P1501, DOI 10.1109/TIFS.2016.2639346; Jain AK, 2014, 2014 IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2014); Joint Research Center of the European Commission, 2013, FINGERPRINT RECOGNIT; Kotzerke J, 2018, IET BIOMETRICS, V7, P567, DOI 10.1049/iet-bmt.2017.0282; Lemes R. P., 2011, P INT JOINT C BIOM, P1; Lin CH, 2018, IEEE T IMAGE PROCESS, V27, P2008, DOI 10.1109/TIP.2017.2788866; LIU E, 2017, COMPUTER VISION PATT, P1653; Malhotra Aakarsh, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P350, DOI 10.1109/TBIOM.2020.2999850; OKAJIMA M, 1975, J MED GENET, V12, P243, DOI 10.1136/jmg.12.3.243; Pankanti S, 2002, IEEE T PATTERN ANAL, V24, P1010, DOI 10.1109/TPAMI.2002.1023799; Preciozzi Javier, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P68, DOI 10.1109/TBIOM.2019.2962188; Rahmun F., 2010, BEST PRACTICE FINGER; Saggese Steven, 2019, Gates Open Res, V3, P1477, DOI 10.12688/gatesopenres.12914.2; Sankaran A, 2015, INT CONF BIOMETR THE; Schneider J. K, 2010, QUANTIFYING DERMATOG; SilkID, 2019, SILK20 READ; Tang Y, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P108, DOI 10.1109/BTAS.2017.8272688; Unicef, FAC FING FEET; Nguyen V, 2020, IET BIOMETRICS, V9, P47, DOI 10.1049/iet-bmt.2019.0017; WHO UNICEF, 2018, PROGR CHALL ACH UN I; World Food Programme, 2018, WFP DEM ACT UNC MIS; World Food Programme Insight, 2018, THES CHANG SHOW WFP; WorldHealth Organization, 2011, PREV NOT CUR TACKL H; Yoon S, 2015, P NATL ACAD SCI USA, V112, P8555, DOI 10.1073/pnas.1410272112; Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262; Zhou BC, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107273; Zhou W, 2013, 2013 6TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), VOLS 1-3, P1736, DOI 10.1109/CISP.2013.6743956; Zhou W, 2014, ELECTRON LETT, V50, P1061, DOI 10.1049/el.2014.1927; Zhu Y, 2007, IEEE T INF FOREN SEC, V2, P391, DOI 10.1109/TIFS.2007.903846	47	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3543	3559		10.1109/TPAMI.2021.3057634	http://dx.doi.org/10.1109/TPAMI.2021.3057634			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33556001	Green Submitted			2022-12-18	WOS:000805820500016
J	Li, ZC; Zhang, ZS; Zhao, H; Wang, R; Chen, KH; Utiyama, M; Sumita, E				Li, Zuchao; Zhang, Zhuosheng; Zhao, Hai; Wang, Rui; Chen, Kehai; Utiyama, Masao; Sumita, Eiichiro			Text Compression-Aided Transformer Encoding	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Encoding; Context modeling; Computational modeling; Machine translation; Bit error rate; Training; Natural language processing; text compression; transformer encoding; neural machine translation; machine reading comprehension		Text encoding is one of the most important steps in Natural Language Processing (NLP). It has been done well by the self-attention mechanism in the current state-of-the-art Transformer encoder, which has brought about significant improvements in the performance of many NLP tasks. Though the Transformer encoder may effectively capture general information in its resulting representations, the backbone information, meaning the gist of the input text, is not specifically focused on. In this paper, we propose explicit and implicit text compression approaches to enhance the Transformer encoding and evaluate models using this approach on several typical downstream tasks that rely on the encoding heavily. Our explicit text compression approaches use dedicated models to compress text, while our implicit text compression approach simply adds an additional module to the main model to handle text compression. We propose three ways of integration, namely backbone source-side fusion, target-side fusion, and both-side fusion, to integrate the backbone information into Transformer-based models for various downstream tasks. Our evaluation on benchmark datasets shows that the proposed explicit and implicit text compression approaches improve results in comparison to strong baselines. We therefore conclude, when comparing the encodings to the baseline models, text compression helps the encoders to learn better language representations.	[Li, Zuchao; Zhang, Zhuosheng; Zhao, Hai; Wang, Rui] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China; [Li, Zuchao; Zhang, Zhuosheng; Zhao, Hai; Wang, Rui] Shanghai Liao Tong Univ, Key Lab, Shanghai Educ Commiss Intelligent Interact & Cogn, Shanghai 200240, Peoples R China; [Li, Zuchao; Zhang, Zhuosheng; Zhao, Hai; Wang, Rui] Shanghai Jiao Tong Univ, AI Inst, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China; [Chen, Kehai; Utiyama, Masao; Sumita, Eiichiro] Natl Inst Informat & Commun Technol NICT, Kyoto 6190289, Japan	Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; National Institute of Information & Communications Technology (NICT) - Japan	Zhao, H; Wang, R (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.; Zhao, H; Wang, R (corresponding author), Shanghai Liao Tong Univ, Key Lab, Shanghai Educ Commiss Intelligent Interact & Cogn, Shanghai 200240, Peoples R China.	charlee@sjtu.edu.cn; hangzs@sjtu.edu.cn; zhaohai@cs.sjtu.edu.cn; wangrui.nlp@gmail.com; khchen@nict.go.jp; mutiyama@nict.go.jp; eiichiro.sumita@nict.go.jp			National Key Research and Development Program of China [2017YFB0304100]; Key Projects of National Natural Science Foundation of China [U1836222, 61733011]; Huawei-SJTU long term AI project, Cutting-edge Machine Reading Comprehension and Language Model	National Key Research and Development Program of China; Key Projects of National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Huawei-SJTU long term AI project, Cutting-edge Machine Reading Comprehension and Language Model	We thank Kevin Parnow, from the Department of Computer Science and Engineering, Shanghai Jiao Tong University (parnow@sjtu.edu.cn) for editing a draft of this manuscript. This paper was partially supported by the National Key Research and Development Program of China under Grant 2017YFB0304100, the Key Projects of National Natural Science Foundation of China under Grant U1836222 and Grant 61733011, Huawei-SJTU long term AI project, Cutting-edge Machine Reading Comprehension and Language Model.	Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223; Cettolo Mauro, 2015, P INT WORKSH SPOK LA; Che WX, 2015, IEEE-ACM T AUDIO SPE, V23, P2111, DOI 10.1109/TASLP.2015.2443982; Cheng JP, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P484; Collins M., 2005, P 43 ANN M ASS COMP, V43, P531, DOI DOI 10.3115/1219840.1219906; Conneau A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2126; Devlin J., 2019, P 2019 C N AM CHAPT, V1, P4171; Dou Zi-Yi, 2018, EMNLP; Duan XY, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3162; Fan A, 2018, NEURAL MACHINE TRANSLATION AND GENERATION, P45; Federmann Christian, 2018, P 3 C MACH TRANSL SH, P272, DOI DOI 10.18653/V1/W18-6401; Fevry Thibault, 2018, P 22 C COMP NAT LANG, P413, DOI DOI 10.18653/V1/K18-1040; Gehring J, 2017, PR MACH LEARN RES, V70; Gu JH, 2018, INT CONF OPTIC MEMS, P175; Gururangan S., 2020, P 58 ANN M ASS COMP, P8342; Halliday MAK., 1985, INTRO FUNCTIONAL GRA; Hasler E, 2017, COMPUT SPEECH LANG, V45, P221, DOI 10.1016/j.csl.2016.12.001; Hermann KM, 2015, ADV NEUR IN, V28; Hu B., 2015, ARXIV150605865, P1967, DOI [DOI 10.18653/V1/D15-1229, DOI 10.18653/V1/D15]; Hu MH, 2019, AAAI CONF ARTIF INTE, P6529; Jia Robin, 2017, P 2017 C EMP METH NA, P2021, DOI DOI 10.18653/V1/D17-1215; Jing HY, 2002, COMPUT LINGUIST, V28, P527, DOI 10.1162/089120102762671972; Kikuchi Yuta, 2016, P 2016 C EMP METH NA, P1328, DOI DOI 10.18653/V1/D16-1140; Knight K, 2002, ARTIF INTELL, V139, P91, DOI 10.1016/S0004-3702(02)00222-9; Lai G., 2017, EMNLP, P785, DOI [10.18653/v1/D17-1082, DOI 10.18653/V1/D17-1082]; Lan Z., 2020, ARXIV; Lewis Mike, 2020, P 58 ANN M ASS COMP, P7871, DOI DOI 10.18653/V1/2020.ACL-MAIN.703; Lin C.-Y., 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.3115/V1/D14-1020; Liu Y, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3730; McCoy RT, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3428; Mellebeek B., 2006, PROC 11 ANN C EUR AS; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Mudrakarta PK, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1896; Napoles Courtney, 2012, P JOINT WORKSH AUT K, P95; Pouget-Abadie J, 2014, P 8 WORKSH SYNT SEM; Qi W., 2020, P 2020 C EMP METH NA, P2401, DOI DOI 10.18653/V1/2020.FINDINGS-EMNLP.217; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Rajpurkar P., 2016, CORR, P2383, DOI [10.18653/v1/D16-1264, DOI 10.18653/V1/D16-1264]; Rajpurkar P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P784; Ran Q., 2019, ARXIV 190303033; Rush Alexander M, 2015, P 2015 C EMP METH NA, P379, DOI DOI 10.18653/V1/D15-1044; See A, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1073, DOI 10.18653/v1/P17-1099; Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715; Song KT, 2019, PR MACH LEARN RES, V97; Sudoh K., 2010, P JOINT 5 WORKSHOP S, P418; Sun F, 2018, ARXIV 181006638; UKEssays, 2018, DIST SENT UTT; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Vinyals O., 2015, ADV NEURAL INFORM PR, P2692; Wang X, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6197; Wang Y, 2018, P 2018 C EMP METH NA, DOI [10.18653/v1/d18-1451, DOI 10.18653/V1/D18-1451]; Williams A., 2018, P C N AM CHAPT ASS C, V1, P1112, DOI DOI 10.18653/V1/N18-1101; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Xiao T, 2016, AAAI CONF ARTIF INTE, P2856; Xiao T, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P563; Yang BS, 2019, AAAI CONF ARTIF INTE, P387; Yang Baosong, 2018, P 2018 C EMP METH NA, P4449; Yang ZW, 2021, ANIM BIOTECHNOL, V32, P67, DOI 10.1080/10495398.2019.1653901; Zhang J., 2020, P MACHINE LEARNING R, V119, P11328, DOI DOI 10.1038/S41746-021-00437-0; Zhang ZS, 2021, AAAI CONF ARTIF INTE, V35, P14506; Zhu P, 2020, ARXIV 200109415	64	1	1	19	20	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3840	3857		10.1109/TPAMI.2021.3058341	http://dx.doi.org/10.1109/TPAMI.2021.3058341			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33577448	Green Submitted			2022-12-18	WOS:000805820500036
J	Tukra, S; Marcus, HJ; Giannarou, S				Tukra, Samyakh; Marcus, Hani J.; Giannarou, Stamatia			See-Through Vision With Unsupervised Scene Occlusion Reconstruction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image reconstruction; Surgery; Convolution; Three-dimensional displays; Training; Generators; Streaming media; Occlusion removal; image reconstruction; video inpainting; unsupervised deep learning; generative models		Among the greatest of the challenges of minimally invasive surgery (MIS) is the inadequate visualisation of the surgical field through keyhole incisions. Moreover, occlusions caused by instruments or bleeding can completely obfuscate anatomical landmarks, reduce surgical vision and lead to iatrogenic injury. The aim of this paper is to propose an unsupervised end-to-end deep learning framework, based on fully convolutional neural networks to reconstruct the view of the surgical scene under occlusions and provide the surgeon with intraoperative see-through vision in these areas. A novel generative densely connected encoder-decoder architecture has been designed which enables the incorporation of temporal information by introducing a new type of 3D convolution, the so called 3D partial convolution, to enhance the learning capabilities of the network and fuse temporal and spatial information. To train the proposed framework, a unique loss function has been proposed which combines feature matching, reconstruction, style, temporal and adversarial loss terms, for generating high fidelity image reconstructions. Advancing the state-of-the-art, our method can reconstruct the underlying view obstructed by irregularly shaped occlusions of divergent size, location and orientation. The proposed method has been validated on in vivo MIS video data, as well as natural scenes on a range of occlusion-to-image (OIR) ratios. It has also been compared against the latest video inpainting models in terms of image reconstruction quality using different assessment metrics. The performance evaluation analysis verifies the superiority of our proposed method and its potential clinical value.	[Tukra, Samyakh; Giannarou, Stamatia] Imperial Coll London, Hamlyn Ctr Robot Surg, Dept Surg & Canc, London SW7 2AZ, England; [Marcus, Hani J.] UCL, UCL Queen Sq Inst Neurol, London WC1E 6BT, England	Imperial College London; University of London; University College London	Tukra, S (corresponding author), Imperial Coll London, Hamlyn Ctr Robot Surg, Dept Surg & Canc, London SW7 2AZ, England.	samyakh.tukra17@imperial.ac.uk; hani.marcus10@imperial.ac.uk; stamatia.giannarou@imperial.ac.uk		Tukra, Sam/0000-0003-4317-7458; Marcus, Hani/0000-0001-8000-392X; Giannarou, Stamatia/0000-0002-8745-1343	Royal Society [UF140290, RGF80084]; NIHR Imperial Biomedical Research Centre (BRC); NIHR University College London (UCL) Biomedical Research Centre (BRC); Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS)	Royal Society(Royal Society of London); NIHR Imperial Biomedical Research Centre (BRC); NIHR University College London (UCL) Biomedical Research Centre (BRC); Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The work of Stamatia Giannarou and Samyakh Tukra was supported by the Royal Society (UF140290 and RGF80084) and the NIHR Imperial Biomedical Research Centre (BRC). The work of Hani J. Marcus was supported by the NIHR University College London (UCL) Biomedical Research Centre (BRC) and the Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS).	Allan Max, 2017, ARXIV190206426; Allan Max, 2018, ARXIV200111190; Armanious K, 2019, INT CONF ACOUST SPEE, P3267, DOI 10.1109/ICASSP.2019.8682677; Barnes C, 2010, LECT NOTES COMPUT SC, V6313, P29; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Giannarou S, 2013, IEEE T PATTERN ANAL, V35, P130, DOI 10.1109/TPAMI.2012.81; Hsu W., 2019, PROC BRIT MACH VIS C; Huang JB, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982398; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Jolicoeur-Martineau Alexia, 2019, P INT C LEARN REPR I; Kim D, 2019, PROC CVPR IEEE, P5785, DOI 10.1109/CVPR.2019.00594; Lee S, 2019, IEEE I CONF COMP VIS, P4412, DOI 10.1109/ICCV.2019.00451; Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6; Liu X., 2007, INT C SERV SYST SERV, P1; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Marcus HJ, 2014, BRIT J NEUROSURG, V28, P606, DOI 10.3109/02688697.2014.887654; Pont-Tuset J., 2017, ARXIV 170400675; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Simonyan K., 2015, ICLR; Sogancioglu E., 2018, ARXIV180901471; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Wang Y, 2018, PROC CVPR IEEE, P4884, DOI 10.1109/CVPR.2018.00513; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60	26	1	1	5	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3779	3790		10.1109/TPAMI.2021.3058410	http://dx.doi.org/10.1109/TPAMI.2021.3058410			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33566758				2022-12-18	WOS:000805820500032
J	Yang, S; Wang, ZY; Liu, JY				Yang, Shuai; Wang, Zhangyang; Liu, Jiaying			Shape-Matching GAN plus plus : Scale Controllable Dynamic Artistic Text Style Transfer	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Strain; Shape; Gallium nitride; Generative adversarial networks; Dynamics; Deformable models; Animation; Text style transfer; structure transfer; scale control; temporal consistency	SPACE	Dynamic artistic text style transfer aims to migrate the style in terms of both the appearance and motion patterns from a reference style video to the target text to create artistic text animation. Recent researches have improved the usability of transfer models by introducing texture control. However, it remains an important open challenge to investigate the control of the stylistic degree with respect to shape deformation. In this paper, we explore a new problem of dynamic artistic text style transfer with glyph stylistic degree control. The key idea is to build multi-scale glyph-style shape mappings through a novel bidirectional shape matching framework. Following this idea, we first introduce a scale-ware Shape-Matching GAN to learn such mappings to simultaneously model the style shape features at multiple scales and transfer them onto the target glyph. Furthermore, an advanced Shape-Matching GAN++ is proposed to animate a static text image based on the reference style video. Our Shape-Matching GAN++ characterizes the short-term consistency of motion patterns via shape matchings within consecutive frames, which are propagated to achieve effective long-term consistency. Experiments show that the proposed method outperforms previous state-of-the-arts both qualitatively and quantitatively, and generate high-quality and controllable artistic text.	[Yang, Shuai; Liu, Jiaying] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100080, Peoples R China; [Wang, Zhangyang] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA	Peking University; University of Texas System; University of Texas Austin	Liu, JY (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100080, Peoples R China.	williamyang@pku.edu.cn; atlaswang@utexas.edu; liujiaying@pku.edu.cn			Fundamental Research Funds for the Central Universities; National Natural Science Foundation of China [61772043]	Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the Fundamental Research Funds for the Central Universities and the National Natural Science Foundation of China under contract No. 61772043.	Azadil S, 2018, PROC CVPR IEEE, P7564, DOI 10.1109/CVPR.2018.00789; BABAUD J, 1986, IEEE T PATTERN ANAL, V8, P26, DOI 10.1109/TPAMI.1986.4767749; Champandard Alex J., 2016, SEMANTIC STYLE TRANS, P2; Chang HW, 2018, PROC CVPR IEEE, P40, DOI 10.1109/CVPR.2018.00012; Chen DD, 2017, IEEE I CONF COMP VIS, P1114, DOI 10.1109/ICCV.2017.126; Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296; Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gupta A, 2017, IEEE I CONF COMP VIS, P4087, DOI 10.1109/ICCV.2017.438; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang HZ, 2017, PROC CVPR IEEE, P7044, DOI 10.1109/CVPR.2017.745; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272; Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43; Li YJ, 2017, ADV NEUR IN, V30; Men Y., 2019, PROC IEEE INT C COMP, P5870; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Rosenberger A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618453; Ruder M, 2016, LECT NOTES COMPUT SC, V9796, P26, DOI 10.1007/978-3-319-45886-1_3; Sanakoyeu A, 2018, LECT NOTES COMPUT SC, V11212, P715, DOI 10.1007/978-3-030-01237-3_43; Wang WJ, 2020, AAAI CONF ARTIF INTE, V34, P12233; Wang WJ, 2019, PROC CVPR IEEE, P5882, DOI 10.1109/CVPR.2019.00604; Wang XT, 2019, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2019.00179; Wang ZY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P451, DOI 10.1145/2733373.2806219; Yang S, 2019, IEEE I CONF COMP VIS, P4441, DOI 10.1109/ICCV.2019.00454; Yang S, 2021, IEEE T PATTERN ANAL, V43, P3709, DOI 10.1109/TPAMI.2020.2983697; Yang S, 2019, AAAI CONF ARTIF INTE, P1238; Yang S, 2019, IEEE T IMAGE PROCESS, V28, P952, DOI 10.1109/TIP.2018.2873064; Yang S, 2017, PROC CVPR IEEE, P2886, DOI 10.1109/CVPR.2017.308; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	39	1	1	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3807	3820		10.1109/TPAMI.2021.3055211	http://dx.doi.org/10.1109/TPAMI.2021.3055211			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33507863				2022-12-18	WOS:000805820500034
J	Yu, Y; Smith, W				Yu, Ye; Smith, William			Outdoor Inverse Rendering From a Single Image Using Multiview Self-Supervision	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Lighting; Rendering (computer graphics); Training; Shape; Estimation; Geometry; Image decomposition; Inverse rendering; shape-from-shading; intrinsic image decomposition; illumination estimation	REFLECTANCE; STEREO; SHAPE	In this paper we show how to perform scene-level inverse rendering to recover shape, reflectance and lighting from a single, uncontrolled image using a fully convolutional neural network. The network takes an RGB image as input, regresses albedo, shadow and normal maps from which we infer least squares optimal spherical harmonic lighting coefficients. Our network is trained using large uncontrolled multiview and timelapse image collections without ground truth. By incorporating a differentiable renderer, our network can learn from self-supervision. Since the problem is ill-posed we introduce additional supervision. Our key insight is to perform offline multiview stereo (MVS) on images containing rich illumination variation. From the MVS pose and depth maps, we can cross project between overlapping views such that Siamese training can be used to ensure consistent estimation of photometric invariants. MVS depth also provides direct coarse supervision for normal map estimation. We believe this is the first attempt to use MVS supervision for learning inverse rendering. In addition, we learn a statistical natural illumination prior. We evaluate performance on inverse rendering, normal map estimation and intrinsic image decomposition benchmarks.	[Yu, Ye; Smith, William] Univ York, Dept Comp Sci, York YO10 5DD, N Yorkshire, England	University of York - UK	Smith, W (corresponding author), Univ York, Dept Comp Sci, York YO10 5DD, N Yorkshire, England.	yy1571@york.ac.uk; william.smith@york.ac.uk			Royal Academy of Engineering under the Leverhulme Trust Senior Fellowship scheme	Royal Academy of Engineering under the Leverhulme Trust Senior Fellowship scheme	The work of William A. P. Smith was supported by the Royal Academy of Engineering under the Leverhulme Trust Senior Fellowship scheme. The Titan Xp used for this research was donated by the NVIDIA Corporation.	Ackermann J, 2012, PROC CVPR IEEE, P262, DOI 10.1109/CVPR.2012.6247684; Aittala M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925917; Aldrian O, 2013, IEEE T PATTERN ANAL, V35, P1080, DOI 10.1109/TPAMI.2012.206; Alldrin N, 2008, PROC CVPR IEEE, P2447; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Baslamisli AS, 2018, PROC CVPR IEEE, P6674, DOI 10.1109/CVPR.2018.00698; Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206; Bi S., 2018, PROC EUROGRAPHICS S, P53; Chen Weifeng, 2016, NEURIPS, P2, DOI DOI 10.5555/3157096.3157178; Dror RO, 2001, PROC CVPR IEEE, P164; Eigen D, 2014, ADV NEUR IN, V27; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Fan QN, 2018, PROC CVPR IEEE, P8944, DOI 10.1109/CVPR.2018.00932; Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Gao D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323042; Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45; Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Goesele M, 2007, IEEE I CONF COMP VIS, P825, DOI 10.1109/iccv.2007.4408933; Goldman D. B., 2009, IEEE T PATTERN ANAL, V32, P1060, DOI DOI 10.1109/TPAMI.2009.102; GOLUB GH, 1973, SIAM J NUMER ANAL, V10, P413, DOI 10.1137/0710036; Haber T, 2009, PROC CVPR IEEE, P627, DOI 10.1109/CVPRW.2009.5206753; Han GY, 2018, IEEE SIGNAL PROC LET, V25, P753, DOI 10.1109/LSP.2018.2820041; hdri, 2020, HDRI SKIES; Janner M, 2017, ADV NEUR IN, V30; Jeon J, 2014, LECT NOTES COMPUT SC, V8695, P218, DOI 10.1007/978-3-319-10584-0_15; Ji MQ, 2017, IEEE I CONF COMP VIS, P2326, DOI 10.1109/ICCV.2017.253; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kanamori Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275104; Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kim K, 2016, LECT NOTES COMPUT SC, V9907, P750, DOI 10.1007/978-3-319-46487-9_46; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; KLEFFNER DA, 1992, PERCEPT PSYCHOPHYS, V52, P18, DOI 10.3758/BF03206757; Kulkarni TD, 2015, ADV NEUR IN, V28; Labs H, SIBL ARCH 2007 20; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Lettry L, 2018, IEEE WINT CONF APPL, P1359, DOI 10.1109/WACV.2018.00153; Li X, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073641; Li ZQ, 2018, PROC CVPR IEEE, P9039, DOI 10.1109/CVPR.2018.00942; Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218; Li ZQ, 2018, LECT NOTES COMPUT SC, V11207, P381, DOI 10.1007/978-3-030-01219-9_23; Li ZQ, 2020, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR42600.2020.00255; Li Zhengqin, 2018, ACM T GRAPHICS, V37, P2; Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152; Liu GL, 2017, IEEE I CONF COMP VIS, P2280, DOI 10.1109/ICCV.2017.248; Lombardi S, 2016, INT CONF 3D VISION, P305, DOI 10.1109/3DV.2016.39; Lombardi S, 2016, IEEE T PATTERN ANAL, V38, P129, DOI 10.1109/TPAMI.2015.2430318; Ma WC, 2018, LECT NOTES COMPUT SC, V11218, P211, DOI 10.1007/978-3-030-01264-9_13; Mather G, 2017, I-PERCEPTION, V8, DOI 10.1177/2041669517710031; Narihira T, 2015, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2015.342; Nestmeyer T, 2019, ARXIV 190603355; Nestmeyer T, 2017, PROC CVPR IEEE, P1771, DOI 10.1109/CVPR.2017.192; Oxholm G, 2016, IEEE T PATTERN ANAL, V38, P376, DOI 10.1109/TPAMI.2015.2450734; Philip J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323013; Romeiro F, 2008, LECT NOTES COMPUT SC, V5305, P859, DOI 10.1007/978-3-540-88693-8_63; Romeiro F, 2010, LECT NOTES COMPUT SC, V6311, P45, DOI 10.1007/978-3-642-15549-9_4; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Sengupta S, 2019, IEEE I CONF COMP VIS, P8597, DOI 10.1109/ICCV.2019.00869; Sengupta S, 2018, PROC CVPR IEEE, P6296, DOI 10.1109/CVPR.2018.00659; Shelhamer E, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P235, DOI 10.1109/ICCVW.2015.39; Shi J, 2017, PROC CVPR IEEE, P5844, DOI 10.1109/CVPR.2017.619; Shu ZX, 2017, PROC CVPR IEEE, P5444, DOI 10.1109/CVPR.2017.578; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Tewari A, 2017, IEEE I CONF COMP VIS, P3735, DOI 10.1109/ICCV.2017.401; Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30; Vasiljevic Igor, 2019, ARXIV190800463; Vijayanarasimhan S, 2017, ARXIV 170407804; Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216; Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897; Xu D, 2017, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2017.25; Ye Yu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P84, DOI 10.1007/978-3-030-58542-6_6; Yu WH, 2019, PETROL SCI TECHNOL, V37, P1338, DOI 10.1080/10916466.2019.1581813; Yu Y, 2019, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2019.00327; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zheng CX, 2018, LECT NOTES COMPUT SC, V11211, P798, DOI 10.1007/978-3-030-01234-2_47; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700; Zou CH, 2018, PROC CVPR IEEE, P2051, DOI 10.1109/CVPR.2018.00219	81	1	1	6	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2022	44	7					3659	3675		10.1109/TPAMI.2021.3058105	http://dx.doi.org/10.1109/TPAMI.2021.3058105			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1V0WH	33560977	Green Accepted, Green Submitted			2022-12-18	WOS:000805820500024
J	Chai, LY; Liu, YT; Liu, WX; Han, GQ; He, SF				Chai, Liangyu; Liu, Yongtuo; Liu, Wenxi; Han, Guoqiang; He, Shengfeng			CrowdGAN: Identity-Free Interactive Crowd Video Generation and Beyond	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Trajectory; Task analysis; Three-dimensional displays; Predictive models; Analytical models; Uncertainty; Solid modeling; Crowd video generation; data augmentation; crowd analysis	ASSIGNMENT; TRACKING; MODEL	In this paper, we introduce a novel yet challenging research problem, interactive crowd video generation, committed to producing diverse and continuous crowd video, and relieving the difficulty of insufficient annotated real-world datasets in crowd analysis. Our goal is to recursively generate realistic future crowd video frames given few context frames, under the user-specified guidance, namely individual positions of the crowd. To this end, we propose a deep network architecture specifically designed for crowd video generation that is composed of two complementary modules, each of which combats the problems of crowd dynamic synthesis and appearance preservation respectively. Particularly, a spatio-temporal transfer module is proposed to infer the crowd position and structure from guidance and temporal information, and a point-aware flow prediction module is presented to preserve appearance consistency by flow-based warping. Then, the outputs of the two modules are integrated by a self-selective fusion unit to produce an identity-preserved and continuous video. Unlike previous works, we generate continuous crowd behaviors beyond identity annotations or matching. Extensive experiments show that our method is effective for crowd video generation. More importantly, we demonstrate the generated video can produce diverse crowd behaviors and be used for augmenting different crowd analysis tasks, i.e., crowd counting, anomaly detection, crowd video prediction. Code is available at https://github.com/Icep2020/CrowdGAN.	[Chai, Liangyu; Liu, Yongtuo; Han, Guoqiang; He, Shengfeng] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China; [Liu, Wenxi] Fuzhou Univ, Coll Math & Comp Sci, Fuzhou 350108, Fujian, Peoples R China	South China University of Technology; Fuzhou University	He, SF (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China.	icepoint1018@gmail.com; csmanlyt@mail.scut.edu.cn; wenxi.liu@hotmail.com; csgqhan@scut.edu.cn; hesfe@scut.edu.cn	; He, Shengfeng/E-5682-2016	Liu, Yongtuo/0000-0001-5953-2771; He, Shengfeng/0000-0002-3802-4644	National Natural Science Foundation of China [61972162, 61702194, 61702104, 62072110, 61472145]; CCFTencent Open Research fund	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); CCFTencent Open Research fund	This work was supported by the National Natural Science Foundation of China (No. 61972162, No. 61702194, No. 61702104, No. 62072110, and No. 61472145), and the CCFTencent Open Research fund. Liangyu Chai and Yongtuo Liu contributed equally to this work.	Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110; Ali S, 2008, LECT NOTES COMPUT SC, V5303, P1, DOI 10.1007/978-3-540-88688-4_1; Byeon W, 2018, LECT NOTES COMPUT SC, V11220, P781, DOI 10.1007/978-3-030-01270-0_46; Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45; Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569; Chen BY, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P358, DOI 10.1145/3126686.3126737; Chen K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.21; Chen XT, 2020, IEEE T MULTIMEDIA, V22, P1591, DOI 10.1109/TMM.2019.2946475; Fang YY, 2019, IEEE INT CON MULTI, P814, DOI 10.1109/ICME.2019.00145; Flagg M, 2013, IEEE T VIS COMPUT GR, V19, P1935, DOI 10.1109/TVCG.2012.317; Glorot X., 2010, J MACH LEARN RES; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gupta T, 2018, LECT NOTES COMPUT SC, V11212, P610, DOI 10.1007/978-3-030-01237-3_37; Hao ZK, 2018, PROC CVPR IEEE, P7854, DOI 10.1109/CVPR.2018.00819; Hartley R., 2003, MULTIPLE VIEW GEOMET; HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282; Hensel M, 2017, ADV NEUR IN, V30; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jaderberg M, 2015, ADV NEUR IN, V28; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kalchbrenner N, 2017, PR MACH LEARN RES, V70; Karamouzas I, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073705; Kingma Diederik P., 2015, 3 INT C LEARN REPRES, V3; Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053; Kwon YH, 2019, PROC CVPR IEEE, P1811, DOI 10.1109/CVPR.2019.00191; Lai WS, 2018, LECT NOTES COMPUT SC, V11219, P179, DOI 10.1007/978-3-030-01267-0_11; Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111; Li YT, 2018, AAAI CONF ARTIF INTE, P7065; Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120; Liang XD, 2017, IEEE I CONF COMP VIS, P1762, DOI 10.1109/ICCV.2017.194; Lisotto M, 2019, IEEE INT CONF COMP V, P2567, DOI 10.1109/ICCVW.2019.00314; Liu LB, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P849; Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524; Liu YT, 2020, IEEE T IMAGE PROCESS, V29, P6800, DOI 10.1109/TIP.2020.2994410; Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI 10.1109/ICCV.2017.478; Lotter W., 2017, ICLR, DOI [DOI 10.48550/ARXIV.1605.08104, 10.48550/arXiv.1605.08104]; Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872; Mathieu M., 2016, INT C LEARN REPR ICL; Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641; MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003; Narain R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618468; Ondrej J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778860; Ranzato M., 2016, ARXIV14126604; Ren WH, 2018, PROC CVPR IEEE, P5353, DOI 10.1109/CVPR.2018.00561; Sabokrou Mohammad, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P56, DOI 10.1109/CVPRW.2015.7301284; Schulter S, 2017, PROC CVPR IEEE, P2730, DOI 10.1109/CVPR.2017.292; Shao J, 2015, PROC CVPR IEEE, P4657, DOI 10.1109/CVPR.2015.7299097; Shao J, 2014, PROC CVPR IEEE, P2227, DOI 10.1109/CVPR.2014.285; Shi XJ, 2015, ADV NEUR IN, V28; SUNG M, 2005, P 2005 ACM SIGGRAPH, P291, DOI DOI 10.1145/1073368.1073410; Thalmann D., 2007, CROWD SIMULATION; Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839; Wang XG, 2009, IEEE T PATTERN ANAL, V31, P539, DOI 10.1109/TPAMI.2008.87; Xu YY, 2018, PROC CVPR IEEE, P5275, DOI 10.1109/CVPR.2018.00553; Xue Tianfan, 2016, ADV NEURAL INFORM PR, P2; Yersin Barbara., 2009, P S INT 3D GRAPH GAM, P207; Yi S, 2015, PROC CVPR IEEE, P3488, DOI 10.1109/CVPR.2015.7298971; Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70; Zhou BL, 2013, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2013.392; Zhou BL, 2012, PROC CVPR IEEE, P2871, DOI 10.1109/CVPR.2012.6248013; Zhu F, 2014, LECT NOTES COMPUT SC, V8694, P139, DOI 10.1007/978-3-319-10599-4_10; Zhu J, 2018, LECT NOTES COMPUT SC, V11209, P379, DOI 10.1007/978-3-030-01228-1_23	64	1	1	8	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2022	44	6					2856	2871		10.1109/TPAMI.2020.3043372	http://dx.doi.org/10.1109/TPAMI.2020.3043372			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1R1DD	33290212				2022-12-18	WOS:000803117500008
J	Fu, WJ; Wang, M; Du, MN; Liu, NH; Hao, SJ; Hu, X				Fu, Weijie; Wang, Meng; Du, Mengnan; Liu, Ninghao; Hao, Shijie; Hu, Xia			Differentiated Explanation of Deep Neural Networks With Skewed Distributions	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Generators; Perturbation methods; Tuning; Neural networks; Convolution; Visualization; Training; Deep neural networks; local explanation; relevance scores; differentiated saliency maps		Over the last decade, deep neural networks (DNNs) are regarded as black-box methods, and their decisions are criticized for the lack of explainability. Existing attempts based on local explanations offer each input a visual saliency map, where the supporting features that contribute to the decision are emphasized with high relevance scores. In this paper, we improve the saliency map based on differentiated explanations, of which the saliency map not only distinguishes the supporting features from backgrounds but also shows the different degrees of importance of the various parts within the supporting features. To do this, we propose to learn a differentiated relevance estimator called DRE, where a carefully-designed distribution controller is introduced to guide the relevance scores towards right-skewed distributions. DRE can be directly optimized under pure classification losses, enabling higher faithfulness of explanations and avoiding non-trivial hyper-parameter tuning. The experimental results on three real-world datasets demonstrate that our differentiated explanations significantly improve the faithfulness with high explainability. Our code and trained models are available at https://github.com/fuweijie/DRE.	[Fu, Weijie; Wang, Meng; Hao, Shijie] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Anhui, Peoples R China; [Fu, Weijie; Wang, Meng] Hefei Univ Technol, Intelligent Interconnected Syst Lab Anhui Prov, Hefei 230009, Anhui, Peoples R China; [Du, Mengnan; Liu, Ninghao; Hu, Xia] Texas A&M Univ, Sch Comp Sci & Engn, College Stn, TX 77840 USA	Hefei University of Technology; Hefei University of Technology; Texas A&M University System; Texas A&M University College Station	Wang, M (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Anhui, Peoples R China.; Wang, M (corresponding author), Hefei Univ Technol, Intelligent Interconnected Syst Lab Anhui Prov, Hefei 230009, Anhui, Peoples R China.	fwj.edu@gmail.com; eric.mengwang@gmail.com; dumengnan@tamu.com; nhliu43@tamu.com; hfut.hsj@gmail.com; hu@cse.tamu.edu	hu, xia hong/GQP-8544-2022	hao, shijie/0000-0003-3181-1220; Liu, Ninghao/0000-0002-9170-2424; Du, Mengnan/0000-0002-1614-6069	National Nature Science Foundation of China [61725203, 62020106007, 61772171]; Fundamental Research Funds for the Central Universities [PA2020GDKC0023, PA2019GDZC0095]; China Scholarship Council	National Nature Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); China Scholarship Council(China Scholarship Council)	The authors would like to thank the associate editor and the reviewers for their great efforts in improving the quality of this paper. This work was supported in part by the National Nature Science Foundation of China under Grants 61725203, 62020106007 and 61772171, the Fundamental Research Funds for the Central Universities under Grants PA2020GDKC0023 and PA2019GDZC0095, and the China Scholarship Council.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, P ADV NEURAL INFORM; Azzopardi L, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P1321, DOI 10.1145/3331184.3331398; Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097; Clauset A, 2009, SIAM REV, V51, P661, DOI 10.1137/070710111; Dabkowski P, 2017, ADV NEUR IN, V30; Doane DP, 2011, J STAT EDUC, V19, DOI 10.1080/10691898.2011.11889611; Du MN, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1358, DOI 10.1145/3219819.3220099; Fong R, 2019, IEEE I CONF COMP VIS, P2950, DOI 10.1109/ICCV.2019.00304; Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371; Forbes C., 2011, STAT DISTRIBUTIONS, V4; Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019; Lundberg SM, 2017, ADV NEUR IN, V30; Montavon G, 2018, DIGIT SIGNAL PROCESS, V73, P1, DOI 10.1016/j.dsp.2017.10.011; Ribeiro MT, 2018, AAAI CONF ARTIF INTE, P1527; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74; Simonyan Karen, 2013, DEEP INSIDE CONVOLUT, P2; Smilkov D, 2017, ARXIV; Springenberg J.T., 2014, ARXIV14126806; Sundararajan M, 2017, PR MACH LEARN RES, V70; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Ulyanov D., 2016, ARXIV160708022; Waissi GR, 1996, APPL MATH COMPUT, V77, P91, DOI 10.1016/0096-3003(95)00190-5; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhou B., 2014, CORR, V1412, P6856	30	1	1	3	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2022	44	6					2909	2922		10.1109/TPAMI.2021.3049784	http://dx.doi.org/10.1109/TPAMI.2021.3049784			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1R1DD	33417537				2022-12-18	WOS:000803117500011
J	Huang, BY; Sun, T; Ling, HB				Huang, Bingyao; Sun, Tao; Ling, Haibin			End-to-End Full Projector Compensation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Surface texture; Cameras; Training; Benchmark testing; Geometry; Task analysis; Pipelines; Projector compensation; projector-camera systems; image warping; image enhancement	DISPLAY	Full projector compensation aims to modify a projector input image to compensate for both geometric and photometric disturbance of the projection surface. Traditional methods usually solve the two parts separately and may suffer from suboptimal solutions. In this paper, we propose the first end-to-end differentiable solution, named CompenNeSt++, to solve the two problems jointly. First, we propose a novel geometric correction subnet, named WarpingNet, which is designed with a cascaded coarse-to-fine structure to learn the sampling grid directly from sampling images. Second, we propose a novel photometric compensation subnet, named CompenNeSt, which is designed with a siamese architecture to capture the photometric interactions between the projection surface and the projected images, and to use such information to compensate the geometrically corrected images. By concatenating WarpingNet with CompenNeSt, CompenNeSt++ accomplishes full projector compensation and is end-to-end trainable. Third, to improve practicability, we propose a novel synthetic data-based pre-training strategy to significantly reduce the number of training images and training time. Moreover, we construct the first setup-independent full compensation benchmark to facilitate future studies. In thorough experiments, our method shows clear advantages over prior art with promising compensation quality and meanwhile being practically convenient.	[Huang, Bingyao; Sun, Tao; Ling, Haibin] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA	State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook	Ling, HB (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.	bihuang@cs.stonybrook.edu; tao@cs.stonybrook.edu; hling@cs.stonybrook.edu		Ling, Haibin/0000-0003-4094-8413; Sun, Tao/0000-0002-6926-0543; Huang, Bingyao/0000-0002-8647-5730	US National Science Foundation [2006665, 1814745]	US National Science Foundation(National Science Foundation (NSF))	This work was supported in part by the US National Science Foundation (No. 2006665 and 1814745).	Aliaga DG, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159518; Asayama H, 2018, IEEE T VIS COMPUT GR, V24, P1077, DOI 10.1109/TVCG.2017.2657634; ASHDOWN M, 2006, P IEEE C COMP VIS PA, P6; Bimber O, 2005, COMPUTER, V38, P48, DOI 10.1109/MC.2005.17; Boroomand A, 2016, IEEE IMAGE PROC, P2951, DOI 10.1109/ICIP.2016.7532900; Community B.O., 2018, BLENDER A 3D MODELLI; Deshpande A, 2017, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2017.307; Donato G, 2002, LECT NOTES COMPUT SC, V2352, P21; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Fujii K, 2005, PROC CVPR IEEE, P814, DOI 10.1109/CVPR.2005.41; Geng J, 2011, ADV OPT PHOTONICS, V3, P128, DOI 10.1364/AOP.3.000128; Grossberg MD, 2004, PROC CVPR IEEE, P452; Grundhofer A, 2015, IEEE T IMAGE PROCESS, V24, P5086, DOI 10.1109/TIP.2015.2478388; Grundhofer A, 2018, COMPUT GRAPH FORUM, V37, P653, DOI 10.1111/cgf.13387; Grundhofer A, 2008, IEEE T VIS COMPUT GR, V14, P97, DOI 10.1109/TVCG.2007.1052; Harville M., 2006, C COMP VIS PATT REC, P5; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang B., 2020, ARXIV 200303040; Huang BY, 2021, IEEE T AUTOM SCI ENG, V18, P1049, DOI 10.1109/TASE.2020.2994223; Huang BY, 2019, IEEE I CONF COMP VIS, P7164, DOI 10.1109/ICCV.2019.00726; Huang BY, 2019, PROC CVPR IEEE, P6803, DOI 10.1109/CVPR.2019.00697; Huang BY, 2018, ADJUNCT PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P15, DOI 10.1109/ISMAR-Adjunct.2018.00023; Huang TH, 2017, IEEE T IMAGE PROCESS, V26, P147, DOI 10.1109/TIP.2016.2592799; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Iizuka S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925974; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jaderberg M, 2015, ADV NEUR IN, V28; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Juang R., 2007, P IEEE C COMP VIS PA, P1; Kageyama Y, 2020, OPT EXPRESS, V28, P20391, DOI 10.1364/OE.396159; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; King DB, 2015, ACS SYM SER, V1214, P1; Kurth P, 2018, IEEE T VIS COMPUT GR, V24, P2886, DOI 10.1109/TVCG.2018.2868530; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li YQ, 2018, COMPUT GRAPH FORUM, V37, P365, DOI 10.1111/cgf.13368; Moreno D, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P464, DOI 10.1109/3DIMPVT.2012.77; Narita G, 2017, IEEE T VIS COMPUT GR, V23, P1235, DOI 10.1109/TVCG.2016.2592910; NAYAR SK, 2003, P ICCV WORKSH PROJ C; OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; Raskar R, 2001, SPRING EUROGRAP, P89; Raskar R, 2003, ACM T GRAPHIC, V22, P809, DOI 10.1145/882262.882349; Raskar R, 2001, PROC CVPR IEEE, P504; Revaud J, 2016, INT J COMPUT VISION, V120, P300, DOI 10.1007/s11263-016-0908-3; Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sajadi B, 2010, LECT NOTES COMPUT SC, V6314, P72, DOI 10.1007/978-3-642-15561-1_6; Shahpaski M., 2017, PROC IEEE C COMPUT V, P3596; Shen ZY, 2019, PROC CVPR IEEE, P4219, DOI [10.1109/CVPR.2019.00435, 10.1109/cvpr.2019.00435]; Siegl Christian, 2017, [Computational Visual Media, 计算可视媒体], V3, P263; Siegl C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818111; Takeda S, 2016, IEEE T VIS COMPUT GR, V22, P1424, DOI 10.1109/TVCG.2016.2518136; Tardif JP, 2003, FOURTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P217, DOI 10.1109/im.2003.1240253; Ueda T, 2020, IEEE T VIS COMPUT GR, V26, P2051, DOI 10.1109/TVCG.2020.2973496; Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wetzstein G, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P391, DOI 10.1109/PG.2007.47; Yang LM, 2016, INT SYM MIX AUGMENT, P63, DOI 10.1109/ISMAR.2016.22; Yoshida T., 2003, P INT C VIRT SYST MU, P161; Zhang L, 2006, ACM T GRAPHIC, V25, P907, DOI 10.1145/1141911.1141974; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718; Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	66	1	1	4	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2022	44	6					2953	2967		10.1109/TPAMI.2021.3050124	http://dx.doi.org/10.1109/TPAMI.2021.3050124			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1R1DD	33417538	Green Submitted			2022-12-18	WOS:000803117500014
J	Huang, Y; Wang, JD; Wang, L				Huang, Yan; Wang, Jingdong; Wang, Liang			Few-Shot Image and Sentence Matching via Aligned Cross-Modal Memory	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Adaptation models; Task analysis; Pattern matching; Logic gates; Visualization; Image color analysis; Data models; Image and sentence matching; aligned cross-modal memory; similarity gated fusion		Image and sentence matching has attracted much attention recently, and many effective methods have been proposed to deal with it. But even the current state-of-the-arts still cannot well associate those challenging pairs of images and sentences containing few-shot content in their regions and words. In fact, such a few-shot matching problem is seldom studied and has become a bottleneck for further performance improvement in real-world applications. In this work, we formulate this challenging problem as few-shot image and sentence matching, and accordingly propose an Aligned Cross-Modal Memory (ACMM) model to deal with it. The model can not only softly align few-shot regions and words in a weakly-supervised manner, but also persistently store and update cross-modal prototypical representations of few-shot classes as references, without using any groundtruth region-word correspondence. The model can also adaptively balance the relative importance between few-shot and common content in the image and sentence, which leads to better measurement of overall similarity. We perform extensive experiments in terms of both few-shot and conventional image and sentence matching, and demonstrate the effectiveness of the proposed model by achieving the state-of-the-art results on two public benchmark datasets.	[Huang, Yan; Wang, Liang] Natl Lab Pattern Recognit NLPR, Inst Automat, Ctr Res Intelligent Percept & Comp CRIPAC, Ctr Excellence Brain Sci & Intelligence Technol C, Beijing 100190, Peoples R China; [Huang, Yan; Wang, Liang] Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing 100864, Peoples R China; [Huang, Yan; Wang, Liang] Chinese Acad Sci CAS AIR, Artificial Intelligence Res, Beijing 100190, Peoples R China; [Wang, Jingdong] Jingdong Wang Media Comp Grp, Microsoft Res Asia, Beijing 100080, Peoples R China	Microsoft; Microsoft Research Asia	Huang, Y (corresponding author), Natl Lab Pattern Recognit NLPR, Inst Automat, Ctr Res Intelligent Percept & Comp CRIPAC, Ctr Excellence Brain Sci & Intelligence Technol C, Beijing 100190, Peoples R China.; Huang, Y (corresponding author), Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing 100864, Peoples R China.; Huang, Y (corresponding author), Chinese Acad Sci CAS AIR, Artificial Intelligence Res, Beijing 100190, Peoples R China.	yhuang@nlpr.ia.ac.cn; jingdw@microsoft.com; wangliang@nlpr.ia.ac.cn	Huang, Yan/HCH-6526-2022		Major Project for New Generation of AI [2018AAA0100400]; National Natural Science Foundation of China [61525306, 61633021, 61721004, 61806194, U1803261, 61976132]; Beijing Nova Program [Z201100006820079]; Shandong Provincial Key Research and Development Program [2019JZZY010119]; CAS-AIR	Major Project for New Generation of AI; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Nova Program(Beijing Municipal Science & Technology Commission); Shandong Provincial Key Research and Development Program; CAS-AIR	This work was jointly supported by Major Project for New Generation of AI under Grant 2018AAA0100400, National Natural Science Foundation of China (61525306, 61633021, 61721004, 61806194, U1803261, and 61976132), Beijing Nova Program (Z201100006820079), Shandong Provincial Key Research and Development Program (2019JZZY010119), and CAS-AIR.	Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636; Chi JZ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P663; Dwibedi D, 2019, PROC CVPR IEEE, P1801, DOI 10.1109/CVPR.2019.00190; Eisenschtat A, 2017, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2017.201; Faghri F., 2018, PROC BRIT MACH VIS C; Frome Andrea, 2013, NEURIPS; Fu Y., 2014, PROC BRIT MACH VIS C; Garcia V., 2018, P INT C LEARN REPR; Graves A., 2014, ARXIV14105401; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750; Gulehre C, 2014, NETWORKS SEQUENCE MO; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hendricks LA, 2016, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2016.8; Huang Y, 2019, IEEE I CONF COMP VIS, P5773, DOI 10.1109/ICCV.2019.00587; Huang Y, 2019, AAAI CONF ARTIF INTE, P8489; Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645; Huang Y, 2020, IEEE T PATTERN ANAL, V42, P636, DOI 10.1109/TPAMI.2018.2883466; Huang YF, 2017, IEEE INT CONF COMP V, P2313, DOI 10.1109/ICCVW.2017.273; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kim JH, 2018, ADV NEUR IN, V31; Kiros R., 2015, T ASS COMPUT LINGUIS; Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Kumar V, 2016, INT CONF ADVAN COMPU; Lee CW, 2018, PROC CVPR IEEE, P1576, DOI 10.1109/CVPR.2018.00170; Lee J, 2019, PR MACH LEARN RES, V97; Lee K, 2018, LECT NOTES COMPUT SC, V11211, P123, DOI 10.1007/978-3-030-01234-2_8; Lev G, 2016, LECT NOTES COMPUT SC, V9910, P833, DOI 10.1007/978-3-319-46466-4_50; Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin X, 2016, LECT NOTES COMPUT SC, V9906, P261, DOI 10.1007/978-3-319-46475-6_17; Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442; Long Y, 2018, AAAI CONF ARTIF INTE, P7210; Ma C, 2018, PROC CVPR IEEE, P6975, DOI 10.1109/CVPR.2018.00729; Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301; Mikolov T., 2013, ARXIV; Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Ren M., 2018, P 6 INT C LEARN REPR; Shen YM, 2018, PROC CVPR IEEE, P3598, DOI 10.1109/CVPR.2018.00379; Simonyan K., 2015, P INT C LEARN REPR, P1, DOI DOI 10.48550/ARXIV.1409.1556; Snell J, 2017, ADV NEUR IN, V30; Socher R., 2013, 13 INT C NEUR INF PR, P935; Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208; Su Z, 2018, PROC CVPR IEEE, P7736, DOI 10.1109/CVPR.2018.00807; Sukhbaatar S., 2015, ADV NEURAL INFORM PR, P175, DOI 10.1145/3130348.3130364; Teney D, 2018, LECT NOTES COMPUT SC, V11219, P229, DOI 10.1007/978-3-030-01267-0_14; Vendrov I., 2016, PROC INT C LEARN REP; Venugopalan S, 2017, PROC CVPR IEEE, P1170, DOI 10.1109/CVPR.2017.130; Vinyals O., 2016, P 30 INT C NEUR INF, P3637, DOI 10.5555/3157382.3157504; Wang JB, 2018, PROC CVPR IEEE, P7512, DOI 10.1109/CVPR.2018.00784; Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921; Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541; Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25; Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717; Wehrmann J, 2018, PROC CVPR IEEE, P7718, DOI 10.1109/CVPR.2018.00805; Wehrmann J, 2019, IEEE I CONF COMP VIS, P5803, DOI 10.1109/ICCV.2019.00590; Weinberger KQ, 2014, ADV NEURAL INFORM PR, P1889; Weston Jason, 2015, P INT C LEARN REPR; Wu H, 2019, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2019.00677; Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15; Xiong CM, 2016, PR MACH LEARN RES, V48; Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966; You QZ, 2018, PROC CVPR IEEE, P5735, DOI 10.1109/CVPR.2018.00601; Young Peter, 2014, T ASSOC COMPUT LING, V2, P67; Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184	70	1	1	9	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2022	44	6					2968	2983		10.1109/TPAMI.2021.3052490	http://dx.doi.org/10.1109/TPAMI.2021.3052490			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1R1DD	33460367				2022-12-18	WOS:000803117500015
J	Sengupta, S; Lichy, D; Kanazawa, A; Castillo, CD; Jacobs, DW				Sengupta, Soumyadip; Lichy, Daniel; Kanazawa, Angjoo; Castillo, Carlos D.; Jacobs, David W.			SfSNet: Learning Shape, Reflectance and Illuminance of Faces in the Wild	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Faces; Lighting; Image reconstruction; Shape; Rendering (computer graphics); Three-dimensional displays; Training; Vision and scene understanding; physically based modeling; shading; 3D; stereo scene analysis; computer vision; reflectance; shading; shape		We present SfSNet, an end-to-end learning framework for producing an accurate decomposition of an unconstrained human face image into shape, reflectance and illuminance. SfSNet is designed to reflect a physical lambertian rendering model. SfSNet learns from a mixture of labeled synthetic and unlabeled real-world images. This allows the network to capture low-frequency variations from synthetic and high-frequency details from real images through the photometric reconstruction loss. SfSNet consists of a new decomposition architecture with residual blocks that learns a complete separation of albedo and normal. This is used along with the original image to predict lighting. SfSNet produces significantly better quantitative and qualitative results than state-of-the-art methods for inverse rendering and independent normal and illumination estimation. We also introduce a companion network, SfSMesh, that utilizes normals estimated by SfSNet to reconstruct a 3D face mesh. We demonstrate that SfSMesh produces face meshes with greater accuracy than state-of-the-art methods on real-world images.	[Sengupta, Soumyadip; Lichy, Daniel; Castillo, Carlos D.; Jacobs, David W.] Univ Maryland, College Pk, MD 20742 USA; [Kanazawa, Angjoo] Univ Calif Berkeley, Berkeley, CA 94720 USA	University System of Maryland; University of Maryland College Park; University of California System; University of California Berkeley	Lichy, D (corresponding author), Univ Maryland, College Pk, MD 20742 USA.	sengupta@umiacs.umd.edu; dlichy@umd.edu; kanazawa@eecs.berkeley.edu; carlos.d.castillo@gmail.com; djacobs@umiacs.umd.edu			National Science Foundation [IIS-1526234]	National Science Foundation(National Science Foundation (NSF))	The authors would like to thank Hao Zhou and Rajeev Ranjan for helpful discussions, Ayush Tewari for providing visual results of MoFA, and Zhixin Shu for providing test images of Neural Face. This work was supported by the National Science Foundation under Grant IIS-1526234.	Alldieck T, 2019, IEEE I CONF COMP VIS, P2293, DOI 10.1109/ICCV.2019.00238; Bansal A, 2016, PROC CVPR IEEE, P5965, DOI 10.1109/CVPR.2016.642; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Chai ML, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818112; Chang Angel X., 2015, ARXIV151203012CSGR P; Chang FJ, 2018, IEEE INT CONF AUTOMA, P122, DOI 10.1109/FG.2018.00027; Chen AP, 2019, IEEE I CONF COMP VIS, P9428, DOI 10.1109/ICCV.2019.00952; Crispell D, 2017, PIX2FACE DIRECT 3DFA; Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33; Feng ZH, 2018, IEEE INT CONF AUTOMA, P780, DOI 10.1109/FG.2018.00123; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Haefner B, 2019, IEEE I CONF COMP VIS, P8538, DOI 10.1109/ICCV.2019.00863; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117; Janner M, 2017, ADV NEUR IN, V30; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kemelmacher-Shlizerman I, 2011, IEEE I CONF COMP VIS, P1746, DOI 10.1109/ICCV.2011.6126439; Kemelmacher-Shlizerman I, 2011, IEEE T PATTERN ANAL, V33, P394, DOI 10.1109/TPAMI.2010.63; Kokkinos I, 2017, PROC CVPR IEEE, P5454, DOI 10.1109/CVPR.2017.579; Kulkarni TD, 2015, ADV NEUR IN, V28; Laine S, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099581; Li ZQ, 2020, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR42600.2020.00255; Liu Ming-Yu, 2017, NIPS; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767; Narihira T, 2015, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2015.342; Or-El R, 2015, PROC CVPR IEEE, P5407, DOI 10.1109/CVPR.2015.7299179; Oxholm G, 2012, LECT NOTES COMPUT SC, V7572, P528, DOI 10.1007/978-3-642-33718-5_38; Paszke A, 2019, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.1912.01703; Prados E, 2006, HANDBOOK OF MATHEMATICAL MODELS IN COMPUTER VISION, P375, DOI 10.1007/0-387-28831-7_23; Ranjan R, 2017, IEEE INT CONF AUTOMA, P17, DOI 10.1109/FG.2017.137; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Roth J, 2016, PROC CVPR IEEE, P4197, DOI 10.1109/CVPR.2016.455; Saito S, 2017, PROC CVPR IEEE, P2326, DOI 10.1109/CVPR.2017.250; Saito S, 2016, LECT NOTES COMPUT SC, V9912, P244, DOI 10.1007/978-3-319-46484-8_15; Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6; Sela M, 2017, IEEE I CONF COMP VIS, P1585, DOI 10.1109/ICCV.2017.175; Sengupta S, 2018, PROC CVPR IEEE, P6296, DOI 10.1109/CVPR.2018.00659; Shi J, 2017, PROC CVPR IEEE, P5844, DOI 10.1109/CVPR.2017.619; Shu ZX, 2018, LECT NOTES COMPUT SC, V11214, P664, DOI 10.1007/978-3-030-01249-6_40; Shu ZX, 2017, PROC CVPR IEEE, P5444, DOI 10.1109/CVPR.2017.578; Tang Y., 2012, ARXIV12064635, P1419, DOI DOI 10.48550/ARXIV.1206.6445; Tappen M. F., 2003, ADV NEURAL INFORM PR, P1367; Tewari A, 2018, PROC CVPR IEEE, P2549, DOI 10.1109/CVPR.2018.00270; Tewari A, 2017, IEEE I CONF COMP VIS, P3735, DOI 10.1109/ICCV.2017.401; Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262; Tran A. T., 2017, EXTREME 3D FACE RECO; Tran AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163; Trigeorgis G, 2017, PROC CVPR IEEE, P340, DOI 10.1109/CVPR.2017.44; Wang Y, 2009, IEEE T PATTERN ANAL, V31, P1968, DOI 10.1109/TPAMI.2008.244; Yu Y., 2020, PROC IEEECVF C COMPU, P2472; Zafeiriou S., 2011, 2011 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops 2011), P132, DOI 10.1109/CVPRW.2011.5981840; Zhengqin Li, 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3272127.3275055; Zhou H, 2018, PROC CVPR IEEE, P6238, DOI 10.1109/CVPR.2018.00653; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu XY, 2019, IEEE T PATTERN ANAL, V41, P78, DOI 10.1109/TPAMI.2017.2778152	60	1	1	5	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2022	44	6					3272	3284		10.1109/TPAMI.2020.3046915	http://dx.doi.org/10.1109/TPAMI.2020.3046915			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1R1DD	33360981	Green Submitted			2022-12-18	WOS:000803117500035
J	Zhang, JH; Sun, DW; Luo, ZX; Yao, AB; Chen, HK; Zhou, L; Shen, TW; Chen, YR; Quan, L; Liao, HE				Zhang, Jiahui; Sun, Dawei; Luo, Zixin; Yao, Anbang; Chen, Hongkai; Zhou, Lei; Shen, Tianwei; Chen, Yurong; Quan, Long; Liao, Hongen			OANet: Learning Two-View Correspondences and Geometry Using Order-Aware Network	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Geometry; Computer architecture; Visualization; Filtering; Feature extraction; Three-dimensional displays; Sun; Sparse matching; graph neural network; two-view geometry; structure-from-motion; visual localization	SCALE; CONSENSUS; MODEL	Establishing correct correspondences between two images should consider both local and global spatial context. Given putative correspondences of feature points in two views, in this paper, we propose Order-Aware Network, which infers the probabilities of correspondences being inliers and regresses the relative pose encoded by the essential or fundamental matrix. Specifically, this proposed network is built hierarchically and comprises three operations. First, to capture the local context of sparse correspondences, the network clusters unordered input correspondences by learning a soft assignment matrix. These clusters are in canonical order and invariant to input permutations. Next, the clusters are spatially correlated to encode the global context of correspondences. After that, the context-encoded clusters are interpolated back to the original size and position to build a hierarchical architecture. We intensively experiment on both outdoor and indoor datasets. The accuracy of the two-view geometry and correspondences are significantly improved over the state-of-the-arts. Besides, based on the proposed method and advanced local feature, we won the first place in CVPR 2019 image matching workshop challenge and also achieve state-of-the-art results in the Visual Localization benchmark. Code is available at https://github.com/zjhthu/OANet.	[Zhang, Jiahui; Liao, Hongen] Tsinghua Univ, Dept Biomed Engn, Beijing 100084, Peoples R China; [Sun, Dawei; Yao, Anbang; Chen, Yurong] Intel Labs China, Beijing, Peoples R China; [Luo, Zixin; Chen, Hongkai; Zhou, Lei; Shen, Tianwei; Quan, Long] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China; [Luo, Zixin; Zhou, Lei] Shenzhen Zhuke Innovat Technol Altizure, Shenzhen, Hong Kong, Peoples R China	Tsinghua University; Intel Corporation; Hong Kong University of Science & Technology	Liao, HE (corresponding author), Tsinghua Univ, Dept Biomed Engn, Beijing 100084, Peoples R China.	jiahui-z15@mails.tsinghua.edu.cn; dawei.sun@intel.com; zluoag@cse.ust.hk; anbang.yao@intel.com; hchencf@cse.ust.hk; lzhouai@cse.ust.hk; tianwei@cse.ust.hk; yurong.chen@intel.com; quan@cse.ust.hk; liao@tsinghua.edu.cn	cao, xiaoxiang/AAR-9291-2021; Zhou, Lei/H-4799-2016	Chen, Yurong/0000-0001-9333-1746; Chen, Hongkai/0000-0003-0308-4732; Zhou, Lei/0000-0003-4988-5084	Natural Science Foundation of China [82027807, 81771940, 81427803]; National Key Research and Development Program of China [2017YFC0108000]; National Beijing National Science Foundation [L172003]; Soochow-Tsinghua Innovation Project [2016SZ0206]; Hong Kong RGC GRF [16206819, 16203518]	Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key Research and Development Program of China; National Beijing National Science Foundation; Soochow-Tsinghua Innovation Project; Hong Kong RGC GRF(Hong Kong Research Grants Council)	This work was supported by the Natural Science Foundation of China (82027807, 81771940, and 81427803), National Key Research and Development Program of China (2017YFC0108000), National Beijing National Science Foundation (L172003), and Soochow-Tsinghua Innovation Project (2016SZ0206). This work was also supported by Hong Kong RGC GRF 16206819 and 16203518. Part of the work was done when Jiahui Zhangwas an intern at ILC supervised by Anbang Yao and visiting HKUST. The authors would also like to thank Vladlen Koltun, Rene Ranftl and David Hafner for helping us reimplement their work. They also thank Daniel Barath for the help in GC-RANSAC and MAGSAC experiments. Jiahui Zhang and Dawei Sun contributed equally to thiswork.	Albarelli A, 2009, IEEE I CONF COMP VIS, P1319, DOI 10.1109/ICCV.2009.5459312; Avrithis Y, 2014, INT J COMPUT VISION, V107, P1, DOI 10.1007/s11263-013-0659-3; Barath D, 2019, PROC CVPR IEEE, P10189, DOI 10.1109/CVPR.2019.01044; Barath D, 2018, PROC CVPR IEEE, P6733, DOI 10.1109/CVPR.2018.00704; Bellavia F, 2018, IEEE T PATTERN ANAL, V40, P931, DOI 10.1109/TPAMI.2017.2697849; Bian J.-W., 2019, PROC BRIT MACH VIS C, V2, P25; Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Brachmann E, 2017, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2017.267; Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3; Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221; Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236; Chum O, 2008, IEEE T PATTERN ANAL, V30, P1472, DOI 10.1109/TPAMI.2007.70787; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; DeTone D., 2018, ARXIV 181203245; DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060; Dusmanu M., 2020, LECT NOTES COMPUT SC, V12346, DOI [10.1007/978-3-030-58452-8_39, DOI 10.1007/978-3-030-58452-8_39]; Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828; Ebel P, 2019, IEEE I CONF COMP VIS, P253, DOI 10.1109/ICCV.2019.00034; Fey M, 2018, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2018.00097; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961; Hamilton WL, 2017, ADV NEUR IN, V30; Hartley R., 2003, MULTIPLE VIEW GEOMET; HARTLEY RI, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P1064; Heinly J, 2015, PROC CVPR IEEE, P3287, DOI 10.1109/CVPR.2015.7298949; Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24; Kipf T. N., 2017, INT C LEARN REPR, DOI [DOI 10.1109/ICDM.2008.17, DOI 10.1109/ICDM.2019.00070]; Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99; Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599; Lebeda K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.95; Li RH, 2018, IEEE INT CONF ROBOT, P7286; Li XC, 2015, PROC CVPR IEEE, P5153, DOI 10.1109/CVPR.2015.7299151; Lin WY, 2018, IEEE T PATTERN ANAL, V40, P34, DOI 10.1109/TPAMI.2017.2652468; Lin WYD, 2014, LECT NOTES COMPUT SC, V8692, P341, DOI 10.1007/978-3-319-10593-2_23; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luo ZX, 2018, LECT NOTES COMPUT SC, V11213, P170, DOI 10.1007/978-3-030-01240-3_11; Luo ZX, 2020, PROC CVPR IEEE, P6588, DOI 10.1109/CVPR42600.2020.00662; Luo ZX, 2019, PROC CVPR IEEE, P2522, DOI 10.1109/CVPR.2019.00263; Ma JY, 2019, INT J COMPUT VISION, V127, P512, DOI 10.1007/s11263-018-1117-z; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Mishchuk A, 2017, ADV NEUR IN, V30; Mishkin D, 2018, LECT NOTES COMPUT SC, V11213, P287, DOI 10.1007/978-3-030-01240-3_18; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671; Myatt D., 2002, P BRIT MACH VIS C, DOI [10.5244/C.16.44, DOI 10.5244/C.16.44]; Ni K, 2009, IEEE I CONF COMP VIS, P2193, DOI 10.1109/ICCV.2009.5459241; Niepert M, 2016, PR MACH LEARN RES, V48; Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374; Ono Yuki, 2018, ADV NEURAL INFORM PR, P6237; Pl_otz T., 2018, PROC 32 INT C NEURAL, P1095; Qi CR, 2017, ADV NEUR IN, V30; Quan, 2018, P AS C COMP VIS, P415; Raguram R, 2013, IEEE T PATTERN ANAL, V35, P2022, DOI 10.1109/TPAMI.2012.257; Ranftl R, 2018, LECT NOTES COMPUT SC, V11205, P292, DOI 10.1007/978-3-030-01246-5_18; Revaud J, 2019, ADV NEUR IN, V32; Rocco I, 2018, ADV NEUR IN, V31; Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Sattler T, 2018, PROC CVPR IEEE, P8601, DOI 10.1109/CVPR.2018.00897; Sattler T, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.76; Sattler T, 2009, IEEE I CONF COMP VIS, P2090, DOI 10.1109/ICCV.2009.5459459; Savinov N, 2017, PROC CVPR IEEE, P3929, DOI 10.1109/CVPR.2017.418; Schonberger JL, 2017, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2017.736; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773; Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802; Tian YR, 2017, PROC CVPR IEEE, P6128, DOI 10.1109/CVPR.2017.649; Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437; Ummenhofer B, 2017, PROC CVPR IEEE, P5622, DOI 10.1109/CVPR.2017.596; Wilson K, 2014, LECT NOTES COMPUT SC, V8691, P61, DOI 10.1007/978-3-319-10578-9_5; Wu Changchang, 2011, VISUALSFM VISUAL STR, P1; Wu X., 2015, PROC BRIT MACH VIS C; Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458; Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6; Yi KM, 2018, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2018.00282; Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28; Ying R, 2018, ADV NEUR IN, V31; Zhang JH, 2018, LECT NOTES COMPUT SC, V11216, P749, DOI 10.1007/978-3-030-01258-8_45; Zhang JH, 2019, IEEE I CONF COMP VIS, P5844, DOI 10.1109/ICCV.2019.00594; Zhang MH, 2018, AAAI CONF ARTIF INTE, P4438; Zhao C, 2019, PROC CVPR IEEE, P215, DOI 10.1109/CVPR.2019.00030; Zhou L, 2018, LECT NOTES COMPUT SC, V11219, P527, DOI 10.1007/978-3-030-01267-0_31; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700	88	1	1	10	31	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2022	44	6					3110	3122		10.1109/TPAMI.2020.3048013	http://dx.doi.org/10.1109/TPAMI.2020.3048013			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1R1DD	33373296	Green Submitted			2022-12-18	WOS:000803117500024
J	Zhou, K; Han, XG; Jiang, NJ; Jia, K; Lu, JB				Zhou, Kun; Han, Xiaoguang; Jiang, Nianjuan; Jia, Kui; Lu, Jiangbo			HEMlets PoSh: Learning Part-Centric Heatmap Triplets for 3D Human Pose and Shape Estimation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Two dimensional displays; Heating systems; Pose estimation; Task analysis; Training; Shape; 3D human pose estimation; deep learning; heatmaps; human body mesh recovery		Estimating 3D human pose from a single image is a challenging task. This work attempts to address the uncertainty of lifting the detected 2D joints to the 3D space by introducing an intermediate state - Part-Centric Heatmap Triplets (HEMlets), which shortens the gap between the 2D observation and the 3D interpretation. The HEMlets utilize three joint-heatmaps to represent the relative depth information of the end-joints for each skeletal body part. In our approach, a Convolutional Network (ConvNet) is first trained to predict HEMlets from the input image, followed by a volumetric joint-heatmap regression. We leverage on the integral operation to extract the joint locations from the volumetric heatmaps, guaranteeing end-to-end learning. Despite the simplicity of the network design, the quantitative comparisons show a significant performance improvement over the best-of-grade methods (e.g., 20 percent on Human3.6M). The proposed method naturally supports training with "in-the-wild" images, where only weakly-annotated relative depth information of skeletal joints is available. This further improves the generalization ability of our model, as validated by qualitative comparisons on outdoor images. Leveraging the strength of the HEMlets pose estimation, we further design and append a shallow yet effective network module to regress the SMPL parameters of the body pose and shape. We term the entire HEMlets-based human pose and shape recovery pipeline HEMlets PoSh. Extensive quantitative and qualitative experiments on the existing human body recovery benchmarks justify the state-of-the-art results obtained with our HEMlets PoSh approach.	[Zhou, Kun; Jiang, Nianjuan] SmartMore Corp Ltd, Shenzhen, Guangdong, Peoples R China; [Han, Xiaoguang] Chinese Univ Hong Kong, Shenzhen Inst Big Data, Shenzhen, Peoples R China; [Jia, Kui] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Peoples R China; [Jia, Kui] Pazhou Lab, Guangzhou 510335, Peoples R China; [Jia, Kui] Peng Cheng Lab, Shenzhen 518005, Peoples R China; [Lu, Jiangbo] SmartMore Corp Ltd, Shenzhen, Peoples R China; [Lu, Jiangbo] South China Univ Technol, Guangzhou 510641, Peoples R China	Chinese University of Hong Kong, Shenzhen; South China University of Technology; Pazhou Lab; Peng Cheng Laboratory; South China University of Technology	Jia, K (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Peoples R China.; Jia, K (corresponding author), Pazhou Lab, Guangzhou 510335, Peoples R China.; Jia, K (corresponding author), Peng Cheng Lab, Shenzhen 518005, Peoples R China.; Lu, JB (corresponding author), SmartMore Corp Ltd, Shenzhen, Peoples R China.; Lu, JB (corresponding author), South China Univ Technol, Guangzhou 510641, Peoples R China.	zhoukun303808@gmail.com; hanxiaoguang@cuhk.edu.cn; jnianjuan@gmail.com; kuijia@scut.edu.cn; jiangbo.lu@gmail.com	Kun, Zhou/GMX-1497-2022	zhou, kun/0000-0003-1726-4697; Han, Xiaoguang/0000-0003-0162-3296; Lu, Jiangbo/0000-0002-0048-3140	National Natural Science Foundation of China [61771201]; Program for Guangdong Introducing Innovative and Enterpreneurial Teams [2017ZT07X183]; Pearl River Talent Recruitment Program Innovative and Entrepreneurial Teams in 2017 [2017ZT07X152]; Shenzhen Fundamental Research Fund [KQTD2015033114415450, ZDSYS 201707251409055]; Department of Science and Technology of Guangdong Province Fund [2018B030338001]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Program for Guangdong Introducing Innovative and Enterpreneurial Teams; Pearl River Talent Recruitment Program Innovative and Entrepreneurial Teams in 2017; Shenzhen Fundamental Research Fund; Department of Science and Technology of Guangdong Province Fund	This work was supported in part by the National Natural Science Foundation of China (Grant 61771201), the Program for Guangdong Introducing Innovative and Enterpreneurial Teams (Grant 2017ZT07X183), the Pearl River Talent Recruitment Program Innovative and Entrepreneurial Teams in 2017 (Grant 2017ZT07X152), the Shenzhen Fundamental Research Fund (Grants KQTD2015033114415450 and ZDSYS 201707251409055), and Department of Science and Technology of Guangdong Province Fund (2018B030338001). The authors would like to thank Yulong Shi and Kaiqi Wang for assisting in some early experiments. This work was mainly done when Kun Zhou, Nianjuan Jiang, and Jiangbo Lu were working in Shenzhen Cloudream Technology Company, Ltd.	Martinez AA, 2017, INT SYMP COMPUT EDUC; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Bo LF, 2010, INT J COMPUT VISION, V87, P28, DOI 10.1007/s11263-008-0204-y; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Chen CH, 2017, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR.2017.610; Chen WZ, 2016, INT CONF 3D VISION, P479, DOI 10.1109/3DV.2016.58; Chen XP, 2019, PROC CVPR IEEE, P10887, DOI 10.1109/CVPR.2019.01115; Dabral R, 2018, LECT NOTES COMPUT SC, V11213, P679, DOI 10.1007/978-3-030-01240-3_41; Fang HS, 2018, AAAI CONF ARTIF INTE, P6821; Ghiasi G, 2014, PROC CVPR IEEE, P1899, DOI 10.1109/CVPR.2014.306; Guler RA, 2019, PROC CVPR IEEE, P10876, DOI 10.1109/CVPR.2019.01114; Guler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Johnson S., 2010, BMVC, DOI [10.5244/C.24.12, DOI 10.5244/C.24.12.CITESEER]; Ke LP, 2018, LECT NOTES COMPUT SC, V11206, P731, DOI 10.1007/978-3-030-01216-8_44; Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234; Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463; Lassner C, 2017, PROC CVPR IEEE, P4704, DOI 10.1109/CVPR.2017.500; Li SJ, 2015, LECT NOTES COMPUT SC, V9004, P332, DOI 10.1007/978-3-319-16808-1_23; Linder T., 2018, IROS WORKSH ROB CO W; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Madadi Meysam, 2018, ARXIV181210766; Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064; Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Nie BX, 2017, IEEE I CONF COMP VIS, P3467, DOI 10.1109/ICCV.2017.373; Park S, 2016, LECT NOTES COMPUT SC, V9915, P156, DOI 10.1007/978-3-319-49409-8_15; Pavlakos G, 2019, IEEE I CONF COMP VIS, P803, DOI 10.1109/ICCV.2019.00089; Pavlakos G, 2018, PROC CVPR IEEE, P7307, DOI 10.1109/CVPR.2018.00763; Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055; Pavlakos G, 2017, PROC CVPR IEEE, P1263, DOI 10.1109/CVPR.2017.139; Pons-Moll G, 2014, PROC CVPR IEEE, P2345, DOI 10.1109/CVPR.2014.300; Rogez G, 2016, ADV NEUR IN, V29; Ronchi Matteo Ruggero, 2018, BMVC; Sharma S, 2019, IEEE I CONF COMP VIS, P2325, DOI 10.1109/ICCV.2019.00241; Shi Y., 2018, ARXIV180609241; Simo-Serra E, 2013, PROC CVPR IEEE, P3634, DOI 10.1109/CVPR.2013.466; Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33; Tekin B, 2017, IEEE I CONF COMP VIS, P3961, DOI 10.1109/ICCV.2017.425; Tekin Bugra, 2016, BRIT MACH VIS C 2016, DOI DOI 10.5244/C.30.130; Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603; Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37; Wandt B, 2019, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2019.00797; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Xu YL, 2019, IEEE I CONF COMP VIS, P7759, DOI 10.1109/ICCV.2019.00785; Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551; Yao P, 2019, CORR; Yasin H, 2016, PROC CVPR IEEE, P4948, DOI 10.1109/CVPR.2016.535; Zhou K, 2019, IEEE I CONF COMP VIS, P2344, DOI 10.1109/ICCV.2019.00243; Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51	56	1	1	5	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2022	44	6					3000	3014		10.1109/TPAMI.2021.3051173	http://dx.doi.org/10.1109/TPAMI.2021.3051173			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1R1DD	33434125	Green Submitted			2022-12-18	WOS:000803117500017
J	Guo, TY; Xu, C; Shi, BX; Xu, C; Tao, DC				Guo, Tianyu; Xu, Chang; Shi, Boxin; Xu, Chao; Tao, Dacheng			Optimizing Latent Distributions for Non-Adversarial Generative Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Training; Generators; Gallium nitride; Optimization; Image reconstruction; Linear programming; Generative adversarial networks; Non-adversarial generation; image generation; distribution optimization		The generator in generative adversarial networks (GANs) is driven by a discriminator to produce high-quality images through an adversarial game. At the same time, the difficulty of reaching a stable generator has been increased. This paper focuses on non-adversarial generative networks that are trained in a plain manner without adversarial loss. The given limited number of real images could be insufficient to fully represent the real data distribution. We therefore investigate a set of distributions in a Wasserstein ball centred on the distribution induced by the training data and propose to optimize the generator over this Wasserstein ball. We theoretically discuss the solvability of the newly defined objective function and develop a tractable reformulation to learn the generator. The connections and differences between the proposed non-adversarial generative networks and GANs are analyzed. Experimental results on real-world datasets demonstrate that the proposed algorithm can effectively learn image generators in a non-adversarial approach, and the generated images are of comparable quality with those from GANs.	[Guo, Tianyu; Xu, Chang] Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing 100871, Peoples R China; [Guo, Tianyu; Xu, Chang] Peking Univ, Dept Machine Intelligence, Cooperat Medianet Innovat Ctr, Beijing 100871, Peoples R China; [Shi, Boxin] Peking Univ, Natl Engn Lab Video Technol, Dept Comp Sci & Technol, Beijing 100871, Peoples R China; [Shi, Boxin] Peking Univ, Inst Artificial Intelligence, Beijing 100871, Peoples R China; [Xu, Chao; Tao, Dacheng] Univ Sydney, UBTECH Sydney Artificial Intelligence Ctr, Darlington, NSW 2008, Australia; [Xu, Chao; Tao, Dacheng] Univ Sydney, Sch Comp Sci, Fac Engn, Darlington, NSW 2008, Australia	Peking University; Peking University; Peking University; Peking University; University of Sydney; University of Sydney	Xu, C (corresponding author), Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing 100871, Peoples R China.; Xu, C (corresponding author), Peking Univ, Dept Machine Intelligence, Cooperat Medianet Innovat Ctr, Beijing 100871, Peoples R China.; Shi, BX (corresponding author), Peking Univ, Natl Engn Lab Video Technol, Dept Comp Sci & Technol, Beijing 100871, Peoples R China.; Shi, BX (corresponding author), Peking Univ, Inst Artificial Intelligence, Beijing 100871, Peoples R China.	tianyuguo@pku.edu.cn; c.xu@sydney.edu.au; shiboxin@pku.edu.cn; xuchao@cis.pku.edu.cn; dacheng.tao@sydney.edu.au		Xu, Chang/0000-0002-4756-0609	Australian Research Council [DE180101438, DP210101859]; National Natural Science Foundation of China [61876007, 61872012]; National Key R&D Program of China [2019YFF0302902]; Beijing Academy of Artificial Intelligence (BAAI)	Australian Research Council(Australian Research Council); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key R&D Program of China; Beijing Academy of Artificial Intelligence (BAAI)	This work was supported in part by Australian Research Council Projects DE180101438 and DP210101859, National Natural Science Foundation of China under Grants 61876007 and 61872012, National Key R&D Program of China (2019YFF0302902), and Beijing Academy of Artificial Intelligence (BAAI).	[Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Berthelot D., 2017, ABS170310717; Bojanowski P, 2018, PR MACH LEARN RES, V80; Brock A., 2018, P INT C LEARN REPR 2; Dai BZ, 2018, 2018 11TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2018); Feng ZL, 2018, ADV NEUR IN, V31; Ghosal P. etal, 2019, PROC 2 INT C ADV COM, P1, DOI DOI 10.1109/ICACCP.2019.8882973; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I., 2017, INT C NEURAL INF PRO; Haibin L., 2006, P IEEE C COMP VIS PA, P246, DOI DOI 10.1109/CVPR.2006.99; Heusel M., 2017, 31 C NEUR INF PROC S, P6626; Hoshen Y, 2019, PROC CVPR IEEE, P5804, DOI 10.1109/CVPR.2019.00596; Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Karras T, 2017, ARXIV171010196; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Liu M. -Y., 2016, ADV NEURAL INFORM PR, P469; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lucic M, 2018, ADV NEUR IN, V31; Mescheder L., 2018, ARXIV 180104406; Miyato T., 2018, P INT C LEARN REPR 2; Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Sajjadi Mehdi SM, 2018, ADV NEURAL INFORM PR, P5234; Salimans T., 2016, ADV NEUR IN, P2234; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tolstikhin I., 2018, P INT C LEARN REPR 2; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; Wasserstein G.A.N., 2017, ARXIV170107875; Xiao H., 2017, FASHION MNIST NOVEL; Xiao Z., 2019, ARXIV 190510485; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7; Zhu Jun-Yan, 2017, ICCV	41	1	1	3	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2022	44	5					2657	2672		10.1109/TPAMI.2020.3043745	http://dx.doi.org/10.1109/TPAMI.2020.3043745			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	1C1XU	33301400				2022-12-18	WOS:000792921400032
J	Yuan, H; Cai, L; Hu, X; Wang, J; Ji, SW				Yuan, Hao; Cai, Lei; Hu, Xia; Wang, Jie; Ji, Shuiwang			Interpreting Image Classifiers by Generating Discrete Masks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Generators; Predictive models; Electronic mail; Training; Computational modeling; Neurons; Computer science; Convolutional neural networks; image classification; interpretability; discrete masks; reinforcement learning		Deep models are commonly treated as black-boxes and lack interpretability. Here, we propose a novel approach to interpret deep image classifiers by generating discrete masks. Our method follows the generative adversarial network formalism. The deep model to be interpreted is the discriminator while we train a generator to explain it. The generator is trained to capture discriminative image regions that should convey the same or similar meaning as the original image from the model's perspective. It produces a probability map from which a discrete mask can be sampled. Then the discriminator is used to measure the quality of the sampled mask and provide feedbacks for updating. Due to the sampling operations, the generator cannot be trained directly by back-propagation. We propose to update it using policy gradient. Furthermore, we propose to incorporate gradients as auxiliary information to reduce the search space and facilitate training. We conduct both quantitative and qualitative experiments on the ILSVRC dataset. Experimental results indicate that our method can provide reasonable explanations for predictions and outperform existing approaches. In addition, our method can pass the model randomization test, indicating that it is reasoning the attribution of network predictions.	[Yuan, Hao; Hu, Xia; Ji, Shuiwang] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA; [Cai, Lei] Washington State Univ, Sch Elect Engn & Comp Sci, Pullman, WA 99164 USA; [Wang, Jie] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230026, Peoples R China	Texas A&M University System; Texas A&M University College Station; Washington State University; Chinese Academy of Sciences; University of Science & Technology of China, CAS	Ji, SW (corresponding author), Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA.	hao.yuan@tamu.edu; lei.cai@wsu.edu; hu@cse.tamu.edu; jiewangx@ustc.edu.cn; sji@tamu.edu	hu, xia hong/GQP-8544-2022		National Science Foundation [DBI-1661289, IIS-1908198]; Defense Advanced Research Projects Agency [N66001-17-2-4031]	National Science Foundation(National Science Foundation (NSF)); Defense Advanced Research Projects Agency(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work was supported in part by National Science Foundation Grants DBI-1661289 and IIS-1908198, and Defense Advanced Research Projects Agency grant N66001-17-2-4031.	Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/2951913.2976746, 10.1145/3022670.2976746]; Adebayo Julius, 2018, ADV NEURAL INFORM PR, V1, P7; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Baehrens D, 2010, J MACH LEARN RES, V11, P1803; Chen JB, 2018, PR MACH LEARN RES, V80; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Dabkowski P, 2017, ADV NEUR IN, V30; Doersch Carl, 2016, ARXIV160605908V2; Du MN, 2020, COMMUN ACM, V63, P68, DOI 10.1145/3359786; Du MN, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1358, DOI 10.1145/3219819.3220099; Erhan D, 2009, 1341 U MONTR, V1341, P1, DOI DOI 10.2464/JILM.23.425; Fakhry A, 2017, IEEE T MED IMAGING, V36, P447, DOI 10.1109/TMI.2016.2613019; Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371; Gao HY, 2020, IEEE T PATTERN ANAL, V42, P1218, DOI 10.1109/TPAMI.2019.2893965; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hooker S, 2019, ADV NEUR IN, V32; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jang Eric, 2017, P 5 INT C LEARN REPR; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kim Y., 2014, P 2014 C EMP METH NA; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lundberg SM, 2017, ADV NEUR IN, V30; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; Mirza M., 2014, ARXIV PREPRINT ARXIV; Paik S, 2004, NEW ENGL J MED, V351, P2817, DOI 10.1056/NEJMoa041588; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74; Shrikumar A, 2017, PR MACH LEARN RES, V70; Silberman N., 2017, TENSORFLOWSLIM IMAGE; Smilkov D, 2017, ARXIV; Springenberg J. T, 2015, ARXIV PREPRINT ARXIV; Sundararajan M, 2017, PR MACH LEARN RES, V70; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Vaswani A, 2017, ADV NEUR IN, V30; Velickovic P., 2018, P INT C LEARN REPR; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yu LT, 2017, AAAI CONF ARTIF INTE, P2852; Yuan H., 2020, PROC 8 INT C LEARN R; Yuan H, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P430, DOI 10.1145/3394486.3403085; Yuan H, 2019, AAAI CONF ARTIF INTE, P5717; Yuan H, 2019, BIOINFORMATICS, V35, P2141, DOI 10.1093/bioinformatics/bty923; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang JM, 2018, INT J COMPUT VISION, V126, P1084, DOI 10.1007/s11263-017-1059-x; Zhang X., 2015, ADV NEURAL INFORM PR, P649; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319	56	1	1	2	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2022	44	4					2019	2030		10.1109/TPAMI.2020.3028783	http://dx.doi.org/10.1109/TPAMI.2020.3028783			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	ZN1PQ	33021938	hybrid			2022-12-18	WOS:000764815300027
J	Cholakkal, H; Sun, GL; Khan, S; Khan, FS; Shao, L; Van Gool, L				Cholakkal, Hisham; Sun, Guolei; Khan, Salman; Khan, Fahad Shahbaz; Shao, Ling; Van Gool, Luc			Towards Partial Supervision for Generic Object Counting in Natural Scenes	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Visualization; Genomics; Bioinformatics; Image segmentation; Modulation; Sun; Graphical models; Generic object counting; reduced supervision; object localization; weakly supervised instance segmentation		Generic object counting in natural scenes is a challenging computer vision problem. Existing approaches either rely on instance-level supervision or absolute count information to train a generic object counter. We introduce a partially supervised setting that significantly reduces the supervision level required for generic object counting. We propose two novel frameworks, named lower-count (LC) and reduced lower-count (RLC), to enable object counting under this setting. Our frameworks are built on a novel dual-branch architecture that has an image classification and a density branch. Our LC framework reduces the annotation cost due to multiple instances in an image by using only lower-count supervision for all object categories. Our RLC framework further reduces the annotation cost arising from large numbers of object categories in a dataset by only using lower-count supervision for a subset of categories and class-labels for the remaining ones. The RLC framework extends our dual-branch LC framework with a novel weight modulation layer and a category-independent density map prediction. Experiments are performed on COCO, Visual Genome and PASCAL 2007 datasets. Our frameworks perform on par with state-of-the-art approaches using higher levels of supervision. Additionally, we demonstrate the applicability of our LC supervised density map for image-level supervised instance segmentation.	[Cholakkal, Hisham; Khan, Salman; Khan, Fahad Shahbaz; Shao, Ling] Mohamed Bin Zayed Univ Artificial Intelligence, Abu Dhabi, U Arab Emirates; [Sun, Guolei; Van Gool, Luc] Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland; [Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates	Mohamed Bin Zayed University of Artificial Intelligence; Swiss Federal Institutes of Technology Domain; ETH Zurich	Sun, GL (corresponding author), Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland.	hisham.cholakkal@mbzuai.ac.ae; guolei.sun@vision.ee.ethz.ch; salman.khan@mbzuai.ac.ae; fahad.khan@mbzuai.ac.ae; ling.shao@inceptioniai.org; vangool@vision.ee.ethz.ch	Khan, Salman Hameed/M-4834-2016	Khan, Salman Hameed/0000-0002-9502-1749; Van Gool, Luc/0000-0002-3445-5711				Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231; Bansal A, 2018, LECT NOTES COMPUT SC, V11205, P397, DOI 10.1007/978-3-030-01246-5_24; Barinova O, 2010, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2010.5539905; Boysen S.T., 2014, DEV NUMERICAL COMPET; Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45; Chattopadhyay P, 2017, PROC CVPR IEEE, P4428, DOI 10.1109/CVPR.2017.471; Cheng ZQ, 2019, IEEE I CONF COMP VIS, P6151, DOI 10.1109/ICCV.2019.00625; Chhavi, 2018, K LARGEST OR SMALLES; Cholakkal H, 2019, PROC CVPR IEEE, P12389, DOI 10.1109/CVPR.2019.01268; Clements D., 1999, TEACHING CHILDREN MA, V5, P400; De Brabandere B, 2016, ADV NEUR IN, V29; Desai C, 2009, IEEE I CONF COMP VIS, P229, DOI 10.1109/ICCV.2009.5459256; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Gao G., 2020, ARXIV; Gao MF, 2018, LECT NOTES COMPUT SC, V11205, P155, DOI 10.1007/978-3-030-01246-5_10; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Goldman E, 2019, PROC CVPR IEEE, P5222, DOI 10.1109/CVPR.2019.00537; Guerrero-Gomez-Olmedo R, 2015, LECT NOTES COMPUT SC, V9117, P423, DOI 10.1007/978-3-319-19390-8_48; Ha David, 2017, ICLR; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He S, 2017, IEEE I CONF COMP VIS, P1059, DOI DOI 10.1109/ICCV.2017.120; Hoffman Judy, 2014, NIPS; Hu R, 2018, PROC CVPR IEEE, P4233, DOI 10.1109/CVPR.2018.00445; Islam MA, 2018, PROC CVPR IEEE, P7142, DOI 10.1109/CVPR.2018.00746; Jansen BRJ, 2014, BRIT J DEV PSYCHOL, V32, P178, DOI 10.1111/bjdp.12032; Jiang XT, 2020, PHYS REP, V848, DOI 10.1016/j.physrep.2019.12.006; Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Lapin M, 2018, IEEE T PATTERN ANAL, V40, P1533, DOI 10.1109/TPAMI.2017.2751607; Laradji I. H, 2019, CORR ABS190701430; Laradji IH, 2018, LECT NOTES COMPUT SC, V11206, P560, DOI 10.1007/978-3-030-01216-8_34; Lempitsky V., 2010, NIPS, V23, P1324; Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu CC, 2019, PROC CVPR IEEE, P1217, DOI 10.1109/CVPR.2019.00131; Liu J, 2018, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2018.00545; Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186; Liu N, 2019, PROC CVPR IEEE, P3220, DOI 10.1109/CVPR.2019.00334; Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524; Liu XL, 2018, PROC CVPR IEEE, P7661, DOI 10.1109/CVPR.2018.00799; Lu E, 2019, LECT NOTES COMPUT SC, V11363, P669, DOI 10.1007/978-3-030-20893-6_42; Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624; MANDLER G, 1982, J EXP PSYCHOL GEN, V111, P1, DOI 10.1037/0096-3445.111.1.1; Maninis KK, 2016, LECT NOTES COMPUT SC, V9905, P580, DOI 10.1007/978-3-319-46448-0_35; Miao YQ, 2020, AAAI CONF ARTIF INTE, V34, P11765; Oquab M, 2015, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2015.7298668; Paszke A, 2017, NEURAL INFORM PROCES; Pont-Tuset J, 2017, IEEE T PATTERN ANAL, V39, P128, DOI 10.1109/TPAMI.2016.2537320; Pont-Tuset J, 2015, IEEE I CONF COMP VIS, P1546, DOI 10.1109/ICCV.2015.181; Rahman S, 2019, LECT NOTES COMPUT SC, V11361, P547, DOI 10.1007/978-3-030-20887-5_34; Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683; Shi MJ, 2019, PROC CVPR IEEE, P7271, DOI 10.1109/CVPR.2019.00745; Sindagi VA, 2019, IEEE I CONF COMP VIS, P1002, DOI 10.1109/ICCV.2019.00109; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Stahl T, 2019, IEEE T IMAGE PROCESS, V28, P1035, DOI 10.1109/TIP.2018.2875353; von Borstel M, 2016, LECT NOTES COMPUT SC, V9905, P365, DOI 10.1007/978-3-319-46448-0_22; Wan F, 2018, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2018.00141; Wan J, 2019, IEEE I CONF COMP VIS, P1130, DOI 10.1109/ICCV.2019.00122; Wan J, 2019, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2019.00416; Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180; Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839; Wu Kesheng, 2005, MED IMAGING 2005 IMA, V5747; Xiong HP, 2019, IEEE I CONF COMP VIS, P8361, DOI 10.1109/ICCV.2019.00845; Xu CF, 2019, IEEE I CONF COMP VIS, P8381, DOI 10.1109/ICCV.2019.00847; Xu J, 2015, PROC CVPR IEEE, P3781, DOI 10.1109/CVPR.2015.7299002; Zhang AR, 2019, IEEE I CONF COMP VIS, P6787, DOI 10.1109/ICCV.2019.00689; Zhang JM, 2015, PROC CVPR IEEE, P4045, DOI 10.1109/CVPR.2015.7299031; Zhao F, 2018, LECT NOTES COMPUT SC, V11219, P20, DOI 10.1007/978-3-030-01267-0_2; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhou YZ, 2018, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2018.00399; Zhu Y, 2017, IEEE I CONF COMP VIS, P1859, DOI 10.1109/ICCV.2017.204	74	1	1	2	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2022	44	3					1604	1622		10.1109/TPAMI.2020.3021025	http://dx.doi.org/10.1109/TPAMI.2020.3021025			19	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YU4MA	32870786	Green Accepted, Green Submitted			2022-12-18	WOS:000752018000038
J	Deng, CR; Wu, Q; Wu, QY; Hu, FY; Lyu, F; Tan, MK				Deng, Chaorui; Wu, Qi; Wu, Qingyao; Hu, Fuyuan; Lyu, Fan; Tan, Mingkui			Visual Grounding Via Accumulated Attention	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Proposals; Visualization; Training; Feature extraction; Task analysis; Grounding; Cognition; Visual grounding; accumulated attention; noised training strategy; bounding box regression		Visual grounding (VG) aims to locate the most relevant object or region in an image, based on a natural language query. Generally, it requires the machine to first understand the query, identify the key concepts in the image, and then locate the target object by specifying its bounding box. However, in many real-world visual grounding applications, we have to face with ambiguous queries and images with complicated scene structures. Identifying the target based on highly redundant and correlated information can be very challenging, and often leading to unsatisfactory performance. To tackle this, in this paper, we exploit an attention module for each kind of information to reduce internal redundancies. We then propose an accumulated attention (A-ATT) mechanism to reason among all the attention modules jointly. In this way, the relation among different kinds of information can be explicitly captured. Moreover, to improve the performance and robustness of our VG models, we additionally introduce some noises into the training procedure to bridge the distribution gap between the human-labeled training data and the real-world poor quality data. With this "noised" training strategy, we can further learn a bounding box regressor, which can be used to refine the bounding box of the target object. We evaluate the proposed methods on four popular datasets (namely ReferCOCO, ReferCOCO+, ReferCOCOg, and GuessWhat?!). The experimental results show that our methods significantly outperform all previous works on every dataset in terms of accuracy.	[Deng, Chaorui; Wu, Qingyao; Tan, Mingkui] South China Univ Technol, Sch Software Engn, Guangzhou 510006, Peoples R China; [Deng, Chaorui] Pazhou Lab, Guangzhou 510335, Peoples R China; [Wu, Qi] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia; [Hu, Fuyuan] Suzhou Univ Sci & Technol, Sch Elect & Informat Engn, Suzhou 215009, Peoples R China; [Lyu, Fan] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China	South China University of Technology; Pazhou Lab; University of Adelaide; Suzhou University of Science & Technology; Tianjin University	Tan, MK (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou 510006, Peoples R China.	secrdyz@mail.scut.edu.cn; qi.wu01@adelaide.edu.au; qyw@scut.edu.cn; fuyuanhu@mail.usts.edu.cn; fanlyu@tju.edu.cn; mingkuitan@scut.edu.cn	Lyu, Fan/AAC-3865-2020	Lyu, Fan/0000-0002-0878-5485; Wu, Qi/0000-0003-3631-256X; Deng, Chaorui/0000-0002-8587-9047	Science and Technology Program of Guangzhou, China [202007030007]; National Natural Science Foundation of China [61876121, 61836003]; Program for Guangdong Introducing Innovative and Enterpreneurial Teams [2017ZT07X183]; Fundamental Research Funds for the Central Universities [D2191240];  [DE190100539]	Science and Technology Program of Guangzhou, China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Program for Guangdong Introducing Innovative and Enterpreneurial Teams; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); 	This work was supported in part by the Science and Technology Program of Guangzhou, China, under Grant 202007030007; the National Natural Science Foundation of China, Key Project, under Grant 61836003; the National Natural Science Foundation of China, under Grant No. 61876121; the Program for Guangdong Introducing Innovative and Enterpreneurial Teams under Grant 2017ZT07X183, and the Fundamental Research Funds for the Central Universities under Grant D2191240. Qi Wu is funded by DE190100539. Chaorui Deng and Qi Wu contributed equally to this work.	Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Andreas Jacob, 2016, ARXIV160101705, P1545, DOI [DOI 10.18653/V1/N16-1181, 10.18653/v1/N16-1181]; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Chelba Ciprian, 2014, 15 ANN C INT SPEECH, DOI DOI 10.21437/INTERSPEECH.2014-564; Chen YP, 2018, ADV NEUR IN, V31; Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Chung Junyoung, 2014, ARXIV PREPRINT ARXIV; Cui YM, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P593, DOI 10.18653/v1/P17-1055; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; de Vries H, 2017, PROC CVPR IEEE, P4466, DOI 10.1109/CVPR.2017.475; Deng CR, 2018, PROC CVPR IEEE, P7746, DOI 10.1109/CVPR.2018.00808; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276; Fukui Akira, 2016, ARXIV160601847; Gao H., 2015, ADV NEURAL INFORM PR, V28, P2296, DOI DOI 10.1145/2733373.2807418; Gehring J, 2017, PR MACH LEARN RES, V70; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu RH, 2019, IEEE I CONF COMP VIS, P10293, DOI 10.1109/ICCV.2019.01039; Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93; Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493; Hudson D. A., 2018, ARXIV 180303067; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kazemzadeh Sahar, 2014, P 2014 C EMP METH NA, P787, DOI DOI 10.3115/V1/D14-1086; King DB, 2015, ACS SYM SER, V1214, P1; Kumar V, 2016, INT CONF ADVAN COMPU; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Lin Z., 2017, PROC INT C LEARN REP; Lu JS, 2016, ADV NEUR IN, V29; Luo RT, 2017, PROC CVPR IEEE, P3125, DOI 10.1109/CVPR.2017.333; Mao J., 2015, ARXIV14126632; Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9; Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sharma S., 2015, NEURAL INFORM PROCES; Strub F, 2018, LECT NOTES COMPUT SC, V11209, P808, DOI 10.1007/978-3-030-01228-1_48; Sukhbaatar S., 2015, ADV NEURAL INFORM PR, P175, DOI 10.1145/3130348.3130364; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Vaswani A, 2017, ADV NEUR IN, V30; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wang P, 2019, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2019.00206; Wang P, 2018, IEEE T PATTERN ANAL, V40, P2413, DOI 10.1109/TPAMI.2017.2754246; Wang P, 2017, PROC CVPR IEEE, P3909, DOI 10.1109/CVPR.2017.416; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Xiao FY, 2017, PROC CVPR IEEE, P5253, DOI 10.1109/CVPR.2017.558; Xiong CM, 2016, PR MACH LEARN RES, V48; Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang Z, 2016, P 2016 C N AM CHAPTE, P1480; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142; Yu LC, 2017, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2017.375; Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5; Zhang H, 2019, PR MACH LEARN RES, V97; Zhang HW, 2018, PROC CVPR IEEE, P4158, DOI 10.1109/CVPR.2018.00437; Zhou Bolei, 2015, ARXIV151202167; Zhuang BH, 2018, PROC CVPR IEEE, P4252, DOI 10.1109/CVPR.2018.00447	66	1	1	5	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2022	44	3					1670	1684		10.1109/TPAMI.2020.3023438	http://dx.doi.org/10.1109/TPAMI.2020.3023438			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YU4MA	32956036				2022-12-18	WOS:000752018000042
J	Lee, J; Kim, D; Lee, W; Ponce, J; Ham, B				Lee, Junghyup; Kim, Dohyung; Lee, Wonkyung; Ponce, Jean; Ham, Bumsub			Learning Semantic Correspondence Exploiting an Object-Level Prior	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Semantics; Training; Task analysis; Clutter; Feature extraction; Strain; Robustness; Semantic correspondence; object-level prior; differentiable argmax function	FLOW	We address the problem of semantic correspondence, that is, establishing a dense flow field between images depicting different instances of the same object or scene category. We propose to use images annotated with binary foreground masks and subjected to synthetic geometric deformations to train a convolutional neural network (CNN) for this task. Using these masks as part of the supervisory signal provides an object-level prior for the semantic correspondence task and offers a good compromise between semantic flow methods, where the amount of training data is limited by the cost of manually selecting point correspondences, and semantic alignment ones, where the regression of a single global geometric transformation between images may be sensitive to image-specific details such as background clutter. We propose a new CNN architecture, dubbed SFNet, which implements this idea. It leverages a new and differentiable version of the argmax function for end-to-end training, with a loss that combines mask and flow consistency with smoothness terms. Experimental results demonstrate the effectiveness of our approach, which significantly outperforms the state of the art on standard benchmarks.	[Lee, Junghyup; Kim, Dohyung; Lee, Wonkyung; Ham, Bumsub] Yonsei Univ, Sch Elect & Elect Engn, Seoul 03722, South Korea; [Ponce, Jean] PSL Univ, CNRS, INRIA, F-75005 Paris, France; [Ponce, Jean] PSL Univ, CNRS, DI ENS, Dept Informat ENS, F-75005 Paris, France	Yonsei University; Centre National de la Recherche Scientifique (CNRS); Inria; UDICE-French Research Universities; PSL Research University Paris; Universite Paris Cite; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite	Ham, B (corresponding author), Yonsei Univ, Sch Elect & Elect Engn, Seoul 03722, South Korea.	junghyup.lee@yonsei.ac.kr; do.hyung@yonsei.ac.kr; wonkyung.lee@yonsei.ac.kr; jean.ponce@inria.fr; bumsub.ham@yonsei.ac.kr		Lee, Junghyup/0000-0001-9427-6179; HAM, BUMSUB/0000-0002-3443-8161; Kim, Dohyung/0000-0002-6984-6325	National Research Foundation of Korea (NRF); Institute of Information and Communications Technology Planning & Evaluation (IITP) - Korea government (MSIT) [NRF-2019R1A2C2084816, 2016-0-00197]; French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program [ANR-19-P3IA0001]; Louis Vuitton/ENS chair on artificial intelligence; Inria/NYU collaboration agreement	National Research Foundation of Korea (NRF)(National Research Foundation of Korea); Institute of Information and Communications Technology Planning & Evaluation (IITP) - Korea government (MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program(French National Research Agency (ANR)); Louis Vuitton/ENS chair on artificial intelligence; Inria/NYU collaboration agreement	This work was supported by the National Research Foundation of Korea (NRF) and Institute of Information and Communications Technology Planning & Evaluation (IITP) grants funded by the Korea government (MSIT) (NRF-2019R1A2C2084816, 2016-0-00197, Development of the high-precision natural 3D view generation technology using smart-car multi sensors and deep learning), and also in part by the Louis Vuitton/ENS chair on artificial intelligence, the Inria/NYU collaboration agreement, and the French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program, reference ANR-19-P3IA0001 (PRAIRIE 3IA Institute). Junghyup Lee and Dohyung Kimcontributed equally to this work.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303; Bristow H, 2015, IEEE I CONF COMP VIS, P4024, DOI 10.1109/ICCV.2015.458; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Brox T, 2009, PROC CVPR IEEE, P41, DOI 10.1109/CVPRW.2009.5206697; Choy CB, 2016, ADV NEUR IN, V29; Dalal N., 2005, P IEEE COMP SOC C CO; Dale K, 2009, IEEE I CONF COMP VIS, P2217, DOI 10.1109/ICCV.2009.5459473; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Duchenne O, 2011, IEEE I CONF COMP VIS, P1792, DOI 10.1109/ICCV.2011.6126445; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Faktor A, 2013, IEEE I CONF COMP VIS, P1297, DOI 10.1109/ICCV.2013.164; Fouhey DF, 2018, PROC CVPR IEEE, P4991, DOI 10.1109/CVPR.2018.00524; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Ham B, 2018, IEEE T PATTERN ANAL, V40, P1711, DOI 10.1109/TPAMI.2017.2724510; Ham B, 2016, PROC CVPR IEEE, P3475, DOI 10.1109/CVPR.2016.378; Han K, 2017, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2017.203; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hosni A, 2013, IEEE T PATTERN ANAL, V35, P504, DOI 10.1109/TPAMI.2012.156; Hur J, 2015, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR.2015.7298745; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jaderberg M, 2015, ADV NEUR IN, V28; Jeon S., 2018, P EUR C COMP VIS, P351; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Joulin A, 2010, PROC CVPR IEEE, P1943, DOI 10.1109/CVPR.2010.5539868; Kanazawa A, 2016, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2016.354; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kim J, 2013, PROC CVPR IEEE, P2307, DOI 10.1109/CVPR.2013.299; Kim S, 2019, IEEE T PATTERN ANAL, V41, P581, DOI 10.1109/TPAMI.2018.2803169; Kim S, 2017, IEEE I CONF COMP VIS, P4539, DOI 10.1109/ICCV.2017.485; Kim S, 2017, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2017.73; Kim Seungryong, 2018, ADV NEURAL INFORM PR, P6129; Kingma D.P., 2015, INT C LEARN REPR, P1; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Lee J, 2019, PROC CVPR IEEE, P2273, DOI 10.1109/CVPR.2019.00238; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Long J.L., 2014, P C NEUR INF PROC SY, V27, P1601; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Meister S, 2018, AAAI CONF ARTIF INTE, P7251; Min J, 2019, IEEE I CONF COMP VIS, P3394, DOI 10.1109/ICCV.2019.00349; Novotny D, 2018, PROC CVPR IEEE, P3637, DOI 10.1109/CVPR.2018.00383; Novotny D, 2017, PROC CVPR IEEE, P2867, DOI 10.1109/CVPR.2017.306; OKUTOMI M, 1993, IEEE T PATTERN ANAL, V15, P353, DOI 10.1109/34.206955; Qiu WC, 2014, IEEE WINT CONF APPL, P1112, DOI 10.1109/WACV.2014.6835734; Revaud J, 2016, INT J COMPUT VISION, V120, P300, DOI 10.1007/s11263-016-0908-3; Rocco I, 2018, ADV NEUR IN, V31; Rocco I, 2018, PROC CVPR IEEE, P6917, DOI 10.1109/CVPR.2018.00723; Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12; Seo PH, 2018, LECT NOTES COMPUT SC, V11208, P367, DOI 10.1007/978-3-030-01225-0_22; Taniai T, 2016, PROC CVPR IEEE, P4246, DOI 10.1109/CVPR.2016.460; Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77; Wang XG, 2019, PROC CVPR IEEE, P8868, DOI [10.1109/CVPR.2019.00908, 10.1109/CVPR.2019.00267]; Xiang Y, 2014, IEEE WINT CONF APPL, P75, DOI 10.1109/WACV.2014.6836101; Yang F, 2017, PROC CVPR IEEE, P4151, DOI 10.1109/CVPR.2017.442; Yang HS, 2014, PROC CVPR IEEE, P3406, DOI 10.1109/CVPR.2014.435; Yang Y, 2013, IEEE T PATTERN ANAL, V35, P2878, DOI 10.1109/TPAMI.2012.261; Zbontar J, 2015, PROC CVPR IEEE, P1592, DOI 10.1109/CVPR.2015.7298767; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhou TH, 2016, PROC CVPR IEEE, P117, DOI 10.1109/CVPR.2016.20; Zhou TH, 2015, PROC CVPR IEEE, P1191, DOI 10.1109/CVPR.2015.7298723; Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI 10.1007/978-3-030-01219-9_; Zuffi S, 2012, PROC CVPR IEEE, P3546, DOI 10.1109/CVPR.2012.6248098	69	1	1	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2022	44	3					1399	1414		10.1109/TPAMI.2020.3013620	http://dx.doi.org/10.1109/TPAMI.2020.3013620			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YU4MA	32750842	Green Submitted			2022-12-18	WOS:000752018000024
J	Nguyen, T; Raich, R				Tam Nguyen; Raich, Raviv			Incomplete Label Multiple Instance Multiple Label Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Labeling; Training; Phase locked loops; Birds; Numerical models; Graphical models; Standards; Incomplete-label learning; learning with missing labels; multiple-instance multiple-label learning; multi-instance multi-label learning; maximum likelihood; marginal maximum likelihood; EM algorithm; graphical models; probabilistic models	CLASSIFICATION	With increasing data volumes, the bottleneck in obtaining data for training a given learning task is the cost of manually labeling instances within the data. To alleviate this issue, various reduced label settings have been considered including semi-supervised learning, partial- or incomplete-label learning, multiple-instance learning, and active learning. Here, we focus on multiple-instance multiple-label learning with missing bag labels. Little research has been done for this challenging yet potentially powerful variant of incomplete supervision learning. We introduce a novel discriminative probabilistic model for missing labels in multiple-instance multiple-label learning. To address inference challenges, we introduce an efficient implementation of the EM algorithm for the model. Additionally, we consider an alternative inference approach that relies on maximizing the label-wise marginal likelihood of the proposed model instead of the joint likelihood. Numerical experiments on benchmark datasets illustrate the robustness of the proposed approach. In particular, comparison to state-of-the-art methods shows that our approach introduces a significantly smaller decrease in performance when the proportion of missing labels is increased.	[Tam Nguyen; Raich, Raviv] Oregon State Univ, Sch Elect Engn & Comp Sci, Corvallis, OR 97331 USA	Oregon State University	Nguyen, T (corresponding author), Oregon State Univ, Sch Elect Engn & Comp Sci, Corvallis, OR 97331 USA.	nguyeta4@oregonstate.edu; raich@eecs.oregonstate.edu			US National Science Foundation [CCF-1254218, DBI-135679]	US National Science Foundation(National Science Foundation (NSF))	This work was supported in part by the US National Science Foundation Grants CCF-1254218 and DBI-135679.	Akbas E., 2007, P C COMP VIS CVPR 20, P1; Briggs F., 2012, P 18 ACM SIGKDD INT, P534, DOI DOI 10.1145/2339530.2339616; Briggs F, 2012, J ACOUST SOC AM, V131, P4640, DOI 10.1121/1.4707424; Chen YC, 2014, IEEE T INF FOREN SEC, V9, P2076, DOI 10.1109/TIFS.2014.2359642; Wang C, 2009, PROC CVPR IEEE, P1903, DOI [10.1109/CVPRW.2009.5206800, 10.1109/CVPR.2009.5206800]; Cour T, 2011, J MACH LEARN RES, V12, P1501; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97; Gong C, 2018, IEEE T CYBERNETICS, V48, P967, DOI 10.1109/TCYB.2017.2669639; Grandvalet Y., 2005, CAP, P529; Huang SJ, 2019, IEEE T PATTERN ANAL, V41, P2614, DOI 10.1109/TPAMI.2018.2861732; Hullermeier E, 2006, INTELL DATA ANAL, V10, P419, DOI 10.3233/IDA-2006-10503; Jin R., 2003, PROC NEURAL INF PROC, P921; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu L., 2012, ADV NEURAL INFORM PR, P548; Mao Q, 2013, IEEE T IMAGE PROCESS, V22, P1583, DOI 10.1109/TIP.2012.2233490; Nguyen N., 2008, P 14 ACM SIGKDD INT, P551, DOI 10.1145/1401890.1401958; Pham AT, 2017, IEEE T PATTERN ANAL, V39, P2381, DOI 10.1109/TPAMI.2017.2647944; Pham AT, 2015, PR MACH LEARN RES, V37, P2427; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Sun YY, 2010, AAAI CONF ARTIF INTE, P593; Tang CZ, 2017, AAAI CONF ARTIF INTE, P2611; Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34; Vannoorenberghe P, 2005, LECT NOTES COMPUT SC, V3571, P956; Winn J, 2005, IEEE I CONF COMP VIS, P1800; Wu BY, 2014, INT C PATT RECOG, P1964, DOI 10.1109/ICPR.2014.343; Wu JS, 2014, IEEE ACM T COMPUT BI, V11, P891, DOI 10.1109/TCBB.2014.2323058; Wu X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2868; Xie MK, 2018, AAAI CONF ARTIF INTE, P4302; Yang S. - J., 2013, P 23 INT JOINT C ART, P1862; YU F, 2016, PROC ASIAN C MACHINE, P96; Yu HF, 2014, PR MACH LEARN RES, V32; Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958; Zhang ML, 2017, IEEE T KNOWL DATA EN, V29, P2155, DOI 10.1109/TKDE.2017.2721942; Zhang ML, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1335, DOI 10.1145/2939672.2939788; Zhang ML, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4048; Zheng Y., 2013, P 25 INN APPL ART IN, P1575; Zhou Y, 2017, IEEE T CYBERNETICS, V47, P4443, DOI 10.1109/TCYB.2016.2611534; Zhou ZH, 2018, NATL SCI REV, V5, P44, DOI 10.1093/nsr/nwx106; Zhu Y, 2017, AAAI CONF ARTIF INTE, P2977	40	1	1	6	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 1	2022	44	3					1320	1337		10.1109/TPAMI.2020.3017456	http://dx.doi.org/10.1109/TPAMI.2020.3017456			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YU4MA	32813649				2022-12-18	WOS:000752018000019
J	Banerjee, M; Chakraborty, R; Bouza, J; Vemuri, BC				Banerjee, Monami; Chakraborty, Rudrasis; Bouza, Jose; Vemuri, Baba C.			VolterraNet: A Higher Order Convolutional Network With Group Equivariance for Homogeneous Manifolds	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Convolution; Manifolds; Correlation; Kernel; Extraterrestrial measurements; Large scale integration; Symmetric matrices; Homogeneous spaces; volterra series; convolutions; geometric deep learning; equivariance		Convolutional neural networks have been highly successful in image-based learning tasks due to their translation equivariance property. Recent work has generalized the traditional convolutional layer of a convolutional neural network to non-euclidean spaces and shown group equivariance of the generalized convolution operation. In this paper, we present a novel higher order Volterra convolutional neural network (VolterraNet) for data defined as samples of functions on Riemannian homogeneous spaces. Analagous to the result for traditional convolutions, we prove that the Volterra functional convolutions are equivariant to the action of the isometry group admitted by the Riemannian homogeneous spaces, and under some restrictions, any non-linear equivariant function can be expressed as our homogeneous space Volterra convolution, generalizing the non-linear shift equivariant characterization of Volterra expansions in euclidean space. We also prove that second order functional convolution operations can be represented as cascaded convolutions which leads to an efficient implementation. Beyond this, we also propose a dilated VolterraNet model. These advances lead to large parameter reductions relative to baseline non-euclidean CNNs. To demonstrate the efficacy of the VolterraNet performance, we present several real data experiments involving classification tasks on spherical-MNIST, atomic energy, Shrec17 data sets, and group testing on diffusion MRI data. Performance comparisons to the state-of-the-art are also presented.	[Banerjee, Monami] FaceBook, Menlo Pk, CA 94025 USA; [Chakraborty, Rudrasis] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Bouza, Jose; Vemuri, Baba C.] Univ Florida3463, Gainesville, FL 32611 USA	Facebook Inc; University of California System; University of California Berkeley	Chakraborty, R (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	monamie@ufl.edu; rchakraborty@ufl.edu; josejbouza@gmail.com; vemuri@cise.ufl.edu		Vemuri, Baba/0000-0002-1400-5844	NSF [IIS-1724174]	NSF(National Science Foundation (NSF))	This work was supported in part by the NSF Grant IIS-1724174 to BCV. The authors would like to thank Professor David Vaillancourt of the University of Florida, Department of Applied Physiology and Kinesiology for providing us with the diffusion MRI scans used in this work. Monami Banerjee and Rudrasis Chakraborty contributed equally to this work.	Archer DB, 2018, CEREB CORTEX, V28, P1685, DOI 10.1093/cercor/bhx066; Bai S., 2018, ICLR; Banerjee M, 2019, I S BIOMED IMAGING, P388, DOI 10.1109/ISBI.2019.8759558; Chakraborty R., 2018, ADV NEURAL INFORM PR, P8883; Cheng G, 2013, SIAM J IMAGING SCI, V6, P592, DOI 10.1137/110853376; Cohen T., 2017, ARXIV170904893; Cohen Taco, 2018, ARXIV181102017; Cohen Taco S, 2018, ICLR; DRISCOLL JR, 1994, ADV APPL MATH, V15, P202, DOI 10.1006/aama.1994.1008; Dummit D. S., 2004, ABSTRACT ALGEBRA, V3; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Esteves C., 2017, ARXIV170901889; Esteves C, 2018, LECT NOTES COMPUT SC, V11217, P54, DOI 10.1007/978-3-030-01261-8_4; Furuya T., 2016, P BMVC; Hackbusch W, 2007, J COMPLEXITY, V23, P697, DOI 10.1016/j.jco.2007.03.007; HAKIM NZ, 1991, CONFERENCE RECORD OF THE TWENTY-FIFTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P1128, DOI 10.1109/ACSSC.1991.186623; Hamilton WR., 1866, ELEMENTS QUATERNIONS; Helgason S., 1962, DIFFERENTIAL GEOMETR, V12; Hewitt E., 2012, ABSTRACT HARMONIC AN, V115; Jaderberg M, 2015, ADV NEUR IN, V28; Kingma D.P, P 3 INT C LEARNING R; Kondor R., 2018, ADV NEURAL INFORM PR, P10117; Kondor R, 2018, PR MACH LEARN RES, V80; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kumar R, 2012, IEEE T PATTERN ANAL, V34, P1423, DOI 10.1109/TPAMI.2011.225; Kumar R, 2009, PROC CVPR IEEE, P150, DOI 10.1109/CVPRW.2009.5206837; Kurz G, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON MULTISENSOR FUSION AND INTEGRATION FOR INTELLIGENT SYSTEMS (MFI), P49; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Michor P. W., 2008, TOPICS DIFFERENTIAL, V93; Montavon G., 2012, ADV NEURAL INF PROCE, P440; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Raj A., 2016, ARXIV161201988; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Tatsuma A, 2009, VISUAL COMPUT, V25, P785, DOI 10.1007/s00371-008-0304-2; Tkatchenko A, 2012, PHYS REV LETT, V108, DOI [10.1103/PhysRevLett.108.058301, 10.1103/PhysRevLett.108.236402]; Triacca U, 2016, ECONOMETRICS, V4, DOI 10.3390/econometrics4030032; Volterra V., 2005, THEORY FUNCTIONALS I; Worrall DE, 2017, PROC CVPR IEEE, P7168, DOI 10.1109/CVPR.2017.758; Yi L., 2017, ARXIV PREPRINT ARXIV; Zoumpourlis G, 2017, IEEE I CONF COMP VIS, P4771, DOI 10.1109/ICCV.2017.510	41	1	1	1	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2022	44	2					823	833		10.1109/TPAMI.2020.3035130	http://dx.doi.org/10.1109/TPAMI.2020.3035130			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YC9LS	33147684				2022-12-18	WOS:000740006100022
J	Dinh, TQ; Xiong, YY; Huang, ZC; Vo, T; Mishra, A; Kim, WH; Ravi, SN; Singh, V				Dinh, Tuan Q.; Xiong, Yunyang; Huang, Zhichun; Vo, Tien; Mishra, Akshay; Kim, Won Hwa; Ravi, Sathya N.; Singh, Vikas			Performing Group Difference Testing on Graph Structured Data From GANs: Analysis and Applications in Neuroimaging	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Gallium nitride; Statistical analysis; Diseases; Machine learning; Training data; Three-dimensional displays; Training; Generative adversarial network; graph theory; hypothesis testing; non-euclidean	GENERATIVE ADVERSARIAL NETWORKS; SIGNALS	Generative adversarial networks (GANs) have emerged as a powerful generative model in computer vision. Given their impressive abilities in generating highly realistic images, they are also being used in novel ways in applications in the life sciences. This raises an interesting question when GANs are used in scientific or biomedical studies. Consider the setting where we are restricted to only using the samples from a trained GAN for downstream group difference analysis (and do not have direct access to the real data). Will we obtain similar conclusions? In this work, we explore if "generated" data, i.e., sampled from such GANs can be used for performing statistical group difference tests in cases versus controls studies, common across many scientific disciplines. We provide a detailed analysis describing regimes where this may be feasible. We complement the technical results with an empirical study focused on the analysis of cortical thickness on brain mesh surfaces in an Alzheimer's disease dataset. To exploit the geometric nature of the data, we use simple ideas from spectral graph theory to show how adjustments to existing GANs can yield improvements. We also give a generalization error bound by extending recent results on Neural Network Distance. To our knowledge, our work offers the first analysis assessing whether the Null distribution in "healthy versus diseased subjects" type statistical testing using data generated from the GANs coincides with the one obtained from the same analysis with real data. The code is available at https://github.com/yyxiongzju/GLapGAN.	[Dinh, Tuan Q.; Xiong, Yunyang; Huang, Zhichun; Vo, Tien; Mishra, Akshay; Singh, Vikas] Univ Wisconsin, Madison, WI 53706 USA; [Kim, Won Hwa] Univ Texas Arlington, Dept Comp Sci & Engn, Arlington, TX 76019 USA; [Ravi, Sathya N.] Univ Illinois, Chicago, IL 60607 USA	University of Wisconsin System; University of Wisconsin Madison; University of Texas System; University of Texas Arlington; University of Illinois System; University of Illinois Chicago; University of Illinois Chicago Hospital	Dinh, TQ (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.	dinh5@wisc.edu; yxiong43@wisc.edu; zhuang294@wisc.edu; tnvo@wisc.edu; akshay.mishra@wisc.edu; won.kim@uta.edu; sathya@uic.edu; vsingh@biostat.wisc.edu		Kim, Won Hwa/0000-0001-5393-0883	UW CPCP [AI117924, R01 EB022883, RF1 AG062336, RF1 AG059312]; UW ADRC [AG033514]; NSF CAREER Award [RI 1252725]; Research Enhancement Program (REP) at the University of Texas at Arlington;  [IITP-2019-0-01579]	UW CPCP; UW ADRC; NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Research Enhancement Program (REP) at the University of Texas at Arlington; 	This work was supported in part by UW CPCP AI117924, R01 EB022883, RF1 AG062336, RF1 AG059312, UW ADRC (AG033514), and NSF CAREER Award RI 1252725. The work of WHK was also supported by IITP-2019-0-01579 and the Research Enhancement Program (REP) at the University of Texas at Arlington. VS is grateful to Jimeng Sun and Martin Reuter for discussions that benefited this work. Yunyang Xiong and Zhichun Huang contributed equally to this work.	Alex V, 2017, PROC SPIE, V10133, DOI 10.1117/12.2254487; Arjovsky M, 2017, PR MACH LEARN RES, V70; Arora S., 2018, ICLR; Azadil S, 2018, PROC CVPR IEEE, P7564, DOI 10.1109/CVPR.2018.00789; Baowaly MK, 2019, J AM MED INFORM ASSN, V26, P228, DOI 10.1093/jamia/ocy142; Bojchevski A., 2018, PR MACH LEARN RES, P610; Bora A, 2017, PR MACH LEARN RES, V70; Borji A, 2019, COMPUT VIS IMAGE UND, V179, P41, DOI 10.1016/j.cviu.2018.10.009; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Brock Andrew, 2018, P ICLR; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Cai Q, 2019, INT CONF BIG DATA, P359; Castro DC, 2019, J MACH LEARN RES, V20; Centers for Disease Control and Prevention (CDC), 2003, MMWR Suppl, V52, P1; Chen X., 2018, INT C MED IM DEEP LE, P1, DOI [10.3929/ethz-b-000321650, DOI 10.3929/ETHZ-B-000321650]; Chen YH, 2018, LECT NOTES COMPUT SC, V11070, P91, DOI 10.1007/978-3-030-00928-1_11; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P53, DOI 10.1016/j.acha.2006.04.004; Corona E., 2008, USING LAPLACIAN METH; Dasarathy G., 2017, P SIGN PROC AD SPARS; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; Dong XW, 2016, IEEE T SIGNAL PROCES, V64, P6160, DOI 10.1109/TSP.2016.2602809; Fan JF, 2018, LECT NOTES COMPUT SC, V11070, P739, DOI 10.1007/978-3-030-00928-1_83; Feizi S., 2017, ARXIV PREPRINT ARXIV; Fischl B, 2012, NEUROIMAGE, V62, P774, DOI 10.1016/j.neuroimage.2012.01.021; Gama F, 2019, IEEE T SIGNAL PROCES, V67, P1034, DOI 10.1109/TSP.2018.2887403; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Han LG, 2018, IEEE WINT CONF APPL, P682, DOI 10.1109/WACV.2018.00080; He Z, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9101042; Hensel M, 2017, ADV NEUR IN, V30; Ho J, 2016, ADV NEUR IN, V29; Hu YP, 2018, LECT NOTES COMPUT SC, V11070, P774, DOI 10.1007/978-3-030-00928-1_87; Izadi S, 2018, I S BIOMED IMAGING, P881; Jack CR, 2008, J MAGN RESON IMAGING, V27, P685, DOI 10.1002/jmri.21049; Jin WG, 2018, PR MACH LEARN RES, V80; Johnson NL, 1970, CONTINUOUS UNIVARIAT, V1; Kalofolias V., 2014, P NEUR INF PROC SYST; Kalofolias V, 2016, JMLR WORKSH CONF PRO, V51, P920; Kim T, 2017, PR MACH LEARN RES, V70; Kim WH, 2017, PROC CVPR IEEE, P5019, DOI 10.1109/CVPR.2017.533; Kim WH, 2014, NEUROIMAGE, V93, P107, DOI 10.1016/j.neuroimage.2014.02.028; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Kipf Thomas N., 2016, ARXIV161107308, V2, P1; Koller D., 2007, INTRO STAT RELATIONA, P13; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kyng K., 2015, PROC MACH LEARN RES, V40, P1190; Lai WS, 2017, ADV NEUR IN, V30; Le CM, 2017, RANDOM STRUCT ALGOR, V51, P538, DOI 10.1002/rsa.20713; LEVIN D. A., 2017, AM MATH SOC, DOI [10.1090/mbk/107, DOI 10.1090/MBK/107]; Li C., 2018, P ADV NEUR INF PROC, P6069; Li C. L., 2019, PROC DEEP GENERATIVE, P1; Li J, 2018, PR MACH LEARN RES, V80; Li Y, 2018, IEEE GLOB CONF SIG, P740, DOI 10.1109/GlobalSIP.2018.8646615; Liu Q, 2018, ADV NEUR IN, V31; Liu S, 2017, ADV NEUR IN, V30; Lucic M., 2018, ADV NEUR IN, P698; Madani A, 2018, NPJ DIGIT MED, V1, DOI [10.1038/s41746-018-0065-x, 10.1038/s41746-017-0013-1]; Mirsky Y, 2019, PROCEEDINGS OF THE 28TH USENIX SECURITY SYMPOSIUM, P461; Mirza M., 2014, ARXIV PREPRINT ARXIV; Nie W, 2019, UAI; Odena A., 2016, SEMISUPERVISED LEARN; Perraudin N., 2014, ARXIV E PRINTS; Poczos B, 2019, ADV NEURAL INFORM PR, P9089; Puy G, 2018, APPL COMPUT HARMON A, V44, P446, DOI 10.1016/j.acha.2016.05.005; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Salha Guillaume, 2020, ARXIV200201910; Sandfort V, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-52737-x; Sandryhaila A, 2014, IEEE SIGNAL PROC MAG, V31, P80, DOI 10.1109/MSP.2014.2329213; Schawinski K, 2017, MON NOT R ASTRON SOC, V467, pL110, DOI 10.1093/mnrasl/slx008; Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192; Simonovsky M, 2018, LECT NOTES COMPUT SC, V11139, P412, DOI 10.1007/978-3-030-01418-6_41; Spielman DA, 2007, ANN IEEE SYMP FOUND, P29, DOI 10.1109/FOCS.2007.56; Sui YL, 2013, INT SYM CODE GENER, P1; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tang YX, 2019, I S BIOMED IMAGING, P1358, DOI 10.1109/ISBI.2019.8759442; Theis Lucas, 2016, ICLR; Uzunova H, 2019, LECT NOTES COMPUT SC, V11769, P112, DOI 10.1007/978-3-030-32226-7_13; VanRullen R, 2019, COMMUN BIOL, V2, DOI 10.1038/s42003-019-0438-y; Vershynin R, 2012, J THEOR PROBAB, V25, P655, DOI 10.1007/s10959-010-0338-z; Wang HW, 2018, AAAI CONF ARTIF INTE, P2508; Wang XQ, 2018, BIOINFORMATICS, V34, P603, DOI 10.1093/bioinformatics/bty563; Wang YX, 2015, JMLR WORKSH CONF PRO, V38, P1042; Wei X., 2018, P ICLR; Wolterink JM, 2017, IEEE T MED IMAGING, V36, P2536, DOI 10.1109/TMI.2017.2708987; Xu K., 2019, ARXIV190913188; Yang G, 2018, IEEE T MED IMAGING, V37, P1310, DOI 10.1109/TMI.2017.2785879; Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890; Ying XD, 2019, PROC CVPR IEEE, P10611, DOI 10.1109/CVPR.2019.01087; Zhang MH, 2019, ADV NEUR IN, V32; Zhang PY, 2018, LECT NOTES COMPUT SC, V11070, P180, DOI 10.1007/978-3-030-00928-1_21; Zhang Y., 2017, PROC INT C MED IMAGE, P408; Zhang ZW, 2022, IEEE T KNOWL DATA EN, V34, P249, DOI 10.1109/TKDE.2020.2981333; Zhu Z., 2017, P INT C MACH LEARN, V97, P7654	99	1	1	2	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2022	44	2					877	889		10.1109/TPAMI.2020.3013433	http://dx.doi.org/10.1109/TPAMI.2020.3013433			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YC9LS	32763848	Green Accepted			2022-12-18	WOS:000740006100025
J	Liu, S; Ren, GH; Sun, Y; Wang, JQ; Wang, CH; Li, B; Yan, SC				Liu, Si; Ren, Guanghui; Sun, Yao; Wang, Jinqiao; Wang, Changhu; Li, Bo; Yan, Shuicheng			Fine-Grained Human-Centric Tracklet Segmentation with Single Frame Supervision	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Labeling; Object segmentation; Image segmentation; Task analysis; Semantics; Training; Face; Video object segmentation; human-centric; fine-grained; optical flow estimation		In this paper, we target at the Fine-grAined human-Centric Tracklet Segmentation (FACTS) problem, where 12 human parts, e.g., face, pants, left-leg, are segmented. To reduce the heavy and tedious labeling efforts, FACTS requires only one labeled frame per video during training. The small size of human parts and the labeling scarcity makes FACTS very challenging. Considering adjacent frames of videos are continuous and human usually do not change clothes in a short time, we explicitly consider the pixel-level and frame-level context in the proposed Temporal Context segmentation Network (TCNet). On the one hand, optical flow is on-line calculated to propagate the pixel-level segmentation results to neighboring frames. On the other hand, frame-level classification likelihood vectors are also propagated to nearby frames. By fully exploiting the pixel-level and frame-level context, TCNet indirectly uses the large amount of unlabeled frames during training and produces smooth segmentation results during inference. Experimental results on four video datasets show the superiority of TCNet over the state-of-the-arts. The newly annotated datasets can be downloaded via http://liusi-group.com/projects/FACTS for the further studies.	[Liu, Si; Li, Bo] Beihang Univ, Sch Comp Sci & Engn, Beijing 100191, Peoples R China; [Wang, Jinqiao] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China; [Ren, Guanghui; Sun, Yao] Chinese Acad Sci, Inst Informat Engn, Beijing 100190, Peoples R China; [Wang, Changhu] ByteDance AI Lab, Beijing, Peoples R China; [Yan, Shuicheng] Qihoo 360 AI Inst, Beijing, Peoples R China	Beihang University; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Institute of Information Engineering, CAS	Wang, JQ (corresponding author), Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.	liusi@buaa.edu.cn; renguanghui@iie.ac.cn; sunyao@iie.ac.cn; jqwang@nlpr.ia.ac.cn; wangchanghu@bytedance.com; boli@buaa.edu.cn; eleyans@nus.edu.sg	Yan, Shuicheng/HCI-1431-2022	Ren, Guanghui/0000-0002-2743-8278; liu, si/0000-0002-9180-2935; wang, jin qiao/0000-0002-9118-2780	National Key R&D Program of China [2016YFC0801003]; Natural Science Foundation of China [U1536203, 61572493, 61876177, 61772527, 61806200]	National Key R&D Program of China; Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was partially supported by the National Key R&D Program of China (Grant No.2016YFC0801003), and Natural Science Foundation of China (Grant U1536203, Grant 61572493, Grant 61876177, Grant 61772527 and Grant 61806200).	Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143; Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565; Caelles S., 2018 DAVIS CHALLENGE; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Fayyaz M., STFCN SPATIOTEMPORAL; Gadde R, 2017, IEEE I CONF COMP VIS, P4463, DOI 10.1109/ICCV.2017.477; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jiang H, 2017, PROC CVPR IEEE, P3435, DOI 10.1109/CVPR.2017.366; Jin XJ, 2017, IEEE I CONF COMP VIS, P5581, DOI 10.1109/ICCV.2017.595; Khoreva A., LUCID DATA DREAMING; Li S, 2017, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2017.551; Li X., VIDEO OBJECT SEGMENT; Li XX, 2018, LECT NOTES COMPUT SC, V11207, P93, DOI 10.1007/978-3-030-01219-9_6; Li YL, 2018, PROC CVPR IEEE, P5997, DOI 10.1109/CVPR.2018.00628; Liang XD, 2015, IEEE I CONF COMP VIS, P1386, DOI 10.1109/ICCV.2015.163; Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360; Liu S, 2017, PROC CVPR IEEE, P1013, DOI 10.1109/CVPR.2017.114; Liu S, 2015, PROC CVPR IEEE, P1419, DOI 10.1109/CVPR.2015.7298748; Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071; Luiten J., PREMVOS PROPOSAL GEN; Luo P, 2013, IEEE I CONF COMP VIS, P2648, DOI 10.1109/ICCV.2013.329; Luo WJ, 2016, PROC CVPR IEEE, P5695, DOI 10.1109/CVPR.2016.614; Mahasseni B, 2017, PROC CVPR IEEE, P2077, DOI 10.1109/CVPR.2017.224; Nilsson D, 2018, PROC CVPR IEEE, P6819, DOI 10.1109/CVPR.2018.00713; Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203; Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85; Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372; Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222; Pont-Tuset J., 2017 DAVIS CHALLENGE; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683; Shelhamer E, 2016, LECT NOTES COMPUT SC, V9915, P852, DOI 10.1007/978-3-319-49409-8_69; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Wang HY, 2015, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2015.7298788; Wang P, 2015, IEEE I CONF COMP VIS, P1573, DOI 10.1109/ICCV.2015.184; Wang TQ, 2016, IEEE T PATTERN ANAL, V38, P2501, DOI 10.1109/TPAMI.2016.2522418; Xia F., 2016, ECCV, P648; Xu N, 2018, LECT NOTES COMPUT SC, V11209, P603, DOI 10.1007/978-3-030-01228-1_36; Yamaguchi K, 2013, IEEE I CONF COMP VIS, P3519, DOI 10.1109/ICCV.2013.437; Zhang Hanwang, 2013, P INT ACM C MULT, P33, DOI DOI 10.1145/2502081.2502093; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhao R, 2014, PROC CVPR IEEE, P144, DOI 10.1109/CVPR.2014.26; Zhou QX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1527, DOI 10.1145/3240508.3240660; Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18; Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186; Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52; Zhu XZ, 2017, PROC CVPR IEEE, P4141, DOI 10.1109/CVPR.2017.441	52	1	1	1	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2022	44	2					610	621		10.1109/TPAMI.2019.2911936	http://dx.doi.org/10.1109/TPAMI.2019.2911936			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YC9LS	30998458				2022-12-18	WOS:000740006100006
J	Ming, X; Wei, FY; Zhang, T; Chen, D; Wen, F; Zhen, N				Ming, Xiang; Wei, Fangyun; Zhang, Ting; Chen, Dong; Wen, Fang; Zhen, Nanning			Group Sampling for Scale Invariant Face Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature extraction; Training; Face; Detectors; Face detection; Network architecture; Electronic mail; Object detection; convolution neural network; sampling	NETWORKS	Detectors based on deep learning tend to detect multi-scale objects on a single input image for efficiency. Recent works, such as FPN and SSD, generally use feature maps from multiple layers with different spatial resolutions to detect objects at different scales, e.g., high-resolution feature maps for small objects. However, we find that objects at all scales can also be well detected with features from a single layer of the network. In this paper, we carefully examine the factors affecting detection performance across a large range of scales, and conclude that the balance of training samples, including both positive and negative ones, at different scales is the key. We propose a group sampling method which divides the anchors into several groups according to the scale, and ensure that the number of samples for each group is the same during training. Our approach using only one single layer of FPN as features is able to advance the state-of-the-arts. Comprehensive analysis and extensive experiments have been conducted to show the effectiveness of the proposed method. Moreover, we show that our approach is favorably applicable to other tasks, such as object detection on COCO dataset, and to other detection pipelines, such as YOLOv3, SSD and R-FCN. Our approach, evaluated on face detection benchmarks including FDDB and WIDER FACE datasets, achieves state-of-the-art results without bells and whistles.	[Ming, Xiang] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China; [Wei, Fangyun] Microsoft Res Asia, Innovat Engn Grp, Beijing 100080, Peoples R China; [Zhang, Ting; Chen, Dong; Wen, Fang] Microsoft Res Asia, Visual Comp Grp, Beijing 100080, Peoples R China; [Zhen, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China	Xi'an Jiaotong University; Microsoft; Microsoft Research Asia; Microsoft; Microsoft Research Asia; Xi'an Jiaotong University	Ming, X (corresponding author), Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.	xjtustu.mx@stu.xjtu.edu.cn; fawe@microsoft.com; tinzhan@microsoft.com; doch@microsoft.com; fangwen@microsoft.com		Ming, Xiang/0000-0002-5512-9800				Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; [Anonymous], 2017, NEUROCOMPUTING; Bao JM, 2018, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2018.00702; Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299; Batista G.E., 2004, ACM SIGKDD EXPLOR NE, V6, P20, DOI [DOI 10.1145/1007730.1007735, 10.1145/1007730.1007735]; Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314; Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116; Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22; Cao KD, 2018, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2018.00544; Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953; Chen D, 2014, LECT NOTES COMPUT SC, V8694, P109, DOI 10.1007/978-3-319-10599-4_8; Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264; Cheng BW, 2018, LECT NOTES COMPUT SC, V11219, P473, DOI 10.1007/978-3-030-01267-0_28; Chi C., 2018, ABS180902693 CORR; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng JK, 2020, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR42600.2020.00525; Douzas G, 2018, EXPERT SYST APPL, V91, P464, DOI 10.1016/j.eswa.2017.09.030; Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667; Efraimidis P.S., 2016, ENCY ALGORITHMS, P2365; Farfade SS, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P643, DOI 10.1145/2671188.2749408; Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167; Ghiasi Golnaz, 2015, ARXIV150608347; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384; Han H, 2005, LECT NOTES COMPUT SC, V3644, P878, DOI 10.1007/11538059_91; Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642; He HB, 2008, IEEE IJCNN, P1322, DOI 10.1109/IJCNN.2008.4633969; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331; Honari S, 2016, PROC CVPR IEEE, P5743, DOI 10.1109/CVPR.2016.619; Hospedales TM, 2013, IEEE T KNOWL DATA EN, V25, P374, DOI 10.1109/TKDE.2011.231; Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166; Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267; Jain V., 2010, UMCS2010009; Jourabloo A, 2017, IEEE I CONF COMP VIS, P3219, DOI 10.1109/ICCV.2017.347; Jourabloo A, 2017, INT J COMPUT VISION, V124, P187, DOI 10.1007/s11263-017-1012-z; Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345; Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98; Law H., 2018, P EUR C COMP VIS ECC, P734; Li HX, 2015, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR.2015.7299170; Li J, 2019, PROC CVPR IEEE, P5055, DOI 10.1109/CVPR.2019.00520; Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508; Li JG, 2013, PROC CVPR IEEE, P3468, DOI 10.1109/CVPR.2013.445; Li YZ, 2016, LECT NOTES COMPUT SC, V9907, P420, DOI 10.1007/978-3-319-46487-9_26; Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713; Liu XY, 2009, IEEE T SYST MAN CY B, V39, P539, DOI 10.1109/TSMCB.2008.2007853; Ming X, 2019, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2019.00356; Najibi M, 2017, IEEE I CONF COMP VIS, P4885, DOI 10.1109/ICCV.2017.522; Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343; Ohn-Bar E, 2016, INT C PATT RECOG, P3350, DOI 10.1109/ICPR.2016.7900151; Paszke A, 2019, ADV NEUR IN, V32; Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5; Ranjan R., 2016, ARXIV160301249; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Shen YJ, 2018, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2018.00092; Shi WZ, 2017, IEEE IMAGE PROC, P977; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Shrivastava Abhinav, 2016, ARXIV161206851; Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377; Tang X., 2018, LECT NOTES COMPUT SC, P812, DOI [10.1007/978-3-030-01240-3_49, DOI 10.1007/978-3-030-01240-3_49]; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Wan WT, 2018, PROC CVPR IEEE, P9117, DOI 10.1109/CVPR.2018.00950; Wang H., 2017, ARXIV170601061; Wang J., 2017, ARXIV171107246; Wang Yitong, 2017, ARXIV170905256; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309; Wu WY, 2018, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2018.00227; Yang B, 2014, 2014 IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2014); Yang HY, 2018, PROC CVPR IEEE, P31, DOI 10.1109/CVPR.2018.00011; Yang JL, 2017, PROC CVPR IEEE, P5216, DOI 10.1109/CVPR.2017.554; Yang S., 2017, ARXIV170602863; Yang S, 2020, IEEE T MULTIMEDIA, V22, P2163, DOI 10.1109/TMM.2019.2947352; Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596; Yang S, 2015, IEEE I CONF COMP VIS, P3676, DOI 10.1109/ICCV.2015.419; Yen SJ, 2009, EXPERT SYST APPL, V36, P5718, DOI 10.1016/j.eswa.2008.06.108; Yu Jiahui., 2016, ACM MM, DOI DOI 10.1145/2964284.2967274; Zhang J., 2017, ARXIV171200721; Zhang KP, 2017, IEEE I CONF COMP VIS, P3190, DOI 10.1109/ICCV.2017.344; Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342; Zhang SF, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P1, DOI 10.1109/BTAS.2017.8272675; Zhang SF, 2017, IEEE I CONF COMP VIS, P192, DOI 10.1109/ICCV.2017.30; Zhou ZH, 2006, IEEE T KNOWL DATA EN, V18, P63, DOI 10.1109/TKDE.2006.17; Zhu CC, 2018, PROC CVPR IEEE, P5127, DOI 10.1109/CVPR.2018.00538; Zhu CC, 2017, ADV COMPUT VIS PATT, P57, DOI 10.1007/978-3-319-61657-5_3; Zhu SZ, 2016, PROC CVPR IEEE, P3409, DOI 10.1109/CVPR.2016.371; Zhu SZ, 2016, LECT NOTES COMPUT SC, V9909, P614, DOI 10.1007/978-3-319-46454-1_37; Zhu X., 2018, PACIFIC ASIA C KNOWL, P349; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	100	1	1	5	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2022	44	2					985	1001		10.1109/TPAMI.2020.3012414	http://dx.doi.org/10.1109/TPAMI.2020.3012414			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YC9LS	32750835				2022-12-18	WOS:000740006100032
J	Mousavi, A; Baraniuk, RG				Mousavi, Ali; Baraniuk, Richard G.			Uniform Partitioning of Data Grid for Association Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Microwave integrated circuits; Mutual information; Partitioning algorithms; Heuristic algorithms; Dynamic programming; Extraterrestrial measurements; Correlation; Detection; estimation; correlation; mutual information; k-nearest neighbor	MUTUAL INFORMATION; DEPENDENCE; ENTROPY	Inferring appropriate information from large datasets has become important. In particular, identifying relationships among variables in these datasets has far-reaching impacts. In this article, we introduce the uniform information coefficient (UIC), which measures the amount of dependence between two multidimensional variables and is able to detect both linear and non-linear associations. Our proposed UIC is inspired by the maximal information coefficient (MIC) [1].; however, the MIC was originally designed to measure dependence between two one-dimensional variables. Unlike the MIC calculation that depends on the type of association between two variables, we show that the UIC calculation is less computationally expensive and more robust to the type of association between two variables. The UIC achieves this by replacing the dynamic programming step in the MIC calculation with a simpler technique based on the uniform partitioning of the data grid. This computational efficiency comes at the cost of not maximizing the information coefficient as done by the MIC algorithm. We present theoretical guarantees for the performance of the UIC and a variety of experiments to demonstrate its quality in detecting associations.	[Mousavi, Ali] Google Res, Kirkland, WA 98033 USA; [Baraniuk, Richard G.] Rice Univ, Dept Elect & Comp Engn, Houston, TX 77005 USA	Google Incorporated; Rice University	Mousavi, A (corresponding author), Google Res, Kirkland, WA 98033 USA.	alimous@google.com; richb@rice.edu		Baraniuk, Richard/0000-0002-0721-8999; Mousavi, Ali/0000-0002-1160-5975	US National Science Foundation [CCF-1911094, IIS-1838177, IIS-1730574]; ONR [N00014-18-12571, N00014-17-1-2551, N00014-18-1-2047]; AFOSR [FA9550-18-1-0478]; DARPA [G001534-7500]; Vannevar Bush Faculty Fellowship	US National Science Foundation(National Science Foundation (NSF)); ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Vannevar Bush Faculty Fellowship	This work was supported in part by US National Science Foundation Grants CCF-1911094, IIS-1838177, and IIS-1730574; ONR Grants N00014-18-12571 and N00014-17-1-2551; AFOSR Grant FA9550-18-1-0478; DARPA Grant G001534-7500; and a Vannevar Bush Faculty Fellowship, ONRGrantN00014-18-1-2047.	Bahl L. R., 1986, ICASSP 86 Proceedings. IEEE-IECEJ-ASJ International Conference on Acoustics, Speech and Signal Processing (Cat. No.86CH2243-4), P49; Belghazi MI, 2018, PR MACH LEARN RES, V80; BELLMAN R, 1966, SCIENCE, V153, P34, DOI 10.1126/science.153.3731.34; BREIMAN L, 1985, J AM STAT ASSOC, V80, P580, DOI 10.2307/2288473; Chattopadhyay I, 2014, J R SOC INTERFACE, V11, DOI 10.1098/rsif.2014.0826; Chen Y, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0157567; CHOW YL, 1990, INT CONF ACOUST SPEE, P701, DOI 10.1109/ICASSP.1990.115863; Cover T. M., 1991, ELEMENTS INFORM THEO, V6; Cramer H, 1999, MATH METHODS STAT, V9; Gao SY, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P278; Gao SY, 2015, JMLR WORKSH CONF PRO, V38, P277; Gentile C, 2017, PR MACH LEARN RES, V70; Khan S, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.026209; Kolmogorov A., 1933, GRUNDBEGRIFFE WAHRSC; Kraskov A, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066138; Krueger D, 2020, ARXIV200300688; Li SZ, 2016, THESIS; MODDEMEIJER R, 1989, SIGNAL PROCESS, V16, P233, DOI 10.1016/0165-1684(89)90132-1; Moon Kevin R., 2017, 2017 IEEE International Symposium on Information Theory (ISIT), P3030, DOI 10.1109/ISIT.2017.8007086; MOON YI, 1995, PHYS REV E, V52, P2318, DOI 10.1103/PhysRevE.52.2318; Mousavi A, 2015, ANN ALLERTON CONF, P650, DOI 10.1109/ALLERTON.2015.7447066; Noshad M, 2019, INT CONF ACOUST SPEE, P2962, DOI 10.1109/ICASSP.2019.8683351; Noshad M, 2017, IEEE INT SYMP INFO, P903, DOI 10.1109/ISIT.2017.8006659; Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272; Pirie W., 1988, ENCY STAT SCI; Renyi A., 1959, ACTA MATH ACADEMIAE, V10, P217, DOI DOI 10.1007/BF02063300; Reshef DN, 2011, SCIENCE, V334, P1518, DOI 10.1126/science.1205438; Sekeh SY, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21080787; Simon N., 2014, SCIENCE; Szekely GJ, 2007, ANN STAT, V35, P2769, DOI 10.1214/009053607000000505	31	1	1	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2022	44	2					1098	1107		10.1109/TPAMI.2020.3029487	http://dx.doi.org/10.1109/TPAMI.2020.3029487			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YC9LS	33026983				2022-12-18	WOS:000740006100039
J	Zafeiriou, S; Bronstein, M; Cohen, T; Vinyals, O; Song, L; Leskovec, J; Lio, P; Bruna, J; Gori, M				Zafeiriou, Stefanos; Bronstein, Michael; Cohen, Taco; Vinyals, Oriol; Song, Le; Leskovec, Jure; Lio, Pietro; Bruna, Joan; Gori, Marco			Guest Editorial: Non-Euclidean Machine Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									[Zafeiriou, Stefanos; Bronstein, Michael] Imperial Coll London, Dept Comp, London SW7 2AZ, England; [Bronstein, Michael] Univ Svizzera Italiana, Inst Computat Sci, CH-6900 Lugano, Switzerland; [Cohen, Taco] Qualcomm Technol Netherlands BV, NL-6546 Nijmegen, Netherlands; [Vinyals, Oriol] Google DeepMind, London AB101, England; [Song, Le] Mohamed bin Zayed Univ Artificial Intelligence, Abu Dhabi 51133, U Arab Emirates; [Leskovec, Jure] Stanford Univ, Comp Sci Dept, Stanford, CA 94305 USA; [Lio, Pietro] Univ Cambridge, Comp Lab, William Gates Bldg, Cambridge CB3 0FD, England; [Bruna, Joan] NYU, Courant Inst Math Sci, New York, NY 10012 USA; [Gori, Marco] Univ Siena, Dept Informat Engn & Math, I-53100 Siena, SI, Italy	Imperial College London; Universita della Svizzera Italiana; Google Incorporated; Mohamed Bin Zayed University of Artificial Intelligence; Stanford University; University of Cambridge; New York University; University of Siena	Zafeiriou, S (corresponding author), Imperial Coll London, Dept Comp, London SW7 2AZ, England.	s.zafeiriou@imperial.ac.uk; m.bronstein@imperial.ac.uk; tacos@qti.qualcomm.com; vinyals@google.com; dasongle@gmail.com; jure@cs.stanford.edu; pl219@cam.ac.uk; bruna@cs.nyu.edu; bruna@cs.nyu.edu		Lio, Pietro/0000-0002-0540-5053	EPSRC project DEFORM [EP/S010203/1]	EPSRC project DEFORM(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This special section would not have been possible without the efforts and interests from all the authors who contributed their submissions. The authors would like to take this opportunity to thank them and are grateful to the reviewers for their careful and valuable comments on the submitted manuscripts and for their detailed and helpful suggestions for improvement. S. Zafeiriou is supported from EPSRC project DEFORM under Grant EP/S010203/	Bai L, 2022, IEEE T PATTERN ANAL, V44, P783, DOI 10.1109/TPAMI.2020.3011866; Banerjee M, 2022, IEEE T PATTERN ANAL, V44, P823, DOI 10.1109/TPAMI.2020.3035130; Chakraborty R, 2022, IEEE T PATTERN ANAL, V44, P799, DOI 10.1109/TPAMI.2020.3003846; Chen X, 2022, IEEE T PATTERN ANAL, V44, P740, DOI 10.1109/TPAMI.2020.3032189; Ciano G, 2022, IEEE T PATTERN ANAL, V44, P758, DOI 10.1109/TPAMI.2021.3054304; Dinh TQ, 2022, IEEE T PATTERN ANAL, V44, P877, DOI 10.1109/TPAMI.2020.3013433; Gopinath K, 2022, IEEE T PATTERN ANAL, V44, P864, DOI 10.1109/TPAMI.2020.3028391; Gui S., 2022, IEEE T PATTERN ANAL, V44, P770; Lee Y, 2022, IEEE T PATTERN ANAL, V44, P834, DOI 10.1109/TPAMI.2020.2997045; Otberdout N, 2022, IEEE T PATTERN ANAL, V44, P848, DOI 10.1109/TPAMI.2020.3002500; Sommer S, 2022, IEEE T PATTERN ANAL, V44, P811, DOI 10.1109/TPAMI.2020.2994507; Tiezzi M, 2022, IEEE T PATTERN ANAL, V44, P727, DOI 10.1109/TPAMI.2021.3073504	12	1	1	2	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 1	2022	44	2					723	726		10.1109/TPAMI.2021.3129857	http://dx.doi.org/10.1109/TPAMI.2021.3129857			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YC9LS		Bronze			2022-12-18	WOS:000740006100014
J	Liu, SC; Li, TY; Chen, WK; Li, H				Liu, Shichen; Li, Tianye; Chen, Weikai; Li, Hao			A General Differentiable Mesh Renderer for Image-Based 3D Reasoning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Vision and scene understanding; modeling and recovery of physical attributes; perceptual reasoning; computer graphics; picture/image generation	APPEARANCE	Rendering bridges the gap between 2D vision and 3D scenes by simulating the physical process of image formation. By inverting such renderer, one can think of a learning approach to infer 3D information from 2D images. However, standard graphics renderers involve a fundamental step called rasterization, which prevents rendering to be differentiable. Unlike the state-of-the-art differentiable renderers (Kato et al. 2018 and Loper 2018), which only approximate the rendering gradient in the backpropagation, we propose a natually differentiable rendering framework that is able to (1) directly render colorized mesh using differentiable functions and (2) back-propagate efficient supervisions to mesh vertices and their attributes from various forms of image representations. The key to our framework is a novel formulation that views rendering as an aggregation function that fuses the probabilistic contributions of all mesh triangles with respect to the rendered pixels. Such formulation enables our framework to flow gradients to the occluded and distant vertices, which cannot be achieved by the previous state-of-the-arts. We show that by using the proposed renderer, one can achieve significant improvement in 3D unsupervised single-view reconstruction both qualitatively and quantitatively. Experiments also demonstrate that our approach can handle the challenging tasks in image-based shape fitting, which remain nontrivial to existing differentiable renders.	[Liu, Shichen; Li, Tianye] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90007 USA; [Liu, Shichen; Li, Tianye] USC Inst Creat Technol, Los Angeles, CA 90094 USA; [Chen, Weikai] Tencent Amer, Los Angeles, CA 94306 USA; [Li, Hao] Pinscreen, Los Angeles, CA 90025 USA	University of Southern California	Chen, WK (corresponding author), Tencent Amer, Los Angeles, CA 94306 USA.	liushichen95@gmail.com; tianye.focus@gmail.com; chenwk891@gmail.com; hao@hao-li.com		Li, Hao/0000-0002-4019-3420				[Anonymous], 2013, J COMPUTER GRAPHICS; Bavoil, 2007, P ACM SIGGRAPH TECHN; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Carpenter L., 1984, Computers & Graphics, V18, P103; Chang Angel X., 2015, ARXIV151203012CSGR P; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Deschaintre V, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201378; Enderton E, 2011, IEEE T VIS COMPUT GR, V17, P1036, DOI 10.1109/TVCG.2010.123; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874; Gkioulekas I, 2016, LECT NOTES COMPUT SC, V9907, P685, DOI 10.1007/978-3-319-46487-9_42; Gkioulekas I, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508377; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Hartley R., 2003, MULTIPLE VIEW GEOMET; HENDERSON P, 2018, P BRIT MACH VIS C; Huang Z, 2018, LECT NOTES COMPUT SC, V11220, P351, DOI 10.1007/978-3-030-01270-0_21; Huynh L, 2018, PROC CVPR IEEE, P8407, DOI 10.1109/CVPR.2018.00877; Insafutdinov E, 2018, ADV NEUR IN, V31; Jansen Jon, 2010, P 2010 ACM SIGGRAPH, P165, DOI [10.1145/1730804.1730831, DOI 10.1145/1730804.1730831]; Kanazawa A, 2018, LECT NOTES COMPUT SC, V11219, P386, DOI 10.1007/978-3-030-01267-0_23; Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411; Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336; King DB, 2015, ACS SYM SER, V1214, P1; Kundu A, 2018, PROC CVPR IEEE, P3559, DOI 10.1109/CVPR.2018.00375; Lensch HPA, 2003, ACM T GRAPHIC, V22, P234, DOI 10.1145/636886.636891; Li TM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275109; Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283; Liu F, 2016, LECT NOTES COMPUT SC, V9909, P545, DOI 10.1007/978-3-319-46454-1_33; Liu GL, 2017, IEEE I CONF COMP VIS, P2280, DOI 10.1109/ICCV.2017.248; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11; Loubet G, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356510; Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767; Mansinghka V., 2013, ADV NEURAL INFORM PR; Masi I, 2016, PROC CVPR IEEE, P4838, DOI 10.1109/CVPR.2016.523; Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951; Maule M., 2013, P ACM SIGGRAPH S INT, P103, DOI [10.1145/2448196.2448212, DOI 10.1145/2448196.2448212]; Meshkin H., 2007, GDC TALK; Myers K., 2008, NVIDIA OPENGL SDK, P1; Nalbach O, 2017, COMPUT GRAPH FORUM, V36, P65, DOI 10.1111/cgf.13225; Nguyen-Phuoc Thu, 2018, ADV NEURAL INFORM PR; Nimier-David M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356498; Pan JY, 2019, IEEE I CONF COMP VIS, P9963, DOI 10.1109/ICCV.2019.01006; Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138; Qi XJ, 2018, PROC CVPR IEEE, P283, DOI 10.1109/CVPR.2018.00037; Rezende DJ, 2016, ADV NEUR IN, V29; Rhodin H, 2015, IEEE I CONF COMP VIS, P765, DOI 10.1109/ICCV.2015.94; Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589; Tewari A, 2018, PROC CVPR IEEE, P2549, DOI 10.1109/CVPR.2018.00270; Tewari A, 2017, IEEE INT CONF COMP V, P1274, DOI 10.1109/ICCVW.2017.153; Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30; Tulsiani S, 2015, PROC CVPR IEEE, P1510, DOI 10.1109/CVPR.2015.7298758; Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4; Wang XL, 2015, PROC CVPR IEEE, P539, DOI 10.1109/CVPR.2015.7298652; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Xiang Y, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Zhang C, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356522; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284; Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18; Zienkiewicz J, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4280, DOI 10.1109/IROS.2016.7759630	66	1	1	4	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2022	44	1					50	62		10.1109/TPAMI.2020.3007759	http://dx.doi.org/10.1109/TPAMI.2020.3007759			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XM0XY	32750808				2022-12-18	WOS:000728561300005
J	Su, B; Wu, Y				Su, Bing; Wu, Ying			Learning Meta-Distance for Sequences by Learning a Ground Metric via Virtual Sequence Regression	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Metric learning; temporal alignment; virtual sequence regression; optimal transport	ACTION RECOGNITION	Distance between sequences is structural by nature because it needs to establish the temporal alignments among the temporally correlated vectors in sequences with varying lengths. Generally, distances for sequences heavily depend on the ground metric between the vectors in sequences to infer the alignments and hence can be viewed as meta-distances upon the ground metric. Learning such meta-distance from multi-dimensional sequences is appealing but challenging. We propose to learn the meta-distance through learning a ground metric for the vectors in sequences. The learning samples are sequences of vectors for which how the ground metric between vectors induces the meta-distance is given. The objective is that the meta-distance induced by the learned ground metric produces large values for sequences from different classes and small values for those from the same class. We formulate the ground metric as a parameter of the meta-distance and regress each sequence to an associated pre-generated virtual sequence w.r.t. the meta-distance, where the virtual sequences for sequences of different classes are well-separated. We develop general iterative solutions to learn both the Mahalanobis metric and the deep metric induced by a neural network for any ground-metric-based sequence distance. Experiments on several sequence datasets demonstrate the effectiveness and efficiency of the proposed methods.	[Su, Bing] Renmin Univ China, Gaoling Sch Artificial Intelligence, Beijing Key Lab Big Data Management & Anal Method, Beijing 100872, Peoples R China; [Wu, Ying] Northwestern Univ, Dept Elect & Comp Engn, Evanston, IL 60208 USA	Renmin University of China; Northwestern University	Su, B (corresponding author), Renmin Univ China, Gaoling Sch Artificial Intelligence, Beijing Key Lab Big Data Management & Anal Method, Beijing 100872, Peoples R China.	subingats@gmail.com; yingwu@ece.northwestern.edu			National Natural Science Foundation of China [61976206, 61832017, 61603373]; Beijing Outstanding Young Scientist Program [BJJWZYJH012019100020098]; Youth Innovation Promotion Association CAS [2019110]; US National Science Foundation [IIS-1619078, IIS-1815561]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Outstanding Young Scientist Program; Youth Innovation Promotion Association CAS; US National Science Foundation(National Science Foundation (NSF))	The authors would like to thank the associate editor and anonymous reviewers for their valuable comments. This work was supported in part by the National Natural Science Foundation of China No. 61976206, No. 61832017, and No. 61603373, Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098, Youth Innovation Promotion Association CAS No. 2019110, and US National Science Foundation grant IIS-1619078, IIS-1815561.	[Anonymous], 2006, P 18 INT C NEUR INF; Bellet A., 2013, ARXIV; Bellet A, 2011, LECT NOTES ARTIF INT, V6911, P188, DOI 10.1007/978-3-642-23780-5_22; Bellet A, 2012, MACH LEARN, V89, P5, DOI 10.1007/s10994-012-5293-8; Ben Tanfous A, 2018, PROC CVPR IEEE, P2840, DOI 10.1109/CVPR.2018.00300; Cavazza J, 2016, INT C PATT RECOG, P408, DOI 10.1109/ICPR.2016.7899668; Cho S, 2020, IEEE WINT CONF APPL, P624, DOI 10.1109/WACV45572.2020.9093639; Cortes C, 2004, J MACH LEARN RES, V5, P1035; Cortes C, 2008, MACHINE LEARN SIGN P, P2, DOI 10.1109/MLSP.2008.4685446; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi M, 2014, J MACH LEARN RES, V15, P533; Deng YX, 2017, IEEE DEVICE RES CONF; Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI [10.1109/TPAMI.2007.1115, 10.1109/TP'AMI.2007.1115]; Escalera S, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2522848.2532595; Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176; Frome Andrea, 2013, NEURIPS; Ge WF, 2018, LECT NOTES COMPUT SC, V11210, P272, DOI 10.1007/978-3-030-01231-1_17; Goldberger Jacob, 2005, ADV NEURAL INFORM PR, V17, P8, DOI DOI 10.1109/TCSVT.2013.2242640; Hadsell R., 2006, 2006 IEEE COMPUTER S, P1735, DOI DOI 10.1109/CVPR.2006.100; Harandi M, 2014, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2014.132; Hermans Alexander, 2017, ARXIV, P1; Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7; Huang G., 2016, PROC NEURAL INF PROC, P4869; Itti, 2016, METRICDTW LOCAL DIST; Ji XP, 2018, SIGNAL PROCESS, V143, P56, DOI 10.1016/j.sigpro.2017.08.016; Jin R., 2009, ADV NEURAL INFORM PR, V22; Kadous M. W., 2002, TEMPORAL CLASSIFICAT; Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486; Koniusz P, 2016, LECT NOTES COMPUT SC, V9908, P37, DOI 10.1007/978-3-319-46493-0_3; Lajugie R., 2014, ADV NEURAL INFORM PR, P1817; Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115; Li C, 2018, IEEE INT CONF SENS, P1, DOI 10.1109/TFUZZ.2018.2878200; Li S, 2018, PROC CVPR IEEE, P5457, DOI 10.1109/CVPR.2018.00572; Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273; Lincham M., 2013, UCI MACH LEARN REPOS; Liu, 2017, KDD WORKSH MIN TIM S; Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306; LOHIT S, 2019, P IEEE C COMP VIS PA, P426; Mahalanobis PC, 2018, SANKHYA SER A, V80, P1, DOI 10.1007/s13171-019-00164-5; Mazagonwalla A., 2019, SKELETON BASED ZERO; Mei JY, 2016, IEEE T CYBERNETICS, V46, P1363, DOI 10.1109/TCYB.2015.2426723; Mei JY, 2014, IEEE T IMAGE PROCESS, V23, P4920, DOI 10.1109/TIP.2014.2359765; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Mokbel B, 2015, NEUROCOMPUTING, V169, P306, DOI 10.1016/j.neucom.2014.11.082; PAASSEN B, 2018, P INT C MACH LEARN, P3976; Perrot M., 2015, ADV NEURAL INFORM PR, P1810; Qi G.-J., 2009, P INT C MACH LEARN J, P841; Ratanamahatana CA, 2004, SIAM PROC S, P11; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055; Salakhutdinov Ruslan, 2007, J MACHINE LEARNING R, P412, DOI DOI 10.1109/ICCV.2017.74; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Schultz M, 2004, ADV NEUR IN, V16, P41; Shahri Alimohammad, 2016, 2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS), P1, DOI 10.1109/RCIS.2016.7549312; Shahroudy A, 2018, IEEE T PATTERN ANAL, V40, P1045, DOI 10.1109/TPAMI.2017.2691321; Shi L., 2019, CVPR; Shi Y, 2014, AAAI CONF ARTIF INTE, P2078; Smagt P. V. D., 2012, P 22 INT C ART NUER, P2638; Sohn K, 2016, ADV NEUR IN, V29; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Song SJ, 2017, AAAI CONF ARTIF INTE, P4263; SU B, 2019, PROC INT C MACH LEAR, P6015; Su B., 2018, P 35 INT C MACH LEAR, P4768; Su B, 2019, IEEE I CONF COMP VIS, P9884, DOI 10.1109/ICCV.2019.00998; Su B, 2020, IEEE T PATTERN ANAL, V42, P2842, DOI 10.1109/TPAMI.2019.2919303; Su B, 2019, IEEE T PATTERN ANAL, V41, P2961, DOI 10.1109/TPAMI.2018.2870154; Su B, 2017, IEEE T IMAGE PROCESS, V26, P5784, DOI 10.1109/TIP.2017.2745212; Su B, 2018, IEEE T PATTERN ANAL, V40, P77, DOI 10.1109/TPAMI.2017.2665545; Su Bing, 2017, P IEEE C COMP VIS PA, P1049; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Trigeorgis G, 2018, IEEE T PATTERN ANAL, V40, P1128, DOI 10.1109/TPAMI.2017.2710047; Trigeorgis G, 2016, PROC CVPR IEEE, P5110, DOI 10.1109/CVPR.2016.552; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wang J, 2013, IEEE I CONF COMP VIS, P2688, DOI 10.1109/ICCV.2013.334; Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813; Wang L, 2015, IEEE I CONF COMP VIS, P4570, DOI 10.1109/ICCV.2015.519; Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Xing E., 2002, ADV NEURAL INFORM PR, V15, P505, DOI DOI 10.5555/2968618.2968683; Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16; Zanfir M, 2013, IEEE I CONF COMP VIS, P2752, DOI 10.1109/ICCV.2013.342; Zhang PF, 2020, IEEE T IMAGE PROCESS, V29, P1061, DOI 10.1109/TIP.2019.2937724; Zheng W, 2019, IEEE INT CON MULTI, P826, DOI 10.1109/ICME.2019.00147; Zhou F., 2009, ADV NEURAL INFORM PR, V22, P2286; Zhou F, 2016, IEEE T PATTERN ANAL, V38, P279, DOI 10.1109/TPAMI.2015.2414429; Zhou F, 2012, PROC CVPR IEEE, P1282, DOI 10.1109/CVPR.2012.6247812; Zhu, 2019, DEEP INDEPENDENTLY R	87	1	1	4	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2022	44	1					286	301		10.1109/TPAMI.2020.3010568	http://dx.doi.org/10.1109/TPAMI.2020.3010568			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XM0XY	32750823				2022-12-18	WOS:000728561300021
J	Franca, G; Rizzo, ML; Vogelstein, JT				Franca, Guilherme; Rizzo, Maria L.; Vogelstein, Joshua T.			Kernel k-Groups via Hartigan's Method	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Kernel; Hilbert space; Probability distribution; Extraterrestrial measurements; Machine learning; Clustering methods; Clustering; energy statistics; kernel methods; graph clustering; community detection; stochastic block model	COMMUNITY; STATISTICS; ENERGY; CUTS	Energy statistics was proposed by Szekely in the 80's inspired by Newton's gravitational potential in classical mechanics and it provides a model-free hypothesis test for equality of distributions. In its original form, energy statistics was formulated in euclidean spaces. More recently, it was generalized to metric spaces of negative type. In this paper, we consider a formulation for the clustering problem using a weighted version of energy statistics in spaces of negative type. We show that this approach leads to a quadratically constrained quadratic program in the associated kernel space, establishing connections with graph partitioning problems and kernel methods in machine learning. To find local solutions of such an optimization problem, we propose kernel k-groups, which is an extension of Hartigan's method to kernel spaces. Kernel k-groups is cheaper than spectral clustering and has the same computational cost as kernel k-means (which is based on Lloyd's heuristic) but our numerical results show an improved performance, especially in higher dimensions. Moreover, we verify the efficiency of kernel k-groups in community detection in sparse stochastic block models which has fascinating applications in several areas of science.	[Franca, Guilherme] Johns Hopkins Univ, Math Inst Data Sci MINDS, Baltimore, MD 21218 USA; [Rizzo, Maria L.] Bowling Green State Univ, Dept Math & Stat, Bowling Green, OH 43403 USA; [Vogelstein, Joshua T.] Johns Hopkins Univ, Ctr Imaging Sci, Dept Biomed Engn, Baltimore, MD 21218 USA; [Vogelstein, Joshua T.] Johns Hopkins Univ, Inst Computat Med, Baltimore, MD 21218 USA	Johns Hopkins University; University System of Ohio; Bowling Green State University; Johns Hopkins University; Johns Hopkins University	Franca, G (corresponding author), Johns Hopkins Univ, Math Inst Data Sci MINDS, Baltimore, MD 21218 USA.	guifranca@jhu.edu; mrizzo@bgsu.edu; jovo@jhu.edu	Rizzo, Maria/HGD-6576-2022	Franca, Guilherme/0000-0003-3424-273X; Reis, AlessanRSS/0000-0001-8486-7469	Transformative Research Award (NIH) [R01NS092474]; Defense Advanced Research Projects Agency's (DARPA) SIMPLEX program through SPAWAR contract [N66001-15-C-4041]; DARPA Lifelong Learning Machines program [FA8650-18-2-7834]	Transformative Research Award (NIH)(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Defense Advanced Research Projects Agency's (DARPA) SIMPLEX program through SPAWAR contract(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DARPA Lifelong Learning Machines program	The authors would like to thank Carey Priebe for discussions. They would like to acknowledge the support of the Transformative Research Award (NIH #R01NS092474) and the Defense Advanced Research Projects Agency's (DARPA) SIMPLEX program through SPAWAR contract N66001-15-C-4041, and also the DARPA Lifelong Learning Machines programthrough contract FA8650-18-2-7834.	Abbe E, 2018, FOUND TRENDS COMMUN, V14, P1, DOI 10.1561/0100000067; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; CHAN PK, 1994, IEEE T COMPUT AID D, V13, P1088, DOI 10.1109/43.310898; Christensen J. P. R., 1984, HARMONIC ANAL SEMIGR, V100; Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI [10.1109/TPAMI.2007.1115, 10.1109/TP'AMI.2007.1115]; Dua D., 2017, UCI MACHINE LEARNING, DOI DOI 10.1002/JCC.23219; Eichler K, 2017, NATURE, V548, P175, DOI 10.1038/nature23455; Filippone M, 2008, PATTERN RECOGN, V41, P176, DOI 10.1016/j.patcog.2007.05.018; FORGY EW, 1965, BIOMETRICS, V21, P768; Fortunato S, 2016, PHYS REP, V659, P1, DOI 10.1016/j.physrep.2016.09.002; Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002; Girolami M, 2002, IEEE T NEURAL NETWOR, V13, P780, DOI 10.1109/TNN.2002.1000150; Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799; Gretton A, 2012, J MACH LEARN RES, V13, P723; Guvenir HA, 1998, ARTIF INTELL MED, V13, P147, DOI 10.1016/S0933-3657(98)00028-1; Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830; Hartigan J.A., 1975, CLUSTERING ALGORITHM; Javanmard A, 2016, P NATL ACAD SCI USA, V113, pE2218, DOI 10.1073/pnas.1523097113; Kernighan B. W., 1970, Bell System Technical Journal, V49, P291; Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110; Kulis B., 2004, P 10 ACM SIGKDD INT, P551, DOI DOI 10.1145/1014052.1014118; Leskovec J, 2007, ACM T WEB, V1, DOI 10.1145/1232722.1232727; Li S., 2017, ARXIV171104359STATME; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Lusseau D, 2003, BEHAV ECOL SOCIOBIOL, V54, P396, DOI 10.1007/s00265-003-0651-y; Lyons R, 2021, ANN PROBAB, V49, P2668, DOI 10.1214/12-AOP803; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; Massoulie L, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P694, DOI 10.1145/2591796.2591857; Mossel E, 2018, COMBINATORICA, V38, P665, DOI 10.1007/s00493-016-3238-8; Ng AY, 2002, ADV NEUR IN, V14, P849; Niu D., 2011, P 14 INT C ART INT S, P552; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Rizzo M, 2018, ENERGY E STAT MULTIV; Rizzo ML, 2010, ANN APPL STAT, V4, P1034, DOI 10.1214/09-AOAS245; SAADE A., 2014, ADV NEURAL INFORM PR, V27, P406; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140; Shen C., 2018, ARXIV180605514STATML; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Slonim N., 2013, 23 INT JOINT C ART I; Song L., 2007, ICML, P815; Szekely GJ, 2007, ANN STAT, V35, P2769, DOI 10.1214/009053607000000505; Szekely GJ, 2017, ANNU REV STAT APPL, V4, P447, DOI 10.1146/annurev-statistics-060116-054026; Szekely GJ, 2013, J STAT PLAN INFER, V143, P1249, DOI 10.1016/j.jspi.2013.03.018; Szekely GJ, 2005, J CLASSIF, V22, P151, DOI 10.1007/s00357-005-0012-9; Telgarsky M., 2010, P 13 INT C ARTIFICIA, P820; Watanabe Y., 2009, ADV NEURAL INF PROCE, P2017; Yu SX, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P313, DOI 10.1109/iccv.2003.1238361; ZACHARY WW, 1977, J ANTHROPOL RES, V33, P452, DOI 10.1086/jar.33.4.3629752	53	1	1	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2021	43	12					4411	4425		10.1109/TPAMI.2020.2998120	http://dx.doi.org/10.1109/TPAMI.2020.2998120			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WR0MQ	32750776	Green Submitted, Green Accepted			2022-12-18	WOS:000714203900019
J	Guo, YW; Chen, L; Chen, YR; Zhang, CS				Guo, Yiwen; Chen, Long; Chen, Yurong; Zhang, Changshui			On Connections Between Regularizations for Improving DNN Robustness	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Robustness; Jacobian matrices; Training; Perturbation methods; Neural networks; Computational modeling; Task analysis; Deep neural networks; adversarial robustness; regularizations; network property		This paper analyzes regularization terms proposed recently for improving the adversarial robustness of deep neural networks (DNNs), from a theoretical point of view. Specifically, we study possible connections between several effective methods, including input-gradient regularization, Jacobian regularization, curvature regularization, and a cross-Lipschitz functional. We investigate them on DNNs with general rectified linear activations, which constitute one of the most prevalent families of models for image classification and a host of other machine learning applications. We shed light on essential ingredients of these regularizations and re-interpret their functionality. Through the lens of our study, more principled and efficient regularizations can possibly be invented in the near future.	[Guo, Yiwen] Bytedance AI Lab, Beijing 100000, Peoples R China; [Chen, Long] Peking Univ, Acad Adv Interdisciplinary Studies, Ctr Data Sci, Beijing 100871, Peoples R China; [Chen, Yurong] Intel Labs, Beijing 100190, Peoples R China; [Zhang, Changshui] Tsinghua Univ, Tsinghua Univ THUAI, Inst Artificial Intelligence,Dept Automat, State Key Lab Intelligent Technol & Syst,Beijing, Beijing 100084, Peoples R China	Peking University; Intel Corporation; Tsinghua University	Guo, YW (corresponding author), Bytedance AI Lab, Beijing 100000, Peoples R China.	guoyiwen.ai@bytedance.com; xidonglc@gmail.com; yurong.chen@intel.com; zcs@mail.tsinghua.edu.cn		Chen, Yurong/0000-0001-9333-1746	Natural Science Foundation of China (NSFC); German Research Foundation [DFG TRR-169]; Beijing Academy of Artificial Intelligence (BAAI)	Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); German Research Foundation(German Research Foundation (DFG)); Beijing Academy of Artificial Intelligence (BAAI)	This work was supported by the Natural Science Foundation of China (NSFC) and the German Research Foundation (DFG TRR-169) in the project of Cross-modal Learning, and the Beijing Academy of Artificial Intelligence (BAAI). Long Chen and Yiwen Guo contributed equally to this paper.	Athalye A, 2018, PR MACH LEARN RES, V80; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chen P.-Y., 2017, P AISEC CCS 2017, P15, DOI DOI 10.1145/3128572.3140448; Chen PY, 2018, AAAI CONF ARTIF INTE, P10; Cisse M, 2017, PR MACH LEARN RES, V70; Drucker H., 1991, INT JOINT C NEUR NET, VII, P145; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Goodfellow I. J., 2014, ARXIV14126572; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hein M, 2017, NIPS 17; Jakubovitz D, 2018, LECT NOTES COMPUT SC, V11216, P525, DOI 10.1007/978-3-030-01258-8_32; KROGH A, 1992, ADV NEUR IN, V4, P950; Kurakin A, 2018, ICLR, P99, DOI DOI 10.1201/9781351251389-8; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lyu CC, 2015, IEEE DATA MINING, P301, DOI 10.1109/ICDM.2015.84; Madry A., 2018, P ICLR VANC BC CAN; Moosavi-Dezfooli SM, 2019, PROC CVPR IEEE, P9070, DOI 10.1109/CVPR.2019.00929; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Nair V., 2010, ICML, P807; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Ros AS, 2018, AAAI CONF ARTIF INTE, P1660; Shang WL, 2016, PR MACH LEARN RES, V48; Simon-Gabriel CJ, 2019, PR MACH LEARN RES, V97; Sokolic J, 2017, IEEE T SIGNAL PROCES, V65, P4265, DOI 10.1109/TSP.2017.2708039; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C, 2014, INTRIGUING PROPERTIE, P6; Tramonti F, 2019, PSYCHOL HEALTH MED, V24, P27, DOI 10.1080/13548506.2018.1510131; Tsipras Dimitris, 2019, ROBUSTNESS MAY BE OD, V1, P2	31	1	1	1	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2021	43	12					4469	4476		10.1109/TPAMI.2020.3006917	http://dx.doi.org/10.1109/TPAMI.2020.3006917			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WR0MQ	32750801	Green Submitted			2022-12-18	WOS:000714203900023
J	Zhu, QS; Han, C; Han, GQ; Wong, TT; He, SF				Zhu, Qianshu; Han, Chu; Han, Guoqiang; Wong, Tien-Tsin; He, Shengfeng			Video Snapshot: Single Image Motion Expansion via Invertible Motion Embedding	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Streaming media; Interpolation; Decoding; Image restoration; Image coding; Dynamics; Animation; Video snapshot; video expansion; information embedding; motion attention		Unlike images, finding the desired video content in a large pool of videos is not easy due to the time cost of loading and watching. Most video streaming and sharing services provide the video preview function for a better browsing experience. In this paper, we aim to generate a video preview from a single image. To this end, we propose two cascaded networks, the motion embedding network and the motion expansion network. The motion embedding network aims to embed the spatio-temporal information into an embedded image, called video snapshot. On the other end, the motion expansion network is proposed to invert the video back from the input video snapshot. To hold the invertibility of motion embedding and expansion during training, we design four tailor-made losses and a motion attention module to make the network focus on the temporal information. In order to enhance the viewing experience, our expansion network involves an interpolation module to produce a longer video preview with a smooth transition. Extensive experiments demonstrate that our method can successfully embed the spatio-temporal information of a video into one "live" image, which can be converted back to a video preview. Quantitative and qualitative evaluations are conducted on a large number of videos to prove the effectiveness of our proposed method. In particular, statistics of PSNR and SSIM on a large number of videos show the proposed method is general, and it can generate a high-quality video from a single image.	[Zhu, Qianshu; Han, Guoqiang; He, Shengfeng] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Peoples R China; [Han, Chu] Guangdong Acad Med Sci, Guangdong Prov Peoples Hosp, Guangzhou, Peoples R China; [Wong, Tien-Tsin] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China	South China University of Technology; Guangdong Academy of Medical Sciences & Guangdong General Hospital; Chinese University of Hong Kong	He, SF (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Peoples R China.	chuchienshu@gmail.com; zq1992@gmail.com; csgqhan@scut.edu.cn; ttwong@cse.cuhk.edu.hk; hesfe@scut.edu.cn	Han, Chu/GWM-9255-2022; He, Shengfeng/E-5682-2016	Han, Chu/0000-0001-7557-9131; He, Shengfeng/0000-0002-3802-4644	National Natural Science Foundation of China [61472145, 61972162, 61702194]; Special Fund of Science and Technology Research and Development of Applications From Guangdong Province (SF-STRDA-GD) [2016B010127003]; Guangzhou Key Industrial Technology Research fund [201802010036]; Guangdong Natural Science Foundation [2017A030312008]; CCF-Tencent Open Research fund [RAGR20190112]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Special Fund of Science and Technology Research and Development of Applications From Guangdong Province (SF-STRDA-GD); Guangzhou Key Industrial Technology Research fund; Guangdong Natural Science Foundation(National Natural Science Foundation of Guangdong Province); CCF-Tencent Open Research fund	This work was supported by the National Natural Science Foundation of China (No. 61472145, No. 61972162, and No. 61702194), the Special Fund of Science and Technology Research and Development of Applications From Guangdong Province (SF-STRDA-GD) (No. 2016B010127003), the Guangzhou Key Industrial Technology Research fund (No. 201802010036), the Guangdong Natural Science Foundation (No. 2017A030312008), and the CCF-Tencent Open Research fund (CCF-Tencent RAGR20190112). Qianshu Zhu and Chu Han contributed equally to this work.	Averbuch-Elor H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130818; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Bojanowski P, 2018, PR MACH LEARN RES, V80; Chen ZB, 2020, IEEE T CIRC SYST VID, V30, P566, DOI 10.1109/TCSVT.2019.2892608; Chuang YY, 2005, ACM T GRAPHIC, V24, P853, DOI 10.1145/1073204.1073273; Denton E, 2018, PR MACH LEARN RES, V80; Dosovitskiy A., 2017, P INT C LEARN REPR; Dvoroznak M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201326; Geng JH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275043; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hornung A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1186644.1186645; Horry Y., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P225, DOI 10.1145/258734.258854; Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Joshi N, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P251; Li Y, 2002, ACM T GRAPHIC, V21, P465; Li Y., 2001, HPL2001191; Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI 10.1109/ICCV.2017.478; Lotter W., 2017, ICLR, DOI [DOI 10.48550/ARXIV.1605.08104, 10.48550/arXiv.1605.08104]; Lu G, 2019, PROC CVPR IEEE, P10998, DOI 10.1109/CVPR.2019.01126; Ma CY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618456; Mahdisoltani F., 2018, FINE GRAINED VIDEO C; Mathieu M., 2016, INT C LEARN REPR ICL; Mercat A, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P297, DOI 10.1145/3339825.3394937; Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183; Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37; Pont-Tuset J., 2017, ARXIV170400675; Potapov D, 2014, LECT NOTES COMPUT SC, V8694, P540, DOI 10.1007/978-3-319-10599-4_35; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ruder S., 2016, ARXIV160904747; Simonyan K., 2015, P INT C LEARN REPR, P1, DOI DOI 10.48550/ARXIV.1409.1556; Soomro K., 2012, ARXIV; Su QK, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174236; Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33; Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Sun M, 2014, LECT NOTES COMPUT SC, V8689, P787, DOI 10.1007/978-3-319-10590-1_51; Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305; VANSCHYNDELL RG, 1994, IEEE IMAGE PROC, P86, DOI 10.1109/ICIP.1994.413536; Wang Yunbo, 2019, ICLR; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Weng CY, 2019, PROC CVPR IEEE, P5901, DOI 10.1109/CVPR.2019.00606; Wengrowski E, 2019, PROC CVPR IEEE, P1515, DOI 10.1109/CVPR.2019.00161; Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165; Wolfgang RB, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P219, DOI 10.1109/ICIP.1996.560423; Wu CY, 2018, LECT NOTES COMPUT SC, V11212, P425, DOI 10.1007/978-3-030-01237-3_26; Xu XM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409070; Yang H, 2015, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2015.526; Yang H, 2019, IEEE I CONF COMP VIS, P1100, DOI 10.1109/ICCV.2019.00119; Zhang K, 2016, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2016.120; Zhou YP, 2018, IEEE WINT CONF APPL, P170, DOI 10.1109/WACV.2018.00025	53	1	1	1	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2021	43	12					4491	4504		10.1109/TPAMI.2020.3001644	http://dx.doi.org/10.1109/TPAMI.2020.3001644			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WR0MQ	32750783				2022-12-18	WOS:000714203900025
J	Cancela, B; Alonso-Betanzos, A				Cancela, Brais; Alonso-Betanzos, Amparo			Wavefront Marching Methods: A Unified Algorithm to Solve Eikonal and Static Hamilton-Jacobi Equations	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Mathematical model; Complexity theory; Two dimensional displays; Three-dimensional displays; Computational efficiency; Computer vision; Computational modeling; Fast marching; eikonal equation; static hamilton-jacobi; isotropic; anisotropic	LEVEL SET METHOD	This paper presents a unified propagation method for dealing with both the classic Eikonal equation, where the motion direction does not affect the propagation, and the more general static Hamilton-Jacobi equations, where it does. While classic Fast Marching Method (FMM) techniques achieve the solution to the Eikonal equation with a O(M log M) (or O(M) assuming some modifications), solving the more general static Hamilton-Jacobi equation requires a higher complexity. The proposed framework maintains the O(M log M) complexity for both problems, while achieving higher accuracy than available state-of-the-art. The key idea behind the proposed method is the creation of 'mini wave-fronts', where the solution is interpolated to minimize the discretization error. Experimental results show how our algorithm can outperform the state-of-the-art both in precision and computational cost.	[Cancela, Brais; Alonso-Betanzos, Amparo] Univ A Coruna, CITIC Res Ctr, Coruna 15008, Spain	Universidade da Coruna	Cancela, B (corresponding author), Univ A Coruna, CITIC Res Ctr, Coruna 15008, Spain.	brais.cancela@udc.es; ciamparo@udc.es	; Alonso-Betanzos, Amparo/K-5057-2014	Cancela, Brais/0000-0002-2295-4142; Alonso-Betanzos, Amparo/0000-0003-0950-0012	Spanish Ministerio de Economia y Competitividad [TIN2015-65069-C2-1-R]; Xunta de Galicia [ED431C 2018/34]; European Union (European Regional Development Fund); Centro Singular de Investigacion de Galicia, accreditation 2016-2019)	Spanish Ministerio de Economia y Competitividad(Spanish Government); Xunta de Galicia(Xunta de GaliciaEuropean Commission); European Union (European Regional Development Fund)(European Commission); Centro Singular de Investigacion de Galicia, accreditation 2016-2019)	The authors would like to thank to the financial support of the Spanish Ministerio de Economia y Competitividad (research project TIN2015-65069-C2-1-R), the Xunta de Galicia (research projects ED431C 2018/34 and Centro Singular de Investigaci ~on de Galicia, accreditation 2016-2019) and by the European Union (European Regional Development Fund). Brais Cancela acknowledges the support of the Xunta de Galicia under its postdoctoral program.	Ahmed S, 2011, SIAM J SCI COMPUT, V33, P2402, DOI 10.1137/10080258X; Benmansour F, 2011, INT J COMPUT VISION, V92, P192, DOI 10.1007/s11263-010-0331-0; BORGEFORS G, 1984, COMPUT VISION GRAPH, V27, P321, DOI 10.1016/0734-189X(84)90035-5; Bornemann F, 2006, COMPUT VIS SCI, V9, P57, DOI 10.1007/s00791-006-0016-y; Cancela B., 2015, IEEE I CONF COMP VIS, P1832, DOI DOI 10.1109/ICCV.2015.213; Chopp DL, 2001, SIAM J SCI COMPUT, V23, P230, DOI 10.1137/S106482750037617X; Cohen LD, 1997, INT J COMPUT VISION, V24, P57, DOI 10.1023/A:1007922224810; Covello P, 2003, J COMPUT APPL MATH, V156, P371, DOI 10.1016/S0377-0427(03)00360-1; Danielsson PE, 2003, LECT NOTES COMPUT SC, V2749, P1154; echaud M. P~, 2009, PROC CVPR IEEE, P336; Hassouna MS, 2007, IEEE T PATTERN ANAL, V29, P1563, DOI 10.1109/TPAMI.2007.1154; Jbabdi S, 2008, INT J BIOMED IMAGING, V2008, DOI 10.1155/2008/320195; KIEFER J, 1953, P AM MATH SOC, V4, P502, DOI 10.2307/2032161; Kim S, 2001, SIAM J SCI COMPUT, V22, P2178, DOI 10.1137/S1064827500367130; Konukoglu E, 2010, IEEE T MED IMAGING, V29, P77, DOI 10.1109/TMI.2009.2026413; Lin Q, 2003, THESIS LINKOPING U; Melonakos J, 2008, IEEE T PATTERN ANAL, V30, P412, DOI 10.1109/TPAMI.2007.70713; Merino-Caviedes S, 2019, IEEE T IMAGE PROCESS, V28, P1967, DOI 10.1109/TIP.2018.2880507; Merlet N, 1996, IEEE T PATTERN ANAL, V18, P426, DOI 10.1109/34.491623; Mirebeau JM, 2018, J MATH IMAGING VIS, V60, P784, DOI 10.1007/s10851-017-0778-5; Mirebeau JM, 2014, SIAM J NUMER ANAL, V52, P1573, DOI 10.1137/120861667; Mirebeau JM, 2014, NUMER MATH, V126, P515, DOI 10.1007/s00211-013-0571-3; MIREBEAU JM, 2017, ANISOTROPIC FAST MAR; Sethian J. A., 1999, LEVEL SET METHODS FA; Sethian JA, 1996, P NATL ACAD SCI USA, V93, P1591, DOI 10.1073/pnas.93.4.1591; Sethian JA, 2003, SIAM J NUMER ANAL, V41, P325, DOI 10.1137/S0036142901392742; Stocker C., 2005, WSEAS Transactions on Circuits and Systems, V4, P111; Yatziv L, 2006, J COMPUT PHYS, V212, P393, DOI 10.1016/j.jcp.2005.08.005; Zhang YT, 2006, J SCI COMPUT, V29, P25, DOI 10.1007/s10915-005-9014-3; Zhao HK, 2005, MATH COMPUT, V74, P603	30	1	1	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2021	43	11					4177	4188		10.1109/TPAMI.2020.2993500	http://dx.doi.org/10.1109/TPAMI.2020.2993500			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	WA1JH	32396072				2022-12-18	WOS:000702649700033
J	Wang, HM; Qiao, H; Lin, JY; Wu, RH; Liu, YB; Dai, QH				Wang, Hongman; Qiao, Hui; Lin, Jingyu; Wu, Rihui; Liu, Yebin; Dai, Qionghai			Model Study of Transient Imaging With Multi-Frequency Time-of-Flight Sensors	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Transient analysis; Sensors; Image reconstruction; Image sensors; Image coding; Frequency measurement; Transient imaging; multi-frequency time-of-flight sensor; tight frame; compressive sampling; wavelet decomposition and reconstruction	RESOLVING MULTIPATH INTERFERENCE; SIGNAL RECOVERY; PHOTOGRAPHY; FRAMES; LIGHT	As an emerging imaging modality, transient imaging that records the transient information of light transport has significantly shaped our understanding of scenes. In spite of the great progress made in computer vision and optical imaging fields, commonly used multi-frequency time-of-flight (ToF) sensors are still afflicted with the band-limited modulation frequency and long acquisition process. To overcome such barriers, more effective image-formation schemes and reconstruction algorithms are highly desired. In this paper, we propose a compressive transient imaging model, without any priori knowledge, by constructing a near-tight-frame based representation of the ToF imaging principle. We prove that the compressibility of sensor measurements can be presented in the Fourier domain and held in the frame, and the ToF measurements possess multi-scale characteristics. Solving the inverse problems in transient imaging with our proposed model consists of two major steps, including a compressed-sensing-based approach for full measurement recovery, which essentially reduces the capture time, and a wavelet-based transient image reconstruction framework, which realizes adaptive transient image reconstruction and achieves highly accurate reconstruction results. The compressive transient imaging model is suitable for various existing multi-frequency ToF sensors and requires no hardware modifications. Experimental results using synthetic and real online datasets demonstrate its promising performance.	[Wang, Hongman] Liaoning Normal Univ, Sch Comp & Informat Technol, Dalian 116029, Peoples R China; [Qiao, Hui; Liu, Yebin; Dai, Qionghai] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, Inst Brain & Cognit Sci, Dept Automat, Beijing 100084, Peoples R China; [Lin, Jingyu] Guangxi Univ, Sch Elect Engn, Nanning 530004, Peoples R China; [Wu, Rihui] Univ Chinese Acad Sci, Inst Comp Technol, Beijing 100084, Peoples R China	Liaoning Normal University; Tsinghua University; Guangxi University; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Dai, QH (corresponding author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, Inst Brain & Cognit Sci, Dept Automat, Beijing 100084, Peoples R China.	wanghmcn@163.com; qiaohui@mail.tsinghua.edu.cn; jylin@gxu.edu.cn; wurihui@ict.ac.cn; liuyebin@mail.tsinghua.edu.cn; qhdai@tsinghua.edu.cn	Dai, Qionghai/ABD-5298-2021	Dai, Qionghai/0000-0001-7043-3061; Lin, Jingyu/0000-0002-7194-3300	National Natural Science Foundation of China [61827805]; Project of Beijing Municipal Science and Technology Commission [Z181100003118014]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Project of Beijing Municipal Science and Technology Commission	This work was supported by the National Natural Science Foundation of China (No. 61827805) and the Project of Beijing Municipal Science and Technology Commission (No. Z181100003118014). H. Wang and H. Qiao contributed equally to this work.	Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x; Bawendi M, 2011, IMAGING SYSTEMS APPL, P366; Bhandari A, 2014, IEEE SENSOR; Bhandari A, 2016, IEEE SIGNAL PROC MAG, V33, P45, DOI 10.1109/MSP.2016.2582218; Bhandari A, 2014, OPT LETT, V39, P1705, DOI 10.1364/OL.39.001705; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; Casazza P. G., 2013, FINITE FRAMES THEORY; Casazza PG, 2003, ADV COMPUT MATH, V18, P387, DOI 10.1023/A:1021349819855; Cossairt O., SER EVENT SPIE COMME; Feigin M, 2016, IEEE SENS J, V16, P3419, DOI 10.1109/JSEN.2015.2421360; Gao L, 2014, NATURE, V516, P74, DOI 10.1038/nature14005; Gariepy G, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7021; Gu J, 2018, VERY POWER EFFICIENT; Gupta M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3152155; Gupta M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2735702; Heide F, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3269977; Heide F, 2014, OPT EXPRESS, V22, P26338, DOI 10.1364/OE.22.026338; Heide F, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461945; Jarabo A, 2017, VIS INFORM, V1, P65, DOI 10.1016/j.visinf.2017.01.008; Jarabo A, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661251; Kadambi A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508428; Kirmani A, 2010, THESIS EDIA ARTS SC; Kirmani A, 2013, IEEE INT CON MULTI; Kirmani A, 2014, SCIENCE, V343, P58, DOI 10.1126/science.1246775; Kirmani A, 2011, INT J COMPUT VISION, V95, P13, DOI 10.1007/s11263-011-0470-y; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lei C, 2016, APPL PHYS REV, V3, DOI 10.1063/1.4941050; Li FQ, 2017, OPT EXPRESS, V25, P31096, DOI 10.1364/OE.25.031096; Lin JY, 2017, FRONT INFORM TECH EL, V18, P1268, DOI 10.1631/FITEE.1700556; Lin JY, 2017, IEEE T PATTERN ANAL, V39, P937, DOI 10.1109/TPAMI.2016.2560814; Lin JY, 2014, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2014.419; Lindlbauer D, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173703; Luan X, 2001, EXPT INVESTIGATION P; Marco J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130884; Naik N, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024205; Nakagawa K, 2014, NAT PHOTONICS, V8, P695, DOI [10.1038/NPHOTON.2014.163, 10.1038/nphoton.2014.163]; O'Toole M, 2017, PROC CVPR IEEE, P2289, DOI 10.1109/CVPR.2017.246; O'Toole M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601103; Peters C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818103; Qiao H, 2015, OPT LETT, V40, P918, DOI 10.1364/OL.40.000918; Ramesh R., 2008, 5D TIME LIGHT TRANSP; Raskar R, 2015, FREQUENCY DOMAIN TOF; Raskar R., 2014, CLEO 2014 OSA OPT SO, P1; Satat G, 2016, SCI REP-UK, V6, DOI 10.1038/srep33946; Shrestha S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925928; Su SC, 2018, PROC CVPR IEEE, P6383, DOI 10.1109/CVPR.2018.00668; Sun Q, 2017, COMPRESSIVE TRANSIEN; Sun QL, 2018, PROC CVPR IEEE, P273, DOI 10.1109/CVPR.2018.00036; Tadano R, 2016, IEEE IMAGE PROC, P1564, DOI 10.1109/ICIP.2016.7532621; Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108; Velten A, 2016, COMMUN ACM, V59, P79, DOI 10.1145/2975165; Velten A, 2012, SIGGRAPH '12: SPECIAL INTEREST GROUP ON COMPUTER GRAPHICS AND INTERACTIVE TECHNIQUES CONFERENCE, DOI 10.1145/2343045.2343100; Velten A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461928; Velten A, 2012, NAT COMMUN, V3, DOI 10.1038/ncomms1747; Wikipedia, 2017, DFTM INT DOC; Wikipedia, 2018, MOOR PENS INV; Wu D, 2014, INT J COMPUT VISION, V107, P123, DOI 10.1007/s11263-013-0668-2; Wu D, 2012, LECT NOTES COMPUT SC, V7572, P542, DOI 10.1007/978-3-642-33718-5_39; Wu RH, 2016, OPT LETT, V41, P3948, DOI 10.1364/OL.41.003948	60	1	2	2	23	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2021	43	10					3523	3539		10.1109/TPAMI.2020.2981574	http://dx.doi.org/10.1109/TPAMI.2020.2981574			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UK8RG	32191880				2022-12-18	WOS:000692232400020
J	Liu, ZY; Pavao, A; Xu, Z; Escalera, S; Ferreira, F; Guyon, I; Hong, SR; Hutter, F; Ji, RR; Jacques, JCS; Li, G; Lindauer, M; Luo, ZP; Madadi, M; Nierhoff, T; Niu, KN; Pan, CG; Stoll, D; Treguer, S; Wang, J; Wang, P; Wu, CL; Xiong, YC; Zela, A; Zhang, Y				Liu, Zhengying; Pavao, Adrien; Xu, Zhen; Escalera, Sergio; Ferreira, Fabio; Guyon, Isabelle; Hong, Sirui; Hutter, Frank; Ji, Rongrong; Jacques, Julio C. S., Jr.; Li, Ge; Lindauer, Marius; Luo, Zhipeng; Madadi, Meysam; Nierhoff, Thomas; Niu, Kangning; Pan, Chunguang; Stoll, Danny; Treguer, Sebastien; Wang, Jin; Wang, Peng; Wu, Chenglin; Xiong, Youcheng; Zela, Arber; Zhang, Yang			Winning Solutions and Post-Challenge Analyses of the ChaLearn AutoDL Challenge 2019	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Deep learning; Task analysis; Videos; Tensors; Computer architecture; Benchmark testing; Internet; AutoML; deep learning; meta-learning; neural architecture search; model selection; hyperparameter optimization		This paper reports the results and post-challenge analyses of ChaLearn's AutoDL challenge series, which helped sorting out a profusion of AutoML solutions for Deep Learning (DL) that had been introduced in a variety of settings, but lacked fair comparisons. All input data modalities (time series, images, videos, text, tabular) were formatted as tensors and all tasks were multi-label classification problems. Code submissions were executed on hidden tasks, with limited time and computational resources, pushing solutions that get results quickly. In this setting, DL methods dominated, though popular Neural Architecture Search (NAS) was impractical. Solutions relied on fine-tuned pre-trained networks, with architectures matching data modality. Post-challenge tests did not reveal improvements beyond the imposed time limit. While no component is particularly original or novel, a high level modular organization emerged featuring a "meta-learner", "data ingestor", "model selector", "model/learner", and "evaluator". This modularity enabled ablation studies, which revealed the importance of (off-platform) meta-learning, ensembling, and efficient data management. Experiments on heterogeneous module combinations further confirm the (local) optimality of the winning solutions. Our challenge legacy includes an ever-lasting benchmark (http://autodl.chalearn.org), the open-sourced code of the winners, and a free "AutoDL self-service."	[Liu, Zhengying; Pavao, Adrien; Guyon, Isabelle] Univ Paris Saclay, F-91190 Gif Sur Yvette, France; [Xu, Zhen] 4Paradigm, AutoML, Beijing 100125, Peoples R China; [Escalera, Sergio] Univ Barcelona, Barcelona 08007, Spain; [Escalera, Sergio] Comp Vis Ctr, Barcelona 08007, Spain; [Ferreira, Fabio; Hutter, Frank; Nierhoff, Thomas; Stoll, Danny; Zela, Arber] Univ Freiburg, Dept Comp Sci, D-79085 Freiburg, Germany; [Hong, Sirui; Wu, Chenglin; Xiong, Youcheng; Zhang, Yang] DeepWisdom Inc, Xiamen 361001, Fujian, Peoples R China; [Ji, Rongrong] Xiamen Univ, Cognit Sci, Xiamen 361005, Fujian, Peoples R China; [Jacques, Julio C. S., Jr.] Univ Oberta Catalunya, Comp Sci Multimedia & Telecommun, Barcelona 08035, Spain; [Li, Ge; Luo, Zhipeng; Niu, Kangning; Pan, Chunguang; Wang, Jin] DeepBlue Technol, Beijing 200336, Peoples R China; [Lindauer, Marius] Leibniz Univ Hannover, Elect Engn & Comp Sci, D-30167 Hannover, Germany; [Madadi, Meysam] Univ Autonoma Barcelona, Comp Vis Ctr, Barcelona 08193, Spain; [Treguer, Sebastien] La Paillasse, AI, F-75002 Paris, France; [Wang, Peng] Lenovo AI Lab, AI, Beijing 100085, Peoples R China	UDICE-French Research Universities; Universite Paris Saclay; University of Barcelona; Centre de Visio per Computador (CVC); University of Freiburg; Xiamen University; UOC Universitat Oberta de Catalunya; Leibniz University Hannover; Autonomous University of Barcelona; Centre de Visio per Computador (CVC)	Liu, ZY (corresponding author), Univ Paris Saclay, F-91190 Gif Sur Yvette, France.	zhengying.liu@inria.fr; adrien.pavao@gmail.com; zhen.xu.15@polytechnique.org; sergio@maia.ub.es; ferreira@cs.uni-freiburg.de; guyon@clopinet.com; stellahong@fuzhi.ai; fh@cs.uni-freiburg.de; rrji@xmu.edu.cn; jsilveira@uoc.edu; lige@deepblueai.com; lindauer@tnt.uni-hannover.de; luozp@deepblueai.com; madadi@gmail.com; nierhoff@cs.uni-freiburg.de; niukn@deepblueai.com; panchg@deepblueai.com; stolld@cs.uni-freiburg.de; streguer@gmail.com; wangjin@deepblueai.com; wangpeng31@lenovo.com; alexanderwu@fuzhi.ai; ycxiong@fuzhi.ai; zelaa@cs.uni-freiburg.de; youngzhang@fuzhi.ai	Escalera, Sergio/L-2998-2015	Escalera, Sergio/0000-0003-0617-8873; Xu, Zhen/0000-0002-8901-1819; Hutter, Frank/0000-0002-2037-3694; Liu, Zhengying/0000-0001-6385-6082; Silveira Jacques Junior, Julio Cezar/0000-0001-6785-7146	Google Research (Zurich); 4Paradigm; Amazon; Microsoft; ICREA through the ICREA Academia Programme; European Research Council (ERC) through the European Union'sHorizon 2020 Research and Innovation Programme [716721]; Robert Bosch GmbH; Spanish project [PID2019-105093GB-I00]	Google Research (Zurich)(Google Incorporated); 4Paradigm; Amazon; Microsoft(Microsoft); ICREA through the ICREA Academia Programme; European Research Council (ERC) through the European Union'sHorizon 2020 Research and Innovation Programme(European Research Council (ERC)); Robert Bosch GmbH; Spanish project(Spanish Government)	The authors would like to thank NVIDIA Corporation for the donation of the GPU used for this research. This work was supported by in part by the Google Research (Zurich), in part by the 4Paradigm, in part by the Amazon, in part by the Microsoft, in part by the ICREA through the ICREA Academia Programme. The work of team automlfreiburg was supported in part by the European Research Council (ERC) through the European Union'sHorizon 2020 Research and Innovation Programme under Grant 716721, in part by the Robert Bosch GmbH, in part by the institutions of the co-authors, and in part by the Spanish project PID2019-105093GB-I00. The authors would also like to thank Olivier Bousquet and Andre Elisseeff at Google for their help with the design of the challenge and the countless hours that Andre spent engineering the data format. The special version of the CodaLab platform we used was implemented by Tyler Thomas, with the help of Eric Carmichael, CK Collab, LLC, USA. Many people contributed time to help formatting datasets, prepare baseline results, and facilitate the logistics. The authors would also like to thank Stephane Ayache (AMU, France), Hubert Jacob Banville (INRIA, France), Mahsa Behzadi (Google, Switzerland), Kristin Bennett (RPI, New York, USA), Hugo Jair Escalante (IANOE, Mexico and ChaLearn, USA), Gavin Cawley (U. East Anglia, UK), BaiyuChen (UCBerkeley, USA), Albert Clapes i Sintes (U. Barcelona, Spain), Bram van Ginneken (Radboud U. Nijmegen, The Netherlands), Alexandre Gramfort (U. Paris-Saclay; INRIA, France), Yi-QiHu (4paradigm, China), Tatiana Merkulova (Google, Switzerland), Shangeth Rajaa (BITS Pilani, India), Herilalaina Rakotoarison (U. Paris-Saclay, INRIA, France), Lukasz Romaszko (The University of Edinburgh, UK), Mehreen Saeed (FAST Nat. U. Lahore, Pakistan), Marc Schoenauer (U. Paris-Saclay, INRIA, France), Michele Sebag (U. Paris-Saclay; CNRS, France), Danny Silver (Acadia University, Canada), Lisheng Sun (U. Paris-Saclay; UPSud, France), Wei-Wei Tu (4paradigm, China), Fengfu Li (4paradigm, China), Lichuan Xiang (4paradigm, China), Jun Wan (Chinese Academy of Sciences, China), Mengshuo Wang (4paradigm, China), JingsongWang (4paradigm, China), and Ju Xu (4paradigm, China). Zhengying Liu, Adrien Pavao, and Zhen Xu contributed equally to this work.	[Anonymous], 2020, SPEARMANS RANK CORRE; Baker Bowen, 2017, ICLR; Bergstra J., 2013, JMLR WORKSHOP C P IC, V28, P115, DOI [10.5555/3042817.3042832, DOI 10.5555/3042817.3042832]; Bergstra James S, 2011, ADV NEURAL INFORM PR, P2546, DOI [10.5555/2986459.2986743, DOI 10.5555/2986459.2986743]; Brain K., 2019, AUTOCLINT AUTOMATIC; Brazdil PB, 2000, LECT NOTES ARTIF INT, V1810, P63; Bridle J. S., 1974, 1003 JOINT SPEECH RE; Cai Han, 2019, INT C LEARN REPR; Caruana Rich, 2004, ICML, DOI DOI 10.1145/1015330.1015432; Chen X., 2019, IEEE DATA MINING, P71, DOI DOI 10.1109/ICDM.2019.00017; Cho K., 2014, P 2014 C EMP METH NA, P1724; Chung JS, 2018, INTERSPEECH, P1086; Cortes C, 2017, PR MACH LEARN RES, V70; Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020; DAVIS SB, 1980, IEEE T ACOUST SPEECH, V28, P357, DOI 10.1109/TASSP.1980.1163420; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Elsken T, 2019, J MACH LEARN RES, V20; Falkner Stefan, 2018, P MACHINE LEARNING R, V80, P1437; Feurer Matthias, 2015, ADV NEURAL INFORM PR, P2962; Finn C, 2019, PR MACH LEARN RES, V97; Finn C, 2017, PR MACH LEARN RES, V70; Fusi N., 2018, ADV NEURAL INFORM PR, P3352, DOI 10.5555/3327144.3327254; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Guan Melody, 2018, ICML, P4095; Guyon I, 1998, IEEE T PATTERN ANAL, V20, P52, DOI 10.1109/34.655649; Guyon I., 2018, SPRINGER SERIES CHAL, P177; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Helber P, 2019, IEEE J-STARS, V12, P2217, DOI 10.1109/JSTARS.2019.2918242; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Joulin A., 2017, P 15 C EUR CHAPT ASS, P427, DOI DOI 10.18653/V1/E17-2068; Kather JN, 2016, SCI REP-UK, V6, DOI 10.1038/srep27988; Ke G., 2017, P ADV NEURAL INFORM, V30, P3146; Kim Y., 2014, P 2014 C EMP METH NA; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; LeCun Y., 2010, ATT LABS, V2; Lim S, 2019, ADV NEUR IN, V32; Lindauer M., 2018, P AUTOML WORKSH; Lindauer M, 2015, J ARTIF INTELL RES, V53, P745, DOI 10.1613/jair.4726; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Liu Z., 2020, NEURIPS 2019 COMPETI, P242; Liu ZY, 2020, PATTERN RECOGN LETT, V135, P196, DOI 10.1016/j.patrec.2020.04.030; Loshchilov I., 2017, P INT C LEARNING REP; Loshkarev IY, 2019, J PHYS CONF SER, V1333, DOI 10.1088/1742-6596/1333/4/042019; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Negrinho R., 2017, ARXIV170408792; NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543; Real E, 2017, PR MACH LEARN RES, V70; Real Esteban, 2020, ARXIV200303384, P8007; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Tan MX, 2019, PR MACH LEARN RES, V97; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Tschandl P, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.161; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Welinder P., 2010, CNSTR2010001 CALTECH; Wolpert D. H., 1997, IEEE Transactions on Evolutionary Computation, V1, P67, DOI 10.1109/4235.585893; Wolpert DH, 2002, SOFT COMPUTING AND INDUSTRY, P25; Xie WD, 2019, INT CONF ACOUST SPEE, P5791, DOI 10.1109/ICASSP.2019.8683120; Xu L., 2011, RCRA WORKSHOP EXPT E, P16; Xu L, 2010, AAAI CONF ARTIF INTE, P210; Xu LL, 2012, PROCEEDINGS OF THE 8TH EURO-ASIA CONFERENCE ON ENVIRONMENT AND CSR: TOURISM, MICE, HOSPITALITY MANAGEMENT AND EDUCATION SESSION (PT III), P55; Yang A., 2020, PROC INT C LEARN REP; YOUNG HP, 1975, SIAM J APPL MATH, V28, P824, DOI 10.1137/0128067; Zoph B, ARXIV161101578	68	1	1	0	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2021	43	9					3108	3125		10.1109/TPAMI.2021.3075372	http://dx.doi.org/10.1109/TPAMI.2021.3075372			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TU6DH	33891549	Green Submitted, Green Accepted			2022-12-18	WOS:000681124300020
J	Garner, PN; Tong, SB				Garner, Philip N.; Tong, Sibo			A Bayesian Approach to Recurrence in Neural Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Logic gates; Probabilistic logic; Bayes methods; Signal processing algorithms; Hidden Markov models; Training; Computer architecture; Neural networks; bayesian statistics; recurrence; forward-backward algorithm; speech recognition; bidirectional LSTM	SPEECH RECOGNITION; CLASSIFICATION; FRAMEWORK; MODEL	We begin by reiterating that common neural network activation functions have simple Bayesian origins. In this spirit, we go on to show that Bayes's theorem also implies a simple recurrence relation; this leads to a Bayesian recurrent unit with a prescribed feedback formulation. We show that introduction of a context indicator leads to a variable feedback that is similar to the forget mechanism in conventional recurrent units. A similar approach leads to a probabilistic input gate. The Bayesian formulation leads naturally to the two pass algorithm of the Kalman smoother or forward-backward algorithm, meaning that inference naturally depends upon future inputs as well as past ones. Experiments on speech recognition confirm that the resulting architecture can perform as well as a bidirectional recurrent network with the same number of parameters as a unidirectional one. Further, when configured explicitly bidirectionally, the architecture can exceed the performance of a conventional bidirectional recurrence.	[Garner, Philip N.; Tong, Sibo] Idiap Res Inst, CH-1920 Martigny, Switzerland		Garner, PN (corresponding author), Idiap Res Inst, CH-1920 Martigny, Switzerland.	pgarner@idiap.ch; stong@idiap.ch			European Union's Horizon 2020 research and innovation programme [688139]	European Union's Horizon 2020 research and innovation programme	We are grateful to our colleagues Apoorv Vyas and Bastian Schnell for battling through an early manuscript and providing comments that rendered it more accessible. We also extend our thanks to the editor and two referees for their insights. Part of this work was conducted within the scope of the Research and Innovation Action SUMMA, which has received funding from the European Union's Horizon 2020 research and innovation programme under Grant No. 688139.	[Anonymous], 1989, NIPS 1989; BAHL LR, 1983, IEEE T PATTERN ANAL, V5, P179, DOI 10.1109/TPAMI.1983.4767370; BAUM LE, 1966, ANN MATH STAT, V37, P1554, DOI 10.1214/aoms/1177699147; Bishop, 1995, NEURAL NETWORKS PATT; Bridle J. S., 1990, Neurocomputing, Algorithms, Architectures and Applications. Proceedings of the NATO Advanced Research Workshop, P227; BRIDLE JS, 1990, SPEECH COMMUN, V9, P83, DOI 10.1016/0167-6393(90)90049-F; Carletta J, 2005, LECT NOTES COMPUT SC, V3869, P28; Cho K., 2014, P 2014 C EMP METH NA, P1724; David E., 1986, PARALLEL DISTRIBUTED, P318, DOI DOI 10.5555/104279.104293; Dighe P, 2018, IEEE W SP LANG TECH, P581, DOI 10.1109/SLT.2018.8639579; Dugas C, 2001, ADV NEUR IN, V13, P472; Garofolo J. S., 1993, 4930 NISTIR; Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015; Gers FA, 2003, J MACH LEARN RES, V3, P115, DOI 10.1162/153244303768966139; Glorot X, 2011, P 14 INT C ART INT S, V15, P315, DOI DOI 10.1177/1753193410395357; Graves A, 2005, IEEE IJCNN, P2047; Graves A., 2006, P INT C MACH LEARN I; Hadian H, 2018, INTERSPEECH, P12; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Honnet PE, 2018, SPEECH COMMUN, V97, P81, DOI 10.1016/j.specom.2017.10.004; Ioffe S., 2015, P ICML; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Kanda N, 2018, INTERSPEECH, P2923; Kingma D.P, P 3 INT C LEARNING R; MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Povey D., 2011, IEEE AUT SPEECH REC; Ravanelli M, 2017, INTERSPEECH, P1308, DOI 10.21437/Interspeech.2017-775; Ravanelli M, 2019, INT CONF ACOUST SPEE, P6465, DOI 10.1109/ICASSP.2019.8683713; Ravanelli M, 2018, IEEE TETCI, V2, P92, DOI 10.1109/TETCI.2017.2762739; Richard MD, 1991, NEURAL COMPUT, V3, P461, DOI 10.1162/neco.1991.3.4.461; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; SCHARF LL, 1991, DETECTION ESTIMATION; Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093; Seide F, 2011, 12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5, P444; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270; Xiong W, 2017, INT CONF ACOUST SPEE, P5255, DOI 10.1109/ICASSP.2017.7953159; Zhou GB, 2016, INT J AUTOM COMPUT, V13, P226, DOI 10.1007/s11633-016-1006-2	39	1	1	6	17	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG 1	2021	43	8					2527	2537		10.1109/TPAMI.2020.2976978	http://dx.doi.org/10.1109/TPAMI.2020.2976978			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TF2YV	32142421	Green Submitted			2022-12-18	WOS:000670578800001
J	Lebl, M; Sroubek, F; Flusser, J				Lebl, Matej; Sroubek, Filip; Flusser, Jan			Blur-Invariant Similarity Measurement of Images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Blurred image classification; invariant distance; subspace projections; constrained minimization	FACE RECOGNITION	This article is a comment on the recent TPAMI paper (Gopalan et al., 2012) that introduced a blur-invariant distance measure between two images. We point out two mistakes of the theory presented in (Gopalan et al., 2012) and propose a correction. We also compare the original and corrected methods experimentally.	[Lebl, Matej; Sroubek, Filip; Flusser, Jan] Czech Acad Sci, Inst Informat Theory & Automat, Pod Vodarenskou Vezi 4, Prague 18208 8, Czech Republic	Czech Academy of Sciences; Institute of Information Theory & Automation of the Czech Academy of Sciences	Flusser, J (corresponding author), Czech Acad Sci, Inst Informat Theory & Automat, Pod Vodarenskou Vezi 4, Prague 18208 8, Czech Republic.	lebl@utia.cas.cz; sroubekf@utia.cas.cz; flusser@utia.cas.cz	Flusser, Jan/F-6209-2014; Sroubek, Filip/G-6882-2014	Flusser, Jan/0000-0003-3747-9214; Sroubek, Filip/0000-0001-6835-4911	Czech Science Foundation [GA18-07247S]	Czech Science Foundation(Grant Agency of the Czech Republic)	This work was supported by the Czech Science Foundation under Grant No. GA18-07247S and by the Praemium Academiae.	Flusser J, 2015, IEEE T PATTERN ANAL, V37, P786, DOI 10.1109/TPAMI.2014.2353644; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Gopalan R, 2012, IEEE T PATTERN ANAL, V34, P1220, DOI 10.1109/TPAMI.2012.15; Lebl M, 2019, LECT NOTES COMPUT SC, V11678, P351, DOI 10.1007/978-3-030-29888-3_28; Vageeswaran P, 2013, IEEE T IMAGE PROCESS, V22, P1362, DOI 10.1109/TIP.2012.2228498; Zhang ZW, 2013, IEEE T IMAGE PROCESS, V22, P3145, DOI 10.1109/TIP.2013.2259840	6	1	1	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG 1	2021	43	8					2882	2884		10.1109/TPAMI.2020.3036630	http://dx.doi.org/10.1109/TPAMI.2020.3036630			3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TF2YV	33156784				2022-12-18	WOS:000670578800028
J	Lee, Y; Kyung, CM				Lee, Yeongmin; Kyung, Chong-Min			A Memory- and Accuracy-Aware Gaussian Parameter-Based Stereo Matching Using Confidence Measure	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Memory management; Bandwidth; Filtering; Real-time systems; Random forests; Pattern matching; Gaussian mixture model; Stereo matching; semiglobal matching; confidence measure; cost aggregation	AGGREGATION	Accurate stereo matching requires a large amount of memory at a high bandwidth, which restricts its use in resource-limited systems such as mobile devices. This problem is compounded by the recent trend of applications requiring significantly high pixel resolution and disparity levels. To alleviate this, we present a memory-efficient and robust stereo matching algorithm. For cost aggregation, we employ the semiglobal parametric approach, which significantly reduces the memory bandwidth by representing the costs of all disparities as a Gaussian mixture model. All costs on multiple paths in an image are aggregated by updating the Gaussian parameters. The aggregation is performed during the scanning in the forward and backward directions. To reduce the amount of memory for the intermediate results during the forward scan, we suggest to store only the Gaussian parameters which contribute significantly to the final disparity selection. We also propose a method to enhance the overall procedure through a learning-based confidence measure. The random forest framework is used to train various features which are extracted from the cost and intensity profile. The experimental results on KITTI dataset show that the proposed method reduces the memory requirement to less than 3 percent of that of semiglobal matching (SGM) while providing a robust depth map compared to those of state-of-the-art SGM-based algorithms.	[Lee, Yeongmin; Kyung, Chong-Min] Korea Adv Inst Sci & Technol, Sch Elect Engn, Deajeon 34141, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Lee, Y (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Deajeon 34141, South Korea.	lym4398@gmail.com; kyung@kaist.ac.kr			Center for Integrated Smart Sensors - Ministry of Science and ICT [CISS-2013M3A6A6073718]	Center for Integrated Smart Sensors - Ministry of Science and ICT	This work was supported by the Center for Integrated Smart Sensors funded by the Ministry of Science and ICT as Global Frontier Project (CISS-2013M3A6A6073718).	[Anonymous], 2015, STATE OF THE ART DSP; [Anonymous], 2015, PRETRAINED MC CNN NE; Boykov Y, 2006, HANDBOOK OF MATHEMATICAL MODELS IN COMPUTER VISION, P79, DOI 10.1007/0-387-28831-7_5; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567; Einecke N, 2015, IEEE INT VEH SYM, P585, DOI 10.1109/IVS.2015.7225748; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339; Haeusler R, 2013, PROC CVPR IEEE, P305, DOI 10.1109/CVPR.2013.46; HAN S., 2015, ARXIV151000149; Hermann S., 2012, AS C COMP VIS, P465; Hirschmuller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56; Hirschmuller H., 2012, ISPRS ANN PHOTOGRAMM, V3, P371, DOI DOI 10.5194/ISPRSANNALS-I-3-371-2012; Honegger D, 2014, IEEE INT C INT ROBOT, P4930, DOI 10.1109/IROS.2014.6943263; Hu XY, 2012, IEEE T PATTERN ANAL, V34, P2121, DOI 10.1109/TPAMI.2012.46; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kim C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461926; Kim H, 2019, PROC CVPR IEEE, P12561, DOI 10.1109/CVPR.2019.01285; Lee KJ, 2017, IEEE J SOLID-ST CIRC, V52, P139, DOI 10.1109/JSSC.2016.2617317; Lee Y, 2018, IEEE SIGNAL PROC LET, V25, P194, DOI 10.1109/LSP.2017.2778306; Li ZY, 2017, ISSCC DIG TECH PAP I, P62; Liang ZF, 2018, PROC CVPR IEEE, P2811, DOI 10.1109/CVPR.2018.00297; Luo WJ, 2016, PROC CVPR IEEE, P5695, DOI 10.1109/CVPR.2016.614; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Mei X, 2011, PROC CVPR IEEE, P1257; Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925; Min DB, 2013, IEEE T PATTERN ANAL, V35, P2539, DOI 10.1109/TPAMI.2013.15; Park MG, 2019, IEEE T PATTERN ANAL, V41, P1397, DOI 10.1109/TPAMI.2018.2837760; Poggi M, 2017, IEEE I CONF COMP VIS, P5238, DOI 10.1109/ICCV.2017.559; Poggi M, 2017, PROC CVPR IEEE, P4541, DOI 10.1109/CVPR.2017.483; Poggi M, 2016, INT CONF 3D VISION, P509, DOI 10.1109/3DV.2016.61; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3; Schmid K, 2013, IEEE INT C INT ROBOT, P3955, DOI 10.1109/IROS.2013.6696922; Schonberger J.L., 2018, P EUR C COMP VIS ECC, P739; Seki A., 2016, BMVC; Seki A, 2017, PROC CVPR IEEE, P6640, DOI 10.1109/CVPR.2017.703; Sinha SN, 2014, PROC CVPR IEEE, P1582, DOI 10.1109/CVPR.2014.205; Spangenberg Robert, 2013, Computer Analysis of Images and Patterns. 15th International Conference, CAIP 2013. Proceedings: LNCS 8048, P34, DOI 10.1007/978-3-642-40246-3_5; Spangenberg R, 2014, IEEE INT VEH SYM, P190; Spyropoulos A, 2016, INT J COMPUT VISION, V118, P300, DOI 10.1007/s11263-015-0877-y; Spyropoulos A, 2014, PROC CVPR IEEE, P1621, DOI 10.1109/CVPR.2014.210; Tan X, 2014, LECT NOTES COMPUT SC, V8693, P17, DOI 10.1007/978-3-319-10602-1_2; The KITTI Vision Benchmark Suite, STEREO EVALUATION 20; Tippetts B, 2016, J REAL-TIME IMAGE PR, V11, P5, DOI 10.1007/s11554-012-0313-2; Tonioni A, 2019, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2019.00028; Tosi F., 2017, P 28 BRIT MACH VIS, P4; Zbontar J, 2015, PROC CVPR IEEE, P1592, DOI 10.1109/CVPR.2015.7298767; Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027; Zhong Z, 2017, MULTIMED TOOLS APPL, V76, P18473, DOI 10.1007/s11042-016-3932-y; Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992	52	1	1	2	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2021	43	6					1845	1858		10.1109/TPAMI.2019.2959613	http://dx.doi.org/10.1109/TPAMI.2019.2959613			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	SA8YQ	31869779				2022-12-18	WOS:000649590200003
J	Luo, Y; Wong, YK; Kankanhalli, M; Zhao, Q				Luo, Yan; Wong, Yongkang; Kankanhalli, Mohan; Zhao, Qi			Direction Concentration Learning: Enhancing Congruency in Machine Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Visualization; Computational modeling; Training; Convergence; Predictive models; Machine learning; Optimization; machine learning; computer vision; accumulated gradient; congruency	VISUAL SALIENCY; ATTENTION; MODEL	One of the well-known challenges in computer vision tasks is the visual diversity of images, which could result in an agreement or disagreement between the learned knowledge and the visual content exhibited by the current observation. In this work, we first define such an agreement in a concepts learning process as congruency. Formally, given a particular task and sufficiently large dataset, the congruency issue occurs in the learning process whereby the task-specific semantics in the training data are highly varying. We propose a Direction Concentration Learning (DCL) method to improve congruency in the learning process, where enhancing congruency influences the convergence path to be less circuitous. The experimental results show that the proposed DCL method generalizes to state-of-the-art models and optimizers, as well as improves the performances of saliency prediction task, continual learning task, and classification task. Moreover, it helps mitigate the catastrophic forgetting problem in the continual learning task. The code is publicly available at https://github.com/luoyan407/congruency.	[Luo, Yan; Zhao, Qi] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA; [Wong, Yongkang; Kankanhalli, Mohan] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore	University of Minnesota System; University of Minnesota Twin Cities; National University of Singapore	Zhao, Q (corresponding author), Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.	luoxx648@umn.edu; yongkang.wong@nus.edu.sg; mohan@comp.nus.edu.sg; qzhao@cs.umn.edu		Kankanhalli, Mohan/0000-0002-4846-2015; Wong, Yongkang/0000-0002-1239-4428; Luo, Yan/0000-0001-5135-0316	US National Science Foundation [1908711, 1849107]; University of Minnesota Department of Computer Science and Engineering Start-up Fund (QZ); National Research Foundation, Prime Minister's Office, Singapore under its Strategic Capability Research Centres Funding Initiative	US National Science Foundation(National Science Foundation (NSF)); University of Minnesota Department of Computer Science and Engineering Start-up Fund (QZ); National Research Foundation, Prime Minister's Office, Singapore under its Strategic Capability Research Centres Funding Initiative(National Research Foundation, Singapore)	This work was supported in part by the US National Science Foundation under Grants 1908711, 1849107, in part by the University of Minnesota Department of Computer Science and Engineering Start-up Fund (QZ), and in part by the National Research Foundation, Prime Minister's Office, Singapore under its Strategic Capability Research Centres Funding Initiative. The computational work for this article was partially performed on resources of the National Supercomputing Centre, Singapore (https://www.nscc.sg).	[Anonymous], 2004, ELECT LETT COMPUT VI, DOI DOI 10.5565/REV/ELCVIA.66; [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Bingjie Xu, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P2019, DOI 10.1109/CVPR.2019.00212; Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727; Bruce N., 2010, J VISION, V7, P950, DOI [10.1167/7.9.950, DOI 10.1167/7.9.950]; Carpenter GA, 1998, COMPUT VIS IMAGE UND, V69, P1, DOI 10.1006/cviu.1997.0561; Chang XJ, 2017, IEEE T PATTERN ANAL, V39, P1617, DOI 10.1109/TPAMI.2016.2608901; Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dorn W.S., 1960, Q APPL MATH, V18, P155, DOI [https://doi.org/10.1090/qam/112751, DOI 10.1090/QAM/112751, 10.1090/qam/112751]; Fernandez-Granda C., 2016, LECT NOTES OPTIMIZAT; French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2; Goodfellow I. J., 2013, ARXIV13126211, DOI DOI 10.1109/ISIE.2007; Gordon RD, 2004, J EXP PSYCHOL HUMAN, V30, P760, DOI 10.1037/0096-1523.30.4.760; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton G., 2012, NEURAL NETWORKS MACH, V264, P1; Hoi SCH, 2014, J MACH LEARN RES, V15, P495; Huang GL, 2017, IEEE ICC; Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Itti L, 2003, PROC SPIE, V5200, P64, DOI 10.1117/12.512618; Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710; Judd T., 2012, MIT TECHNICAL REPORT; Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Knight E., 2018, ARXIV180307482; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kummerer M, 2017, IEEE I CONF COMP VIS, P4799, DOI 10.1109/ICCV.2017.513; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li JN, 2017, IEEE I CONF COMP VIS, P2669, DOI 10.1109/ICCV.2017.289; Li YL, 2019, PROC CVPR IEEE, P3580, DOI 10.1109/CVPR.2019.00370; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lopez-Paz D, 2017, ADV NEUR IN, V30; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; MCNAUGHTON BL, 1983, EXP BRAIN RES, V52, P41; NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543; Nesterov Y, 2014, INTRO LECT CONVEX OP; Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Rothenstein AL, 2008, IMAGE VISION COMPUT, V26, P114, DOI 10.1016/j.imavis.2005.08.011; Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15; Simonyan K., 2015, ICLR; Tan MX, 2019, PR MACH LEARN RES, V97; Tao DC, 2009, IEEE T PATTERN ANAL, V31, P260, DOI 10.1109/TPAMI.2008.70; Tibshirani R, 2013, LECT NOTES OPTIMIZAT; TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5; Underwood G, 2006, Q J EXP PSYCHOL, V59, P1931, DOI 10.1080/17470210500416342; Underwood Geoffrey, 2007, P563, DOI 10.1016/B978-008044980-7/50028-8; Vapnik VN, 1999, IEEE T NEURAL NETWOR, V10, P988, DOI 10.1109/72.788640; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wolfe JM, 2017, NAT HUM BEHAV, V1, DOI 10.1038/s41562-017-0058; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xu J, 2014, J VISION, V14, DOI 10.1167/14.1.28; Yang S, 2020, IEEE T MULTIMEDIA, V22, P2163, DOI 10.1109/TMM.2019.2947352; Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023; You QZ, 2017, AAAI CONF ARTIF INTE, P231	61	1	1	3	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2021	43	6					1928	1946		10.1109/TPAMI.2019.2963387	http://dx.doi.org/10.1109/TPAMI.2019.2963387			19	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	SA8YQ	31902755	Green Submitted			2022-12-18	WOS:000649590200009
J	Wang, C; Yang, XM; Fei, SM; Zhou, K; Gong, XF; Du, M; Luo, RS				Wang, Chen; Yang, Xiaomei; Fei, Shaomin; Zhou, Kai; Gong, Xiaofeng; Du, Miao; Luo, Ruisen			Scalar Quantization as Sparse Least Square Optimization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Scalar quantization; l(0) least square; l(1) least square; clustering; approximation	VECTOR QUANTIZATION; SELECTION	Quantization aims to form new vectors or matrices with shared values close to the original. In recent years, the popularity of scalar quantization has been soaring as it is found huge utilities in reducing the resource cost of neural networks. Popular clustering-based techniques suffers substantially from the problems of dependency on the seed, empty or out-of-the-range clusters, and high time complexity. To overcome the problems, in this paper, scalar quantization is examined from a new perspective, namely sparse least square optimization. Specifically, several quantization algorithms based on l(1) least square are proposed and implemented. In addition, similar schemes with l(1) + l(2) and l(0) regularization are proposed. Furthermore, to compute quantization results with given amount of values/clusters, this paper proposes an iterative method and a clustering-based method, and both of them are built on sparse least square optimization. The algorithms proposed are tested under three data scenarios and their computational performance, including information loss, time consumption, and distribution of values of sparse vectors are compared. The paper offers a new perspective to probe the area of quantization, and the algorithms proposed are superior especially under bit-width reduction scenarios, where the required post-quantization resolution (the number of values) is not significantly lower than the original scalar.	[Wang, Chen; Yang, Xiaomei; Zhou, Kai; Gong, Xiaofeng; Du, Miao; Luo, Ruisen] Sichuan Univ, Coll Elect Engn, 24 South Sect 1,One Ring Rd, Chengdu 610065, Sichuan, Peoples R China; [Wang, Chen] Rutgers Univ New Brunswick, Dept Comp Sci, Piscataway, NJ 08854 USA; [Fei, Shaomin] Chengdu Univ Informat Technol, Engn Practice Ctr, Chengdu 610059, Peoples R China	Sichuan University; Rutgers State University New Brunswick; Chengdu University of Information Technology	Luo, RS (corresponding author), Sichuan Univ, Coll Elect Engn, 24 South Sect 1,One Ring Rd, Chengdu 610065, Sichuan, Peoples R China.	rsluo@scu.edu.cn		Wang, Chen/0000-0003-4044-9438	National Natural Science Foundation of China [61876114]; Research Project of State Key Laboratory of Southwest Jiaotong University [TPL1502]; University-Enterprise Cooperation Project [17H1199, 19H0355]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Research Project of State Key Laboratory of Southwest Jiaotong University; University-Enterprise Cooperation Project	The authors would like to thank Mr. Feng Chen from Northwestern Polytechnical University, China, and Mr. Shixiong Wang from the National University of Singapore, Singapore. Mr. Chen and Mr. Wang provided the authors many useful comments for the research. This work is supported by the National Natural Science Foundation of China (Grant No. 61876114), the Research Project of State Key Laboratory of Southwest Jiaotong University (No.TPL1502), and the University-Enterprise Cooperation Project (17H1199, 19H0355).	Agustsson E, 2017, ADV NEUR IN, V30; Ai LF, 2017, MULTIMEDIA SYST, V23, P169, DOI 10.1007/s00530-015-0470-9; Annapureddy V. S., 2015, FIX POINT QUANT DEEP; Azimi R, 2017, EXPERT SYST APPL, V76, P59, DOI 10.1016/j.eswa.2017.01.024; Boufounos PT, 2012, IEEE T INFORM THEORY, V58, P1861, DOI 10.1109/TIT.2011.2173899; Candes E, 2007, INVERSE PROBL, V23, P969, DOI 10.1088/0266-5611/23/3/008; Carreira-Perpin M. A., 2017, ARXIV PREPRINT ARXIV; COSMAN PC, 1993, P IEEE, V81, P1326, DOI 10.1109/5.237540; Dhillon P S, 2013, ADV NEURAL INFORM PR, P360; Elad M, 2010, P IEEE, V98, P972, DOI 10.1109/JPROC.2009.2037655; Gazagnes S, 2017, I S BIOMED IMAGING, P28, DOI 10.1109/ISBI.2017.7950460; Gong Y., 2014, ARXIV PREPRINT ARXIV; Guo Y, 2018, ARXIV PREPRINT ARXIV; Hammer B, 2014, NEUROCOMPUTING, V131, P43, DOI 10.1016/j.neucom.2013.05.054; Han S., 2015, ARXIV151000149, P1; Hazimeh H., 2018, ARXIV PREPRINT ARXIV; Hong MY, 2017, MATH PROGRAM, V163, P85, DOI 10.1007/s10107-016-1057-8; Hsieh IS, 2000, PATTERN RECOGN LETT, V21, P337, DOI 10.1016/S0167-8655(99)00165-8; Hubara I, 2018, J MACH LEARN RES, V18; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Le Capitaine H, 2011, ADV INTEL SYS RES, P1074; MARTINETZ TM, 1993, IEEE T NEURAL NETWOR, V4, P558, DOI 10.1109/72.238311; Nie F., 2010, ADV NEURAL INFORM PR, V1, P1813, DOI DOI 10.1007/978-3-319-10690-8_12; NOWLAN SJ, 1992, NEURAL COMPUT, V4, P473, DOI 10.1162/neco.1992.4.4.473; Ozan EC, 2016, IEEE T KNOWL DATA EN, V28, P2884, DOI 10.1109/TKDE.2016.2597834; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Saleem M, 2016, 2016 SIXTH INTERNATIONAL CONFERENCE ON INNOVATIVE COMPUTING TECHNOLOGY (INTECH), P199, DOI 10.1109/INTECH.2016.7845048; Schmidt M, 2007, LECT NOTES ARTIF INT, V4701, P286; Sze V., 2017, EFFICIENT PROCESSING; Tibshirani R, 2011, J R STAT SOC B, V73, P273, DOI 10.1111/j.1467-9868.2011.00771.x; Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108; Ullrich K, 2017, ARXIV PREPRINT ARXIV; Villmann T, 2011, NEURAL COMPUT, V23, P1343, DOI 10.1162/NECO_a_00110; Wang C, 2017, PRACTICAL IMPLEMENTA; Wen Q, 2011, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2011-118; Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470; Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3; XIANG ZG, 1994, IEEE COMPUT GRAPH, V14, P44, DOI 10.1109/38.279043; Yang JF, 2011, SIAM J SCI COMPUT, V33, P250, DOI 10.1137/090777761; Zhi-Quan Luo, 1993, Annals of Operations Research, V46-47, P157; Zhou WG, 2016, IEEE T PATTERN ANAL, V38, P159, DOI 10.1109/TPAMI.2015.2430329; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	42	1	1	3	23	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY 1	2021	43	5					1678	1690		10.1109/TPAMI.2019.2952096	http://dx.doi.org/10.1109/TPAMI.2019.2952096			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RJ3YD	31722473	Green Submitted			2022-12-18	WOS:000637533800014
J	Im, S; Ha, H; Jeon, HG; Lin, S; Kweon, IS				Im, Sunghoon; Ha, Hyowon; Jeon, Hae-Gon; Lin, Stephen; Kweon, In So			Deep Depth from Uncalibrated Small Motion Clip	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cameras; Bundle adjustment; Geometry; Image reconstruction; Estimation; Calibration; 3D reconstruction; geometry; deep learning; structure from motion; bundle adjustment; plane sweeping algorithm		We propose a novel approach to infer a high-quality depth map from a set of images with small viewpoint variations. In general, techniques for depth estimation from small motion consist of camera pose estimation and dense reconstruction. In contrast to prior approaches that recover scene geometry and camera motions using pre-calibrated cameras, we introduce in this paper a self-calibrating bundle adjustment method tailored for small motion which enables computation of camera poses without the need for camera calibration. For dense depth reconstruction, we present a convolutional neural network called DPSNet (Deep Plane Sweep Network) whose design is inspired by best practices of traditional geometry-based approaches. Rather than directly estimating depth or optical flow correspondence from image pairs as done in many previous deep learning methods, DPSNet takes a plane sweep approach that involves building a cost volume from deep features using the plane sweep algorithm, regularizing the cost volume, and regressing the depth map from the cost volume. The cost volume is constructed using a differentiable warping process that allows for end-to-end training of the network. Through the effective incorporation of conventional multiview stereo concepts within a deep learning framework, the proposed method achieves state-of-the-art results on a variety of challenging datasets.	[Im, Sunghoon] Daegu Gyeongbuk Inst Sci & Technol, Dept Informat & Commun Engn, Daegu 42988, South Korea; [Ha, Hyowon] Apple Inc, Cupertino, CA 95014 USA; [Jeon, Hae-Gon] Gwangju Inst Sci & Technol, Artificial Intelligence Grad Sch, Gwangju 61005, South Korea; [Jeon, Hae-Gon] Gwangju Inst Sci & Technol, Sch Elect Engn & Comp Sci, Gwangju 61005, South Korea; [Lin, Stephen] Microsoft Res, Beijing, Peoples R China; [Kweon, In So] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea	Daegu Gyeongbuk Institute of Science & Technology (DGIST); Apple Inc; Gwangju Institute of Science & Technology (GIST); Gwangju Institute of Science & Technology (GIST); Microsoft; Korea Advanced Institute of Science & Technology (KAIST)	Jeon, HG (corresponding author), Gwangju Inst Sci & Technol, Artificial Intelligence Grad Sch, Gwangju 61005, South Korea.; Jeon, HG (corresponding author), Gwangju Inst Sci & Technol, Sch Elect Engn & Comp Sci, Gwangju 61005, South Korea.	sunghoonim@dgist.ac.kr; hyowon_ha@apple.com; haegonj@gist.ac.kr; stevelin@microsoft.com; iskweon77@kaist.ac.kr		Jeon, Hae-Gon/0000-0003-1105-1666	DGIST Start-up Fund Program of the Ministry of Science and ICT [2019090016]; Global University Project (GUP) Grant - GIST; Institute of Information & communications Technology Planning & Evaluation (IITP) - Korea government (MSIT) [2019-0-01842]	DGIST Start-up Fund Program of the Ministry of Science and ICT; Global University Project (GUP) Grant - GIST; Institute of Information & communications Technology Planning & Evaluation (IITP) - Korea government (MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea)	This work was supported by the DGIST Start-up Fund Program of the Ministry of Science and ICT (2019090016). This work was also supported by Global University Project (GUP) Grant funded by the GIST in 2019, and by Institute of Information & communications Technology Planning & Evaluation (IITP) Grant funded by the Korea government (MSIT) (No.2019-0-01842, Artificial Intelligence Graduate School Support). H. Ha was with KAIST when participating in this work.	Agarwal S., 2010, CERES SOLVER; [Anonymous], 2019, BLENDER; Chang A, 2017, INT CONF 3D VISION, P667, DOI 10.1109/3DV.2017.00081; Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Civera J, 2008, IEEE T ROBOT, V24, P932, DOI 10.1109/TRO.2008.2003276; Collins RT, 1996, PROC CVPR IEEE, P358, DOI 10.1109/CVPR.1996.517097; Crandall D., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3001, DOI 10.1109/CVPR.2011.5995626; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; FANG JQ, 1984, COMPUT VISION GRAPH, V26, P183, DOI 10.1016/0734-189X(84)90182-8; Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595; Gallup D., 2008, 2008 IEEE C COMPUTER, P1, DOI 10.1109/CVPR.2008.4587671; Ha H, 2016, PROC CVPR IEEE, P5413, DOI 10.1109/CVPR.2016.584; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213; Hu XY, 2012, IEEE T PATTERN ANAL, V34, P2121, DOI 10.1109/TPAMI.2012.46; Huang PH, 2018, PROC CVPR IEEE, P2821, DOI 10.1109/CVPR.2018.00298; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Im S, 2016, LECT NOTES COMPUT SC, V9907, P156, DOI 10.1007/978-3-319-46487-9_10; Im S, 2015, IEEE I CONF COMP VIS, P837, DOI 10.1109/ICCV.2015.102; Jaderberg M, 2015, ADV NEUR IN, V28; Ji MQ, 2017, IEEE I CONF COMP VIS, P2326, DOI 10.1109/ICCV.2017.253; Kahl F, 2005, IEEE I CONF COMP VIS, P1002; Kahl F, 2008, INT J COMPUT VISION, V79, P271, DOI 10.1007/s11263-007-0117-1; Kar A, 2017, ADV NEUR IN, V30; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Krahenbuhl P., 2011, ADV NEURAL INF PROCE, V24, P109; Li RH, 2018, IEEE INT CONF ROBOT, P7286; Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8; Lucas B.D., 1981, ITERATIVE IMAGE REGI, P674; Ma L., 2004, INT J INF ACQUISITIO, V1, P135; Mei X, 2013, PROC CVPR IEEE, P313, DOI 10.1109/CVPR.2013.47; OKUTOMI M, 1993, IEEE T PATTERN ANAL, V15, P353, DOI 10.1109/34.206955; Oliensis J, 1998, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.1998.698610; Oliensis J, 1999, INT J COMPUT VISION, V34, P163, DOI 10.1023/A:1008139920864; Oliensis J, 2005, INT J COMPUT VISION, V61, P259, DOI 10.1023/B:VISI.0000045326.88734.8b; Rhemann C, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995372; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Sim K., 2006, P IEEE C COMP VIS PA, V1, P1230, DOI 10.1109/CVPR.2006.247; Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3; Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964; Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773; Tamaki T, 2002, INT C PATT RECOG, P584, DOI 10.1109/ICPR.2002.1048370; Tang CR, 2020, PHARMACOLOGY, V105, P339, DOI 10.1159/000503865; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; Ummenhofer B, 2017, PROC CVPR IEEE, P5622, DOI 10.1109/CVPR.2017.596; Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458; Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47; Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567; Yu F., 2016, INT C LEARN REPRESEN; Yu F, 2014, PROC CVPR IEEE, P3986, DOI 10.1109/CVPR.2014.509; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zitnick C. L, 2014, MSRTR201473	57	1	1	4	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2021	43	4					1225	1238		10.1109/TPAMI.2019.2946806	http://dx.doi.org/10.1109/TPAMI.2019.2946806			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QT3YJ	31613749				2022-12-18	WOS:000626525300009
J	Kostkova, J; Suk, T; Flusser, J				Kostkova, Jitka; Suk, Tomas; Flusser, Jan			Affine Invariants of Vector Fields	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Pattern matching; Mathematical model; Strain; Task analysis; Color; Wind speed; Vector field; total affine transformation; affine invariants; template matching; vector field moments		Vector fields are a special kind of multidimensional data, which are in a certain sense similar to digital color images, but are distinct from them in several aspects. In each pixel, the field is assigned to a vector that shows the direction and the magnitude of the quantity, which has been measured. To detect the patterns of interest in the field, special matching methods must be developed. In this paper, we propose a method for the description and matching of vector field patterns under an unknown affine transformation of the field. Unlike digital images, transformations of vector fields act not only on the spatial coordinates but also on the field values, which makes the detection different from the image case. To measure the similarity between the template and the field patch, we propose original invariants with respect to total affine transformation. They are designed from the vector field moments. It is demonstrated by experiments on real data from fluid mechanics that they perform significantly better than potential competitors.	[Kostkova, Jitka; Suk, Tomas; Flusser, Jan] Czech Acad Sci, Inst Informat Theory & Automat, Vodarenskou Vezi 4, Prague 18208 8, Czech Republic	Czech Academy of Sciences; Institute of Information Theory & Automation of the Czech Academy of Sciences	Flusser, J (corresponding author), Czech Acad Sci, Inst Informat Theory & Automat, Vodarenskou Vezi 4, Prague 18208 8, Czech Republic.	kostkova@utia.cas.cz; suk@utia.cas.cz; flusser@utia.cas.cz	Flusser, Jan/F-6209-2014	Flusser, Jan/0000-0003-3747-9214	Czech Science Foundation [GA18-07247S]; Ministry of Education, Youth, and Sports of the Czech Republic (MSMT CR) [SGS18/188/OHK4/3T/14]; Praemium Academiae	Czech Science Foundation(Grant Agency of the Czech Republic); Ministry of Education, Youth, and Sports of the Czech Republic (MSMT CR)(Ministry of Education, Youth & Sports - Czech Republic); Praemium Academiae(Czech Academy of Sciences)	This work has been supported by the Czech Science Foundation under the grant No. GA18-07247S, by the Grant SGS18/188/OHK4/3T/14 provided by the Ministry of Education, Youth, and Sports of the Czech Republic (MSMT CR), and by the Praemium Academiae. The authors would like to thank Prof. Mario Hlawitschka and Dr. Roxana Bujack for providing the Karman vortex street data, and the NOAA ESRL Physical Sciences Division, Boulder, Colorado, for providing the wind maps.	ABUMOSTAFA YS, 1984, IEEE T PATTERN ANAL, V6, P698, DOI 10.1109/TPAMI.1984.4767594; BANKS DC, 1995, IEEE T VIS COMPUT GR, V1, P151, DOI 10.1109/2945.468404; BERDAHL CH, 1993, AIAA J, V31, P97, DOI 10.2514/3.11324; Brown AB, 1935, T AM MATH SOC, V38, P379, DOI 10.2307/1989688; Bujack R., 2017, P INT C CENTR EUR CO, P11; Bujack R, 2014, PATTERN RECOGN LETT, V46, P46, DOI 10.1016/j.patrec.2014.05.005; Bujack R, 2014, IEEE PAC VIS SYMP, P41, DOI 10.1109/PacificVis.2014.16; Chen QG, 2015, PHYS FLUIDS, V27, DOI 10.1063/1.4927647; FINLAYSON GD, 1994, J OPT SOC AM A, V11, P3011, DOI 10.1364/JOSAA.11.003011; Flusser J, 2000, PATTERN RECOGN, V33, P1405, DOI 10.1016/S0031-3203(99)00127-2; FLUSSER J, 1993, PATTERN RECOGN, V26, P167, DOI 10.1016/0031-3203(93)90098-H; Flusser J, 2002, PATTERN RECOGN, V35, P3015, DOI 10.1016/S0031-3203(02)00093-6; Flusser J., 2009, MOMENTS MOMENT INVAR; Flusser J., 2016, 2D 3D IMAGE ANAL MOM; Gevers T., 1996, PROC 1 INT WORKSHOP, P17; Gong M, 2017, COMPUT VIS IMAGE UND, V162, P46, DOI 10.1016/j.cviu.2017.07.003; Grace John Hilton, 1903, ALGEBRA INVARIANTS; Gurevich G.B., 1964, FDN THEORY ALGEBRAIC; Hickman MS, 2012, J MATH IMAGING VIS, V44, P223, DOI 10.1007/s10851-011-0323-x; Hilbert D., 1993, THEORY ALGEBRAIC INV; HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692; JEONG J, 1995, J FLUID MECH, V285, P69, DOI 10.1017/S0022112095000462; Jiang M., 2011, VISUALIZATION HDB; Kalnay E, 1996, B AM METEOROL SOC, V77, P437, DOI 10.1175/1520-0477(1996)077<0437:TNYRP>2.0.CO;2; Kostkova J, 2019, LECT NOTES COMPUT SC, V11678, P402, DOI 10.1007/978-3-030-29888-3_32; Langbein M, 2009, LECT NOTES COMPUT SC, V5876, P1151, DOI 10.1007/978-3-642-10520-3_110; LEVY Y, 1990, AIAA J, V28, P1347, DOI 10.2514/3.25224; Liu MH, 2012, PATTERN RECOGN, V45, P2532, DOI 10.1016/j.patcog.2012.01.014; Liu W, 2012, PATTERN RECOGN, V45, P3912, DOI 10.1016/j.patcog.2012.04.025; Mindru F, 2004, COMPUT VIS IMAGE UND, V94, P3, DOI 10.1016/j.cviu.2003.10.011; Mindru F, 1999, INTERNATIONAL CONFERENCE ON ADVANCES IN PATTERN RECOGNITION, P113; Reiss T.H., 1993, RECOGNIZING PLANAR O; REISS TH, 1991, IEEE T PATTERN ANAL, V13, P830, DOI 10.1109/34.85675; Roth M, 1998, VISUALIZATION '98, PROCEEDINGS, P143, DOI 10.1109/VISUAL.1998.745296; Rothe I, 1996, IEEE T PATTERN ANAL, V18, P366, DOI 10.1109/34.491618; Sadarjoen IA, 1998, VISUALIZATION '98, PROCEEDINGS, P419, DOI 10.1109/VISUAL.1998.745333; Schlemmer M, 2007, IEEE T VIS COMPUT GR, V13, P1743, DOI 10.1109/TVCG.2007.70579; Schur I, 1968, VORLESUNGEN INVARIAN; Suk T, 2004, INT C PATT RECOG, P192, DOI 10.1109/ICPR.2004.1334093; Suk T, 2011, PATTERN RECOGN, V44, P2047, DOI 10.1016/j.patcog.2010.05.015; Suk T, 2009, LECT NOTES COMPUT SC, V5702, P334, DOI 10.1007/978-3-642-03767-2_41; Suk T, 2008, INT C PATT RECOG, P3270; Sylvester J. J, 1879, AM J MATH, V2, P293; Sylvester JJ, 1879, AM J MATH, V2, P223, DOI 10.2307/2369240; Yang B, 2017, IEEE IMAGE PROC, P2359; Yang B, 2018, PATTERN RECOGN, V74, P110, DOI 10.1016/j.patcog.2017.09.004	46	1	1	1	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2021	43	4					1140	1155		10.1109/TPAMI.2019.2951664	http://dx.doi.org/10.1109/TPAMI.2019.2951664			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QT3YJ	31714215				2022-12-18	WOS:000626525300003
J	Silva, M; Ramos, W; Campos, M; Nascimento, ER				Silva, Michel; Ramos, Washington; Campos, Mario; Nascimento, Erickson R.			A Sparse Sampling-Based Framework for Semantic Fast-Forward of First-Person Videos	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Videos; Semantics; Visualization; Acceleration; Cameras; Encoding; Pattern analysis; First-person video; fast-forward; semantic information; sparse coding; minimum sparse reconstruction problem	EGOCENTRIC VIDEOS	Technological advances in sensors have paved the way for digital cameras to become increasingly ubiquitous, which, in turn, led to the popularity of the self-recording culture. As a result, the amount of visual data on the Internet is moving in the opposite direction of the available time and patience of the users. Thus, most of the uploaded videos are doomed to be forgotten and unwatched stashed away in some computer folder or website. In this paper, we address the problem of creating smooth fast-forward videos without losing the relevant content. We present a new adaptive frame selection formulated as a weighted minimum reconstruction problem. Using a smoothing frame transition and filling visual gaps between segments, our approach accelerates first-person videos emphasizing the relevant segments and avoids visual discontinuities. Experiments conducted on controlled videos and also on an unconstrained dataset of First-Person Videos (FPVs) show that, when creating fast-forward videos, our method is able to retain as much relevant information and smoothness as the state-of-the-art techniques, but in less processing time.	[Silva, Michel; Ramos, Washington; Campos, Mario; Nascimento, Erickson R.] Univ Fed Minas Gerais, Dept Comp Sci, Comp Vis & Robot Lab, BR-31270901 Belo Horizonte, MG, Brazil	Universidade Federal de Minas Gerais	Silva, M (corresponding author), Univ Fed Minas Gerais, Dept Comp Sci, Comp Vis & Robot Lab, BR-31270901 Belo Horizonte, MG, Brazil.	michelms@dcc.ufmg.br; washington.ramos@dcc.ufmg.br; mario@dcc.ufmg.br; erickson@dcc.ufmg.br	Nascimento, Erickson R./G-5374-2014; Campos, Mario F. M./C-4647-2013	Nascimento, Erickson R./0000-0003-2973-2232; Campos, Mario F. M./0000-0002-8336-9190; Silva, Michel/0000-0002-2499-9619; Ramos, Washington/0000-0002-0411-8677	CAPES; CNPq; FAPEMIG	CAPES(Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES)); CNPq(Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPQ)); FAPEMIG(Fundacao de Amparo a Pesquisa do Estado de Minas Gerais (FAPEMIG))	The authors would like to thank CAPES, CNPq, and FAPEMIG for funding different parts of this work. We also thank NVIDIA for the donation of a TITAN Xp GPU.	del Molino AG, 2017, IEEE T HUM-MACH SYST, V47, P65, DOI 10.1109/THMS.2016.2623480; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Fu TJ, 2019, IEEE WINT CONF APPL, P1579, DOI 10.1109/WACV.2019.00173; Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928; Halperin T, 2018, IEEE T CIRC SYST VID, V28, P1248, DOI 10.1109/TCSVT.2017.2651051; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Joshi N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766954; Karpenko A, TECHNOLOGY HYPERLAPS; Kopf J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601195; Lai WS, 2018, IEEE T VIS COMPUT GR, V24, P2610, DOI 10.1109/TVCG.2017.2750671; Lal S, 2019, IEEE WINT CONF APPL, P471, DOI 10.1109/WACV.2019.00056; Lan SY, 2018, PROC CVPR IEEE, P6771, DOI 10.1109/CVPR.2018.00708; Mahasseni B, 2017, PROC CVPR IEEE, P2077, DOI 10.1109/CVPR.2017.224; Mei S., 2014, P IEEE INT C MULT EX, P1; Mei SH, 2015, PATTERN RECOGN, V48, P522, DOI 10.1016/j.patcog.2014.08.002; Melo Silva Michel, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P557, DOI 10.1007/978-3-319-46604-0_40; Ogawa M, 2017, IEEE IMAGE PROC, P2124; Okamoto M., 2013, P 6 PAC RIM S IM VID, P431; Oliveira GL, 2014, IEEE T IMAGE PROCESS, V23, P2719, DOI 10.1109/TIP.2014.2317988; Otani M., 2016, AS C COMP VIS, P361; Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199; Plummer BA, 2017, PROC CVPR IEEE, P1052, DOI 10.1109/CVPR.2017.118; Poleg Y, 2015, PROC CVPR IEEE, P4768, DOI 10.1109/CVPR.2015.7299109; Ramos WLS, 2016, IEEE IMAGE PROC, P3334, DOI 10.1109/ICIP.2016.7532977; Sharghi A, 2017, PROC CVPR IEEE, P2127, DOI 10.1109/CVPR.2017.229; Silva M, 2018, PROC CVPR IEEE, P2383, DOI 10.1109/CVPR.2018.00253; Silva MM, 2018, J VIS COMMUN IMAGE R, V53, P55, DOI 10.1016/j.jvcir.2018.02.013; Traffic-Inquiries, 2018, 1543280537836565 CIS; Venkatesh K. S, 2017, P NAT C COMP VIS PAT, P3; Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018; Wang M, 2018, IEEE T IMAGE PROCESS, V27, P1735, DOI 10.1109/TIP.2017.2749143; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Yao T, 2016, PROC CVPR IEEE, P982, DOI 10.1109/CVPR.2016.112; Zhao B, 2014, PROC CVPR IEEE, P2513, DOI 10.1109/CVPR.2014.322	34	1	1	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2021	43	4					1438	1444		10.1109/TPAMI.2020.2983929	http://dx.doi.org/10.1109/TPAMI.2020.2983929			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QT3YJ	32248095	Green Submitted			2022-12-18	WOS:000626525300023
J	Xie, JW; Zheng, ZL; Fang, XL; Zhu, SC; Wu, YN				Xie, Jianwen; Zheng, Zilong; Fang, Xiaolin; Zhu, Song-Chun; Wu, Ying Nian			Cooperative Training of Fast Thinking Initializer and Slow Thinking Solver for Conditional Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Linear programming; Iterative methods; Generators; Gallium nitride; Training; Task analysis; Planning; Deep generative models; cooperative learning; energy-based models; langevin dynamics; conditional learning	NETWORKS	This paper studies the problem of learning the conditional distribution of a high-dimensional output given an input, where the output and input may belong to two different domains, e.g., the output is a photo image and the input is a sketch image. We solve this problem by cooperative training of a fast thinking initializer and slow thinking solver. The initializer generates the output directly by a non-linear transformation of the input as well as a noise vector that accounts for latent variability in the output. The slow thinking solver learns an objective function in the form of a conditional energy function, so that the output can be generated by optimizing the objective function, or more rigorously by sampling from the conditional energy-based model. We propose to learn the two models jointly, where the fast thinking initializer serves to initialize the sampling of the slow thinking solver, and the solver refines the initial output by an iterative algorithm. The solver learns from the difference between the refined output and the observed output, while the initializer learns from how the solver refines its initial output. We demonstrate the effectiveness of the proposed method on various conditional learning tasks, e.g., class-to-image generation, image-to-image translation, and image recovery. The advantage of our method over GAN-based methods is that our method is equipped with a slow thinking process that refines the solution guided by a learned objective function.	[Xie, Jianwen] Baidu Res, Cognit Comp Lab, Bellevue, WA 98004 USA; [Zheng, Zilong] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA; [Fang, Xiaolin] MIT, Dept Comp Sci, Cambridge, MA 02139 USA; [Zhu, Song-Chun] Tsinghua Univ, Beijing 100084, Peoples R China; [Zhu, Song-Chun] Peking Univ, Beijing 100871, Peoples R China; [Wu, Ying Nian] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90095 USA	Baidu; University of California System; University of California Los Angeles; Massachusetts Institute of Technology (MIT); Tsinghua University; Peking University; University of California System; University of California Los Angeles	Xie, JW (corresponding author), Baidu Res, Cognit Comp Lab, Bellevue, WA 98004 USA.	jianwen@ucla.edu; z.zheng@ucla.edu; xiaolinf@csail.mit.edu; sczhu@stat.ucla.edu; ywu@stat.ucla.edu			NVIDIA Corporation; NSF [DMS-2015577]; DARPA SIMPLEX [N66001-15-C-4035]; ONR MURI [N00014-16-1-2007]; DARPA ARO [W911NF-16-1-0579]; DARPA [N66001-17-2-4029]; XSEDE grant [ASC180018]	NVIDIA Corporation; NSF(National Science Foundation (NSF)); DARPA SIMPLEX; ONR MURI(MURIOffice of Naval Research); DARPA ARO(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); XSEDE grant	The authors gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research. This work was supported by the NSF DMS-2015577, DARPA SIMPLEX N66001-15-C-4035, ONR MURI N00014-16-1-2007, DARPA ARO W911NF-16-1-0579, DARPA N66001-17-2-4029, and XSEDE grant ASC180018. Jianwen Xie and Zilong Zheng contributed equally to this work.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299; Barbu A., 2020, MONTE CARLO METHODS; Berthelot D., 2017, ARXIV 170310717; Bojanowski P, 2018, PR MACH LEARN RES, V80; Brooks S, 2011, CH CRC HANDB MOD STA, P1, DOI 10.1201/b10905; Chen X, 2016, ADV NEUR IN, V29; Dinh L., 2017, PROC INT C LEARN REP; Dinh Laurent, 2014, ARXIV14108516; Dumoulin Vincent, 2017, ICLR 2017, P4; Gao RQ, 2018, PROC CVPR IEEE, P9155, DOI 10.1109/CVPR.2018.00954; Goncalves GR, 2018, SIBGRAPI, P110, DOI 10.1109/SIBGRAPI.2018.00021; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grover A, 2020, AAAI CONF ARTIF INTE, V34, P4028; Gulrajani I, 2017, P NIPS 2017; Han T, 2017, AAAI CONF ARTIF INTE, P1976; Hensel M, 2017, ADV NEUR IN, V30; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Ho J, 2016, ADV NEUR IN, V29; Hoshen Y, 2019, PROC CVPR IEEE, P5804, DOI 10.1109/CVPR.2019.00596; Hu M., 2019, PROC ASIAN C MACH LE, P109; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Kingma D.P., 2013, P 2 INT C LEARN REPR; Kingma DP, 2017, P INT C LEARN REPR I; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Liu R, 2019, PROC CVPR IEEE, P7984, DOI 10.1109/CVPR.2019.00818; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Mirza M., 2014, ARXIV; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Nijkamp E, 2019, ADV NEUR IN, V32; Nijkamp E, 2020, AAAI CONF ARTIF INTE, V34, P5272; Odena A, 2017, PR MACH LEARN RES, V70; Ostrovski G, 2018, PR MACH LEARN RES, V80; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S, 2016, PR MACH LEARN RES, V48; Ronneberger O., 2015, P INT C MED IMAG COM, P234, DOI [DOI 10.1007/978-3-319-24574-4_28, DOI 10.48550/ARXIV.1505.04597]; Salimans T, 2016, ADV NEUR IN, V29; Sohn K, 2015, ADV NEUR IN, V28; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tylecek R, 2013, LECT NOTES COMPUT SC, V8142, P364, DOI 10.1007/978-3-642-40602-7_39; van den Oord A, 2016, PR MACH LEARN RES, V48; Wang D., 2016, ARXIV161101722; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang TC, 2018, ADV NEUR IN, V31; Xiao H., 2017, ARXIV 170807747; Xiao Z., 2019, ARXIV 190510485; Xie J, PROC 35 AAAI C ARTIF, P2021; Xie JW, 2022, IEEE T PATTERN ANAL, V44, P2468, DOI 10.1109/TPAMI.2020.3045010; Xie JW, 2018, AAAI CONF ARTIF INTE, P4292; Xie JW, 2021, IEEE T PATTERN ANAL, V43, P516, DOI 10.1109/TPAMI.2019.2934852; Xie JW, 2020, IEEE T PATTERN ANAL, V42, P27, DOI 10.1109/TPAMI.2018.2879081; Xie JW, 2018, PROC CVPR IEEE, P8629, DOI 10.1109/CVPR.2018.00900; Xie JW, 2016, PR MACH LEARN RES, V48; Xie JW, 2017, PROC CVPR IEEE, P1061, DOI 10.1109/CVPR.2017.119; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Younes L., 1999, STOCHASTICS STOCHAST, V65, P177, DOI DOI 10.1080/17442509908834179; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhao J, 2017, PROC INT C LEARN REP; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Ziebart B. D., 2008, AAAI, V8, P1433	71	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 26	2021	44	8					3957	3973		10.1109/TPAMI.2021.3069023	http://dx.doi.org/10.1109/TPAMI.2021.3069023			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	2Q6HL	33769930	Green Submitted			2022-12-18	WOS:000820521500001
J	Liu, MM; Shang, ZF; Yang, Y; Cheng, G				Liu, Meimei; Shang, Zuofeng; Yang, Yun; Cheng, Guang			Nonparametric Testing Under Randomized Sketching	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Testing; Kernel; Smoothing methods; Computational modeling; Estimation; Eigenvalues and eigenfunctions; Upper bound; Computational limit; kernel ridge regression; minimax optimality; nonparametric testing; random sketch	REGRESSION; INFERENCE; HYPOTHESIS; APPROXIMATION; MATRIX; ERROR; MODEL	A common challenge in nonparametric inference is its high computational complexity when data volume is large. In this paper, we develop computationally efficient nonparametric testing by employing a random projection strategy. In the specific kernel ridge regression setup, a simple distance-based test statistic is proposed. Notably, we derive the minimum number of random projections that is sufficient for achieving testing optimality in terms of the minimax rate. An adaptive testing procedure is further established without prior knowledge of regularity. One technical contribution is to establish upper bounds for a range of tail sums of empirical kernel eigenvalues. Simulations and real data analysis are conducted to support our theory.	[Liu, Meimei] Virginia Tech, Dept Stat, Blacksburg, VA 24060 USA; [Shang, Zuofeng] New Jersey Inst Technol, Dept Math Sci, Newark, NJ 07102 USA; [Yang, Yun] Univ Illinois, Dept Stat, Champaign, IL 61820 USA; [Cheng, Guang] Purdue Univ, Dept Stat, W Lafayette, IN 47906 USA	Virginia Polytechnic Institute & State University; New Jersey Institute of Technology; University of Illinois System; University of Illinois Urbana-Champaign; Purdue University System; Purdue University; Purdue University West Lafayette Campus	Liu, MM (corresponding author), Virginia Tech, Dept Stat, Blacksburg, VA 24060 USA.	meimeiliu@vt.edu; zshang@njit.edu; yy84@illinois.edu; chengg@purdue.edu						Ailon N, 2009, SIAM J COMPUT, V39, P302, DOI 10.1137/060673096; Alaoui A., 2015, P 28 INT C NEURAL IN, P775; ANDERSON TW, 1952, ANN MATH STAT, V23, P193, DOI 10.1214/aoms/1177729437; AZZALINI A, 1993, J ROY STAT SOC B MET, V55, P549; Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282; Bifet A, 2010, J MACH LEARN RES, V11, P1601; Boutsidis C, 2013, SIAM J MATRIX ANAL A, V34, P1301, DOI 10.1137/120874540; Braun ML, 2006, J MACH LEARN RES, V7, P2303; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Chakrabarti D, 2017, J MACH LEARN RES, V18, P1; COX D, 1988, ANN STAT, V16, P113, DOI 10.1214/aos/1176350693; Cramer H., 2016, MATH METHODS STAT PM; Dieuleveut A, 2016, ANN STAT, V44, P1363, DOI 10.1214/15-AOS1391; EUBANK RL, 1990, J AM STAT ASSOC, V85, P387, DOI 10.2307/2289774; Fan JQ, 2001, ANN STAT, V29, P153, DOI 10.1214/aos/996986505; Gittens A., 2013, INT C MACHINE LEARNI, P567; Gu C, 2013, SPRINGER SER STAT, V297, P1, DOI 10.1007/978-1-4614-5369-7; Guo WS, 2002, J ROY STAT SOC B, V64, P887, DOI 10.1111/1467-9868.00367; HARDLE W, 1993, ANN STAT, V21, P1926, DOI 10.1214/aos/1176349403; Huang C., 2017, ARXIV170106054; Jordan M. I., 2014, P 10 INT C MACH LEAR, P159; Kim YJ, 2004, J R STAT SOC B, V66, P337, DOI 10.1046/j.1369-7412.2003.05316.x; Liu A, 2004, J STAT COMPUT SIM, V74, P581, DOI 10.1080/00949650310001623416; Liu M., 2018, PROC ADV NEURAL INF, P3985; Lopes M., 2011, ADV NEURAL INFORM PR, P1206; Ma P, 2017, STAT SINICA, V27, P1757, DOI 10.5705/ss.202015.0423; Minh HQ, 2006, LECT NOTES ARTIF INT, V4005, P154, DOI 10.1007/11776420_14; Musco C, 2017, ADV NEUR IN, V30; Pilanci M, 2015, IEEE T INFORM THEORY, V61, P5096, DOI 10.1109/TIT.2015.2450722; Rudi A., 2015, ADV NEURAL INFORM PR, V28, P1657, DOI DOI 10.5555/2969239.2969424; Schifano ED, 2016, TECHNOMETRICS, V58, P393, DOI 10.1080/00401706.2016.1142900; Scholkopf B., 1999, ADV KERNEL METHODS S; Shang ZF, 2017, J MACH LEARN RES, V18; Shang ZF, 2013, ANN STAT, V41, P2608, DOI 10.1214/13-AOS1164; Shawe-Taylor J, 2005, IEEE T INFORM THEORY, V51, P2510, DOI 10.1109/TIT.2005.850052; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Spokoiny VG, 1996, ANN STAT, V24, P2477; Volgushev S, 2019, ANN STAT, V47, P1634, DOI 10.1214/18-AOS1730; Wahba G., 1990, SPLINE MODELS OBSERV, V59, DOI [10.1137/1.9781611970128, DOI 10.1137/1.9781611970128]; Wei Y., 2017, P ADV NEUR INF PROC, P6067; Wei YT, 2020, IEEE T INFORM THEORY, V66, P5110, DOI 10.1109/TIT.2020.2981313; Xu G., 2018, INT C MACHINE LEARNI, V80, P5483; Yang Y, 2017, ANN STAT, V45, P991, DOI 10.1214/16-AOS1472; Zhang Yuchen, 2013, C LEARN THEOR, P592617	48	1	1	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR 3	2021	44	8					4280	4290		10.1109/TPAMI.2021.3063223	http://dx.doi.org/10.1109/TPAMI.2021.3063223			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	2S0MO	33656986				2022-12-18	WOS:000821496200001
J	Zhang, H; Sun, AX; Jing, W; Zhen, LL; Zhou, JT; Goh, RSM				Zhang, Hao; Sun, Aixin; Jing, Wei; Zhen, Liangli; Zhou, Joey Tianyi; Goh, Rick Siow Mong			Natural Language Video Localization: A Revisit in Span-Based Question Answering Framework	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Location awareness; Knowledge discovery; Task analysis; Standards; Feature extraction; Degradation; Semantics; Natural language video localization; single video moment retrieval; temporal sentence grounding; cross-modal retrieval; multimodal learning; span-based question answering; multi-paragraph question answering; cross-modal interaction		Natural Language Video Localization (NLVL) aims to locate a target moment from an untrimmed video that semantically corresponds to a text query. Existing approaches mainly solve the NLVL problem from the perspective of computer vision by formulating it as ranking, anchor, or regression tasks. These methods suffer from large performance degradation when localizing on long videos. In this work, we address the NLVL from a new perspective, i.e., span-based question answering (QA), by treating the input video as a text passage. We propose a video span localizing network (VSLNet), on top of the standard span-based QA framework (named VSLBase), to address NLVL. VSLNet tackles the differences between NLVL and span-based QA through a simple yet effective query-guided highlighting (QGH) strategy. QGH guides VSLNet to search for the matching video span within a highlighted region. To address the performance degradation on long videos, we further extend VSLNet to VSLNet-L by applying a multi-scale split-and-concatenation strategy. VSLNet-L first splits the untrimmed video into short clip segments; then, it predicts which clip segment contains the target moment and suppresses the importance of other segments. Finally, the clip segments are concatenated, with different confidences, to locate the target moment accurately. Extensive experiments on three benchmark datasets show that the proposed VSLNet and VSLNet-L outperform the state-of-the-art methods; VSLNet-L addresses the issue of performance degradation on long videos. Our study suggests that the span-based QA framework is an effective strategy to solve the NLVL problem.	[Zhang, Hao; Zhen, Liangli; Zhou, Joey Tianyi; Goh, Rick Siow Mong] ASTAR, Inst High Performance Comp, Singapore 138632, Singapore; [Zhang, Hao; Sun, Aixin] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore; [Jing, Wei] ASTAR, Inst Infocomm Res, Singapore 138632, Singapore	Agency for Science Technology & Research (A*STAR); A*STAR - Institute of High Performance Computing (IHPC); Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Agency for Science Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R)	Zhou, JT (corresponding author), ASTAR, Inst High Performance Comp, Singapore 138632, Singapore.	hzhang26@outlook.com; axsun@ntu.edu.sg; 21wjing@gmail.com; zhenll@ihpc.a-star.edu.sg; joey.tianyi.zhou@gmail.com; gohsm@ihpc.a-star.edu.sg	Hao, Zhang/ABE-3767-2021; Sun, Aixin/A-9852-2008; Zhen, Liangli/HCG-8485-2022	Hao, Zhang/0000-0002-2725-6458; Sun, Aixin/0000-0003-0764-4258; Zhen, Liangli/0000-0003-0481-3298	Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme [A18A1b0045, A18A2b0046]	Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme(Agency for Science Technology & Research (A*STAR))	This work was supported by the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme (Project #A18A1b0045 and #A18A2b0046).	Bahdanau D., 2015, P 3 INT C LEARN REPR; Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen JY, 2019, AAAI CONF ARTIF INTE, P8175; Chen Jingyuan, 2018, P C EMP METH NAT LAN, P162; Chen L, 2020, AAAI CONF ARTIF INTE, V34, P10551; Chen SX, 2019, AAAI CONF ARTIF INTE, P8199; Clark C, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P845; Daizong Liu, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P4070, DOI 10.1145/3394171.3414026; Escorcia Victor, 2019, ARXIV190712763; Gao JY, 2018, PROC CVPR IEEE, P6576, DOI 10.1109/CVPR.2018.00688; Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563; Ge RZ, 2019, IEEE WINT CONF APPL, P245, DOI 10.1109/WACV.2019.00032; Ghosh Soham, 2019, ARXIV190402755, P1984; He DL, 2019, AAAI CONF ARTIF INTE, P8393; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hendricks L. A., 2018, PROC C EMPIRICAL MET, P1380; Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618; Hinton, 2016, ARXIV PREPRINT ARXIV; Hu RH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6551; Huang H-Y, 2018, 6 INT C LEARN REPR I, P1; Le H, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P5612; Jonghwan Mun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10807, DOI 10.1109/CVPR42600.2020.01082; Kingma DP, 2017, P INT C LEARN REPR I; Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83; Lei Jie, 2020, P 58 ANN M ASS COMP, P8211, DOI DOI 10.18653/V1/2020.ACL-MAIN.730; Lin DW, 2019, LECT NOTES COMPUT SC, V11772, P121, DOI 10.1007/978-3-030-31624-2_10; Lin ZJ, 2020, AAAI CONF ARTIF INTE, V34, P11539; Liu BB, 2018, LECT NOTES COMPUT SC, V11207, P569, DOI 10.1007/978-3-030-01219-9_34; Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549; Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003; Lu CJ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5144; Mithun NC, 2019, PROC CVPR IEEE, P11584, DOI 10.1109/CVPR.2019.01186; Pang L, 2019, AAAI CONF ARTIF INTE, P6875; Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI DOI 10.3115/V1/D14-1162; Qu XY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4280, DOI 10.1145/3394171.3414053; Rajpurkar P., 2016, CORR, P2383, DOI [10.18653/v1/D16-1264, DOI 10.18653/V1/D16-1264]; Regneri M., 2013, TACL, V1, P25, DOI DOI 10.1162/TACL_A_00207; Rodriguez-Opazo C, 2020, IEEE WINT CONF APPL, P2453, DOI 10.1109/WACV45572.2020.9093328; Rohrbach M, 2012, LECT NOTES COMPUT SC, V7572, P144, DOI 10.1007/978-3-642-33718-5_11; Seo Minjoon, 2017, ABS161101603 ARXIV; Shao D, 2018, LECT NOTES COMPUT SC, V11213, P202, DOI 10.1007/978-3-030-01240-3_13; Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P333, DOI 10.1007/978-3-030-58548-8_20; Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vinyals O., 2015, ADV NEURAL INFORM PR, P2692; Wang JW, 2020, AAAI CONF ARTIF INTE, V34, P12168; Wang S., 2017, ICLR 2017; Wang Shuohang, 2016, P 2016 C N AM CHAPT, P1442, DOI DOI 10.18653/V1/N16-1170; Wang WN, 2019, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2019.00042; Wang WH, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P189, DOI 10.18653/v1/P17-1018; Wang YZ, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1918; Wu J, 2020, AAAI CONF ARTIF INTE, V34, P12386; Xiong C, 2017, 5 INT C LEARN REPR I; Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062; Yu A. W., 2018, PROC INT C LEARN REP; Yu Z, 2019, AAAI CONF ARTIF INTE, P9127; Yuan YT, 2019, ADV NEUR IN, V32; Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159; Zeng R., 2020, PROC IEEECVF C COMPU, p10 287; Zhang D, 2019, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2019.00134; Zhang Hao, 2020, ARXIV200413931, P6; Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870; Zhang SY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1230, DOI 10.1145/3343031.3350879; Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235; Zhou LW, 2020, AAAI CONF ARTIF INTE, V34, P13041	68	1	1	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB 23	2021	44	8					4252	4266		10.1109/TPAMI.2021.3060449	http://dx.doi.org/10.1109/TPAMI.2021.3060449			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	2Q6HN	33621165	Green Submitted			2022-12-18	WOS:000820521700003
J	Johnson, R; Zhang, T				Johnson, Rie; Zhang, Tong			A Framework of Composite Functional Gradient Methods for Generative Adversarial Models	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Generative adversarial models; functional gradient learning; neural networks; image generation		Generative adversarial networks (GAN) are trained through a minimax game between a generator and a discriminator to generate data that mimics observations. While being widely used, GAN training is known to be empirically unstable. This paper presents a new theory for generative adversarial methods that does not rely on the traditional minimax formulation. Our theory shows that with a strong discriminator, a good generator can be obtained by composite functional gradient learning, so that several distance measures (including the KL divergence and the JS divergence) between the probability distributions of real data and generated data are simultaneously improved after each functional gradient step until converging to zero. This new point of view leads to stable procedures for training generative models. It also gives a new theoretical insight into the original GAN. Empirical results on image generation show the effectiveness of our new method.	[Johnson, Rie] RJ Res Consulting, Tarrytown, NY 10591 USA; [Zhang, Tong] HKUST, Clear Water Bay, Hong Kong, Peoples R China	Hong Kong University of Science & Technology	Johnson, R (corresponding author), RJ Res Consulting, Tarrytown, NY 10591 USA.	riejohnson@gmail.com; tongzhang@tongzhang-ml.org	Zhang, Tong/HGC-1090-2022		 [ICML2018]		A shorter version of the paperwas presented at the 35th International Conference on Machine Learning (ICML2018) [19].	Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Amari S, 1997, ADV NEUR IN, V9, P127; [Anonymous], 2017, P INT C LEARN REPR; [Anonymous], 2011, P NIPS WORKSHOP DEEP; Arjovsky M., 2017, P INT C LEARN REPR; Berthelot D., 2017, ARXIV170310717CSLG; Brock A., 2019, P INT C LEARN REPR; Che T., 2017, P INT C LEARN REPR; DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I., 2017, INT C NEURAL INF PRO; Heusel M., 2017, P 31 INT C NEUR INF, P6629; Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202; Johnson R., 2018, P INT C MACH LEARN, P2371; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kingma D.P, P 3 INT C LEARNING R; Lazarow J, 2017, IEEE I CONF COMP VIS, P2793, DOI 10.1109/ICCV.2017.302; Lehtinen J, 2018, ICLR; Li CL, 2017, I C INTELL COMPUT TE, P203, DOI 10.1109/ICICTA.2017.52; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Mescheder L, 2018, PR MACH LEARN RES, V80; Mescheder Lars, 2017, ADV NEURAL INFORM PR, P1825; Miyato T., 2018, CGANS PROJECTION DIS, DOI [10.48550/arxiv.1802.05637, DOI 10.48550/ARXIV.1802.05637]; Mroueh Y., 2017, P ADV NEUR INF PROC, P2513; Nagarajan V, 2017, ADV NEUR IN, V30; Nitanda A., 2018, P INT C ART INT STAT, V21, P1008; Odena A, 2018, PR MACH LEARN RES, V80; Pascanu R., 2014, ICLR; Radford A., 2016, P INT C LEARN REPR; Roth K., 2017, ADV NEURAL INFORM PR, P2015; Roux N. L., 2007, ADV NEURAL INFORM PR, V20, P849; Salimans T., 2016, ADV NEUR IN, P2234; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Theis L., 2016, C TRACK P; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tolstikhin I. O., 2017, ADV NEURAL INFORM PR, P5424, DOI DOI 10.5555/3295222.3295294; Vinyals O., 2014, P DEEP LEARN REPR LE; Warde-Farley D., 2017, P INT C LEARN REPR; Wasserstein G.A.N., 2017, ARXIV170107875; Yang J., 2017, P INT C LEARN REPR P INT C LEARN REPR; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhang H, 2019, PROCEEDINGS OF 2019 IEEE 3RD INTERNATIONAL ELECTRICAL AND ENERGY CONFERENCE (CIEEC), P735, DOI 10.1109/CIEEC47146.2019.CIEEC-2019293; Zhang T, 2004, ANN STAT, V32, P56	48	1	1	4	17	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2021	43	1					17	32		10.1109/TPAMI.2019.2924428	http://dx.doi.org/10.1109/TPAMI.2019.2924428			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PC7WN	31247543				2022-12-18	WOS:000597206900002
J	Niu, YL; Zhang, HW; Lu, ZW; Chang, SF				Niu, Yulei; Zhang, Hanwang; Lu, Zhiwu; Chang, Shih-Fu			Variational Context: Exploiting Visual and Textual Context for Grounding Referring Expressions	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Grounding referring expression; variational Bayesian model; referring expression generation		We focus on grounding (i.e., localizing or linking) referring expressions in images, e.g., "largest elephant standing behind baby elephant". This is a general yet challenging vision-language task since it does not only require the localization of objects, but also the multimodal comprehension of context - visual attributes (e.g., "largest", "baby") and relationships (e.g., "behind") that help to distinguish the referent from other objects, especially those of the same category. Due to the exponential complexity involved in modeling the context associated with multiple image regions, existing work oversimplifies this task to pairwise region modeling by multiple instance learning. In this paper, we propose a variational Bayesian method, called Variational Context, to solve the problem of complex context modeling in referring expression grounding. Specifically, our framework exploits the reciprocal relation between the referent and context, i.e., either of them influences estimation of the posterior distribution of the other, and thereby the search space of context can be greatly reduced. In addition to reciprocity, our framework considers the semantic information of context, i.e., the referring expression can be reproduced based on the estimated context. We also extend the model to unsupervised setting where no annotation for the referent is available. Extensive experiments on various benchmarks show consistent improvement over state-of-the-art methods in both supervised and unsupervised settings.	[Niu, Yulei; Lu, Zhiwu] Renmin Univ China, Sch Informat, Beijing 100872, Peoples R China; [Niu, Yulei; Lu, Zhiwu] Renmin Univ China, Gaoling Sch Artificial Intelligence, Beijing 100872, Peoples R China; [Zhang, Hanwang] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore; [Chang, Shih-Fu] Columbia Univ, Dept Elect Engn, New York, NY 10027 USA	Renmin University of China; Renmin University of China; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Columbia University	Niu, YL (corresponding author), Renmin Univ China, Sch Informat, Beijing 100872, Peoples R China.; Niu, YL (corresponding author), Renmin Univ China, Gaoling Sch Artificial Intelligence, Beijing 100872, Peoples R China.	niu@ruc.edu.cn; hanwangzhang@gmail.com; zhiwu.lu@gmail.com; sfchang@ee.columbia.edu	Niu, Yulei/AAX-4556-2020	Lu, Zhiwu/0000-0001-6429-7956; Zhang, Hanwang/0000-0001-7374-8739	National Natural Science Foundation of China [61573363, 61832017]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was partially supported by National Natural Science Foundation of China (61573363 and 61832017), and NTU-Alibaba JRI.	Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Chen Xinpeng, 2018, ARXIV181203426; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Dai B, 2017, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR.2017.352; Das A, 2017, IEEE I CONF COMP VIS, P2970, DOI 10.1109/ICCV.2017.321; Deng CR, 2018, PROC CVPR IEEE, P7746, DOI 10.1109/CVPR.2018.00808; Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fox CW, 2012, ARTIF INTELL REV, V38, P85, DOI 10.1007/s10462-011-9236-8; Glorot X., 2010, PROC MACH LEARN RES, P249; Golland D., 2010, P 2010 C EMP METH NA, P410; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93; Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470; Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493; Kazemzadeh Sahar, 2014, P 2014 C EMP METH NA, P787, DOI DOI 10.3115/V1/D14-1086; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krahmer E, 2012, COMPUT LINGUIST, V38, P173, DOI 10.1162/COLI_a_00088; Li YK, 2018, PROC CVPR IEEE, P6116, DOI 10.1109/CVPR.2018.00640; Li YK, 2017, PROC CVPR IEEE, P7244, DOI 10.1109/CVPR.2017.766; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu JY, 2017, IEEE I CONF COMP VIS, P4866, DOI 10.1109/ICCV.2017.520; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; Lu JH, 2016, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING TECHNOLOGY (CSET2015), MEDICAL SCIENCE AND BIOLOGICAL ENGINEERING (MSBE2015), P289; Luo RT, 2017, PROC CVPR IEEE, P3125, DOI 10.1109/CVPR.2017.333; Makhzani A., 2015, ARXIV151105644; Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9; Mitchell Margaret, 2010, P 6 INT NAT LANG GEN, P95; Mitchell Margaret, 2013, P 2013 C N AM CHAPT; Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Plummer BA, 2017, IEEE I CONF COMP VIS, P1946, DOI 10.1109/ICCV.2017.213; Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49; Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093; Schuster Sebastian, 2015, P 4 WORKSH VIS LANG, P70, DOI DOI 10.18653/V1/W15-2812; Sohn Kihyuk, 2015, ADV NEURAL INFORM PR, P3483, DOI DOI 10.5555/2969442.2969628; Sun QR, 2017, PROC CVPR IEEE, P435, DOI 10.1109/CVPR.2017.54; Thomason J., 2017, P 1 WORKSH LANG ROB, P20; van Deemter Kees, 2006, P 4 INT C NAT LANG G, P2874; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Weaver Lex., 2001, P 17 C UNC ART INT U, P538; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xiao FY, 2017, PROC CVPR IEEE, P5253, DOI 10.1109/CVPR.2017.558; Xu X, 2015, IEEE ICC, P2048, DOI 10.1109/ICC.2015.7248627; Xue J, 2016, SYM REL DIST SYST, P91, DOI [10.1109/SRDS.2016.19, 10.1109/SRDS.2016.021]; Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47; Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142; Yu LC, 2017, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2017.375; Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5; Zhang HW, 2018, PROC CVPR IEEE, P4158, DOI 10.1109/CVPR.2018.00437; Zhang HW, 2017, IEEE I CONF COMP VIS, P4243, DOI 10.1109/ICCV.2017.454; Zhang Hanwang, 2017, PROC CVPR IEEE, P5532, DOI [DOI 10.1109/CVPR.2017.331, DOI 10.1109/CVPR.2018.00611]; Zhao Z, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3518; Zhuang BH, 2018, PROC CVPR IEEE, P4252, DOI 10.1109/CVPR.2018.00447; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	62	1	1	2	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2021	43	1					347	359		10.1109/TPAMI.2019.2926266	http://dx.doi.org/10.1109/TPAMI.2019.2926266			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PC7WN	31283493	Green Submitted			2022-12-18	WOS:000597206900023
J	Bennamoun, M; Guo, YL; Tombari, F; Youcef-Toumi, K; Nishino, K				Bennamoun, Mohammed; Guo, Yulan; Tombari, Federico; Youcef-Toumi, Kamal; Nishino, Ko			Guest Editors' Introduction to the Special Issue on RGB-D Vision: Methods and Applications	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material						Special issues and sections; Computer vision; Three-dimensional displays; Cameras; Image color analysis			[Bennamoun, Mohammed] Univ Western Australia, Dept Comp Sci & Software Engn, Crawley, WA 6009, Australia; [Guo, Yulan] Natl Univ Def Technol, Coll Elect Sci & Technol, Changsha 410073, Hunan, Peoples R China; [Guo, Yulan] Sun Yat Sen Univ, Sch Elect & Commun Engn, Guangzhou 510006, Guangdong, Peoples R China; [Tombari, Federico] Google, Mountain View, CA 94043 USA; [Tombari, Federico] Tech Univ Munich, D-80333 Munich, Germany; [Youcef-Toumi, Kamal] MIT, Mech Engn Dept, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Nishino, Ko] Kyoto Univ, Dept Intelligence Sci & Technol, Grad Sch Informat, Kyoto 6068501, Japan	University of Western Australia; National University of Defense Technology - China; Sun Yat Sen University; Google Incorporated; Technical University of Munich; Massachusetts Institute of Technology (MIT); Kyoto University	Bennamoun, M (corresponding author), Univ Western Australia, Dept Comp Sci & Software Engn, Crawley, WA 6009, Australia.	mohammed.bennamoun@uwa.edu.au; yulan.guo@nudt.edu.cn; tombari@in.tum.de; youcef@mit.edu; kon@i.kyoto-u.ac.jp	Bennamoun, Mohammed/C-2789-2013; Guo, Yulan/E-7102-2014	Bennamoun, Mohammed/0000-0002-6603-3257; YOUCEF-TOUMI, KAMAL/0000-0001-6755-1534; Nishino, Ko/0000-0002-3534-3447; Guo, Yulan/0000-0001-7051-841X; Tombari, Federico/0000-0001-5598-5212					0	1	1	1	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2020	42	10					2329	2332		10.1109/TPAMI.2020.2976227	http://dx.doi.org/10.1109/TPAMI.2020.2976227			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NL5QY		Bronze			2022-12-18	WOS:000567471300001
J	Hong, WX; Tang, XY; Meng, JJ; Yuan, JS				Hong, Weixiang; Tang, Xueyan; Meng, Jingjing; Yuan, Junsong			Asymmetric Mapping Quantization for Nearest Neighbor Search	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Vector quantization; nearest neighbour search; image retrieval; distributed optimization		Nearest neighbor search is a fundamental problem in computer vision and machine learning. The straightforward solution, linear scan, is both computationally and memory intensive in large scale high-dimensional cases, hence is not preferable in practice. Therefore, there have been a lot of interests in algorithms that perform approximate nearest neighbor (ANN) search. In this paper, we propose a novel addition-based vector quantization algorithm, Asymmetric Mapping Quantization (AMQ), to efficiently conduct ANN search. Unlike existing addition-based quantization methods that suffer from handling the problem caused by the norm of database vector, we map the query vector and database vector using different mapping functions to transform the computation of L-2 distance to inner product similarity, thus do not need to evaluate the norm of database vector. Moreover, we further propose Distributed Asymmetric Mapping Quantization (DAMQ) to enable AMQ to work on very large dataset by distributed learning. Extensive experiments on approximate nearest neighbor search and image retrieval validate the merits of the proposed AMQ and DAMQ.	[Hong, Weixiang] Natl Univ Singapore, Singapore 119077, Singapore; [Tang, Xueyan] Nanyang Technol Univ, Singapore 639798, Singapore; [Meng, Jingjing; Yuan, Junsong] SUNY Buffalo, Buffalo, NY 14260 USA	National University of Singapore; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; State University of New York (SUNY) System; State University of New York (SUNY) Buffalo	Hong, WX (corresponding author), Natl Univ Singapore, Singapore 119077, Singapore.	weixiang.hong@outlook.com; asxytang@ntu.edu.sg; jmeng2@buffalo.edu; jsyuan@buffalo.edu	Tang, Xueyan/A-3703-2011		University at Buffalo	University at Buffalo	This work is supported in part by start-up funds from University at Buffalo.	Babenko A, 2017, IEEE I CONF COMP VIS, P4895, DOI 10.1109/ICCV.2017.523; Babenko A, 2016, PROC CVPR IEEE, P2055, DOI 10.1109/CVPR.2016.226; Babenko A, 2015, PROC CVPR IEEE, P4240, DOI 10.1109/CVPR.2015.7299052; Babenko A, 2014, PROC CVPR IEEE, P931, DOI 10.1109/CVPR.2014.124; Chang SY, 2014, IEEE DATA MINING, P60, DOI 10.1109/ICDM.2014.115; Corbett JC, 2013, ACM T COMPUT SYST, V31, DOI 10.1145/2491245; Du C., 2014, ARXIV14064966; Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240; Ghemawat S., 2003, Operating Systems Review, V37, P29, DOI 10.1145/1165389.945450; Greenhill S., 2007, P 15 ACM INT C MULT, P413; Heo JP, 2014, PROC CVPR IEEE, P2139, DOI 10.1109/CVPR.2014.274; Hestenes M. R., 1969, Journal of Optimization Theory and Applications, V4, P303, DOI 10.1007/BF00927673; Hong WX, 2018, AAAI CONF ARTIF INTE, P61; Hong WX, 2018, AAAI CONF ARTIF INTE, P69; Hong WX, 2018, IEEE T IMAGE PROCESS, V27, P4825, DOI 10.1109/TIP.2018.2846670; Hong WX, 2017, PROC CVPR IEEE, P6221, DOI 10.1109/CVPR.2017.659; Howard A, 2002, DISTRIBUTED AUTONOMOUS ROBOTIC SYSTEMS 5, P299; Huiskes Mark J, 2008, P 1 ACM INT C MULTIM, P39, DOI DOI 10.1145/1460096.1460104; Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24; Jegou H, 2011, INT CONF ACOUST SPEE, P861; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Li K, 2017, IEEE T PATTERN ANAL, V39, P1825, DOI 10.1109/TPAMI.2016.2610969; Liang JL, 2014, IEEE T IMAGE PROCESS, V23, P2528, DOI 10.1109/TIP.2014.2316373; Martinez J, 2016, LECT NOTES COMPUT SC, V9906, P137, DOI 10.1007/978-3-319-46475-6_9; NOCEDAL J, 1980, MATH COMPUT, V35, P773, DOI 10.1090/S0025-5718-1980-0572855-7; Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388; Perronnin F, 2007, PROC CVPR IEEE, P2272; Qi G.-J., 2009, P 17 ACM INT C MULT, P243; Qi GJ, 2012, PROC INT CONF DATA, P534, DOI 10.1109/ICDE.2012.77; Wang J, 2017, IEEE INFOCOM SER, DOI 10.1007/s12083-017-0556-6; Wang JD, 2019, IEEE T PATTERN ANAL, V41, P1308, DOI 10.1109/TPAMI.2018.2835468; Wang XJ, 2016, PROC CVPR IEEE, P2018, DOI 10.1109/CVPR.2016.222; Wang ZZ, 2018, IEEE T IMAGE PROCESS, V27, P4503, DOI 10.1109/TIP.2018.2839901; Wang ZZ, 2015, IEEE I CONF COMP VIS, P2866, DOI 10.1109/ICCV.2015.328; Wu X., 2017, ADV NEURAL INFORM PR, P5749; Ye J, 2020, IEEE T PATTERN ANAL, V42, P126, DOI 10.1109/TPAMI.2018.2874455; Ye J, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P99, DOI 10.1145/2671188.2749340; Zhang QF, 2014, INT CONF MACH LEARN, P807, DOI 10.1109/ICMLC.2014.7009713; Zhang T, 2015, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2015.7299085	41	1	1	1	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL 1	2020	42	7					1783	1790		10.1109/TPAMI.2019.2925347	http://dx.doi.org/10.1109/TPAMI.2019.2925347			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MC0DH	31251177				2022-12-18	WOS:000542967200020
J	Zunic, J; Rosin, PL				Zunic, Jovisa; Rosin, Paul L.			Measuring Shapes with Desired Convex Polygons	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Shape; Shape measurement; Extraterrestrial measurements; Linearity; Area measurement; Tuning; Rotation measurement; Shape; shape descriptors; shape measure; shape convexity; image processing; pattern recognition	PATTERN-RECOGNITION; ELLIPTICITY; CIRCULARITY; DESCRIPTOR; RETRIEVAL; ORIENTATION; INVARIANT; MOMENTS	In this paper we have developed a family of shape measures. All the measures from the family evaluate the degree to which a shape looks like a predefined convex polygon. A quite new approach in designing object shape based measures has been applied. In most cases such measures were defined by exploiting some shape properties. Such properties are optimized (e.g., maximized or minimized) by certain shapes and based on this, the new shape measures were defined. An illustrative example might be the shape circularity measure derived by exploiting the well-known result that the circle has the largest area among all the shapes with the same perimeter. Of course, there are many more such examples (e.g., ellipticity, linearity, elongation, and squareness measures are some of them). There are different approaches as well. In the approach applied here, no desired property is needed and no optimizing shape has to be found. We start from a desired convex polygon, and develop the related shape measure. The method also allows a tuning parameter. Thus, there is a new 2-fold family of shape measures, dependent on a predefined convex polygon, and a tuning parameter, that controls the measure's behavior. The measures obtained range over the interval (0,1] and pick the maximal possible value, equal to 1, if and only if the measured shape coincides with the selected convex polygon that was used to develop the particular measure. All the measures are invariant with respect to translations, rotations, and scaling transformations. An extension of the method leads to a family of new shape convexity measures.	[Zunic, Jovisa] Serbian Acad Sci, Math Inst, Belgrade 11000, Serbia; [Rosin, Paul L.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF10 3AT, Wales	Serbian Academy of Sciences & Arts; Cardiff University	Zunic, J (corresponding author), Serbian Acad Sci, Math Inst, Belgrade 11000, Serbia.	jovisa_zunic@mi.sanu.ac.rs; RosinPL@cardiff.ac.uk		Zunic, Jovisa/0000-0002-1271-4153; Rosin, Paul/0000-0002-4965-3884	Serbian Ministry of Sciences	Serbian Ministry of Sciences(Ministry of Education, Science & Technological Development, Serbia)	The work of J. Zunic is supported by the Serbian Ministry of Sciences. We thank Thanh Phuong Nguyen for providing code for computing polygonality.	Aktas MA, 2013, SIAM J IMAGING SCI, V6, P765, DOI 10.1137/120866026; [Anonymous], [No title captured]; [Anonymous], [No title captured]; Arandjelovic O, 2012, PATTERN RECOGN, V45, P92, DOI 10.1016/j.patcog.2011.07.002; ARKIN EM, 1991, IEEE T PATTERN ANAL, V13, P209, DOI 10.1109/34.75509; Boutsen L, 2001, PERCEPT PSYCHOPHYS, V63, P404, DOI 10.3758/BF03194408; Bowman ET, 2001, GEOTECHNIQUE, V51, P545, DOI 10.1680/geot.51.6.545.40465; Chambers CP, 2010, Q J POLIT SCI, V5, P27, DOI 10.1561/100.00009022; Du Buf H., 1999, Proceedings 10th International Conference on Image Analysis and Processing, P734, DOI 10.1109/ICIAP.1999.797682; Duda R.O., 2000, PATTERN CLASSIFICATI; DUPAIN Y, 1986, ARCH RATION MECH AN, V94, P155, DOI 10.1007/BF00280431; Elshoura SM, 2013, J VIS COMMUN IMAGE R, V24, P567, DOI 10.1016/j.jvcir.2013.03.021; FLUSSER J, 1993, PATTERN RECOGN, V26, P167, DOI 10.1016/0031-3203(93)90098-H; Flusser J., 2009, MOMENTS MOMENT INVAR; Gautama T, 2003, IEEE T MED IMAGING, V22, P636, DOI 10.1109/TMI.2003.812248; Grisan E, 2008, IEEE T MED IMAGING, V27, P310, DOI 10.1109/TMI.2007.904657; Horton BP, 2006, J FORENSIC SCI, V51, P643, DOI 10.1111/j.1556-4029.2006.00120.x; HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692; Huxley MN, 2006, COMP IMAG VIS, P221; Jain AK, 1998, PATTERN RECOGN, V31, P1369, DOI 10.1016/S0031-3203(97)00131-3; Kopanja L, 2016, MEASUREMENT, V92, P252, DOI 10.1016/j.measurement.2016.06.021; LEE DR, 1970, GEOGR REV, V60, P555, DOI 10.2307/213774; Lekshmi S, 2003, ASTRON ASTROPHYS, V405, P1163, DOI 10.1051/0004-6361:20030541; Mei Y, 2009, IEEE SIGNAL PROC LET, V16, P877, DOI 10.1109/LSP.2009.2026119; Misztal K, 2016, J MATH IMAGING VIS, V55, P136, DOI 10.1007/s10851-015-0618-4; Nikolic VN, 2017, CERAM INT, V43, P7497, DOI 10.1016/j.ceramint.2017.03.030; Paul CA, 2010, PALAEOGEOGR PALAEOCL, V291, P205, DOI 10.1016/j.palaeo.2010.02.030; PROFFITT D, 1982, PATTERN RECOGN, V15, P383, DOI 10.1016/0031-3203(82)90041-3; PUDIL P, 1994, PATTERN RECOGN LETT, V15, P1119, DOI 10.1016/0167-8655(94)90127-9; Rahtu E, 2006, IEEE T PATTERN ANAL, V28, P1501, DOI 10.1109/TPAMI.2006.175; Rosin PL, 2006, COMPUT VIS IMAGE UND, V103, P101, DOI 10.1016/j.cviu.2006.04.002; Rosin PL, 2011, J MATH IMAGING VIS, V39, P13, DOI 10.1007/s10851-010-0221-7; Rosin PL, 2003, MACH VISION APPL, V14, P172, DOI 10.1007/s00138-002-0118-6; Russell JC, 2009, ECOLOGY, V90, P2007, DOI 10.1890/08-1069.1; Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972; Sonka Milan, 2014, CENGAGE LEARNING, DOI DOI 10.1007/978-1-4899-3216-7; Stojmenovic M, 2008, PATTERN RECOGN, V41, P2503, DOI 10.1016/j.patcog.2008.01.013; Tangelder JWH, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P145, DOI 10.1109/SMI.2004.1314502; TEH CH, 1988, IEEE T PATTERN ANAL, V10, P496, DOI 10.1109/34.3913; Nguyen TP, 2015, IEEE T IMAGE PROCESS, V24, P305, DOI 10.1109/TIP.2014.2370954; Tool AQ, 1910, PHYS REV, V31, P1, DOI 10.1103/PhysRevSeriesI.31.1; Wang B, 2011, OPT COMMUN, V284, P3504, DOI 10.1016/j.optcom.2011.03.063; Xie J, 2017, IEEE T PATTERN ANAL, V39, P1335, DOI 10.1109/TPAMI.2016.2596722; Xu D, 2008, PATTERN RECOGN, V41, P240, DOI 10.1016/j.patcog.2007.05.001; Zunic J, 2004, IEEE T PATTERN ANAL, V26, P923, DOI 10.1109/TPAMI.2004.19; Zunic J, 2016, J MATH IMAGING VIS, V56, P125, DOI 10.1007/s10851-016-0638-8; Zunic J, 2010, PATTERN RECOGN, V43, P47, DOI 10.1016/j.patcog.2009.06.017	48	1	1	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN 1	2020	42	6					1394	1407		10.1109/TPAMI.2019.2898830	http://dx.doi.org/10.1109/TPAMI.2019.2898830			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	LR3TM	30762528	Green Accepted			2022-12-18	WOS:000535615700008
J	Meng, GF; Pan, CH; Xiang, SM; Wu, Y				Meng, Gaofeng; Pan, Chunhong; Xiang, Shiming; Wu, Ying			Baselines Extraction from Curved Document Images via Slope Fields Recovery	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Estimation; Image segmentation; Layout; Distortion; Strips; Image quality; Degradation; Document image processing; curved baselines extraction; slope fields recovery; geometric distortion rectification	TEXT; SEGMENTATION; RECTIFICATION; ROBUST; SET	Baselines estimation is a critical preprocessing step for many tasks of document image processing and analysis. The problem is very challenging due to arbitrarily complicated page layouts and various types of image quality degradations. This paper proposes a method based on slope fields recovery for curved baseline extraction from a distorted document image captured by a hand-held camera. Our method treats the curved baselines as the solution curves of an ordinary differential equation defined on a slope field. By assuming the page shape is a smooth and developable surface, we investigate a type of intrinsic geometric constraints of baselines to estimate the latent slope field. The curved baselines are finally obtained by solving an ordinary differential equation through the Euler method. Unlike the traditional text-lines based methods, our method is free from text-lines detection and segmentation. It can exploit multiple visual cues other than horizontal text-lines available in images for baselines extraction and is quite robust to document scripts, various types of image quality degradation (e.g., image distortion, blur and non-uniform illumination), large areas of non-textual objects and complex page layouts. Extensive experiments on synthetic and real-captured document images are implemented to evaluate the performance of the proposed method.	[Meng, Gaofeng; Pan, Chunhong; Xiang, Shiming] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Zhongguancun East Rd 95, Beijing 100190, Peoples R China; [Wu, Ying] Northwestern Univ, 2145 Sheridan Rd, Evanston, IL 60208 USA	Chinese Academy of Sciences; Institute of Automation, CAS; Northwestern University	Xiang, SM (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Zhongguancun East Rd 95, Beijing 100190, Peoples R China.	gfmeng@nlpr.ia.ac.cn; chpan@nlpr.ia.ac.cn; smxiang@nlpr.ia.ac.cn; yingwu@eecs.northwestern.edu		Koochak, Atousa/0000-0001-6547-2728	National Natural Science Foundation of China [91646207, 61573352, 61802407]; US National Science Foundation [IIS1217302, IIS-1619078]; Army Research Office ARO [W911NF-16-1-0138]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); US National Science Foundation(National Science Foundation (NSF)); Army Research Office ARO	We would like to thank the anonymous reviewers and editor for their valuable comments and suggestions. This work was supported in part by the National Natural Science Foundation of China under Grants 91646207, 61573352, 61802407, US National Science Foundation under grant IIS1217302, IIS-1619078 and the Army Research Office ARO W911NF-16-1-0138.	Arvanitopoulos N, 2014, INT CONF FRONT HAND, P726, DOI 10.1109/ICFHR.2014.127; Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461; Bar-Yosef Itay, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P1161, DOI 10.1109/ICDAR.2009.191; Basu S, 2007, PATTERN RECOGN, V40, P1825, DOI 10.1016/j.patcog.2006.10.002; Bukhari SS, 2009, P 3 INT WORKSH CAM B, P34; Bukhari SS, 2011, PROC INT CONF DOC, P579, DOI 10.1109/ICDAR.2011.122; Cao HG, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P228, DOI 10.1109/ICCV.2003.1238346; Chakraborty D, 2016, PATTERN RECOGN LETT, V74, P74, DOI 10.1016/j.patrec.2016.02.003; Dohm S, 2020, J CHEM THEORY COMPUT, V16, P2002, DOI 10.1021/acs.jctc.9b01266; dos Santos Rodolfo P., 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P651, DOI 10.1109/ICDAR.2009.183; Fu B., 2007, P 2 INTERNATIONALWOR, P63; Fujimoto K, 2007, PROC INT CONF DOC, P267; Gatos B, 2007, PROC INT CONF DOC, P989; Il Koo H, 2010, LECT NOTES COMPUT SC, V6312, P421; Koo HI, 2016, IEEE T IMAGE PROCESS, V25, P5358, DOI 10.1109/TIP.2016.2607418; Koo HI, 2012, IEEE T IMAGE PROCESS, V21, P1169, DOI 10.1109/TIP.2011.2166972; Li Y, 2008, IEEE T PATTERN ANAL, V30, P1313, DOI 10.1109/TPAMI.2007.70792; Liang J., 2005, International Journal on Document Analysis and Recognition, V7, P84, DOI 10.1007/s10032-004-0138-z; Liang J, 2005, PROC CVPR IEEE, P338; Liang J, 2008, IEEE T PATTERN ANAL, V30, P591, DOI 10.1109/TPAMI.2007.70724; Likforman-Sulem L, 2007, INT J DOC ANAL RECOG, V9, P123, DOI 10.1007/s10032-006-0023-z; Lu SJ, 2005, IMAGE VISION COMPUT, V23, P541, DOI 10.1016/j.imavis.2005.01.003; Masalovitch A., 2007, P INT WORKSH CAM BAS, P45; Meng GF, 2015, IEEE I CONF COMP VIS, P3925, DOI 10.1109/ICCV.2015.447; Meng GF, 2012, IEEE T PATTERN ANAL, V34, P707, DOI 10.1109/TPAMI.2011.151; Meng GF, 2010, IEEE T IMAGE PROCESS, V19, P1837, DOI 10.1109/TIP.2010.2045677; Nagy G, 2000, IEEE T PATTERN ANAL, V22, P38, DOI 10.1109/34.824820; Nikolova M, 2005, SIAM J SCI COMPUT, V27, P937, DOI 10.1137/030600862; Pal U, 2003, PROC INT CONF DOC, P1128; Papavassiliou V, 2010, PATTERN RECOGN, V43, P369, DOI 10.1016/j.patcog.2009.05.007; Pilu M, 2001, PROC CVPR IEEE, P363; Razak Z, 2008, INT J COMPUT SCI NET, V8, P12; Reddy J.N., 1993, INTRO FINITE ELEMENT; Saabni R, 2014, PATTERN RECOGN LETT, V35, P23, DOI 10.1016/j.patrec.2013.07.007; Schneider DC, 2007, PROC INT CONF DOC, P113; Shafait F, 2007, 2 INT WORKSH CAM BAS, P181; Shafait F, 2008, PROC SPIE, V6815, DOI 10.1117/12.767755; Stamatopoulos N, 2011, IEEE T IMAGE PROCESS, V20, P910, DOI 10.1109/TIP.2010.2080280; Tian YD, 2011, PROC CVPR IEEE, P377, DOI 10.1109/CVPR.2011.5995540; Ulges A, 2005, PROC INT CONF DOC, P1001, DOI 10.1109/ICDAR.2005.90; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Wu JC, 2008, MACH VISION APPL, V19, P195, DOI 10.1007/s00138-007-0092-0; Yin F, 2009, PATTERN RECOGN, V42, P3146, DOI 10.1016/j.patcog.2008.12.013; Yu B, 1996, PATTERN RECOGN, V29, P1599, DOI 10.1016/0031-3203(96)00020-9; Zhang Z, 2003, PROC INT CONF DOC, P589; Zhang Z, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P977, DOI 10.1109/ICIP.2002.1039138	46	1	1	0	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2020	42	4					793	808		10.1109/TPAMI.2018.2886900	http://dx.doi.org/10.1109/TPAMI.2018.2886900			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	LE2GI	30571616	hybrid			2022-12-18	WOS:000526541100003
J	Shahlaei, D; Blanz, V				Shahlaei, Davoud; Blanz, Volker			Hierarchical Bayesian Inverse Lighting of Portraits with a Virtual Light Stage	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Lighting; Face; Three-dimensional displays; Geometry; Solid modeling; Light sources; Estimation; Inverse lighting; 3D morphable model; single face image; virtual light stage; hierarchical Bayesian optimization; hyperparameters; generative model	FACE RECOGNITION; REFLECTANCE; IMAGE; DECOMPOSITION; MODELS; SKIN	From a single RGB image of an unknown face, taken under unknown conditions, we estimate a physically plausible lighting model. First, the 3D geometry and texture of the face are estimated by fitting a 3D Morphable Model to the 2D input. With this estimated 3D model and a Virtual Light Stage (VLS), we generate a gallery of images of the face with all the same conditions, but different lighting. We consider non-lambertian reflectance and non-convex geometry to handle more realistic illumination effects in complex lighting conditions. Our hierarchical Bayesian approach automatically suppresses inconsistencies between the model and the input. It estimates the RGB values for the light sources of a VLS to reconstruct the input face with the estimated 3D face model. We discuss the relevance of the hierarchical approach to this minimally constrained inverse rendering problem and show how the hyperparameters can be controlled to improve the results of the algorithm for complex effects, such as cast shadows. Our algorithm is a contribution to single image face modeling and analysis, provides information about the imaging condition and facilitates realistic reconstruction of the input image, relighting, lighting transfer and lighting design.	[Shahlaei, Davoud; Blanz, Volker] Univ Siegen, ETI, D-57068 Siegen, Nrw, Germany	Universitat Siegen	Shahlaei, D (corresponding author), Univ Siegen, ETI, D-57068 Siegen, Nrw, Germany.	davoud.shahlaei@uni-siegen.de; volker.blanz@uni-siegen.de			German Research Foundation (DFG) [GRK 1564]	German Research Foundation (DFG)(German Research Foundation (DFG))	This work was funded by the German Research Foundation (DFG) as part of the research training group GRK 1564 `Imaging New Modalities.'	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Aldrian O, 2013, IEEE T PATTERN ANAL, V35, P1080, DOI 10.1109/TPAMI.2012.206; [Anonymous], [No title captured]; [Anonymous], [No title captured]; [Anonymous], [No title captured]; [Anonymous], [No title captured]; [Anonymous], [No title captured]; [Anonymous], [No title captured]; Belhumeur PN, 1998, INT J COMPUT VISION, V28, P245, DOI 10.1023/A:1008005721484; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983; Blanz V, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P293, DOI 10.1109/TDPVT.2004.1335212; Chen D, 2009, BIRTH NUMERICAL ANAL, P109, DOI [10.1142/9789812836267_0008, DOI 10.1142/9789812836267_0008]; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S1064827596304010; Conde MH, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P226, DOI 10.1109/ICCVW.2015.38; Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855; Dessein A, 2017, COMPUT GRAPH FORUM, V36, P95, DOI 10.1111/cgf.12997; Dessein A, 2015, IEEE I CONF COMP VIS, P3898, DOI 10.1109/ICCV.2015.444; Dessein A, 2014, IEEE IMAGE PROC, P2031, DOI 10.1109/ICIP.2014.7025407; Fuchs M, 2005, IEEE T VIS COMPUT GR, V11, P296, DOI 10.1109/TVCG.2005.47; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Graham P, 2013, COMPUT GRAPH FORUM, V32, P335, DOI 10.1111/cgf.12053; HOOKE R, 1961, J ACM, V8, P212, DOI 10.1145/321062.321069; Igarashi T, 2007, FOUND TRENDS COMPUT, V3, P1, DOI 10.1561/0600000013; Jensen HW, 2002, ACM T GRAPHIC, V21, P576, DOI 10.1145/566570.566619; Jimenez J, 2015, COMPUT GRAPH FORUM, V34, P188, DOI 10.1111/cgf.12529; Jimenez J, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1609967.1609970; Li C, 2014, LECT NOTES COMPUT SC, V8693, P218, DOI 10.1007/978-3-319-10602-1_15; Li JY, 2013, INT CONF MEAS, P1, DOI 10.1109/ICMTMA.2013.11; Li Y, 2006, INT C PATT RECOG, P408; LUCY LB, 1974, ASTRON J, V79, P745, DOI 10.1086/111605; MohammadDjafari A, 1996, FUND THEOR, V79, P135; Moses Y., 1994, Computer Vision - ECCV'94. Third European Conference on Computer Vision. Proceedings. Vol.I, P286; Panagopoulos A, 2011, PROC CVPR IEEE, P673, DOI 10.1109/CVPR.2011.5995585; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Peyre G, 2006, INT J COMPUT VISION, V69, P145, DOI 10.1007/s11263-006-6859-3; Ramamoorthi R, 2001, COMP GRAPH, P117, DOI 10.1145/383259.383271; RICHARDSON WH, 1972, J OPT SOC AM, V62, P55, DOI 10.1364/JOSA.62.000055; Romdhani S, 2005, PROC CVPR IEEE, P986; Romdhani S, 2006, P IEEE, V94, P1977, DOI 10.1109/JPROC.2006.886019; Schumacher Matthaeus, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163124; Shahlaei Davoud, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163128; Shahlaei D, 2016, IEEE IMAGE PROC, P1579, DOI 10.1109/ICIP.2016.7532624; Sim T, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P53, DOI 10.1109/AFGR.2002.1004130; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wang Y, 2007, PROCEEDINGS OF THE SIXTH IEEE INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS, P3; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wen Z, 2003, PROC CVPR IEEE, P158; Weyrich T, 2006, ACM T GRAPHIC, V25, P1013, DOI 10.1145/1141911.1141987; Zhang L, 2006, IEEE T PATTERN ANAL, V28, P351, DOI 10.1109/TPAMI.2006.53	50	1	1	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR 1	2020	42	4					865	879		10.1109/TPAMI.2019.2891638	http://dx.doi.org/10.1109/TPAMI.2019.2891638			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	LE2GI	30629493				2022-12-18	WOS:000526541100007
J	Sarvadevabhatla, RK; Surya, S; Mittal, T; Babu, RV				Sarvadevabhatla, Ravi Kiran; Surya, Shiv; Mittal, Trisha; Babu, R. Venkatesh			Pictionary-Style Word Guessing on Hand-Drawn Object Sketches: Dataset, Analysis and Deep Network Models	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Games; Computational modeling; Visualization; Task analysis; Knowledge discovery; Robots; Deep learning; pictionary; games; sketch; visual question answering		The ability of intelligent agents to play games in human-like fashion is popularly considered a benchmark of progress in Artificial Intelligence. In our work, we introduce the first computational model aimed at Pictionary, the popular word-guessing social game. We first introduce Sketch-QA, a guessing task. Styled after Pictionary, Sketch-QA uses incrementally accumulated sketch stroke sequences as visual data. Sketch-QA involves asking a fixed question ("What object is being drawn?") and gathering open-ended guess-words from human guessers. We analyze the resulting dataset and present many interesting findings therein. To mimic Pictionary-style guessing, we propose a deep neural model which generates guess-words in response to temporally evolving human-drawn object sketches. Our model even makes human-like mistakes while guessing, thus amplifying the human mimicry factor. We evaluate our model on the large-scale guess-word dataset generated via Sketch-QA task and compare with various baselines. We also conduct a Visual Turing Test to obtain human impressions of the guess-words generated by humans and our model. Experimental results demonstrate the promise of our approach for Pictionary and similarly themed games.	[Sarvadevabhatla, Ravi Kiran; Surya, Shiv; Mittal, Trisha; Babu, R. Venkatesh] Indian Inst Sci, Dept Computat & Data Sci, Video Analyt Lab, Bangalore 560012, Karnataka, India	Indian Institute of Science (IISC) - Bangalore	Sarvadevabhatla, RK (corresponding author), Indian Inst Sci, Dept Computat & Data Sci, Video Analyt Lab, Bangalore 560012, Karnataka, India.	ravika@gmail.com; shivparat@gmail.com; mittaltrisha22@gmail.com; venky@iisc.ac.in		Sarvadevabhatla, Ravi Kiran/0000-0003-4134-1154	Qualcomm Innovation Fellowship	Qualcomm Innovation Fellowship	An earlier brief version of this work was presented in AAAI-18 [1]. Ravi Kiran Sarvadevabhatla was supported by Qualcomm Innovation Fellowship 2016.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2015, ARXIV150705738; [Anonymous], 2015, ARXIV150107873; [Anonymous], 2007, P DIGRA 2007 C SIT P; [Anonymous], DEEP BLUE VERSUS KAS; [Anonymous], P 32 AAAI C ART INT; [Anonymous], P HUM FACT ERG SOC A; [Anonymous], ENCHANT SPELLCHECKER; [Anonymous], VISUAL ANAL VISUAL M; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Baugh LA, 2010, ENCYCLOPEDIA OF BEHAVIORAL NEUROSCIENCE, VOL 1: A-G, P27; Branson S, 2010, LECT NOTES COMPUT SC, V6314, P438, DOI 10.1007/978-3-642-15561-1_32; CHAN JC, 1991, EDUC PSYCHOL MEAS, V51, P531, DOI 10.1177/0013164491513002; Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856; Das A, 2017, IEEE I CONF COMP VIS, P2970, DOI 10.1109/ICCV.2017.321; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540; Fay N, 2013, COGNITIVE SCI, V37, P1356, DOI 10.1111/cogs.12048; Frome Andrea, 2013, NEURIPS; Gao H., 2015, ADV NEURAL INFORM PR, V28, P2296, DOI DOI 10.1145/2733373.2807418; Geman D, 2015, P NATL ACAD SCI USA, V112, P3618, DOI 10.1073/pnas.1422953112; Groen M, 2012, COMPUT HUM BEHAV, V28, P1575, DOI 10.1016/j.chb.2012.03.019; Halacsy Peter, 2007, P 45 ANN M ASS COMP, P209; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Holmes BM, 2009, SOCIAL PSYCHOLOGY: NEW RESEARCH, P117; Lev G, 2016, LECT NOTES COMPUT SC, V9910, P833, DOI 10.1007/978-3-319-46466-4_50; Ma SG, 2016, PROC CVPR IEEE, P1942, DOI 10.1109/CVPR.2016.214; Malinowski M., 2014, ARXIV14108027; Mikolov T., 2013, WORKSHOP TRACK P; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Qin T, 2008, INFORM PROCESS MANAG, V44, P838, DOI 10.1016/j.ipm.2007.07.016; Ren MY, 2015, ADV NEUR IN, V28; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Saggar M, 2015, SCI REP-UK, V5, DOI 10.1038/srep10894; Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954; Sarvadevabhatla R.K., 2016, P P 24 ACM INT C MUL, P247; Sarvadevabhatla RK, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P271, DOI 10.1145/2733373.2806230; Schneider RG, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661231; Seddati O, 2015, INT WORK CONTENT MUL; Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; TESAURO G, 1994, NEURAL COMPUT, V6, P215, DOI 10.1162/neco.1994.6.2.215; Ullman S, 2016, P NATL ACAD SCI USA, V113, P2744, DOI 10.1073/pnas.1513198113; Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515; Von Ahn Luis, 2004, P SIGCHI C HUM FACT, P319, DOI DOI 10.1145/985692.985733; WILCOXON F, 1946, J ECON ENTOMOL, V39, P269, DOI 10.1093/jee/39.2.269; WU ZB, 1994, 32ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P133; Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28; Xu K, 2015, PR MACH LEARN RES, V37, P2048	52	1	1	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2020	42	1					221	231		10.1109/TPAMI.2018.2877996	http://dx.doi.org/10.1109/TPAMI.2018.2877996			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	JV3VQ	30369439				2022-12-18	WOS:000502294300017
J	Shamai, G; Zibulevsky, M; Kimmel, R				Shamai, Gil; Zibulevsky, Michael; Kimmel, Ron			Efficient Inter-Geodesic Distance Computation and Fast Classical Scaling	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Manifolds; Complexity theory; Interpolation; Matrix decomposition; Surface reconstruction; Shape; Laplace equations; Geodesic distance; pairwise geodesics; dimensionality reduction; flat embedding; fast classical scaling	EIGENMAPS	Multidimensional scaling (MDS) is a dimensionality reduction tool used for information analysis, data visualization and manifold learning. Most MDS procedures embed data points in low-dimensional euclidean (flat) domains, such that distances between the points are as close as possible to given inter-point dissimilarities. We present an efficient solver for classical scaling, a specific MDS model, by extrapolating the information provided by distances measured from a subset of the points to the remainder. The computational and space complexities of the new MDS methods are thereby reduced from quadratic to quasi-linear in the number of data points. Incorporating both local and global information about the data allows us to construct a low-rank approximation of the inter-geodesic distances between the data points. As a by-product, the proposed method allows for efficient computation of geodesic distances.	[Shamai, Gil; Zibulevsky, Michael; Kimmel, Ron] Technion Israel Inst Technol, Comp Sci Dept, IL-3200003 Haifa, Israel	Technion Israel Institute of Technology	Shamai, G (corresponding author), Technion Israel Inst Technol, Comp Sci Dept, IL-3200003 Haifa, Israel.	gil.shamai@gmail.com; mzib@cs.technion.ac.il; ron@cs.technion.ac.il			Israel Ministry of Science [3-14719]; Technion Hiroshi Fujiwara Cyber Security Research Center; Israel cyber directorate	Israel Ministry of Science(Ministry of Science, Technology and Space (MOST), Israel); Technion Hiroshi Fujiwara Cyber Security Research Center; Israel cyber directorate	This research was partially supported by the Israel Ministry of Science, grant number 3-14719, and by the Technion Hiroshi Fujiwara Cyber Security Research Center and Israel cyber directorate.	Aflalo Y, 2013, P NATL ACAD SCI USA, V110, P18052, DOI 10.1073/pnas.1308708110; Arcolano N, 2010, INT CONF ACOUST SPEE, P3606, DOI 10.1109/ICASSP.2010.5495906; Belkin M, 2002, ADV NEUR IN, V14, P585; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; BORG I., 2005, MODERN MULTIDIMENSIO, P207; Botsch M, 2008, IEEE T VIS COMPUT GR, V14, P213, DOI 10.1109/TVCG.2007.1054; Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1; Campen M, 2011, COMPUT GRAPH FORUM, V30, P623, DOI 10.1111/j.1467-8659.2011.01896.x; Civril A, 2007, LECT NOTES COMPUT SC, V4372, P30; Crane K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516977; De Silva Vin, 2002, NIPS 02 P 15 INT C N, V15, P705, DOI DOI 10.5555/2968618.2968708; Dijkstra EW, 1959, NUMER MATH, V1, P269, DOI 10.1007/BF01386390; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100; Drineas P, 2006, SIAM J COMPUT, V36, P184, DOI 10.1137/S0097539704442702; Drury HA, 1996, J COGNITIVE NEUROSCI, V8, P1, DOI 10.1162/jocn.1996.8.1.1; Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902; Fang HR, 2012, OPTIM METHOD SOFTW, V27, P695, DOI 10.1080/10556788.2011.643888; HOCHBAUM DS, 1985, MATH OPER RES, V10, P180, DOI 10.1287/moor.10.2.180; Kimmel R, 1998, P NATL ACAD SCI USA, V95, P8431, DOI 10.1073/pnas.95.15.8431; Kohonen T, 1998, NEUROCOMPUTING, V21, P1, DOI 10.1016/S0925-2312(98)00030-7; Lian Z., 2015, P EUR WORKSH 3D OBJ, P107; Lipman Y, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1805964.1805971; Liu R, 2006, LECT NOTES COMPUT SC, V4035, P172; Mahoney MW, 2009, P NATL ACAD SCI USA, V106, P697, DOI [10.1073/pnas.0803205105, 10.1073/pnas.0803205106]; Panozzo D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461935; Pinkall U., 1993, EXPT MATH, V2, P15, DOI DOI 10.1080/10586458.1993.10504266; Platt J. C., 2005, P 10 INT WORKSH ART, P261; Pless R, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1433; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Rubner Y., 2001, PERCEPTUAL METRICS I; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; SCHWARTZ EL, 1989, IEEE T PATTERN ANAL, V11, P1005, DOI 10.1109/34.35506; Schweitzer H, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P566, DOI 10.1109/ICCV.2001.937676; Shamai G., 2017, P IEEE C COMP VIS PA, P6410; Shamai G., 2015, P 2015 EUR WORKSH 3D, P71; Shamai G, 2015, IEEE I CONF COMP VIS, P2255, DOI 10.1109/ICCV.2015.260; Stein O, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3186564; Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Xin S.-Q., 2012, PROC ACM SIGGRAPH S, P31; Yu H., 2012, ADV INF SCI SERV SCI, V4, P370; Yu Kai, 2009, ADV NEURAL INFORM PR, P2223; Zhou Y, 2013, IEEE SIGNAL PROC LET, V20, P335, DOI 10.1109/LSP.2013.2246513; Zigelman G, 2002, IEEE T VIS COMPUT GR, V8, P198, DOI 10.1109/2945.998671	44	1	1	0	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN 1	2020	42	1					74	85		10.1109/TPAMI.2018.2877961	http://dx.doi.org/10.1109/TPAMI.2018.2877961			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JV3VQ	30369438	Green Submitted			2022-12-18	WOS:000502294300006
J	Gao, ZN; Wang, L; Jojic, N; Niu, ZX; Zheng, NN; Hua, G				Gao, Zhanning; Wang, Le; Jojic, Nebojsa; Niu, Zhenxing; Zheng, Nanning; Hua, Gang			Video Imprint	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Feature extraction; Cognition; Computational modeling; Correlation; Neural networks; Layout; Event videos; feature alignment; feature aggregation; reasoning network	EVENT DETECTION; IMAGE FEATURES; CLASSIFICATION; RECOGNITION	A new unified video analytics framework (ER3) is proposed for complex event retrieval, recognition and recounting, based on the proposed video imprint representation, which exploits temporal correlations among image features across video frames. With the video imprint representation, it is convenient to reverse map back to both temporal and spatial locations in video frames, allowing for both key frame identification and key areas localization within each frame. In the proposed framework, a dedicated feature alignment module is incorporated for redundancy removal across frames to produce the tensor representation, i.e., the video imprint. Subsequently, the video imprint is individually fed into both a reasoning network and a feature aggregation module, for event recognition/recounting and event retrieval tasks, respectively. Thanks to its attention mechanism inspired by the memory networks used in language modeling, the proposed reasoning network is capable of simultaneous event category recognition and localization of the key pieces of evidence for event recounting. In addition, the latent structure in our reasoning network highlights the areas of the video imprint, which can be directly used for event recounting. With the event retrieval task, the compact video representation aggregated from the video imprint contributes to better retrieval results than existing state-of-the-art methods.	[Gao, Zhanning; Wang, Le; Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China; [Jojic, Nebojsa; Hua, Gang] Microsoft Res, Redmond, WA 98052 USA; [Niu, Zhenxing] Alibaba Grp, Hangzhou 311121, Zhejiang, Peoples R China	Xi'an Jiaotong University; Microsoft; Alibaba Group	Wang, L (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.	zhanninggao@gmail.com; lewang@mail.xjtu.edu.cn; jojic@microsoft.com; zhenxing.nzx@alibaba-inc.com; nnzheng@mail.xjtu.edu.cn; ganghua@gmail.com		Gao, Zhanning/0000-0003-2031-2805; Wang, Le/0000-0001-6636-6396	National Key R&D Program of China [2017YFA0700800]; National Natural Science Foundation of China [61629301, 61773312, 91748208, 61503296]; China Postdoctoral Science Foundation [2017T100752, 2015M572563]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); China Postdoctoral Science Foundation(China Postdoctoral Science Foundation)	This work was supported partly by National Key R&D Program of China Grant 2017YFA0700800, National Natural Science Foundation of China Grants 61629301, 61773312, 91748208, and 61503296, China Postdoctoral Science Foundation Grants 2017T100752 and 2015M572563.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Baillie M, 2003, LECT NOTES COMPUT SC, V2728, P300; Baraldi L, 2018, PROC CVPR IEEE, P7804, DOI 10.1109/CVPR.2018.00814; Bhattacharya S, 2014, PROC CVPR IEEE, P2243, DOI 10.1109/CVPR.2014.287; Bilen H, 2018, IEEE T PATTERN ANAL, V40, P2799, DOI 10.1109/TPAMI.2017.2769085; Cao LL, 2012, LECT NOTES COMPUT SC, V7573, P688, DOI 10.1007/978-3-642-33709-3_49; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chang XJ, 2017, IEEE T PATTERN ANAL, V39, P1617, DOI 10.1109/TPAMI.2016.2608901; Chang X, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P581, DOI 10.1145/2733373.2806218; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Douze M, 2013, IEEE I CONF COMP VIS, P1825, DOI 10.1109/ICCV.2013.229; Tran D, 2014, IEEE T PATTERN ANAL, V36, P404, DOI 10.1109/TPAMI.2013.137; Gan C, 2016, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2016.106; Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872; Gao ZN, 2017, PROC CVPR IEEE, P2107, DOI 10.1109/CVPR.2017.227; Gao ZN, 2016, IEEE T MULTIMEDIA, V18, P1661, DOI 10.1109/TMM.2016.2568748; Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; HEILBRON FC, 2015, PROC CVPR IEEE, P961, DOI DOI 10.1109/CVPR.2015.7298698; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Jegou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235; Jegou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Jiang Y.-G., 2011, ICMR, P1; Jiang YG, 2018, IEEE T PATTERN ANAL, V40, P352, DOI 10.1109/TPAMI.2017.2670560; Jiang YG, 2013, INT J MULTIMED INF R, V2, P73, DOI 10.1007/s13735-012-0024-2; Jojic N, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P34; Jojic N., 2011, P 27 C UNC ART INT, P547; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lai KT, 2014, PROC CVPR IEEE, P2251, DOI 10.1109/CVPR.2014.288; Lai KT, 2014, LECT NOTES COMPUT SC, V8691, P675, DOI 10.1007/978-3-319-10578-9_44; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Ma ZG, 2013, PROC CVPR IEEE, P2627, DOI 10.1109/CVPR.2013.339; Nagel M., 2015, BMVC, V2, P6; Over P., 2014, P TRECVID, P52; Perina A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1985, DOI 10.1109/CVPR.2011.5995742; Perina A, 2015, IEEE T PATTERN ANAL, V37, P2374, DOI 10.1109/TPAMI.2015.2424864; Perronnin F, 2015, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR.2015.7298998; Poullot S, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P381, DOI 10.1145/2733373.2806228; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Revaud J, 2013, PROC CVPR IEEE, P2459, DOI 10.1109/CVPR.2013.318; Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Sukhbaatar S, 2015, ADV NEUR IN, V28; Sun C, 2014, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2014.329; Sun C, 2013, IEEE I CONF COMP VIS, P913, DOI 10.1109/ICCV.2013.453; Szeliski R., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P251, DOI 10.1145/258734.258861; Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009; Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56; Tsai C. -Y., 2014, P INT C MULT RETR, P419; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Weston Jason, 2016, ICLR; Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222; Wu ZX, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P791, DOI 10.1145/2964284.2964328; Wu ZX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P167, DOI 10.1145/2647868.2654931; Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789; Yandex Artem Babenko, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1269, DOI 10.1109/ICCV.2015.150; Yang J., 2017, P IEEE C COMP VIS PA, P2492; Zha S.X., 2015, P BRIT MACH VIS C BM, P60; Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI 10.1109/ICCV.2017.233; Zhang QL, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P561, DOI 10.1145/2733373.2806224	62	1	1	1	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2019	41	12					3086	3099		10.1109/TPAMI.2018.2866114	http://dx.doi.org/10.1109/TPAMI.2018.2866114			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JQ0XI	30130178	Green Submitted			2022-12-18	WOS:000498677600020
J	Talker, L; Moses, Y; Shimshoni, I				Talker, Lior; Moses, Yael; Shimshoni, Ilan			Estimating the Number of Correct Matches Using Only Spatial Order	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature matching; RANSAC; spatial order; correct matches	SEARCH; MODEL	Correctly matching feature points in a pair of images is an important preprocessing step for many computer vision applications. In this paper we propose an efficient method for estimating the number of correct matches without explicitly computing them. To this end, we propose to analyze the set of matches using the spatial order of the features, as projected to the $x$x-axis of the image. The set of features in each image is thus represented by a sequence, and analyzed using the Kendall and Spearman Footrule distance metrics between permutations. This result is interesting in its own right. Moreover, we demonstrate three useful applications of our method: (i) a new halting condition for RANSAC based epipolar geometry estimation methods, (ii) discarding spatially unrelated image pairs in the Structure-from-Motion pipeline, and (iii) computing the probability that a given match is correct based on the rank of the features within the sequences. Our experiments on a large number of synthetic and real data demonstrate the effectiveness of our method. For example, the running time of the image matching stage in the Structure-from-Motion pipeline may be reduced by about 90 percent while preserving about 85 percent of the image pairs with spatial overlap.	[Talker, Lior] Univ Haifa, Informat Syst, Fac Nat Sci, IL-3498838 Haifa, Israel; [Moses, Yael] Interdisciplinary Ctr, Comp Sci, IL-46150 Herzliyya, Israel; [Shimshoni, Ilan] Univ Haifa, Dept Informat Syst, IL-31905 Haifa, Israel	University of Haifa; Reichman University; University of Haifa	Talker, L (corresponding author), Univ Haifa, Informat Syst, Fac Nat Sci, IL-3498838 Haifa, Israel.	ltalke01@campus.haifa.ac.il; yael@idc.ac.il; ishimshoni@mis.haifa.ac.il		Shimshoni, Ilan/0000-0002-5276-0242; Talker, Lior/0000-0002-6633-9790	Israel Science Foundation [930/12]; Israeli Ministry of Science [3-8744]; Israeli Innovation Authority in the Ministry of Economy and Industry	Israel Science Foundation(Israel Science Foundation); Israeli Ministry of Science(Ministry of Science, Technology and Space (MOST), Israel); Israeli Innovation Authority in the Ministry of Economy and Industry	This work was partially supported by the Israel Science Foundation, grant no. 930/12, by the Israeli Ministry of Science, grant no. 3-8744, and by the Israeli Innovation Authority in the Ministry of Economy and Industry.	Altwaijry H., 2016, P BRIT MACH VIS C, P3539; [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.445; [Anonymous], 2008, INTRO PROBABILITY TH; Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207; Arnold R. D., 1980, Proceedings of the Society of Photo-Optical Instrumentation Engineers, V238, P281; Avrithis Y, 2014, INT J COMPUT VISION, V107, P1, DOI 10.1007/s11263-013-0659-3; Baker H. H., 1982, DEPTH EDGE INTENSITY; Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Bobick AF, 1999, INT J COMPUT VISION, V33, P181, DOI 10.1023/A:1008150329890; Brahmachari AS, 2013, IEEE T PATTERN ANAL, V35, P755, DOI 10.1109/TPAMI.2012.227; Chum O, 2005, PROC CVPR IEEE, P220, DOI 10.1109/cvpr.2005.221; Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236; Chum O, 2008, IEEE T PATTERN ANAL, V30, P1472, DOI 10.1109/TPAMI.2007.70787; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; Cohen A, 2016, LECT NOTES COMPUT SC, V9907, P285, DOI 10.1007/978-3-319-46487-9_18; Cohen A, 2012, PROC CVPR IEEE, P1514, DOI 10.1109/CVPR.2012.6247841; Deza M., 1998, J COMBINATORICS INF; DIACONIS P, 1977, J ROY STAT SOC B MET, V39, P262, DOI 10.1111/j.2517-6161.1977.tb01624.x; Dwork C., 2001, P 10 INT C WORLD WID, P613, DOI [10.1145/371920.372165, DOI 10.1145/371920.372165]; Fragoso V, 2013, IEEE I CONF COMP VIS, P2472, DOI 10.1109/ICCV.2013.307; Goldman Y, 2017, IMAGE VISION COMPUT, V67, P16, DOI 10.1016/j.imavis.2017.09.006; Goshen L, 2008, IEEE T PATTERN ANAL, V30, P1230, DOI 10.1109/TPAMI.2007.70768; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hartmann W, 2014, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2014.9; Havlena M, 2010, LECT NOTES COMPUT SC, V6312, P100, DOI 10.1007/978-3-642-15552-9_8; Hirschmuller Heiko, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383248; Hu Z, 2016, 2016 PROGRESS IN ELECTROMAGNETICS RESEARCH SYMPOSIUM (PIERS), P134, DOI 10.1109/PIERS.2016.7734268; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Johns E, 2014, LECT NOTES COMPUT SC, V8690, P504, DOI 10.1007/978-3-319-10605-2_33; Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.2307/2332226; Kleinberg J., 2006, ALGORITHM DESIGN; Li XC, 2015, PROC CVPR IEEE, P5153, DOI 10.1109/CVPR.2015.7299151; Litman R, 2015, PROC CVPR IEEE, P5243, DOI 10.1109/CVPR.2015.7299161; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Meneghetti Giulia, 2015, Image Analysis. 19th Scandinavian Conference, SCIA 2015. Proceedings: LNCS 9127, P428, DOI 10.1007/978-3-319-19665-7_36; Mills S, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P416, DOI 10.1109/3DV.2015.54; Myatt D. R., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P458; Ozyesil O, 2017, ACTA NUMER, V26, P305, DOI 10.1017/S096249291700006X; Raguram R, 2013, IEEE T PATTERN ANAL, V35, P2022, DOI 10.1109/TPAMI.2012.257; Ramalingam S, 2015, PROC CVPR IEEE, P1238, DOI 10.1109/CVPR.2015.7298728; Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544; Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3; Schonberger JL, 2015, LECT NOTES COMPUT SC, V9358, P53, DOI 10.1007/978-3-319-24947-6_5; Schonberger JL, 2015, PROC CVPR IEEE, P1009, DOI 10.1109/CVPR.2015.7298703; Shah R, 2015, IEEE WINT CONF APPL, P278, DOI 10.1109/WACV.2015.44; SHAO H, 2003, COMPUT VIS LAB SWISS, V260, P6; Snavely N., 2008, P IEEE C COMP VIS PA, V1, P2, DOI DOI 10.1109/CVPR.2008.4587678; Steele J.M., 1995, DISCRETE PROBABILITY, V72, P111; Szeliski R., 2010, COMPUTER VISION ALGO, DOI DOI 10.1007/978-3-030-34372-9; Talker L., 2015, HAVE LOOK WHAT I SEE; Tolias G, 2016, INT J COMPUT VISION, V116, P247, DOI 10.1007/s11263-015-0810-4; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; Verri A., 1984, THESIS; Wilson K, 2014, LECT NOTES COMPUT SC, V8691, P61, DOI 10.1007/978-3-319-10578-9_5; Wu C., 2013, LINEAR TIME INCREMEN; Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28; YUILLE AL, 1984, GEN ORDERING CONSTRA; Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064; Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749	60	1	1	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2019	41	12					2846	2860		10.1109/TPAMI.2018.2869560	http://dx.doi.org/10.1109/TPAMI.2018.2869560			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JQ0XI	30207949				2022-12-18	WOS:000498677600005
J	Ye, JW; Ji, Y; Zhou, MY; Kang, SB; Yu, JY				Ye, Jinwei; Ji, Yu; Zhou, Mingyuan; Kang, Sing Bing; Yu, Jingyi			Content Aware Image Pre-Compensation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image deconvolution; pre-compensation; high contrast; ringing-free; non-linear tone mapping; saliency	CAMERA RESPONSE FUNCTIONS; THEORETICAL-ANALYSIS; ATTENTION; CONTRAST	The goal of image pre-compensation is to process an image such that after being convolved with a known kernel, will appear close to the sharp reference image. In a practical setting, the pre-compensated image has significantly higher dynamic range than the latent image. As a result, some form of tone mapping is needed. In this paper, we show how global tone mapping functions affect contrast and ringing in image pre-compensation. We further enhance contrast and reduce ringing by considering the visual saliency. Specifically, we prioritize contrast preservation in salient regions while tolerating more blurriness elsewhere. For quantitative analysis, we design new metrics to measure the contrast of an image with ringing. Specifically, we set out to find its "equivalent ringing-free" image that matches its intensity histogram and uses its contrast as the measure. We illustrate our approach on projector defocus compensation and visual acuity enhancement. Compared with the state-of-the-art, our approach significantly improves the contrast. We also perform user studies to demonstrate that our method can effectively improve the viewing experience for users with impaired vision.	[Ye, Jinwei] Louisiana State Univ, Div Comp Sci & Engn, Baton Rouge, LA 70803 USA; [Ji, Yu] Plex VR, Baton Rouge, LA 70820 USA; [Zhou, Mingyuan; Yu, Jingyi] Univ Delaware, Newark, DE 19716 USA; [Kang, Sing Bing] Microsoft Res, Redmond, WA 98052 USA; [Yu, Jingyi] Shanghai Tech Univ, Shanghai 201210, Peoples R China	Louisiana State University System; Louisiana State University; University of Delaware; Microsoft; ShanghaiTech University	Ye, JW (corresponding author), Louisiana State Univ, Div Comp Sci & Engn, Baton Rouge, LA 70803 USA.	jye@csc.lsu.edu; yu.ji@plex-vr.com; mzhou@udel.edu; sbkang@microsoft.com; yu@eecis.udel.edu		Zhou, Mingyuan/0000-0001-6722-1623; Ye, Jinwei/0000-0001-7780-7943	National Science Foundation [IIS-CAREER-0845268]	National Science Foundation(National Science Foundation (NSF))	This project was supported by the National Science Foundation under grant IIS-CAREER-0845268. J. Ye and Y. Ji contributed equally to this work.	Alonso M, 2003, P ANN INT IEEE EMBS, V25, P556, DOI 10.1109/IEMBS.2003.1279804; Alonso M.  Jr., 2004, ASSETS 2004. The Sixth International ACM SIGACCESS Conference on Computers and Accessibility, P126; Alonso M, 2008, INVERSE PROBL SCI EN, V16, P957, DOI 10.1080/17415970802082823; Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833; Brown M.S., 2006, P IEEE C COMP VIS PA, V2, P1956, DOI DOI 10.1109/CVPR.2006.145; Chen XG, 2012, LECT NOTES COMPUT SC, V7578, P333, DOI 10.1007/978-3-642-33786-4_25; Cho S, 2011, IEEE I CONF COMP VIS, P495, DOI 10.1109/ICCV.2011.6126280; Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491; Donoser M, 2009, IEEE I CONF COMP VIS, P817, DOI 10.1109/ICCV.2009.5459296; Farid H, 2001, IEEE T IMAGE PROCESS, V10, P1428, DOI 10.1109/83.951529; Fattal R, 2002, ACM T GRAPHIC, V21, P249; Fujii K, 2005, PROC CVPR IEEE, P814, DOI 10.1109/CVPR.2005.41; Grosse M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1805964.1805966; Harel J., 2006, PAPER PRESENTED INT, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073; Harel J., SALIENCY IMPLEMENTAT; Huang F.-C., 2011, UCBEECS2011162; Huang FC, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366204; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Ji Y, 2014, PROC CVPR IEEE, P3350, DOI 10.1109/CVPR.2014.428; Joshi N., 2008, CVPR, P1; Joshi N, 2009, PROC CVPR IEEE, P1550, DOI 10.1109/CVPRW.2009.5206802; Kenig T, 2010, IEEE T PATTERN ANAL, V32, P2191, DOI 10.1109/TPAMI.2010.45; Kim S, 2012, PROC CVPR IEEE, P25, DOI [10.1109/MMBIA.2012.6164736, 10.1109/CVPR.2012.6247654]; Krishnan D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531402; Levin A., 2006, ADV NEURAL INFORM PR, V19, P841; Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521; Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147; Mao LM, 2008, I C WIREL COMM NETW, P10001; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Masia B, 2013, COMPUT GRAPH-UK, V37, P1012, DOI 10.1016/j.cag.2013.10.003; Montalto C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2717307; More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105; Nayar S., 2004, P IEEE C COMP VIS PA, P425; Nayar S. K., 2003, P ICCV WORKSH PROJ, V3, P1; Oyamada Y., 2007, P IEEE C COMP VIS PA, P1; Pamplona VF, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185577; PELI E, 1990, J OPT SOC AM A, V7, P2032, DOI 10.1364/JOSAA.7.002032; Proakis J. G., 1995, DIGIT SIGNAL PROCESS, V3rd; RICHARDSON WH, 1972, J OPT SOC AM, V62, P55, DOI 10.1364/JOSA.62.000055; Rutishauser U, 2004, PROC CVPR IEEE, P37; Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672; Tai YW, 2013, IEEE T PATTERN ANAL, V35, P2498, DOI 10.1109/TPAMI.2013.40; WANG JY, 1980, APPL OPTICS, V19, P1510, DOI 10.1364/AO.19.001510; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xu L., 2014, INT C NEUR INF PROC, V27, P1790; Yitzhaky Y, 1998, J OPT SOC AM A, V15, P1512, DOI 10.1364/JOSAA.15.001512; Zhang L, 2006, ACM T GRAPHIC, V25, P907, DOI 10.1145/1141911.1141974	47	1	1	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2019	41	7					1545	1558		10.1109/TPAMI.2018.2839115	http://dx.doi.org/10.1109/TPAMI.2018.2839115			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	IC4XW	29994299				2022-12-18	WOS:000470972300002
J	Fouhey, DF; Gupta, A; Zisserman, A				Fouhey, David F.; Gupta, Abhinav; Zisserman, Andrew			From Images to 3D Shape Attributes	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						3D understanding; shape perception; attributes; convolutional neural networks	SURFACE; PERCEPTION; RECOGNITION; PICTORIAL; OBJECTS	Our goal in this paper is to investigate properties of 3D shape that can be determined from a single image. We define 3D shape attributes-generic properties of the shape that capture curvature, contact and occupied space. Our first objective is to infer these 3D shape attributes from a single image. A second objective is to infer a 3D shape embedding - a low dimensional vector representing the 3D shape. We study how the 3D shape attributes and embedding can be obtained from a single image by training a Convolutional Neural Network (CNN) for this task. We start with synthetic images so that the contribution of various cues and nuisance parameters can be controlled. We then turn to real images and introduce a large scale image dataset of sculptures containing 143K images covering 2197 works from 242 artists. For the CNN trained on the sculpture dataset we show the following: (i) which regions of the imaged sculpture are used by the CNN to infer the 3D shape attributes; (ii) that the shape embedding can be used to match previously unseen sculptures largely independent of viewpoint; and (iii) that the 3D attributes generalize to images of other (non-sculpture) object classes.	[Fouhey, David F.] Univ Calif Berkeley, EECS Dept, Berkeley, CA 94720 USA; [Gupta, Abhinav] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15232 USA; [Zisserman, Andrew] Univ Oxford, Dept Engn Sci, Visual Geometry Grp, Oxford OX1 3PJ, England	University of California System; University of California Berkeley; Carnegie Mellon University; University of Oxford	Fouhey, DF (corresponding author), Univ Calif Berkeley, EECS Dept, Berkeley, CA 94720 USA.	dfouhey@eecs.berkeley.edu; abhinavg@cs.cmu.edu; az@robots.ox.ac.uk			EPSRC Programme [Seebibyte EP/M013774/1, ONR MURI N000141612007]; Intel/US National Science Foundation Visual and Experiential Computing award [IIS-1539099]; NDSEG fellowship; EPSRC [EP/M013774/1] Funding Source: UKRI	EPSRC Programme(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Intel/US National Science Foundation Visual and Experiential Computing award(National Science Foundation (NSF)); NDSEG fellowship; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	Financial support for this and our previous work was provided by the EPSRC Programme Grant Seebibyte EP/M013774/1, ONR MURI N000141612007, Intel/US National Science Foundation Visual and Experiential Computing award IIS-1539099 and a NDSEG fellowship to David Fouhey. The authors thank: Olivia Wiles for tools for dataset cleaning; Omkar Parkhi, Xiaolong Wang, and Phillip Isola for helpful conversations; and NVIDIA for GPU donations.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arandjelovic R., 2012, P 2 ACM INT C MULT M, P3; Arandjelovic R, 2011, IEEE I CONF COMP VIS, P375, DOI 10.1109/ICCV.2011.6126265; Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49; Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Bo LF, 2011, IEEE INT C INT ROBOT, P821, DOI 10.1109/IROS.2011.6048717; Bulthoff H., 1989, J OPT SOC AM OPT IMA, V5, P1749; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Cutting J. E., 1995, PERCEPTION SPACE MOT, P69, DOI DOI 10.1016/B978-012240530-3/50005-5; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Eigen David, 2014, NEURIPS; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fan H, 2016, IEEE C COMP VIS PATT; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010; Ferrari V., 2007, P INT C NEUR INF PRO; Forsyth DA, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P447, DOI 10.1109/ICCV.2001.937659; Fouhey DF, 2016, PROC CVPR IEEE, P1516, DOI 10.1109/CVPR.2016.168; Fouhey DF, 2013, IEEE I CONF COMP VIS, P3392, DOI 10.1109/ICCV.2013.421; Fouhey DF, 2014, LECT NOTES COMPUT SC, V8694, P687, DOI 10.1007/978-3-319-10599-4_44; Gibson James J., 1950, PERCEPTION VISUAL WO, P3; Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29; Gong BQ, 2013, IEEE T MULTIMEDIA, V15, P369, DOI 10.1109/TMM.2012.2231059; Guo RQ, 2013, IEEE I CONF COMP VIS, P2144, DOI 10.1109/ICCV.2013.266; Gupta A, 2010, LECT NOTES COMPUT SC, V6314, P482, DOI 10.1007/978-3-642-15561-1_35; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Hane C, 2014, PROC CVPR IEEE, P652, DOI 10.1109/CVPR.2014.89; Hedau V, 2012, PROC CVPR IEEE, P2807; Hedau V, 2009, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2009.5459411; Hoiem D, 2005, IEEE I CONF COMP VIS, P654; Hoiem D, 2008, INT J COMPUT VISION, V80, P3, DOI 10.1007/s11263-008-0137-5; Huang G.B., 2008, WORKSHOP FACESREAL L; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; JOHNSTON A, 1994, VISION RES, V34, P3005, DOI 10.1016/0042-6989(94)90273-9; Kar A, 2015, PROC CVPR IEEE, P1966, DOI 10.1109/CVPR.2015.7298807; Koenderink J., 1990, SOLID SHAPE; KOENDERINK JJ, 1992, PERCEPT PSYCHOPHYS, V52, P487, DOI 10.3758/BF03206710; Koenderink JJ, 1996, PERCEPT PSYCHOPHYS, V58, P163, DOI 10.3758/BF03211873; KOENDERINK JJ, 1992, IMAGE VISION COMPUT, V10, P557, DOI 10.1016/0262-8856(92)90076-F; KOENDERINK JJ, 1984, PERCEPTION, V13, P321, DOI 10.1068/p130321; KOENDERINK JJ, 1995, IMAGE VISION COMPUT, V13, P321, DOI 10.1016/0262-8856(95)99719-H; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250; Ladicky L, 2014, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2014.19; Ladicky L, 2012, INT J COMPUT VISION, V100, P122, DOI 10.1007/s11263-011-0489-0; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Lee D., 2010, P ADV NEUR INF PROC, P1288; Lee DC, 2009, PROC CVPR IEEE, P2136, DOI 10.1109/CVPRW.2009.5206872; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Madison C, 2001, PERCEPT PSYCHOPHYS, V63, P187, DOI 10.3758/BF03194461; NORMAN JF, 1995, PERCEPT PSYCHOPHYS, V57, P826, DOI 10.3758/BF03206798; Norman JF, 1996, PERCEPTION, V25, P381, DOI 10.1068/p250381; Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Rock J, 2015, PROC CVPR IEEE, P2484, DOI 10.1109/CVPR.2015.7298863; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Schultz M., 2004, P C NEUR INF PROC SY; SHEPARD RN, 1971, SCIENCE, V171, P701, DOI 10.1126/science.171.3972.701; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Simonyan K., 2014, P WORKSH INT C LEARN; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Tarr MJ, 1998, COGNITION, V67, P1, DOI 10.1016/S0010-0277(98)00026-2; Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30; Ummenhofer B, 2013, IEEE I CONF COMP VIS, P969, DOI 10.1109/ICCV.2013.124; Van Gool, 2008, P INT C CONT BAS IM, P47, DOI DOI 10.1145/1386352.1386363; Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320; Welchman AE, 2005, NAT NEUROSCI, V8, P820, DOI 10.1038/nn1461; Yan X., 2016, P C NEUR INF PROC SY; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhou TH, 2015, PROC CVPR IEEE, P1191, DOI 10.1109/CVPR.2015.7298723; ZISSERMAN A, 1989, IMAGE VISION COMPUT, V7, P38, DOI 10.1016/0262-8856(89)90018-8; Zoran D, 2015, IEEE I CONF COMP VIS, P388, DOI 10.1109/ICCV.2015.52	76	1	1	2	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	2019	41	1					93	106		10.1109/TPAMI.2017.2782810	http://dx.doi.org/10.1109/TPAMI.2017.2782810			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	HD3QX	29990013	Green Submitted, hybrid			2022-12-18	WOS:000452434800008
J	Chen, L; Casperson, D; Gao, LX				Chen, Liang; Casperson, David; Gao, Lixin			Ghost Numbers	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image set classification; deep learning; experiment; ghost number		We comment on a paper describing an algorithm for image set classification. Following the general practice in computer vision research, the performance of the algorithm was evaluated on benchmarks in order to support the claim of its advantage over other algorithms in the literature. We have examined the reported data of experiences on two datasets, and found that many numbers are not a possible answer regardless how the random partitions were selected and regardless how the algorithms performed in each partition. Our finding suggests that the experimental results in the paper ("Deep Reconstruction Models for Image Set Classification", IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 37, no. 4, pp. 713-727, April 2015) has serious flaws to the extent that all the experimental results should be re-examined.	[Chen, Liang; Gao, Lixin] Wenzhou Univ, Coll Math & Informat Sci, Wenzhou 325027, Zhejiang, Peoples R China; [Chen, Liang; Casperson, David; Gao, Lixin] Univ Northern British Columbia, Dept Comp Sci, Prince George, BC V2N 4Z9, Canada	Wenzhou University; University of Northern British Columbia	Chen, L (corresponding author), Wenzhou Univ, Coll Math & Informat Sci, Wenzhou 325027, Zhejiang, Peoples R China.; Chen, L (corresponding author), Univ Northern British Columbia, Dept Comp Sci, Prince George, BC V2N 4Z9, Canada.	chen.liang.97@gmail.com; David.Casperson@unbc.ca; lxgao@wzu.edu.cn			Wenzhou University in China; National Natural Science Foundation of China [61473212]; Zhejiang Provincial Natural Science Foundation of China [LY17F030003]; Discovery Grant of NSERC in Canada [RGPIN-2016-06631]	Wenzhou University in China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Zhejiang Provincial Natural Science Foundation of China(Natural Science Foundation of Zhejiang Province); Discovery Grant of NSERC in Canada	The authors of this work are partially supported by a research grant for Adjunct Professors at Wenzhou University in China, National Natural Science Foundation of China (Grant No. 61473212), Zhejiang Provincial Natural Science Foundation of China (Grant No. LY17F030003) and Discovery Grant of NSERC in Canada (Grant No. RGPIN-2016-06631).	Hayat M, 2015, IEEE T PATTERN ANAL, V37, P713, DOI 10.1109/TPAMI.2014.2353635; Hayat M, 2014, PROC CVPR IEEE, P1915, DOI 10.1109/CVPR.2014.246	2	1	1	0	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2018	40	10					2538	2539		10.1109/TPAMI.2017.2757489	http://dx.doi.org/10.1109/TPAMI.2017.2757489			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GS7IZ	28961104				2022-12-18	WOS:000443875500020
J	Modolo, D; Ferrari, V				Modolo, Davide; Ferrari, Vittorio			Learning Semantic Part-Based Models from Google Images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Part detection; web learning; curriculum learning		We propose a technique to train semantic part-based models of object classes from Google Images. Our models encompass the appearance of parts and their spatial arrangement on the object, specific to each viewpoint. We learn these rich models by collecting training instances for both parts and objects, and automatically connecting the two levels. Our framework works incrementally, by learning from easy examples first, and then gradually adapting to harder ones. A key benefit of this approach is that it requires no manual part location annotations. We evaluate our models on the challenging PASCAL-Part dataset [1] and show how their performance increases at every step of the learning, with the final models more than doubling the performance of directly training from images retrieved by querying for part names (from 12.9 to 27.2 AP). Moreover, we show that our part models can help object detection performance by enriching the R-CNN detector with parts.	[Modolo, Davide; Ferrari, Vittorio] Univ Edinburgh, IPAB Inst, Edinburgh EH8 9YL, Midlothian, Scotland	University of Edinburgh	Modolo, D (corresponding author), Univ Edinburgh, IPAB Inst, Edinburgh EH8 9YL, Midlothian, Scotland.	davide.modolo@gmail.com; vittoferrari@gmail.com			ERC Starting Grant VisCul	ERC Starting Grant VisCul	Support by ERC Starting Grant VisCul.	[Anonymous], 2014, 2014 IEEE C COMP VIS, P580, DOI [10.1109/CVPR.2014.81, DOI 10.1109/CVPR.2014.81]; Arbelaez P, 2012, PROC CVPR IEEE, P3378, DOI 10.1109/CVPR.2012.6248077; Bourdev L, 2009, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2009.5459303; Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254; Chen XL, 2015, IEEE I CONF COMP VIS, P1431, DOI 10.1109/ICCV.2015.168; Chen XL, 2013, IEEE I CONF COMP VIS, P1409, DOI 10.1109/ICCV.2013.178; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Divvala SK, 2014, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2014.412; Endres I, 2013, PROC CVPR IEEE, P939, DOI 10.1109/CVPR.2013.126; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fergus R, 2005, IEEE I CONF COMP VIS, P1816; Gkioxari G, 2015, IEEE I CONF COMP VIS, P2470, DOI 10.1109/ICCV.2015.284; Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642; Jia Y., 2013, CAFFE OPEN SOURCE CO; Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kuettel D, 2012, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2012.6247721; Li LJ, 2010, INT J COMPUT VISION, V88, P147, DOI 10.1007/s11263-009-0265-6; Li QN, 2013, PROC CVPR IEEE, P851, DOI 10.1109/CVPR.2013.115; Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775; Liu JX, 2014, LECT NOTES COMPUT SC, V8690, P456, DOI 10.1007/978-3-319-10605-2_30; Liu JX, 2012, LECT NOTES COMPUT SC, V7572, P172, DOI 10.1007/978-3-642-33718-5_13; Novotny D., 2016, P 12 EUR C COMP VIS, P218; Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092; Rosenfeld A, 2011, IEEE I CONF COMP VIS, P1371, DOI 10.1109/ICCV.2011.6126391; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Schroff F, 2011, IEEE T PATTERN ANAL, V33, P754, DOI 10.1109/TPAMI.2010.133; Sun M, 2011, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2011.6126309; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Ukita N, 2012, PROC CVPR IEEE, P3154, DOI 10.1109/CVPR.2012.6248049; Vijayanarasimhan S., 2008, P IEEE C COMP VIS PA, p[1, 1], DOI DOI 10.1109/CVPR.2008.4587632; Wang HY, 2015, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2015.7298788; Zhang N, 2013, IEEE I CONF COMP VIS, P729, DOI 10.1109/ICCV.2013.96; Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54	34	1	1	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	2018	40	6					1502	1509		10.1109/TPAMI.2017.2724029	http://dx.doi.org/10.1109/TPAMI.2017.2724029			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	GE9BK	28692966	Green Accepted, Green Submitted			2022-12-18	WOS:000431524700017
J	Hughes, NJ; Goodhill, GJ				Hughes, Nicholas J.; Goodhill, Geoffrey J.			Estimating Cortical Feature Maps with Dependent Gaussian Processes	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Gaussian processes; multitask learning; neuroimaging; visual cortical maps	OCULAR-DOMINANCE COLUMNS; VISUAL-CORTEX; SPATIAL-FREQUENCY; ORIENTATION; CAT; ORGANIZATION	A striking example of brain organisation is the stereotyped arrangement of cell preferences in the visual cortex for edges of particular orientations in the visual image. These "orientation preference maps" appear to have remarkably consistent statistical properties across many species. However fine scale analysis of these properties requires the accurate reconstruction of maps from imaging data which is highly noisy. A new approach for solving this reconstruction problem is to use Bayesian Gaussian process methods, which produce more accurate results than classical techniques. However, so far this work has not considered the fact that maps for several other features of visual input coexist with the orientation preference map and that these maps have mutually dependent spatial arrangements. Here we extend the Gaussian process framework to the multiple output case, so that we can consider multiple maps simultaneously. We demonstrate that this improves reconstruction of multiple maps compared to both classical techniques and the single output approach, can encode the empirically observed relationships, and is easily extendible. This provides the first principled approach for studying the spatial relationships between feature maps in visual cortex.	[Hughes, Nicholas J.; Goodhill, Geoffrey J.] Univ Queensland, Queensland Brain Inst, St Lucia, Qld 4072, Australia; [Hughes, Nicholas J.; Goodhill, Geoffrey J.] Univ Queensland, Sch Math & Phys, St Lucia, Qld 4072, Australia	University of Queensland; University of Queensland	Hughes, NJ (corresponding author), Univ Queensland, Queensland Brain Inst, St Lucia, Qld 4072, Australia.; Hughes, NJ (corresponding author), Univ Queensland, Sch Math & Phys, St Lucia, Qld 4072, Australia.	nicholas.hughes1@uqconnect.edu.au; g.goodhill@uq.edu.au		Goodhill, Geoffrey/0000-0001-9789-9355				ANDERSON PA, 1988, J NEUROSCI, V8, P2183; Bach FR, 2003, J MACH LEARN RES, V3, P1, DOI 10.1162/153244303768966085; BARTFELD E, 1992, P NATL ACAD SCI USA, V89, P11905, DOI 10.1073/pnas.89.24.11905; BLASDEL GG, 1986, NATURE, V321, P579, DOI 10.1038/321579a0; BONHOEFFER T, 1991, NATURE, V353, P429, DOI 10.1038/353429a0; Boyle P., 2005, ADV NEURAL INFORM PR, P217; Carreira-Perpinan MA, 2005, CEREB CORTEX, V15, P1222, DOI 10.1093/cercor/bhi004; Cloherty SL, 2016, ELIFE, V5, DOI 10.7554/eLife.13911; Damianou Andreas, 2013, ARTIF INTELL, P207, DOI DOI 10.1002/NME.1296; ERWIN E, 1995, NEURAL COMPUT, V7, P425, DOI 10.1162/neco.1995.7.3.425; Giacomantonio CE, 2010, NEUROIMAGE, V52, P875, DOI 10.1016/j.neuroimage.2009.12.066; HUBEL DH, 1977, PROC R SOC SER B-BIO, V198, P1, DOI 10.1098/rspb.1977.0085; Hubener M, 1997, J NEUROSCI, V17, P9270; Hughes NJ, 2014, NEUROIMAGE, V95, P305, DOI 10.1016/j.neuroimage.2014.03.031; Issa NP, 2000, J NEUROSCI, V20, P8504, DOI 10.1523/JNEUROSCI.20-22-08504.2000; Kaschube M, 2010, SCIENCE, V330, P1113, DOI 10.1126/science.1194869; Kim DS, 1999, NEUROREPORT, V10, P2515, DOI 10.1097/00001756-199908200-00015; LEVAY S, 1978, J COMP NEUROL, V179, P223, DOI 10.1002/cne.901790113; Macke JH, 2011, NEUROIMAGE, V56, P570, DOI 10.1016/j.neuroimage.2010.04.272; MORTON RRA, 1966, J APPL PROBAB, V3, P559, DOI 10.2307/3212140; MOVSHON JA, 1978, J PHYSIOL-LONDON, V283, P101, DOI 10.1113/jphysiol.1978.sp012490; Nauhaus I, 2012, NAT NEUROSCI, V15, P1683, DOI 10.1038/nn.3255; OBERMAYER K, 1993, J NEUROSCI, V13, P4114; Pouratian N., 2002, BRAIN MAPPING METHOD, V2nd, P97; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; ROJER AS, 1990, BIOL CYBERN, V62, P381, DOI 10.1007/BF00197644; Sengpiel F, 1999, NAT NEUROSCI, V2, P727, DOI 10.1038/11192; Sirovich L, 2004, P NATL ACAD SCI USA, V101, P16941, DOI 10.1073/pnas.0407450101; Snelson E, 2004, ADV NEUR IN, V16, P337; Swindale NV, 2000, CEREB CORTEX, V10, P633, DOI 10.1093/cercor/10.7.633; Swindale NV, 1996, NETWORK-COMP NEURAL, V7, P161, DOI 10.1088/0954-898X/7/2/002; Titsias MK, 2010, P 13 INT C ARTIFICIA, V9, P844; Zapeda A, 2004, J NEUROSCI METH, V136, P1, DOI 10.1016/j.jneumeth.2004.02.025; Zhang F., 2005, NUMERICAL METHODS AL, V4, DOI DOI 10.1007/B105056	34	1	1	0	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2017	39	10					1918	1928		10.1109/TPAMI.2016.2624295	http://dx.doi.org/10.1109/TPAMI.2016.2624295			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	FF3NI	27831860				2022-12-18	WOS:000408807600002
J	Tariq, A; Karim, A; Foroosh, H				Tariq, Amara; Karim, Asim; Foroosh, Hassan			NELasso: Group-Sparse Modeling for Characterizing Relations Among Named Entities in News Articles	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Sparse group learning; LASSO; named entities; semantic network construction; news understanding		Named entities such as people, locations, and organizations play a vital role in characterizing online content. They often reflect information of interest and are frequently used in search queries. Although named entities can be detected reliably from textual content, extracting relations among them is more challenging, yet useful in various applications (e.g., news recommending systems). In this paper, we present a novel model and system for learning semantic relations among named entities from collections of news articles. We model each named entity occurrence with sparse structured logistic regression, and consider the words (predictors) to be grouped based on background semantics. This sparse group LASSO approach forces the weights of word groups that do not influence the prediction towards zero. The resulting sparse structure is utilized for defining the type and strength of relations. Our unsupervised system yields a named entities' network where each relation is typed, quantified, and characterized in context. These relations are the key to understanding news material over time and customizing newsfeeds for readers. Extensive evaluation of our system on articles from TIME magazine and BBC News shows that the learned relations correlate with static semantic relatedness measures like WLM, and capture the evolving relationships among named entities over time.	[Tariq, Amara] Forman Christian Coll, Dept Comp Sci, Lahore, Pakistan; [Karim, Asim] Lahore Univ Management & Sci, Dept Informat Technol, Lahore 54792, Pakistan; [Foroosh, Hassan] Univ Cent Florida, Dept Comp Sci, Orlando, FL 32816 USA	Lahore University of Management Sciences; State University System of Florida; University of Central Florida	Tariq, A (corresponding author), Forman Christian Coll, Dept Comp Sci, Lahore, Pakistan.	amaratariq@fccollege.edu.pk; akarim@lums.edu.pk; foroosh@cs.ucf.edu	Tariq, Amara/Z-1211-2019	Tariq, Amara/0000-0001-5932-2491	National Science Foundation [IIS-1212948]	National Science Foundation(National Science Foundation (NSF))	This work was supported in part by the National Science Foundation under grant number IIS-1212948.	Agichtein E., 2000, ACM 2000. Digital Libraries. Proceedings of the Fifth ACM Conference on Digital Libraries, P85, DOI 10.1145/336597.336644; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bollegala Danushka, 2010, P 19 INT C WORLD WID, p151 , DOI DOI 10.1145/1772690.1772707; Brown P. F., 1992, Computational Linguistics, V18, P467; Chen X, 2012, P 15 INT C ART INT S, P208; Corley C., 2005, P ACL WORKSHOP EMPIR, P13, DOI DOI 10.3115/1631862.1631865; Dai XY, 2011, IEEE INT C SEMANT CO, P189, DOI 10.1109/ICSC.2011.25; Doddington George, 2004, P LREC; Fader A., 2011, P C EMP METH NAT LAN, P1535, DOI DOI 10.1234/12345678; Fleiss L., 1981, STAT METHODS RATES P; Gabrilovich E., 2004, P INT C WORLD WIDE W, P482, DOI [10.1145/988672.988738, DOI 10.1145/988672.988738]; HASEGAWA T, 2004, P 42 ANN M ASS COMP; Hirano T., 2007, P 45 ANN M ACL DEM P, P157; Jin YZ, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P453, DOI 10.1145/2556195.2556230; Jin YZ, 2013, INT CONF ACOUST SPEE, P8575, DOI 10.1109/ICASSP.2013.6639339; Junejo KN, 2008, IEEE DATA MINING, P323, DOI 10.1109/ICDM.2008.26; Li YH, 2003, IEEE T KNOWL DATA EN, V15, P871, DOI 10.1109/TKDE.2003.1209005; Martins Andre<prime>F. T., 2011, P EMNLP; Meier L, 2008, J R STAT SOC B, V70, P53, DOI 10.1111/j.1467-9868.2007.00627.x; Milne D. N., 2008, P AAAI WORKSH WIK AR, P25; Mintz M., 2009, P ACL, P1003, DOI DOI 10.3115/1690219.1690287; Mishne G, 2006, LECT NOTES COMPUT SC, V3936, P289; Navigli R, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P216; Rosenfeld B, 2007, P 16 ACM C C INF KNO, P411, DOI DOI 10.1145/1321440.1321499; Schmitz M., 2012, EMNLP CONLL 2012 201, P523; Shen W., 2012, P 21 WORLD WID WEB C, P449; Simon N, 2013, J COMPUT GRAPH STAT, V22, P231, DOI 10.1080/10618600.2012.681250; Szumlanski S. R., 2010, PROC 19 ACM C INF KN, P19; Tariq A., 2011, 20 ACM INT C INF KNO, P2233; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wu F, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P118; Yin Junming, 2012, Proc Int Conf Mach Learn, V2012, P871; Yogatama Dani, 2012, P 50 ANN M ASS COMP, V1, P685; Zhu J., 2009, P 18 INT C WORLD WID, P101, DOI DOI 10.1145/1526709.1526724	34	1	1	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2017	39	10					2000	2014		10.1109/TPAMI.2016.2632117	http://dx.doi.org/10.1109/TPAMI.2016.2632117			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	FF3NI	27893385	hybrid			2022-12-18	WOS:000408807600008
J	Premachandran, V; Tarlow, D; Yuille, AL; Batra, D				Premachandran, Vittal; Tarlow, Daniel; Yuille, Alan L.; Batra, Dhruv			Empirical Minimum Bayes Risk Prediction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Diverse predictions; DivMBest; image segmentation; object segmentation; human pose estimation		When building vision systems that predict structured objects such as image segmentations or human poses, a crucial concern is performance under task-specific evaluation measures (e.g., Jaccard Index or Average Precision). An ongoing research challenge is to optimize predictions so as to maximize performance on such complex measures. In this work, we present a simple meta-algorithm that is surprisingly effective - Empirical Min Bayes Risk. EMBR takes as input a pre-trained model that would normally be the final product and learns three additional parameters so as to optimize performance on the complex instance-level high-order task-specific measure. We demonstrate EMBR in several domains, taking existing state-of-the-art algorithms and improving performance up to 8 percent, simply by learning three extra parameters. Our code is publicly available and the results presented in this paper can be replicated from our code-release.	[Premachandran, Vittal] Johns Hopkins Univ, Baltimore, MD 21218 USA; [Yuille, Alan L.] Johns Hopkins Univ, Cognit Sci & Comp Sci, Baltimore, MD 21218 USA; [Tarlow, Daniel] Microsoft Res, Machine Intelligence & Percept Grp, Cambridge, England; [Batra, Dhruv] Virginia Tech, Bradley Dept Elect & Comp Engn, Blacksburg, VA USA; [Batra, Dhruv] Virginia Tech, VT Machine Learning & Percept Grp, Blacksburg, VA USA	Johns Hopkins University; Johns Hopkins University; Microsoft; Virginia Polytechnic Institute & State University; Virginia Polytechnic Institute & State University	Premachandran, V (corresponding author), Johns Hopkins Univ, Baltimore, MD 21218 USA.	vittalp@jhu.edu; dtarlow@microsoft.com; yuille@stat.ucla.edu; dbatra@vt.edu		Yuille, Alan L./0000-0001-5207-9249	National Science Foundation [CCF-1317376, IIS-1353694, IIS-1350553]; Army Research Office YIP Award [W911NF-14-1-0180]; Office of Naval Research [N00014-14-1-0679]	National Science Foundation(National Science Foundation (NSF)); Army Research Office YIP Award; Office of Naval Research(Office of Naval Research)	The authors thank Varun Ramakrishna for helpful discussions during early stages of this work. They thank Xianjie Chen for help with using his pose estimation code. VP and AY were partially supported by the National Science Foundation grant CCF-1317376. DB was partially supported by the National Science Foundation under grants IIS-1353694 and IIS-1350553, the Army Research Office YIP Award W911NF-14-1-0180, and the Office of Naval Research grant N00014-14-1-0679. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the US Government or any sponsor. This work was done while V. Premachandran and A. Yuille were at the University of California, Los Angeles.	[Anonymous], P NIPS; Barbu A, 2005, IEEE T PATTERN ANAL, V27, P1239, DOI 10.1109/TPAMI.2005.161; Batra D., 2012, P UNC ART INT UAI, P121; Batra D, 2012, LECT NOTES COMPUT SC, V7576, P1, DOI 10.1007/978-3-642-33715-4_1; Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32; Carreira J, 2010, PROC CVPR IEEE, P3241, DOI 10.1109/CVPR.2010.5540063; Chen X., 2014, P 27 ANN C NEURAL IN, P1736, DOI DOI 10.1109/CVPR.2018.00742; Eichner M., 2010, TECHNICAL REPORT; Everingham M., 2012, PASCAL VISUAL OBJECT; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Guzman-Rivera A., 2012, ADV NEURAL INFORM PR, P1808; Guzman-Rivera A, 2014, JMLR WORKSH CONF PRO, V33, P284; Huszar F., 2012, ARXIV12041664; IVANESCU PL, 1965, OPER RES, V13, P388, DOI 10.1287/opre.13.3.388; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003; Murray I., 2006, P 22 ANN C UNCERTAIN; Niculescu-Mizil Alexandru, 2005, P 22 INT C MACHINE L, P625, DOI 10.1145/1102351.1102430; Nilsson D, 1998, STAT COMPUT, V8, P159, DOI 10.1023/A:1008990218483; Papandreou G, 2011, IEEE I CONF COMP VIS, P193, DOI 10.1109/ICCV.2011.6126242; Park D, 2011, IEEE I CONF COMP VIS, P2627, DOI 10.1109/ICCV.2011.6126552; Park James D, 2002, P 19 C UNC ART INT, P459; Pearl J., 1982, AAAI 82 P 2 AAAI C A, P133; Platt JC, 2000, ADV NEUR IN, P61; Porway J, 2011, IEEE T PATTERN ANAL, V33, P1713, DOI 10.1109/TPAMI.2011.27; Premachandran V, 2014, PROC CVPR IEEE, P1043, DOI 10.1109/CVPR.2014.137; SHIMONY SE, 1994, ARTIF INTELL, V68, P399, DOI 10.1016/0004-3702(94)90072-8; Tarlow D., 2012, INT C ART INT STAT, P1212; Taylor M., 2008, P 2008 INT C WEB SEA, P77, DOI DOI 10.1145/1341531.1341544; Tu ZW, 2002, IEEE T PATTERN ANAL, V24, P657, DOI 10.1109/34.1000239; Valiant L. G., 1979, Theoretical Computer Science, V8, P189, DOI 10.1016/0304-3975(79)90044-6; Welling Max, 2009, P 26 ANN INT C MACH, P1121, DOI DOI 10.1145/1553374.1553517; Yang Y, 2013, IEEE T PATTERN ANAL, V35, P2878, DOI 10.1109/TPAMI.2012.261; Zadrozny Bianca, 2001, ICML	35	1	1	1	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	2017	39	1					75	86		10.1109/TPAMI.2016.2537807	http://dx.doi.org/10.1109/TPAMI.2016.2537807			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	EF6DP	26960219	hybrid			2022-12-18	WOS:000390421300008
J	Lee, NH; Tang, RZ; Priebe, CE; Rosen, M				Lee, Nam H.; Tang, Runze; Priebe, Carey E.; Rosen, Michael			A Model Selection Approach for Clustering a Multinomial Sequence with Non-Negative Factorization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Model selection; non-negative data; networks/graphs; stochastic; statistics; pattern recognition		We consider a problem of clustering a sequence of multinomial observations by way of a model selection criterion. We propose a form of a penalty term for the model selection procedure. Our approach subsumes both the conventional AIC and BIC criteria but also extends the conventional criteria in a way that it can be applicable also to a sequence of sparse multinomial observations, where even within a same cluster, the number of multinomial trials may be different for different observations. In addition, as a preliminary estimation step to maximum likelihood estimation, and more generally, to maximum L-q estimation, we propose to use reduced rank projection in combination with non-negative factorization. We motivate our approach by showing that our model selection criterion and preliminary estimation step yield consistent estimates under simplifying assumptions. We also illustrate our approach through numerical experiments using real and simulated data.	[Lee, Nam H.; Rosen, Michael] Johns Hopkins Univ, Sch Med, Armstrong Inst Patient Safety & Qual, Baltimore, MD 21218 USA; [Tang, Runze; Priebe, Carey E.] Johns Hopkins Univ, Dept Appl Math & Stat, Whiting Sch Engn, Baltimore, MD USA	Johns Hopkins University; Johns Hopkins University	Lee, NH (corresponding author), Johns Hopkins Univ, Sch Med, Armstrong Inst Patient Safety & Qual, Baltimore, MD 21218 USA.	nhlee@jhu.edu; rtang13@jhu.edu; cep@jhu.edu; mrosen44@jhmi.edu			Johns Hopkins University Armstrong Institute for Patient Safety and Quality; XDATA program of the Defense Advanced Research Projects Agency (DARPA) through Air Force Research Laboratory [FA8750-12-2-0303]	Johns Hopkins University Armstrong Institute for Patient Safety and Quality; XDATA program of the Defense Advanced Research Projects Agency (DARPA) through Air Force Research Laboratory	This work is partially supported by Johns Hopkins University Armstrong Institute for Patient Safety and Quality and the XDATA program of the Defense Advanced Research Projects Agency (DARPA) administered through Air Force Research Laboratory contract FA8750-12-2-0303. The authors thank Youngser Park for his assistance in performing numerical experiments. They thank the anonymous referees for their valuable comments.	Berry MW, 2007, COMPUT STAT DATA AN, V52, P155, DOI 10.1016/j.csda.2006.11.006; Bouguila N, 2008, IEEE T KNOWL DATA EN, V20, P462, DOI 10.1109/TKDE.2007.190726; Brunet JP, 2004, P NATL ACAD SCI USA, V101, P4164, DOI 10.1073/pnas.0308531101; Chatterjee S., 2013, ARXIV12121247; Duda R.O., 1973, J ROYAL STAT SOC SER; Ferrari D, 2010, ANN STAT, V38, P753, DOI 10.1214/09-AOS687; Fraley C, 2002, J AM STAT ASSOC, V97, P611, DOI 10.1198/016214502760047131; Gillis N, 2014, J MACH LEARN RES, V15, P1249; Jarrell TA, 2012, SCIENCE, V337, P437, DOI 10.1126/science.1221762; Kim H, 2008, SIAM J MATRIX ANAL A, V30, P713, DOI 10.1137/07069239X; Linhart H, 1986, WILEY SERIES PROBABI; Lyzinski V., 2015, PARALLEL COMPUT; Pavlovic DM, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0097584; RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239; Ripley BD., 1996; ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7; Tang M., 2014, ARXIV14092344; Wang YX, 2015, ARXIV150202069; Zhu M, 2006, COMPUT STAT DATA AN, V51, P918, DOI 10.1016/j.csda.2005.09.010	21	1	1	0	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2016	38	12					2345	2358		10.1109/TPAMI.2016.2522443	http://dx.doi.org/10.1109/TPAMI.2016.2522443			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	EC2WJ	27824580	Green Submitted			2022-12-18	WOS:000387984700001
J	Alahari, K; Batra, D; Ramalingam, S; Paragios, N; Zemel, R				Alahari, Karteek; Batra, Dhruv; Ramalingam, Srikumar; Paragios, Nikos; Zemel, Richard			Guest Editors' Introduction: Special Section on Higher Order Graphical Models in Computer Vision	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									[Alahari, Karteek] Inria, LEAR Team, Grenoble, France; [Alahari, Karteek] Inria Paris Rocquencourt, WILLOW Team, Rocquencourt, France; [Alahari, Karteek] Ecole Normale Super, Paris, France; [Batra, Dhruv] Virginia Tech, Dept Elect & Comp Engn, Blacksburg, VA USA; [Batra, Dhruv] VT Machine Learning & Percept Grp, Blacksburg, VA USA; [Batra, Dhruv] Toyota Technol Inst, Chicago, IL USA; [Ramalingam, Srikumar] Mitsubishi Elect Res Lab MERL, Boston, MA USA; [Paragios, Nikos] Ecole Cent Paris, Paris, France; [Paragios, Nikos] Inst Univ France, Paris, France; [Zemel, Richard] Univ Toronto, Toronto, ON M5S 1A1, Canada; [Zemel, Richard] Salk Inst Biol Studies, La Jolla, CA 92037 USA; [Zemel, Richard] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Virginia Polytechnic Institute & State University; Toyota Technological Institute - Chicago; UDICE-French Research Universities; Universite Paris Saclay; Institut Universitaire de France; University of Toronto; Salk Institute; Carnegie Mellon University	Alahari, K (corresponding author), Inria, LEAR Team, Grenoble, France.								0	1	1	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2015	37	7					1321	1322		10.1109/TPAMI.2015.2434651	http://dx.doi.org/10.1109/TPAMI.2015.2434651			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CK0YG	26598766				2022-12-18	WOS:000355931100001
J	Yarlagadda, P; Ommer, B				Yarlagadda, Pradeep; Ommer, Bjoern			Beyond the Sum of Parts: Voting with Groups of Dependent Entities	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Object detection; recognition; hough voting; grouping; visual learning	OBJECT DETECTION; RECOGNITION; MODELS	The high complexity of multi-scale, category-level object detection in cluttered scenes is efficiently handled by Hough voting methods. However, the main shortcoming of the approach is that mutually dependent local observations are independently casting their votes for intrinsically global object properties such as object scale. Object hypotheses are then assumed to be a mere sum of their part votes. Popular representation schemes are, however, based on a dense sampling of semi-local image features, which are consequently mutually dependent. We take advantage of part dependencies and incorporate them into probabilistic Hough voting by deriving an objective function that connects three intimately related problems: i) grouping mutually dependent parts, ii) solving the correspondence problem conjointly for dependent parts, and iii) finding concerted object hypotheses using extended groups rather than based on local observations alone. Early commitments are avoided by not restricting parts to only a single vote for a locally best correspondence and we learn a weighting of parts during training to reflect their differing relevance for an object. Experiments successfully demonstrate the benefit of incorporating part dependencies through grouping into Hough voting. The joint optimization of groupings, correspondences, and votes not only improves the detection accuracy over standard Hough voting and a sliding window baseline, but it also reduces the computational complexity by significantly decreasing the number of candidate hypotheses.	[Yarlagadda, Pradeep; Ommer, Bjoern] Heidelberg Univ, Dept Math & Comp Sci, D-69115 Heidelberg, Germany; [Yarlagadda, Pradeep; Ommer, Bjoern] Heidelberg Collaboratory Image Proc HCI, D-69115 Heidelberg, Germany	Ruprecht Karls University Heidelberg	Yarlagadda, P (corresponding author), Heidelberg Univ, Dept Math & Comp Sci, Speyerer Str 6, D-69115 Heidelberg, Germany.	pyarlaga@iwr.uni-heidelberg.de; bommer@iwr.uni-heidelberg.de			Excellence Initiative of the German Federal Government, DFG [ZUK 49/1]	Excellence Initiative of the German Federal Government, DFG(German Research Foundation (DFG))	This work was supported by the Excellence Initiative of the German Federal Government, DFG project number ZUK 49/1.	Amit Y, 1999, NEURAL COMPUT, V11, P1691, DOI 10.1162/089976699300016197; Arbelaez P, 2009, PROC CVPR IEEE, P2294, DOI 10.1109/CVPRW.2009.5206707; BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1; Berg AC, 2005, PROC CVPR IEEE, P26; Blaschko MB, 2008, PROC CVPR IEEE, P93, DOI 10.1109/cvpr.2008.4587586; Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598; Carneiro G, 2006, LECT NOTES COMPUT SC, V3953, P29, DOI 10.1007/11744078_3; Crandall D, 2005, PROC CVPR IEEE, P10; Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Estrada FJ, 2009, PROC CVPR IEEE, P1279, DOI 10.1109/CVPRW.2009.5206514; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Fergus R, 2003, PROC CVPR IEEE, P264; Fidler S., 2007, 2007 IEEE C COMP VIS, P1, DOI [10.1109/CVPR.2007.383269, DOI 10.1109/CVPR.2007.383269]; Fidler S, 2010, LECT NOTES COMPUT SC, V6315, P687, DOI 10.1007/978-3-642-15555-0_50; Gall J, 2009, PROC CVPR IEEE, P1022, DOI 10.1109/CVPRW.2009.5206740; Gu CH, 2009, PROC CVPR IEEE, P1030, DOI 10.1109/CVPRW.2009.5206727; Karlinsky L, 2010, PROC CVPR IEEE, P25, DOI 10.1109/CVPR.2010.5540232; Lazebnik S., 2006, P IEEE INT C COMP VI, P2169, DOI DOI 10.1109/CVPR.2006.68; Lehmann B. L. A., 2008, P BRIT MACH VIS C, P1; Leibe B, 2008, INT J COMPUT VISION, V77, P259, DOI 10.1007/s11263-007-0095-3; Li FX, 2010, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2010.5539839; Lin L, 2012, PROC CVPR IEEE, P135, DOI 10.1109/CVPR.2012.6247668; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Ma TY, 2011, PROC CVPR IEEE, P1441, DOI 10.1109/CVPR.2011.5995591; Maire M., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587420; Maji S, 2009, PROC CVPR IEEE, P1038, DOI 10.1109/CVPRW.2009.5206693; Medioni G., 2000, RECONNAISSANCE FORME; Monroy A, 2012, LECT NOTES COMPUT SC, V7574, P580, DOI 10.1007/978-3-642-33712-3_42; Ommer B, 2009, IEEE I CONF COMP VIS, P484, DOI 10.1109/ICCV.2009.5459200; Ommer B, 2010, IEEE T PATTERN ANAL, V32, P501, DOI 10.1109/TPAMI.2009.22; Opelt A., 2006, P IEEE C COMP VIS PA, P3; Porway J, 2010, INT J COMPUT VISION, V88, P254, DOI 10.1007/s11263-009-0306-1; Riemenschneider H, 2010, LECT NOTES COMPUT SC, V6315, P29, DOI 10.1007/978-3-642-15555-0_3; Sala P, 2010, LECT NOTES COMPUT SC, V6315, P603, DOI 10.1007/978-3-642-15555-0_44; Shotton J, 2005, IEEE I CONF COMP VIS, P503; Sivic J, 2005, IEEE I CONF COMP VIS, P370; Srinivasan P, 2010, PROC CVPR IEEE, P1673, DOI 10.1109/CVPR.2010.5539834; Sudderth EB, 2005, IEEE I CONF COMP VIS, P1331; Todorovic S, 2008, PROC CVPR IEEE, P195; Toshev A, 2010, PROC CVPR IEEE, P950, DOI 10.1109/CVPR.2010.5540114; van de Sande KEA, 2011, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2011.6126456; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Williams C., 2006, EDIINFRR0719; Yarlagadda P, 2012, LECT NOTES COMPUT SC, V7572, P766, DOI 10.1007/978-3-642-33718-5_55; Yarlagadda P, 2010, LECT NOTES COMPUT SC, V6315, P197, DOI 10.1007/978-3-642-15555-0_15; Zhu QH, 2008, LECT NOTES COMPUT SC, V5303, P774	49	1	1	0	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	2015	37	6					1134	1147		10.1109/TPAMI.2014.2363456	http://dx.doi.org/10.1109/TPAMI.2014.2363456			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CH9SR	26357338				2022-12-18	WOS:000354377100002
J	Adams, RP; Fox, EB; Sudderth, EB; Teh, YW				Adams, Ryan P.; Fox, Emily B.; Sudderth, Erik B.; Teh, Yee Whye			Guest Editors' Introduction to the Special Issue on Bayesian Nonparametrics	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									[Adams, Ryan P.] Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA; [Fox, Emily B.] Univ Washington, Dept Stat, Seattle, WA 98195 USA; [Sudderth, Erik B.] Brown Univ, Dept Comp Sci, Providence, RI 02912 USA; [Teh, Yee Whye] Univ Oxford, Dept Stat, Oxford OX1 3TG, England	Harvard University; University of Washington; University of Washington Seattle; Brown University; University of Oxford	Adams, RP (corresponding author), Harvard Univ, Sch Engn & Appl Sci, 33 Oxford St, Cambridge, MA 02138 USA.	rpa@seas.harvard.edu; ebfox@stat.washington.edu; sudderth@cs.brown.edu; y.w.teh@stats.ox.ac.uk		Sudderth, Erik/0000-0002-0595-9726; Fox, Emily/0000-0003-3188-9685					0	1	1	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2015	37	2					209	211		10.1109/TPAMI.2014.2380478	http://dx.doi.org/10.1109/TPAMI.2014.2380478			3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	CB4VD	26598765				2022-12-18	WOS:000349625500001
J	Castaneda, V; Mateus, D; Navab, N				Castaneda, Victor; Mateus, Diana; Navab, Nassir			Stereo Time-of-Flight with Constructive Interference	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Time-of-Flight; multi-view system; constructive interference; sensor		This paper describes a novel method to acquire depth images using a pair of ToF (Time-of-Flight) cameras. As opposed to approaches that filter, calibrate or do 3D reconstructions posterior to the image acquisition, we combine the measurements of the two cameras within a modified acquisition procedure. The new proposed stereo-ToF acquisition is composed of three stages during which we actively modify the infrared lighting of the scene: first, the two cameras emit an infrared signal one after the other (stages 1 and 2), and then, simultaneously (stage 3). Assuming the scene is static during the three stages, we gather the depth measurements obtained with both cameras and define a cost function to optimize the two depth images. A qualitative and quantitative evaluation of the performance of the proposed stereo-ToF acquisition is provided both for simulated and real ToF cameras. In both cases, the stereo-ToF acquisition produces more accurate depth measurements. Moreover, an extension to the multi-view ToF case and a detailed study on the interference specifications of the system are included.	[Castaneda, Victor; Mateus, Diana; Navab, Nassir] Tech Univ Munich, Dept Comp Sci, D-85748 Munich, Bavaria, Germany; [Castaneda, Victor] Univ Chile, Fac Med, Lab Sci Image Anal, SCIAN Lab,BNI,ICBM, Santiago 8380453, Chile; [Mateus, Diana] Helmholtz Zentrum, ICB, Munich, Germany	Technical University of Munich; Universidad de Chile; Helmholtz Association	Castaneda, V (corresponding author), Tech Univ Munich, Dept Comp Sci, D-85748 Munich, Bavaria, Germany.	castaned@in.tum.de; mateus@in.tum.de; navab@in.tum.de	Peters, Terry M/K-6853-2013	Peters, Terry M/0000-0003-1440-7488; Mateus, Diana/0000-0002-2252-8717; Castaneda, Victor/0000-0003-0554-0324	German Academic Exchange Service (DAAD); Chilean National Commission for Science and Technology (CONICYT); BNI [ICM P09-015-F]	German Academic Exchange Service (DAAD)(Deutscher Akademischer Austausch Dienst (DAAD)); Chilean National Commission for Science and Technology (CONICYT); BNI	The authors would like to acknowledge the help of A. Sanchez and the support of the German Academic Exchange Service (DAAD), the Chilean National Commission for Science and Technology (CONICYT), and the BNI (ICM P09-015-F).	[Anonymous], 2008, INT ARCH PHOTOGRAMM; Beder Christian, 2008, International Journal of Intelligent Systems Technologies and Applications, V5, P285, DOI 10.1504/IJISTA.2008.021291; Beder C, 2007, LECT NOTES COMPUT SC, V4713, P11; Bohme M, 2010, COMPUT VIS IMAGE UND, V114, P1329, DOI 10.1016/j.cviu.2010.08.001; CUI Y., 2010, P CVPR; Cui Y, 2013, IEEE T PATTERN ANAL, V35, P1039, DOI 10.1109/TPAMI.2012.190; Fuchs S., 2008, P IEEE CVPR; Guomundsson Sigurjon Arni, 2008, International Journal of Intelligent Systems Technologies and Applications, V5, P425, DOI 10.1504/IJISTA.2008.021305; Hannemann W., 2008, International Journal of Intelligent Systems Technologies and Applications, V5, P393, DOI 10.1504/IJISTA.2008.021302; Huhle B, 2010, COMPUT VIS IMAGE UND, V114, P1336, DOI 10.1016/j.cviu.2009.11.004; Kavli T., 2008, P SPIE, V7066-4; Keller M, 2009, SIMUL MODEL PRACT TH, V17, P967, DOI 10.1016/j.simpat.2009.03.004; Koch R, 2009, LECT NOTES COMPUT SC, V5742, P126, DOI 10.1007/978-3-642-03778-8_10; Kolb A, 2010, COMPUT GRAPH FORUM, V29, P141, DOI 10.1111/j.1467-8659.2009.01583.x; Lange R., 2000, THESIS U SIEGEN SIEG; Lindner M, 2010, COMPUT VIS IMAGE UND, V114, P1318, DOI 10.1016/j.cviu.2009.11.002; May S, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1673, DOI 10.1109/IROS.2009.5354684; Steiger O., 2008, P 15 IEEE ICIP SAN D; YANG Q, 2007, P CVPR; Young Min Kim, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1542, DOI 10.1109/ICCVW.2009.5457430; Zhu JJ, 2010, IEEE T PATTERN ANAL, V32, P899, DOI 10.1109/TPAMI.2009.68	21	1	1	3	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2014	36	7					1402	1413		10.1109/TPAMI.2013.195	http://dx.doi.org/10.1109/TPAMI.2013.195			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AK1WS	26353311				2022-12-18	WOS:000338209900009
J	Leichter, I; Lindenbaum, M; Rivlin, E				Leichter, Ido; Lindenbaum, Michael; Rivlin, Ehud			The Cues in "Dependent Multiple Cue Integration for Robust Tracking" Are Independent	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Bayesian tracking; multiple cue integration	FILTERS	A methodology for integrating multiple cues for tracking was proposed in several papers. These papers claim that, unlike other methodologies, conditional independence of the cues is not assumed. This brief communication 1) refutes this claim and 2) points out other major problems in the methodology.	[Leichter, Ido] Microsoft Res, Adv Technol Labs Israel, Microsoft R&D Ctr, IL-31905 Haifa, Israel; [Lindenbaum, Michael; Rivlin, Ehud] Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Leichter, I (corresponding author), Microsoft Res, Adv Technol Labs Israel, Microsoft R&D Ctr, Bldg 23,Matam Pk, IL-31905 Haifa, Israel.	idol@microsoft.com; mic@cs.technion.ac.il; ehudr@cs.technion.ac.il						Moreno-Noguer F, 2005, IEEE I CONF COMP VIS, P1713, DOI 10.1109/ICCV.2005.126; Moreno-Noguer F, 2005, LECT NOTES COMPUT SC, V3522, P93; Moreno-Noguer F, 2008, IEEE T PATTERN ANAL, V30, P670, DOI 10.1109/TPAMI.2007.70727; Moreno-Noguer F, 2006, IEEE INT CONF ROBOT, P4081, DOI 10.1109/ROBOT.2006.1642329	4	1	1	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	2014	36	3					620	621		10.1109/TPAMI.2010.170	http://dx.doi.org/10.1109/TPAMI.2010.170			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AA9YX	24605376				2022-12-18	WOS:000331450100017
J	Sagar, BSD				Sagar, B. S. Daya			Visualization of Spatiotemporal Behavior of Discrete Maps via Generation of Recursive Median Elements (vol 32, pg 378, 2010)	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Correction									Indian Stat Inst, Bangalore Ctr, Syst Sci & Informat Unit, Bangalore, Karnataka, India	Indian Statistical Institute; Indian Statistical Institute Bangalore	Sagar, BSD (corresponding author), Indian Stat Inst, Bangalore Ctr, Syst Sci & Informat Unit, 8th Mile,Mysore Rd,RV Coll PO, Bangalore, Karnataka, India.	bsdsagar@isibang.ac.in						Sagar BSD, 2010, IEEE T PATTERN ANAL, V32, P378, DOI 10.1109/TPAMI.2009.163	1	1	1	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	2014	36	3								10.1109/TPAMI.2009.163	http://dx.doi.org/10.1109/TPAMI.2009.163			1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AA9YX					2022-12-18	WOS:000331450100002
J	Hendrikse, A; Veldhuis, R; Spreeuwers, L				Hendrikse, Anne; Veldhuis, Raymond; Spreeuwers, Luuk			Likelihood-Ratio-Based Verification in High-Dimensional Spaces	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						High-dimensional verification; eigenvalue bias correction; variance correction; euclidean distance; principle component analysis; Marcenko Pastur equation; eigenwise correction; fixed-point eigenvalue correction	SAMPLE COVARIANCE MATRICES; EMPIRICAL DISTRIBUTION; STRONG-CONVERGENCE; EIGENVALUES; RECOGNITION	The increase of the dimensionality of data sets often leads to problems during estimation, which are denoted as the curse of dimensionality. One of the problems of second-order statistics (SOS) estimation in high-dimensional data is that the resulting covariance matrices are not full rank, so their inversion, for example, needed in verification systems based on the likelihood ratio, is an ill-posed problem, known as the singularity problem. A classical solution to this problem is the projection of the data onto a lower dimensional subspace using principle component analysis (PCA) and it is assumed that any further estimation on this dimension-reduced data is free from the effects of the high dimensionality. Using theory on SOS estimation in high-dimensional spaces, we show that the solution with PCA is far from optimal in verification systems if the high dimensionality is the sole source of error. For moderate dimensionality, it is already outperformed by solutions based on euclidean distances and it breaks down completely if the dimensionality becomes very high. We propose a new method, the fixed-point eigenwise correction, which does not have these disadvantages and performs close to optimal.	[Hendrikse, Anne; Veldhuis, Raymond; Spreeuwers, Luuk] Univ Twente, Fac Elect Engn, Signals & Syst Grp, NL-7500 AE Enschede, Overijsel, Netherlands	University of Twente	Hendrikse, A (corresponding author), Univ Twente, Fac Elect Engn, Signals & Syst Grp, Drienerlolaan 5,POB 217, NL-7500 AE Enschede, Overijsel, Netherlands.	a.j.hendrikse@ewi.utwente.nl						Anderson T. W, 1984, INTRO MULTIVARIATE S; Bai ZD, 1996, STAT SINICA, V6, P311; Baik J, 2006, J MULTIVARIATE ANAL, V97, P1382, DOI 10.1016/j.jmva.2005.08.003; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; Bellman R., 1961, ADAPTIVE CONTROL PRO; Casella G, 2002, DUXBURY; Chen BB, 2012, BERNOULLI, V18, P1405, DOI 10.3150/11-BEJ381; Couillet R., 2011, RANDOM MATRIX METHOD; Diaz-Uriarte R, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-3; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; El Karoui N, 2008, ANN STAT, V36, P2757, DOI 10.1214/07-AOS581; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; Hampel FR., 2011, WILEY SERIES PROBABI; Hendrikse Anne, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P589, DOI 10.1109/ICPR.2010.149; Hendrikse A, 2008, P 29 S INF THEOR BEN, P27; Hendrikse A, 2009, P 30 S INF THEOR BEN, P25; Hendrikse A.J., 2010, 31 S INF THEOR BEN E, P105; Hendrikse A.J., 2011, TECHNICAL REPORT; Hendrikse A, 2009, LECT NOTES COMPUT SC, V5558, P189, DOI 10.1007/978-3-642-01793-3_20; Hubert M, 2005, TECHNOMETRICS, V47, P64, DOI 10.1198/004017004000000563; Jain AK, 2000, IEEE T PATTERN ANAL, V22, P4, DOI 10.1109/34.824819; JAYNES ET, 1982, P IEEE, V70, P939, DOI 10.1109/PROC.1982.12425; Jiang XD, 2006, ELECTRON LETT, V42, P1089, DOI 10.1049/el:20062035; Jiang XD, 2008, IEEE T PATTERN ANAL, V30, P383, DOI 10.1109/TPAMI.2007.70708; Jiang XD, 2011, IEEE SIGNAL PROC MAG, V28, P16, DOI 10.1109/MSP.2010.939041; Jiang XD, 2009, IEEE T PATTERN ANAL, V31, P931, DOI 10.1109/TPAMI.2008.258; Johnstone I.M., 2000, TECHNICAL REPORT; Ledoit O., 2003, J EMPIR FINANC, V10, P603, DOI [10.1016/S0927-5398(03)00007-0, DOI 10.1016/S0927-5398(03)00007-0]; Ledoit O., 2009, IEWWP407 I EMP RES E; Marenko V. A., 1967, MATH USSR SB, V1, P457, DOI [10.1070/SM1967v001n04ABEH001994, DOI 10.1070/SM1967V001N04ABEH001994]; MESTRE X, 2006, P 2 INT S COMM CONTR; Middleton D., 1960, INTRO STAT COMMUNICA; Millane S.A.R.P., 2003, P IM VIS COMP; Neyman J, 1933, PHILOS T R SOC LOND, V231, P289, DOI 10.1098/rsta.1933.0009; Pan GM, 2010, J MULTIVARIATE ANAL, V101, P1330, DOI 10.1016/j.jmva.2010.02.001; Paul D., 2004, TECHNICAL REPORT; Sarela J, 2004, J MACH LEARN RES, V4, P1447, DOI 10.1162/jmlr.2003.4.7-8.1447; Serneels S, 2008, COMPUT STAT DATA AN, V52, P1712, DOI 10.1016/j.csda.2007.05.024; Silverstein JW, 1995, J MULTIVARIATE ANAL, V55, P331, DOI 10.1006/jmva.1995.1083; Soltane Mohamed, 2010, INT J ADV SCI TECHNO, V21; Srivastava S, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1-6, PROCEEDINGS, P2294, DOI 10.1109/ISIT.2006.261976; TAKESHITA T, 1995, PATTERN RECOGN LETT, V16, P307, DOI 10.1016/0167-8655(94)00099-O; Tan XY, 2007, LECT NOTES COMPUT SC, V4778, P235; TOLHURST DJ, 1992, OPHTHAL PHYSL OPT, V12, P229, DOI 10.1111/j.1475-1313.1992.tb00296.x; TRUNK GV, 1979, IEEE T PATTERN ANAL, V1, P306, DOI 10.1109/TPAMI.1979.4766926	46	1	1	0	15	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	2014	36	1					127	139		10.1109/TPAMI.2013.93	http://dx.doi.org/10.1109/TPAMI.2013.93			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	265PV	24231871				2022-12-18	WOS:000327965100011
J	Thomas, D; Sugimoto, A				Thomas, Diego; Sugimoto, Akihiro			Range Image Registration Using a Photometric Metric under Unknown Lighting	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Range image; registration; photometry; spherical harmonics; photometric reprojection	POINT SET REGISTRATION; OPTIMIZATION	Based on the spherical harmonics representation of image formation, we derive a new photometric metric for evaluating the correctness of a given rigid transformation aligning two overlapping range images captured under unknown, distant, and general illumination. We estimate the surrounding illumination and albedo values of points of the two range images from the point correspondences induced by the input transformation. We then synthesize the color of both range images using albedo values transferred using the point correspondences to compute the photometric reprojection error. This way allows us to accurately register two range images by finding the transformation that minimizes the photometric reprojection error. We also propose a practical method using the proposed photometric metric to register pairs of range images devoid of salient geometric features, captured under unknown lighting. Our method uses a hypothesize-and-test strategy to search for the transformation that minimizes our photometric metric. Transformation candidates are efficiently generated by employing the spherical representation of each range image. Experimental results using both synthetic and real data demonstrate the usefulness of the proposed metric.	[Thomas, Diego; Sugimoto, Akihiro] Res Org Informat & Syst, Natl Inst Informat, Chiyoda Ku, Tokyo 1018430, Japan	Research Organization of Information & Systems (ROIS); National Institute of Informatics (NII) - Japan	Thomas, D (corresponding author), Res Org Informat & Syst, Natl Inst Informat, Chiyoda Ku, 2-1-2 Hitotsubashi, Tokyo 1018430, Japan.	diego_thomas@nii.ac.jp; sugimoto@nii.ac.jp		Thomas, Diego/0000-0002-8525-7133	JST, CREST; Grants-in-Aid for Scientific Research [23650092] Funding Source: KAKEN	JST, CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST)); Grants-in-Aid for Scientific Research(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This work was supported in part by JST, CREST.	Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Breitenreicher D, 2009, LECT NOTES COMPUT SC, V5681, P274, DOI 10.1007/978-3-642-03641-5_21; Choi S., 2009, P BRIT MACH VIS C BM; Debevec Paul, 2004, LIGHT PROBE IMAGE GA; Enqvist O, 2011, PROC CVPR IEEE; GUTMAN B, 2008, 2 MICCAI WORKSH MATH, P56; Henry P., 2010, P INT S EXP ROB; HORN BKP, 1988, J OPT SOC AM A, V5, P1127, DOI 10.1364/JOSAA.5.001127; Izadi S., 2011, P ACM S US INT SOFTW; Jian B, 2005, IEEE I CONF COMP VIS, P1246; Johnson AE, 1999, IMAGE VISION COMPUT, V17, P135, DOI 10.1016/S0262-8856(98)00117-6; Johnson AE, 1997, INTERNATIONAL CONFERENCE ON RECENT ADVANCES IN 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P121, DOI 10.1109/IM.1997.603857; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Okatani IS, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P789, DOI 10.1109/TDPVT.2004.1335396; Papazov C, 2009, LECT NOTES COMPUT SC, V5875, P1043, DOI 10.1007/978-3-642-10331-5_97; Ramamoorthi R., 2006, FACE PROCESSING ADV, P385; Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423; Seo JK, 2005, PROC CVPR IEEE, P1140; Sweldens W., 1998, P ACM SIGGRAPH, P343; Tachikawa T., 2009, P VIS MOD VIS WORKSH, P37; Thomas D., 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P97, DOI 10.1109/WACV.2012.6163041; Thomas D, 2011, COMPUT VIS IMAGE UND, V115, P649, DOI 10.1016/j.cviu.2010.11.016; Vedaldi A., 2006, SIFT CODE MATLAB; Zhou K, 2004, COMPUT AIDED DESIGN, V36, P363, DOI 10.1016/S0010-4485(03)00098-8	26	1	1	0	20	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2013	35	9					2252	2269		10.1109/TPAMI.2013.21	http://dx.doi.org/10.1109/TPAMI.2013.21			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	186GB	23868783				2022-12-18	WOS:000322029000015
J	Keren, D; Werman, M; Feinberg, J				Keren, Daniel; Werman, Michael; Feinberg, Joshua			A Probabilistic Approach to Pattern Matching in the Continuous Domain	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Pattern matching; distance between signals; sampling; energy of a signal; regularization; probability; path integrals	RESTORATION	The goal of this paper is to solve the following basic problem: Given discrete noisy samples from a continuous signal, compute the probability distribution of its distance from a fixed template. As opposed to the typical restoration problem, which considers a single optimal signal, the computation of the entire probability distribution necessitates integrating over the entire signal space. To achieve this, we apply path integration techniques. The problem is studied in one and two dimensions, and an accurate solution as well as an efficient approximation scheme are provided.	[Keren, Daniel] Univ Haifa, Dept Comp Sci, IL-31905 Haifa, Israel; [Werman, Michael] Hebrew Univ Jerusalem, Inst Comp Sci, IL-91904 Jerusalem, Israel; [Feinberg, Joshua] Univ Haifa, Dept Math & Phys, Fac Sci & Sci Educ, IL-36006 Tivon, Israel; [Feinberg, Joshua] Technion Israel Inst Technol, Dept Phys, IL-32000 Haifa, Israel	University of Haifa; Hebrew University of Jerusalem; University of Haifa; Technion Israel Institute of Technology	Keren, D (corresponding author), Univ Haifa, Dept Comp Sci, IL-31905 Haifa, Israel.	dkeren@cs.haifa.ac.il; werman@cs.huji.ac.il; joshua@physics.technion.ac.il						Adams R. A., 2003, SOBOLEV SPACES, VSecond; Chambolle A, 2004, J MATH IMAGING VIS, V20, P89; Chan TF, 1998, IEEE T IMAGE PROCESS, V7, P370, DOI 10.1109/83.661187; Feynman R. P., 1965, QUANTUM MECH PATH IN; Galatsanos NP, 1992, IEEE T IMAGE PROCESS, V1, P322, DOI 10.1109/83.148606; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Keren D, 1999, J MATH IMAGING VIS, V11, P27, DOI 10.1023/A:1008317210576; Keren D, 2001, IEEE T PATTERN ANAL, V23, P747, DOI 10.1109/34.935848; KEREN D, 1993, IEEE T PATTERN ANAL, V15, P982, DOI 10.1109/34.254057; Larkin F. M., 1972, ROCKY MT J MATH, V2, P379; Lee AB, 2003, INT J COMPUT VISION, V54, P83, DOI 10.1023/A:1023705401078; LEE D, 1986, ROCKY MT J MATH, V16, P641, DOI 10.1216/RMJ-1986-16-4-641; Moghaddam B, 2007, IEEE I CONF COMP VIS, P2073, DOI 10.1109/cvpr.2007.383092; Nikolova M, 2004, J MATH IMAGING VIS, V20, P99, DOI 10.1023/B:JMIV.0000011920.58935.9c; Osadchy M, 2004, IEEE T CIRC SYST VID, V14, P534, DOI 10.1109/TCSVT.2004.825530; Osadchy M., 2006, P IEEE C COMP VIS PA, P2095; Ramani S, 2008, IEEE T SIGNAL PROCES, V56, P1055, DOI 10.1109/TSP.2007.908997; Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4; SCHULMAN LS, 1981, TECHNIQUES APPL PATH; Shankar R., 1994, PRINCIPLES QUANTUM M; STALKER J, 1998, COMPLEX ANAL FUNDAME; SZELISKI R, 1989, BAYESIAN MODELING UN; TERZOPOULOS D, 1984, MULTIRESOLUTION IMAG; Tikhonov A.N., 1977, SOLUTION ILL POSED P; Turner R.E., 2010, P IEEE INT C AC SPEE; Turner R.E., 2007, P ADV NEUR INF PROC; WAHBA G, 1983, J ROY STAT SOC B MET, V45, P133	27	1	1	0	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2012	34	10					1873	1885		10.1109/TPAMI.2011.284	http://dx.doi.org/10.1109/TPAMI.2011.284			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	988WY	22213767	Green Submitted			2022-12-18	WOS:000307522700001
J	Neilson, D; Yang, YH				Neilson, Daniel; Yang, Yee-Hong			A Component-Wise Analysis of Constructible Match Cost Functions for Global Stereopsis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Stereopsis; stereo matching; stereo correspondence; global algorithms; match cost functions		Match cost functions are common elements of every stereopsis algorithm that are used to provide a dissimilarity measure between pixels in different images. Global stereopsis algorithms incorporate assumptions about the smoothness of the resulting distance map that can interact with match cost functions in unpredictable ways. In this paper, we present a large-scale study on the relative performance of a structured set of match cost functions within several global stereopsis frameworks. We compare 272 match cost functions that are built from component parts in the context of four global stereopsis frameworks with a data set consisting of 57 stereo image pairs at three different variances of synthetic sensor noise. From our analysis, we infer a set of general rules that can be used to guide derivation of match cost functions for use in global stereopsis algorithms.	[Neilson, Daniel] Univ Saskatchewan, Saskatoon, SK S7N 5C9, Canada; [Yang, Yee-Hong] Univ Alberta, Edmonton, AB T6G 2E8, Canada	University of Saskatchewan; University of Alberta	Neilson, D (corresponding author), Univ Saskatchewan, 176-110 Sci Pl, Saskatoon, SK S7N 5C9, Canada.	ddneilson@ieee.org; yang@cs.ualberta.ca			NSERC; iCORE; University of Alberta	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); iCORE; University of Alberta(University of Alberta)	The authors would like to thank NSERC, iCORE, and the University of Alberta for providing financial support for this work. Additionally, they are grateful to WestGrid for providing the computing resources without which this study would have been impossible.	Bhat DN, 1996, PROC CVPR IEEE, P351, DOI 10.1109/CVPR.1996.517096; Birchfield S, 1998, IEEE T PATTERN ANAL, V20, P401, DOI 10.1109/34.677269; Black MJ, 1996, INT J COMPUT VISION, V19, P57, DOI 10.1007/BF00131148; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Bradley D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360698; Chen L, 2004, 13TH IEEE INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING, PROCEEDINGS, P192, DOI 10.1109/HPDC.2004.1323528; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; DeSouza GN, 2002, IEEE T PATTERN ANAL, V24, P237, DOI 10.1109/34.982903; El-Mahassani E.D., 2007, P C AUSTR PATT REC S, P144; Felzenszwalb PR, 2004, PROC CVPR IEEE, P261; HEALEY GE, 1994, IEEE T PATTERN ANAL, V16, P267, DOI 10.1109/34.276126; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; Hirschmuller H, 2009, IEEE T PATTERN ANAL, V31, P1582, DOI 10.1109/TPAMI.2008.221; Khaleghi B, 2007, CAN CON EL COMP EN, P1476; Lizotte DJ, 2008, THESIS U ALBERTA; Mattoccia S, 2007, LECT NOTES COMPUT SC, V4844, P517; Neilson D., 2008, P IEEE C COMP VIS PA; NEILSON D, 2009, THESIS U ALBERTA; Pharr M., 2004, PHYS BASED RENDERING; Pollefeys M, 2008, INT J COMPUT VISION, V78, P143, DOI 10.1007/s11263-007-0086-4; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; RAYKAR VC, 2005, CSTR4774 U MAR; Scharstein D, 2007, P IEEE C COMP VIS PA; Scharstein D., 2003, P IEEE C COMP VIS PA; Scharstein D., 2011, MIDDLEBURY STEREO VI; SHEATHER SJ, 1991, J ROY STAT SOC B MET, V53, P683; Sheskin D. J., 2011, HDB PARAMETRIC NONPA, V5th; Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815; Tombari F., 2008, P IEEE C COMP VIS PA; TOMBARI F, 2007, P PAC RIM S IM VID T; Ulges A., 2004, P ACM S DOC ENG, P198; Yang RG, 2004, IEEE T PATTERN ANAL, V26, P956, DOI 10.1109/TPAMI.2004.27; Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70; Zhang L, 2007, IEEE T PATTERN ANAL, V29, P331, DOI 10.1109/TPAMI.2007.36	34	1	1	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2011	33	11					2147	2159		10.1109/TPAMI.2011.67	http://dx.doi.org/10.1109/TPAMI.2011.67			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	820MM	21422494				2022-12-18	WOS:000294910000003
J	Arabadjis, D; Rousopoulos, P; Papaodysseus, C; Panagopoulos, M; Loumou, P; Theodoropoulos, G				Arabadjis, Dimitris; Rousopoulos, Panayiotis; Papaodysseus, Constantin; Panagopoulos, Michalis; Loumou, Panayiota; Theodoropoulos, Georgios			A General Methodology for the Determination of 2D Bodies Elastic Deformation Invariants: Application to the Automatic Identification of Parasites	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Deformation invariant elastic properties; automatic curve classification; parasite automatic identification; straightening deformed objects; image analysis; elastic deformation; pattern classification techniques		A novel methodology is introduced here that exploits 2D images of arbitrary elastic body deformation instances so as to quantify mechanoelastic characteristics that are deformation invariant. Determination of such characteristics allows for developing methods offering an image of the undeformed body. General assumptions about the mechanoelastic properties of the bodies are stated which lead to two different approaches for obtaining bodies' deformation invariants. One was developed to spot a deformed body's neutral line and its cross sections, while the other solves deformation PDEs by performing a set of equivalent image operations on the deformed body images. Both of these processes may furnish a body-undeformed version from its deformed image. This was confirmed by obtaining the undeformed shape of deformed parasites, cells (protozoa), fibers, and human lips. In addition, the method has been applied to the important problem of parasite automatic classification from their microscopic images. To achieve this, we first apply the previous method to straighten the highly deformed parasites, and then, apply a dedicated curve classification method to the straightened parasite contours. It is demonstrated that essentially different deformations of the same parasite give rise to practically the same undeformed shape, thus confirming the consistency of the introduced methodology. Finally, the developed pattern recognition method classifies the unwrapped parasites into six families, with an accuracy rate of 97.6 percent.	[Arabadjis, Dimitris; Rousopoulos, Panayiotis] Natl Tech Univ Athens, Dept Elect & Comp Engn, Sch Elect & Comp Engn, GR-15773 Athens, Greece; [Panagopoulos, Michalis] Ionian Univ, Dept Audio & Visual Arts, Corfu 49100, Greece; [Theodoropoulos, Georgios] Agr Univ Athens, Dept Anat & Physiol Domest Animals, Fac Anim Sci & Prod, Athens 11855, Greece; Natl Tech Univ Athens, Dept Elect & Comp Engn, Multimedia Lab, GR-15773 Athens, Greece	National Technical University of Athens; Ionian University; Agricultural University of Athens; National Technical University of Athens	Arabadjis, D (corresponding author), Natl Tech Univ Athens, Dept Elect & Comp Engn, Sch Elect & Comp Engn, Iroon Polytechneiou 9, GR-15773 Athens, Greece.	alphad.d@gmail.com; panrous@mail.ntua.gr; cpapaod@cs.ntua.gr; mpanagop@ionio.gr; naya.nale@gmail.com; gtheo@aua.gr	Arabadjis, Dimitris/AAS-4975-2021; Papaodysseus, Constantin/AAI-6900-2020; Arabadjis, Dimitris/F-2974-2012	Arabadjis, Dimitris/0000-0002-8438-0471; Arabadjis, Dimitris/0000-0002-8438-0471; Papaodysseus, Constantin/0000-0002-5238-5833; Panagopoulos, Michail/0000-0003-4585-8185				ADAM G, 1996, IEEE T PATTERN ANAL, V18, P673; Ahn SJ, 2002, IEEE T PATTERN ANAL, V24, P620, DOI 10.1109/34.1000237; BROCKETT RW, 1994, IEEE T SIGNAL PROCES, V42, P3377, DOI 10.1109/78.340774; Christensen GE, 1996, IEEE T IMAGE PROCESS, V5, P1435, DOI 10.1109/83.536892; CRAIG D, 1999, IEEE T PATTERN ANAL, V21, P31; Cremers D, 2006, INT J COMPUT VISION, V69, P335, DOI 10.1007/s11263-006-7533-5; Di Ruberto C, 2002, IMAGE VISION COMPUT, V20, P133, DOI 10.1016/S0262-8856(01)00092-0; Lai KF, 1998, IMAGE VISION COMPUT, V16, P55, DOI 10.1016/S0262-8856(97)00043-7; Lee D, 1997, IEEE SIGNAL PROC LET, V4, P2, DOI 10.1109/97.551685; MANDUCA A, 1998, P INT C MED IM COMP, P606; McMurtry LW, 2000, VET PARASITOL, V90, P73, DOI 10.1016/S0304-4017(00)00230-2; Nguyen HT, 2003, IEEE T PATTERN ANAL, V25, P330, DOI 10.1109/TPAMI.2003.1182096; PANAGOPOULOS T, 2004, WSEAS T ELECT, V1, P108; Papaodysseus C, 2005, IEEE T IMAGE PROCESS, V14, P862, DOI 10.1109/TIP.2005.849297; Pennec X, 2005, LECT NOTES COMPUT SC, V3750, P943, DOI 10.1007/11566489_116; Roman-Roldan R, 2001, PATTERN RECOGN, V34, P969, DOI 10.1016/S0031-3203(00)00052-2; TERZOPOULOS D, 1987, INT J COMPUT VISION, V1, P211, DOI 10.1007/BF00127821; Theodoropoulos G, 2000, COMPUT METH PROG BIO, V62, P69, DOI 10.1016/S0169-2607(99)00056-5; Trouve A, 1998, INT J COMPUT VISION, V28, P213, DOI 10.1023/A:1008001603737; Washington CW, 2004, IEEE T MED IMAGING, V23, P1117, DOI 10.1109/TMI.2004.830532	20	1	1	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	2010	32	5					799	814		10.1109/TPAMI.2009.70	http://dx.doi.org/10.1109/TPAMI.2009.70			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	569AW	20299706	Green Submitted			2022-12-18	WOS:000275569300003
J	Wang, H				Wang, Hui			Neighborhood Counting Measure and Minimum Risk Metric	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Minimum risk metric; neighborhood counting measure; k-nearest neighbor		The neighborhood counting measure (NCM) is a similarity measure based on the counting of all common neighborhoods in a data space [5]. The minimum risk metric (MRM) [2] is a distance measure based on the minimization of the risk of misclassification. The paper by Argentini and Blanzieri [1] refutes a remark in [5] about the time complexity of MRM, and presents an experimental comparison of MRM and NCM. This paper is a response to the paper by Argentini and Blanzieri [1]. The original remark is clarified by a combination of theoretical analysis of different implementations of MRM and experimental comparison of MRM and NCM using straightforward implementations of the two measures.	Univ Ulster, Sch Comp & Math, Newtownabbey BT37 0QB, Co Antrim, North Ireland	Ulster University	Wang, H (corresponding author), Univ Ulster, Sch Comp & Math, Newtownabbey BT37 0QB, Co Antrim, North Ireland.	h.wang@ulster.ac.uk		Wang, Hui/0000-0003-2633-6015				Argentini A, 2010, IEEE T PATTERN ANAL, V32, P763, DOI 10.1109/TPAMI.2009.69; Blanzieri E, 1999, LECT NOTES ARTIF INT, V1650, P14; Elzinga C, 2008, THEOR COMPUT SCI, V409, P394, DOI 10.1016/j.tcs.2008.08.035; Lin ZW, 2008, PROCEEDINGS OF THE 2008 INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE AND DESIGN, VOL 1, P29, DOI 10.1109/ISCID.2008.20; Wang H, 2006, IEEE T PATTERN ANAL, V28, P942, DOI 10.1109/TPAMI.2006.126; WANG H, 2008, P INT C ROUGH SET KN; Wang H, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P635	7	1	2	0	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	2010	32	4					766	768		10.1109/TPAMI.2010.16	http://dx.doi.org/10.1109/TPAMI.2010.16			3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	555XA					2022-12-18	WOS:000274548800017
J	Ji, Q; Luo, JB; Metaxas, D; Torralba, A; Huang, TS; Sudderth, EB				Ji, Qiang; Luo, Jiebo; Metaxas, Dimitris; Torralba, Antonio; Huang, Thomas S.; Sudderth, Erik B.			Guest Editors' Introduction to the Special Section on Probabilistic Graphical Models	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									[Ji, Qiang] Rensselaer Polytech Inst, Dept Elect Comp & Syst Engn, Troy, NY 12180 USA; [Luo, Jiebo] Kodak Res Labs, Rochester, NY 14650 USA; [Metaxas, Dimitris] Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA; [Torralba, Antonio] MIT, Dept Elect Engn & Comp Sci, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA; [Huang, Thomas S.] Univ Illinois, Beckman Inst Adv Sci & Technol, Urbana, IL 61801 USA; [Sudderth, Erik B.] Brown Univ, Dept Comp Sci, Providence, RI 02912 USA	Rensselaer Polytechnic Institute; Eastman Kodak; Rutgers State University New Brunswick; Massachusetts Institute of Technology (MIT); University of Illinois System; University of Illinois Urbana-Champaign; Brown University	Ji, Q (corresponding author), Rensselaer Polytech Inst, Dept Elect Comp & Syst Engn, Troy, NY 12180 USA.	qji@ecse.rpi.edu; jiebo.luo@kodak.com; dnm@cs.rutgers.edu; torralba@csail.mit.edu; huang@ifp.uiuc.edu; sudderth@cs.brown.edu	Luo, Jiebo/AAI-7549-2020	Sudderth, Erik/0000-0002-0595-9726; Luo, Jiebo/0000-0002-4516-9729				BINFORD T, 1987, P 3 ANN C UNC ART IN, P73; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4	2	1	1	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2009	31	10					1729	1732		10.1109/TPAMI.2009.160	http://dx.doi.org/10.1109/TPAMI.2009.160			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	483VK	19757542	Green Published			2022-12-18	WOS:000268996500001
J	Chen, K				Chen, Ke			Adaptive smoothing via contextual and local discontinuities (vol 27, pg 1552, 2005)	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Correction									Univ Manchester, Sch Comp Sci, Manchester M13 9PL, Lancs, England	University of Manchester	Chen, K (corresponding author), Univ Manchester, Sch Comp Sci, Manchester M13 9PL, Lancs, England.	chen@cs.manchester.ac.uk	, Ke/ABG-5874-2020; Chen, Ke/C-2560-2008	Chen, Ke/0000-0001-9457-9364				Chen K, 2005, IEEE T PATTERN ANAL, V27, P1552, DOI 10.1109/TPAMI.2005.190	1	1	1	0	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2008	30	10					1872	1872		10.1109/TPAMI.2008.186	http://dx.doi.org/10.1109/TPAMI.2008.186			1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	336DQ					2022-12-18	WOS:000258344900016
J	Xie, XH; Mirmehdi, M				Xie, Xianghua; Mirmehdi, Majid			MAC: Magnetostatic active contour model (vol 30, pg 632, 2008)	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Correction									[Xie, Xianghua; Mirmehdi, Majid] Univ Wales Swansea, Dept Comp Sci, Swansea SA2 8PP, W Glam, Wales; [Mirmehdi, Majid] Univ Bristol, Dept Comp Sci, Bristol BS8 1UB, Avon, England	Swansea University; University of Bristol	Xie, XH (corresponding author), Univ Wales Swansea, Dept Comp Sci, Faraday Tower,Singleton Pk, Swansea SA2 8PP, W Glam, Wales.	majid@cs.bris.ac.uk						Xie XH, 2008, IEEE T PATTERN ANAL, V30, P632, DOI 10.1109/TPAMI.2007.70737	1	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	2008	30	5												1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	272SI					2022-12-18	WOS:000253879700013
J	Trias-Sanz, R; Pierrot-Deseilligny, M; Louchet, J; Stamon, G				Trias-Sanz, Roger; Pierrot-Deseilligny, Marc; Louchet, Jean; Stamon, Georges			Methods for fine registration of cadastre graphs to images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						remote sensing; registration; graph labeling; stochastic methods; cartography	NONRIGID REGISTRATION; RELAXATION; ALGORITHM; CONVERGENCE	We propose two algorithms to match edges in a geometrically imprecise graph to geometrically precise strong boundaries in an image, where the graph is meant to give an a priori partition of the image into objects. This can be used to partition an image into objects described by imprecise external data and, thus, to simplify the segmentation problem. We apply them to the problem of registering cadastre data to georeferenced aerial images, thus correcting the lack of geometrical detail of the cadastre data and the fact that cadastre data gives information of a different nature than that found in images ( fiscal information as opposed to actual land use).	Google Switzerland, CH-8002 Zurich, Switzerland; Natl Lab MATIS, Inst Geog, F-94165 St Mande, France; Equipe COMPLEX, INRIA, F-78153 Rocquencourt, France; Univ Paris 05, Lab SIP CRIP5, F-75006 Paris, France	Google Incorporated; Inria; UDICE-French Research Universities; Universite Paris Cite	Trias-Sanz, R (corresponding author), Google Switzerland, Freigutstr 12, CH-8002 Zurich, Switzerland.	rogerts@google.com; Marc.Pierrot-Deseilligny@ign.fr; jean.louchet@inria.fr; stamon@math-info.univ-paris5.fr						Cachier P, 2003, COMPUT VIS IMAGE UND, V89, P272, DOI 10.1016/S1077-3142(03)00002-X; CHEN Q, 1995, PATTERN RECOGN, V28, P1705, DOI 10.1016/0031-3203(95)00033-V; CHEN Q, 1994, PATTERN RECOGN, V27, P165; Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2; Dijkstra EW, 1959, NUMER MATH, V1, P269, DOI 10.1007/BF01386390; FAUGERAS OD, 1981, IEEE T PATTERN ANAL, V3, P633, DOI 10.1109/TPAMI.1981.4767164; Fu AMN, 1997, PATTERN RECOGN, V30, P1905, DOI 10.1016/S0031-3203(97)00009-5; GAUTAMA S, 2003, P IEEE INT GEOSC REM; Gori M, 2005, IEEE T PATTERN ANAL, V27, P1100, DOI 10.1109/TPAMI.2005.138; Goshtasby A, 2003, COMPUT VIS IMAGE UND, V89, P109, DOI 10.1016/S1077-3142(03)00016-X; GUIGUES L, 2003, P IEEE INT C IMAG P; HIVERNAT C, 1998, RR3529 I RECH INF AU; JUNGNICKEL D, 1999, GRAPHS NETWORKS ALGO, V5; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; Law T, 1996, IEEE T PATTERN ANAL, V18, P481, DOI 10.1109/34.494638; Marr D., 1982, VISION; ROSENFELD A, 1976, IEEE T SYST MAN CYB, V6, P320; TRIASSANZ R, 2004, P ACIVS 2004 BRUSS B, P333; TRIASSANZ R, 2004, P IEEE INT C IMAG PR; VIGLINO JM, 2002, P 13 C REC FORM INT, P135; Wilson RC, 1997, IEEE T PATTERN ANAL, V19, P634, DOI 10.1109/34.601251; ZUCKER SW, 1978, IEEE T SYST MAN CYB, V8, P41; [No title captured]	23	1	3	0	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	2007	29	11					1990	2000		10.1109/TPAMI.2007.1108	http://dx.doi.org/10.1109/TPAMI.2007.1108			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	208UE	17848779				2022-12-18	WOS:000249343900009
J	Balagani, KS; Phoha, VV				Balagani, Kiran S.; Phoha, Vir V.			On the relationship between dependence tree classification error and Bayes error rate	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Bayes error rate; entropy; mutual information; classification; dependence tree approximation	DISCRETE PROBABILITY-DISTRIBUTIONS	Wong and Poon [ 1] showed that Chow and Liu's tree dependence approximation can be derived by minimizing an upper bound of the Bayes error rate. Wong and Poon's result was obtained by expanding the conditional entropy H(omega vertical bar X). We derive the correct expansion of H(omega vertical bar X) and present its implication.	Louisiana Tech Univ, Coll Engn & Sci, Ruston, LA 71272 USA	University of Louisiana System; Louisiana Technical University	Balagani, KS (corresponding author), Louisiana Tech Univ, Coll Engn & Sci, Nethken Hall,Arizona Ave, Ruston, LA 71272 USA.	ksb011@latech.edu; phoha@latech.edu		Phoha, Vir/0000-0002-5390-8253				CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; HELLMAN ME, 1970, IEEE T INFORM THEORY, V16, P368, DOI 10.1109/TIT.1970.1054466; Kruskal J. B., 1956, P AM MATH SOC, V7, P48, DOI [DOI 10.1090/S0002-9939-1956-0078686-7, 10.2307/2033241]; WONG SKM, 1989, IEEE T PATTERN ANAL, V11, P333, DOI 10.1109/34.21803	6	1	1	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	2007	29	10					1866	1868		10.1109/TPAMI.2007.1184	http://dx.doi.org/10.1109/TPAMI.2007.1184			3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	199LA	17699930				2022-12-18	WOS:000248696100015
J	Gustafson, SC; Parker, DR; Martin, RK				Gustafson, Steven C.; Parker, David R.; Martin, Richard K.			Cardinal interpolation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Bayesian statistics; interpolation; modeling and prediction; probability and statistics; regression	KERNEL REGRESSION; NETWORKS	A Bayesian probability density for an interpolating function is developed, and its desirable properties and practical potential are demonstrated. This density has an often needed but previously unachieved property, here called cardinal interpolation, which ensures extrapolation to the density of the least-squares linear model. In particular, the mean of the cardinal interpolation density is a smooth function that intersects given (x, y) points and which extrapolates to their least-squares line, and the variance of this density is a smooth function that is zero at the point x values, that increases with distance from the nearest point x value, and that extrapolates to the well-known quadratic variance function for the least-squares line. The new cardinal interpolation density is developed for Gaussian radial basis interpolators using fully Bayesian methods that optimize interpolator smoothness. This optimization determines the basis function widths and yields an interpolating density that is non-Gaussian except for large magnitude x and which is therefore not the outcome of a Gaussian process. Further, new development shows that the salient property of extrapolation to the density of the least-squares linear model can be achieved for more general approximating (not just interpolating) functions.	USAF, Inst Technol, Wright Patterson AFB, OH 45433 USA; USAF, Res Lab, Wright Patterson AFB, OH 45433 USA	United States Department of Defense; United States Air Force; US Air Force Research Laboratory; United States Department of Defense; United States Air Force; US Air Force Research Laboratory	Gustafson, SC (corresponding author), USAF, Inst Technol, 2950 Hobson Way, Wright Patterson AFB, OH 45433 USA.	steven.gustafson@afit.edu; david.parker2@wpafb.af.mil; richard.martin@afit.edu						Aires F, 2004, NEURAL COMPUT, V16, P2415, DOI 10.1162/0899766041941925; Andrieu C, 2001, NEURAL COMPUT, V13, P2359, DOI 10.1162/089976601750541831; Bailer-Jones CAL, 1999, MATER SCI TECH SER, V15, P287, DOI 10.1179/026708399101505851; Benjamin MA, 2003, J AM STAT ASSOC, V98, P214, DOI 10.1198/016214503388619238; Bishop, 1995, NEURAL NETWORKS PATT; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Christianini N., 2000, INTRO SUPPORT VECTOR; DiMatteo I, 2001, BIOMETRIKA, V88, P1055, DOI 10.1093/biomet/88.4.1055; Hastie T., 2009, ELEMENTS STAT LEARNI, V2nd, DOI DOI 10.1007/978-0-387-21606-5; Lee H. K., 2004, BAYESIAN NONPARAMETR; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; RASMUSSEN C, 2006, GAUSSIAN PROCESSES W; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rivals I, 2000, NEURAL NETWORKS, V13, P463, DOI 10.1016/S0893-6080(99)00080-5; Rudin W., 1976, PRINCIPLES MATH ANAL, V3; Scholkopf B., 2001, LEARNING KERNELS SUP; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Sivia D.S, 1996, DATA ANAL; Stein M.L., 1999, INTERPOLATION SPATIA; Sugiyama M, 2004, NEURAL COMPUT, V16, P1077, DOI 10.1162/089976604773135113; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Wackerly D., 2001, MATH STAT APPL; Weigend A. S, 1994, TIME SERIES PREDICTI; Zhang T, 2005, NEURAL COMPUT, V17, P2077, DOI 10.1162/0899766054323008	27	1	1	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2007	29	9					1538	1545		10.1109/TPAMI.2007.1170	http://dx.doi.org/10.1109/TPAMI.2007.1170			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	189CD	17627042				2022-12-18	WOS:000247965600004
J	Yu, L; Wu, LN				Yu, Lu; Wu, Lenan			Comments on "a separable low complexity 2D HMM with application to face recognition"	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						2D HMM; low complexity; separability		The separable low complexity 2D HMM proposed in [1] builds on the assumption of conditional independence in the relationship between adjacent blocks. The authors view this assumption as the key assumption to reduce the complexity. In this communication, we show that this key assumption is entirely unnecessary.	SE Univ, Dept Radio Engn, Nanjing 210096, Peoples R China; PLA Univ Sci & Technol, Inst Commun Engn, Nanjing 210007, Peoples R China; SE Univ, Dept Radio Engn, Nanjing 210096, Peoples R China	Southeast University - China; Army Engineering University of PLA; Southeast University - China	Yu, L (corresponding author), SE Univ, Dept Radio Engn, Nanjing 210096, Peoples R China.	yulu_mail@263.net; wuln@seu.edu.cn		Yu, Lu/0000-0001-9692-7190				Othman H, 2003, IEEE T PATTERN ANAL, V25, P1229, DOI 10.1109/TPAMI.2003.1233897	1	1	4	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2007	29	2					368	368		10.1109/TPAMI.2007.27	http://dx.doi.org/10.1109/TPAMI.2007.27			1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	116TV	17170489				2022-12-18	WOS:000242826900017
J	Kriegman, DJ				Kriegman, DJ			Untitled	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									Univ Calif San Diego, Dept Comp Sci & Engn, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Kriegman, DJ (corresponding author), Univ Calif San Diego, Dept Comp Sci & Engn, Appl Phys & Math Bldg,9500 Gilman Dr, La Jolla, CA 92093 USA.								0	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	2005	27	1					2	3		10.1109/TPAMI.2005.10	http://dx.doi.org/10.1109/TPAMI.2005.10			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	870BE					2022-12-18	WOS:000225028200001
J	Jacobs, DW; Lindenbaum, M				Jacobs, DW; Lindenbaum, M			Guest editors' introduction to the special section on perceptual organization in computer vision	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA; Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	University System of Maryland; University of Maryland College Park; Technion Israel Institute of Technology	Jacobs, DW (corresponding author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.								0	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	2003	25	6					641	641		10.1109/TPAMI.2003.1201816	http://dx.doi.org/10.1109/TPAMI.2003.1201816			1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	680DP					2022-12-18	WOS:000182961300001
J	Sun, ZH; Tekalp, AM; Navab, N; Ramesh, V				Sun, ZH; Tekalp, AM; Navab, N; Ramesh, V			Interactive optimization of 3D shape and 2D correspondence using multiple geometric constraints via POCS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						geometric constrained shape recovery; structure from motion.; interactive optimization; the factorization approach; projections onto convex sets (POCS)	MOTION	The traditional approach of handling motion tracking and structure from motion (SFM) independently in successive steps exhibits inherent limitations in terms of achievable precision and incorporation of prior geometric constraints about the scene. This paper proposes a projections onto convex sets (POCS) framework for iterative refinement of the measurement matrix in the well-known factorization method to incorporate multiple geometric constraints about the scene, thereby improving the accuracy of both 2D feature point tracking and 3D structure estimates. Regularities in the scene, such as points on line and plane and parallel lines and planes, that can be interactively identified and marked at each POCS iteration, enforce rank and parallelism constraints on appropriately defined local measurement matrices, one for each constraint. The POCS framework allows for the integration of the information in each of these local measurement matrices into a single measurement matrix that is "closest" to the initial observed measurement matrix in Frobenius norm, which is then factored in the usual manner. Experimental results demonstrate that the proposed interactive POCS framework consistently improves both 2D correspondences and 3D shape/motion estimates and similar results can not be achieved by enforcing these constraints as either post or preprocessing.	Kodak Res Labs, Imaging Sci Technol, Rochester, NY 14650 USA; Univ Rochester, Dept Elect & Comp Engn, Rochester, NY 14627 USA; Siemens Corp Res, Imaging & Visualizat Dept, Princeton, NJ 08540 USA	Eastman Kodak; University of Rochester; Siemens AG	Sun, ZH (corresponding author), Kodak Res Labs, Imaging Sci Technol, Bldg 65,1700 Dewey Ave, Rochester, NY 14650 USA.	sun@image.kodak.com; tekalp@ece.rochester.edu; navab@scr.siemens.com; rameshv@scr.siemens.com	Tekalp, Murat/AAW-1060-2020	Sun, Zhaohui/0000-0001-9011-6426; Tekalp, Ahmet Murat/0000-0003-1465-8121				AZARBAYEJANI A, 1995, IEEE T PATTERN ANAL, V17, P562, DOI 10.1109/34.387503; Beardsley PA, 1997, INT J COMPUT VISION, V23, P235, DOI 10.1023/A:1007923216416; BROIDA TJ, 1991, IEEE T PATTERN ANAL, V13, P497, DOI 10.1109/34.87338; Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191; HUANG TS, 1994, P IEEE, V82, P252, DOI 10.1109/5.265351; IRANI M, 2000, EUR C COMP VIS JUN; Liebowitz D., 1999, P EUROGRAPHICS, V18, P39; Morris DD, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P696, DOI 10.1109/ICCV.1998.710793; Poelman CJ, 1997, IEEE T PATTERN ANAL, V19, P206, DOI 10.1109/34.584098; Quan L, 1996, INT J COMPUT VISION, V19, P93, DOI 10.1007/BF00131149; Sezan M I, 1982, IEEE Trans Med Imaging, V1, P95, DOI 10.1109/TMI.1982.4307556; STARK H, 1998, VECT SPAC PROJ NUM A; Sturm P., 1996, LECT NOTES COMPUTER, V1065, P709, DOI [DOI 10.1007/3-540-61123-1, 10.1007/3-540-61123-1_183, DOI 10.1007/3-540-61123-1_183]; Sun ZH, 2001, COMPUT VIS IMAGE UND, V82, P110, DOI 10.1006/cviu.2001.0910; Szeliski R., 1998, 3D Structure from Multiple Images of Large-Scale Environments. European Workshop, SMILE'98. Proceedings, P171; Szeliski R., 1994, Journal of Visual Communication and Image Representation, V5, P10, DOI 10.1006/jvci.1994.1002; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Triggs B., 2000, LECT NOTES COMPUTER, V1883, P298, DOI [DOI 10.1007/3-540-44480-7, DOI 10.1007/3-540-44480-7_21]; WEINSHALL D, 1995, IEEE T PATTERN ANAL, V17, P512, DOI 10.1109/34.391392; WENG JY, 1993, IEEE T PATTERN ANAL, V15, P864, DOI 10.1109/34.232074; Youla D C, 1982, IEEE Trans Med Imaging, V1, P81, DOI 10.1109/TMI.1982.4307555	21	1	1	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	2002	24	4					562	569		10.1109/34.993563	http://dx.doi.org/10.1109/34.993563			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	534FM					2022-12-18	WOS:000174574100012
J	Bowyer, K; Flynn, P				Bowyer, K; Flynn, P			A 20th Anniversary Survey: Introduction to "Content-Based Image Retrieval at the End of the Early Years"	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material													Flynn, Patrick J/J-3388-2013	Flynn, Patrick J/0000-0002-5446-114X					0	1	2	0	11	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2000	22	12					1348	1348		10.1109/TPAMI.2000.895971	http://dx.doi.org/10.1109/TPAMI.2000.895971			1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	383UR					2022-12-18	WOS:000165901900001
J	Cong, G; Parvin, B				Cong, G; Parvin, B			Shape recovery from equal thickness contours	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						shape-from-X; 3D construction; shape from multiple views	SATELLITE RADAR INTERFEROMETRY; PHASE-UNWRAPPING TECHNIQUES; RECONSTRUCTION	A unique imaging modality based on Equal Thickness Contours (ETC) has introduced a new opportunity far 3D shape reconstruction from multiple views. These ETCs can be generated through an interference between transmitted and diffracted beams. We present a computational framework for representing each view of an object in terms of its object thickness and then integrating these representations into a 3D surface by algebraic reconstruction. In this framework, the object thickness is first derived from ideal contours and then extended to real data. For real data, the object thickness is inferred by grouping curve segments that correspond to points of second derivative maxima. At each step of the process, we use some form of regularization to ensure closeness to the original features as well as neighborhood continuity. We apply our approach to images of a submicron crystal structure obtained through a holographic process.	Univ Calif Berkeley, Lawrence Berkeley Lab, Berkeley, CA 94720 USA	United States Department of Energy (DOE); Lawrence Berkeley National Laboratory; University of California System; University of California Berkeley	Cong, G (corresponding author), Univ Calif Berkeley, Lawrence Berkeley Lab, MS 50B-2239,1 Cyclotron Rd, Berkeley, CA 94720 USA.	gcong@media.lbl.gov; parvin@media.lbl.gov						ALLEN CT, 1995, IEEE GEOSCIENCE  SEP, P6; ARONSSON G, 1967, ARIKV MATEMATIK, V7, P133; ARONSSON G, 1981, MANUSCRIPTA MATH, V41, P133; BOISSONNAT JD, 1988, COMPUT VISION GRAPH, V44, P1, DOI 10.1016/S0734-189X(88)80028-8; BOLLE RM, 1991, IEEE T PATTERN ANAL, V13, P1, DOI 10.1109/34.67626; Cong G, 1998, PROC CVPR IEEE, P502, DOI 10.1109/CVPR.1998.698652; CONG G, 1998, P INT C PATT REC; EVANS L, 1993, ELECT J DIFFERENTIAL, P1; Fornaro G, 1997, J OPT SOC AM A, V14, P2702, DOI 10.1364/JOSAA.14.002702; Fornaro G, 1996, J OPT SOC AM A, V13, P2355, DOI 10.1364/JOSAA.13.002355; GABOR D, 1948, NATURE, V777; GABRIEL AK, 1989, J GEOPHYS RES-SOLID, V94, P9183, DOI 10.1029/JB094iB07p09183; GOLDSTEIN RM, 1988, RADIO SCI, V23, P713, DOI 10.1029/RS023i004p00713; GORDON R, 1971, COMMUN ACM, V14, P759, DOI 10.1145/362919.362925; KASHYAP RL, 1975, IEEE T COMPUT, V24, P915, DOI 10.1109/T-C.1975.224337; KREIS T, 1986, J OPT SOC AM A, V3, P847, DOI 10.1364/JOSAA.3.000847; Massonnet D, 1997, SCI AM, V276, P46, DOI 10.1038/scientificamerican0297-46; MASSONNET D, 1993, NATURE, V364, P138, DOI 10.1038/364138a0; MohammadDjafari A, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P165, DOI 10.1109/ICIP.1997.647414; SAINTMARC P, 1991, IEEE T PATTERN ANAL, V13, P514, DOI 10.1109/34.87339; TAKAJO H, 1988, J OPT SOC AM A, V5, P416, DOI 10.1364/JOSAA.5.000416; TONOMURA A, 1995, PROGR HOLOGRAPHIC IN	22	1	1	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2000	22	9					1055	1061		10.1109/34.877527	http://dx.doi.org/10.1109/34.877527			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	361TY					2022-12-18	WOS:000089741300012
J	Arnold, DG; Sturtz, K; Velten, V; Nandhakumar, N				Arnold, DG; Sturtz, K; Velten, V; Nandhakumar, N			Dominant-subspace invariants	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						dominant-subspace invariants; Lie group analysis; principal basis; quasi-invariants; thermophysical invariance; thermophysical model		Object recognition requires robust and stable features that are unique in feature space. Lie group analysis provides a constructive procedure to determine such features, called invariants, when they exist. Absolute invariants are rare in general, so quasi-invariants relax the restrictions required for absolute invariants and, potentially, can be just as useful in real-world applications. This paper develops the concept of a dominant-subspace invariant, a particular type of quasi-invariant, using the theory of Lie groups. A constructive algorithm is provided that fundamentally seeks to determine an integral submanifold which, in practice, is a good approximation to the orbit of the Lie group action. This idea is applied to the long-wave infrared problem and experimental results are obtained supporting the approach. Other application areas are cited.	USAF, Res Lab, AFRL SNAT, Dayton, OH 45433 USA; Veridian Inc, Dayton, OH 45431 USA; Triveni Digital Inc, Princeton Junction, NJ 08550 USA	United States Department of Defense; United States Air Force	Arnold, DG (corresponding author), USAF, Res Lab, AFRL SNAT, Bldg 620,2241 Avion Circle, Dayton, OH 45433 USA.							ARNOLD G, 1999, THESIS U VIRGINIA; ARNOLD G, 1997, P DARPA IM UND WORKS; BINFORD TO, 1993, P DARPA IM UND WORKS, P819; Gonzalez R.C, 2002, DIGITAL IMAGE PROCES; KAPOR D, 1995, P IEEE INT S COMP VI, P97; MICHEL J, 1996, THESIS U VIRGINIA; Mundy J., 1992, GEOMETRIC INVARIANCE; NANDHAKUMAR N, 1988, IEEE T PATTERN ANAL, V10, P469, DOI 10.1109/34.3911; Olver PJ, 1993, APPL LIE GROUPS DIFF, DOI DOI 10.1007/978-1-4684-0274-2; QUAN L, 1995, IEEE T PATTERN ANAL, V17; SHASHUA A, 1995, IEEE T PATTERN ANAL, V17, P779, DOI 10.1109/34.400567; SLATER D, 1998, P SPIE INT C APR; Stiller PF, 1997, P SOC PHOTO-OPT INS, V3168, P262, DOI 10.1117/12.292781; Therrien C. W., 1989, DECISION ESTIMATION; WEISS I, 1998, P DARPA IM UND WORKS, P641; [No title captured]	17	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	2000	22	7					649	662		10.1109/34.865182	http://dx.doi.org/10.1109/34.865182			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	347LV					2022-12-18	WOS:000088931800001
J	Dorst, L; van den Boomgaard, R				Dorst, L; van den Boomgaard, R			The support cone: A representational tool for the analysis of boundaries and their interactions	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						boundary representation; orientation-based representation; Gauss map; slope transform; Legendre transform; support function; Huygens wave propagation; collision detection; mathematical morphology		We present a directional boundary representation which deals locally and consistently with the boundary's "inside." We show that collision and wave propagation are reduced to addition on the spectrum of directions and we derive transformation laws for differential geometrical properties such as directed curvature.	Univ Amsterdam, NL-1098 SJ Amsterdam, Netherlands	University of Amsterdam	Dorst, L (corresponding author), Univ Amsterdam, Kruislaan 403, NL-1098 SJ Amsterdam, Netherlands.	leo@wins.uva.nl						Arnold VI., 1989, MATH METHODS CLASSIC, V2, DOI [10.1007/978-1-4757-1693-1, 10.1007/978-1-4757-2063-1, DOI 10.1007/978-1-4757-2063-1]; Bruce J.W., 1992, CURVES SINGULARITIES; DORST L, 1994, SIGNAL PROCESS, V38, P79, DOI 10.1016/0165-1684(94)90058-2; Dorst L, 1994, COMP IMAG VIS, V2, P161; DORST L, 1999, 9 ISIS U AMST DEP CO; GHOSH PK, 1993, COMPUT GRAPH, V17, P357, DOI 10.1016/0097-8493(93)90023-3; Haralick R.M., 1993, COMPUTER ROBOT VISIO; Heijmans H., 1994, MORPHOLOGICAL IMAGE; HORN BKP, 1984, P IEEE, V72, P1671, DOI 10.1109/PROC.1984.13073; LIANG P, 1994, IEEE T PATTERN ANAL, V16, P249, DOI 10.1109/34.276124; NALWA VS, 1989, INT J COMPUT VISION, V3, P131, DOI 10.1007/BF00126429; ONeill B., 1966, ELEMENTARY DIFFERENT; OSHER S, 1988, J COMPUT PHYS, V79, P12, DOI 10.1016/0021-9991(88)90002-2; PAPPAS RC, 1996, CLIFFORD ALGEBRAS NU, P233; PORTEOUS IR, 1994, GEOMETRIC DIFFERENTI; Schmitt M, 1996, COMP IMAG VIS, P15; Stolfi J., 1991, ORIENTED PROJECTIVE; VANDENBOOMGAARD R, 1992, THESIS U AMSTERDAM, pCH8	18	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	FEB	2000	22	2					174	178		10.1109/34.825755	http://dx.doi.org/10.1109/34.825755			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	292JU					2022-12-18	WOS:000085791400004
J	Soatto, S; Perona, P				Soatto, S; Perona, P			Reducing 'Structure from motion': A general framework for dynamic vision Part 2: Implementation and experimental assessment (vol 20, pg 933, 1998)	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Correction									Washington Univ, Dept Elect Engn, St Louis, MO 63130 USA; CALTECH, Dept Elect Engn & Computat & Neural Syst, Pasadena, CA 91125 USA	Washington University (WUSTL); California Institute of Technology	Soatto, S (corresponding author), Washington Univ, Dept Elect Engn, Campus Box 1127,1 Brookings Dr, St Louis, MO 63130 USA.							Soatto S, 1998, IEEE T PATTERN ANAL, V20, P933, DOI 10.1109/34.713360	1	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT	1998	20	10					1117	1117						1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	128QB					2022-12-18	WOS:000076416400011
J	Sarukkai, RR; Ballard, DH				Sarukkai, RR; Ballard, DH			Phonetic set indexing for fast lexical access	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						speech recognition; lexical access; phonetic set indexing; fast matches; sets; pattern recognition; word prefetching		A novel nonsequential indexing mechanism (termed phonetic set indexing) has been evaluated for the purpose of fast word pre-selection. Our approach to handling the lexical access problem stems from the primary observation that the set of phones which are present in the transcription of a word is sparsely distributed across the vocabulary, and is thus suitable as an indexing key for retrieving a short-list of word possibilities.	Lernout & Hauspie Speech Prod, Kurzweil Dictat Div, Burlington, MA USA; Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA	University of Rochester	Sarukkai, RR (corresponding author), Lernout & Hauspie Speech Prod, Kurzweil Dictat Div, 54 Third Ave, Burlington, MA USA.							Bahl LR, 1993, IEEE T SPEECH AUDI P, V1, P59, DOI 10.1109/89.221368; BAHL LR, 1988, P ICASSP; BURTON DK, 1985, IEEE T ACOUST SPEECH, V33, P837, DOI 10.1109/TASSP.1985.1164650; Ito A., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P299, DOI 10.1109/ICASSP.1993.319296; JAMES DA, 1994, P IEEE ICASSP 9J, P377; JELENIK F, 1989, READINGS SPEECH RECO, P450; Kenny P., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P656, DOI 10.1109/ICASSP.1993.319395; Lee K. F., 1989, AUTOMATIC SPEECH REC; LOEB EP, 1987, P IEEE ICASSP 87; RENALS S, 1995, P IEEE INT C AC SPEE, P596; ROBINSON AJ, 1994, IEEE T NEURAL NETWOR, V5, P298, DOI 10.1109/72.279192; Sarukkai RR, 1997, IEEE T SPEECH AUDI P, V5, P438, DOI 10.1109/89.622567; SARUKKAI RR, 1996, P IEEE ICASSP, P857; SARUKKAI RR, 1995, P ECSA EUROSPEECH 95, P1427	14	1	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	1998	20	1					78	82		10.1109/34.655651	http://dx.doi.org/10.1109/34.655651			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	YV876					2022-12-18	WOS:000071872400007
J	Kay, J				Kay, J			A comparative analysis of methods for pruning decision trees - Comment	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material								This note discusses the inferential procedure used by Esposito et al. [1] to compare the performance of methods of classification, makes some links with recent research on resampling methodology, and mentions some alternative approaches.			Kay, J (corresponding author), UNIV GLASGOW,GLASGOW G12 8QQ,LANARK,SCOTLAND.							Efron B., 1994, MONOGR STAT APPL PRO, DOI DOI 10.1007/978-1-4899-4541-9; ESPOSITO F, 1997, IEEE T PATTERN ANAL, V19; GEISSER S, 1975, J AM STAT ASSOC, V70, P320, DOI 10.2307/2285815; KOHAVI CR, 1995, P INT JOINT C ART IN; RAO CR, 1973, LINEAR STATISTICAL I; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; Shao J., 2012, JACKKNIFE BOOTSTRAP, DOI [10.1007/978-1-4612-0795-5, DOI 10.1007/978-1-4612-0795-5]; Tibshirani R., 1996, BIAS VARIANCE PREDIC; WU CFJ, 1990, ANN STAT, P1438	9	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	1997	19	5					492	493		10.1109/TPAMI.1997.589208	http://dx.doi.org/10.1109/TPAMI.1997.589208			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	XB163					2022-12-18	WOS:A1997XB16300007
J	Gross, AD; Boult, TE				Gross, AD; Boult, TE			Recovery of SHGCs from a single intensity view (vol 18, pg 161, 1996)	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Correction, Addition													Boult, Terrance E./AAT-2134-2021	Boult, Terrance E./0000-0001-5007-2529				Gross AD, 1996, IEEE T PATTERN ANAL, V18, P161, DOI 10.1109/34.481541	1	1	1	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	1996	18	4					471	479		10.1109/TPAMI.1996.491631	http://dx.doi.org/10.1109/TPAMI.1996.491631			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	UG345					2022-12-18	WOS:A1996UG34500015
J	Laurentini, A				Laurentini, A			Efficiently computing and representing aspect graphs of polyhedral objects - Comments	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						aspect graphs; polyhedra; topological definition of aspect		This correspondence is originated by the definition of aspect even in a recent paper [1] on the computation of aspect graphs of polyhedra. Simple examples are presented which show that the data stored according to this definition can be unsuitable for identifying an object or its attitude through a topological match. A definition is suggested which does not incur the problems pointed out.			Laurentini, A (corresponding author), POLITECN TORINO, DIPARTIMENTO AUTOMAT & INFORMAT, TURIN, ITALY.							GIGUS Z, 1991, IEEE T PATTERN ANAL, V13, P542, DOI 10.1109/34.87341; PLANTINGA H, 1990, INT J COMPUT VISION, V5, P137, DOI 10.1007/BF00054919; Stewman J., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P494, DOI 10.1109/CCV.1988.590029	3	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	1996	18	1					57	58		10.1109/34.476011	http://dx.doi.org/10.1109/34.476011			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TP315					2022-12-18	WOS:A1996TP31500006
J	MONTIEL, ME; AGUADO, AS; GARZAJINICH, MA; ALARCON, J				MONTIEL, ME; AGUADO, AS; GARZAJINICH, MA; ALARCON, J			IMAGE MANIPULATION USING M-FILTERS IN A PYRAMIDAL COMPUTER-MODEL	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						MATHEMATICAL MORPHOLOGY; MULTIRESOLUTION; IMAGE PYRAMIDS; IMAGE SEGMENTATION; PARALLEL ALGORITHMS	MATHEMATICAL MORPHOLOGY; DECOMPOSITION	This paper presents how morphological transformations can be related to representations of a set on different lattices. A hierarchical definition of structuring element conveys to a class of multigrid transformations Psi(k) that handle changes on discrete representations of regions. The transformations correspond to upward and downward processes in a hierarchical structure. Based on multigrid transformations, requiring O(log n) time is presented. The approach considers grey level regions as sets and processes through a pyramid to carry out geometric manipulations. Extending the concept of boundary to cope with hierarchical representations of a set, a second method which identifies the boundaries in an image is discussed.	UNIV NACL AUTONOMA MEXICO, INST INVEST MATEMAT APLICADAS & SISTEMAS, MEXICO CITY 01000, DF, MEXICO	Universidad Nacional Autonoma de Mexico	MONTIEL, ME (corresponding author), UNIV SOUTHAMPTON, MOUNTBATTEN BLDG 53, ROOM 4035, SOUTHAMPTON SO17 1BJ, HANTS, ENGLAND.		Alarcon, Juan José/G-7828-2011					Cantoni V., 1986, Intermediate-level image processing, P181; CHEN MH, 1989, IEEE T PATTERN ANAL, V11, P694, DOI 10.1109/34.192464; Gratzer G., 1978, GEN LATTICE THEORY; GROSKY WI, 1986, IEEE T PATTERN ANAL, V8, P639, DOI 10.1109/TPAMI.1986.4767837; GROSS AD, 1987, COMPUT VISION GRAPH, V39, P102, DOI 10.1016/S0734-189X(87)80204-9; HARALICK RM, 1987, IEEE T PATTERN ANAL, V9, P532, DOI 10.1109/TPAMI.1987.4767941; HEIJMANS HJAM, 1990, COMPUT VISION GRAPH, V50, P245, DOI 10.1016/0734-189X(90)90148-O; HEIJMANS HJAM, 1991, IEEE T PATTERN ANAL, V13, P568, DOI 10.1109/34.87343; HONG TH, 1984, IEEE T PATTERN ANAL, V6, P229, DOI 10.1109/TPAMI.1984.4767506; HONG TH, 1984, IEEE T PATTERN ANAL, V6, P222, DOI 10.1109/TPAMI.1984.4767505; MARAGOS P, 1987, IEEE T ACOUST SPEECH, V35, P1170, DOI 10.1109/TASSP.1987.1165254; MARAGOS P, 1987, IEEE T ACOUST SPEECH, V35, P1153, DOI 10.1109/TASSP.1987.1165259; MARAGOS P, 1989, IEEE T PATTERN ANAL, V11, P586, DOI 10.1109/34.24793; MEER P, 1987, IEEE T PATTERN ANAL, V9, P512, DOI 10.1109/TPAMI.1987.4767939; MEYER F, 1986, COMPUT VISION GRAPH, V35, P356, DOI 10.1016/0734-189X(86)90005-8; MORALES A, 1992, JUN P IEEE CS C COMP, P527; PRINCEN J, 1989, COMPUTER VISION 89 P, P92; RONSE C, 1990, SIGNAL PROCESS, V21, P129, DOI 10.1016/0165-1684(90)90046-2; SERRA J, 1986, COMPUT VISION GRAPH, V35, P283, DOI 10.1016/0734-189X(86)90002-2; Serra J., 1982, IMAGE ANAL MATH MORP, pChap11; STEPHEN JL, 1992, ANAL QUANTITATIVE CY, V4, P187; STERNBERG SR, 1986, COMPUT VISION GRAPH, V35, P333, DOI 10.1016/0734-189X(86)90004-6; TOET A, 1990, PATTERN RECOGN LETT, V11, P267, DOI 10.1016/0167-8655(90)90065-A; ZHUANG XH, 1986, COMPUT VISION GRAPH, V35, P370, DOI 10.1016/0734-189X(86)90006-X	25	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	1995	17	11					1110	1115		10.1109/34.473240	http://dx.doi.org/10.1109/34.473240			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	TD854					2022-12-18	WOS:A1995TD85400012
J	SASTRY, R; RANGANATHAN, N; JAIN, RC				SASTRY, R; RANGANATHAN, N; JAIN, RC			VLSI ARCHITECTURES FOR HIGH-SPEED RANGE ESTIMATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						RANGE ESTIMATION; IMAGE PROCESSING; VERY LARGE SCALE INTEGRATION (VLSI) IMPLEMENTATION; INTENSITY GRADIENT; SYSTOLIC ALGORITHM; SPECIAL PURPOSE ARCHITECTURE; HARDWARE ALGORITHM		Depth recovery from gray-scale images is an important topic in the field of computer and robot vision. Intensity gradient analysis (IGA) is a robust technique for inferring depth information from a sequence of images acquired by a sensor undergoing translational motion. IGA obviates the need for explicitly solving the correspondence problem and hence is an efficient technique for range estimation. Many applications require real time processing at very high frame rates. The design of special purpose hardware could significantly speed up the computations in IGA. In this paper, we propose two VLSI architectures for high-speed range estimation based on IGA. The architectures fully utilize the principles of pipelining and parallelism in order to obtain high speed and throughput The designs are conceptually simple and suitable for implementation in VLSI.	UNIV S FLORIDA,DEPT COMP SCI & ENGN,CTR MICROELECTR RES,TAMPA,FL 33620; UNIV CALIF SAN DIEGO,DEPT ELECT & COMP ENGN,LA JOLLA,CA 92093	State University System of Florida; University of South Florida; University of California System; University of California San Diego	SASTRY, R (corresponding author), HAL COMP SYST INC,1315 DELL AVE,CAMPBELL,CA 95008, USA.							BARNARD ST, 1980, IEEE T PATTERN ANAL, V2; BOYER KL, 1988, IEEE T PATTERN ANAL, V10, P144, DOI 10.1109/34.3880; Kung SY., 1988, VLSI ARRAY PROCESSOR; Mukherjee A., 1993, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, V1, P203, DOI 10.1109/92.238415; SANDINI G, 1990, IEEE T PATTERN ANAL, V12, P13, DOI 10.1109/34.41380; SKIFSTAD K, 1989, MACH VISION APPL, V2, P81; Skifstad KD, 1991, HIGH SPEED RANGE EST	7	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1995	17	9					894	899		10.1109/34.406655	http://dx.doi.org/10.1109/34.406655			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RR989					2022-12-18	WOS:A1995RR98900006
J	HELOR, Y; WERMAN, M				HELOR, Y; WERMAN, M			POSE ESTIMATION BY FUSING NOISY DATA OF DIFFERENT DIMENSIONS (VOL 17, PG 195, 1995)	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Correction, Addition																		HELOR Y, 1995, IEEE T PATTERN ANAL, V17, P195, DOI 10.1109/34.368169	1	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	1995	17	5					544	544						1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QW394					2022-12-18	WOS:A1995QW39400012
J	KASTURI, R				KASTURI, R			UNTITLED	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material																			0	1	1	0	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	1995	17	1					1	1						1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QB394					2022-12-18	WOS:A1995QB39400001
J	GROSS, AD				GROSS, AD			TOWARD OBJECT-BASED HEURISTICS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note						SHAPE FROM CONTOUR; OBJECT-BASED HEURISTICS; NONPARAMETRIC SHAPE RECOVERY; OBJECT RECOGNITION; PERCEPTUAL ORGANIZATION	CONTOURS	Recovering the 3-D shape of an object from its 2-D image contour is an important problem in computer vision. In this correspondence, we motivate and develop two object-based heuristics. The structured nature of objects is the motivation for the nonaccidental alignment criterion: Parallel coordinate axes within the object's bounding contour correspond to object-centered coordinate axes. The regularity and symmetry inherent in many man-made objects is the motivation for the orthogonal basis constraint: An oblique set of coordinate axes in the image is presumed to he the projection of an orthogonal set of 3-D coordinate axes in the scene. These object-based heuristics are used to recover shape in both real and synthetic images.	COLUMBIA UNIV,CTR RES INTELLIGENT SYST,NEW YORK,NY 10027	Columbia University	GROSS, AD (corresponding author), CUNY QUEENS COLL,DEPT COMP SCI,FLUSHING,NY 11367, USA.							[Anonymous], 1985, PERCEPTUAL ORG VISUA; BARNARD S, 1984, IEEE WORKSHOP COMPUT, P225; BRADY JM, 1983, MITAIM711 AI LAB TEC; BRADY JM, 1985, 2ND P INT S ROB RES; GROSS AD, 1991, 1991 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, P744; GROSS AD, 1990, 1990 P IEEE COMP SOC, P790; GROSS AD, 1992, THESIS COLUMBIA U NE; KANADE T, 1981, ARTIF INTELL, V17, P409, DOI 10.1016/0004-3702(81)90031-X; KRIEGMAN DJ, 1990, IEEE T PATTERN ANAL, V12, P1127, DOI 10.1109/34.62602; MOHAN R, 1989, THESIS U SO CALIFORN; PONCE J, 1989, IEEE T PATTERN ANAL, V11, P951, DOI 10.1109/34.35498; Ulupinar F., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P414, DOI 10.1109/CCV.1988.590018	12	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	AUG	1994	16	8					794	802		10.1109/34.308474	http://dx.doi.org/10.1109/34.308474			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PB475					2022-12-18	WOS:A1994PB47500004
J	SASTRY, PS; THATHACHAR, MAL				SASTRY, PS; THATHACHAR, MAL			ANALYSIS OF STOCHASTIC AUTOMATA ALGORITHM FOR RELAXATION LABELING	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter						RELAXATION LABELING; LEARNING AUTOMATA; CONSTRAINT SATISFACTION		A parallel stochastic algorithm for relaxation labeling is analyzed. For the case of symmetric compatibility functions, it is Proved that the algorithm will always converge to a consistent labeling.			SASTRY, PS (corresponding author), INDIAN INST SCI,DEPT ELECT ENGN,BANGALORE 560012,KARNATAKA,INDIA.		Sastry, P. S./AAO-4694-2020					BANERJEE S, 1989, THESIS IND I SCI DEP; Basar T., 1999, DYNAMIC NONCOOPERATI, V2; DAVIS LS, 1981, ARTIF INTELL, V17, P245, DOI 10.1016/0004-3702(81)90026-6; HUMMEL RA, 1983, IEEE T PATTERN ANAL, V5, P267, DOI 10.1109/TPAMI.1983.4767390; JIANHUA W, 1988, THEORY GAMES; Kushner HJ., 1984, APPROXIMATION WEAK C; Narendra K.S., 1989, STABLE ADAPTIVE SYST; NARENDRA KS, 1989, LEARNING AUTOMATA IN; SASTRY PS, 1990, SADHANA-ACAD P ENG S, V15, P251, DOI 10.1007/BF02811324; THATHACHAR MAL, 1986, IEEE T PATTERN ANAL, V8, P256, DOI 10.1109/TPAMI.1986.4767779; ZUCKER SW, 1981, IEEE T PATTERN ANAL, V3, P117, DOI 10.1109/TPAMI.1981.4767069	11	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	1994	16	5					538	542		10.1109/34.291442	http://dx.doi.org/10.1109/34.291442			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NP141		Green Accepted			2022-12-18	WOS:A1994NP14100011
J	HUANG, TS; STUCKI, P				HUANG, TS; STUCKI, P			INTRODUCTION TO THE SPECIAL SECTION ON 3-D MODELING IN IMAGE-ANALYSIS AND SYNTHESIS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									UNIV ZURICH,CH-8006 ZURICH,SWITZERLAND; UNIV ILLINOIS,BECKMAN INST,URBANA,IL 61801	University of Zurich; University of Illinois System; University of Illinois Urbana-Champaign	HUANG, TS (corresponding author), UNIV ILLINOIS,COORDINATED SCI LAB,URBANA,IL 61801, USA.								0	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	1993	15	6					529	530						2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	LF257					2022-12-18	WOS:A1993LF25700001
J	VALIVETI, RS; OOMMEN, BJ				VALIVETI, RS; OOMMEN, BJ			RECOGNIZING SOURCES OF RANDOM STRINGS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note						APPROXIMATION AND MODELING; BAYESIAN DECISION; CLOSENESS OF APPROXIMATION; PARAMETER ESTIMATION; PARAMETRIC APPROXIMATION; PATTERN RECOGNITION; PROBABILITY DISTRIBUTIONS; RANDOM PERMUTATION GENERATION		Let us assume that we have a number of independent sources where each source generates random strings of fixed length M, composed of symbols drawn from an alphabet R. Each source generates these random strings according to its own distribution. The problem we consider in this correspondence is one of identifying the source given a sequence of random strings. Two modes of random string generation are analyzed. In the first mode, arbitrary strings are generated in which the individual symbols can occur many times in the strings. In the second mode the individual symbols occur exactly once in each random string. The latter case corresponds to the situation in which the sources generate random permutations. In both these cases, the best match to the distribution being used by each source can be obtained by maintaining an exponential number of statistics. This being infeasible, we propose a simple parametrization of the distributions. For arbitrary strings, the simple unigram based model (U-model) has been proposed. For the case of permutations, we have proposed a new model called the S-model and employed it to analyze and/or approximate unknown distributions of permutations. The relevant estimation procedures together with the applications to source recognition have been presented. Considering the fact that the symbolic data is processed, and statistically analyzed, our method clearly presents a unique blend of syntactic and statistical pattern recognition.			VALIVETI, RS (corresponding author), CARLETON UNIV,SCH COMP SCI,OTTAWA K1S 5B6,ONTARIO,CANADA.		Oommen, B. John/P-6323-2017	Oommen, B. John/0000-0002-5105-1575				DENNING DER, 1983, ENCRYPTION DATA SECU; Duda R.O., 1973, J ROYAL STAT SOC SER; HALL PAV, 1980, ACM COMPUT SURV, V12, P381; KASHYAP RL, 1984, PATTERN RECOGN LETT, V2, P147, DOI 10.1016/0167-8655(84)90038-2; LAWRENCE R, 1975, ACM, V22, P177; Oommen B. J., 1989, Seventeenth Annual ACM Computer Science Conference, P27, DOI 10.1145/75427.75428; OOMMEN BJ, SCSTR138 CARL U OTTA; RAO SS, 1980, OPTIMIZATION METHODS; VALIVETI RS, THESIS; VALIVETI RS, SCSTR161 CARL U OTTA	10	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	APR	1991	13	4					386	394		10.1109/34.88575	http://dx.doi.org/10.1109/34.88575			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	FL566					2022-12-18	WOS:A1991FL56600010
J	GURWITZ, C; OVERTON, ML				GURWITZ, C; OVERTON, ML			A GLOBALLY CONVERGENT ALGORITHM FOR MINIMIZING OVER THE ROTATION GROUP OF QUADRATIC-FORMS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									NYU,COURANT INST MATH SCI,DEPT COMP & INFORMAT SCI,NEW YORK,NY 10012	New York University	GURWITZ, C (corresponding author), CUNY BROOKLYN COLL,DEPT COMP & INFORMAT SCI,BROOKLYN,NY 11210, USA.							AYACHE N, 1988, ROBOTICS RES, P337; FAUGERAS OD, 1987, 1ST P INT C COMP VIS, P25; Fletcher R., 1981, PRACTICAL METHODS OP, P224; Gill P. E., 1981, PRACTICAL OPTIMIZATI; GILL PE, 1985, NUMERICAL OPTIMIZATI, P139; GILL PE, 1985, ACM SIGNUM NEWSLETTE, V20, P13; GURWITZ CB, 1986, 219 NEW YORK U COMP; HAN SP, 1976, MATH PROGRAM, V11, P263, DOI 10.1007/BF01580395; LOWE DG, 1987, INT J COMPUT VISION, V1, P57, DOI 10.1007/BF00128526; MURRAY W, 1982, MATH PROGRAM STUD, V16, P62; NOCEDAL J, 1985, SIAM J NUMER ANAL, V22, P821, DOI 10.1137/0722050; Powell M. J. D., 1978, NONLINEAR PROGRAM, P27; SCHWARTZ JT, 1985, 165 NEW YORK U DEP C	13	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	1989	11	11					1228	1232		10.1109/34.42863	http://dx.doi.org/10.1109/34.42863			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AW796					2022-12-18	WOS:A1989AW79600012
J	YANG, MCK; YANG, CC				YANG, MCK; YANG, CC			IMAGE-ENHANCEMENT FOR SEGMENTATION BY SELF-INDUCED AUTOREGRESSIVE FILTERING	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note									USN,RES LAB,WASHINGTON,DC 20375	United States Department of Defense; United States Navy; Naval Research Laboratory	YANG, MCK (corresponding author), UNIV FLORIDA,DEPT STAT,GAINESVILLE,FL 32611, USA.							Allan T. D., 1983, SATELLITE MICROWAVE; [Anonymous], 1976, TIME SERIES ANAL; Ashjari B., 1985, Proceedings CVPR '85: IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No. 85CH2145-1), P172; Box G.E., 1970, J AM STAT ASSOC, V65, P1509, DOI DOI 10.1080/01621459.1970.10481180; CHANG H, 1978, IEEE T CIRCUITS SYST, V25, P1051, DOI 10.1109/TCS.1978.1084432; CHELLAPPA R, 1985, IEEE T ACOUST SPEECH, V33, P174; CROSS GR, 1983, IEEE T PATTERN ANAL, V5, P25, DOI 10.1109/TPAMI.1983.4767341; DELP EJ, 1979, PATTERN RECOGN, V11, P313, DOI 10.1016/0031-3203(79)90041-4; DIACONIS P, 1981, J MATH PSYCHOL, V24, P112, DOI 10.1016/0022-2496(81)90039-0; FULLER WA, 1976, INTRO STATISTICAL TI; HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328; JAIN AK, 1981, P IEEE, V69, P502, DOI 10.1109/PROC.1981.12021; JAIN AK, 1977, J OPTIMIZ THEORY APP, V23, P65, DOI 10.1007/BF00932298; JULESZ B, 1968, METHODOL PATTERN REC, P297; Kashyap RL, 1982, PATTERN RECOGN LETT, V1, P43, DOI 10.1016/0167-8655(82)90050-2; KASHYAP RL, 1986, IEEE T PATTERN ANAL, V8, P472, DOI 10.1109/TPAMI.1986.4767811; KASHYAP RL, 1984, IEEE T INFORM THEORY, V30, P736, DOI 10.1109/TIT.1984.1056955; KEYTE GE, 1986, IEEE T GEOSCI REMOTE, V24, P552, DOI 10.1109/TGRS.1986.289670; KINSMAN B, 1965, WIND WAVES; LEE JS, 1986, OPT ENG, V25, P636, DOI 10.1117/12.7973877; LIM JS, 1979, P IEEE, V67, P1586, DOI 10.1109/PROC.1979.11540; MCLEOD AI, 1978, J ROY STAT SOC B MET, V40, P296; MONALDO FM, 1986, IEEE T GEOSCI REMOTE, V24, P543, DOI 10.1109/TGRS.1986.289669; Ozaki T., 1977, J R STAT SOC C-APPL, V26, P290, DOI DOI 10.2307/2346970; RABINER LR, 1975, THEORY APPLICATION D; ROSENFELD A, 1985, COMPUT VISION GRAPH, V34, P204; THERRIEN CW, 1986, P IEEE, V74, P532, DOI 10.1109/PROC.1986.13504; THERRIEN CW, 1983, COMPUT VISION GRAPH, V22, P313, DOI 10.1016/0734-189X(83)90079-8; TJOSTHEIM D, 1978, ADV APPL PROBAB, V10, P130, DOI 10.1017/S0001867800029517; TOU JT, 1981, IMAGE MODELING, P391; VANGOOL L, 1985, COMPUT VISION GRAPH, V29, P336, DOI 10.1016/0734-189X(85)90130-6; YOUSSEF MN, 1984, AT&T TECH J, V63, P819, DOI 10.1002/j.1538-7305.1984.tb00107.x	32	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	1989	11	6					655	661		10.1109/34.24800	http://dx.doi.org/10.1109/34.24800			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	U6749					2022-12-18	WOS:A1989U674900011
J	NOMURA, Y; NARUSE, H				NOMURA, Y; NARUSE, H			REDUCTION OF OBSCURATION NOISE USING MULTIPLE IMAGES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											NOMURA, Y (corresponding author), NIPPON TELEGRAPH & TEL PUBL CORP, IBARAKI ELECT COMMUN LABS, TOKAI, IBARAKI 31911, JAPAN.							BROWN CM, 1983, IEEE T PATTERN ANAL, V5, P493, DOI 10.1109/TPAMI.1983.4767428; Gonzalez R.C., 1977, DIGITAL IMAGE PROCES; HALL EL, 1974, IEEE T COMPUT, VC 23, P207, DOI 10.1109/T-C.1974.223892; HALL EL, 1971, IEEE T COMPUT, VC 20, P1032, DOI 10.1109/T-C.1971.223399; KASTURI R, 1985, IEEE T SYST MAN CYB, V15, P352, DOI 10.1109/TSMC.1985.6313370; KOBAYASHI Y, 1985, HITACHI HYORON, V67, P63; KUAN DT, 1985, IEEE T PATTERN ANAL, V7, P165, DOI 10.1109/TPAMI.1985.4767641; PREWITT JMS, 1966, ANN NY ACAD SCI, V128, P1031; Rosenfeld Azriel, 1976, DIGITAL PICTURE PROC, V2, P8; WESZKA JS, 1979, IEEE T SYST MAN CYB, V9, P38; YANG CJ, 1981, COMPUT GRAPHICS IMAG, V15, P224	11	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	1988	10	2					267	270		10.1109/34.3888	http://dx.doi.org/10.1109/34.3888			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	M2974					2022-12-18	WOS:A1988M297400010
J	SANZ, JLC				SANZ, JLC			INTRODUCTION TO THE SPECIAL PAMI ISSUES ON INDUSTRIAL MACHINE VISION AND COMPUTER VISION TECHNOLOGY	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									UNIV CALIF DAVIS,COMP VIS RES LAB,DAVIS,CA 95616	University of California System; University of California Davis	SANZ, JLC (corresponding author), IBM CORP,RES LAB,DEPT COMP SCI,SAN JOSE,CA 95114, USA.							BLANZ EW, IN PRESS IEEE J ROBO; RUETZ P, 1988, IN PRESS MACHINE VIS	2	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	1988	10	1					1	3						3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	L4366					2022-12-18	WOS:A1988L436600001
J	MORGERA, SD				MORGERA, SD			TOWARD A FUNDAMENTAL THEORY OF OPTIMAL FEATURE-SELECTION .2. IMPLEMENTATION AND COMPUTATIONAL-COMPLEXITY	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											MORGERA, SD (corresponding author), MCGILL UNIV, DEPT ELECT ENGN, 3480 UNIV ST, MONTREAL H3A 2A7, QUEBEC, CANADA.							Andrew A. L., 1973, Linear Algebra and Its Applications, V7, P151, DOI 10.1016/0024-3795(73)90049-9; Bauer Friedrich L., 1957, Z ANGEW MATH PHYS, V8, P214, DOI DOI 10.1007/BF01600502; CANTONI A, 1976, LINEAR ALGEBRA APPL, V13, P275, DOI 10.1016/0024-3795(76)90101-4; DANIELSSON PE, 1984, IEEE T COMPUT, V33, P652, DOI 10.1109/TC.1984.5009339; DATTA L, 1986, IEEE T ACOUST SPEECH, V34, P992, DOI 10.1109/TASSP.1986.1164890; Datta L., 1984, Seventh International Conference on Pattern Recognition (Cat. No. 84CH2046-1), P138; DATTA L, 1984, THESIS CONCORDIA U M; DATTA L, 1985, 4TH P SCAND C IM AN, P87; DATTA L, 1984, DEC P INT C COMP SYS, P81; Davis P. J., 1979, CIRCULANT MATRICES; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; Hotelling H, 1933, J EDUC PSYCHOL, V24, P498, DOI 10.1037/h0070888; HU YH, 1985, IEEE T ACOUST SPEECH, V33, P1264, DOI 10.1109/TASSP.1985.1164672; KRISHNA H, UNPUB IEEE T ACOUST; KUNG SY, 1983, IEEE T ACOUST SPEECH, V31, P66, DOI 10.1109/TASSP.1983.1164051; MERCHANT GA, 1982, IEEE T ACOUST SPEECH, V30, P40, DOI 10.1109/TASSP.1982.1163845; Morgera S. D., 1984, Seventh International Conference on Pattern Recognition (Cat. No. 84CH2046-1), P134; MORGERA SD, 1984, IEEE T PATTERN ANAL, V6, P601, DOI 10.1109/TPAMI.1984.4767573; MORGERA SD, 1977, IEEE T INFORM THEORY, V23, P728, DOI 10.1109/TIT.1977.1055798; MORGERA SD, 1982, SIGNAL PROCESS, V4, P425, DOI 10.1016/0165-1684(82)90057-3; Okamoto M., 1961, OSAKA MATH J, V13, P1; Wilkinson JH., 1965, ALGEBRAIC EIGENVALUE	22	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JAN	1987	9	1					29	38		10.1109/TPAMI.1987.4767870	http://dx.doi.org/10.1109/TPAMI.1987.4767870			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	F3785	21869375				2022-12-18	WOS:A1987F378500003
J	KRUEGER, FR				KRUEGER, FR			ANALYSIS OF THE MULTITHRESHOLD THRESHOLD ELEMENT - COMMENTS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											KRUEGER, FR (corresponding author), STANFORD UNIV,DEPT OPERAT RES,STANFORD,CA 94305, USA.								0	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV	1986	8	6					760	761		10.1109/TPAMI.1986.4767858	http://dx.doi.org/10.1109/TPAMI.1986.4767858			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	E4469					2022-12-18	WOS:A1986E446900008
J	PAVLIDIS, T				PAVLIDIS, T			LOW-LEVEL SEGMENTATION - AN EXPERT SYSTEM - COMMENT	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									AT&T BELL LABS,MURRAY HILL,NJ 07974	AT&T; Nokia Corporation; Nokia Bell Labs	PAVLIDIS, T (corresponding author), SUNY STONY BROOK,DEPT ELECT ENGN,STONY BROOK,NY 11794, USA.							CHEN PC, 1979, COMPUT VISION GRAPH, V10, P172, DOI 10.1016/0146-664X(79)90049-2; HOROWITZ SL, 1976, J ACM, V23, P368, DOI 10.1145/321941.321956	2	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	1986	8	5					675	676		10.1109/TPAMI.1986.4767843	http://dx.doi.org/10.1109/TPAMI.1986.4767843			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	D7584					2022-12-18	WOS:A1986D758400011
J	MEIER, A; ILG, M				MEIER, A; ILG, M			CONSISTENT OPERATIONS ON A SPATIAL DATA STRUCTURE	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note									SWISS FED INST TECHNOL,IKT,IMAGE SCI GRP,CH-8092 ZURICH,SWITZERLAND	Swiss Federal Institutes of Technology Domain; ETH Zurich	MEIER, A (corresponding author), SWISS FED INST TECHNOL,DEPT COMP SCI,CH-8092 ZURICH,SWITZERLAND.							Abrial J.R., 1974, DATA SEMANTICS DATA, P1; BRODIE LM, 1981, P C VERY LARGE DATA, P32; CHANG SK, 1981, COMPUTER, V14; CLAUS V, 1979, LECTURE NOTES COMPUT, V73; Codd E. F., 1979, ACM Transactions on Database Systems, V4, P397, DOI 10.1145/320107.320109; De Antonellis V., 1981, Proceedings of the Seventh International Conference on Very Large Data Bases, P23; EHRIG H, 1983, LECTURE NOTES COMPUT, V153; HARALICK RM, 1980, MAP DATA PROCESSING, P63; Levesque H., 1979, ASS NETWORKS REPRESE, P93; LORIE RA, 1984, GEOPROCESSING, V2; MEIER A, 1985, INFORM SYST, V10, P9, DOI 10.1016/0306-4379(85)90004-3; MEIER A, 1982, 6TH P INT COMP SOFTW, P476; Minsky M., 1969, PERCEPTRONS INTRO CO, DOI 10.7551/mitpress/11301.001.0001; NAGY G, 1979, COMPUT SURV, V11, P139, DOI 10.1145/356770.356777; Schmidt J. W., 1977, ACM Transactions on Database Systems, V2, P247, DOI 10.1145/320557.320568; [No title captured]	16	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUL	1986	8	4					532	538		10.1109/TPAMI.1986.4767818	http://dx.doi.org/10.1109/TPAMI.1986.4767818			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	C7400					2022-12-18	WOS:A1986C740000013
J	NIKOLIC, ZJ; FU, KS				NIKOLIC, ZJ; FU, KS			AN ALGORITHM FOR LEARNING WITHOUT EXTERNAL SUPERVISION AND ITS APPLICATION TO LEARNING CONTROL-SYSTEMS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											NIKOLIC, ZJ (corresponding author), PURDUE UNIV,W LAFAYETTE,IN 47907, USA.							AIZERMAN, 1964, AVTOMATIKA TELEMEKHA, V25; AIZERMAN, 1964, AVTOMATIKA TELEMEKHA, V25, P917; Bellman R., 1961, ADAPTIVE CONTROL PRO; DOBROVIDOV AV, 1964, AVTOMATIKA TELEMEKHA, V25; DVORETZKY A, 1956, 3RD P BERK S MATH ST, V1; FELDBAUM AA, 1965, DISCIPLINES TECHNIQU; Fisz M., 1963, PROBABILITY THEORY M; FU KS, 1964, COMPUTER INFORMATION; GOOD IJ, 1959, SCIENCE, V129; MATHEWS J, 1964, MATH METHODS PHYSICS; MCLAREN RW, 1964 INT C MICR CIRC; MCMURTRY GJ, 1965 P NEC, V21, P494; MCMURTRY GJ, 1965, THESIS PURDUE U LAFA; MCMURTRY GJ, 1964, 2ND ALL C CIRC SYST; SKLANSKY J, 1963, 2 MODE THRESHOLD LEA; TAKACS L, 1960, METHUENS MONOGRAPHS; TOU JT, 1965, MODERN CONTROL THEOR; TSETLIN ML, 1963, AVTOMATIKA TELEMAKHA, V22; VARSHAVSKIJ VI, 1963, AVTOMATIKA TELEMEKHA, V21	19	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAY	1986	8	3					304	312		10.1109/TPAMI.1986.4767793	http://dx.doi.org/10.1109/TPAMI.1986.4767793			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	C0841	21869349				2022-12-18	WOS:A1986C084100002
J	ARAVENA, JL; PORTER, WA				ARAVENA, JL; PORTER, WA			ONE-DIMENSIONAL SCAN SELECTION FOR TWO-DIMENSIONAL SIGNAL RESTORATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											ARAVENA, JL (corresponding author), LOUISIANA STATE UNIV,DEPT ELECT & COMP ENGN,BATON ROUGE,LA 70803, USA.							ARAVENA JL, 1985, IEEE T ACOUST SPEECH, V33; DESANTIS RM, 1982, INT J CONTR; DESANTIS RM, 1978, BASIC OPTIMAL ESTIMA; EKSTROM MP, 1982, IEEE T ACOUST SPEECH, V30, P31, DOI 10.1109/TASSP.1982.1163844; FEINTUCH A, 1982, SYSTEM THEORY HILBER; KATAYAMA T, 1982, IEEE T AUTOMAT CONTR, V27, P1024, DOI 10.1109/TAC.1982.1103064; MERSEREAU RM, 1975, P IEEE, V63, P610, DOI 10.1109/PROC.1975.9795; Naylor A., 1971, LINEAR OPERATOR THEO; PORTER W, 1966, MODERN F SYSTEMS ENG; SAEKS R, 1973, RESOLUTION SPACE OPE; WOODS JW, 1977, IEEE T INFORM THEORY, V23, P473, DOI 10.1109/TIT.1977.1055750; WOODS JW, 1981, IEEE T ACOUST SPEECH, V29, P188, DOI 10.1109/TASSP.1981.1163533	12	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	MAR	1986	8	2					282	284		10.1109/TPAMI.1986.4767782	http://dx.doi.org/10.1109/TPAMI.1986.4767782			3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	A1073	21869347				2022-12-18	WOS:A1986A107300016
J	APTE, CV; WEISS, SM				APTE, CV; WEISS, SM			AN APPROACH TO EXPERT CONTROL OF INTERACTIVE SOFTWARE SYSTEMS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									RUTGERS STATE UNIV,DEPT COMP SCI,NEW BRUNSWICK,NJ 08903	Rutgers State University New Brunswick	APTE, CV (corresponding author), IBM CORP,THOMAS J WATSON RES CTR,DEPT COMP TECHNOL,YORKTOWN HTS,NY 10598, USA.							AIKINS JS, 1982, HPP82013 STANF U DEP; [Anonymous], 1980, PRINCIPLES ARTIFICIA; BENNETT J, 1978, HPP7823 STANF U TECH; CLANCEY WJ, 1984, P AAAI, P49; Davis R., 1981, P 7 INT JOINT C ART, P846; Duda R., 1981, READ ARTIF INTELL, P334; FAGAN L, 1978, HPP7816 STANF U TECH; MCDERMOTT J, 1982, ARTIF INTELL, V19, P39, DOI 10.1016/0004-3702(82)90021-2; Shortliffe E.H., 2012, COMPUTER BASED MED C; VESONDER GT, 1983, 8TH P INT JOINT C AR, P116; WEISS SM, 1978, ARTIF INTELL, V11, P145, DOI 10.1016/0004-3702(78)90015-2; WEISS SM, 1979, 6TH P INT JOINT C AR, P942; WEISS SM, 1981, 7TH P INT JOINT C AR, P853	13	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1985	7	5					586	591		10.1109/TPAMI.1985.4767705	http://dx.doi.org/10.1109/TPAMI.1985.4767705			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	AQH94	21869295				2022-12-18	WOS:A1985AQH9400009
J	DINSTEIN, I; YEN, DWL; FLICKNER, MD				DINSTEIN, I; YEN, DWL; FLICKNER, MD			HANDLING MEMORY OVERFLOW IN CONNECTED COMPONENT LABELING APPLICATIONS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									BEN GURION UNIV NEGEV,DEPT ELECT & COMP ENGN,IL-84105 BEERSHEBA,ISRAEL	Ben Gurion University								AGRAWALA AK, 1977, COMPUT VISION GRAPH, V6, P538, DOI 10.1016/S0146-664X(77)80015-4; DINSTEIN I, 1983, IBM RJ3937 RES REP; FOITH JP, 1981, DIGITAL IMAGE PROCES; LUMIA R, 1983, COMPUT VISION GRAPH, V22, P287, DOI 10.1016/0734-189X(83)90071-3; Minsky M., 1972, PERCEPTRONS INTRO CO; Rosenfeld Azriel, 1976, DIGITAL PICTURE PROC, V2, P8; SELKOW SM, 1972, J ACM, V19, P283, DOI 10.1145/321694.321701; VEILLON F, 1979, SIGNAL PROCESS, V1, P175, DOI 10.1016/0165-1684(79)90018-5	8	1	3	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1985	7	1					116	121		10.1109/TPAMI.1985.4767627	http://dx.doi.org/10.1109/TPAMI.1985.4767627			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	ABF09	21869249				2022-12-18	WOS:A1985ABF0900014
J	SELFRIDGE, PG; MAHAKIAN, S				SELFRIDGE, PG; MAHAKIAN, S			DISTRIBUTED COMPUTING FOR VISION - ARCHITECTURE AND A BENCHMARK TEST	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									VIEW ENGN,SIMI VALLEY,CA 93063		SELFRIDGE, PG (corresponding author), AT&T BELL LABS,DEPT ROBOT SYST RES,HOLMDEL,NJ 07733, USA.							Ahuja S. R., 1983, IEEE Journal on Selected Areas in Communications, VSAC-1, P751, DOI 10.1109/JSAC.1983.1145984; ALLEBACH JP, 1980, IBM RC817735509 RES; BRACHO R, 1982, CMURITR823 CARN U RO; BRIGGS FA, 1982, IEEE T COMPUT, V31, P969, DOI 10.1109/TC.1982.1675906; Duff M. J. B., 1976, 3rd International Joint Conference on Pattern Recognition, P728; KELLY MD, 1981, MACHINE INTELLIGENCE, V6, P379; KUIPERS BJ, 1975, REPRESENTATION UNDER; KUSHNER T, 1982, IEEE T COMPUT, V31, P943, DOI 10.1109/TC.1982.1675903; Mimaroglu T., 1982, Proceedings of PRIP 82. IEEE Computer Society Conference on Pattern Recognition and Image Processing, P386; ROSENFELD A, 1984, MULTIRESOLUTION IMAG; SEGEN J, 1983, SPIE INTELLIGENT ROB, V449, P676; SELFRIDGE PG, 1982, P SOC PHOTO-OPT INST, V336, P173, DOI 10.1117/12.933626; SELFRIDGE PG, 1983, 13TH P INT S IND ROB, V7, P17; STERNBERG R, 1983, COMPUTER, P22	14	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1985	7	5					623	626		10.1109/TPAMI.1985.4767710	http://dx.doi.org/10.1109/TPAMI.1985.4767710			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	AQH94	21869300				2022-12-18	WOS:A1985AQH9400014
J	HANSON, AR; RISEMAN, EM; NAGIN, PA				HANSON, AR; RISEMAN, EM; NAGIN, PA			IMAGE SEGMENTATION - STUDIES IN GLOBAL AND LOCAL HISTOGRAM-GUIDED RELAXATION ALGORITHMS - REPLY	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									TUFTS UNIV,NEW ENGLAND MED CTR,DEPT OPHTHALMOL,BOSTON,MA 02111	Tufts Medical Center; Tufts University	HANSON, AR (corresponding author), UNIV MASSACHUSETTS,DEPT COMP & INFORMAT SCI,AMHERST,MA 01002, USA.							KOHLER R, THESIS U MASSACHUSET; OHLANDER R, 1978, COMPUT VISION GRAPH, V8, P313, DOI 10.1016/0146-664X(78)90060-6	2	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1984	6	2					249	249		10.1109/TPAMI.1984.4767510	http://dx.doi.org/10.1109/TPAMI.1984.4767510			1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	SF591	21869191				2022-12-18	WOS:A1984SF59100013
J	LAHART, MJ				LAHART, MJ			ESTIMATION OF ERROR RATES IN CLASSIFICATION OF DISTORTED IMAGERY	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Note									USN,RES LAB,WASHINGTON,DC 20375	United States Department of Defense; United States Navy; Naval Research Laboratory								BENNETT RS, 1969, IEEE T INFORM THEORY, V15, P517, DOI 10.1109/TIT.1969.1054365; CASASENT D, 1977, APPL OPTICS, V16, P1652, DOI 10.1364/AO.16.001652; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; DUDA RO, 1973, PATTERN CLASSIFICATI, P100; FUKUNAGA K, 1982, TREE8236 PURD U TECH; JOHN S, 1960, SANKHYA, V22, P301; LAHART MJ, 1970, J OPT SOC AM, V60, P319, DOI 10.1364/JOSA.60.000319; LAHART MJ, 1971, J OPT SOC AM, V61, P985; MOSTAFAVI H, 1978, IEEE T AERO ELEC SYS, V14, P487, DOI 10.1109/TAES.1978.308610; SHORT RD, 1981, IEEE T INFORM THEORY, V27, P622, DOI 10.1109/TIT.1981.1056403	10	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1984	6	4					535	542		10.1109/TPAMI.1984.4767560	http://dx.doi.org/10.1109/TPAMI.1984.4767560			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	SY289	21869223	Green Submitted			2022-12-18	WOS:A1984SY28900016
J	AGARWAL, KK				AGARWAL, KK			SOLVING PROBLEMS IN ROBOTICS WITH SEMANTIC NETWORKS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											AGARWAL, KK (corresponding author), TRINITY UNIV,DEPT COMP & INFORMAT SCI,SAN ANTONIO,TX 78284, USA.							AGARWAL KK, 1978, COMPUT CHEM, V2, P75, DOI 10.1016/0097-8485(78)87005-3; AGARWAL KK, 1976, 63 STAT U NEW YORK T; AGARWAL KK, 1980, MAR P PRINC U C INF; BUNDY A, 1978, ARTIFICIAL INTELLIGE; Ernst G, 1969, GPS CASE STUDY GEN P; FIKES RE, 1971, ARTIF INTELL, V2, P189, DOI 10.1016/0004-3702(71)90010-5; GELERNTER HL, 1977, SCIENCE         0909; HUNT EB, 1975, ARTIFICIAL INTELLIGE; Nilsson N.J., 1971, PROBLEM SOLVING METH; RAPHAEL B, 1976, THINKING COMPUTER MI; SANDERS AF, 1976, THESIS STATE U NEW Y; SLAGLE JR, 1971, ARTIFICIAL INTELLIGE; Winston P. H., 1977, ARTIFICIAL INTELLIGE; [No title captured]	14	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1983	5	2					213	217		10.1109/TPAMI.1983.4767374	http://dx.doi.org/10.1109/TPAMI.1983.4767374			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)	Computer Science; Engineering	QJ974	21869103				2022-12-18	WOS:A1983QJ97400011
J	BACKER, E; HAAS, HPA; GETREUER, R				BACKER, E; HAAS, HPA; GETREUER, R			ON THE STABILITY OF SHARED NEAR-NEIGHBOR CLUSTERING	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											BACKER, E (corresponding author), DELFT UNIV TECHNOL,DEPT ELECT ENGN,DELFT,NETHERLANDS.							BACKER E, 1982, 6TH P INT C PATT REC; DUBES R, 1976, PATTERN RECOGN, V8, P247, DOI 10.1016/0031-3203(76)90045-5; Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x; FU KS, 1976, DIGITAL PATTERN RECO; GOWDA KC, 1978, PATTERN RECOGN, V10, P105; JARVIS RA, 1973, IEEE T COMPUT, VC-22, P1025, DOI 10.1109/T-C.1973.223640; JARVIS RA, 1982, IEEE COMPUTER, V15, P8; JARVIS RA, 1976, 4TH P INT C PATT REC; ZAHN CT, 1971, IEEE T COMPUT, VC 20, P68, DOI 10.1109/T-C.1971.223083	9	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1983	5	2					220	224		10.1109/TPAMI.1983.4767376	http://dx.doi.org/10.1109/TPAMI.1983.4767376			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	QJ974	21869105				2022-12-18	WOS:A1983QJ97400013
J	JARVIS, JF; MUNDY, JL				JARVIS, JF; MUNDY, JL			SPECIAL SECTION ON INDUSTRIAL APPLICATIONS OF MACHINE VISION - INTRODUCTION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									RENSSELAER POLYTECH INST,DEPT MATH SCI,TROY,NY 12181	Rensselaer Polytechnic Institute	JARVIS, JF (corresponding author), BELL TEL LABS INC,DEPT ROBOT SYST RES,HOLMDEL,NJ 07733, USA.							JARVIS JF, 1982, IEEE COMPUTER, V15, P55; 1982 WORKSH IND APPL	2	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1983	5	6					561	562		10.1109/TPAMI.1983.4767444	http://dx.doi.org/10.1109/TPAMI.1983.4767444			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	RV488					2022-12-18	WOS:A1983RV48800001
J	LAVIN, P				LAVIN, P			RESTORATION OF A FEATURE CLOSED CLASS OF TWO-DIMENSIONAL IMAGES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									HARVARD UNIV,SCH PUBL HLTH,DEPT BIOSTAT,BOSTON,MA 02115	Harvard University; Harvard T.H. Chan School of Public Health	LAVIN, P (corresponding author), SIDNEY FARBER CANC INST,DIV BIOSTAT & EPIDEMIOL,BOSTON,MA 02115, USA.							BASS LJ, 1967, MATH COMPUT, V21, P712, DOI 10.2307/2005017; CARLSSON S, 1967, SKAND AKTUARIETIDSSK, V3, P113; EWALD G, 1965, MATH ANN, V162, P140; FU KS, 1981, PATTERN RECOGN, V13, P3, DOI 10.1016/0031-3203(81)90028-5; GRENANDE.U, 1969, Q APPL MATH, V27, P1; Grenander Ulf, 1970, ADV COMPUT, V10, P175, DOI DOI 10.1016/S0065-2458(08)60436-2; PAVEL M, 1981, PATTERN RECOGN, V14, P117, DOI 10.1016/0031-3203(81)90052-2; PAVEL M, 1979, PATTERN RECOGN, V11, P325, DOI 10.1016/0031-3203(79)90042-6; Renyi A., 1964, Z WAHRSCHEIN, V3, P138, DOI DOI 10.1007/BF00535973; RENYI A, 1967, Z WAHRSCHEINLICH KEI, V2, P75	10	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1983	5	1					14	24		10.1109/TPAMI.1983.4767340	http://dx.doi.org/10.1109/TPAMI.1983.4767340			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PZ844	21869079				2022-12-18	WOS:A1983PZ84400003
J	CHEN, CH				CHEN, CH			SPECIAL SECTION ON DIGITAL SIGNAL AND WAVEFORM ANALYSIS - INTRODUCTION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material											CHEN, CH (corresponding author), SOUTHEASTERN MASSACHUSETTS UNIV,N DARTMOUTH,MA 02747, USA.							Kittler J, 1978, PATTERN RECOGNITION; 1981, 2ND P INT S COMP AID	2	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	2					97	98		10.1109/TPAMI.1982.4767212	http://dx.doi.org/10.1109/TPAMI.1982.4767212			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NE957					2022-12-18	WOS:A1982NE95700001
J	MOHR, R				MOHR, R			A REFINEMENT OF A SPHERICAL DECOMPOSITION ALGORITHM	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									UNIV PENN,MOORE SCH ELECT ENGN,DEPT COMP & INFORMAT SCI,PHILADELPHIA,PA 19104	University of Pennsylvania								OROURKE J, 1979, IEEE T PATTERN ANAL, V1, P295, DOI 10.1109/TPAMI.1979.4766925	1	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	1					51	51		10.1109/TPAMI.1982.4767194	http://dx.doi.org/10.1109/TPAMI.1982.4767194			1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MY534	21869003				2022-12-18	WOS:A1982MY53400008
J	SNYDER, WE; TANG, DA				SNYDER, WE; TANG, DA			A COUNTEREXAMPLE TO A DIAMETER ALGORITHM FOR CONVEX POLYGONS - COMMENT	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											SNYDER, WE (corresponding author), N CAROLINA STATE UNIV,DEPT ELECT ENGN,RALEIGH,NC 27607, USA.								1	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	3					309	309		10.1109/TPAMI.1982.4767249	http://dx.doi.org/10.1109/TPAMI.1982.4767249			1	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	NN069	21869040				2022-12-18	WOS:A1982NN06900010
J	WEGMAN, EJ; GOULD, J				WEGMAN, EJ; GOULD, J			AUTOMATA IN RANDOM-ENVIRONMENTS WITH APPLICATION TO MACHINE INTELLIGENCE	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									HUGHES AIRCRAFT CO,FULLERTON,CA 92634	Hughes Aircraft Company	WEGMAN, EJ (corresponding author), US OFF NAVAL RES,PROGRAM STAT & PROBABIL,ARLINGTON,VA 22217, USA.							ASO H, 1976, IEEE T SYST MAN CYB, V6, P494, DOI 10.1109/TSMC.1976.4309535; ECCLES JC, 1964, PHYSL SYNAPSIS; GOULD J, 1980, FUNDAMENTA INFORMATI, V3, P117; GOULD J, 1976, P INT S INFORM THEOR, P837; GOULD J, 1980, FUNDAMENTA INFORMATI, V3, P1; JOHNSTON TD, 1981, BEHAV BRAIN SCI, V4, P125, DOI 10.1017/S0140525X00007950; KENT EW, 1981, BRAINS MEN MACHINES; Minsky M.L., 1967, COMPUTATION; PAZ A, 1970, PROBABILISTIC AUTOMA; PYLYSHYN ZW, 1980, BEHAV BRAIN SCI, V3, P111, DOI 10.1017/S0140525X00002053; RABIN MO, 1963, INFORM CONTR, V13, P230; Tsetlin M. L, 1961, AUTOMAT REM CONTR+, V22, P1345; Turing AM, 1937, P LOND MATH SOC, V42, P230, DOI 10.1112/plms/s2-42.1.230; Varshavskii V. I., 1963, AVTOMAT TELEMEKH, V24, P353; WEGMAN EJ, 1977, 1ST P INT C MATH MOD, V1, P501	15	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1982	4	5					485	492		10.1109/TPAMI.1982.4767292	http://dx.doi.org/10.1109/TPAMI.1982.4767292			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	PG894	21869067				2022-12-18	WOS:A1982PG89400004
J	BARRERO, A; GONZALEZ, RC; THOMASON, MG				BARRERO, A; GONZALEZ, RC; THOMASON, MG			EQUIVALENCE AND REDUCTION OF EXPANSIVE TREE-GRAMMARS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									UNIV TENNESSEE,DEPT COMP SCI,KNOXVILLE,TN 37916	University of Tennessee System; University of Tennessee Knoxville	BARRERO, A (corresponding author), UNIV TENNESSEE,DEPT ELECT ENGN,KNOXVILLE,TN 37916, USA.							Aho A.V., 1972, THEORY PARSING TRANS; BRAINERD WS, 1969, INFORM CONTROL, V14, P217, DOI 10.1016/S0019-9958(69)90065-5; BRAINERD WS, 1968, INFORM CONTROL, V13, P484, DOI 10.1016/S0019-9958(68)90917-0; FU KS, 1973, IEEE T COMPUT, VC 22, P1087, DOI 10.1109/T-C.1973.223654; Gonzalez RC, 1978, SYNTACTIC PATTERN RE; Tou JT, 1974, PATTERN RECOGN	6	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1981	3	2					204	206		10.1109/TPAMI.1981.4767080	http://dx.doi.org/10.1109/TPAMI.1981.4767080			3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MN968	21868937				2022-12-18	WOS:A1981MN96800012
J	INIGO, RM; MCVEY, ES				INIGO, RM; MCVEY, ES			CCD IMPLEMENTATION OF A 3-DIMENSIONAL VIDEO-TRACKING ALGORITHM	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											INIGO, RM (corresponding author), UNIV VIRGINIA,SCH ENGN & APPL SCI,CHARLOTTESVILLE,VA 22901, USA.							CAFFORIO C, 1976, IEEE T INFORM THEORY, V22, P573, DOI 10.1109/TIT.1976.1055602; CLINE RE, 1964, SIAM J, V12, P589; DENYER PB, 1979, JAN P IEEE, V67; FENNEMA CL, 1979, COMPUT VISION GRAPH, V9, P301, DOI 10.1016/0146-664X(79)90097-2; GANS D, 1969, TRANSFORMATIONS GEOM; GILBERT B, 1975, IEEE J SOLID-ST CIRC, V10, P437, DOI 10.1109/JSSC.1975.1050639; GILBERT B, 1980, ISSCC 80 DIG TECH PA; HAGUE YA, 1977, IEEE J SOLID STATE C, V12, P642; LIMB JO, 1975, COMPUTERS GRAPHI DEC, V4; NAHI NE, 1978, IEEE T AUTOMAT CONTR, V23, P834, DOI 10.1109/TAC.1978.1101841; SCHALKOFF RJ, 1979, NOV COMPSAC P; SCHALKOFF RJ, 1979, IEEE T MACHINE I JUN; SMITH J, 1975, IEEE J SOLID-ST CIRC, V10, P448, DOI 10.1109/JSSC.1975.1050640	13	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1981	3	2					230	240		10.1109/TPAMI.1981.4767086	http://dx.doi.org/10.1109/TPAMI.1981.4767086			11	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MN968	21868943				2022-12-18	WOS:A1981MN96800018
J	SCLOVE, SL				SCLOVE, SL			PATTERN-RECOGNITION IN IMAGE-PROCESSING USING INTERPIXEL CORRELATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											SCLOVE, SL (corresponding author), UNIV ILLINOIS,DEPT MATH,CHICAGO,IL 60680, USA.							HABIBI A, 1972, PR INST ELECTR ELECT, V60, P878, DOI 10.1109/PROC.1972.8787; HASSNER M, 1978, MAY P IEEE COMP SOC, P346; JAIN AK, 1977, IEEE T COMPUT, V26, P343, DOI 10.1109/TC.1977.1674844; JAIN AK, 1978, IEEE T AUTOMAT CONTR, V23, P817, DOI 10.1109/TAC.1978.1101881; JAIN AK, 1977, J OPTIMIZ THEORY APP, V23, P65, DOI 10.1007/BF00932298; LANDGREBE DA, 1978, MAY P IEEE COMP SOC, P470	6	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1981	3	2					206	208		10.1109/TPAMI.1981.4767081	http://dx.doi.org/10.1109/TPAMI.1981.4767081			3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	MN968	21868938				2022-12-18	WOS:A1981MN96800013
J	ANDERSON, WL				ANDERSON, WL			OPTICAL-PROCESSING FOR LEAST MEAN-SQUARES ANALYSIS OF BIOMEDICAL PATTERNS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article											ANDERSON, WL (corresponding author), UNIV HOUSTON,DEPT ELECT ENGN,COHERENT OPT LAB,HOUSTON,TX 77004, USA.							ANDERSON WL, 1968, J OPT SOC AM, V58, P1566; ANDERSON WL, 1979, J OPT SOC AM, V69, P1684, DOI 10.1364/JOSA.69.001684; ANDERSON WL, 1971, APPL OPTICS, V10, P1503, DOI 10.1364/AO.10.001503; BARD Y, 1974, NONLINEAR PARAMETER, P59; CARLSON FP, 1978, IEEE T BIO-MED ENG, V25, P361, DOI 10.1109/TBME.1978.326262; CROWELL JM, 1978, IEEE T BIO-MED ENG, V25, P519, DOI 10.1109/TBME.1978.326285; CROWELL JM, 1978, IEEE T BIOMED ENG, V25, P525; DODGE C, 1974, J OPTICAL SOC AM, V64, pA544; Goodman J. W., 2005, MCGRAW HILL PHYS QUA; HICKMAN CP, 1970, INTEGRATED PRINCIPLE, P617; LANCZOS C, 1956, APPLIED ANAL; PREWITT J M S, 1976, P287; SALZMAN GC, 1975, ACTA CYTOL, V19, P374; SHEN SY, 1974, J OPT SOC AM, V64, P1399; STACK H, 1976, APPL OPTICS, V15, P2246; STARK H, 1975, J OPT SOC AM, V65, P425, DOI 10.1364/JOSA.65.000425; STARK H, 1975, J OPT SOC AM, V65, P1436, DOI 10.1364/JOSA.65.001436; STARK H, 1977, APPL OPTICS, V16, P1670, DOI 10.1364/AO.16.001670; WARD JH, 1968, J OPT SOC AM, V58, P1566; 1958, VANNOSTRANDS SCI ENC, P202	20	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1980	2	5					458	463		10.1109/TPAMI.1980.6592367	http://dx.doi.org/10.1109/TPAMI.1980.6592367			6	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	KW185					2022-12-18	WOS:A1980KW18500009
J	FILIPSKI, AJ				FILIPSKI, AJ			A LEAST MEAN-SQUARED ERROR APPROACH TO SYNTACTIC CLASSIFICATION	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											FILIPSKI, AJ (corresponding author), ARIZONA STATE UNIV,DEPT MATH,TEMPE,AZ 85281, USA.							BOOTH TL, 1973, IEEE T COMPUT, VC 22, P442, DOI 10.1109/T-C.1973.223746; BOULLION TL, 1971, GENERALIZED INVERSE; Duda RO, 1973, PATTERN RECOGNITION; FILIPSKI AJ, 1976, TR7602 MICH STAT U C; FILIPSKI AJ, 1977, APR IEEE WORKSH PICT; Fu K.S., 1974, MATH SCI ENG; LEE HC, 1972, INFORMATION SYSTEMS; LEE HC, 1972, TREE7217 PURD U SCH; Tou JT, 1974, PATTERN RECOGN	9	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1980	2	3					252	255		10.1109/TPAMI.1980.4767012	http://dx.doi.org/10.1109/TPAMI.1980.4767012			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JR843	21868898				2022-12-18	WOS:A1980JR84300012
J	MASCARENHAS, NDA; FERNANDES, LFV				MASCARENHAS, NDA; FERNANDES, LFV			NEW METHODS FOR PICTURE RECONSTRUCTION - RECURSIVE AND CAUSAL TECHNIQUES	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter									CTR TECN AEROESP,INST TECNOL AERONAUT,SAN JOSE DOS CAMPOS,BRAZIL	Comando-Geral de Tecnologia Aeroespacial (CTA); Instituto Tecnologico de Aeronautica (ITA)								ALBERT A, 1972, REGRESSION MOOREPENR, P125; ALBERT A, 1972, REGRESSION MOORE PEN, P19; ALBERT AE, 1967, STOCHASTIC APPROXIMA, pCH7; Fedorov V.V., 1972, THEORY OPTIMAL EXPT; GRAUPE D, 1976, IDENTIFICATION SYSTE, P125; HERMAN GT, 1976, INFORM CONTROL, V31, P364, DOI 10.1016/S0019-9958(76)80013-7; Kashyap R. L., 1973, 1st International Joint Conference on Pattern Recognition, P286; KASHYAP RL, 1975, IEEE T COMPUT, V24, P915, DOI 10.1109/T-C.1975.224337; Krishnamurthy E. V., 1974, COMPUT GRAPHICS IMAG, V3, P336; MASCARENHAS NDA, 1975, IEEE T CIRCUITS SYST, VCA22, P252, DOI 10.1109/TCS.1975.1084026; MASCARENHAS NDA, 1972, USCEE425 U SO CAL EL, P58; SARIDIS GN, 1974, IEEE T AUTOMAT CONTR, VAC19, P798, DOI 10.1109/TAC.1974.1100716; TEWARSON RP, 1975, AUG IM PROC 2D 3D RE; WEE WG, 1976, IEEE T SYST MAN CYB, V6, P486, DOI 10.1109/TSMC.1976.4309534	14	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1980	2	4					369	376		10.1109/TPAMI.1980.4767036	http://dx.doi.org/10.1109/TPAMI.1980.4767036			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	JZ206	21868913				2022-12-18	WOS:A1980JZ20600010
J	CHIEN, YT; PAVLIDIS, T				CHIEN, YT; PAVLIDIS, T			SPECIAL ISSUE ON SELECTED PAPERS FROM THE 1978 PRINCETON-WORKSHOP-ON-PATTERN-RECOGNITION-AND-ARTIFICIAL-INTELLIGENCE - PREFACE	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									PRINCETON UNIV,DEPT ELECT ENGN & COMP SCI,PRINCETON,NJ 08540	Princeton University	CHIEN, YT (corresponding author), UNIV CONNECTICUT,DEPT ELECT ENGN & COMP SCI,STORRS,CT 06268, USA.							MYERS W, 1976, COMPUTER, V9, P48, DOI 10.1109/C-M.1976.218441; 1976, COMPUTER, V9; 1976, IEEE T COMPUT, V25	3	1	1	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1979	1	2					125	126		10.1109/TPAMI.1979.4766897	http://dx.doi.org/10.1109/TPAMI.1979.4766897			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HA304					2022-12-18	WOS:A1979HA30400001
J	LINKS, LH; BIEMOND, J				LINKS, LH; BIEMOND, J			NON-SEPARABILITY OF IMAGE MODELS	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											LINKS, LH (corresponding author), DELFT UNIV TECHNOL,DEPT ELECT ENGN,INFORMAT THEORY GRP,DELFT,NETHERLANDS.							Bellman R., 1972, DYNAMIC PROGRAMMING; HABIBI A, 1972, PR INST ELECTR ELECT, V60, P878, DOI 10.1109/PROC.1972.8787; JAIN AK, 1974, IEEE T COMPUT, V13, P470; PRATT WK, 1975, COMPUT GRAPHICS IMAG, V4, P1	4	1	1	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1979	1	4					409	411		10.1109/TPAMI.1979.4766950	http://dx.doi.org/10.1109/TPAMI.1979.4766950			3	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HV227	21868876				2022-12-18	WOS:A1979HV22700010
J	NAHIN, PJ				NAHIN, PJ			SIMPLIFIED DERIVATION OF FREI HISTOGRAM HYPERBOLIZATION FOR IMAGE-ENHANCEMENT	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Letter											NAHIN, PJ (corresponding author), UNIV NEW HAMPSHIRE,DEPT ELECT & COMP ENGN,DURHAM,NH 03824, USA.							DEUTSCH S, 1967, MODELS NERVOUS SYSTE, P201; FREI W, 1977, COMPUT VISION GRAPH, V6, P286, DOI 10.1016/S0146-664X(77)80030-0; GREGORY RL, 1974, EYE BRAIN PSYCHOLOGY, P79; HUMMEL R, 1977, COMPUT VISION GRAPH, V6, P184, DOI 10.1016/S0146-664X(77)80011-7; Hummel R. A., 1975, COMPUT GRAPH IMAGE P, V4, P209; MACLEOD IDG, 1970, IEEE T COMPUT, VC 19, P160, DOI 10.1109/T-C.1970.222883; PERRY B, 1964, COMMUN ACM, V7, P311, DOI 10.1145/364099.364336; WOODWARD PM, 1964, PROBABILITY INFORMAT, P21	8	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1979	1	4					414	415		10.1109/TPAMI.1979.4766952	http://dx.doi.org/10.1109/TPAMI.1979.4766952			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HV227	21868878				2022-12-18	WOS:A1979HV22700012
J	SRIHARI, SN				SRIHARI, SN			RECURSIVE IMPLEMENTATION OF A 2-STEP NONPARAMETRIC DECISION RULE	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article									WAYNE STATE UNIV,COMP SCI SECT,DETROIT,MI 48202	Wayne State University			Srihari, Sargur N/E-8100-2011					COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; COVER TM, 1976, COMMUNICATION CYBERN, V10; Dudani S. A., 1976, IEEE Transactions on Systems, Man and Cybernetics, VSMC-6, P325, DOI 10.1109/TSMC.1976.5408784; Forsythe GE., 1977, COMPUTER METHODS MAT; FRALICK SC, 1971, IEEE T INFORM THEORY, V17, P440, DOI 10.1109/TIT.1971.1054663; KANAL L, 1974, IEEE T INFORM THEORY, V20, P697, DOI 10.1109/TIT.1974.1055306; MEISEL WS, 1969, IEEE T COMPUT, VC 18, P911, DOI 10.1109/T-C.1969.222546; MURTHY VK, 1965, ANN MATH STAT, V36, P1027, DOI 10.1214/aoms/1177700074; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; SPECHT DF, 1967, IEEE TRANS ELECTRON, VEC16, P308, DOI 10.1109/PGEC.1967.264667; WAGNER TJ, 1975, IEEE T INFORM THEORY, V21, P438, DOI 10.1109/TIT.1975.1055408	11	1	1	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.		1979	1	1					90	94		10.1109/TPAMI.1979.4766881	http://dx.doi.org/10.1109/TPAMI.1979.4766881			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	HA303	21868836				2022-12-18	WOS:A1979HA30300012
J	Ahmed, H; Wilbur, RB; Bharadwaj, HM; Siskind, JM				Ahmed, Hamad; Wilbur, Ronnie B.; Bharadwaj, Hari M.; Siskind, Jeffrey Mark			Confounds in the Data-Comments on "Decoding Brain Representations by Multimodal Learning of Neural Activity and Visual Features"	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material						Object classification; EEG; human vision; neuroscience; neuroimaging; brain-computer interface		Neuroimaging experiments in general, and EEG experiments in particular, must take care to avoid confounds. A recent TPAMI paper uses data that suffers from a serious previously reported confound. We demonstrate that their new model and analysis methods do not remedy this confound, and therefore that their claims of high accuracy and neuroscience relevance are invalid.	[Ahmed, Hamad; Siskind, Jeffrey Mark] Purdue Univ, Siskind Elmore Family Sch Elect & Comp, W Lafayette, IN 47907 USA; [Wilbur, Ronnie B.; Bharadwaj, Hari M.] Purdue Univ, Dept Speech Language & Hearing Sci, W Lafayette, IN 47907 USA; [Wilbur, Ronnie B.] Purdue Univ, Dept Linguist, W Lafayette, IN 47907 USA; [Bharadwaj, Hari M.] Purdue Univ, Weldon Sch Biomed Engn, W Lafayette, IN 47907 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus; Purdue University System; Purdue University; Purdue University West Lafayette Campus; Purdue University System; Purdue University; Purdue University West Lafayette Campus; Purdue University System; Purdue University; Purdue University West Lafayette Campus	Siskind, JM (corresponding author), Purdue Univ, Siskind Elmore Family Sch Elect & Comp, W Lafayette, IN 47907 USA.	ahmed90@purdue.edu; wilbur@purdue.edu; hbharadwaj@purdue.edu; qobi@purdue.edu		Bharadwaj, Hari/0000-0001-8685-9630	U.S. National Science Foundation [1522954-IIS, 1734938-IIS]; Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) [D17PC00341]; National Institutes of Health [R01DC015989]; Siemens Corporation, Corporate Technology	U.S. National Science Foundation(National Science Foundation (NSF)); Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Siemens Corporation, Corporate Technology	This work was supported, in part by the U.S. National Science Foundation under Grants 1522954-IIS and 1734938-IIS, in part by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DOI/IBC) Contract under Grant D17PC00341, in part by the National Institutes of Health under Grant R01DC015989, and in part by Siemens Corporation, Corporate Technology.	Deng J., 2009, 2009 IEEE C COMP VIS, P248, DOI [DOI 10.1109/CVPR.2009.5206848, 10.1109/CVPR.2009.5206848]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lawhern VJ, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aace8c; Li R, 2021, IEEE T PATTERN ANAL, V43, P316, DOI 10.1109/TPAMI.2020.2973153; Li YT, 2017, ADV NEUR IN, V30; Palazzo S, 2021, IEEE T PATTERN ANAL, V43, P3833, DOI 10.1109/TPAMI.2020.2995909; Spampinato C, 2017, PROC CVPR IEEE, P4503, DOI 10.1109/CVPR.2017.479; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308	10	0	0	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9217	9220		10.1109/TPAMI.2021.3121268	http://dx.doi.org/10.1109/TPAMI.2021.3121268			4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34665721	Green Accepted, hybrid			2022-12-18	WOS:000880661400048
J	Artacho, B; Savakis, A				Artacho, Bruno; Savakis, Andreas			UniPose plus : A Unified Framework for 2D and 3D Human Pose Estimation in Images and Videos	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Pose estimation; Videos; Computer architecture; Task analysis; Decoding; Biological system modeling; Human pose estimation; 3D human pose estimation; computer vision; deep learning		We propose UniPose+, a unified framework for 2D and 3D human pose estimation in images and videos. The UniPose+ architecture leverages multi-scale feature representations to increase the effectiveness of backbone feature extractors, with no significant increase in network size and no postprocessing. Current pose estimation methods heavily rely on statistical postprocessing or predefined anchor poses for joint localization. The UniPose+ framework incorporates contextual information across scales and joint localization with Gaussian heatmap modulation at the decoder output to estimate 2D and 3D human pose in a single stage with state-of-the-art accuracy, without relying on predefined anchor poses. The multi-scale representations allowed by the waterfall module in the UniPose+ framework leverage the efficiency of progressive filtering in the cascade architecture, while maintaining multi-scale fields-of-view comparable to spatial pyramid configurations. Our results on multiple datasets demonstrate that UniPose+, with a HRNet, ResNet or SENet backbone and waterfall module, is a robust and efficient architecture for single person 2D and 3D pose estimation in single images and videos.	[Artacho, Bruno; Savakis, Andreas] Rochester Inst Technol, Dept Comp Engn, Rochester, NY 14623 USA	Rochester Institute of Technology	Savakis, A (corresponding author), Rochester Inst Technol, Dept Comp Engn, Rochester, NY 14623 USA.	bmartacho@mail.rit.edu; andreas.savakis@rit.edu			National Science Foundation [1749376]	National Science Foundation(National Science Foundation (NSF))	This work was supported by the National Science Foundation under Grant 1749376.	Alexander C. Berg, 2015, Arxiv, DOI arXiv:1506.04579; Alexander Toshev, 2016, Arxiv, DOI arXiv:1605.02346; Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Artacho B, 2020, PROC CVPR IEEE, P7033, DOI 10.1109/CVPR42600.2020.00706; Artacho B, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19245361; Belagiannis V, 2017, IEEE INT CONF AUTOMA, P468, DOI 10.1109/FG.2017.64; Benzine A, 2020, PROC CVPR IEEE, P6855, DOI 10.1109/CVPR42600.2020.00689; Bulat A, 2020, IEEE INT CONF AUTOMA, P8, DOI 10.1109/FG47880.2020.00014; Bulat A, 2016, LECT NOTES COMPUT SC, V9911, P717, DOI 10.1007/978-3-319-46478-7_44; Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Cao ZZ, 2019, IEEE INT CONF MULTI, P567, DOI 10.1109/ICMEW.2019.00103; Chen CH, 2017, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR.2017.610; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543; Cheng Y, 2019, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2019.00081; Chu X, 2017, PROC CVPR IEEE, P5669, DOI 10.1109/CVPR.2017.601; Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214; Guanghan Ning, 2019, Arxiv, DOI arXiv:1905.02822; Guler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762; Guohui Zhang, 2019, Arxiv, DOI arXiv:1902.07837; Hao ZX, 2018, INT CONF 3D VISION, P304, DOI 10.1109/3DV.2018.00043; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Hu JJ, 2019, IEEE WINT CONF APPL, P1043, DOI 10.1109/WACV.2019.00116; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Iqbal U, 2017, IEEE INT CONF AUTOMA, P438, DOI 10.1109/FG.2017.61; Jiao JB, 2018, LECT NOTES COMPUT SC, V11219, P55, DOI 10.1007/978-3-030-01267-0_4; Johnson S., 2010, P BRIT MACH VIS C; Katircioglu I, 2018, INT J COMPUT VISION, V126, P1326, DOI 10.1007/s11263-018-1066-6; Ke LP, 2018, LECT NOTES COMPUT SC, V11206, P731, DOI 10.1007/978-3-030-01216-8_44; Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530; Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234; Li SJ, 2015, LECT NOTES COMPUT SC, V9004, P332, DOI 10.1007/978-3-319-16808-1_23; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Luo Y, 2018, PROC CVPR IEEE, P5207, DOI 10.1109/CVPR.2018.00546; Martin Thoma, 2016, Arxiv, DOI arXiv:1602.06541; Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Nibali A, 2019, IEEE WINT CONF APPL, P1477, DOI 10.1109/WACV.2019.00162; Papandreou G, 2018, LECT NOTES COMPUT SC, V11218, P282, DOI 10.1007/978-3-030-01264-9_17; Park D, 2011, IEEE I CONF COMP VIS, P2627, DOI 10.1109/ICCV.2011.6126552; Pavlakos G, 2018, PROC CVPR IEEE, P7307, DOI 10.1109/CVPR.2018.00763; Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138; Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794; Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533; Pishchulin L, 2013, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2013.82; Remelli E, 2020, PROC CVPR IEEE, P6039, DOI 10.1109/CVPR42600.2020.00608; Rogez G, 2020, IEEE T PATTERN ANAL, V42, P1146, DOI 10.1109/TPAMI.2019.2892985; Rogez G, 2017, PROC CVPR IEEE, P1216, DOI 10.1109/CVPR.2017.134; Song J, 2017, PROC CVPR IEEE, P5563, DOI 10.1109/CVPR.2017.590; Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584; Tang W, 2018, LECT NOTES COMPUT SC, V11207, P197, DOI 10.1007/978-3-030-01219-9_12; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175; Xiaotian Chen, 2019, Arxiv, DOI arXiv:1907.06023; Xu D, 2018, PROC CVPR IEEE, P675, DOI 10.1109/CVPR.2018.00077; Xu D, 2017, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2017.25; Xu JW, 2020, PROC CVPR IEEE, P896, DOI 10.1109/CVPR42600.2020.00098; Yang Y, 2013, IEEE T PATTERN ANAL, V35, P2878, DOI 10.1109/TPAMI.2012.261; Yu F., 2016, P ICLR 2016; Zanfir A, 2018, ADV NEUR IN, V31; Zanfir A, 2018, PROC CVPR IEEE, P2148, DOI 10.1109/CVPR.2018.00229; Zerui Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P715, DOI 10.1007/978-3-030-58580-8_42; Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712; Zhang WY, 2013, IEEE I CONF COMP VIS, P2248, DOI 10.1109/ICCV.2013.280; Zhou XW, 2019, IEEE T PATTERN ANAL, V41, P901, DOI 10.1109/TPAMI.2018.2816031; Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537; Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51; Zhou XY, 2016, LECT NOTES COMPUT SC, V9915, P186, DOI 10.1007/978-3-319-49409-8_17	77	0	0	3	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9641	9653		10.1109/TPAMI.2021.3124736	http://dx.doi.org/10.1109/TPAMI.2021.3124736			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34727028				2022-12-18	WOS:000880661400077
J	Bian, JW; Zhan, HY; Wang, NY; Chin, TJ; Shen, CH; Reid, I				Bian, Jia-Wang; Zhan, Huangying; Wang, Naiyan; Chin, Tat-Jun; Shen, Chunhua; Reid, Ian			Auto-Rectify Network for Unsupervised Indoor Depth Estimation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cameras; Videos; Training; Geometry; Estimation; Unsupervised learning; Three-dimensional displays; Single-view depth estimation; unsupervised learning; image rectification	MONOCULAR DEPTH	Single-View depth estimation using the CNNs trained from unlabelled videos has shown significant promise. However, excellent results have mostly been obtained in street-scene driving scenarios, and such methods often fail in other settings, particularly indoor videos taken by handheld devices. In this work, we establish that the complex ego-motions exhibited in handheld settings are a critical obstacle for learning depth. Our fundamental analysis suggests that the rotation behaves as noise during training, as opposed to the translation (baseline) which provides supervision signals. To address the challenge, we propose a data pre-processing method that rectifies training images by removing their relative rotations for effective learning. The significantly improved performance validates our motivation. Towards end-to-end learning without requiring pre-processing, we propose an Auto-Rectify Network with novel loss functions, which can automatically learn to rectify images during training. Consequently, our results outperform the previous unsupervised SOTA method by a large margin on the challenging NYUv2 dataset. We also demonstrate the generalization of our trained model in ScanNet and Make3D, and the universality of our proposed learning method on 7-Scenes and KITTI datasets.	[Bian, Jia-Wang; Zhan, Huangying; Chin, Tat-Jun; Reid, Ian] Univ Adelaide, Adelaide, SA 5005, Australia; [Bian, Jia-Wang; Zhan, Huangying; Chin, Tat-Jun; Reid, Ian] Australian Ctr Robot Vis, Brisbane, Qld 4000, Australia; [Wang, Naiyan] TuSimple, Beijing 100016, Peoples R China; [Shen, Chunhua] Zhejiang Univ, Hangzhou 310027, Peoples R China	University of Adelaide; Australian Centre for Robotic Vision; Zhejiang University	Bian, JW (corresponding author), Univ Adelaide, Adelaide, SA 5005, Australia.	jiawang.bian@gmail.com; huangying.zhan@adelaide.edu.au; tatjun@gmail.com; chunhua.shen@adelaide.edu.au; ian.reid@adelaide.edu.au		Bian, Jiawang/0000-0003-2046-3363; Shen, Chunhua/0000-0002-8648-8718	Australian Centre of Excellence for Robotic Vision [CE140100016]; ARC Laureate Fellowship [FL130100102]	Australian Centre of Excellence for Robotic Vision; ARC Laureate Fellowship(Australian Research Council)	This work was supported in part by the Australian Centre of Excellence for Robotic Vision under Grant CE140100016 and in part by ARC Laureate Fellowship under Grant FL130100102 to Prof. Ian Reid.	Bian J.-W., 2019, PROC BRIT MACH VIS C; Bian JW, 2021, INT J COMPUT VISION, V129, P2548, DOI 10.1007/s11263-021-01484-6; Bian JW, 2020, INT J COMPUT VISION, V128, P1580, DOI 10.1007/s11263-019-01280-3; Casser V, 2019, AAAI CONF ARTIF INTE, P8001; Chakrabarti A., 2016, PROC NEURAL INF PROC, P2666; Chen YH, 2019, IEEE I CONF COMP VIS, P7062, DOI 10.1109/ICCV.2019.00716; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049; DeTone Daniel, 2016, ARXIV160603798; Do T., 2020, P EUR C COMP VIS, P265; Eigen D, 2014, ADV NEUR IN, V27; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214; Fusiello A, 2000, MACH VISION APPL, V12, P16, DOI 10.1007/s001380050120; Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Gordon A, 2019, IEEE I CONF COMP VIS, P8976, DOI 10.1109/ICCV.2019.00907; Guizilini V, 2020, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR42600.2020.00256; Hartley R., 2003, MULTIPLE VIEW GEOMET; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; Jaderberg M, 2015, ADV NEUR IN, V28; Karsch K, 2014, IEEE T PATTERN ANAL, V36, P2144, DOI 10.1109/TPAMI.2014.2316835; Kingma D.P., 2015, INT C LEARN REPR, P1; Ladicky L, 2014, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2014.19; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Lee S, 2021, AAAI CONF ARTIF INTE, V35, P1863; Li B, 2015, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2015.7298715; Li J, 2017, IEEE I CONF COMP VIS, P3392, DOI 10.1109/ICCV.2017.365; Li ZQ, 2021, IEEE T PATTERN ANAL, V43, P4229, DOI 10.1109/TPAMI.2020.2974454; Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283; Liu MM, 2014, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2014.97; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luo CX, 2020, IEEE T PATTERN ANAL, V42, P2624, DOI 10.1109/TPAMI.2019.2930258; Luo X, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392377; Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594; Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671; Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; Ranjan A, 2019, PROC CVPR IEEE, P12232, DOI 10.1109/CVPR.2019.01252; Roy A, 2016, PROC CVPR IEEE, P5506, DOI 10.1109/CVPR.2016.594; Saxena Ashutosh, 2006, ADV NEURAL INFORM PR, P1, DOI [DOI 10.1109/TPAMI.2015.2505283A, 10.1109/TPAMI.2015.2505283a]; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Shotton J, 2013, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2013.377; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Teed Z., 2018, PROC INT C LEARN REP, P1; Trucco E., 1998, INTRO TECHNIQUES 3D; Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216; Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897; Watson J, 2019, IEEE I CONF COMP VIS, P2162, DOI 10.1109/ICCV.2019.00225; Yin W, 2019, IEEE I CONF COMP VIS, P5683, DOI 10.1109/ICCV.2019.00578; Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212; Zehao Yu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P206, DOI 10.1007/978-3-030-58586-0_13; Zhan HY, 2019, IEEE INT CONF ROBOT, P4811, DOI 10.1109/ICRA.2019.8793984; Zhan HY, 2018, PROC CVPR IEEE, P340, DOI 10.1109/CVPR.2018.00043; Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561; Zhao W., 2020, P IEEE CVF C COMP VI, P9151, DOI DOI 10.1109/CVPR42600.2020.00917; Zhou JS, 2019, IEEE I CONF COMP VIS, P8617, DOI 10.1109/ICCV.2019.00871; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700; Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI 10.1007/978-3-030-01219-9_	65	0	0	5	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9802	9813		10.1109/TPAMI.2021.3136220	http://dx.doi.org/10.1109/TPAMI.2021.3136220			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34919516	Green Submitted			2022-12-18	WOS:000880661400089
J	Bonnaire, T; Decelle, A; Aghanim, N				Bonnaire, Tony; Decelle, Aurelien; Aghanim, Nabila			Regularization of Mixture Models for Robust Principal Graph Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Gaussian mixture models; expectation-maximization; graph regularization; principal graph; manifold learning	DIMENSIONALITY REDUCTION; LAPLACIAN EIGENMAPS	A regularized version of Mixture Models is proposed to learn a principal graph from a distribution of D-dimensional datapoints. In the particular case of manifold learning for ridge detection, we assume that the underlying structure can be modeled as a graph acting like a topological prior for the Gaussian clusters turning the problem into a maximum a posteriori estimation. Parameters of the model are iteratively estimated through an Expectation-Maximization procedure making the learning of the structure computationally efficient with guaranteed convergence for any graph prior in a polynomial time. We also embed in the formalism a natural way to make the algorithm robust to outliers of the pattern and heteroscedasticity of the manifold sampling coherently with the graph structure. The method uses a graph prior given by the minimum spanning tree that we extend using random sub-samplings of the dataset to take into account cycles that can be observed in the spatial distribution.	[Bonnaire, Tony; Decelle, Aurelien] CNRS, Lab Rech Informat, F-91190 Gif Sur Yvette, France; [Bonnaire, Tony; Decelle, Aurelien; Aghanim, Nabila] Univ Paris Saclay, F-91190 Gif Sur Yvette, France; [Bonnaire, Tony; Aghanim, Nabila] CNRS, Inst Astrophys Spatiale, F-91405 Bures Sur Yvette, France; [Decelle, Aurelien] Univ Complutense, Dept Fis Teor 1, Madrid 28040, Spain	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay; UDICE-French Research Universities; Universite Paris Saclay; Centre National de la Recherche Scientifique (CNRS); Complutense University of Madrid	Bonnaire, T (corresponding author), CNRS, Lab Rech Informat, F-91190 Gif Sur Yvette, France.; Bonnaire, T (corresponding author), Univ Paris Saclay, F-91190 Gif Sur Yvette, France.	tony.bonnaire@ias.u-psud.fr; aurelien.decelle@lri.fr; nabila.aghanim@ias.u-psud.fr		Bonnaire, Tony/0000-0003-2149-8795; Decelle, Aurelien/0000-0002-3017-0858	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program [ERC-2015-AdG 695561]; Comunidad de Madrid; Complutense University of Madrid (Spain) through the Atraccion de Talento Program [2019-T1/TIC-13298]	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program(European Research Council (ERC)); Comunidad de Madrid(Comunidad de Madrid); Complutense University of Madrid (Spain) through the Atraccion de Talento Program	This work was supported in part by the funding for the ByoPiC Project from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program under Grant ERC-2015-AdG 695561. The work of A.Decelle was supported by the Comunidad de Madrid and the Complutense University of Madrid (Spain) through the Atraccion de Talento Program under Grant 2019-T1/TIC-13298.	Ahmed M, 2015, GEOINFORMATICA, V19, P601, DOI 10.1007/s10707-014-0222-6; Albergante L, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22030296; BARROW JD, 1985, MON NOT R ASTRON SOC, V216, P17, DOI 10.1093/mnras/216.1.17; Belkin M, 2002, ADV NEUR IN, V14, P585; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Bonnaire T, 2021, PHYS REV E, V103, DOI 10.1103/PhysRevE.103.012105; Bonnaire T, 2020, ASTRON ASTROPHYS, V637, DOI 10.1051/0004-6361/201936859; Chazal F, 2018, J MACH LEARN RES, V18; Chazal F, 2011, FOUND COMPUT MATH, V11, P733, DOI 10.1007/s10208-011-9098-0; Chen Y. -c., 2014, ARXIV; CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Edelsbrunner H, 2002, DISCRETE COMPUT GEOM, V28, P511, DOI 10.1007/s00454-002-2885-2; FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330; Genovese CR, 2014, ANN STAT, V42, P1511, DOI 10.1214/14-AOS1218; Gerber S, 2013, J MACH LEARN RES, V14, P1285; Gorban A, 2005, COMPUTING, V75, P359, DOI 10.1007/s00607-005-0122-6; Gorban A. N., 2009, P HDB RES MACH LEARN, P28; Gorban A. N., 2016, ARXIV; HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936; Heidenreich NB, 2013, ASTA-ADV STAT ANAL, V97, P403, DOI 10.1007/s10182-013-0216-y; Huang J, 2018, IEEE ACCESS, V6; Kegl B, 2000, IEEE T PATTERN ANAL, V22, P281, DOI 10.1109/34.841759; Kruskal J. B., 1956, P AM MATH SOC, V7, P48, DOI [DOI 10.1090/S0002-9939-1956-0078686-7, 10.2307/2033241]; Kurlin V, 2015, COMPUT GRAPH FORUM, V34, P253, DOI 10.1111/cgf.12713; Libeskind NI, 2018, MON NOT R ASTRON SOC, V473, P1195, DOI 10.1093/mnras/stx1976; Mao Q., 2015, P 2015 SIAM INT C DA, P792, DOI [10.1137/1.9781611974010.89, DOI 10.1137/1.9781611974010.89]; Mao Q, 2017, IEEE T PATTERN ANAL, V39, P2227, DOI 10.1109/TPAMI.2016.2635657; McLachlan, 1997, EM ALGORITHM EXTENSI; Moccia S, 2018, COMPUT METH PROG BIO, V158, P71, DOI 10.1016/j.cmpb.2018.02.001; Nelson Dylan, 2019, Computational Astrophysics and Cosmology, V6, DOI 10.1186/s40668-019-0028-x; Ozertem U, 2011, J MACH LEARN RES, V12, P1249; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Smola A. J., 2003, KERNELS REGULARIZATI; Stanford DC, 2000, IEEE T PATTERN ANAL, V22, P601, DOI 10.1109/34.862198; Tibshirani R., 1992, Statistics and Computing, V2, P183, DOI 10.1007/BF01889678; Ueda N, 1998, NEURAL NETWORKS, V11, P271, DOI 10.1016/S0893-6080(97)00133-0; Yuille AL, 1990, NEURAL COMPUT, V2, P1, DOI 10.1162/neco.1990.2.1.1	39	0	0	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9119	9130		10.1109/TPAMI.2021.3124973	http://dx.doi.org/10.1109/TPAMI.2021.3124973			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34757901	Green Submitted			2022-12-18	WOS:000880661400041
J	Bryner, D; Srivastava, A				Bryner, Darshan; Srivastava, Anuj			Shape Analysis of Functional Data With Elastic Partial Matching	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Functional data analysis; elastic partial matching; phase variability; COVID-19 rates; elastic Riemannian metric	REGISTRATION; RECOGNITION; CURVES	Elastic Riemannian metrics have been used successfully for statistical treatments of functional and curve shape data. However, this usage suffers from a significant restriction: the function boundaries are assumed to be fixed and matched. In practice, functional data often comes with unmatched boundaries. It happens, for example, in dynamical systems with variable evolution rates, such as COVID-19 infection rate curves associated with different geographical regions. Here, we develop a Riemannian framework that allows for partial matching, comparing, and clustering of functions with phase variability and uncertain boundaries. We extend past work by (1) Defining a new diffeomorphism group G over the positive reals that is the semidirect product of a time-warping group and a time-scaling group; (2) Introducing a metric that is invariant to the action of G; (3) Imposing a Riemannian Lie group structure on G to allow for an efficient gradient-based optimization for elastic partial matching; and (4) Presenting a modification that, while losing the metric property, allows one to control the amount of boundary disparity in the registration. We illustrate this framework by registering and clustering shapes of COVID-19 rate curves, identifying basic patterns, minimizing mismatch errors, and reducing variability within clusters compared to previous methods.	[Bryner, Darshan] Naval Surface Warfare Ctr, Panama City Div, Panama City, FL 32407 USA; [Srivastava, Anuj] Florida State Univ, Dept Stat, Tallahassee, FL 32306 USA	State University System of Florida; Florida State University	Bryner, D (corresponding author), Naval Surface Warfare Ctr, Panama City Div, Panama City, FL 32407 USA.	darshan.bryner@navy.mil; anuj@stat.fsu.edu		Srivastava, Anuj/0000-0001-7406-0338	NSF CDSE [DMS 1621787, DMS 1953087]; NIH [R01 GM135927]; US Office of Naval Research (ONR) [N00014-21-WX-02038]; Naval Innovative Science and Engineering (NISE) program; Naval Surface Warfare Center Panama City Division (NSWC PCD)	NSF CDSE; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); US Office of Naval Research (ONR)(Office of Naval Research); Naval Innovative Science and Engineering (NISE) program; Naval Surface Warfare Center Panama City Division (NSWC PCD)	This work of Anuj Srivastava was supported in part by the Grants NSF CDS&E DMS 1621787, NIH R01 GM135927, and NSF CDS&E DMS 1953087, and the work of Darshan Bryner was supported in part by the US Office of Naval Research (ONR) under Grant N00014-21-WX-02038 (Bob Headrick) and the Naval Innovative Science and Engineering (NISE) program with the Naval Surface Warfare Center Panama City Division (NSWC PCD).	ALT H, 1995, INT J COMPUT GEOM AP, V5, P75, DOI 10.1142/S0218195995000064; Altendji B, 2018, J NONPARAMETR STAT, V30, P472, DOI 10.1080/10485252.2018.1438609; Anirudh R, 2017, IEEE T PATTERN ANAL, V39, P922, DOI 10.1109/TPAMI.2016.2564409; Anuj Srivastava, 2011, Arxiv, DOI arXiv:1103.3817; Ben Amor B, 2016, IEEE T PATTERN ANAL, V38, P1, DOI 10.1109/TPAMI.2015.2439257; Buchin K, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P645; Cui M, 2009, PATTERN RECOGN LETT, V30, P1, DOI 10.1016/j.patrec.2008.08.013; Delaigle A, 2013, J AM STAT ASSOC, V108, P1269, DOI 10.1080/01621459.2013.824893; Elmi A F, 2009, THESIS U PENN; Ferraty F., 2006, SPR S STAT; Funkhouser T., 2006, PROC EUROGRAPHICS S, P131; Hsing T., 2015, THEORETICAL FDN FUNC; Huang W, 2016, J MATH IMAGING VIS, V54, P320, DOI 10.1007/s10851-015-0606-8; Klein J.P., 2003, SURVIVAL ANAL TECHNI; Kneip A, 2008, J AM STAT ASSOC, V103, P1155, DOI 10.1198/016214508000000517; Kokoszka P., 2017, INTRO FUNCTIONAL DAT, DOI 10.1201/9781315117416; Kong D, 2018, BIOMETRICS, V74, P109, DOI 10.1111/biom.12748; Lee S., 2017, ARXIV; Maheshwari A, 2011, LECT NOTES COMPUT SC, V6942, P518, DOI 10.1007/978-3-642-23719-5_44; Marron JS, 2015, STAT SCI, V30, P468, DOI 10.1214/15-STS524; Marron JS, 2014, ELECTRON J STAT, V8, P1697, DOI 10.1214/14-EJS901; McBride J. C., 2003, PROC C COMPUT VIS PA; Ramsay J.O., 2005, FUNCTIONAL DATA ANAL; RAMSAY JO, 1995, ANN HUM BIOL, V22, P413, DOI 10.1080/03014469500004092; Ramsay JO, 1998, J ROY STAT SOC B, V60, P351, DOI 10.1111/1467-9868.00129; Robinson D. T., 2012, FUNCTIONAL DATA ANAL; Sebastian TB, 2003, IEEE T PATTERN ANAL, V25, P116, DOI 10.1109/TPAMI.2003.1159951; Srivastava A, 2016, SPRINGER SER STAT, P1, DOI 10.1007/978-1-4939-4020-2; Srivastava A, 2011, IEEE T PATTERN ANAL, V33, P1415, DOI 10.1109/TPAMI.2010.184; Takagishi M., 2019, BEHAVIORMETRIKA, V46, P177; Tucker JD, 2013, COMPUT STAT DATA AN, V61, P50, DOI 10.1016/j.csda.2012.12.001; Veeraraghavan A, 2009, IEEE T IMAGE PROCESS, V18, P1326, DOI 10.1109/TIP.2009.2017143; Yang CZ, 2018, NEUROCOMPUTING, V275, P1160, DOI 10.1016/j.neucom.2017.09.067; Zhang ZW, 2015, J STAT PLAN INFER, V166, P171, DOI 10.1016/j.jspi.2015.04.007	34	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9589	9602		10.1109/TPAMI.2021.3130535	http://dx.doi.org/10.1109/TPAMI.2021.3130535			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34818189	Green Submitted			2022-12-18	WOS:000880661400073
J	Cermelli, F; Mancini, M; Bulo, SR; Ricci, E; Caputo, B				Cermelli, Fabio; Mancini, Massimiliano; Bulo, Samuel Rota; Ricci, Elisa; Caputo, Barbara			Modeling the Background for Incremental and Weakly-Supervised Semantic Segmentation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Semantics; Image segmentation; Annotations; Task analysis; Training; Automobiles; Standards	NETWORKS	Deep neural networks have enabled major progresses in semantic segmentation. However, even the most advanced neural architectures suffer from important limitations. First, they are vulnerable to catastrophic forgetting, i.e., they perform poorly when they are required to incrementally update their model as new classes are available. Second, they rely on large amount of pixel-level annotations to produce accurate segmentation maps. To tackle these issues, we introduce a novel incremental class learning approach for semantic segmentation taking into account a peculiar aspect of this task: since each training step provides annotation only for a subset of all possible classes, pixels of the background class exhibit a semantic shift. Therefore, we revisit the traditional distillation paradigm by designing novel loss terms which explicitly account for the background shift. Additionally, we introduce a novel strategy to initialize classifier's parameters at each step in order to prevent biased predictions toward the background class. Finally, we demonstrate that our approach can be extended to point- and scribble-based weakly supervised segmentation, modeling the partial annotations to create priors for unlabeled pixels. We demonstrate the effectiveness of our approach with an extensive evaluation on the Pascal-VOC, ADE20K, and Cityscapes datasets, significantly outperforming state-of-the-art methods.	[Cermelli, Fabio; Caputo, Barbara] Politecn Torino, DAUIN Dept Control & Comp Engn, I-10138 Turin, Italy; [Cermelli, Fabio; Caputo, Barbara] Italian Inst Technol, I-10129 Turin, TO, Italy; [Mancini, Massimiliano] Univ Tubingen, Cluster Excellence Machine Learning, D-72074 Tubingen, Germany; [Bulo, Samuel Rota] Facebook, Menlo Pk, CA 94025 USA; [Ricci, Elisa] Fdn Bruno Kessler, I-38123 Povo, TN, Italy; [Ricci, Elisa] Univ Trento, Dept Informat Engn & Comp Sci, I-38122 Trento, TN, Italy	Polytechnic University of Turin; Istituto Italiano di Tecnologia - IIT; Eberhard Karls University of Tubingen; Facebook Inc; Fondazione Bruno Kessler; University of Trento	Cermelli, F (corresponding author), Politecn Torino, DAUIN Dept Control & Comp Engn, I-10138 Turin, Italy.	fabio.cermelli@polito.it; massimiliano.mancini@uni-tuebingen.de; rotabulo@fb.com; eliricci@fbk.eu; barbara.caputo@polito.it		Cermelli, Fabio/0000-0001-7077-697X	ERC [637076, 853489]; DFG [2064/1-390727645]	ERC(European Research Council (ERC)European Commission); DFG(German Research Foundation (DFG))	This work was supported in part by ERC Grant 637076 -RoboExNovo obtained by Barbara Caputo, in part by the ERC under Grant 853489DEXIM, and in part by the DFG under Grant 2064/1-390727645.	ADAMS R, 1994, IEEE T PATTERN ANAL, V16, P641, DOI 10.1109/34.295913; Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231; Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523; Aljundi R, 2018, LECT NOTES COMPUT SC, V11207, P144, DOI 10.1007/978-3-030-01219-9_9; Rusu AA, 2016, Arxiv, DOI arXiv:1606.04671; Araslanov N, 2020, PROC CVPR IEEE, P4252, DOI 10.1109/CVPR42600.2020.00431; Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34; Castro FM, 2018, LECT NOTES COMPUT SC, V11216, P241, DOI 10.1007/978-3-030-01258-8_15; Cermelli Fabio, 2020, P IEEE CVF C COMP VI, P9233; Chaudhry A, 2018, LECT NOTES COMPUT SC, V11215, P556, DOI 10.1007/978-3-030-01252-6_33; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191; De Lange M., 2019, ARXIV190908383, V2; Dhar P, 2019, PROC CVPR IEEE, P5133, DOI 10.1109/CVPR.2019.00528; Everingham M., 2012, PASCAL VISUAL OBJECT; Fini Enrico, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P720, DOI 10.1007/978-3-030-58604-1_43; Florian Schroff, 2017, Arxiv, DOI arXiv:1706.05587; Geoffrey Hinton, 2015, Arxiv, DOI arXiv:1503.02531; Ghiasi G, 2016, LECT NOTES COMPUT SC, V9907, P519, DOI 10.1007/978-3-319-46487-9_32; Guolei Sun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P347, DOI 10.1007/978-3-030-58536-5_21; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hou SH, 2019, PROC CVPR IEEE, P831, DOI 10.1109/CVPR.2019.00092; Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733; Khoreva A, 2017, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2017.181; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Lee J, 2019, PROC CVPR IEEE, P5262, DOI 10.1109/CVPR.2019.00541; Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081; Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mallya A, 2018, LECT NOTES COMPUT SC, V11208, P72, DOI 10.1007/978-3-030-01225-0_5; Mallya A, 2018, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2018.00810; Michieli U, 2019, IEEE INT CONF COMP V, P3205, DOI 10.1109/ICCVW.2019.00400; Oh SJ, 2017, PROC CVPR IEEE, P5038, DOI 10.1109/CVPR.2017.535; Ostapenko Oleksiy, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11313, DOI 10.1109/CVPR.2019.01158; Ozdemir F, 2019, INT J COMPUT ASS RAD, V14, P1187, DOI 10.1007/s11548-019-01984-4; Ozdemir F, 2018, LECT NOTES COMPUT SC, V11073, P361, DOI 10.1007/978-3-030-00937-3_42; Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203; Qian R, 2019, AAAI CONF ARTIF INTE, P8843; Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shin H, 2017, ADV NEUR IN, V30; Shmelkov K, 2017, IEEE I CONF COMP VIS, P3420, DOI 10.1109/ICCV.2017.368; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Tang M, 2018, LECT NOTES COMPUT SC, V11220, P524, DOI 10.1007/978-3-030-01270-0_31; Tang M, 2018, PROC CVPR IEEE, P1818, DOI 10.1109/CVPR.2018.00195; Tasar O, 2019, IEEE J-STARS, V12, P3524, DOI 10.1109/JSTARS.2019.2925416; Wang B, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3663; Wu C, 2018, PROC INT C NEURAL IN, P5966; Wu Y, 2019, PROC CVPR IEEE, P374, DOI 10.1109/CVPR.2019.00046; Xian YQ, 2019, PROC CVPR IEEE, P8248, DOI 10.1109/CVPR.2019.00845; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yu-Ting Chang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8988, DOI 10.1109/CVPR42600.2020.00901; Zenke F, 2017, PR MACH LEARN RES, V70; Zhang ZL, 2018, LECT NOTES COMPUT SC, V11214, P273, DOI 10.1007/978-3-030-01249-6_17; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319	69	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					10099	10113		10.1109/TPAMI.2021.3133954	http://dx.doi.org/10.1109/TPAMI.2021.3133954			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34882548	Green Submitted			2022-12-18	WOS:000880661400108
J	Cha, E; Lee, C; Jang, M; Ye, JC				Cha, Eunju; Lee, Chanseok; Jang, Mooseok; Ye, Jong Chul			DeepPhaseCut: Deep Relaxation in Phase for Unsupervised Fourier Phase Retrieval	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Imaging; Phase measurement; Image reconstruction; Physics; Neural networks; Matrix converters; Extraterrestrial measurements; Fourier phase retrieval; PhaseCut; Fienup algorithm; unsupervised learning; CycleGAN	IMAGE-RECONSTRUCTION; ALGORITHM; MAGNITUDE; RECOVERY; MODULUS; OBJECT	Fourier phase retrieval is a classical problem of restoring a signal only from the measured magnitude of its Fourier transform. Although Fienup-type algorithms, which use prior knowledge in both spatial and Fourier domains, have been widely used in practice, they can often stall in local minima. Convex relaxation methods such as PhaseLift and PhaseCut may offer performance guarantees, but these algorithms are usually computationally expensive for practical use. To address this problem, here we propose a novel unsupervised feed-forward neural network for Fourier phase retrieval which generates high quality reconstruction immediately. Unlike the existing deep learning approaches that use a neural network as a regularization term or an end-to-end blackbox model for supervised training, our algorithm is a feed-forward neural network implementation of physics-driven constraints in an unsupervised learning framework. Specifically, our network is composed of two generators: one for the phase estimation using PhaseCut loss, followed by another generator for image reconstruction, all of which are trained simultaneously without matched data. The link to the classical Fienup-type algorithms and the recent symmetry-breaking learning approach is also revealed. Extensive experiments demonstrate that the proposed method outperforms all existing approaches in Fourier phase retrieval problems.	[Cha, Eunju] Samsung Elect, Samsung Adv Inst Technol, Gyeonggi Do 16678, South Korea; [Lee, Chanseok; Jang, Mooseok; Ye, Jong Chul] Korea Adv Inst Sci & Technol KAIST, Dept Bio & Brain Engn, Daejeon 34141, South Korea	Samsung; Samsung Electronics; Korea Advanced Institute of Science & Technology (KAIST)	Ye, JC (corresponding author), Korea Adv Inst Sci & Technol KAIST, Dept Bio & Brain Engn, Daejeon 34141, South Korea.	eunju.cha@samsung.com; cslee@kaist.ac.kr; mooseok@kaist.ac.kr; jong.ye@kaist.ac.kr		Lee, Chanseok/0000-0002-3221-6426; Ye, Jong Chul/0000-0001-9763-9609; Cha, Eunju/0000-0002-1371-5318; Jang, Mooseok/0000-0003-1977-9539	National Research Foundation of Korea (NRF) [NRF-2020R1A2B5B03001980]; National Research Foundation of Korea (NRF) - Korea Government (MSIT) [NRF-2021R1A5A1032937]	National Research Foundation of Korea (NRF)(National Research Foundation of Korea); National Research Foundation of Korea (NRF) - Korea Government (MSIT)(National Research Foundation of KoreaMinistry of Science, ICT & Future Planning, Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea)	This work was supported in part by the National Research Foundation of Korea (NRF) under Grant NRF-2020R1A2B5B03001980, and also in part by the National Research Foundation of Korea (NRF) Grant funded by the Korea Government (MSIT) under Grant NRF-2021R1A5A1032937.	Aaron Defazio, 2019, Arxiv, DOI arXiv:1811.08839; Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150; AKIYAMA K, 2019, ASTROPHYS J LETT, V875, DOI DOI 10.3847/2041-8213/AB0E85; Andrea Vedaldi, 2017, Arxiv, DOI arXiv:1607.08022; BATES RHT, 1982, OPTIK, V61, P247; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); BRUCK YM, 1979, OPT COMMUN, V30, P304, DOI 10.1016/0030-4018(79)90358-4; Bunk O, 2007, ACTA CRYSTALLOGR A, V63, P306, DOI 10.1107/S0108767307021903; Chen CC, 2007, PHYS REV B, V76, DOI 10.1103/PhysRevB.76.064113; Ding CHQ, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P107, DOI 10.1109/ICDM.2001.989507; Eldar YC, 2015, IEEE SIGNAL PROC LET, V22, P638, DOI 10.1109/LSP.2014.2364225; Fienup C., 1987, IMAGE RECOVERY THEOR, Vvol 231, pp 275; FIENUP JR, 1978, OPT LETT, V3, P27, DOI 10.1364/OL.3.000027; FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758; FIENUP JR, 1987, J OPT SOC AM A, V4, P118, DOI 10.1364/JOSAA.4.000118; Fogel F, 2016, MATH PROGRAM COMPUT, V8, P311, DOI 10.1007/s12532-016-0103-0; GERCHBERG RW, 1972, OPTIK, V35, P237; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hand P, 2018, ADV NEUR IN, V31; HAYES MH, 1982, IEEE T ACOUST SPEECH, V30, P140, DOI 10.1109/TASSP.1982.1163863; Hyder Rakib, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P425, DOI 10.1007/978-3-030-58577-8_26; Isil C, 2019, APPL OPTICS, V58, P5422, DOI 10.1364/AO.58.005422; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jaganathan K, 2013, IEEE INT SYMP INFO, P1022, DOI 10.1109/ISIT.2013.6620381; Katz O, 2014, NAT PHOTONICS, V8, P784, DOI [10.1038/nphoton.2014.189, 10.1038/NPHOTON.2014.189]; Kingma D.P, P 3 INT C LEARNING R; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Luke DR, 2005, INVERSE PROBL, V21, P37, DOI 10.1088/0266-5611/21/1/004; Malm E, 2021, OPT LETT, V46, P13, DOI 10.1364/OL.408452; Manekar R., 2020, PROC INT C MACH LEAR; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Metzler CA, 2020, OPTICA, V7, P63, DOI 10.1364/OPTICA.374026; Metzler CA, 2018, PR MACH LEARN RES, V80; MILLANE RP, 1990, J OPT SOC AM A, V7, P394, DOI 10.1364/JOSAA.7.000394; Pauwels EJR, 2018, IEEE T SIGNAL PROCES, V66, P982, DOI 10.1109/TSP.2017.2780044; Rodriguez JA, 2013, J APPL CRYSTALLOGR, V46, P312, DOI 10.1107/S0021889813002471; Shechtman Y, 2015, IEEE SIGNAL PROC MAG, V32, P87, DOI 10.1109/MSP.2014.2352673; Sidorenko P, 2016, OPTICA, V3, P1320, DOI 10.1364/OPTICA.3.001320; Sim B, 2020, SIAM J IMAGING SCI, V13, P2281, DOI 10.1137/20M1317992; Sinha A, 2017, OPTICA, V4, P1117, DOI 10.1364/OPTICA.4.001117; Tao QY, 2019, LECT NOTES COMPUT SC, V11769, P185, DOI 10.1007/978-3-030-32226-7_21; Uelwer T, 2021, INT C PATT RECOG, P731, DOI 10.1109/ICPR48806.2021.9412523; Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9; Wang Yaotian, 2020, INT C MACHINE LEARNI, P10007; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; WATSON JD, 1953, NATURE, V171, P737, DOI 10.1038/171737a0; Wu ZH, 2019, IEEE INT CONF COMP V, P3887, DOI 10.1109/ICCVW.2019.00482; Yoon S, 2020, NAT REV PHYS, V2, P141, DOI 10.1038/s42254-019-0143-2; Zheng GA, 2013, NAT PHOTONICS, V7, P739, DOI [10.1038/NPHOTON.2013.187, 10.1038/nphoton.2013.187]; Zhu B, 2018, NATURE, V555, P487, DOI 10.1038/nature25988; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	55	0	0	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9931	9943		10.1109/TPAMI.2021.3138897	http://dx.doi.org/10.1109/TPAMI.2021.3138897			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34962865	Green Submitted			2022-12-18	WOS:000880661400098
J	Chen, X; Wujek, B				Chen, Xu; Wujek, Brett			A Unified Framework for Automatic Distributed Active Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Optimization; Semisupervised learning; Machine learning; Distributed databases; Big Data; Search problems; Heuristic algorithms; Automation; optimization; query selection; regularization; active learning		We propose a novel unified frameork for automated distributed active learning (AutoDAL) to address multiple challenging problems in active learning such as limited labeled data, imbalanced datasets, automatic hyperparameter selection as well as scalability to big data. First, automated graph-based semi-supervised learning is conducted by aggregating the proposed cost functions from different compute nodes and jointly optimizing hyperparameters in both the classification and query selection stages. For dense datasets, clustering-based uncertainty sampling with maximum entropy (CME) loss is applied in the optimization. For sparse and imbalanced datasets, shrinkage optimized KL-divergence regularization and local selection based active learning (SOAR) loss are further naturally adapted in AutoDAL. The optimization is efficiently resolved by iteratively executing a genetic algorithm (GA) refined with a local generating set search (GSS) and solving an integer linear programming (ILP) problem. Moreover, we propose an efficient distributed active learning algorithm which is scalable for big data. The proposed AutoDAL algorithm is applied to multiple benchmark datasets and two real-world datasets including an electrocardiogram (ECG) dataset and a credit fraud detection dataset for classification. We demonstrate that the proposed AutoDAL algorithm is capable of achieving significantly better performance compared to several state-of-the-art AutoML approaches and active learning algorithms.	[Chen, Xu; Wujek, Brett] SAS Inst Inc, Artificial Intelligence & Machine Learning, Cary, NC 27513 USA	SAS Institute Inc	Chen, X (corresponding author), SAS Inst Inc, Artificial Intelligence & Machine Learning, Cary, NC 27513 USA.	steven.xu.chen@gmail.com; Brett.Wujek@sas.com		CHEN, XU/0000-0003-0003-0210				Beatty G, 2018, IEEE INT C SEMANT CO, P306, DOI 10.1109/ICSC.2018.00059; Chakraborty S, 2018, IEEE WINT CONF APPL, P1833, DOI 10.1109/WACV.2018.00203; Chang CC, 2019, IEEE T CYBERNETICS, V49, P4460, DOI 10.1109/TCYB.2018.2869861; Chang XY, 2017, J MACH LEARN RES, V18; Chen X, 2020, AAAI CONF ARTIF INTE, V34, P3537; Feurer M, 2015, ADV NEUR IN, V28; Fu WJ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1396, DOI 10.1145/3219819.3219954; Griffin JD, 2010, APPL MATH RES EXPRES, P36, DOI 10.1093/amrx/abq003; Guyon I., 2016, PROC AUTOML WORKSHOP, P21; He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239; James W., 1961, P 4 BERKELEY S MATH, V1, P361, DOI DOI 10.1007/978-1-4612-0919-5; Jia JT, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P761, DOI 10.1145/3292500.3330872; Koch P, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P443, DOI 10.1145/3219819.3219837; Li YF, 2019, AAAI CONF ARTIF INTE, P4237; Li YF, 2016, AAAI CONF ARTIF INTE, P1816; Lin C. H., 2018, PROC AAAI C ARTIF IN, P98; Liu W, 2012, P IEEE, V100, P2624, DOI 10.1109/JPROC.2012.2197809; Martello S, 2000, EUR J OPER RES, V123, P325, DOI 10.1016/S0377-2217(99)00260-X; Maystre L, 2017, PR MACH LEARN RES, V70; Ong CS, 2005, J MACH LEARN RES, V6, P1043; Pumsirirat A., 2018, INT J APPL COMPUT SC, V9; Roy Nicholas, 2001, P 18 INT C MACH LEAR, P441; Sener O., 2018, PROC INT C LEARN REP; Settles B., 2009, ACTIVE LEARNING LIT; Settles B, 2008, P 2008 C EMP METH NA, ppp1070, DOI DOI 10.3115/1613715.1613855; Shen PC, 2016, IEEE ACCESS, V4, P2572, DOI 10.1109/ACCESS.2016.2572198; Thornton C, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P847, DOI 10.1145/2487575.2487629; Tong S., 2000, PROC NEURAL INF PROC, P626; Wang H., 2014, PROC NEURAL INF PROC; Wang Z, 2015, ACM T KNOWL DISCOV D, V9, DOI 10.1145/2700408; Yang Y, 2015, INT J COMPUT VISION, V113, P113, DOI 10.1007/s11263-014-0781-x; Yu HL, 2019, IEEE T NEUR NET LEAR, V30, P1088, DOI 10.1109/TNNLS.2018.2855446; Zhou DY, 2004, ADV NEUR IN, V16, P321	34	0	0	2	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9774	9786		10.1109/TPAMI.2021.3129793	http://dx.doi.org/10.1109/TPAMI.2021.3129793			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34813465				2022-12-18	WOS:000880661400087
J	Choi, M; Choi, J; Baik, S; Kim, TH; Lee, KM				Choi, Myungsub; Choi, Janghoon; Baik, Sungyong; Kim, Tae Hyun; Lee, Kyoung Mu			Test-Time Adaptation for Video Frame Interpolation via Meta-Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Interpolation; Adaptation models; Estimation; Task analysis; Visualization; Superresolution; Performance gain; Video frame interpolation; test-time adaptation; meta-learning; slow motion; self-supervision; image synthesis; MAML		Video frame interpolation is a challenging problem that involves various scenarios depending on the variety of foreground and background motions, frame rate, and occlusion. Therefore, generalizing across different scenes is difficult for a single network with fixed parameters. Ideally, one could have a different network for each scenario, but this will be computationally infeasible for practical applications. In this work, we propose MetaVFI, an adaptive video frame interpolation algorithm that uses additional information readily available at test time but has not been exploited in previous works. We initially show the benefits of test-time adaptation through simple fine-tuning of a network and then greatly improve its efficiency by incorporating meta-learning. Thus, we obtain significant performance gains with only a single gradient update without introducing any additional parameters. Moreover, the proposed MetaVFI algorithm is model-agnostic which can be easily combined with any video frame interpolation network. We show that our adaptive framework greatly improves the performance of baseline video frame interpolation networks on multiple benchmark datasets.	[Choi, Myungsub] Google Res, Seoul 06236, South Korea; [Choi, Janghoon] Kookmin Univ, Seoul 02707, South Korea; [Baik, Sungyong; Lee, Kyoung Mu] Seoul Natl Univ, Automat & Syst Res Inst ASRI, Dept Elect & Comp Engn, Seoul 08826, South Korea; [Kim, Tae Hyun] Hanyang Univ, Dept Comp Sci, Seoul 04763, South Korea	Kookmin University; Seoul National University (SNU); Hanyang University	Lee, KM (corresponding author), Seoul Natl Univ, Automat & Syst Res Inst ASRI, Dept Elect & Comp Engn, Seoul 08826, South Korea.	cms6539@gmail.com; jhchoi09@kookmin.ac.kr; dsybaik@snu.ac.kr; taehyunkim@hanyang.ac.kr; kyoungmu@snu.ac.kr		Kim, Tae Hyun/0000-0002-7995-3984; Choi, Myungsub/0000-0003-4731-3074	IITP Grant - Korea Government [2021-0-01343]; Artificial Intelligence Graduate School Program (Seoul National University); Hyundai Motor Group through HMG-SNU AI Consortium fund [526420190101]	IITP Grant - Korea Government(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of Korea); Artificial Intelligence Graduate School Program (Seoul National University); Hyundai Motor Group through HMG-SNU AI Consortium fund	This work was supported in part by IITP Grant funded by the Korea Government under Grant 2021-0-01343, in part by the Artificial Intelligence Graduate School Program (Seoul National University)], and in part by the Hyundai Motor Group through HMG-SNU AI Consortium fund under Grant 526420190101.	Alex Nichol, 2018, Arxiv, DOI arXiv:1803.02999; Amir Roshan Zamir, 2012, Arxiv, DOI arXiv:1212.0402; Antoniou A., 2019, PROC INT C LEARN REP; Baik S, 2020, PROC CVPR IEEE, P2376, DOI 10.1109/CVPR42600.2020.00245; Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2; Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382; Bengio Samy, 1992, C OPT ART BIOL NEUR, P6; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Cheng JC, 2017, IEEE I CONF COMP VIS, P686, DOI 10.1109/ICCV.2017.81; Choi J, 2021, IEEE WINT CONF APPL, P596, DOI 10.1109/WACV48630.2021.00064; Choi M, 2020, AAAI CONF ARTIF INTE, V34, P10663; Choi Myungsub, 2020, P IEEE C COMP VIS PA, P9444, DOI DOI 10.1109/CVPR42600.2020.00946; Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733; Donghyeon Cho, 2020, Arxiv, DOI arXiv:2001.02905; Fei Chen, 2017, Arxiv, DOI arXiv:1707.09835; Finn C, 2017, PR MACH LEARN RES, V70; Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271; Hochreiter Sepp, 2001, INT C ART NEUR NETW, P87, DOI [10.1007/3-540-44668-0, DOI 10.1007/3-540-44668-0]; Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156; Huang JJ, 2017, IEEE COMPUT SOC CONF, P1067, DOI 10.1109/CVPRW.2017.144; Huang Z., 2020, ARXIV; Janghoon Choi, 2019, Arxiv, DOI arXiv:1712.09153; Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938; Junheum Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P109, DOI 10.1007/978-3-030-58568-6_7; Kalluri T., 2020, ARXIV; Kingma D.P, P 3 INT C LEARNING R; Koch G., 2015, ICML DEEP LEARN WORK; Li HP, 2020, INT CONF ACOUST SPEE, P2613, DOI 10.1109/ICASSP40776.2020.9053987; Liu YL, 2019, AAAI CONF ARTIF INTE, P8794; Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI 10.1109/ICCV.2017.478; Long GC, 2016, LECT NOTES COMPUT SC, V9910, P434, DOI 10.1007/978-3-319-46466-4_26; Meyer S, 2018, PROC CVPR IEEE, P498, DOI 10.1109/CVPR.2018.00059; Meyer S, 2015, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2015.7298747; Michaeli T, 2013, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2013.121; Ming-Hsuan Yang, 2019, Arxiv, DOI arXiv:1810.08768; Munkhdalai T, 2018, PR MACH LEARN RES, V80; Munkhdalai T, 2017, PR MACH LEARN RES, V70; Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465; Niklaus S, 2021, IEEE WINT CONF APPL, P1098, DOI 10.1109/WACV48630.2021.00114; Niklaus S, 2020, PROC CVPR IEEE, P5436, DOI 10.1109/CVPR42600.2020.00548; Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183; Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244; Oreshkin BN, 2018, ADV NEUR IN, V31; Paliwal A., 2018, PYTORCH IMPLEMENTATI; Rajeswaran A, 2019, ADV NEUR IN, V32; Reda FA, 2019, IEEE I CONF COMP VIS, P892, DOI 10.1109/ICCV.2019.00098; Santoro A, 2016, PR MACH LEARN RES, V48; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131; Schmidhuber J, 1987, THESIS; Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329; Simonyan K, 2014, ADV NEUR IN, V27; Snell J, 2017, ADV NEUR IN, V30; Soh JW, 2020, PROC CVPR IEEE, P3513, DOI 10.1109/CVPR42600.2020.00357; Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Thrun S., 2012, LEARNING LEARN; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2; Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52	61	0	0	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9615	9628		10.1109/TPAMI.2021.3129819	http://dx.doi.org/10.1109/TPAMI.2021.3129819			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34813468				2022-12-18	WOS:000880661400075
J	Dong, YP; Cheng, SY; Pang, TY; Su, H; Zhu, J				Dong, Yinpeng; Cheng, Shuyu; Pang, Tianyu; Su, Hang; Zhu, Jun			Query-Efficient Black-Box Adversarial Attacks Guided by a Transfer-Based Prior	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Estimation; Optimization; Analytical models; Numerical models; Deep learning; Approximation algorithms; Weight measurement; Adversarial examples; black-box attacks; zeroth-order optimization; query efficiency; transferability		Adversarial attacks have been extensively studied in recent years since they can identify the vulnerability of deep learning models before deployed. In this paper, we consider the black-box adversarial setting, where the adversary needs to craft adversarial examples without access to the gradients of a target model. Previous methods attempted to approximate the true gradient either by using the transfer gradient of a surrogate white-box model or based on the feedback of model queries. However, the existing methods inevitably suffer from low attack success rates or poor query efficiency since it is difficult to estimate the gradient in a high-dimensional input space with limited information. To address these problems and improve black-box attacks, we propose two prior-guided random gradient-free (PRGF) algorithms based on biased sampling and gradient averaging, respectively. Our methods can take the advantage of a transfer-based prior given by the gradient of a surrogate model and the query information simultaneously. Through theoretical analyses, the transfer-based prior is appropriately integrated with model queries by an optimal coefficient in each method. Extensive experiments demonstrate that, in comparison with the alternative state-of-the-arts, both of our methods require much fewer queries to attack black-box models with higher success rates.	[Dong, Yinpeng; Cheng, Shuyu; Pang, Tianyu; Su, Hang; Zhu, Jun] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, Tsinghua Bosch Joint Ctr Machine Learning, Inst Artificial Intelligence,Dept Comp Sci & Tech, Beijing 100084, Peoples R China; [Dong, Yinpeng; Cheng, Shuyu; Pang, Tianyu; Su, Hang; Zhu, Jun] Pazhou Lab, Guangzhou 510330, Peoples R China	Tsinghua University; Pazhou Lab	Zhu, J (corresponding author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, Tsinghua Bosch Joint Ctr Machine Learning, Inst Artificial Intelligence,Dept Comp Sci & Tech, Beijing 100084, Peoples R China.	dyp17@mails.tsinghua.edu.cn; chengsy18@mails.tsinghua.edu.cn; pty17@mails.tsinghua.edu.cn; suhangss@tsinghua.edu.cn; dcszj@tsinghua.edu.cn			National Key Research and Development Program of China [2020AAA0104304]; NSFC [61620106010, 62061136001, 61621136008, 62076147, U19B2034, U1811461, U19A2081]; Beijing NSF Project [JQ19016]; Beijing Academy of Artificial Intelligence (BAAI); Tsinghua-Huawei Joint Research Program; Tsinghua Institute for Guo Qiang; Tsinghua-OPPO Joint Research Center for Future Terminal Technology; Tsinghua-ChinaMobile Communications Group Co., Ltd. Joint Institute	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Beijing NSF Project; Beijing Academy of Artificial Intelligence (BAAI); Tsinghua-Huawei Joint Research Program; Tsinghua Institute for Guo Qiang; Tsinghua-OPPO Joint Research Center for Future Terminal Technology; Tsinghua-ChinaMobile Communications Group Co., Ltd. Joint Institute	This work was supported by the National Key Research and Development Program of China under Grant 2020AAA0104304, NSFC Projects under Grants 61620106010, 62061136001, 61621136008, 62076147, U19B2034, U1811461, U19A2081, Beijing NSF Project No. JQ19016, Beijing Academy of Artificial Intelligence (BAAI), Tsinghua-Huawei Joint Research Program, Tsinghua Institute for Guo Qiang, Tsinghua-OPPO Joint Research Center for Future Terminal Technology and Tsinghua-ChinaMobile Communications Group Co., Ltd. Joint Institute.	Athalye A, 2018, PR MACH LEARN RES, V80; Bhagoji AN, 2018, LECT NOTES COMPUT SC, V11216, P158, DOI 10.1007/978-3-030-01258-8_10; Biggio Battista, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P387, DOI 10.1007/978-3-642-40994-3_25; Brendel Wieland, 2017, DECISION BASED ADVER; Brunner T, 2019, IEEE I CONF COMP VIS, P4957, DOI 10.1109/ICCV.2019.00506; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chen P.-Y., 2017, P AISEC CCS 2017, P15, DOI DOI 10.1145/3128572.3140448; Cheng SY, 2019, ADV NEUR IN, V32; Dong YP, 2020, PROC CVPR IEEE, P318, DOI 10.1109/CVPR42600.2020.00040; Dong YP, 2019, PROC CVPR IEEE, P7706, DOI 10.1109/CVPR.2019.00790; Dong YP, 2019, PROC CVPR IEEE, P4307, DOI 10.1109/CVPR.2019.00444; Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957; Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P77; Guo C, 2020, PR MACH LEARN RES, V115, P1127; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Ian Goodfellow, 2016, Arxiv, DOI arXiv:1605.07277; Ilyas A., 2019, P INT C LEARN REPR; Ilyas A, 2018, PR MACH LEARN RES, V80; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; Lax P. D., 2014, CALCULUS APPL; Li YD, 2019, PR MACH LEARN RES, V97; Liu Y., 2017, PROC INT C LEARN REP; Madry Aleksander, 2017, ARXIV; Maheswaranathan N, 2019, PR MACH LEARN RES, V97; Nesterov Y, 2017, FOUND COMPUT MATH, V17, P527, DOI 10.1007/s10208-015-9296-2; Oh S. J., 2018, P INT C LEARN REPR; Pang T., 2021, PROC INT C LEARN REP; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Simonyan K., 2015, VERY DEEP CONVOLUTIO; Su D, 2018, LECT NOTES COMPUT SC, V11216, P644, DOI 10.1007/978-3-030-01258-8_39; Szegedy C, 2013, 2 INT C LEARNING REP; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tu CC, 2019, AAAI CONF ARTIF INTE, P742; Uesato J, 2018, PR MACH LEARN RES, V80; Xie CH, 2019, PROC CVPR IEEE, P2725, DOI 10.1109/CVPR.2019.00284; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87	46	0	0	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9536	9548		10.1109/TPAMI.2021.3126733	http://dx.doi.org/10.1109/TPAMI.2021.3126733			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34752388	Green Submitted			2022-12-18	WOS:000880661400069
J	Gao, CY; Zhu, Q; Wang, P; Li, H; Liu, YL; Van den Hengel, A; Wu, Q				Gao, Chenyu; Zhu, Qi; Wang, Peng; Li, Hui; Liu, Yuliang; van den Hengel, Anton; Wu, Qi			Structured Multimodal Attentions for TextVQA	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Optical character recognition software; Cognition; Visualization; Text recognition; Task analysis; Knowledge discovery; Annotations; TextVQA; graph attention network; transformer		Text based Visual Question Answering (TextVQA) is a recently raised challenge requiring models to read text in images and answer natural language questions by jointly reasoning over the question, textual information and visual content. Introduction of this new modality - Optical Character Recognition (OCR) tokens ushers in demanding reasoning requirements. Most of the state-of-the-art (SoTA) VQA methods fail when answer these questions because of three reasons: (1) poor text reading ability; (2) lack of textual-visual reasoning capacity; and (3) choosing discriminative answering mechanism over generative couterpart (although this has been further addressed by M4C). In this paper, we propose an end-to-end structured multimodal attention (SMA) neural network to mainly solve the first two issues above. SMA first uses a structural graph representation to encode the object-object, object-text and text-text relationships appearing in the image, and then designs a multimodal graph attention network to reason over it. Finally, the outputs from the above modules are processed by a global-local attentional answering module to produce an answer splicing together tokens from both OCR and general vocabulary iteratively by following M4C. Our proposed model outperforms the SoTA models on TextVQA dataset and two tasks of ST-VQA dataset among all models except pre-training based TAP. Demonstrating strong reasoning ability, it also won first place in TextVQA Challenge 2020. We extensively test different OCR methods on several reasoning models and investigate the impact of gradually increased OCR performance on TextVQA benchmark. With better OCR results, different models share dramatic improvement over the VQA accuracy, but our model benefits most blessed by strong textual-visual reasoning ability. To grant our method an upper bound and make a fair testing base available for further works, we also provide human-annotated ground-truth OCR annotations for the TextVQA dataset, which were not given in the original release. The code and ground-truth OCR annotations for the TextVQA dataset are available at https://github.com/ChenyuGAO-CS/SMA.	[Gao, Chenyu; Wang, Peng] Northwestern Polytech Univ, Ningbo Inst, Xian 710060, Shaanxi, Peoples R China; [Gao, Chenyu] Northwestern Polytech Univ, Sch Software, Xian 710060, Shaanxi, Peoples R China; [Gao, Chenyu; Wang, Peng] Natl Engn Lab & Grated Aerosp Ground Ocean Big Da, Xian, Peoples R China; [Zhu, Qi; Wang, Peng] Northwestern Polytech Univ, Sch Comp Sci, Xian 710060, Shaanxi, Peoples R China; [Li, Hui; Liu, Yuliang; van den Hengel, Anton; Wu, Qi] Univ Adelaide, Adelaide, SA 5005, Australia	Northwestern Polytechnical University; Northwestern Polytechnical University; Northwestern Polytechnical University; University of Adelaide	Wang, P (corresponding author), Northwestern Polytech Univ, Ningbo Inst, Xian 710060, Shaanxi, Peoples R China.; Wang, P (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710060, Shaanxi, Peoples R China.; Wu, Q (corresponding author), Univ Adelaide, Adelaide, SA 5005, Australia.	chenyugao@mail.nwpu.edu.cn; zephyrzhuqi@gmail.com; peng.wang@nwpu.edu.cn; huili03855@gmail.com; liu.yuliang@mail.scut.edu.cn; Anton.vandenHengel@adelaide.edu.au; qi.wu01@adelaide.edu.au		van den Hengel, Anton/0000-0003-3027-8364; Gao, Chenyu/0000-0002-8946-1668; Wu, Qi/0000-0003-3631-256X	National Natural Science Foundation of China [U19B2037, 61876152]; National Key R&D Program of China [2020AAA0106900]; Ningbo Natural Science Foundation [202003N4369]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key R&D Program of China; Ningbo Natural Science Foundation	This work was supported in part by the National Natural Science Foundation of China under Grants U19B2037 and 61876152, and National Key R&D Program of China under Grant 2020AAA0106900, and Ningbo Natural Science Foundation under Grant 202003N4369.	A. Submission, 2019, MSFT VTI TEXTVQACHAL; Almazan J, 2014, IEEE T PATTERN ANAL, V36, P2552, DOI 10.1109/TPAMI.2014.2339814; Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Biten Ali Furkan, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1563, DOI 10.1109/ICDAR.2019.00251; Biten AF, 2019, IEEE I CONF COMP VIS, P4290, DOI 10.1109/ICCV.2019.00439; Borisyuk F, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P71, DOI 10.1145/3219819.3219861; Cadene R, 2019, PROC CVPR IEEE, P1989, DOI 10.1109/CVPR.2019.00209; Cheng ZZ, 2018, PROC CVPR IEEE, P5571, DOI 10.1109/CVPR.2018.00584; Devlin J., 2019, P 2019 C N AM CHAPTE, P4171, DOI [10.18653/v1/n19-1423, DOI 10.18653/V1/N19-1423]; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Gao Difei, 2020, CVPR; Gao JY, 2018, PROC CVPR IEEE, P6576, DOI 10.1109/CVPR.2018.00688; Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670; Gurari D, 2018, PROC CVPR IEEE, P3608, DOI 10.1109/CVPR.2018.00380; Han W., 2020, PROC INT C COMPUT LI; He T, 2018, PROC CVPR IEEE, P5020, DOI 10.1109/CVPR.2018.00527; Hu Ronghang, 2020, P IEEE CVF C COMP VI, P9989; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Kafle K, 2018, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2018.00592; Kahou S. E., 2018, P 6 INT C LEARN REPR; Kant Yash, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P715, DOI 10.1007/978-3-030-58545-7_41; Kembhavi A, 2017, PROC CVPR IEEE, P5376, DOI 10.1109/CVPR.2017.571; Krasin Ivan, 2017, OPENIMAGES PUBLIC DA, V2, P18; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Li H, 2019, AAAI CONF ARTIF INTE, P8610; Li H, 2017, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2017.560; Lin Y., 2019, DCD ZJU TEXTVQA CHAL; Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595; Liu YL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3052; Liu YL, 2019, PROC CVPR IEEE, P9604, DOI 10.1109/CVPR.2019.00984; Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788; Lyu PY, 2018, LECT NOTES COMPUT SC, V11218, P71, DOI 10.1007/978-3-030-01264-9_5; Minghui Liao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P706, DOI 10.1007/978-3-030-58621-8_41; Narasimhan M, 2018, ADV NEUR IN, V31; Norcliffe-Brown W, 2018, ADV NEUR IN, V31; Rao V. N, 2021, PROC 3 WORKSHOP MULT, P19; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Shi BG, 2016, PROC CVPR IEEE, P4168, DOI 10.1109/CVPR.2016.452; Sidorov Oleksii, 2020, EUR C COMP VIS, V12347, P742, DOI DOI 10.1007/978-3-030-58536-544; Singh A, 2019, PROC CVPR IEEE, P8309, DOI 10.1109/CVPR.2019.00851; Singh Amanpreet, 2021, P IEEE CVF C COMP VI, P8802; Teney D, 2017, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2017.344; Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436; Wang P, 2018, IEEE T PATTERN ANAL, V40, P2413, DOI 10.1109/TPAMI.2017.2754246; Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956; Weston J, 2015, PROC INT C LEARN REP; Yang L, 2020, NEUROCOMPUTING, V414, P67, DOI 10.1016/j.neucom.2020.07.010; Yang Zhengyuan, 2021, P IEEE CVF C COMP VI, P8751; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142; Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644; Zhan FN, 2019, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2019.00216	55	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9603	9614		10.1109/TPAMI.2021.3132034	http://dx.doi.org/10.1109/TPAMI.2021.3132034			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34855584	Green Submitted			2022-12-18	WOS:000880661400074
J	Glover, A; Dinale, A; Rosa, LD; Bamford, S; Bartolozzi, C				Glover, Arren; Dinale, Aiko; Rosa, Leandro De Souza; Bamford, Simeon; Bartolozzi, Chiara			luvHarris: A Practical Corner Detector for Event-Cameras	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Event-driven vision; robotic-vision; event-camera; corner-detection; real-time		There have been a number of corner detection methods proposed for event cameras in the last years, since event-driven computer vision has become more accessible. Current state-of-the-art have either unsatisfactory accuracy or real-time performance when considered for practical use, for example when a camera is randomly moved in an unconstrained environment. In this paper, we present yet another method to perform corner detection, dubbed look-up event-Harris (luvHarris), that employs the Harris algorithm for high accuracy but manages an improved event throughput. Our method has two major contributions, 1. a novel "threshold ordinal event-surface" that removes certain tuning parameters and is well suited for Harris operations, and 2. an implementation of the Harris algorithm such that the computational load per event is minimised and computational heavy convolutions are performed only 'as-fast-as-possible', i.e., only as computational resources are available. The result is a practical, real-time, and robust corner detector that runs more than 2.6x the speed of current state-of-the-art; a necessity when using a high-resolution event-camera in real-time. We explain the considerations taken for the approach, compare the algorithm to current state-of-the-art in terms of computational performance and detection accuracy, and discuss the validity of the proposed approach for event cameras.	[Glover, Arren; Dinale, Aiko; Rosa, Leandro De Souza; Bamford, Simeon; Bartolozzi, Chiara] Ist Italiano Tecnol, Event Driven Percept Robot Grp, I-16163 Genoa, Italy	Istituto Italiano di Tecnologia - IIT	Glover, A (corresponding author), Ist Italiano Tecnol, Event Driven Percept Robot Grp, I-16163 Genoa, Italy.	arren.glover@iit.it; aiko.dinale@iit.it; leandro.desouzarosa@iit.it; simeon.bamford@iit.it; chiara.bartolozzi@iit.it		Bartolozzi, Chiara/0000-0003-3465-6449; de Souza Rosa, Leandro/0000-0003-3457-9164				Alzugaray I, 2018, IEEE ROBOT AUTOM LET, V3, P3177, DOI 10.1109/LRA.2018.2849882; Bradski G., 2000, DOBBS J SOFTW TOOLS; Brandli C, 2014, IEEE J SOLID-ST CIRC, V49, P2333, DOI 10.1109/JSSC.2014.2342715; Chiberre P, 2021, IEEE COMPUT SOC CONF, P1387, DOI 10.1109/CVPRW53098.2021.00153; Harris C, 1988, P ALVEY VISION C AVC, P1, DOI DOI 10.5244/C.2.23; Li RX, 2019, IEEE INT C INT ROBOT, P6223, DOI 10.1109/IROS40897.2019.8968491; Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337; Luo Z., 2013, INT J RECENT TECHNOL, V2, P184; Manderscheid J., 2019, PROC IEEECVF C COMPU, p10 237; Mueggler E, 2017, INT J ROBOT RES, V36, P142, DOI 10.1177/0278364917691115; Mueggler Elias, 2017, BRIT MACH VIS C BMVC; Posch C, 2008, IEEE INT SYMP CIRC S, P2130, DOI 10.1109/ISCAS.2008.4541871; Rebecq H, 2019, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2019.00398; Scheerlinck C, 2019, IEEE ROBOT AUTOM LET, V4, P816, DOI 10.1109/LRA.2019.2893427; Vasco V, 2017, 2017 18TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P530, DOI 10.1109/ICAR.2017.8023661; Vasco V, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4144, DOI 10.1109/IROS.2016.7759610; Vidal AR, 2018, IEEE ROBOT AUTOM LET, V3, P994, DOI 10.1109/LRA.2018.2793357; Yilmaz O, 2021, J IMAGING, V7, DOI 10.3390/jimaging7020025	18	0	0	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					10087	10098		10.1109/TPAMI.2021.3135635	http://dx.doi.org/10.1109/TPAMI.2021.3135635			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34910630	Green Submitted			2022-12-18	WOS:000880661400107
J	Hang, JY; Zhang, ML				Hang, Jun-Yi; Zhang, Min-Ling			Collaborative Learning of Label Semantics and Deep Label-Specific Features for Multi-Label Classification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Semantics; Feature extraction; Correlation; Deep learning; Representation learning; Encoding; Collaboration; Machine learning; multi-label classification; label-specific features; label semantics; collaborative learning	SELECTION	In multi-label classification, the strategy of label-specific features has been shown to be effective to learn from multi-label examples by accounting for the distinct discriminative properties of each class label. However, most existing approaches exploit the semantic relations among labels as immutable prior knowledge, which may not be appropriate to constrain the learning process of label-specific features. In this paper, we propose to learn label semantics and label-specific features in a collaborative way. Accordingly, a deep neural network (DNN) based approach named Clif, i.e., Collaborative Learning of label semantIcs and deep label-specific Features for multi-label classification, is proposed. By integrating a graph autoencoder for encoding semantic relations in the label space and a tailored feature-disentangling module for extracting label-specific features, Clif is able to employ the learned label semantics to guide mining label-specific features and propagate label-specific discriminative properties to the learning process of the label semantics. In such a way, the learning of label semantics and label-specific features interact and facilitate with each other so that label semantics can provide more accurate guidance to label-specific feature learning. Comprehensive experiments on 14 benchmark data sets show that our approach outperforms other well-established multi-label classification algorithms.	[Hang, Jun-Yi; Zhang, Min-Ling] Southeast Univ, Sch Comp Sci & Engn, Nanjing 210096, Peoples R China; [Hang, Jun-Yi; Zhang, Min-Ling] Southeast Univ, Key Lab Comp Network & Informat Integrat, Minist Educ, Nanjing, Peoples R China	Southeast University - China; Southeast University - China	Zhang, ML (corresponding author), Southeast Univ, Sch Comp Sci & Engn, Nanjing 210096, Peoples R China.	hangjy@seu.edu.cn; zhangml@seu.edu.cn		Hang, Jun-Yi/0000-0002-0345-8637	National Science Foundation of China [62176055]	National Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Science Foundation of China under Grant 62176055.	Bai JW, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4313; Bhatia Kush, 2015, ADV NEURAL INFORM PR, P730, DOI DOI 10.5555/2969239.2969321; Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009; Brinker C, 2014, IEEE DATA MINING, P731, DOI 10.1109/ICDM.2014.102; Cabral R. S., 2011, ADV NEURAL INFORM PR, P190; Canuto S, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P53, DOI 10.1145/2835776.2835821; Charte F, 2014, LECT NOTES COMPUT SC, V8669, P1, DOI 10.1007/978-3-319-10840-7_1; Che XY, 2020, INFORM SCIENCES, V512, P795, DOI 10.1016/j.ins.2019.10.022; Chen C, 2019, AAAI CONF ARTIF INTE, P3304; Chen D, 2018, PR MACH LEARN RES, V80; Chen TS, 2022, IEEE T PATTERN ANAL, V44, P1371, DOI 10.1109/TPAMI.2020.3025814; Chen Y.N., 2012, P ADV NEUR INF PROC, P1529; Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532; Chen ZS., 2019, PROC 11 ASIAN C MACH, P411; Chu HM, 2018, LECT NOTES COMPUT SC, V11206, P409, DOI 10.1007/978-3-030-01216-8_25; Demsar J, 2006, J MACH LEARN RES, V7, P1; DUNN OJ, 1961, J AM STAT ASSOC, V56, P52, DOI 10.2307/2282330; Elisseeff A, 2002, ADV NEUR IN, V14, P681; Guo Y., 2011, PROC 22 INT JOINT C, P1300, DOI DOI 10.5591/978-1-57735-516-8/IJCA111-220; Guo YM, 2019, ACM T KNOWL DISCOV D, V13, DOI 10.1145/3319911; Hamilton WL, 2017, ADV NEUR IN, V30; Han HR, 2019, IEEE ACCESS, V7, P11474, DOI 10.1109/ACCESS.2019.2891611; Hartvigsen T, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1382, DOI 10.1145/3394486.3403191; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Huang J, 2018, IEEE T CYBERNETICS, V48, P876, DOI 10.1109/TCYB.2017.2663838; Huang J, 2016, IEEE T KNOWL DATA EN, V28, P3309, DOI 10.1109/TKDE.2016.2608339; Huang J, 2015, IEEE DATA MINING, P181, DOI 10.1109/ICDM.2015.67; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jia XY, 2020, J COMPUT SCI TECH-CH, V35, P247, DOI 10.1007/s11390-020-9900-z; Lanchantin Jack, 2019, PROC MACH LEARN KNOW, P138; Li JB, 2022, IEEE T CYBERNETICS, V52, P7732, DOI 10.1109/TCYB.2021.3049630; Ma JH, 2022, IEEE T NEUR NET LEAR, V33, P315, DOI 10.1109/TNNLS.2020.3027745; Maas A.L., 2013, P ICML CIT, V30, P3; McCallum A., 1999, P AAAI 99 WORKSH TEX; Mencia EL, 2008, IEEE IJCNN, P2899, DOI 10.1109/IJCNN.2008.4634206; Mittal A, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P3721, DOI 10.1145/3442381.3449815; Pereira RB, 2018, ARTIF INTELL REV, V49, P57, DOI 10.1007/s10462-016-9516-4; Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5; Ridnik T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P82, DOI 10.1109/ICCV48922.2021.00015; Rubin TN, 2012, MACH LEARN, V88, P157, DOI 10.1007/s10994-011-5272-5; Shen XB, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2675; Sun L, 2016, INT C PATT RECOG, P1612, DOI 10.1109/ICPR.2016.7899867; Sun YP, 2021, FRONT COMPUT SCI-CHI, V15, DOI 10.1007/s11704-020-9294-7; Trohidis K, 2011, EURASIP J AUDIO SPEE, DOI 10.1186/1687-4722-2011-426793; Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34; Tsoumakas G, 2011, IEEE T KNOWL DATA EN, V23, P1079, DOI 10.1109/TKDE.2010.164; Vallet A., 2015, J INF PROCESS, V23, P767, DOI DOI 10.2197/IPSJJIP.23.767; Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251; Wang K, 2018, PROC 10 ASIAN C MACH, P1; Wang Z., 2019, ADV NEURAL INFORM PR, P5820; Wei T, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3842; Weng W, 2020, NEUROCOMPUTING, V377, P85, DOI 10.1016/j.neucom.2019.10.016; Weng W, 2018, NEUROCOMPUTING, V273, P385, DOI 10.1016/j.neucom.2017.07.044; WILCOXON F, 1946, J ECON ENTOMOL, V39, P269, DOI 10.1093/jee/39.2.269; Wu B, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P117, DOI 10.1145/2647868.2654904; Wu X, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3884; Xu JH, 2021, IEEE T MULTIMEDIA, V23, P1696, DOI 10.1109/TMM.2020.3002185; Xu K.-Y, 2019, PROC 7 INT C LEARN R; Xu M, 2021, SCI CHINA INFORM SCI, V64, DOI 10.1007/s11432-020-3132-4; Xu N, 2021, IEEE T KNOWL DATA EN, V33, P1632, DOI 10.1109/TKDE.2019.2947040; Xu SP, 2016, KNOWL-BASED SYST, V104, P52, DOI 10.1016/j.knosys.2016.04.012; Yang L, 2020, FRONT ARTIF INTEL AP, V325, P1634, DOI 10.3233/FAIA200274; Yang YM, 2012, MACH LEARN, V88, P47, DOI 10.1007/s10994-011-5270-7; Yazici Vacit Oguz, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13437, DOI 10.1109/CVPR42600.2020.01345; Yeh CK, 2017, AAAI CONF ARTIF INTE, P2838; Yu TZ, 2016, IEEE SIGNAL PROC LET, V23, P795, DOI 10.1109/LSP.2016.2554361; Yu ZB, 2022, IEEE T PATTERN ANAL, V44, P5199, DOI 10.1109/TPAMI.2021.3070215; Zhan W, 2017, PR INT CONF DATA SC, P129, DOI 10.1109/DSAA.2017.75; Zhang CQ, 2018, AAAI CONF ARTIF INTE, P4414; Zhang CY, 2021, NEUROCOMPUTING, V419, P59, DOI 10.1016/j.neucom.2020.07.107; Zhang J, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2738, DOI 10.1145/3219819.3219958; Zhang ML, 2006, IEEE T KNOWL DATA EN, V18, P1338, DOI 10.1109/TKDE.2006.162; Zhang ML, 2021, IEEE T KNOWL DATA EN, V33, P2057, DOI 10.1109/TKDE.2019.2951561; Zhang ML, 2018, FRONT COMPUT SCI-CHI, V12, P191, DOI 10.1007/s11704-017-7031-7; Zhang ML, 2015, IEEE T PATTERN ANAL, V37, P107, DOI 10.1109/TPAMI.2014.2339815; Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39	76	0	0	2	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9860	9871		10.1109/TPAMI.2021.3136592	http://dx.doi.org/10.1109/TPAMI.2021.3136592			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34928787				2022-12-18	WOS:000880661400093
J	He, JW; Dong, C; Liu, YH; Qiao, Y				He, Jingwen; Dong, Chao; Liu, Yihao; Qiao, Yu			Interactive Multi-Dimension Modulation for Image Restoration	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Degradation; Modulation; Image restoration; Task analysis; Noise reduction; Estimation; Adaptation models; Image restoration; deep learning		Interactive image restoration aims to generate restored images by adjusting a controlling coefficient which determines the restoration level. Previous works are restricted in modulating image with a single coefficient. However, real images always contain multiple types of degradation, which cannot be well determined by one coefficient. To make a step forward, this paper presents a new problem setup, called multi-dimension (MD) modulation, which aims at modulating output effects across multiple degradation types and levels. Compared with the previous single-dimension (SD) modulation, the MD setup to handle multiple degradations adaptively and relief data unbalancing problem in different degradation types. We also propose a deep architecture - CResMD with newly introduced controllable residual connections for multi-dimension modulation. Specifically, we add a controlling variable on the conventional residual connection to allow a weighted summation of input and residual. The values of these weights are generated by another condition network. We further propose a new data sampling strategy based on beta distribution together with a simple loss reweighting approach to balance different degradation types and levels. With corrupted image and degradation information as inputs, the network can output the corresponding restored image. By tweaking the condition vector, users can control the output effects in MD space at test time. Moreover, we also provide an estimation network to predict the condition vector, thus the base network could directly output the restored image without modulation from users. Extensive experiments demonstrate that the proposed CResMD achieves excellent performance on both SD and MD modulation tasks.	[He, Jingwen; Liu, Yihao] Chinese Acad Sci, Shenzhen Inst Adv Technol, ShenZhen Key Lab Comp Vis & Pattern Recognit, SIAT SenseTime Joint Lab, Shenzhen 518055, Peoples R China; [Dong, Chao; Qiao, Yu] Chinese Acad Sci, Shenzhen Inst Adv Technol, ShenZhen Key Lab Comp Vis & Pattern Recognit, SIAT SenseTime Joint Lab, Beijing 100864, Peoples R China; [Dong, Chao; Qiao, Yu] Shanghai AI Lab, Shanghai 201803, Peoples R China; [Liu, Yihao] Univ Chinese Acad Sci, Beijing 100049, Peoples R China	Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS; Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Qiao, Y (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, ShenZhen Key Lab Comp Vis & Pattern Recognit, SIAT SenseTime Joint Lab, Beijing 100864, Peoples R China.	hejingwenhejingwen@outlook.com; chao.dong@siat.ac.cn; yh.liu4@siat.ac.cn; yu.qiao@siat.ac.cn		Liu, Yihao/0000-0001-9874-0602; He, Jingwen/0000-0001-9021-1167	Shanghai Committee of Science and Technology, China [21DZ1100100]; National Natural Science Foundation of China [61906184]; Science and Technology Service Network Initiative of Chinese Academy of Sciences [KFJ-STS-QYZX-092]; Shenzhen Research Program [RCJC20200714114557087]	Shanghai Committee of Science and Technology, China(Shanghai Science & Technology Committee); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Science and Technology Service Network Initiative of Chinese Academy of Sciences; Shenzhen Research Program	This work was supported in part by the Shanghai Committee of Science and Technology, China under Grant 21DZ1100100, the National Natural Science Foundation of China under Grant 61906184, the Science and Technology Service Network Initiative of Chinese Academy of Sciences under Grant KFJ-STS-QYZX-092, and the Shenzhen Research Program under Grant RCJC20200714114557087.	Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25; Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Fan QN, 2018, LECT NOTES COMPUT SC, V11217, P455, DOI 10.1007/978-3-030-01261-8_27; Gu JJ, 2019, PROC CVPR IEEE, P1604, DOI 10.1109/CVPR.2019.00170; Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181; He JW, 2019, PROC CVPR IEEE, P11048, DOI 10.1109/CVPR.2019.01131; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hendrycks D., 2018, PROC INT C LEARN REP; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jingwen He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P53, DOI 10.1007/978-3-030-58565-5_4; Lebrun M, 2015, IMAGE PROCESS ON LIN, V5, P1, DOI 10.5201/ipol.2015.125; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; Roth S, 2005, PROC CVPR IEEE, P860; Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Shoshan A, 2019, IEEE I CONF COMP VIS, P3214, DOI 10.1109/ICCV.2019.00331; Suganuma M, 2019, PROC CVPR IEEE, P9031, DOI 10.1109/CVPR.2019.00925; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wang W, 2019, IEEE I CONF COMP VIS, P4139, DOI 10.1109/ICCV.2019.00424; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Wang XT, 2019, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2019.00179; Yu K, 2022, IEEE T PATTERN ANAL, V44, P7078, DOI 10.1109/TPAMI.2021.3096255; Yu K, 2018, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR.2018.00259; Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30; Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891; Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zhang WL, 2019, IEEE I CONF COMP VIS, P3096, DOI 10.1109/ICCV.2019.00319; Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262	35	0	0	2	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9363	9379		10.1109/TPAMI.2021.3129345	http://dx.doi.org/10.1109/TPAMI.2021.3129345			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34797761				2022-12-18	WOS:000880661400058
J	Hong, S; Chae, J				Hong, Songnam; Chae, Jeongmin			Communication-Efficient Randomized Algorithm for Multi-Kernel Online Federated Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Kernel; Servers; Uplink; Collaborative work; Downlink; Data models; Predictive models; Federated learning; online learning; kernel-based learning; online biconvex optimization; RKHS		Online federated learning (OFL) is a promising framework to learn a sequence of global functions from distributed sequential data at local devices. In this framework, we first introduce a single kernel-based OFL (termed S-KOFL) by incorporating random-feature (RF) approximation, online gradient descent (OGD), and federated averaging (FedAvg). As manifested in the centralized counterpart, an extension to multi-kernel method is necessary. Harnessing the extension principle in the centralized method, we construct a vanilla multi-kernel algorithm (termed vM-KOFL) and prove its asymptotic optimality. However, it is not practical as the communication overhead grows linearly with the size of a kernel dictionary. Moreover, this problem cannot be addressed via the existing communication-efficient techniques (e.g., quantization and sparsification) in the conventional federated learning. Our major contribution is to propose a novel randomized algorithm (named eM-KOFL), which exhibits similar performance to vM-KOFL while maintaining low communication cost. We theoretically prove that eM-KOFL achieves an optimal sublinear regret bound. Mimicking the key concept of eM-KOFL in an efficient way, we propose a more practical pM-KOFL having the same communication overhead as S-KOFL. Via numerical tests with real datasets, we demonstrate that pM-KOFL yields the almost same performance as vM-KOFL (or eM-KOFL) on various online learning tasks.	[Hong, Songnam] Hanyang Univ, Dept Elect Engn, Seoul 04763, South Korea; [Chae, Jeongmin] Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90089 USA	Hanyang University; University of Southern California	Hong, S (corresponding author), Hanyang Univ, Dept Elect Engn, Seoul 04763, South Korea.	snhong@hanyang.ac.kr; chaej@usc.edu			National Research Foundation of Korea (NRF) - Korea government (MSIT) [NRF2020R1A2C1099836]; MSIT (Ministry of Science and ICT), Korea [IITP-2021-2017-0-01637]	National Research Foundation of Korea (NRF) - Korea government (MSIT)(National Research Foundation of KoreaMinistry of Science, ICT & Future Planning, Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); MSIT (Ministry of Science and ICT), Korea(Ministry of Science & ICT (MSIT), Republic of KoreaMinistry of Science, ICT & Future Planning, Republic of Korea)	This work was supported in part by the National Research Foundation of Korea (NRF) funded by the Korea government (MSIT) under Grant NRF2020R1A2C1099836 and in part by the MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information Technology Research Center) support program under Grant IITP-2021-2017-0-01637 supervised by the IITP(Institute for Information & Communications Technology Planning & Evaluation).	Alistarh D, 2018, ADV NEUR IN, V31; Alistarh D, 2017, ADV NEUR IN, V30; Ameet Talwalkar, 2020, Arxiv, DOI arXiv:1812.06127; Anguita D., 2013, ESANN, V3, P3; [Anonymous], GEORGES HEBRAIL UCI; Bernstein J, 2018, PR MACH LEARN RES, V80; Bubeck S., 2011, LECT NOTES; Camero A, 2019, LECT NOTES COMPUT SC, V11353, P386, DOI 10.1007/978-3-030-05348-2_32; De Vito S, 2008, SENSOR ACTUAT B-CHEM, V129, P750, DOI 10.1016/j.snb.2007.09.060; Dua D., 2017, UCI MACHINE LEARNING, DOI DOI 10.1002/JCC.23219; Franeois Kawala E. G., 2013, 4IEME C MODELES LANA, P16; Gonen M, 2011, J MACH LEARN RES, V12, P2211; Hamidieh K, 2018, COMP MATER SCI, V154, P346, DOI 10.1016/j.commatsci.2018.07.052; Hard A., 2019, ARXIV; Hazan E, FOUND TRENDS; Hong S, 2021, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2021.3105146; Hong SN, 2022, IEEE T NEUR NET LEAR, V33, P2980, DOI 10.1109/TNNLS.2020.3047953; Kairouz P., 2019, ARXIV; Karimireddy SP, 2020, PR MACH LEARN RES, V119; Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991; Li T, 2019, CONF REC ASILOMAR C, P1227, DOI 10.1109/IEEECONF44664.2019.9049023; Peters J, 2017, ADAPT COMPUT MACH LE; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rashidi P, 2009, IEEE T SYST MAN CY A, V39, P949, DOI 10.1109/TSMCA.2009.2025137; Richard C, 2009, IEEE T SIGNAL PROCES, V57, P1058, DOI 10.1109/TSP.2008.2009895; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Shen S, 2002, PROC DEP ELECT ENG, P1; Shen YN, 2019, J MACH LEARN RES, V20; Shi SH, 2019, INT CON DISTR COMP S, P2238, DOI 10.1109/ICDCS.2019.00220; Smith V, 2017, ADV NEUR IN, V30; Wainwright MJ, 2019, CA ST PR MA, P1, DOI 10.1017/9781108627771; Wangni JQ, 2018, ADV NEUR IN, V31; Yang Q, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3298981; Yuan H., 2020, ARXIV; Zhao Y, 2022, Arxiv, DOI arXiv:1806.00582	36	0	0	6	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9872	9886		10.1109/TPAMI.2021.3129809	http://dx.doi.org/10.1109/TPAMI.2021.3129809			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34813467	Green Submitted			2022-12-18	WOS:000880661400094
J	Hsu, H; Salamatian, S; Calmon, FP				Hsu, Hsiang; Salamatian, Salman; Calmon, Flavio P.			Generalizing Correspondence Analysis for Applications in Machine Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Correlation; Random variables; Data visualization; Principal component analysis; Kernel; Optimization; Hilbert space; Correspondence analysis; principal inertia components; principal functions; maximal correlation; canonical correlation analysis; interpretability; visualization; multi-view learning; multi-modal learning	CONNECTION	Correspondence analysis (CA) is a multivariate statistical tool used to visualize and interpret data dependencies by finding maximally correlated embeddings of pairs of random variables. CA has found applications in fields ranging from epidemiology to social sciences. However, current methods for CA do not scale to large, high-dimensional datasets. In this paper, we provide a novel interpretation of CA in terms of an information-theoretic quantity called the principal inertia components. We show that estimating the principal inertia components, which consists in solving a functional optimization problem over the space of finite variance functions of two random variable, is equivalent to performing CA. We then leverage this insight to design algorithms to perform CA at scale. Specifically, we demonstrate how the principal inertia components can be reliably approximated from data using deep neural networks. Finally, we show how the maximally correlated embeddings of pairs of random variables in CA further play a central role in several learning problems including multi-view and multi-modal learning methods and visualization of classification boundaries.	[Hsu, Hsiang; Calmon, Flavio P.] Harvard Univ, John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA; [Salamatian, Salman] DE Shaw Grp, New York, NY 10036 USA	Harvard University	Hsu, H (corresponding author), Harvard Univ, John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA.	hsianghsu@g.harvard.edu; salman.salamatian@gmail.com; flavio@seas.harvard.edu		Calmon, Flavio P./0000-0002-7493-1428	National Science Foundation [CAREER-1845852, CIF-1900750]; Amazon Research Award	National Science Foundation(National Science Foundation (NSF)); Amazon Research Award	This work was supported by the National Science Foundation under Grants CAREER-1845852 and CIF-1900750 and by an Amazon Research Award.	Aaker JL, 1997, J MARKETING RES, V34, P347, DOI 10.2307/3151897; Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Abbe E, 2012, IEEE T INFORM THEORY, V58, P721, DOI 10.1109/TIT.2011.2169536; Adrian Benton, 2017, Arxiv, DOI arXiv:1702.02519; Ando RK, 2005, J MACH LEARN RES, V6, P1817; Andrew Galen, 2013, ICML; Anuran Makur, 2018, Arxiv, DOI arXiv:1510.01844; Arora R, 2016, PR MACH LEARN RES, V48; Arora R, 2017, ADV NEUR IN, V30; Arora R, 2013, INT CONF ACOUST SPEE, P7135, DOI 10.1109/ICASSP.2013.6639047; ASIMOV D, 1985, SIAM J SCI STAT COMP, V6, P128, DOI 10.1137/0906011; Asoodeh S, 2015, 2015 IEEE 14TH CANADIAN WORKSHOP ON INFORMATION THEORY (CWIT), P27, DOI 10.1109/CWIT.2015.7255145; Asuncion A, 2007, UCI MACHINE LEARNING; Bach FR, 2003, J MACH LEARN RES, V3, P1, DOI 10.1162/153244303768966085; Benzecri Jean-Paul, 1973, ANAL DONNEES, V2; BREIMAN L, 1985, J AM STAT ASSOC, V80, P580, DOI 10.2307/2288473; Busold CH, 2005, BIOINFORMATICS, V21, P2424, DOI 10.1093/bioinformatics/bti367; Calmon FD, 2017, IEEE T INFORM THEORY, V63, P5011, DOI 10.1109/TIT.2017.2700857; Carrington P. J., 2005, MODELS METHODS SOCIA; Chaudhuri K., 2009, PROC INT C MACHINE L, P129, DOI DOI 10.1145/1553374.1553391; Chung F.R.K., 1997, AM MATH SOC, DOI DOI 10.1090/CBMS/092; Cullum J., 1975, NONDIFFERENTIABLE OP, P35; David Tse, 2017, Arxiv, DOI arXiv:1702.05471; Dhurandhar A, 2018, ADV NEUR IN, V31; Ferrari A, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12222; Fukumizu K, 2007, J MACH LEARN RES, V8, P361; Gao C, 2019, J MACH LEARN RES, V20; Gary Klein, 2019, Arxiv, DOI arXiv:1812.04608; Gebelein H, 1941, Z ANGEW MATH MECH, V21, P364, DOI 10.1002/zamm.19410210604; GISH H, 1990, INT CONF ACOUST SPEE, P1361, DOI 10.1109/ICASSP.1990.115636; Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P77; GOWER JC, 2004, OX STAT SCI, V30, pR13; GREENACRE M, 1987, J AM STAT ASSOC, V82, P437, DOI 10.2307/2289445; Greenacre M.J., 2017, CORRES ANAL PRACTICE, V3rd Edn, DOI [10.1201/9781420011234, DOI 10.1201/9781315369983, 10.1201/9781315369983]; Greenacre M.J., 1984, THEORY APPL CORRES A; Guidotti R, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3236009; Hannan E. J., 1961, J AUSTR MATH SOC, V2, P229; Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814; Hirschfeld H, 1935, P CAMB PHILOS SOC, V31, P520, DOI 10.1017/S0305004100013517; Hoare J, 2019, INT J MARKET RES, V61, P12, DOI 10.1177/1470785318801480; Hoffmann H, 2007, PATTERN RECOGN, V40, P863, DOI 10.1016/j.patcog.2006.07.009; Honglak Lee, 2017, Arxiv, DOI arXiv:1610.03454; Horn R.A., 2012, MATRIX ANAL, DOI [DOI 10.1017/CBO9780511810817, 10.1017/CBO9780511810817]; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025; Hsu H, 2019, PR MACH LEARN RES, V89; Hu JL, 2018, IEEE T PATTERN ANAL, V40, P2281, DOI 10.1109/TPAMI.2017.2749576; Huang SL, 2017, IEEE INT SYMP INFO, P1336, DOI 10.1109/ISIT.2017.8006746; Huang SR, 2019, PLANT SIGNAL BEHAV, V14, DOI 10.1080/15592324.2019.1629266; Ionescu C, 2015, IEEE I CONF COMP VIS, P2965, DOI 10.1109/ICCV.2015.339; James Melville, 2020, Arxiv, DOI arXiv:1802.03426; Janzing D, 2020, PR MACH LEARN RES, V108, P2907; Jeon J., 2003, P ACM SIGIR C RES DE, P119; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky Alex, 2009, LEARNING MULTIPLE LA, P6; LANCASTER HO, 1958, ANN MATH STAT, V29, P719, DOI 10.1214/aoms/1177706532; Lebart L., 2013, P 5 C INT FED CLASS, P423; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li T, 2006, IEEE DATA MINING, P362; Li YM, 2019, IEEE T KNOWL DATA EN, V31, P1863, DOI 10.1109/TKDE.2018.2872063; Linardatos P, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23010018; Lu Y., 2014, ADV NEURAL INF PROCE, P91; Ma Z, 2015, PR MACH LEARN RES, V37, P169; Makur A, 2016, ANN ALLERTON CONF, P633, DOI 10.1109/ALLERTON.2016.7852291; Makur A, 2015, ANN ALLERTON CONF, P1422, DOI 10.1109/ALLERTON.2015.7447175; Michaeli T, 2016, PR MACH LEARN RES, V48; Ngiam Jiquan, 2011, ICML, DOI DOI 10.5555/3104482.3104569; Nie FP, 2017, SCI CHINA INFORM SCI, V60, DOI 10.1007/s11432-016-9021-9; ODonnell Ryan, 2014, ANAL BOOLEAN FUNCTIO; Ormoli L, 2015, SCI REP-UK, V5, DOI 10.1038/srep08574; Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303; Renyi A., 1959, ACTA MATH ACAD SCI H, V10, DOI [DOI 10.1007/BF02024507, 10.1007/BF02024507]; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Sanderson M, 2010, NAT LANG ENG, V16, P100, DOI 10.1017/S1351324909005129; Sourial N, 2010, J CLIN EPIDEMIOL, V63, P638, DOI 10.1016/j.jclinepi.2009.08.008; Srivastava Nitish, 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49; Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613; Tekaia F, 2016, BIOINFORM BIOL INSIG, V10, P59, DOI 10.4137/BBI.S39614; ter Braak CJF, 2004, ECOLOGY, V85, P834, DOI 10.1890/03-0021; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang H, 2017, ANN ALLERTON CONF, P886; Wang LC, 2019, AAAI CONF ARTIF INTE, P5281; Wang WR, 2015, PR MACH LEARN RES, V37, P1083; Wang WR, 2015, ANN ALLERTON CONF, P688, DOI 10.1109/ALLERTON.2015.7447071; WITSENHAUSEN HS, 1975, SIAM J APPL MATH, V28, P100, DOI 10.1137/0128010; Yummly, 2015, KAGGL WHATS COOK	92	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9347	9362		10.1109/TPAMI.2021.3127870	http://dx.doi.org/10.1109/TPAMI.2021.3127870			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34767505	Green Submitted			2022-12-18	WOS:000880661400057
J	Hu, D; Wei, YK; Qian, R; Lin, WY; Song, RH; Wen, JR				Hu, Di; Wei, Yake; Qian, Rui; Lin, Weiyao; Song, Ruihua; Wen, Ji-Rong			Class-Aware Sounding Objects Localization via Audiovisual Correspondence	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Location awareness; Visualization; Task analysis; Annotations; Semantics; Dictionaries; Videos; Class-aware sounding object localization; audiovisual correspondence; distribution alignment	IDENTIFICATION; VISION	Audiovisual scenes are pervasive in our daily life. It is commonplace for humans to discriminatively localize different sounding objects but quite challenging for machines to achieve class-aware sounding objects localization without category annotations, i.e., localizing the sounding object and recognizing its category. To address this problem, we propose a two-stage step-by-step learning framework to localize and recognize sounding objects in complex audiovisual scenarios using only the correspondence between audio and vision. First, we propose to determine the sounding area via coarse-grained audiovisual correspondence in the single source cases. Then visual features in the sounding area are leveraged as candidate object representations to establish a category-representation object dictionary for expressive visual character extraction. We generate class-aware object localization maps in cocktail-party scenarios and use audiovisual correspondence to suppress silent areas by referring to this dictionary. Finally, we employ category-level audiovisual consistency as the supervision to achieve fine-grained audio and sounding object distribution alignment. Experiments on both realistic and synthesized videos show that our model is superior in localizing and recognizing objects as well as filtering out silent ones. We also transfer the learned audiovisual network into the unsupervised object detection task, obtaining reasonable performance.	[Hu, Di; Wei, Yake; Song, Ruihua; Wen, Ji-Rong] Renmin Univ China, Gaoling Sch Artificial Intelligence, Beijing 100872, Peoples R China; [Hu, Di; Wei, Yake; Song, Ruihua; Wen, Ji-Rong] Renmin Univ China, Beijing Key Lab Big Data Management & Anal Method, Beijing 100872, Peoples R China; [Qian, Rui] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China; [Lin, Weiyao] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China	Renmin University of China; Renmin University of China; Chinese University of Hong Kong; Shanghai Jiao Tong University	Hu, D (corresponding author), Renmin Univ China, Gaoling Sch Artificial Intelligence, Beijing 100872, Peoples R China.; Hu, D (corresponding author), Renmin Univ China, Beijing Key Lab Big Data Management & Anal Method, Beijing 100872, Peoples R China.	dihu@ruc.edu.cn; yakewei@ruc.edu.cn; qr021@ie.cuhk.edu.hk; wylin@sjtu.edu.cn; rsong@ruc.edu.cn; jrwen@ruc.edu.cn		Lin, Weiyao/0000-0001-8307-7107	Intelligent Social Governance Platform, Major Innovation & Planning Interdisciplinary Platform for the "DoubleFirst Class" Initiative, Renmin University of China; Intelligent Social Governance Platform, Major Innovation & Planning Interdisciplinary Platform for the "DoubleFirst Class" Initiative, Renmin University of China, Beijing Outstanding Young Scientist Program [BJJWZYJH012019100020098]; Public Computing Cloud, Renmin University of China, National Natural Science Foundation of China [62106272]; 2021 Tencent AI Lab Rhino-Bird Focused Research Program; Research Funds of Renmin University of China [21XNLG17]; National Natural Science Foundation of China [U21B2013]	Intelligent Social Governance Platform, Major Innovation & Planning Interdisciplinary Platform for the "DoubleFirst Class" Initiative, Renmin University of China; Intelligent Social Governance Platform, Major Innovation & Planning Interdisciplinary Platform for the "DoubleFirst Class" Initiative, Renmin University of China, Beijing Outstanding Young Scientist Program; Public Computing Cloud, Renmin University of China, National Natural Science Foundation of China; 2021 Tencent AI Lab Rhino-Bird Focused Research Program; Research Funds of Renmin University of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	The work was supported in part by Intelligent Social Governance Platform, Major Innovation & Planning Interdisciplinary Platform for the "DoubleFirst Class" Initiative, Renmin University of China, Beijing Outstanding Young Scientist Program under Grant BJJWZYJH012019100020098, in part by Public Computing Cloud, Renmin University of China, National Natural Science Foundation of China under Grant 62106272, in part by 2021 Tencent AI Lab Rhino-Bird Focused Research Program under Grant R202141 and the cooperation project with Big Data Lab, Baidu, and in part by the Research Funds of Renmin University of China under Grant 21XNLG17 and in part by National Natural Science Foundation of China under Grant U21B2013.	Alayrac Jean-Baptiste, 2020, NEURIPS, V2, P6; Alwassel H., 2020, PROC 34 INT C NEURAL, V33, P9785; Arandjelovic R, 2018, LECT NOTES COMPUT SC, V11205, P451, DOI 10.1007/978-3-030-01246-5_27; Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73; Asano Yuki, 2020, ADV NEURAL INF PROCE, V33, P4660; Baek K, 2020, AAAI CONF ARTIF INTE, V34, P10451; Barzelay Z., 2007, P IEEE INT C COMP VI, P1; Bazzani L, 2016, IEEE WINT CONF APPL, DOI 10.1109/wacv.2016.7477688; Blauert J, 1997, SPATIAL HEARING PSYC; Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9; Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097; Chen HL, 2021, PROC CVPR IEEE, P16862, DOI 10.1109/CVPR46437.2021.01659; Chen HL, 2020, INT CONF ACOUST SPEE, P721, DOI 10.1109/ICASSP40776.2020.9053174; Coates Adam, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P561, DOI 10.1007/978-3-642-35289-8_30; Dejing Dou, 2020, Arxiv, DOI arXiv:2001.09414; Deng J., 2009, 2009 IEEE C COMP VIS, P248, DOI [DOI 10.1109/CVPR.2009.5206848, 10.1109/CVPR.2009.5206848]; Di Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P68, DOI 10.1007/978-3-030-58586-0_5; ELMAN JL, 1993, COGNITION, V48, P71, DOI 10.1016/0010-0277(93)90058-4; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Gan C, 2019, IEEE I CONF COMP VIS, P7052, DOI 10.1109/ICCV.2019.00715; Gao RH, 2019, IEEE I CONF COMP VIS, P3878, DOI 10.1109/ICCV.2019.00398; Gao RH, 2019, PROC CVPR IEEE, P324, DOI 10.1109/CVPR.2019.00041; Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hershey J, 2000, ADV NEUR IN, V12, P813; Hu D, 2019, PROC CVPR IEEE, P9240, DOI 10.1109/CVPR.2019.00947; Hu Di, 2020, ADV NEURAL INF PROCE, V33, P10077; Izadinia H, 2013, IEEE T MULTIMEDIA, V15, P378, DOI 10.1109/TMM.2012.2228476; JONES B, 1975, PERCEPT PSYCHOPHYS, V17, P241, DOI 10.3758/BF03203206; Kidron E, 2005, PROC CVPR IEEE, P88; Krasin Ivan, 2017, OPENIMAGES PUBLIC DA, V2, P18; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Majdak P, 2010, ATTEN PERCEPT PSYCHO, V72, P454, DOI 10.3758/APP.72.2.454; Morgado P., 2018, PROC NEURIPS, P360; Norman D. A., 1976, MEMORY ATTENTION INT; Oquab M, 2015, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2015.7298668; Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222; Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39; Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48; Oya T., 2020, PROC ASIAN C COMPUT, P119; Proulx MJ, 2014, NEUROSCI BIOBEHAV R, V41, P16, DOI 10.1016/j.neubiorev.2012.11.017; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rui Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P292, DOI 10.1007/978-3-030-58565-5_18; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Senocak A, 2018, PROC CVPR IEEE, P4358, DOI 10.1109/CVPR.2018.00458; SHELTON BR, 1980, PERCEPT PSYCHOPHYS, V28, P589, DOI 10.3758/BF03198830; Stein B E, 1989, J Cogn Neurosci, V1, P12, DOI 10.1162/jocn.1989.1.1.12; Tian Hao, 2021, P IEEECVF C COMPUTER, P5962; Tian YP, 2021, PROC CVPR IEEE, P2744, DOI 10.1109/CVPR46437.2021.00277; Tian YP, 2018, LECT NOTES COMPUT SC, V11206, P252, DOI 10.1007/978-3-030-01216-8_16; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Van Trees H. L, 2004, OPTIMUM ARRAY PROC 4; Zhao H., 2018, LECT NOTES COMPUT SC, P570, DOI DOI 10.1007/978-3-030-01246-5_35; Zhao H, 2019, IEEE I CONF COMP VIS, P1735, DOI 10.1109/ICCV.2019.00182; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zunino A, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P693, DOI 10.1109/ICCVW.2015.95	58	0	0	2	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9844	9859		10.1109/TPAMI.2021.3137988	http://dx.doi.org/10.1109/TPAMI.2021.3137988			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34941503	Green Submitted			2022-12-18	WOS:000880661400092
J	Hu, XB; Ren, WQ; Yang, JL; Cao, XC; Wipf, D; Menze, B; Tong, X; Zha, HB				Hu, Xiaobin; Ren, Wenqi; Yang, Jiaolong; Cao, Xiaochun; Wipf, David; Menze, Bjoern; Tong, Xin; Zha, Hongbin			Face Restoration via Plug-and-Play 3D Facial Priors	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Face recognition; Three-dimensional displays; Faces; Image restoration; Superresolution; Task analysis; Neural networks; Face restoration; 3D facial priors; 3D morphable knowledge; facial structures; identity knowledge	DEEP CONVOLUTIONAL NETWORK; REPRESENTATION; HALLUCINATION; IMAGES	State-of-the-art face restoration methods employ deep convolutional neural networks (CNNs) to learn a mapping between degraded and sharp facial patterns by exploring local appearance knowledge. However, most of these methods do not well exploit facial structures and identity information, and only deal with task-specific face restoration (e.g., face super-resolution or deblurring). In this paper, we propose cross-tasks and cross-models plug-and-play 3D facial priors to explicitly embed the network with the sharp facial structures for general face restoration tasks. Our 3D priors are the first to explore 3D morphable knowledge based on the fusion of parametric descriptions of face attributes (e.g., identity, facial expression, texture, illumination, and face pose). Furthermore, the priors can easily be incorporated into any network and are very efficient in improving the performance and accelerating the convergence speed. Firstly, a 3D face rendering branch is set up to obtain 3D priors of salient facial structures and identity knowledge. Secondly, for better exploiting this hierarchical information (i.e., intensity similarity, 3D facial structure, and identity content), a spatial attention module is designed for the image restoration problems. Extensive face restoration experiments including face super-resolution and deblurring demonstrate that the proposed 3D priors achieve superior face restoration results over the state-of-the-art algorithms.	[Hu, Xiaobin; Ren, Wenqi; Cao, Xiaochun] Sun Yat Sen Univ, Sch Cyber Sci & Technol, Shenzhen Campus, Shenzhen 518107, Guangdong, Peoples R China; [Hu, Xiaobin; Menze, Bjoern] Tech Univ Munich, Dept Informat, D-80333 Munich, Germany; [Ren, Wenqi] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing 100093, Peoples R China; [Yang, Jiaolong; Tong, Xin] Microsoft Res Asia, Beijing 100080, Peoples R China; [Wipf, David] Amazon AI Lab Shanghai, Beijing 100125, Peoples R China; [Zha, Hongbin] Peking Univ, Dept Machine Intelligence, Key Lab Machine Percept MOE, Beijing 100080, Peoples R China	Sun Yat Sen University; Technical University of Munich; Chinese Academy of Sciences; Institute of Information Engineering, CAS; Microsoft; Microsoft Research Asia; Peking University	Ren, WQ (corresponding author), Sun Yat Sen Univ, Sch Cyber Sci & Technol, Shenzhen Campus, Shenzhen 518107, Guangdong, Peoples R China.	xbhunanu@gmail.com; renwenqi@iie.ac.cn; jiaoyan@microsoft.com; caoxiaochun@iie.ac.cn; davidwipf@gmail.com; bjoern.menze@uzh.ch; xtong@microsoft.com; zha@cis.pku.edu.cn		Menze, Bjoern/0000-0003-4136-5690	National Key R&D Program of China [2020AAA0109304]; National Natural Science Foundation of China [62172409, 62072454, 62025604, 61971016]; Beijing Natural Science Foundation [4202084]; Beihang University [VRLAB2021C06]; Beijing Nova Program [Z201100006820074]; Youth Innovation Promotion Association CAS	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation); Beihang University; Beijing Nova Program(Beijing Municipal Science & Technology Commission); Youth Innovation Promotion Association CAS	This work was supported in part by the National Key R&D Program of China under Grant 2020AAA0109304, in part by the National Natural Science Foundation of China under Grants 62172409, 62072454, 62025604, and 61971016, in part by the Beijing Natural Science Foundation under Grant 4202084, in part by Beihang University under Grant VRLAB2021C06, in part by Beijing Nova Program under Grant Z201100006820074, and in part by the Youth Innovation Promotion Association CAS.	Anwar S, 2019, IEEE T PATTERN ANAL, V41, P2112, DOI 10.1109/TPAMI.2018.2855177; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598; Boracchi G, 2012, IEEE T IMAGE PROCESS, V21, P3502, DOI 10.1109/TIP.2012.2192126; Bulat A, 2018, PROC CVPR IEEE, P109, DOI 10.1109/CVPR.2018.00019; Bulat A, 2018, LECT NOTES COMPUT SC, V11210, P187, DOI 10.1007/978-3-030-01231-1_12; Cao QX, 2017, PROC CVPR IEEE, P1656, DOI 10.1109/CVPR.2017.180; Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264; Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491; Dahl R, 2017, IEEE I CONF COMP VIS, P5449, DOI 10.1109/ICCV.2017.581; David Berthelot, 2020, Arxiv, DOI arXiv:2003.02365; Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038; Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Fritsche M., 2019, PROC IEEE C COMPUT V, P428; Grm K, 2020, IEEE T IMAGE PROCESS, V29, P2150, DOI 10.1109/TIP.2019.2945835; HaCohen Y, 2013, IEEE I CONF COMP VIS, P2384, DOI 10.1109/ICCV.2013.296; Han CR, 2018, LECT NOTES COMPUT SC, V11213, P120, DOI 10.1007/978-3-030-01240-3_8; Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187; Jaderberg M, 2015, ADV NEUR IN, V28; Kae A, 2013, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2013.263; Zhang K, 2020, Arxiv, DOI arXiv:2008.13751; Kim D., 2019, BRIT MACH VIS C; Kim J., 2016, IEEE C COMP VIS PATT, P5835; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kim TH, 2015, PROC CVPR IEEE, P5426, DOI 10.1109/CVPR.2015.7299181; Kim TH, 2013, IEEE I CONF COMP VIS, P3160, DOI 10.1109/ICCV.2013.392; Kim TH, 2014, PROC CVPR IEEE, P2766, DOI 10.1109/CVPR.2014.348; Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521; Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897; Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854; Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618; Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49; Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0; Lian SL, 2019, LECT NOTES COMPUT SC, V11554, P151, DOI 10.1007/978-3-030-22796-8_17; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5; Liu W, 2005, PROC CVPR IEEE, P478; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Ma C, 2020, PROC CVPR IEEE, P5568, DOI 10.1109/CVPR42600.2020.00561; Meishvili G, 2020, PROC CVPR IEEE, P1361, DOI 10.1109/CVPR42600.2020.00144; Menon S, 2020, PROC CVPR IEEE, P2434, DOI 10.1109/CVPR42600.2020.00251; Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35; Pan JS, 2019, IEEE T PATTERN ANAL, V41, P1412, DOI 10.1109/TPAMI.2018.2832125; Pan JS, 2014, LECT NOTES COMPUT SC, V8695, P47, DOI 10.1007/978-3-319-10584-0_4; Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317; Ren WQ, 2019, IEEE I CONF COMP VIS, P9387, DOI 10.1109/ICCV.2019.00948; Sanderson C, 2004, DIGIT SIGNAL PROCESS, V14, P449, DOI 10.1016/j.dsp.2004.05.001; Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672; Shen J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P1003, DOI 10.1109/ICCVW.2015.132; Shen ZY, 2018, PROC CVPR IEEE, P8260, DOI 10.1109/CVPR.2018.00862; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Sim T, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P53, DOI 10.1109/AFGR.2002.1004130; Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33; Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298; Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853; Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262; van den Oord A, 2016, PR MACH LEARN RES, V48; Wang XG, 2005, IEEE T SYST MAN CY C, V35, P425, DOI 10.1109/TSMCC.2005.848171; Wang XT, 2021, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR46437.2021.00905; Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070; Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166; Xiaobin Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P763, DOI 10.1007/978-3-030-58548-8_44; Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147; Yang JL, 2017, PROC CVPR IEEE, P5216, DOI 10.1109/CVPR.2017.554; Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596; Yang T, 2021, PROC CVPR IEEE, P672, DOI 10.1109/CVPR46437.2021.00073; Yang Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7353, DOI 10.1109/CVPR42600.2020.00738; Yu X, 2018, LECT NOTES COMPUT SC, V11213, P219, DOI 10.1007/978-3-030-01240-3_14; Yu X, 2020, INT J COMPUT VISION, V128, P500, DOI 10.1007/s11263-019-01254-5; Yu X, 2018, PROC CVPR IEEE, P908, DOI 10.1109/CVPR.2018.00101; Yu X, 2018, IEEE T IMAGE PROCESS, V27, P2747, DOI 10.1109/TIP.2018.2808840; Yu X, 2017, PROC CVPR IEEE, P5367, DOI 10.1109/CVPR.2017.570; Yu X, 2016, LECT NOTES COMPUT SC, V9909, P318, DOI 10.1007/978-3-319-46454-1_20; Zafeiriou S, 2017, IEEE COMPUT SOC CONF, P2116, DOI 10.1109/CVPRW.2017.263; Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344; Zhang KH, 2020, PROC CVPR IEEE, P2734, DOI 10.1109/CVPR42600.2020.00281; Zhang KP, 2018, LECT NOTES COMPUT SC, V11215, P196, DOI 10.1007/978-3-030-01252-6_12; Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI [10.1007/978-3-030-01234-2_18, 10.1007/978-3-030-01240-3_22]; Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262; Zhao J, 2019, IEEE T PATTERN ANAL, V41, P2380, DOI 10.1109/TPAMI.2018.2858819; Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342; Zhong L, 2013, PROC CVPR IEEE, P612, DOI 10.1109/CVPR.2013.85; Zhou EJ, 2015, AAAI CONF ARTIF INTE, P3871; Zhu SZ, 2016, LECT NOTES COMPUT SC, V9909, P614, DOI 10.1007/978-3-319-46454-1_37	90	0	0	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					8910	8926		10.1109/TPAMI.2021.3123085	http://dx.doi.org/10.1109/TPAMI.2021.3123085			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34705635	Green Accepted			2022-12-18	WOS:000880661400028
J	Jeon, S; Kim, S; Min, D; Sohn, K				Jeon, Sangryul; Kim, Seungryong; Min, Dongbo; Sohn, Kwanghoon			Pyramidal Semantic Correspondence Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Semantics; Computer architecture; Proposals; Strain; Feature extraction; Robustness; Microprocessors; Dense semantic correspondence; spatial pyramid model; coarse-to-fine inference	DENSE; FLOW	This paper presents a deep architecture, called pyramidal semantic correspondence networks (PSCNet), that estimates locally-varying affine transformation fields across semantically similar images. To deal with large appearance and shape variations that commonly exist among different instances within the same object category, we leverage a pyramidal model where the affine transformation fields are progressively estimated in a coarse-to-fine manner so that the smoothness constraint is naturally imposed. Different from the previous methods which directly estimate global or local deformations, our method first starts to estimate the transformation from an entire image and then progressively increases the degree of freedom of the transformation by dividing coarse cell into finer ones. To this end, we propose two spatial pyramid models by dividing an image in a form of quad-tree rectangles or into multiple semantic elements of an object. Additionally, to overcome the limitation of insufficient training data, a novel weakly-supervised training scheme is introduced that generates progressively evolving supervisions through the spatial pyramid models by leveraging a correspondence consistency across image pairs. Extensive experimental results on various benchmarks including TSS, Proposal Flow-WILLOW, Proposal Flow-PASCAL, Caltech-101, and SPair-71k demonstrate that the proposed method outperforms the lastest methods for dense semantic correspondence.	[Jeon, Sangryul; Sohn, Kwanghoon] Yonsei Univ, Sch Elect & Elect Engn, Seoul 03722, South Korea; [Kim, Seungryong] Korea Univ, Dept Comp Sci & Engn, Seoul 02841, South Korea; [Min, Dongbo] Ewha Womans Univ, Dept Comp Sci & Engn, Seoul 03760, South Korea	Yonsei University; Korea University; Ewha Womans University	Sohn, K (corresponding author), Yonsei Univ, Sch Elect & Elect Engn, Seoul 03722, South Korea.	cheonjsr@microsoft.com; seungryong_kim@korea.ac.kr; dbmin@ewha.ac.kr; khsohn@yonsei.ac.kr		Min, Dongbo/0000-0003-4825-5240; Kim, Seungryong/0000-0003-2927-6273	R&D program for Advanced Integrated-intelligence for Identification (AIID) through the National Research Foundation [NRF-2018M3E3A1057289]; Yonsei University [2021-22-0001]; MSIT (Ministry of Science and ICT) under the ICT Creative Consilience program [IITP-2021-2020-0-01819]	R&D program for Advanced Integrated-intelligence for Identification (AIID) through the National Research Foundation; Yonsei University; MSIT (Ministry of Science and ICT) under the ICT Creative Consilience program	This work was supported in part by the R&D program for Advanced Integrated-intelligence for Identification (AIID) through the National Research Foundation under Grant NRF-2018M3E3A1057289 and Yonsei University Research Fund under Grant 2021-22-0001. This work of Seungryong Kim was supported in part by MSIT (Ministry of Science and ICT) under the ICT Creative Consilience program under Grant IITP-2021-2020-0-01819.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Brox T, 2009, PROC CVPR IEEE, P41, DOI 10.1109/CVPRW.2009.5206697; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254; Choy CB, 2016, ADV NEUR IN, V29; Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172; Collins E, 2018, LECT NOTES COMPUT SC, V11218, P352, DOI 10.1007/978-3-030-01264-9_21; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; HaCohen Y, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964965; Ham B, 2018, IEEE T PATTERN ANAL, V40, P1711, DOI 10.1109/TPAMI.2017.2724510; Ham B, 2016, PROC CVPR IEEE, P3475, DOI 10.1109/CVPR.2016.378; Han K, 2017, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2017.203; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang SY, 2019, IEEE I CONF COMP VIS, P2010, DOI 10.1109/ICCV.2019.00210; Hung WC, 2019, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2019.00096; Hur J, 2015, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR.2015.7298745; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Jaderberg M, 2015, ADV NEUR IN, V28; Jean Ponce, 2019, Arxiv, DOI arXiv:1908.10543; Jeon S, 2018, LECT NOTES COMPUT SC, V11210, P355, DOI 10.1007/978-3-030-01231-1_22; Juhong Min, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P346, DOI 10.1007/978-3-030-58555-6_21; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kim J, 2013, PROC CVPR IEEE, P2307, DOI 10.1109/CVPR.2013.299; Kim S, 2017, IEEE I CONF COMP VIS, P4539, DOI 10.1109/ICCV.2017.485; Kim S, 2017, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2017.73; Kim Seungryong, 2018, ADV NEURAL INFORM PR, P6129; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee J, 2022, IEEE T PATTERN ANAL, V44, P1399, DOI 10.1109/TPAMI.2020.3013620; Lee J, 2019, PROC CVPR IEEE, P2273, DOI 10.1109/CVPR.2019.00238; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Li S., 2020, PROC IEEECVF C COMPU, P10196; Lin CH, 2017, PROC CVPR IEEE, P2252, DOI 10.1109/CVPR.2017.242; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Liu SL, 2021, PROC CVPR IEEE, P8351, DOI 10.1109/CVPR46437.2021.00825; Liu YB, 2020, PROC CVPR IEEE, P4462, DOI 10.1109/CVPR42600.2020.00452; Long J. L., 2014, ADV NEURAL INFORM PR, V27, P1601; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Min J, 2019, IEEE I CONF COMP VIS, P3394, DOI 10.1109/ICCV.2019.00349; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Novotny D, 2018, PROC CVPR IEEE, P3637, DOI 10.1109/CVPR.2018.00383; Novotny D, 2017, PROC CVPR IEEE, P2867, DOI 10.1109/CVPR.2017.306; Revaud J, 2016, INT J COMPUT VISION, V120, P300, DOI 10.1007/s11263-016-0908-3; Rocco I, 2018, ADV NEUR IN, V31; Rocco I, 2018, PROC CVPR IEEE, P6917, DOI 10.1109/CVPR.2018.00723; Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12; Sangryul Jeon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P631, DOI 10.1007/978-3-030-58604-1_38; Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Seo PH, 2018, LECT NOTES COMPUT SC, V11208, P367, DOI 10.1007/978-3-030-01225-0_22; Taniai T, 2016, PROC CVPR IEEE, P4246, DOI 10.1109/CVPR.2016.460; Thewlis J, 2017, IEEE I CONF COMP VIS, P3229, DOI 10.1109/ICCV.2017.348; Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77; Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832; Ufer N, 2017, PROC CVPR IEEE, P5929, DOI 10.1109/CVPR.2017.628; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Xiang Y, 2014, IEEE WINT CONF APPL, P75, DOI 10.1109/WACV.2014.6836101; Yang F, 2017, PROC CVPR IEEE, P4151, DOI 10.1109/CVPR.2017.442; Yang HS, 2014, PROC CVPR IEEE, P3406, DOI 10.1109/CVPR.2014.435; Zbontar J, 2016, J MACH LEARN RES, V17; Zhang YT, 2018, PROC CVPR IEEE, P2694, DOI 10.1109/CVPR.2018.00285; Zhou TH, 2015, PROC CVPR IEEE, P1191, DOI 10.1109/CVPR.2015.7298723	64	0	0	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9102	9118		10.1109/TPAMI.2021.3123679	http://dx.doi.org/10.1109/TPAMI.2021.3123679			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34714738	hybrid			2022-12-18	WOS:000880661400040
J	Kang, L; Riba, P; Rusinol, M; Fornes, A; Villegas, M				Kang, Lei; Riba, Pau; Rusinol, Marcal; Fornes, Alicia; Villegas, Mauricio			Content and Style Aware Generation of Text-Line Images for Handwriting Recognition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Visualization; Text recognition; Writing; Training; Handwriting recognition; Image recognition; Vocabulary; Handwritten text recognition; transformers; generative adversarial networks; synthetic data generation		Handwritten Text Recognition has achieved an impressive performance in public benchmarks. However, due to the high inter- and intra-class variability between handwriting styles, such recognizers need to be trained using huge volumes of manually labeled training data. To alleviate this labor-consuming problem, synthetic data produced with TrueType fonts has been often used in the training loop to gain volume and augment the handwriting style variability. However, there is a significant style bias between synthetic and real data which hinders the improvement of recognition performance. To deal with such limitations, we propose a generative method for handwritten text-line images, which is conditioned on both visual appearance and textual content. Our method is able to produce long text-line samples with diverse handwriting styles. Once properly trained, our method can also be adapted to new target data by only accessing unlabeled text-line images to mimic handwritten styles and produce images with any textual content. Extensive experiments have been done on making use of the generated samples to boost Handwritten Text Recognition performance. Both qualitative and quantitative results demonstrate that the proposed approach outperforms the current state of the art.	[Kang, Lei] Shantou Univ, Comp Sci Dept, Shantou 515063, Peoples R China; [Riba, Pau] Helsing AI, D-80331 Munich, Germany; [Rusinol, Marcal] AllRead MLT, Barcelona 08039, Spain; [Fornes, Alicia] Univ Autonoma Barcelona, Comp Vis Ctr, Comp Sci Dept, Bellaterra 08193, Spain; [Villegas, Mauricio] Omni Us, D-10559 Berlin, Germany	Shantou University; Autonomous University of Barcelona; Centre de Visio per Computador (CVC)	Kang, L (corresponding author), Shantou Univ, Comp Sci Dept, Shantou 515063, Peoples R China.	lkang@stu.edu.cn; pau.riba@helsing.ai; marcal@allread.ai; afornes@cvc.uab.es; mauricio@omnius.com	Riba, Pau/H-9075-2015; Villegas, Mauricio/L-7182-2014	Riba, Pau/0000-0002-4710-0864; Fornes, Alicia/0000-0002-9692-5336; Villegas, Mauricio/0000-0001-7450-6707	Shantou University [140/09421059, RTI2018-095645-B-C21]; Secretaria d'Universitats i Recerca del Departament d'Economia i Coneixement de la Generalitat de Catalunya [2016-DI-087]; Ramon y Cajal Fellowship [RYC-2014-16831]; CERCA Program/Generalitat de Catalunya	Shantou University; Secretaria d'Universitats i Recerca del Departament d'Economia i Coneixement de la Generalitat de Catalunya(Generalitat de Catalunya); Ramon y Cajal Fellowship(Spanish Government); CERCA Program/Generalitat de Catalunya	This work was supported in part by the Shantou University under Grant 140/09421059, in part by the Spanish Project RTI2018-095645-B-C21, in part by the Secretaria d'Universitats i Recerca del Departament d'Economia i Coneixement de la Generalitat de Catalunya under Grant 2016-DI-087, in part by the Ramon y Cajal Fellowship RYC-2014-16831, and in part by the CERCA Program/Generalitat de Catalunya.	Alex Graves, 2014, Arxiv, DOI arXiv:1308.0850; Alonso Eloi, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P481, DOI 10.1109/ICDAR.2019.00083; Augustin E., 2006, INT WORKSHOP FRONTIE, P231; Azadil S, 2018, PROC CVPR IEEE, P7564, DOI 10.1109/CVPR.2018.00789; Caiming Xiong, 2016, Arxiv, DOI arXiv:1609.07843; Chang B, 2018, IEEE WINT CONF APPL, P199, DOI 10.1109/WACV.2018.00028; Charles Elkan, 2015, Arxiv, DOI arXiv:1506.00019; Chih-Yuan Yang, 2020, Arxiv, DOI arXiv:2005.12500; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Davis B., 2020, P BRIT MACH VIS C; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Dong HW, 2018, AAAI CONF ARTIF INTE, P34; DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X; Fogel S, 2020, PROC CVPR IEEE, P4323, DOI 10.1109/CVPR42600.2020.00438; Frinken V., 2014, HDB DOCUMENT IMAGE P, P391, DOI [10.1007/978-0-85729-859-1_12, DOI 10.1007/978-0-85729-859-1_12]; Ganin Y, 2018, PR MACH LEARN RES, V80; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A., 2006, P INT C MACH LEARN I; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Ha D., 2018, PROC INT C LEARN REP; Haines TSF, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2886099; Hensel M, 2017, ADV NEUR IN, V30; Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11; Jiang HC, 2018, LECT NOTES COMPUT SC, V11305, P483, DOI 10.1007/978-3-030-04221-9_43; Kang L., 2020, ARXIV; Kang L, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107790; Kang Lei, 2020, LNCS, P273, DOI DOI 10.1007/978-3-030-58592-117; Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813; Kingma D.P., 2014, P 2 INT C LEARN REPR, DOI DOI 10.1093/BIOINFORMATICS/BTAA169; Konidaris T, 2007, INT J DOC ANAL RECOG, V9, P167, DOI 10.1007/s10032-007-0042-4; Krishnan P, 2019, INT J DOC ANAL RECOG, V22, P387, DOI 10.1007/s10032-019-00336-x; Lin ZC, 2007, PATTERN RECOGN, V40, P2097, DOI 10.1016/j.patcog.2006.11.024; Lyu P., 2017, PROC INT C DOCUMENT; Marti U.-V., 2002, International Journal on Document Analysis and Recognition, V5, P39, DOI 10.1007/s100320200071; Mayr M, 2020, Arxiv, DOI arXiv:2003.10593; Mehdi Mirza, 2014, Arxiv, DOI arXiv:1411.1784; Paszke A, 2017, PROC NEURIPS AUTODIF; Qiangpeng Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14688, DOI 10.1109/CVPR42600.2020.01471; Roy Prasun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13225, DOI 10.1109/CVPR42600.2020.01324; Thomas AO, 2009, PATTERN RECOGN, V42, P3365, DOI 10.1016/j.patcog.2008.12.018; Tian Yuchen, 2017, ZI2ZI MASTER CHINESE; Toselli AH, 2004, INT J PATTERN RECOGN, V18, P519, DOI 10.1142/S0218001404003344; Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165; Vaswani A, 2017, ADV NEUR IN, V30; Wang J., 2005, International Journal on Document Analysis and Recognition, V7, P219, DOI 10.1007/s10032-004-0131-6; Wang P, 2017, IEEE T CIRC SYST VID, V27, P2613, DOI 10.1109/TCSVT.2016.2576761; Wu L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1500, DOI 10.1145/3343031.3350929; Yu LT, 2017, AAAI CONF ARTIF INTE, P2852; Zhang XY, 2018, IEEE T PATTERN ANAL, V40, P849, DOI 10.1109/TPAMI.2017.2695539; Zheng N., 2019, PROC INT C LEARN REP	50	0	0	1	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					8846	8860		10.1109/TPAMI.2021.3122572	http://dx.doi.org/10.1109/TPAMI.2021.3122572			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34699351	Green Submitted			2022-12-18	WOS:000880661400024
J	Kuo, MYJ; Murai, S; Kawahara, R; Nobuhara, S; Nishino, K				Kuo, Meng-Yu Jennifer; Murai, Satoshi; Kawahara, Ryo; Nobuhara, Shohei; Nishino, Ko			Surface Normals and Shape From Water	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Surface reconstruction; Shape; Three-dimensional displays; Absorption; Light sources; Surface waves; Water resources; Computational photography; underwater reconstruction; near-infrared light; absorption	PHOTOMETRIC STEREO	In this paper, we introduce a novel method for reconstructing surface normals and depth of dynamic objects in water. Past shape recovery methods have leveraged various visual cues for estimating shape (e.g., depth) or surface normals. Methods that estimate both compute one from the other. We show that these two geometric surface properties can be simultaneously recovered for each pixel when the object is observed underwater. Our key idea is to leverage multi-wavelength near-infrared light absorption along different underwater light paths in conjunction with surface shading. Our method can handle both Lambertian and non-Lambertian surfaces. We derive a principled theory for this surface normals and shape from water method and a practical calibration method for determining its imaging parameters values. By construction, the method can be implemented as a one-shot imaging system. We prototype both an off-line and a video-rate imaging system and demonstrate the effectiveness of the method on a number of real-world static and dynamic objects. The results show that the method can recover intricate surface features that are otherwise inaccessible.	[Kuo, Meng-Yu Jennifer; Murai, Satoshi; Kawahara, Ryo; Nobuhara, Shohei; Nishino, Ko] Kyoto Univ, Grad Sch Informat, Dept Intelligence Sci & Technol, Kyoto 6068501, Japan	Kyoto University	Kuo, MYJ (corresponding author), Kyoto Univ, Grad Sch Informat, Dept Intelligence Sci & Technol, Kyoto 6068501, Japan.	jennifer@vision.ist.i.kyoto-u.ac.jp; kawahara@vision.ist.i.kyoto-u.ac.jp; ollopa.gnisir.dds.7@gmail.com; nob@i.kyoto-u.ac.jp; kon@i.kyoto-u.ac.jp		Kuo, MengYu Jennifer/0000-0002-6705-7971; Nishino, Ko/0000-0002-3534-3447	JSPS KAKENHI [15H05918, 17K20143, 18K19815, 26240023, 20H05951]; JST [JPMJCR20G7]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); JST(Japan Science & Technology Agency (JST))	This work was in part supported by JSPS KAKENHI under Grants 15H05918, 17K20143, 18K19815, 26240023, and 20H05951, and JST under Grant JPMJCR20G7.	Aliaga D. G, 2008, PROC IEEE C COMPUT V, P1; Asano Y, 2016, LECT NOTES COMPUT SC, V9910, P635, DOI 10.1007/978-3-319-46466-4_38; Bleyer M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.14; Chen XD, 2017, IEEE T IMAGE PROCESS, V26, P4553, DOI 10.1109/TIP.2017.2716194; FRANKOT RT, 1988, IEEE T PATTERN ANAL, V10, P439, DOI 10.1109/34.3909; Fujimura Y, 2018, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2018.00777; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Golub G. H, 1996, MATRIX COMPUTATIONS, V3rd, P256; Haque SM, 2014, PROC CVPR IEEE, P2283, DOI 10.1109/CVPR.2014.292; Hernandez C, 2007, IEEE I CONF COMP VIS, P873; Higo T, 2009, IEEE I CONF COMP VIS, P1234, DOI 10.1109/ICCV.2009.5459331; Hullin MB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360686; Ishihara S, 2020, INT CONF 3D VISION, P32, DOI 10.1109/3DV50981.2020.00013; Jordt-Sedlazeck A, 2013, IEEE I CONF COMP VIS, P57, DOI 10.1109/ICCV.2013.14; Kadambi A, 2015, IEEE I CONF COMP VIS, P3370, DOI 10.1109/ICCV.2015.385; Kuo MYJ, 2021, IEEE T PATTERN ANAL, V43, P2220, DOI 10.1109/TPAMI.2021.3075450; Lim J, 2005, IEEE I CONF COMP VIS, P1635; Morris NJW, 2011, IEEE T PATTERN ANAL, V33, P1518, DOI 10.1109/TPAMI.2011.24; Murez Z, 2015, IEEE I CONF COMP VIS, P3415, DOI 10.1109/ICCV.2015.390; Qian YM, 2018, LECT NOTES COMPUT SC, V11207, P776, DOI 10.1007/978-3-030-01219-9_46; Qian YM, 2017, PROC CVPR IEEE, P6650, DOI 10.1109/CVPR.2017.704; Reinhard E., 2008, COLOR IMAGING FUNDAM; Roubtsova Nadejda, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P251, DOI 10.1109/3DV.2014.59; SHAFER SA, 1985, COLOR RES APPL, V10, P210, DOI 10.1002/col.5080100409; Sun B, 2005, ACM T GRAPHIC, V24, P1040, DOI 10.1145/1073204.1073309; Trifonov B., 2006, PROC EUROGRAPHICS C, P51; Tsiotsios C, 2017, IMAGE VISION COMPUT, V57, P44, DOI 10.1016/j.imavis.2016.10.005; Tsiotsios C, 2014, PROC CVPR IEEE, P2259, DOI 10.1109/CVPR.2014.289; Wu CL, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661232; Yu LF, 2013, PROC CVPR IEEE, P1415, DOI 10.1109/CVPR.2013.186; Zhang Q, 2012, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2012.6247962; Zhou MY, 2020, IEEE T PATTERN ANAL, V42, P1594, DOI 10.1109/TPAMI.2020.2986764; Zickler TE, 2002, INT J COMPUT VISION, V49, P215, DOI 10.1023/A:1020149707513	35	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9150	9162		10.1109/TPAMI.2021.3121963	http://dx.doi.org/10.1109/TPAMI.2021.3121963			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34673484				2022-12-18	WOS:000880661400043
J	Lattas, A; Moschoglou, S; Ploumpis, S; Gecer, B; Ghosh, A; Zafeiriou, S				Lattas, Alexandros; Moschoglou, Stylianos; Ploumpis, Stylianos; Gecer, Baris; Ghosh, Abhijeet; Zafeiriou, Stefanos			AvatarMe(++): Facial Shape and BRDF Inference With Photorealistic Rendering-Aware GANs	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image reconstruction; Rendering (computer graphics); Faces; Three-dimensional displays; Lighting; Shape; Geometry; 3D reconstruction; reflectance; differentiable rendering; face; GAN; 3DMM; computer vision; graphics		Over the last years, with the advent of Generative Adversarial Networks (GANs), many face analysis tasks have accomplished astounding performance, with applications including, but not limited to, face generation and 3D face reconstruction from a single "in-the-wild" image. Nevertheless, to the best of our knowledge, there is no method which can produce render-ready high-resolution 3D faces from "in-the-wild" images and this can be attributed to the: (a) scarcity of available data for training, and (b) lack of robust methodologies that can successfully be applied on very high-resolution data. In this paper, we introduce the first method that is able to reconstruct photorealistic render-ready 3D facial geometry and BRDF from a single "in-the-wild" image. To achieve this, we capture a large dataset of facial shape and reflectance, which we have made public. Moreover, we define a fast and photorealistic differentiable rendering methodology with accurate facial skin diffuse and specular reflection, self-occlusion and subsurface scattering approximation. With this, we train a network that disentangles the facial diffuse and specular reflectance components from a mesh and texture with baked illumination, scanned or reconstructed with a 3DMM fitting method. As we demonstrate in a series of qualitative and quantitative experiments, our method outperforms the existing arts by a significant margin and reconstructs authentic, 4K by 6K-resolution 3D faces from a single low-resolution image, that are ready to be rendered in various applications and bridge the uncanny valley.	[Lattas, Alexandros; Moschoglou, Stylianos; Ploumpis, Stylianos; Gecer, Baris; Ghosh, Abhijeet; Zafeiriou, Stefanos] Imperial Coll London, Dept Comp, London SW7 2AZ, England	Imperial College London	Lattas, A (corresponding author), Imperial Coll London, Dept Comp, London SW7 2AZ, England.	a.lattas@imperial.ac.uk; s.moschoglou@imperial.ac.uk; s.ploumpis@imperial.ac.uk; b.gecer@imperial.ac.uk; ghosh@imperial.ac.uk; s.zafeiriou@imperial.ac.uk		Gecer, Baris/0000-0002-5684-2843	EPSRC Project DEFORM [EP/S010203/1]; Imperial College DTA; EPSRC Early Career Fellowship [EP/N006259/1]; Google Faculty Fellowship; EPSRC Fellowship [DEFORMEP/S010203/1]	EPSRC Project DEFORM(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Imperial College DTA; EPSRC Early Career Fellowship(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Google Faculty Fellowship(Google Incorporated); EPSRC Fellowship(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	AL was supported by EPSRC Project DEFORM EP/S010203/1 and SM by an Imperial College DTA. AG acknowledges funding by the EPSRC Early Career Fellowship EP/N006259/1 and SZ from a Google Faculty Fellowship and the EPSRC Fellowship DEFORMEP/S010203/1.	Alexander O, 2010, IEEE COMPUT GRAPH, V30, P20, DOI 10.1109/MCG.2010.65; Amberg B, 2007, IEEE I CONF COMP VIS, P1326; Artem Rozantsev, 2019, Arxiv, DOI arXiv:1911.05063; Asselin LP, 2020, INT CONF 3D VISION, P1157, DOI 10.1109/3DV50981.2020.00126; Beeler T, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185613; Berretti S, 2012, LECT NOTES COMPUT SC, V7583, P73, DOI 10.1007/978-3-642-33863-2_8; Bi S, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459829; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; BLINN JF, 1976, COMMUN ACM, V19, P542, DOI [10.1145/360349.360353, 10.1145/965143.563322]; Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598; Borshukov George, 2005, ACM SIGGRAPH 2005 CO, P13; Chen AP, 2019, IEEE I CONF COMP VIS, P9428, DOI 10.1109/ICCV.2019.00952; Chen WZ, 2019, ADV NEUR IN, V32; Clevert D. A., 2015, ARXIV, DOI DOI 10.48550/ARXIV.1511.07289; Cole F, 2017, PROC CVPR IEEE, P3386, DOI 10.1109/CVPR.2017.361; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; David Novotny, 2020, Arxiv, DOI arXiv:2007.08501; Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855; Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482; Deschaintre V, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201378; Dib A, 2021, COMPUT GRAPH FORUM, V40, P153, DOI 10.1111/cgf.142622; Donner C., 2006, RENDERING TECHN, V2006, P409, DOI DOI 10.2312/EGWR/EGSR06/409-417; Egger B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3395208; Gecer Baris, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P415, DOI 10.1007/978-3-030-58526-6_25; Gecer B, 2021, PROC CVPR IEEE, P7624, DOI 10.1109/CVPR46437.2021.00754; Gecer B, 2019, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2019.00125; Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874; Ghosh A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024163; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gotardo P, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275073; Guo YD, 2019, IEEE T PATTERN ANAL, V41, P1294, DOI 10.1109/TPAMI.2018.2837742; Hable J., 2010, PROC SIGGRAPH ADV RE; Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579; Huang Gary B, 2008, P WORKSH FAC REAL LI; Huynh L, 2018, PROC CVPR IEEE, P8407, DOI 10.1109/CVPR.2018.00877; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jaakko Lehtinen, 2018, Arxiv, DOI arXiv:1710.10196; Kampouris C., 2018, PROC EUROGRAPH WORKS, P1; Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411; Lattas A., 2019, PROC ACM SIGGRAPH 20, P1; Lattas A, 2020, PROC CVPR IEEE, P757, DOI 10.1109/CVPR42600.2020.00084; Li RL, 2020, PROC CVPR IEEE, P3407, DOI 10.1109/CVPR42600.2020.00347; Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813; Li TM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275109; Li X, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073641; Li ZQ, 2018, LECT NOTES COMPUT SC, V11207, P74, DOI 10.1007/978-3-030-01219-9_5; Liu SC, 2019, IEEE I CONF COMP VIS, P7707, DOI 10.1109/ICCV.2019.00780; Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11; Loubet G, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356510; Luo HW, 2021, PROC CVPR IEEE, P11657, DOI 10.1109/CVPR46437.2021.01149; Ma W.-C., 2007, PROC 18 EUR C RENDER, P183, DOI 10.2312/EGWR/EGSR07/183-194; Mallikarjun B R, 2021, PROC IEEECVF C COMPU, P4791; Nehab D, 2005, ACM T GRAPHIC, V24, P536, DOI 10.1145/1073204.1073226; Nishino K, 2004, ACM T GRAPHIC, V23, P704, DOI 10.1145/1015706.1015783; Patow G, 2003, COMPUT GRAPH FORUM, V22, P663, DOI 10.1111/j.1467-8659.2003.00716.x; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Ploumpis S, 2021, IEEE T PATTERN ANAL, V43, P4142, DOI 10.1109/TPAMI.2020.2991150; Ploumpis S, 2019, PROC CVPR IEEE, P10926, DOI 10.1109/CVPR.2019.01119; Richardson E, 2016, INT CONF 3D VISION, P460, DOI 10.1109/3DV.2016.56; Sagonas C, 2013, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2013.132; Saito S, 2017, PROC CVPR IEEE, P2326, DOI 10.1109/CVPR.2017.250; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Sela M, 2017, IEEE I CONF COMP VIS, P1585, DOI 10.1109/ICCV.2017.175; Sengupta S, 2018, PROC CVPR IEEE, P6296, DOI 10.1109/CVPR.2018.00659; Shu ZX, 2017, PROC CVPR IEEE, P5444, DOI 10.1109/CVPR.2017.578; Smith WAP, 2020, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR42600.2020.00506; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tewari A, 2019, PROC CVPR IEEE, P10804, DOI 10.1109/CVPR.2019.01107; Tewari A, 2018, PROC CVPR IEEE, P2549, DOI 10.1109/CVPR.2018.00270; Tewari A, 2017, IEEE I CONF COMP VIS, P3735, DOI 10.1109/ICCV.2017.401; Tran AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163; Tran L, 2021, IEEE T PATTERN ANAL, V43, P157, DOI 10.1109/TPAMI.2019.2927975; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Weyrich T, 2006, ACM T GRAPHIC, V25, P1013, DOI 10.1145/1141911.1141987; Yamaguchi S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201364; Yang HT, 2020, PROC CVPR IEEE, P598, DOI 10.1109/CVPR42600.2020.00068; Yu Y, 2019, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2019.00327; Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI [10.1007/978-3-030-01234-2_18, 10.1007/978-3-030-01240-3_22]; Zhou YX, 2019, PROC CVPR IEEE, P1097, DOI 10.1109/CVPR.2019.00119; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu WB, 2020, PROC CVPR IEEE, P4957, DOI 10.1109/CVPR42600.2020.00501	82	0	0	3	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9269	9284		10.1109/TPAMI.2021.3125598	http://dx.doi.org/10.1109/TPAMI.2021.3125598			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34748477	Green Submitted			2022-12-18	WOS:000880661400052
J	Le, H; Samaras, D				Le, Hieu; Samaras, Dimitris			Physics-Based Shadow Image Decomposition for Shadow Removal	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Lighting; Deep learning; Training; Solid modeling; Image decomposition; Image color analysis; Computational modeling; Shadow removal; physical illumination model; matting; deep neural network	ILLUMINATION	We propose a novel deep learning method for shadow removal. Inspired by physical models of shadow formation, we use a linear illumination transformation to model the shadow effects in the image that allows the shadow image to be expressed as a combination of the shadow-free image, the shadow parameters, and a matte layer. We use two deep networks, namely SP-Net and M-Net, to predict the shadow parameters and the shadow matte respectively. This system allows us to remove the shadow effects from images. We then employ an inpainting network, I-Net, to further refine the results. We train and test our framework on the most challenging shadow removal dataset (ISTD). Our method improves the state-of-the-art in terms of mean absolute error (MAE) for the shadow area by 20%. Furthermore, this decomposition allows us to formulate a patch-based weakly-supervised shadow removal method. This model can be trained without any shadow- free images (that are cumbersome to acquire) and achieves competitive shadow removal results compared to state-of-the-art methods that are trained with fully paired shadow and shadow-free images. Last, we introduce SBU-Timelapse, a video shadow removal dataset for evaluating shadow removal methods.	[Le, Hieu] Amazon Robot, North Reading, MA 01864 USA; [Samaras, Dimitris] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA	State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook	Le, H (corresponding author), Amazon Robot, North Reading, MA 01864 USA.	hle@cs.stonybrook.edu; samaras@cs.stonybrook.edu		Samaras, Dimitris/0000-0002-1373-0294	NSF EarthCube program Award [1740595]; National Neographic/Microsoft AI for Earth program; Partner University Fund; SUNY2020 Infrastructure Transportation Security Center	NSF EarthCube program Award; National Neographic/Microsoft AI for Earth program(Microsoft); Partner University Fund; SUNY2020 Infrastructure Transportation Security Center	This work was supported in part by the NSF EarthCube program Award under Grant 1740595, in part by the National Neographic/Microsoft AI for Earth program, in part by the Partner University Fund, in part by the SUNY2020 Infrastructure Transportation Security Center, and in part by a gift from Adobe. Computational support provided by the Institute for Advanced Computational Science and a GPU donation from NVIDIA.	Barrow H. G., 1978, COMPUTER VISION SYST; Chuang YY, 2003, ACM T GRAPHIC, V22, P494, DOI 10.1145/882262.882298; Cook R. D., 1986, STAT SCI, V1, P393; Cun XD, 2020, AAAI CONF ARTIF INTE, V34, P10680; Das S, 2019, IEEE I CONF COMP VIS, P131, DOI 10.1109/ICCV.2019.00022; Das S, 2020, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS45731.2020.9180403; Drew M. S., 2003, P INT C COMP VIS WOR, P32; Finlayson GD, 2006, IEEE T PATTERN ANAL, V28, P59, DOI 10.1109/TPAMI.2006.18; Finlayson GD, 2002, LECT NOTES COMPUT SC, V2353, P823; Finlayson GD, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P473, DOI 10.1109/ICCV.2001.937663; Finlayson G, 2016, J OPT SOC AM A, V33, P589, DOI 10.1364/JOSAA.33.000589; Finlayson GD, 2009, INT J COMPUT VISION, V85, P35, DOI 10.1007/s11263-009-0243-z; Gong H, 2016, J OPT SOC AM A, V33, P1798, DOI 10.1364/JOSAA.33.001798; Guo RQ, 2013, IEEE T PATTERN ANAL, V35, P2956, DOI 10.1109/TPAMI.2012.214; Hieu Le, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P264, DOI 10.1007/978-3-030-58621-8_16; Hieu Le, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11206), P680, DOI 10.1007/978-3-030-01216-8_41; Hu S., 2021, ARXIV; Hu XW, 2019, IEEE I CONF COMP VIS, P2472, DOI 10.1109/ICCV.2019.00256; Hu XW, 2020, IEEE T PATTERN ANAL, V42, P2795, DOI 10.1109/TPAMI.2019.2919616; Huang X, 2011, IEEE I CONF COMP VIS, P898, DOI 10.1109/ICCV.2011.6126331; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; KaewTraKulPong P, 2002, VIDEO-BASED SURVEILLANCE SYSTEMS: COMPUTER VISION AND DISTRIBUTED PROCESSING, P135; Karras T., 2017, PROGR GROWING GANS I; Kingma D.P, P 3 INT C LEARNING R; Lalonde JF, 2010, LECT NOTES COMPUT SC, V6312, P322, DOI 10.1007/978-3-642-15552-9_24; Le H, 2019, IEEE I CONF COMP VIS, P8577, DOI 10.1109/ICCV.2019.00867; Le HE, 2017, IEEE INT CONF COMP V, P1103, DOI 10.1109/ICCVW.2017.134; Le H, 2017, LECT NOTES COMPUT SC, V10111, P275, DOI 10.1007/978-3-319-54181-5_18; Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177; Liu Daquan, 2020, CVPR; M Le H., 2019, P IEEECVF C COMPUTER, P18; Mullett TL, 2020, PERS SOC PSYCHOL B, V46, P79, DOI 10.1177/0146167219843918; Odena A, 2016, DISTILL, DOI [10.23915/distill.00003.-URL, 10.23915/distill.00003]; Okabe T, 2009, IEEE I CONF COMP VIS, P1693, DOI 10.1109/ICCV.2009.5459381; Panagopoulos A, 2013, IEEE T PATTERN ANAL, V35, P437, DOI 10.1109/TPAMI.2012.110; Panagopoulos A, 2009, PROC CVPR IEEE, P651, DOI 10.1109/CVPRW.2009.5206665; Qu LQ, 2017, PROC CVPR IEEE, P2308, DOI 10.1109/CVPR.2017.248; Shor Y, 2008, COMPUT GRAPH FORUM, V27, P577, DOI 10.1111/j.1467-8659.2008.01155.x; Smith A. R., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P259, DOI 10.1145/237170.237263; Su N, 2016, IEEE J-STARS, V9, P2568, DOI 10.1109/JSTARS.2016.2570234; Sun W, 2006, SPRINGER SER OPTIM A, V1, P1, DOI 10.1007/b106451; Sunkavalli K, 2010, LECT NOTES COMPUT SC, V6312, P251, DOI 10.1007/978-3-642-15552-9_19; Surkutlawar S, 2013, INT J ADV COMPUT SC, V4, P164; Vicente TFY, 2015, LECT NOTES COMPUT SC, V8927, P309, DOI 10.1007/978-3-319-16199-0_22; Vicente TFY, 2018, IEEE T PATTERN ANAL, V40, P682, DOI 10.1109/TPAMI.2017.2691703; Vincente TFY, 2016, LECT NOTES COMPUT SC, V9910, P816, DOI 10.1007/978-3-319-46466-4_49; Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wright S., 2001, DIGITAL COMPOSITING; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xu Jingyi, 2021, P IEEE C COMPUTER VI, P8812; Yang QX, 2012, IEEE T IMAGE PROCESS, V21, P4361, DOI 10.1109/TIP.2012.2208976; Yifan Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P438, DOI 10.1007/978-3-030-58607-2_26; Zhang L, 2015, IEEE T IMAGE PROCESS, V24, P4623, DOI 10.1109/TIP.2015.2465159; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhang WM, 2019, IEEE T PATTERN ANAL, V41, P611, DOI 10.1109/TPAMI.2018.2803179; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu L, 2018, LECT NOTES COMPUT SC, V11210, P122, DOI 10.1007/978-3-030-01231-1_8; Zhu Z, 2012, REMOTE SENS ENVIRON, V118, P83, DOI 10.1016/j.rse.2011.10.028	60	0	0	5	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9088	9101		10.1109/TPAMI.2021.3124934	http://dx.doi.org/10.1109/TPAMI.2021.3124934			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34735336	Green Submitted			2022-12-18	WOS:000880661400039
J	Li, CX; Xu, K; Zhu, J; Liu, JS; Zhang, B				Li, Chongxuan; Xu, Kun; Zhu, Jun; Liu, Jiashuo; Zhang, Bo			Triple Generative Adversarial Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Generators; Generative adversarial networks; Task analysis; Semisupervised learning; Games; Entropy; Linear programming; Generative adversarial network; deep generative model; semi-supervised learning; extremely low data regime; conditional image generation		We propose a unified game-theoretical framework to perform classification and conditional image generation given limited supervision. It is formulated as a three-player minimax game consisting of a generator, a classifier and a discriminator, and therefore is referred to as Triple Generative Adversarial Network (Triple-GAN). The generator and the classifier characterize the conditional distributions between images and labels to perform conditional generation and classification, respectively. The discriminator solely focuses on identifying fake image-label pairs. Theoretically, the three-player formulation guarantees consistency. Namely, under a nonparametric assumption, the unique equilibrium of the game is that the distributions characterized by the generator and the classifier converge to the data distribution. As a byproduct of the three-player formulation, Triple-GAN is flexible to incorporate different semi-supervised classifiers and GAN architectures. We evaluate Triple-GAN in two challenging settings, namely, semi-supervised learning and the extremely low data regime. In both settings, Triple-GAN can achieve excellent classification results and generate meaningful samples in a specific class simultaneously. In particular, using a commonly adopted 13-layer CNN classifier, Triple-GAN outperforms extensive semi-supervised learning methods substantially on several benchmarks no matter data augmentation is applied or not.	[Li, Chongxuan; Xu, Kun; Zhu, Jun; Liu, Jiashuo; Zhang, Bo] Tsinghua Univ, Ctr Bioinspired Comp Res, TNList Lab, Dept Comp Sci & Technol,Inst AI, Beijing 100084, Peoples R China; [Li, Chongxuan] Renmin Univ China, Gaoling Sch AI, Beijing 100872, Peoples R China	Tsinghua University; Renmin University of China	Zhu, J (corresponding author), Tsinghua Univ, Ctr Bioinspired Comp Res, TNList Lab, Dept Comp Sci & Technol,Inst AI, Beijing 100084, Peoples R China.	chongxuanli1991@gmail.com; kunxu.thu@gmail.com; dcszj@tsinghua.edu.cn; liu-js16@mails.tsinghua.edu.cn; dcszb@tsinghua.edu.cn		liu, jiashuo/0000-0002-9159-1752	NSFC [61620106010, 62061136001, 61621136008, 62076145, U1811461, U19B2034, U19A2081]; Beijing NSF Project [JQ19016]; Tsinghua-Bosch Joint Center for Machine Learning, Beijing Academy of Artificial Intelligence (BAAI); Tsinghua Institute for Guo Qiang; Tiangong Institute for Intelligent Computing; NVIDIA NVAIL Program; GPU/DGX Acceleration	NSFC(National Natural Science Foundation of China (NSFC)); Beijing NSF Project; Tsinghua-Bosch Joint Center for Machine Learning, Beijing Academy of Artificial Intelligence (BAAI); Tsinghua Institute for Guo Qiang; Tiangong Institute for Intelligent Computing; NVIDIA NVAIL Program; GPU/DGX Acceleration	This work was supported by NSFC Projects under Grants 61620106010, 62061136001, 61621136008, 62076145, U1811461, U19B2034, and U19A2081, Beijing NSF Project JQ19016, Tsinghua-Bosch Joint Center for Machine Learning, Beijing Academy of Artificial Intelligence (BAAI), a grant from Tsinghua Institute for Guo Qiang, Tiangong Institute for Intelligent Computing, and the NVIDIA NVAIL Program with GPU/DGX Acceleration.	Al-Rfou R., 2016, ARXIV; Andrew Gordon Wilson, 2019, Arxiv, DOI arXiv:1806.05594; Arjovsky M, 2017, PR MACH LEARN RES, V70; Augustus Odena, 2016, Arxiv, DOI arXiv:1606.01583; Berthelot D, 2019, PROC INT C LEARN REP; Berthelot D, 2019, ADV NEUR IN, V32; Brock A., 2018, PROC INT C LEARN REP; Chen X, 2016, ADV NEUR IN, V29; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Dai Z., 2017, P ADV NEURAL INFORM, P6510; Denton Emily L, 2015, NEURIPS, V2, P4; Donahue J., 2016, ARXIV160509782; Donahue J, 2019, ADV NEUR IN, V32; Dong Liu, 2018, Arxiv, DOI arXiv:1809.00981; Dumoulin Vincent, 2017, ICLR 2017, P4; Dziugaite GK, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P258; Gan Z, 2017, ADV NEUR IN, V30; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grandvalet Yves, 2004, NIPS, P529; Gregor K, 2014, PR MACH LEARN RES, V32, P1242; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hensel M, 2017, ADV NEUR IN, V30; Iscen A, 2019, PROC CVPR IEEE, P5065, DOI 10.1109/CVPR.2019.00521; Gulrajani I, 2017, ADV NEUR IN, V30; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jost Tobias Springenberg, 2016, Arxiv, DOI arXiv:1511.06390; Karras T., 2020, PROC ANN C NEURAL IN; Karras T., 2017, PROGR GROWING GANS I; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kingma D.P., 2013, P 2 INT C LEARN REPR; Kingma DP, 2014, ADV NEUR IN, V27; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; Laine S., 2016, INT C LEARN REPR; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lee D., 2013, INT C MACH LEARN ICM; Li C., 2018, PROC 32 INT C NEURAL, P6072; Li CX, 2018, IEEE T PATTERN ANAL, V40, P2762, DOI 10.1109/TPAMI.2017.2766142; Li CX, 2015, ADV NEUR IN, V28; Li Chunyuan, 2017, NIPS; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Locatello F, 2019, PR MACH LEARN RES, V97; Luo YC, 2018, PROC CVPR IEEE, P8896, DOI 10.1109/CVPR.2018.00927; Maaloe L, 2016, PR MACH LEARN RES, V48; Mehdi Mirza, 2014, Arxiv, DOI arXiv:1411.1784; Miyato T., 2018, INT C LEARN REPR, P2; Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821; Miyato Takeru, 2018, ARXIV180205637; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Odena A, 2017, PR MACH LEARN RES, V70; Oliver A, 2018, ADV NEUR IN, V31; Paszke A, 2019, ADV NEUR IN, V32; Pu YC, 2018, PR MACH LEARN RES, V80; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rasmus A, 2015, ADV NEUR IN, V28; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Salimans T, 2016, ADV NEUR IN, V29; Simonyan K., 2015, VERY DEEP CONVOLUTIO; Sohn K, 2020, PROC INT C NEURAL IN; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tarvainen A, 2017, ADV NEUR IN, V30; Theis Lucas, 2016, ICLR; van den Oord A, 2016, ADV NEUR IN, V29; Verma V, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3635; Yang Jimei, 2015, NIPS; Zhang H., 2017, PROC INT C LEARN REP; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	70	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9629	9640		10.1109/TPAMI.2021.3127558	http://dx.doi.org/10.1109/TPAMI.2021.3127558			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34767502	Green Submitted			2022-12-18	WOS:000880661400076
J	Li, HF; Xu, C; Ma, L; Bo, HJ; Zhang, D				Li, Haifeng; Xu, Cong; Ma, Lin; Bo, Hongjian; Zhang, David			MODENN: A Shallow Broad Neural Network Model Based on Multi-Order Descartes Expansion	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Neurons; Biological neural networks; Task analysis; Brain modeling; Parallel processing; Information processing; Training; Multi-order Descartes expansion; single-layer perceptron; shallow broad neural network; data pre-processing; feature extraction; parallel computing	ARCHITECTURE; DEEP	Deep neural networks have achieved great success in almost every field of artificial intelligence. However, several weaknesses keep bothering researchers due to its hierarchical structure, particularly when large-scale parallelism, faster learning, better performance, and high reliability are required. Inspired by the parallel and large-scale information processing structures in the human brain, a shallow broad neural network model is proposed on a specially designed multi-order Descartes expansion operation. Such Descartes expansion acts as an efficient feature extraction method for the network, improve the separability of the original pattern by transforming the raw data pattern into a high-dimensional feature space, the multi-order Descartes expansion space. As a result, a single-layer perceptron network will be able to accomplish the classification task. The multi-order Descartes expansion neural network (MODENN) is thus created by combining the multi-order Descartes expansion operation and the single-layer perceptron together, and its capacity is proved equivalent to the traditional multi-layer perceptron and the deep neural networks. Three kinds of experiments were implemented, the results showed that the proposed MODENN model retains great potentiality in many aspects, including implementability, parallelizability, performance, robustness, and interpretability, indicating MODENN would be an excellent alternative to mainstream neural networks.	[Li, Haifeng; Xu, Cong; Ma, Lin; Bo, Hongjian] Harbin Inst Technol, Fac Comp, Harbin 150001, Heilongjiang, Peoples R China; [Zhang, David] Chinese Univ Hong Kong Shenzhen, Sch Data Sci, Shenzhen 518172, Guangdong, Peoples R China	Harbin Institute of Technology; Chinese University of Hong Kong, Shenzhen	Li, HF (corresponding author), Harbin Inst Technol, Fac Comp, Harbin 150001, Heilongjiang, Peoples R China.	lihaifeng@hit.edu.cn; congxu@hit.edu.cn; malin_li@hit.edu.cn; bohongjian@gmail.com; csdzhang@comp.polyu.edu.hk			National Key R&D Program of China [2020YFC0833204]; Provincial Key R&D Program of Heilongjiang [GY2021ZB0206]; National Natural Science Foundation of China [U20A20383]; Shenzhen Foundational Research Funding [JCYJ20180507183608379, JCYJ20200109150814370]; Basic and Applied Basic Research of Guangdong [2019A1515111179, 2021A1515011903]; Open Funding of State Key Laboratory of Robotics and System, Heilongjiang Touyan Team	National Key R&D Program of China; Provincial Key R&D Program of Heilongjiang; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shenzhen Foundational Research Funding; Basic and Applied Basic Research of Guangdong; Open Funding of State Key Laboratory of Robotics and System, Heilongjiang Touyan Team	This work was supported in part by the National Key R&D Program of China under Grant 2020YFC0833204 in part by the Provincial Key R&D Program of Heilongjiang under Grant GY2021ZB0206, in part by the National Natural Science Foundation of China under Grant U20A20383, in part by the Shenzhen Foundational Research Funding under Grants JCYJ20180507183608379 and JCYJ20200109150814370, in part by the Basic and Applied Basic Research of Guangdong under Grants 2019A1515111179 and 2021A1515011903), and in part by the Open Funding of State Key Laboratory of Robotics and System, Heilongjiang Touyan Team.	Ba LJ, 2014, ADV NEUR IN, V27; Bear M. F, 2016, NEUROSCIENCE EXPLORI; Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052; COVER TM, 1965, IEEE TRANS ELECTRON, VEC14, P326, DOI 10.1109/PGEC.1965.264137; Ganguli S., 2014, INT C LEARN REPR; George D, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000532; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kubilius J, 2019, ADV NEUR IN, V32; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Li H., 2017, INT C LEARN REPR; Luo L., 2020, PRINCIPLES NEUROBIOL, DOI DOI 10.1201/9781003053972; McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02459570; McDonnell MD, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0134254; Minsky M., 1969, PERCEPTRONS INTRO CO, DOI 10.7551/mitpress/11301.001.0001; Nikos Komodakis, 2017, Arxiv, DOI arXiv:1605.07146; Potter MC, 2014, ATTEN PERCEPT PSYCHO, V76, P270, DOI 10.3758/s13414-013-0605-z; POTTER MC, 1976, J EXP PSYCHOL-HUM L, V2, P509, DOI 10.1037/0278-7393.2.5.509; Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819; Rivest F., 2004, P ADV NEUR INF PROC, V17, P1129; ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519; Ruan F. J. G. J., 2002, METHOD APPL NEURAL D; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Serre T, 2007, P NATL ACAD SCI USA, V104, P6424, DOI 10.1073/pnas.0700622104; Simard PY, 2003, PROC INT CONF DOC, P958; Simonyan K., 2015, VERY DEEP CONVOLUTIO; Stone MH, 1937, T AM MATH SOC, V41, P375, DOI 10.2307/1989788; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Yang XF, 2018, IEEE T GEOSCI REMOTE, V56, P5408, DOI 10.1109/TGRS.2018.2815613	35	0	0	3	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9417	9433		10.1109/TPAMI.2021.3125690	http://dx.doi.org/10.1109/TPAMI.2021.3125690			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34748480				2022-12-18	WOS:000880661400061
J	Li, YL; Xu, Y; Xu, XY; Mao, XH; Lu, CW				Li, Yong-Lu; Xu, Yue; Xu, Xinyu; Mao, Xiaohan; Lu, Cewu			Learning Single/Multi-Attribute of Object With Symmetry and Group	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Correlation; Couplings; Visualization; Task analysis; Feature extraction; Deep learning; Computational modeling; Attribute-object composition; compositional zero-shot learning; single; multi-attribute; symmetry; group axioms		Attributes and objects can compose diverse compositions. To model the compositional nature of these concepts, it is a good choice to learn them as transformations, e.g., coupling and decoupling. However, complex transformations need to satisfy specific principles to guarantee rationality. Here, we first propose a previously ignored principle of attribute-object transformation: Symmetry. For example, coupling peeled-apple with attribute peeled should result in peeled-apple, and decoupling peeled from apple should still output apple. Incorporating the symmetry, we propose a transformation framework inspired by group theory, i.e., SymNet. It consists of two modules: Coupling Network and Decoupling Network. We adopt deep neural networks to implement SymNet and train it in an end-to-end paradigm with the group axioms and symmetry as objectives. Then, we propose a Relative Moving Distance (RMD) based method to utilize the attribute change instead of the attribute pattern itself to classify attributes. Besides the compositions of single-attribute and object, our RMD is also suitable for complex compositions of multiple attributes and objects when incorporating attribute correlations. SymNet can be utilized for attribute learning, compositional zero-shot learning and outperforms the state-of-the-art on four widely-used benchmarks. Code is at https://github.com/DirtyHarryLYL/SymNet.	[Li, Yong-Lu; Xu, Yue; Xu, Xinyu; Mao, Xiaohan] Shanghai Jiao Tong Univ, Dept Elect & Comp Engn, Shanghai 200240, Peoples R China; [Lu, Cewu] Shanghai Jiao Tong Univ, MoE Key Lab Artificial Intelligence, AI Inst, Shanghai Qi Zhi Inst, Shanghai 200240, Peoples R China	Shanghai Jiao Tong University; Shanghai Jiao Tong University	Lu, CW (corresponding author), Shanghai Jiao Tong Univ, MoE Key Lab Artificial Intelligence, AI Inst, Shanghai Qi Zhi Inst, Shanghai 200240, Peoples R China.	yonglu_li@sjtu.edu.cn; silicxuyue@sjtu.edu.cn; xuxinyu2000@sjtu.edu.cn; mxh1999@sjtu.edu.cn; lucewu@sjtu.edu.cn			National Key R&D Program of China [2017YFA0700800]; National Natural Science Foundation of China [61772332, 2018-RGZN-02046]; Baidu Scholorship	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Baidu Scholorship	This work was supported in part by the National Key R&D Program of China under Grant 2017YFA0700800, in part by the National Natural Science Foundation of China under Grant 61772332, in part by the Shanghai Qi Zhi InstituteSHEITC (2018-RGZN-02046), and in part by the Baidu Scholorship. (Corresponding author: Cewu Lu.)	Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; Amir Roshan Zamir, 2012, Arxiv, DOI arXiv:1212.0402; Bourdev L, 2011, IEEE I CONF COMP VIS, P1543, DOI 10.1109/ICCV.2011.6126413; Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4; Chao YW, 2018, IEEE WINT CONF APPL, P381, DOI 10.1109/WACV.2018.00048; Chen CY, 2014, PROC CVPR IEEE, P200, DOI 10.1109/CVPR.2014.33; Cheng ZQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P90, DOI 10.1145/3240508.3240518; Choi SW, 2014, LECT NOTES COMPUT SC, V8695, P361, DOI 10.1007/978-3-319-10584-0_24; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Deng YB, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P789, DOI 10.1145/2647868.2654966; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Felix Kreuk, 2020, Arxiv, DOI arXiv:2006.14610; Gan C, 2016, PROC CVPR IEEE, P87, DOI 10.1109/CVPR.2016.17; Greg Corrado, 2013, Arxiv, DOI arXiv:1301.3781; Hand EM, 2017, AAAI CONF ARTIF INTE, P4068; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hwang SJ, 2011, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2011.5995543; Isola P, 2015, PROC CVPR IEEE, P1383, DOI 10.1109/CVPR.2015.7298744; Jiarou Fan, 2020, Arxiv, DOI arXiv:2005.11576; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Li YK, 2017, IEEE I CONF COMP VIS, P1270, DOI 10.1109/ICCV.2017.142; Li YL, 2020, PROC CVPR IEEE, P379, DOI 10.1109/CVPR42600.2020.00046; Li YL, 2019, PROC CVPR IEEE, P3580, DOI 10.1109/CVPR.2019.00370; Li YZ, 2020, NEUROCOMPUTING, V415, P96, DOI 10.1016/j.neucom.2020.07.054; Liang KM, 2019, IEEE T PATTERN ANAL, V41, P1747, DOI 10.1109/TPAMI.2018.2836461; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lu YX, 2017, PROC CVPR IEEE, P1131, DOI 10.1109/CVPR.2017.126; Mahajan D, 2011, IEEE I CONF COMP VIS, P1227, DOI 10.1109/ICCV.2011.6126373; Misra I, 2017, PROC CVPR IEEE, P1160, DOI 10.1109/CVPR.2017.129; Naeem MF, 2021, Arxiv, DOI arXiv:2102.01987; Nagarajan T, 2018, LECT NOTES COMPUT SC, V11205, P172, DOI 10.1007/978-3-030-01246-5_11; Nan ZX, 2019, AAAI CONF ARTIF INTE, P8811; Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281; Patterson G, 2016, LECT NOTES COMPUT SC, V9910, P85, DOI 10.1007/978-3-319-46466-4_6; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Purushwalkam S, 2019, IEEE I CONF COMP VIS, P3592, DOI 10.1109/ICCV.2019.00369; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Tang C., 2019, ARXIV; Tokmakov P, 2019, IEEE I CONF COMP VIS, P6381, DOI 10.1109/ICCV.2019.00647; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang X, 2019, PROC CVPR IEEE, P1831, DOI 10.1109/CVPR.2019.00193; Welinder P., 2010, CALIFORNIA I TECHNOL; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Yu A, 2017, IEEE I CONF COMP VIS, P5571, DOI 10.1109/ICCV.2017.594	46	0	0	4	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9043	9055		10.1109/TPAMI.2021.3119406	http://dx.doi.org/10.1109/TPAMI.2021.3119406			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34637379	Green Submitted			2022-12-18	WOS:000880661400036
J	Liao, YB; Zhu, HY; Zhang, YG; Ye, CG; Chen, T; Fan, JY				Liao, Yongbin; Zhu, Hongyuan; Zhang, Yanggang; Ye, Chuangguan; Chen, Tao; Fan, Jiayuan			Point Cloud Instance Segmentation With Semi-Supervised Bounding-Box Mining	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Point cloud compression; Semantics; Proposals; Perturbation methods; Three-dimensional displays; Task analysis; Annotations; Semi-supervised learning; weakly-supervised learning; point cloud instance segmentation		Point cloud instance segmentation has achieved huge progress with the emergence of deep learning. However, these methods are usually data-hungry with expensive and time-consuming dense point cloud annotations. To alleviate the annotation cost, unlabeled or weakly labeled data is still less explored in the task. In this paper, we introduce the first semi-supervised point cloud instance segmentation framework (SPIB) using both labeled and unlabelled bounding boxes as supervision. To be specific, our SPIB architecture involves a two-stage learning procedure. For stage one, a bounding box proposal generation network is trained under a semi-supervised setting with perturbation consistency regularization (SPCR). The regularization works by enforcing an invariance of the bounding box predictions over different perturbations applied to the input point clouds, to provide self-supervision for network learning. For stage two, the bounding box proposals with SPCR are grouped into some subsets, and the instance masks are mined inside each subset with a novel semantic propagation module and a property consistency graph module. Moreover, we introduce a novel occupancy ratio guided refinement module to refine the instance masks. Extensive experiments on the challenging ScanNet v2 dataset demonstrate our method can achieve competitive performance compared with the recent fully-supervised methods.	[Liao, Yongbin; Zhang, Yanggang; Ye, Chuangguan; Chen, Tao] Fudan Univ, Sch Informat Sci & Technol, Embedded Deep Learning & Visual Anal Grp, Shanghai 200433, Peoples R China; [Zhu, Hongyuan] Agcy Sci Technol & Res, Singapore 138632, Singapore; [Fan, Jiayuan] Fudan Univ, Acad Engn & Technol, Shanghai 200433, Peoples R China	Fudan University; Agency for Science Technology & Research (A*STAR); Fudan University	Chen, T (corresponding author), Fudan Univ, Sch Informat Sci & Technol, Embedded Deep Learning & Visual Anal Grp, Shanghai 200433, Peoples R China.	ybliao19@fudan.edu.cn; hongyuanzhu.cn@gmail.com; ygzhang19@fudan.edu.cn; cgye19@fudan.edu.cn; eetchen@fudan.edu.cn; jyfan@fudan.edu.cn		Chen, Tao/0000-0002-0779-9818	National Natural Science Foundation of China [62071127, U1909207]; Shanghai Pujiang Program [19PJ1402000]; Shanghai Municipal Science and Technology Major Project [2021SHZDZX0103]; Shanghai Engineering Research Center of AI Robotics and Engineering Research Center of AI Robotics, Ministry of Education in China; Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme [A18A2b0046]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shanghai Pujiang Program(Shanghai Pujiang Program); Shanghai Municipal Science and Technology Major Project; Shanghai Engineering Research Center of AI Robotics and Engineering Research Center of AI Robotics, Ministry of Education in China; Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme(Agency for Science Technology & Research (A*STAR))	This work was supported in part by the National Natural Science Foundation of China under Grants 62071127 and U1909207, in part by the Shanghai Pujiang Program under Grant 19PJ1402000, in part by the Shanghai Municipal Science and Technology Major Project under Grant 2021SHZDZX0103, in part by the Shanghai Engineering Research Center of AI Robotics and Engineering Research Center of AI Robotics, Ministry of Education in China, and in part by the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme under Grant A18A2b0046.	Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231; Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523; Tao A, 2020, Arxiv, DOI arXiv:2012.10217; Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34; Chen Liu, 2019, Arxiv, DOI arXiv:1902.04478; Chen Shaoyu, 2021, P IEEECVF INT C COMP, P15467; Cheng MM, 2021, AAAI CONF ARTIF INTE, V35, P1140; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191; Durand T, 2017, PROC CVPR IEEE, P5957, DOI 10.1109/CVPR.2017.631; Elich C, 2019, LECT NOTES COMPUT SC, V11824, P48, DOI 10.1007/978-3-030-33676-9_4; Engelmann Francis, 2020, P IEEE CVF C COMP VI, P9031, DOI DOI 10.1109/CVPR42600.2020.00905; Han L, 2020, PROC CVPR IEEE, P2937, DOI 10.1109/CVPR42600.2020.00301; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hong S, 2017, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2017.239; Hou J, 2021, PROC CVPR IEEE, P15582, DOI 10.1109/CVPR46437.2021.01533; Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455; Hsu CC, 2019, ADV NEUR IN, V32; Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733; Hwann-Tzong Chen, 2020, Arxiv, DOI arXiv:2007.09860; Jiang L, 2020, PROC CVPR IEEE, P4866, DOI 10.1109/CVPR42600.2020.00492; Khoreva A, 2017, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2017.181; Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42; Lahoud J, 2019, IEEE I CONF COMP VIS, P9255, DOI 10.1109/ICCV.2019.00935; Laine Samuli, 2017, P INT C LEARN REPR I, P3; Liang Zhihao, 2021, P IEEECVF INT C COMP, P2783; Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344; Mei JL, 2020, IEEE T INTELL TRANSP, V21, P2496, DOI 10.1109/TITS.2019.2919741; Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821; Na Zhao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11076, DOI 10.1109/CVPR42600.2020.01109; Oh SJ, 2017, PROC CVPR IEEE, P5038, DOI 10.1109/CVPR.2017.535; Pont-Tuset J, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481406; Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937; Pham QH, 2019, PROC CVPR IEEE, P8819, DOI 10.1109/CVPR.2019.00903; Rasmus A, 2015, ADV NEUR IN, V28; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Saining Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P574, DOI 10.1007/978-3-030-58580-8_34; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Sohn Kihyuk, 2020, ARXIV200107685; Song CF, 2019, PROC CVPR IEEE, P3131, DOI 10.1109/CVPR.2019.00325; Tan JG, 2021, IMAGE VISION COMPUT, V109, DOI 10.1016/j.imavis.2021.104129; Tarvainen A, 2017, ADV NEUR IN, V30; Vernaza P, 2017, PROC CVPR IEEE, P2953, DOI 10.1109/CVPR.2017.315; Wang WY, 2018, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2018.00272; Wang XL, 2019, PROC CVPR IEEE, P4091, DOI 10.1109/CVPR.2019.00422; Wei JC, 2020, PROC CVPR IEEE, P4383, DOI 10.1109/CVPR42600.2020.00444; Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687; Wu G., 2020, PROC ASIAN C COMPUT; Xun Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13703, DOI 10.1109/CVPR42600.2020.01372; Yang B, 2019, ADV NEUR IN, V32; Yi L, 2019, PROC CVPR IEEE, P3942, DOI 10.1109/CVPR.2019.00407; Yude Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12272, DOI 10.1109/CVPR42600.2020.01229; Zhang BA, 2021, PROC CVPR IEEE, P8879, DOI 10.1109/CVPR46437.2021.00877; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319; Zhou YZ, 2018, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2018.00399	55	0	0	5	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					10159	10170		10.1109/TPAMI.2021.3131120	http://dx.doi.org/10.1109/TPAMI.2021.3131120			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34847018	Green Submitted			2022-12-18	WOS:000880661400112
J	Liu, FL; Wu, X; You, CY; Ge, S; Zou, YX; Sun, X				Liu, Fenglin; Wu, Xian; You, Chenyu; Ge, Shen; Zou, Yuexian; Sun, Xu			Aligning Source Visual and Target Language Domains for Unpaired Video Captioning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Visualization; Pipelines; Training; Data models; Decoding; Task analysis; Feature extraction; Video captioning; unpaired video captioning; pipeline system; pseudo supervised training; adversarial training		Training supervised video captioning model requires coupled video-caption pairs. However, for many targeted languages, sufficient paired data are not available. To this end, we introduce the unpaired video captioning task aiming to train models without coupled video-caption pairs in target language. To solve the task, a natural choice is to employ a two-step pipeline system: first utilizing video-to-pivot captioning model to generate captions in pivot language and then utilizing pivot-to-target translation model to translate the pivot captions to the target language. However, in such a pipeline system, 1) visual information cannot reach the translation model, generating visual irrelevant target captions; 2) the errors in the generated pivot captions will be propagated to the translation model, resulting in disfluent target captions. To address these problems, we propose the Unpaired Video Captioning with Visual Injection system (UVC-VI). UVC-VI first introduces the Visual Injection Module (VIM), which aligns source visual and target language domains to inject the source visual information into the target language domain. Meanwhile, VIM directly connects the encoder of the video-to-pivot model and the decoder of the pivot-to-target model, allowing end-to-end inference by completely skipping the generation of pivot captions. To enhance the cross-modality injection of the VIM, UVC-VI further introduces a pluggable video encoder, i.e., Multimodal Collaborative Encoder (MCE). The experiments show that UVC-VI outperforms pipeline systems and exceeds several supervised systems. Furthermore, equipping existing supervised systems with our MCE can achieve 4% and 7% relative margins on the CIDEr scores to current state-of-the-art models on the benchmark MSVD and MSR-VTT datasets, respectively.	[Liu, Fenglin; Zou, Yuexian] Peking Univ, Sch ECE, ADSPLAB, Shenzhen 518055, Peoples R China; [Wu, Xian; Ge, Shen] Tencent, Shenzhen 518054, Peoples R China; [You, Chenyu] Yale Univ, Dept Elect Engn, New Haven, CT 06520 USA; [Zou, Yuexian] Peng Cheng Lab, Shenzhen 518066, Peoples R China; [Sun, Xu] Peking Univ, MOE Key Lab Computat Linguist, Sch EECS, Beijing, Peoples R China; [Sun, Xu] Beijing Acad Artificial Intelligence, Beijing, Peoples R China	Peking University; Tencent; Yale University; Peng Cheng Laboratory; Peking University	Zou, YX (corresponding author), Peking Univ, Sch ECE, ADSPLAB, Shenzhen 518055, Peoples R China.; Wu, X (corresponding author), Tencent, Shenzhen 518054, Peoples R China.; Sun, X (corresponding author), Peking Univ, MOE Key Lab Computat Linguist, Sch EECS, Beijing, Peoples R China.	fenglinliu98@pku.edu.cn; kevinxwu@tencent.com; chenyu.you@yale.edu; shenge@tencent.com; zouyx@pku.edu.cn; xusun@pku.edu.cn		Liu, Fenglin/0000-0001-7715-5228; ZOU, Yue-Xian/0000-0001-9999-6140	National Natural Science Foundation of China [NSFC 62176008]; Natural Science Foundation of China (NSFC) [62176002]; Beijing Academy of Artificial Intelligence (BAAI)	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Beijing Academy of Artificial Intelligence (BAAI)	This work was supported in part by the National Natural Science Foundation of China under Grant NSFC 62176008, in part by the Natural Science Foundation of China (NSFC) under Grant 62176002, and in part by the Beijing Academy of Artificial Intelligence (BAAI).	Aafaq N, 2019, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR.2019.01277; Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636; Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; Ba J.L., 2016, ARXIV, DOI DOI 10.48550/ARXIV.1607.06450; Banerjee S., 2005, P ACL WORKSH INTR EX, P65; Bengio S, 2015, ADV NEUR IN, V28; C.Lawrence Zitnick, 2015, Arxiv, DOI arXiv:1504.00325; Chen JW, 2019, AAAI CONF ARTIF INTE, P8167; Chen SX, 2019, AAAI CONF ARTIF INTE, P8191; Chen YY, 2018, LECT NOTES COMPUT SC, V11217, P367, DOI 10.1007/978-3-030-01261-8_22; Chenyu You, 2020, Arxiv, DOI arXiv:2010.08923; You CY, 2021, Arxiv, DOI arXiv:2105.07059; Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059; Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121; David E., 1986, PARALLEL DISTRIBUTED, P318, DOI DOI 10.5555/104279.104293; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754; Feng Y, 2019, PROC CVPR IEEE, P4120, DOI 10.1109/CVPR.2019.00425; Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gu JX, 2018, LECT NOTES COMPUT SC, V11205, P519, DOI 10.1007/978-3-030-01246-5_31; Gu JX, 2019, IEEE I CONF COMP VIS, P10322, DOI 10.1109/ICCV.2019.01042; Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kingma D.P, P 3 INT C LEARNING R; Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83; Lai G., 2017, EMNLP, P785, DOI [10.18653/v1/D17-1082, DOI 10.18653/V1/D17-1082]; Laina I, 2019, IEEE I CONF COMP VIS, P7413, DOI 10.1109/ICCV.2019.00751; Lan WY, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1549, DOI 10.1145/3123266.3123366; Lee CH, 2019, IEEE-ACM T AUDIO SPE, V27, P1469, DOI 10.1109/TASLP.2019.2913499; Lin C.-Y., 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.3115/V1/D14-1020; Liu FL, 2019, ADV NEUR IN, V32; Liu S, 2021, IEEE T PATTERN ANAL, V43, P3259, DOI 10.1109/TPAMI.2019.2940007; Mehta R, 2017, I S BIOMED IMAGING, P437, DOI 10.1109/ISBI.2017.7950555; Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111; Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497; Pancoast Stephanie, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1370, DOI 10.1109/ICASSP.2014.6853821; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Pei WJ, 2019, PROC CVPR IEEE, P8339, DOI 10.1109/CVPR.2019.00854; Ryu H, 2021, AAAI CONF ARTIF INTE, V35, P2514; Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x; Shiang SR, 2014, INTERSPEECH, P263; Song YQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P784, DOI 10.1145/3343031.3350996; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Tseng BH, 2016, INTERSPEECH, P2731, DOI 10.21437/Interspeech.2016-876; Utiyama Masao, 2007, HUMAN LANGUAGE TECHN, P484; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vaswani A, 2017, ADV NEUR IN, V30; Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087; Venugopalan S., 2015, P C N AM CHAPT ASS C, P1494; Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640; Voykinska V, 2016, ACM CONFERENCE ON COMPUTER-SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING (CSCW 2016), P1584, DOI 10.1145/2818048.2820013; Wang BR, 2019, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2019.00273; Wang X, 2019, IEEE I CONF COMP VIS, P4580, DOI 10.1109/ICCV.2019.00468; Wong KCL, 2018, MED IMAGE ANAL, V49, P105, DOI 10.1016/j.media.2018.07.010; Wu F., 2019, PROC INT C LEARN REP; Wu H., 2007, PROC ANN M ASS COMPU, P856; Wu JH, 2019, IEEE INT CON MULTI, P1480, DOI 10.1109/ICME.2019.00256; Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29; Xu J, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P537, DOI 10.1145/3123266.3123448; Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576; Yang B, 2021, AAAI CONF ARTIF INTE, V35, P3119; Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094; You C., 2021, P IEEE INT C AC SPEE, P7793; You CY, 2021, INTERSPEECH, P3211, DOI 10.21437/Interspeech.2021-110; Yu D, 2017, IEEE-CAA J AUTOMATIC, V4, P396, DOI 10.1109/JAS.2017.7510508; Yu LT, 2017, AAAI CONF ARTIF INTE, P2852; Yu Y, 2017, PROC CVPR IEEE, P3261, DOI 10.1109/CVPR.2017.347; Zahabi S. T., 2013, PROC ANN M ASS COMPU, P318; Zhang W, 2020, IEEE T PATTERN ANAL, V42, P3088, DOI 10.1109/TPAMI.2019.2920899; Zheng Q., 2020, P IEEE CVF C COMP VI; Zhifeng Chen, 2016, ARXIV; Zhou LW, 2018, AAAI CONF ARTIF INTE, P7590; Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	87	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9255	9268		10.1109/TPAMI.2021.3132229	http://dx.doi.org/10.1109/TPAMI.2021.3132229			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34855588	Green Submitted			2022-12-18	WOS:000880661400051
J	Liu, YX; Rehg, JM; Taylor, CJ; Wu, Y				Liu, Yanxi; Rehg, James M.; Taylor, Camillo J.; Wu, Ying			Introduction to the Special Section of CVPR 2017	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Editorial Material									[Liu, Yanxi] Penn State Univ, Sch EECS, State Coll, PA 16801 USA; [Rehg, James M.] Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA; [Taylor, Camillo J.] Univ Penn, Comp & Informat Sci Dept, State Coll, PA 16801 USA; [Wu, Ying] Northwestern Univ, Elect Engn & Comp Sci Dept, Evanston, IL 60208 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; University System of Georgia; Georgia Institute of Technology; University of Pennsylvania; Northwestern University	Liu, YX (corresponding author), Penn State Univ, Sch EECS, State Coll, PA 16801 USA.	yul11@psu.edu; rehg@gatech.edu; cjtaylor@seas.upenn.edu; yingwu@northwestern.edu							0	0	0	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					8702	8703		10.1109/TPAMI.2022.3201636	http://dx.doi.org/10.1109/TPAMI.2022.3201636			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX		Bronze			2022-12-18	WOS:000880661400013
J	Lukezic, A; Matas, J; Kristan, M				Lukezic, Alan; Matas, Jiri; Kristan, Matej			A Discriminative Single-Shot Segmentation Network for Visual Object Tracking	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Visual object tracking; video object segmentation; discriminative tracking; single-shot segmentation	CORRELATION FILTER TRACKER	Template-based discriminative trackers are currently the dominant tracking paradigm due to their robustness, but are restricted to bounding box tracking and a limited range of transformation models, which reduces their localization accuracy. We propose a discriminative single-shot segmentation tracker - D3S(2), which narrows the gap between visual object tracking and video object segmentation. A single-shot network applies two target models with complementary geometric properties, one invariant to a broad range of transformations, including non-rigid deformations, the other assuming a rigid object to simultaneously achieve robust online target segmentation. The overall tracking reliability is further increased by decoupling the object and feature scale estimation. Without per-dataset finetuning, and trained only for segmentation as the primary output, D3S(2) outperforms all published trackers on the recent short-term tracking benchmark VOT2020 and performs very close to the state-of-the-art trackers on the GOT-10k, TrackingNet, OTB100 and LaSoT. D3S(2) outperforms the leading segmentation tracker SiamMask on video object segmentation benchmarks and performs on par with top video object segmentation algorithms.	[Lukezic, Alan; Kristan, Matej] Univ Ljubljana, Fac Comp & Informat Sci, Ljubljana 1000, Slovenia; [Matas, Jiri] Czech Tech Univ, Fac Elect Engn, Ctr Machine Percept, Dept Cybernet, Prague 16636, Czech Republic	University of Ljubljana; Czech Technical University Prague	Lukezic, A (corresponding author), Univ Ljubljana, Fac Comp & Informat Sci, Ljubljana 1000, Slovenia.	alan.lukezic@fri.uni-lj.si; matas@cmp.felk.cvut.cz; matej.kristan@fri.uni-lj.si		Kristan, Matej/0000-0002-4252-4342	Slovenian Research Agency [P20214, J2-2506, J2-9433]; Young Researcher Program of the ARRS; Czech Science Foundation [GA18-05360S]	Slovenian Research Agency(Slovenian Research Agency - Slovenia); Young Researcher Program of the ARRS; Czech Science Foundation(Grant Agency of the Czech Republic)	This work was supported in part by Slovenian Research Agency program P20214 and Projects J2-2506 and J2-9433. The work of Alan Lukezic was supported in part by The Young Researcher Program of the ARRS. The work of Jiri Matas was supported in part by The Czech Science Foundation under Grant GA18-05360S.	Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156; Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56; Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P777, DOI 10.1007/978-3-030-58536-5_46; Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628; Bhat G, 2018, LECT NOTES COMPUT SC, V11206, P493, DOI 10.1007/978-3-030-01216-8_30; Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960; Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565; Chen YH, 2018, PROC CVPR IEEE, P1189, DOI 10.1109/CVPR.2018.00130; Chen YP, 2019, IEEE I CONF COMP VIS, P3434, DOI 10.1109/ICCV.2019.00353; Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670; Cheng JC, 2018, PROC CVPR IEEE, P7415, DOI 10.1109/CVPR.2018.00774; Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991; Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479; Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733; Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29; Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143; Dingcheng Yue, 2018, Arxiv, DOI arXiv:1809.03327; Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552; Galoogahi HK, 2013, IEEE I CONF COMP VIS, P3072, DOI 10.1109/ICCV.2013.381; Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630; Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196; Gupta DK, 2021, PROC CVPR IEEE, P12357, DOI 10.1109/CVPR46437.2021.01218; He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45; Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390; Hu YT, 2018, LECT NOTES COMPUT SC, V11212, P56, DOI 10.1007/978-3-030-01237-3_4; Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464; Johnander J, 2019, PROC CVPR IEEE, P8945, DOI 10.1109/CVPR.2019.00916; Kart U, 2019, PROC CVPR IEEE, P1339, DOI 10.1109/CVPR.2019.00143; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Kristan M, 2020, EUROPEAN C COMPUTER, P547, DOI [10.1007/978-3-030-68238-5_39, DOI 10.1007/978-3-030-68238-5_39]; Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1; Kristan M, 2017, IEEE INT CONF COMP V, P1949, DOI 10.1109/ICCVW.2017.230; Kristan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P564, DOI 10.1109/ICCVW.2015.79; Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441; Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935; Li PX, 2019, IEEE I CONF COMP VIS, P6161, DOI 10.1109/ICCV.2019.00626; Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lukezic A, 2020, PROC CVPR IEEE, P7131, DOI 10.1109/CVPR42600.2020.00716; Lukezic A, 2019, LECT NOTES COMPUT SC, V11362, P595, DOI 10.1007/978-3-030-20890-5_38; Lukezic A, 2018, INT J COMPUT VISION, V126, P671, DOI 10.1007/s11263-017-1061-3; Lukezic A, 2018, IEEE T CYBERNETICS, V48, P1849, DOI 10.1109/TCYB.2017.2716101; Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Muller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19; Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152; Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465; Oh SW, 2019, IEEE I CONF COMP VIS, P9225, DOI 10.1109/ICCV.2019.00932; Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85; Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5; Pont-Tuset J., 2017, ARXIV; Possegger H, 2015, PROC CVPR IEEE, P2113, DOI 10.1109/CVPR.2015.7298823; Robinson Andreas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7404, DOI 10.1109/CVPR42600.2020.00743; Ronneberger O., 2015, P MED IM COMP ASS IN, P234, DOI DOI 10.1007/978-3-319-24574-4_28; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937; Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Trottier L, 2017, 2017 16TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P207, DOI 10.1109/ICMLA.2017.00038; Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531; Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661; Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971; Voigtlaender Paul, 2017, ARXIV170609364; Wang GT, 2020, PROC CVPR IEEE, P6287, DOI 10.1109/CVPR42600.2020.00632; Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142; Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226; Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549; Yang LJ, 2018, PROC CVPR IEEE, P6499, DOI 10.1109/CVPR.2018.00680; Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975; Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199; Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI [10.1007/978-3-030-01234-2_18, 10.1007/978-3-030-01240-3_22]; Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46; Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7	75	0	0	3	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9742	9755		10.1109/TPAMI.2021.3137933	http://dx.doi.org/10.1109/TPAMI.2021.3137933			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34941502	Green Submitted			2022-12-18	WOS:000880661400085
J	Maalouf, A; Jubran, I; Feldman, D				Maalouf, Alaa; Jubran, Ibrahim; Feldman, Dan			Fast and Accurate Least-Mean-Squares Solvers for High Dimensional Data	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Regression; Least Mean Squares Solvers; Coresets; Sketches; Caratheodory's Theorem; Big Data	REGRESSION; SELECTION	Least-mean-squares (LMS) solvers such as Linear / Ridge-Regression and SVD not only solve fundamental machine learning problems, but are also the building blocks in a variety of other methods, such as matrix factorizations. We suggest an algorithm that gets a finite set of n d-dimensional real vectors and returns a subset of d + 1 vectors with positive weights whose weighted sum is exactly the same. The constructive proof in Caratheodory's Theorem computes such a subset in O(n(2)d(2)) time and thus not used in practice. Our algorithm computes this subset in O(nd + d(4)log n) time, using O(log n) calls to Caratheodory's construction on small but "smart" subsets. This is based on a novel paradigm of fusion between different data summarization techniques, known as sketches and coresets. For large values of d, we suggest a faster construction that takes O(nd) time and returns a weighted subset of O(d) sparsified input points. Here, a sparsified point means that some of its entries were set to zero. As an application, we show how to boost the performance of existing LMS solvers, such as those in scikit-learn library, up to x100. Generalization for streaming and distributed data is trivial. Extensive experimental results and open source code are provided.	[Maalouf, Alaa; Jubran, Ibrahim; Feldman, Dan] Univ Haifa, Dept Comp Sci, Robot & Big Data Labs, IL-3498838 Haifa, Israel	University of Haifa	Maalouf, A (corresponding author), Univ Haifa, Dept Comp Sci, Robot & Big Data Labs, IL-3498838 Haifa, Israel.	alaamalouf12@gmail.com; ibrahim.jub@gmail.com; dannyf.post@gmail.com						Adiel Statman, 2019, Arxiv, DOI arXiv:1907.01433; Agarwal PK, 2004, J ACM, V51, P606, DOI 10.1145/1008731.1008736; Alessandro Abate, 2020, Arxiv, DOI arXiv:2006.01819; [Anonymous], 2015, HOUSE SALES KING COU; [Anonymous], 2021, INDIVIDUAL HOUSEHOLD; Barman S, 2015, ACM S THEORY COMPUT, P361, DOI 10.1145/2746539.2746566; Bauckhage C., 2015, NUMPYSCIPY RECIPES D; Bertin-Mahieux Thierry, 2011, P 12 INT C MUS INF R, DOI DOI 10.7916/D8NZ8J07; Bjorck A., 1967, BIT, V7, P1; Caratheodory C, 1907, MATH ANN, V64, P95, DOI 10.1007/BF01449883; Clarkson KL, 2017, J ACM, V63, DOI 10.1145/3019134; Clarkson KL, 2009, ACM S THEORY COMPUT, P205; COOK WD, 1972, CAN MATH BULLETIN, V15, P293, DOI 10.4153/CMB-1972-053-6; COPAS JB, 1983, J R STAT SOC B, V45, P311; Cormen T. H., 2009, INTRO ALGORITHMS, V3rd; Dan Feldman, 2017, Arxiv, DOI arXiv:1511.09120; Drineas P, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1127, DOI 10.1145/1109557.1109682; Dua D., 2017, UCI MACHINE LEARNING, DOI DOI 10.1002/JCC.23219; Feldman D., 2016, P ADV NEUR INF PROC, P2774; Feldman D, 2010, PROC APPL MATH, V135, P630; Gallagher NM, 2017, ADV NEUR IN, V30; Golub G. H., 2012, LOAN LOAN MATRIX COM; GOLUB GH, 1970, NUMER MATH, V14, P403, DOI 10.1007/BF02163027; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI 10.1080/00401706.1970.10488634; Intel LTD, 2019, ACC PYTH PERF; Jeff M. Phillips, 2016, Arxiv, DOI arXiv:1601.00617; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; Jubran I., 2019, ARXIV; Jubran I., 2019, ARXIV; Kang B., 2011, PROC C NEURAL INF PR; Kaul M, 2013, 2013 IEEE 14TH INTERNATIONAL CONFERENCE ON MOBILE DATA MANAGEMENT (MDM 2013), VOL 1, P137, DOI 10.1109/MDM.2013.24; Kohavi R., 1995, IJCAI-95. Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, P1137; Laparra V, 2015, IEEE J-STSP, V9, P1026, DOI 10.1109/JSTSP.2015.2417833; Liang Y., 2013, P NIPS BIG LEARN WOR; Maalouf A., 2019, OPEN SOURCE CODE ALL; Maalouf A, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2051, DOI 10.1145/3394486.3403256; Ngo TT, 2012, SIAM REV, V54, P545, DOI 10.1137/120864799; Noble WS, 2006, NAT BIOTECHNOL, V24, P1565, DOI 10.1038/nbt1206-1565; Peng X, 2015, AAAI CONF ARTIF INTE, P3827; Porco A., 2015, PROC WORKSHOP NETW S; SAFAVIAN SR, 1991, IEEE T SYST MAN CYB, V21, P660, DOI 10.1109/21.97458; Seber GAF, 2003, WILEY SERIES PROBABI, V2nd; Song Z., 2019, PROC ADV NEURAL INF, P828; Srivastava S, 2007, J MACH LEARN RES, V8, P1277; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wang Q, 2022, IEEE T PATTERN ANAL, V44, P390, DOI 10.1109/TPAMI.2020.3007673; Wang YX, 2013, IEEE T KNOWL DATA EN, V25, P1336, DOI 10.1109/TKDE.2012.51; Wikipedia contributors, 2019, LIST NV GRAPH PROC U; Wikipedia contributors, 2019, CPYTHON WIKIPEDIA FR; Wikipedia contributors, 2019, LINEAR DISCRIMINANT; Zhang YL, 2018, ADV NEUR IN, V31; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	52	0	0	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9977	9994		10.1109/TPAMI.2021.3139612	http://dx.doi.org/10.1109/TPAMI.2021.3139612			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	35015632				2022-12-18	WOS:000880661400101
J	Malali, N; Keller, Y				Malali, Noam; Keller, Yosi			Learning to Embed Semantic Similarity for Joint Image-Text Retrieval	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Text and image fusion; deep learning; joint embedding		We present a deep learning approach for learning the joint semantic embeddings of images and captions in a euclidean space, such that the semantic similarity is approximated by the L-2 distances in the embedding space. For that, we introduce a metric learning scheme that utilizes multitask learning to learn the embedding of identical semantic concepts using a center loss. By introducing a differentiable quantization scheme into the end-to-end trainable network, we derive a semantic embedding of semantically similar concepts in euclidean space. We also propose a novel metric learning formulation using an adaptive margin hinge loss, that is refined during the training phase. The proposed scheme was applied to the MS-COCO, Flicke30K and Flickr8K datasets, and was shown to compare favorably with contemporary state-of-the-art approaches.	[Malali, Noam; Keller, Yosi] Bar Ilan Univ, Fac Engn, IL-5290002 Ramat Gan, Israel	Bar Ilan University	Keller, Y (corresponding author), Bar Ilan Univ, Fac Engn, IL-5290002 Ramat Gan, Israel.	nmalali@gmail.com; yosi.keller@gmail.com						Akaho S., 2006, ARXIV; Andrew G., 2013, INT C MACH LEARN, p1247?1255; Aoxue Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12573, DOI 10.1109/CVPR42600.2020.01259; Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/CVPR.2016.572, 10.1109/TPAMI.2017.2711011]; Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483; Bert De Brabandere, 2017, Arxiv, DOI arXiv:1708.02551; Chandar S, 2016, NEURAL COMPUT, V28, P257, DOI 10.1162/NECO_a_00801; Chechik G, 2010, J MACH LEARN RES, V11, P1109; Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145; Eisenschtat A, 2017, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2017.201; Faghri F., 2018, PROC BRIT MACH VIS C; Frome A, 2007, IEEE I CONF COMP VIS, P94; Frome Andrea, 2013, NEURIPS; Goldberger Jacob, 2005, ADV NEURAL INFORM PR, V17, P8, DOI DOI 10.1109/TCSVT.2013.2242640; Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645; Huang Y, 2017, PROC CVPR IEEE, P7254, DOI 10.1109/CVPR.2017.767; Jabri A, 2016, LECT NOTES COMPUT SC, V9912, P727, DOI 10.1007/978-3-319-46484-8_44; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073; Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13; Lev G, 2016, LECT NOTES COMPUT SC, V9910, P833, DOI 10.1007/978-3-319-46466-4_50; Li AN, 2009, PROC CVPR IEEE, P605, DOI 10.1109/CVPRW.2009.5206659; Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin X, 2016, LECT NOTES COMPUT SC, V9906, P261, DOI 10.1007/978-3-319-46475-6_17; Malinowski M, 2015, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2015.9; Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232; Norouzi M., 2011, INT C MACHINE LEARNI, P353; Patrick M., 2021, PROC INT C LEARN REP; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Richard S. Zemel, 2014, Arxiv, DOI arXiv:1411.2539; Socher R., 2014, J T ASS COMPUT LINGU, V2, P207, DOI DOI 10.1162/TACL_A_00177; Tanti M., 2017, P INT C NAT LANG GEN, P51; Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921; Wang WR, 2015, PR MACH LEARN RES, V37, P1083; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Weston J., 2011, P INT JOINT C ART IN, P2; Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309; Young Peter, 2014, T ASSOC COMPUT LING, V2, P67	42	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					10252	10260		10.1109/TPAMI.2021.3132163	http://dx.doi.org/10.1109/TPAMI.2021.3132163			9	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34855587	Green Submitted			2022-12-18	WOS:000880661400120
J	Malik, J; Shimada, S; Elhayek, A; Ali, SA; Theobalt, C; Golyanik, V; Stricker, D				Malik, Jameel; Shimada, Soshi; Elhayek, Ahmed; Ali, Sk Aziz; Theobalt, Christian; Golyanik, Vladislav; Stricker, Didier			HandVoxNet++: 3D Hand Shape and Pose Estimation Using Voxel-Based Neural Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						3D hand shape and pose from a single depth map; voxelized hand shape; graph convolutions; TSDF; 3D data augmentation; shape registration; GCN-MeshReg; NRGA plus		3D hand shape and pose estimation from a single depth map is a new and challenging computer vision problem with many applications. Existing methods addressing it directly regress hand meshes via 2D convolutional neural networks, which leads to artifacts due to perspective distortions in the images. To address the limitations of the existing methods, we develop HandVoxNet++, i.e., a voxel-based deep network with 3D and graph convolutions trained in a fully supervised manner. The input to our network is a 3D voxelized-depth-map-based on the truncated signed distance function (TSDF). HandVoxNet++ relies on two hand shape representations. The first one is the 3D voxelized grid of hand shape, which does not preserve the mesh topology and which is the most accurate representation. The second representation is the hand surface that preserves the mesh topology. We combine the advantages of both representations by aligning the hand surface to the voxelized hand shape either with a new neural Graph-Convolutions-based Mesh Registration (GCN-MeshReg) or classical segment-wise Non-Rigid Gravitational Approach (NRGA++) which does not rely on training data. In extensive evaluations on three public benchmarks, i.e., SynHand5M, depth-based HANDS19 challenge and HO-3D, the proposed HandVoxNet++ achieves the state-of-the-art performance. In this journal extension of our previous approach presented at CVPR 2020, we gain 41.09% and 13.7% higher shape alignment accuracy on SynHand5M and HANDS19 datasets, respectively. Our method is ranked first on the HANDS19 challenge dataset (Task 1: Depth-Based 3D Hand Pose Estimation) at the moment of the submission of our results to the portal in August 2020.	[Malik, Jameel; Ali, Sk Aziz; Stricker, Didier] TU Kaiserslautern, D-67663 Kaiserslautern, Germany; [Malik, Jameel] NUST, Islamabad 44000, Pakistan; [Shimada, Soshi; Theobalt, Christian; Golyanik, Vladislav] MPI Informat, Saarbrcken, Germany; [Shimada, Soshi] Saarland Informat Campus, D-66123 Saarbrcken, Germany; [Elhayek, Ahmed] UPM, Medina 42241, Saudi Arabia; [Ali, Sk Aziz; Stricker, Didier] DFKI, D-67663 Kaiserslautern, Germany	University of Kaiserslautern; National University of Sciences & Technology - Pakistan; Max Planck Society; German Research Center for Artificial Intelligence (DFKI)	Malik, J (corresponding author), TU Kaiserslautern, D-67663 Kaiserslautern, Germany.	jameel.malik@dfki.de; sshimack@mpi-inf.mpg.de; a.elhayek@upm.edu.sa; Sk_Aziz_Ali@upm.edu.sa; theobalt@mpi-inf.mpg.de; golyanik@mpi-inf.mpg.de; didier.stricker@dfki.de			German Federal Ministry of Education and Research [01IW18002, 01IW21001]; ERC [770784]	German Federal Ministry of Education and Research(Federal Ministry of Education & Research (BMBF)); ERC(European Research Council (ERC)European Commission)	This work was funded by the German Federal Ministry of Education and Research as part of the project VIDETE under Grant 01IW18002, DECODE under Grant 01IW21001, and the ERC Consolidator under Grant 770784. (Corresponding author: Jameel Malik.)	Ali SA, 2018, INT CONF 3D VISION, P756, DOI 10.1109/3DV.2018.00091; Armagan Anil, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P85, DOI 10.1007/978-3-030-58592-1_6; Baek S, 2020, PROC CVPR IEEE, P6120, DOI 10.1109/CVPR42600.2020.00616; Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236; Cai YJ, 2021, IEEE T PATTERN ANAL, V43, P3739, DOI 10.1109/TPAMI.2020.2993627; Chen YJ, 2019, IEEE I CONF COMP VIS, P6960, DOI 10.1109/ICCV.2019.00706; Cheng S., 2020, PROC ASIAN C COMPUT, P188; Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2; Doosti B, 2020, PROC CVPR IEEE, P6607, DOI 10.1109/CVPR42600.2020.00664; Garcia-Hernando G, 2018, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2018.00050; Ge LH, 2018, LECT NOTES COMPUT SC, V11217, P489, DOI 10.1007/978-3-030-01261-8_29; Ge LH, 2019, PROC CVPR IEEE, P10825, DOI 10.1109/CVPR.2019.01109; Ge LH, 2019, IEEE T PATTERN ANAL, V41, P956, DOI 10.1109/TPAMI.2018.2827052; Ge LH, 2017, PROC CVPR IEEE, P5679, DOI 10.1109/CVPR.2017.602; Golyanik V, 2018, LECT NOTES COMPUT SC, V11162, P51, DOI 10.1007/978-3-030-01790-3_4; Guo HK, 2017, IEEE IMAGE PROC, P4512; Hampali S, 2020, PROC CVPR IEEE, P3193, DOI 10.1109/CVPR42600.2020.00326; Hasson Y, 2020, PROC CVPR IEEE, P568, DOI 10.1109/CVPR42600.2020.00065; Huang WT, 2020, AAAI CONF ARTIF INTE, V34, P11061; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Jian B, 2005, IEEE I CONF COMP VIS, P1246; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Kipf T. N., 2017, INT C LEARN REPR, DOI [DOI 10.1109/ICDM.2008.17, DOI 10.1109/ICDM.2019.00070]; Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463; Malik J, 2020, IEEE ACCESS, V8, P195832, DOI 10.1109/ACCESS.2020.3033848; Malik J, 2020, PROC CVPR IEEE, P7111, DOI 10.1109/CVPR42600.2020.00714; Malik J, 2019, COMPUT GRAPH-UK, V85, P85, DOI 10.1016/j.cag.2019.10.002; Malik J, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19173784; Malik J, 2018, LECT NOTES COMPUT SC, V11162, P3, DOI 10.1007/978-3-030-01790-3_1; Malik J, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18113872; Malik J, 2018, INT CONF 3D VISION, P110, DOI 10.1109/3DV.2018.00023; Malik J, 2017, INT CONF 3D VISION, P557, DOI 10.1109/3DV.2017.00069; Moon G, 2018, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR.2018.00533; Mueller F, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322958; Mueller F, 2018, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2018.00013; Neng Qian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P54, DOI 10.1007/978-3-030-58621-8_4; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Oberweger M, 2017, IEEE INT CONF COMP V, P585, DOI 10.1109/ICCVW.2017.75; Park G, 2020, IEEE T VIS COMPUT GR, V26, P1891, DOI 10.1109/TVCG.2020.2973057; Poier G, 2019, IEEE WINT CONF APPL, P1393, DOI 10.1109/WACV.2019.00153; Rad M, 2018, PROC CVPR IEEE, P4663, DOI 10.1109/CVPR.2018.00490; Ranjan A, 2018, LECT NOTES COMPUT SC, V11207, P725, DOI 10.1007/978-3-030-01219-9_43; Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883; Sharp T, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3633, DOI 10.1145/2702123.2702179; Shimada S, 2019, INT CONF 3D VISION, P27, DOI 10.1109/3DV.2019.00013; Shimada S, 2019, IEEE COMPUT SOC CONF, P2876, DOI 10.1109/CVPRW.2019.00347; Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94; Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33; Sun X, 2015, PROC CVPR IEEE, P824, DOI 10.1109/CVPR.2015.7298683; Supancic JS, 2018, INT J COMPUT VISION, V126, P1180, DOI 10.1007/s11263-018-1081-7; Tang DH, 2014, PROC CVPR IEEE, P3786, DOI 10.1109/CVPR.2014.490; Taylor J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925965; Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500; Tretschk Edgar, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P601, DOI 10.1007/978-3-030-58548-8_35; Vollmer J, 1999, COMPUT GRAPH FORUM, V18, pC131, DOI 10.1111/1467-8659.00334; Wan CD, 2018, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2018.00540; Xiong F, 2019, IEEE I CONF COMP VIS, P793, DOI 10.1109/ICCV.2019.00088; Yang John, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P122, DOI 10.1007/978-3-030-58610-2_8; Yang LL, 2019, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2019.00242; Yuan SX, 2018, PROC CVPR IEEE, P2636, DOI 10.1109/CVPR.2018.00279; Yuan SX, 2017, PROC CVPR IEEE, P2605, DOI 10.1109/CVPR.2017.279; Zhang X, 2019, IEEE I CONF COMP VIS, P2354, DOI 10.1109/ICCV.2019.00244; Zhou YX, 2020, PROC CVPR IEEE, P5345, DOI 10.1109/CVPR42600.2020.00539	63	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					8962	8974		10.1109/TPAMI.2021.3122874	http://dx.doi.org/10.1109/TPAMI.2021.3122874			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34727024	Green Submitted			2022-12-18	WOS:000880661400031
J	Marcon, M; Spezialetti, R; Salti, S; Silva, L; Di Stefano, L				Marcon, Marlon; Spezialetti, Riccardo; Salti, Samuele; Silva, Luciano; Di Stefano, Luigi			Unsupervised Learning of Local Equivariant Descriptors for Point Clouds	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Deep learning; Decoding; Computer architecture; Training; Training data; Shape; Deep learning on point clouds; local features; equivariant; unsupervised; feature learning; registration	OBJECT RECOGNITION; FOURIER-TRANSFORMS; UNIQUE SIGNATURES; HISTOGRAMS; SURFACE	Correspondences between 3D keypoints generated by matching local descriptors are a key step in 3D computer vision and graphic applications. Learned descriptors are rapidly evolving and outperforming the classical handcrafted approaches in the field. Yet, to learn effective representations they require supervision through labeled data, which are cumbersome and time-consuming to obtain. Unsupervised alternatives exist, but they lag in performance. Moreover, invariance to viewpoint changes is attained either by relying on data augmentation, which is prone to degrading upon generalization on unseen datasets, or by learning from handcrafted representations of the input which are already rotation invariant but whose effectiveness at training time may significantly affect the learned descriptor. We show how learning an equivariant 3D local descriptor instead of an invariant one can overcome both issues. LEAD (Local EquivAriant Descriptor) combines Spherical CNNs to learn an equivariant representation together with plane-folding decoders to learn without supervision. Through extensive experiments on standard surface registration datasets, we show how our proposal outperforms existing unsupervised methods by a large margin and achieves competitive results against the supervised approaches, especially in the practically very relevant scenario of transfer learning.	[Marcon, Marlon] Univ Tecnol Fed Parana, Dept Software Engn, BR-86812460 Curitiba, Parana, Brazil; [Spezialetti, Riccardo; Salti, Samuele; Di Stefano, Luigi] Univ Bologna, Dept Comp Sci & Engn DISI, I-40126 Bologna, Italy; [Silva, Luciano] Univ Fed Parana, Dept Comp Sci, BR-86812460 Curitiba, Parana, Brazil	Universidade Tecnologica Federal do Parana; University of Bologna; Universidade Federal do Parana	Marcon, M (corresponding author), Univ Tecnol Fed Parana, Dept Software Engn, BR-86812460 Curitiba, Parana, Brazil.	marlonmarcon@utfpr.edu.br; riccardo.spezialetti@unibo.it; samuele.salti@unibo.it; luciano@ufpr.br; luigi.distefano@unibo.it		Marcon, Marlon/0000-0002-3698-8570; Silva, Luciano/0000-0001-6341-1323; Spezialetti, Riccardo/0000-0001-9748-2824; Di Stefano, Luigi/0000-0001-6014-6421; Salti, Samuele/0000-0001-5609-426X	UTFPR	UTFPR	The authors would like to thank Injenia srl and UTFPR for partly supporting this research work.	Aldoma A, 2012, IEEE ROBOT AUTOM MAG, V19, P80, DOI 10.1109/MRA.2012.2206675; Bai XY, 2020, PROC CVPR IEEE, P6358, DOI 10.1109/CVPR42600.2020.00639; Hua BS, 2018, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2018.00109; Birdal T, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P527, DOI 10.1109/3DV.2015.65; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Choy C, 2019, IEEE I CONF COMP VIS, P8957, DOI 10.1109/ICCV.2019.00905; Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319; Cohen T.S., 2018, INT C LEARN REPR; Cohen TS, 2019, PR MACH LEARN RES, V97; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Deng HW, 2018, LECT NOTES COMPUT SC, V11209, P620, DOI 10.1007/978-3-030-01228-1_37; Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028; DRISCOLL JR, 1994, ADV APPL MATH, V15, P202, DOI 10.1006/aama.1994.1008; Drost B, 2010, PROC CVPR IEEE, P998, DOI 10.1109/CVPR.2010.5540108; Esteves C, 2018, LECT NOTES COMPUT SC, V11217, P54, DOI 10.1007/978-3-030-01261-8_4; Folland G. B, 2016, COURSE ABSTRACT HARM, V29; Gojcic Z, 2019, PROC CVPR IEEE, P5540, DOI 10.1109/CVPR.2019.00569; Groh F, 2019, LECT NOTES COMPUT SC, V11361, P105, DOI 10.1007/978-3-030-20887-5_7; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Guo YL, 2016, INT J COMPUT VISION, V116, P66, DOI 10.1007/s11263-015-0824-y; Guo YL, 2013, INT J COMPUT VISION, V105, P63, DOI 10.1007/s11263-013-0627-y; Halber M, 2017, PROC CVPR IEEE, P6660, DOI 10.1109/CVPR.2017.705; HOPPE H, 1992, COMP GRAPH, V26, P71, DOI 10.1145/142920.134011; Jaderberg M, 2015, ADV NEUR IN, V28; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Khoury M, 2017, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2017.26; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99; Kostelec PJ, 2008, J FOURIER ANAL APPL, V14, P145, DOI 10.1007/s00041-008-9013-5; Lai K, 2014, IEEE INT CONF ROBOT, P3050, DOI 10.1109/ICRA.2014.6907298; Li L, 2020, PROC CVPR IEEE, P1916, DOI 10.1109/CVPR42600.2020.00199; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Maslen D. K., 1997, GROUPS COMPUTATION 2, V28, P183; Maslen DK, 1998, J FOURIER ANAL APPL, V4, P19, DOI 10.1007/BF02475926; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Petrelli A, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P403, DOI 10.1109/3DIMPVT.2012.51; Pomerleau F, 2012, INT J ROBOT RES, V31, P1705, DOI 10.1177/0278364912458814; Qi CR, 2017, ADV NEUR IN, V30; Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16; Rao YM, 2019, PROC CVPR IEEE, P452, DOI 10.1109/CVPR.2019.00054; Risbo T, 1996, J GEODESY, V70, P383, DOI 10.1007/BF01090814; Rusu RB, 2009, IEEE INT CONF ROBOT, P1848; Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011; Schultz M, 2004, ADV NEUR IN, V16, P41; Shotton J, 2013, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2013.377; Spezialetti R., 2020, PROC INT C NEURAL IN, V33; Spezialetti R, 2019, IEEE I CONF COMP VIS, P6410, DOI 10.1109/ICCV.2019.00650; Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Sugiura M., 1990, UNITARY REPRESENTATI; Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230; Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651; Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26; Valentin J, 2016, INT CONF 3D VISION, P323, DOI 10.1109/3DV.2016.41; Wei LY, 2016, PROC CVPR IEEE, P1544, DOI 10.1109/CVPR.2016.171; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309; Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458; Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6; Yang JQ, 2018, IEEE T IMAGE PROCESS, V27, P3766, DOI 10.1109/TIP.2018.2827330; Yang JQ, 2017, PATTERN RECOGN, V65, P175, DOI 10.1016/j.patcog.2016.11.019; Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029; You Y, 2020, AAAI CONF ARTIF INTE, V34, P12717; Zeng A, 2017, PROC CVPR IEEE, P199, DOI 10.1109/CVPR.2017.29; Zhang ZY, 2019, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2019.00169; Zhang ZY, 2019, INT CONF 3D VISION, P204, DOI 10.1109/3DV.2019.00031; Zhao YH, 2019, PROC CVPR IEEE, P1009, DOI 10.1109/CVPR.2019.00110	68	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9687	9702		10.1109/TPAMI.2021.3126713	http://dx.doi.org/10.1109/TPAMI.2021.3126713			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34752387				2022-12-18	WOS:000880661400080
J	Melacci, S; Ciravegna, G; Sotgiu, A; Demontis, A; Biggio, B; Gori, M; Roli, F				Melacci, Stefano; Ciravegna, Gabriele; Sotgiu, Angelo; Demontis, Ambra; Biggio, Battista; Gori, Marco; Roli, Fabio			Domain Knowledge Alleviates Adversarial Attacks in Multi-Label Classifiers	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Training; Training data; Robustness; Task analysis; Adversarial machine learning; Ink; Semisupervised learning; Learning from constraints; domain knowledge; adversarial machine learning; multi-label classification		Adversarial attacks on machine learning-based classifiers, along with defense mechanisms, have been widely studied in the context of single-label classification problems. In this paper, we shift the attention to multi-label classification, where the availability of domain knowledge on the relationships among the considered classes may offer a natural way to spot incoherent predictions, i.e., predictions associated to adversarial examples lying outside of the training data distribution. We explore this intuition in a framework in which first-order logic knowledge is converted into constraints and injected into a semi-supervised learning problem. Within this setting, the constrained classifier learns to fulfill the domain knowledge over the marginal distribution, and can naturally reject samples with incoherent predictions. Even though our method does not exploit any knowledge of attacks during training, our experimental analysis surprisingly unveils that domain-knowledge constraints can help detect adversarial examples effectively, especially if such constraints are not known to the attacker. We show how to implement an adaptive attack exploiting knowledge of the constraints and, in a specifically-designed setting, we provide experimental comparisons with popular state-of-the-art attacks. We believe that our approach may provide a significant step towards designing more robust multi-label classifiers.	[Melacci, Stefano; Gori, Marco] Univ Siena, Dept Informat Engn & Math, I-53100 Siena, Italy; [Ciravegna, Gabriele] Univ Florence, Dept Informat Engn, I-50121 Florence, Italy; [Sotgiu, Angelo; Demontis, Ambra; Biggio, Battista] Univ Cagliari, Dept Informat & Elect Engn, I-09124 Cagliari, Italy; [Gori, Marco] Univ Cote dAzur, MAA SAI, F-06108 Nice, France; [Roli, Fabio] Univ Genoa, Dept Informat Bioengn Robot & Syst Engn, I-16145 Genoa, Italy	University of Siena; University of Florence; University of Cagliari; UDICE-French Research Universities; Universite Cote d'Azur; University of Genoa	Melacci, S (corresponding author), Univ Siena, Dept Informat Engn & Math, I-53100 Siena, Italy.	mela@diism.unisi.it; gabriele.ciravegna@unifi.it; angelo.sotgiu@unica.it; ambra.demontis@unica.it; battista.biggio@unica.it; marco.gori@unisi.it; fabio.roli@unige.it	; BIGGIO, BATTISTA/M-5931-2016	Ciravegna, Gabriele/0000-0002-6799-1043; ROLI, FABIO/0000-0003-4103-9190; BIGGIO, BATTISTA/0000-0001-7752-509X; Sotgiu, Angelo/0000-0003-2100-9517	PRIN 2017 Project RexLearn, through the Italian Ministry of Education, University and Research [2017TWNMH2]	PRIN 2017 Project RexLearn, through the Italian Ministry of Education, University and Research	This work was supported by PRIN 2017 Project RexLearn, through the Italian Ministry of Education, University and Research under Grant 2017TWNMH2.	Akcay S, 2019, LECT NOTES COMPUT SC, V11363, P622, DOI 10.1007/978-3-030-20893-6_39; Alayrac J., 2019, NEURIPS, V32, P12192; Aleksander Madry, 2019, Arxiv, DOI arXiv:1902.06705; Alexandre Araujo, 2020, Arxiv, DOI arXiv:1903.10219; Andriushchenko Maksym, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P484, DOI 10.1007/978-3-030-58592-1_29; Athalye A, 2018, PR MACH LEARN RES, V80; Babbar R., 2018, ARXIV; Barbiero P., 2021, ARXIV; Bendale A, 2016, PROC CVPR IEEE, P1563, DOI 10.1109/CVPR.2016.173; Biggio Battista, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P387, DOI 10.1007/978-3-642-40994-3_25; Biggio B, 2018, PATTERN RECOGN, V84, P317, DOI 10.1016/j.patcog.2018.07.023; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Carlini Nicholas, 2017, P 10 ACM WORKSHOP AR, P3, DOI [10.1145/3128572.3140444, DOI 10.1145/3128572.3140444]; Carmon Y, 2019, ADV NEUR IN, V32; Chen Dan, 2019, Arxiv, DOI arXiv:1906.00555; Ciravegna G, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234; Croce F, 2020, PR MACH LEARN RES, V119; Croce F, 2020, PR MACH LEARN RES, V119; Demontis A, 2019, PROCEEDINGS OF THE 28TH USENIX SECURITY SYMPOSIUM, P321; Donadello I, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1596; Garcez AD, 2019, J APPL LOG-IFCOLOG, V6, P611; Gnecco G, 2015, NEURAL COMPUT, V27, P388, DOI 10.1162/NECO_a_00686; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Goodfellow I, 2018, COMMUN ACM, V61, P56, DOI 10.1145/3134599; Gori M, 2013, IEEE T NEUR NET LEAR, V24, P825, DOI 10.1109/TNNLS.2013.2241787; Hein M, 2019, PROC CVPR IEEE, P41, DOI 10.1109/CVPR.2019.00013; Ian Goodfellow, 2016, Arxiv, DOI arXiv:1605.07277; Joshi A, 2019, IEEE I CONF COMP VIS, P4772, DOI 10.1109/ICCV.2019.00487; Kathrin Grosse, 2017, Arxiv, DOI arXiv:1702.06280; Klement E., 2000, TRIANGULAR NORMS; Ma X., 2018, 2018 15 INT S WIRELE, P1; Madry A., 2018, P ICLR VANC BC CAN; Melacci S, 2018, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2018), P1012; Melacci S, 2011, J MACH LEARN RES, V12, P1149; Melis M, 2017, IEEE INT CONF COMP V, P751, DOI 10.1109/ICCVW.2017.94; Miller DJ, 2020, P IEEE, V108, P402, DOI 10.1109/JPROC.2020.2970615; Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821; Miyato Takeru, 2016, ARXIV160507725; Morgado P, 2017, PROC CVPR IEEE, P2037, DOI 10.1109/CVPR.2017.220; Najafi A., 2019, P ADV NEUR INF PROC, P5542; Naseer M. M., 2019, P ADV NEUR INF PROC, V32, P12905; Pang TY, 2018, ADV NEUR IN, V31; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Park S, 2018, AAAI CONF ARTIF INTE, P3917; Pi T, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2599; Sheatsley R, 2021, Arxiv, DOI arXiv:2105.08619; Samangouei Pouya, 2018, ARXIV180506605; Shafagatova A, 2019, LECT NOTES COMPUT SC, V11675, P386, DOI 10.1007/978-3-030-26619-6_25; Sinha A, 2018, IEEE C EVOL COMPUTAT, P213, DOI 10.1109/CEC.2018.8477763; Song QQ, 2018, IEEE DATA MINING, P1242, DOI 10.1109/ICDM.2018.00166; Sotgiu A, 2020, EURASIP J INF SECUR, V2020, DOI 10.1186/s13635-020-00105-y; Stefano Teso, 2019, Arxiv, DOI arXiv:1912.10834; Szegedy C., 2014, ICLR 2014; Winston P. H., 1988, LISP, V3; Wu Y, 2017, EMNLP, P1778, DOI [DOI 10.18653/V1/D17-1187, 10.18653/v1/D17-1187]; Yu M, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P545, DOI 10.3115/v1/p14-2089	60	0	0	2	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9944	9959		10.1109/TPAMI.2021.3137564	http://dx.doi.org/10.1109/TPAMI.2021.3137564			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34941498	Green Accepted, Green Submitted			2022-12-18	WOS:000880661400099
J	Mensink, T; Uijlings, J; Kuznetsova, A; Gygli, M; Ferrari, V				Mensink, Thomas; Uijlings, Jasper; Kuznetsova, Alina; Gygli, Michael; Ferrari, Vittorio			Factors of Influence for Transfer Learning Across Diverse Appearance Domains and Task Types	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Transfer learning; Semantics; Image segmentation; Computer vision; Training; Object detection; Transfer learning; computer vision	SEMANTIC SEGMENTATION; OBJECT CLASSES; ADAPTATION	Transfer learning enables to re-use knowledge learned on a source task to help learning a target task. A simple form of transfer learning is common in current state-of-the-art computer vision models, i.e., pre-training a model for image classification on the ILSVRC dataset, and then fine-tune on any target task. However, previous systematic studies of transfer learning have been limited and the circumstances in which it is expected to work are not fully understood. In this paper we carry out an extensive experimental exploration of transfer learning across vastly different image domains (consumer photos, autonomous driving, aerial imagery, underwater, indoor scenes, synthetic, close-ups) and task types (semantic segmentation, object detection, depth estimation, keypoint detection). Importantly, these are all complex, structured output tasks types relevant to modern computer vision applications. In total we carry out over 2000 transfer learning experiments, including many where the source and target come from different image domains, task types, or both. We systematically analyze these experiments to understand the impact of image domain, task type, and dataset size on transfer learning performance. Our study leads to several insights and concrete recommendations: (1) for most tasks there exists a source which significantly outperforms ILSVRC'12 pre-training; (2) the image domain is the most important factor for achieving positive transfer; (3) the source dataset should include the image domain of the target dataset to achieve best results; (4) at the same time, we observe only small negative effects when the image domain of the source task is much broader than that of the target; (5) transfer across task types can be beneficial, but its success is heavily dependent on both the source and target task types.	[Mensink, Thomas] Google Res, Amsterdam, Netherlands; [Uijlings, Jasper; Kuznetsova, Alina; Gygli, Michael; Ferrari, Vittorio] Google Res, Zurich, Switzerland	Google Incorporated	Mensink, T (corresponding author), Google Res, Amsterdam, Netherlands.	mensink@google.com; jrru@google.com; akuznetsa@google.com; michael@gygli.net; vittoferrari@google.com		Mensink, Thomas/0000-0002-5730-713X				Alex Nichol, 2018, Arxiv, DOI arXiv:1803.02999; Alexander Kolesnikov, 2020, Arxiv, DOI arXiv:1910.04867; Dosovitskiy A, 2020, Arxiv, DOI arXiv:2010.11929; Amirreza Shaban, 2017, Arxiv, DOI arXiv:1709.03410; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; [Anonymous], 2012, IMAGENET LARGE SCALE; [Anonymous], COMMON OBJECTS CONTE; [Anonymous], ROBUST VISION CHALLE; Azizpour H, 2016, IEEE T PATTERN ANAL, V38, P1790, DOI 10.1109/TPAMI.2015.2500224; Biggs Benjamin, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P195, DOI 10.1007/978-3-030-58621-8_12; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005; Carlucci FM, 2017, IEEE I CONF COMP VIS, P5077, DOI 10.1109/ICCV.2017.542; Chang WG, 2019, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2019.00753; Chen T, 2020, PR MACH LEARN RES, V119; Chen YL, 2019, INT CONF 3D VISION, P173, DOI 10.1109/3DV.2019.00028; Chu B, 2016, LECT NOTES COMPUT SC, V9915, P435, DOI 10.1007/978-3-319-49409-8_34; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Daiyi Peng, 2018, Arxiv, DOI arXiv:1811.07056; Davidson G., 2020, P IEEE CVF C COMP VI, P9279; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Dequan Wang, 2016, Arxiv, DOI arXiv:1612.02649; Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667; Ericsson L, 2021, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR46437.2021.00537; Everingham M., 2021, PASCAL VISUAL OBJECT; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Finn C, 2017, PR MACH LEARN RES, V70; Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214; Fulton Michael S, 2020, DRUM, DOI 10.13020/X0QN-Y082; Gaidon A, 2016, PROC CVPR IEEE, P4340, DOI 10.1109/CVPR.2016.470; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Ge WF, 2017, PROC CVPR IEEE, P10, DOI 10.1109/CVPR.2017.9; Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Gordon A, 2019, IEEE I CONF COMP VIS, P8976, DOI 10.1109/ICCV.2019.00907; Gygli M., 2020, PROC ASS ADV ARTIF I, P7620; Haider Ali, 2018, Arxiv, DOI arXiv:1712.02286; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huh Minyoung, 2016, ARXIV160808614; Islam MJ, 2020, IEEE INT C INT ROBOT, P1769, DOI 10.1109/IROS45743.2020.9340821; Janoch A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1168, DOI 10.1109/ICCVW.2011.6130382; Jiaxi Wu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P456, DOI 10.1007/978-3-030-58517-4_27; Jong-Chyi Su, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P645, DOI 10.1007/978-3-030-58571-6_38; Joulin A, 2016, LECT NOTES COMPUT SC, V9911, P67, DOI 10.1007/978-3-319-46478-7_5; Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.2307/2332226; Khosla A., 2011, P WORKSH FIN GRAIN V; Kirillov A, 2019, PROC CVPR IEEE, P9396, DOI 10.1109/CVPR.2019.00963; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Koch G., 2015, ICML DEEP LEARN WORK; Kokkinos I, 2017, PROC CVPR IEEE, P5454, DOI 10.1109/CVPR.2017.579; Kolesnikov Alexander, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P491, DOI 10.1007/978-3-030-58558-7_29; Kornblith S, 2019, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2019.00277; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z; Ladicky L, 2014, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2014.19; Lambert J, 2020, PROC CVPR IEEE, P2876, DOI 10.1109/CVPR42600.2020.00295; Lee S, 2021, AAAI CONF ARTIF INTE, V35, P8306; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu Y. -C., 2019, PROC INT C LEARN REP; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Long MS, 2015, PR MACH LEARN RES, V37, P97; Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12; Martin Humenberger, 2020, Arxiv, DOI arXiv:2001.10773; Mensink T, 2013, IEEE T PATTERN ANAL, V35, P2624, DOI 10.1109/TPAMI.2013.83; Liang MJ, 2019, Arxiv, DOI arXiv:1911.12476; Mustafa B., 2021, ARXIV; Nath Kundu Jogendra, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12373, DOI 10.1109/CVPR42600.2020.01239; Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Peng XC, 2018, IEEE COMPUT SOC CONF, P2102, DOI 10.1109/CVPRW.2018.00271; Puigcerver J., 2020, PROC INT C LEARN REP; Qi H, 2018, PROC CVPR IEEE, P5822, DOI 10.1109/CVPR.2018.00610; Raghu Aniruddh, 2020, ICLR; Ravi Sachin, 2017, INT C LEARN REPR, V2, P5; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rottensteiner F, 2014, ISPRS J PHOTOGRAMM, V93, P143, DOI 10.1016/j.isprsjprs.2014.04.009; Roy S, 2019, PROC CVPR IEEE, P9463, DOI 10.1109/CVPR.2019.00970; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Russo P, 2018, PROC CVPR IEEE, P8099, DOI 10.1109/CVPR.2018.00845; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887; Shmelkov K, 2017, IEEE I CONF COMP VIS, P3420, DOI 10.1109/ICCV.2017.368; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Snell J, 2017, ADV NEUR IN, V30; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35; Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97; Sun Yu, 2019, ARXIV; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Uijlings JRR, 2018, PROC CVPR IEEE, P1101, DOI 10.1109/CVPR.2018.00121; Varma G, 2019, IEEE WINT CONF APPL, P1743, DOI 10.1109/WACV.2019.00190; Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572; Vinyals O., 2016, P 30 INT C NEUR INF, P3637, DOI 10.5555/3157382.3157504; Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686; Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929; Wang Xin, 2020, ARXIV200306957; Waqas Zamir S., 2019, P IEEE CVF C COMP VI, P28; Weinzaepfel P, 2019, PROC CVPR IEEE, P5617, DOI 10.1109/CVPR.2019.00578; Weng Lilian, 2018, METALEARNING LEARNIN; Xia GS, 2018, PROC CVPR IEEE, P3974, DOI 10.1109/CVPR.2018.00418; Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458; Xie J, 2016, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2016.401; Yan X, 2020, PROC CVPR IEEE, P3892, DOI 10.1109/CVPR42600.2020.00395; Yonglong Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P266, DOI 10.1007/978-3-030-58568-6_16; Yu F, 2021, AAAI CONF ARTIF INTE, V35, P10754; Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zamir AR, 2018, PROC CVPR IEEE, P3712, DOI 10.1109/CVPR.2018.00391; Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544; Zhou X., 2019, ARXIV, DOI DOI 10.48550/ARXIV.1904.07850; Zhu J, 2019, IEEE I CONF COMP VIS, P3838, DOI 10.1109/ICCV.2019.00394; Zoph Barret, 2020, RETHINKING PRETRAINI, P3	123	0	0	6	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9298	9314		10.1109/TPAMI.2021.3129870	http://dx.doi.org/10.1109/TPAMI.2021.3129870			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34813469	hybrid, Green Submitted			2022-12-18	WOS:000880661400054
J	Okawa, H; Shimano, M; Asano, Y; Bise, R; Nishino, K; Sato, I				Okawa, Hiroki; Shimano, Mihoko; Asano, Yuta; Bise, Ryoma; Nishino, Ko; Sato, Imari			Estimation of Wetness and Color from a Single Multispectral Image	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Sea surface; Image color analysis; Optical surface waves; Absorption; Liquids; Light scattering; Absorption and scattering analysis; surface wetness; monochromatic appearance change; spectral sharpening	SPECTRAL ALBEDO; MODEL; WET; REFLECTANCE; SCATTERING; SNOW; DRY	Recognizing wet surfaces and their degrees of wetness is essential for many computer vision applications. Surface wetness can inform us slippery spots on a road to autonomous vehicles, muddy areas of a trail to humanoid robots, and the freshness of groceries to us. The fact that surfaces darken when wet, i.e., monochromatic appearance change, has been modeled to recognize wet surfaces in the past. In this paper, we show that color change, particularly in its spectral behavior, carries rich information about surface wetness. We first derive an analytical spectral appearance model of wet surfaces that expresses the characteristic spectral sharpening due to multiple scattering and absorption in the surface. We present a novel method for estimating key parameters of this spectral appearance model, which enables the recovery of the original surface color and the degree of wetness from a single multispectral image. Applied to a multispectral image, the method estimates the spatial map of wetness together with the dry spectral distribution of the surface. To our knowledge, this is the first work to model and leverage the spectral characteristics of wet surfaces to decipher its appearance. We conduct comprehensive experimental validation with a number of wet real surfaces. The results demonstrate the accuracy of our model and the effectiveness of our method for surface wetness and color estimation.	[Okawa, Hiroki; Asano, Yuta] Tokyo Inst Technol, Meguro, Tokyo 1528550, Japan; [Shimano, Mihoko; Sato, Imari] Natl Inst Informat, Tokyo, Tokyo 1000003, Japan; [Bise, Ryoma] Kyoto Univ, Kyoto 6068501, Japan; [Nishino, Ko] Natl Inst Informat, Tokyo, Tokyo 1000003, Japan	Tokyo Institute of Technology; Research Organization of Information & Systems (ROIS); National Institute of Informatics (NII) - Japan; Kyoto University; Research Organization of Information & Systems (ROIS); National Institute of Informatics (NII) - Japan	Okawa, H (corresponding author), Tokyo Inst Technol, Meguro, Tokyo 1528550, Japan.	okawa.h.ac@m.titech.ac.jp; miho@nii.ac.jp; asano.y.ac@m.titech.ac.jp; bise@ait.kyushu-u.ac.jp; kon@i.kyoto-u.ac.jp; imarik@nii.ac.jp		Bise, Ryoma/0000-0002-2214-6719; Nishino, Ko/0000-0002-3534-3447	JSPS KAKENHI [JP15H05918]; Office of Naval Research [N00014-16-1-2158 (N00014-14-1-0316)]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); Office of Naval Research(Office of Naval Research)	This work was supported in part by JSPS KAKENHI Grant Number JP15H05918 to I.S., and the Office of Naval Research grant N00014-16-1-2158 (N00014-14-1-0316) to K.N.	Angstrom A., 1925, GEOGR ANN, V7, P323; Asano Y, 2016, LECT NOTES COMPUT SC, V9910, P635, DOI 10.1007/978-3-319-46466-4_38; Born M., 1965, PRINCIPLES OPTICS; GORDON HR, 1988, J GEOPHYS RES-ATMOS, V93, P10909, DOI 10.1029/JD093iD09p10909; Gorodnichev E. E., 1995, Journal of Experimental and Theoretical Physics, V80, P112; Gu JW, 2006, ACM T GRAPHIC, V25, P762, DOI 10.1145/1141911.1141952; Jacques SL, 2013, PHYS MED BIOL, V58, P5007, DOI 10.1088/0031-9155/58/14/5007; Jensen HW, 1999, SPRING EUROGRAP, P273; LEKNER J, 1988, APPL OPTICS, V27, P1278, DOI 10.1364/AO.27.001278; Lu J., 2005, P EUR WORKSH NAT PHE; MALL HB, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P963, DOI 10.1109/ICCV.1995.466830; Mukaigawa Y, 2010, PROC CVPR IEEE, P153, DOI 10.1109/CVPR.2010.5540216; Narasimhan SG, 2006, ACM T GRAPHIC, V25, P1003, DOI 10.1145/1141911.1141986; Sawayama M, 2017, J VISION, V17, DOI 10.1167/17.5.7; Shimano M, 2017, PROC CVPR IEEE, P321, DOI 10.1109/CVPR.2017.42; Stam J, 1995, SPRING COMP SCI, P41; Sun B, 2007, IEEE T VIS COMPUT GR, V13, P595, DOI 10.1109/TVCG.2007.1013; Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349; Tuchin V. V, 1994, SELECTED PAPERS TISS; TWOMEY SA, 1986, APPL OPTICS, V25, P431, DOI 10.1364/AO.25.000431; WARREN SG, 1980, J ATMOS SCI, V37, P2734, DOI 10.1175/1520-0469(1980)037<2734:AMFTSA>2.0.CO;2; WISCOMBE WJ, 1980, J ATMOS SCI, V37, P2712, DOI 10.1175/1520-0469(1980)037<2712:AMFTSA>2.0.CO;2; Yacoob Y, 2013, IEEE I CONF COMP VIS, P2952, DOI 10.1109/ICCV.2013.367; Zhang H, 2006, APPL OPTICS, V45, P8753, DOI 10.1364/AO.45.008753	24	0	0	0	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					8740	8753		10.1109/TPAMI.2019.2903496	http://dx.doi.org/10.1109/TPAMI.2019.2903496			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	30843820				2022-12-18	WOS:000880661400017
J	Pokala, PK; Hemadri, RV; Seelamantula, CS				Pokala, Praveen Kumar; Hemadri, Raghu Vamshi; Seelamantula, Chandra Sekhar			Iteratively Reweighted Minimax-Concave Penalty Minimization for Accurate Low-rank Plus Sparse Matrix Decomposition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Nuclear-norm minimization; minimax-concave penalty; weighted l(1) minimization; equivalent minimax-concave penalty; low-rank and sparse matrix decomposition; nonconvex penalty	NUCLEAR NORM MINIMIZATION; VARIABLE SELECTION; ALGORITHMS; REGULARIZATION; OPTIMIZATION; COMPLETION; SEPARATION; GRADIENT	Low-rank plus sparse matrix decomposition (LSD) is an important problem in computer vision and machine learning. It has been solved using convex relaxations of the matrix rank and l(0)-pseudo-norm, which are the nuclear norm and l(1)-norm, respectively. Convex approximations are known to result in biased estimates, to overcome which, nonconvex regularizers such as weighted nuclear-norm minimization and weighted Schatten p-norm minimization have been proposed. However, works employing these regularizers have used heuristic weight-selection strategies. We propose weighted minimax-concave penalty (WMCP) as the nonconvex regularizer and show that it admits an equivalent representation that enables weight adaptation. Similarly, an equivalent representation to the weighted matrix gamma norm (WMGN) enables weight adaptation for the low-rank part. The optimization algorithms are based on the alternating direction method of multipliers technique. We show that the optimization frameworks relying on the two penalties, WMCP and WMGN, coupled with a novel iterative weight update strategy, result in accurate low-rank plus sparse matrix decomposition. The algorithms are also shown to satisfy descent properties and convergence guarantees. On the applications front, we consider the problem of foreground-background separation in video sequences. Simulation experiments and validations on standard datasets, namely, I2R, CDnet 2012, and BMC 2012 show that the proposed techniques outperform the benchmark techniques.	[Pokala, Praveen Kumar; Seelamantula, Chandra Sekhar] Indian Inst Sci, Dept Elect Engn, Bengaluru 560012, Karnataka, India; [Hemadri, Raghu Vamshi] Natl Inst Technol, Dept Elect & Commun Engn, Fathimanagar 506004, Telangana, India	Indian Institute of Science (IISC) - Bangalore; National Institute of Technology (NIT System); National Institute of Technology Warangal	Pokala, PK (corresponding author), Indian Inst Sci, Dept Elect Engn, Bengaluru 560012, Karnataka, India.	praveenkumar.pokala@gmail.com; vamshi.hemadri@gmail.com; chandra.sekhar@ieee.org		POKALA, PRAVEEN KUMAR/0000-0001-5513-5891; Seelamantula, Chandra Sekhar/0000-0001-9049-1912; Vamshi, Hemadri Raghu/0000-0002-0272-9714	Visvesvaraya fellowship from Ministry of Electronics and Information Technology; Mathematical Research Impact Centric Support (MATRICS) scheme; Science and Engineering Research Board, India	Visvesvaraya fellowship from Ministry of Electronics and Information Technology; Mathematical Research Impact Centric Support (MATRICS) scheme; Science and Engineering Research Board, India	This work was supported in part by the Visvesvaraya fellowship from Ministry of Electronics and Information Technology, in part by the Mathematical Research Impact Centric Support (MATRICS) scheme, and in part by the Core Research Grant by the Science and Engineering Research Board, India. (Corresponding author: Praveen Kumar Pokala.)	Agarwal A., 2010, P ADV NEUR INF PROC, P37; Angang Cui, 2019, Arxiv, DOI arXiv:1807.01276; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Bei Li, 2018, Arxiv, DOI arXiv:1702.04463; Bouwmans T, 2018, IEEE J-STSP, V12, P1127, DOI 10.1109/JSTSP.2018.2879245; Bouwmans T, 2017, COMPUT SCI REV, V23, P1, DOI 10.1016/j.cosrev.2016.11.001; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Candes EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x; Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793; Chartrand R, 2012, IEEE T SIGNAL PROCES, V60, P5810, DOI 10.1109/TSP.2012.2208955; Chen Lixiat, 2019, Journal of Computer Applications, V39, P1170, DOI 10.11772/j.issn.1001-9081.2018092038; Chi YJ, 2019, IEEE T SIGNAL PROCES, V67, P5239, DOI 10.1109/TSP.2019.2937282; Cho-Ying Wu, 2019, Arxiv, DOI arXiv:1906.02433; Ebadi SE, 2016, LECT NOTES COMPUT SC, V9905, P314, DOI 10.1007/978-3-319-46448-0_19; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Fazel M, 2001, P AMER CONTR CONF, P4734, DOI 10.1109/ACC.2001.945730; Gao C., 2011, PROC ASS ADV ARTIF I, P356; Gao Z, 2012, LECT NOTES COMPUT SC, V7576, P690, DOI 10.1007/978-3-642-33715-4_50; github, TNNM CODE; github, WSNM CODE; google, WNNM CODE; google, NNM CODE; Goyette N., 2012, 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), DOI 10.1109/CVPRW.2012.6238919; Gu SH, 2017, INT J COMPUT VISION, V121, P183, DOI 10.1007/s11263-016-0930-5; Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366; Guo XJ, 2014, LECT NOTES COMPUT SC, V8695, P535, DOI 10.1007/978-3-319-10584-0_35; He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848; Hsu D, 2011, IEEE T INFORM THEORY, V57, P7221, DOI 10.1109/TIT.2011.2158250; Hu Y, 2013, IEEE T PATTERN ANAL, V35, P2117, DOI 10.1109/TPAMI.2012.271; Hu ZX, 2021, NEURAL NETWORKS, V136, P218, DOI 10.1016/j.neunet.2020.09.021; Javed S, 2018, 2018 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP), P836; Javed S, 2016, INT C PATT RECOG, P120, DOI 10.1109/ICPR.2016.7899619; Javed S, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P930, DOI 10.1109/ICCVW.2015.123; Ji S., 2009, P 26 ANN INT C MACH, P457, DOI DOI 10.1145/1553374.1553434; Jia XX, 2019, INFORM SCIENCES, V476, P83, DOI 10.1016/j.ins.2018.10.003; Jiang WH, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3590; Kanagal B., 2010, P ADV NEUR INF PROC, P10; Larsson V, 2016, INT J COMPUT VISION, V120, P194, DOI 10.1007/s11263-016-0904-7; Lee J., 2013, PROC 30 INT C MACH L, P82; Lin Z., 2009, P IEEE INT WORKSH CO, P1; Liu RS, 2014, NEUROCOMPUTING, V142, P529, DOI 10.1016/j.neucom.2014.03.046; Liu YJ, 2012, MATH PROGRAM, V133, P399, DOI 10.1007/s10107-010-0437-8; Lu CY, 2016, IEEE T IMAGE PROCESS, V25, P829, DOI 10.1109/TIP.2015.2511584; Lu CY, 2014, PROC CVPR IEEE, P4130, DOI 10.1109/CVPR.2014.526; Marjanovic G, 2012, IEEE T SIGNAL PROCES, V60, P5714, DOI 10.1109/TSP.2012.2212015; Mazumder R, 2010, J MACH LEARN RES, V11, P2287; Minming Chen, 2013, Arxiv, DOI arXiv:1009.5055; Mohimani H, 2009, IEEE T SIGNAL PROCES, V57, P289, DOI 10.1109/TSP.2008.2007606; Narayanamurthy P, 2019, IEEE T INFORM THEORY, V65, P1547, DOI 10.1109/TIT.2018.2872023; Narayanamurthy P, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4684; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; Netrapalli P., 2014, P ADV NEUR INF PROC, P1107; Nie F., 2012, PROC 26 AAAI C ARTIF, P655; Oh TH, 2015, PROC CVPR IEEE, P4484, DOI 10.1109/CVPR.2015.7299078; Oh TH, 2015, IEEE T PATTERN ANAL, V37, P1219, DOI 10.1109/TPAMI.2014.2361338; Otazo R, 2015, MAGN RESON MED, V73, P1125, DOI 10.1002/mrm.25240; Panda R, 2017, IEEE T MULTIMEDIA, V19, P2010, DOI 10.1109/TMM.2017.2708981; Parekh A, 2016, IEEE SIGNAL PROC LET, V23, P493, DOI 10.1109/LSP.2016.2535227; Parikh N., 2014, FDN TRENDS OPTIM, V1, P127, DOI DOI 10.1561/2400000003; Paterek A., 2007, P ACM KDD CUP WORKSH, P5, DOI DOI 10.1145/1557019.1557072; Pokala PK, 2020, I S BIOMED IMAGING, P1929, DOI 10.1109/ISBI45749.2020.9098517; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Rodriguez P, 2016, J MATH IMAGING VIS, V55, P1, DOI 10.1007/s10851-015-0610-z; Selesnick I, 2017, IEEE T SIGNAL PROCES, V65, P4481, DOI 10.1109/TSP.2017.2711501; Selesnick IW, 2014, IEEE T SIGNAL PROCES, V62, P1078, DOI 10.1109/TSP.2014.2298839; Shang FH, 2018, IEEE T PATTERN ANAL, V40, P2066, DOI 10.1109/TPAMI.2017.2748590; Sobral A., 2016, LRSLIBRARY LOW RANK; Sobral A., 2015, PROC 12 IEEE INT C A, P1, DOI [10.1109/AVSS.2015.7301753, DOI 10.1109/AVSS.2015.7301753]; Sun RY, 2016, IEEE T INFORM THEORY, V62, P6535, DOI 10.1109/TIT.2016.2598574; Toh KC, 2010, PAC J OPTIM, V6, P615; Vacavant A., 2012, P AS C COMP VIS, P291; Vaswani N, 2018, P IEEE, V106, P1274, DOI 10.1109/JPROC.2018.2853498; Vidal R, 2014, PATTERN RECOGN LETT, V43, P47, DOI 10.1016/j.patrec.2013.08.006; Wang S., 2013, IJCAI, P1764; Waters A. E., 2011, 25 ANN C NEURAL INFO, P1089; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Wright Y., 2009, ADV NEURAL INFORM PR, V22, DOI DOI 10.5555/2984093.2984326; Wu L, 2011, LECT NOTES COMPUT SC, V6494, P703, DOI 10.1007/978-3-642-19318-7_55; Xie Y, 2016, IEEE T IMAGE PROCESS, V25, P4842, DOI 10.1109/TIP.2016.2599290; Xu P., 2017, P ADV NEUR INF PROC, P1930; Xue ZC, 2019, VISUAL COMPUT, V35, P1549, DOI 10.1007/s00371-018-1555-1; Yao QM, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4002; Yi JR, 2020, IEEE T INFORM THEORY, V66, P6597, DOI 10.1109/TIT.2020.2990948; Yin M, 2016, IEEE T PATTERN ANAL, V38, P504, DOI 10.1109/TPAMI.2015.2462360; Yu A. W., 2014, P ADV NEUR INF PROC, P1350; Zarmehi N, 2020, IEEE T CIRC SYST VID, V30, P2046, DOI 10.1109/TCSVT.2019.2923816; Zha ZY, 2019, IEEE IMAGE PROC, P2050, DOI 10.1109/ICIP.2019.8803145; Zha ZY, 2018, NEUROCOMPUTING, V311, P209, DOI 10.1016/j.neucom.2018.05.073; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; Zhang HM, 2020, IEEE T IMAGE PROCESS, V29, P3132, DOI 10.1109/TIP.2019.2957925; Zhang T, 2010, J MACH LEARN RES, V11, P1081; Zhang ZD, 2012, INT J COMPUT VISION, V99, P1, DOI 10.1007/s11263-012-0515-x; Zhao Q, 2014, PR MACH LEARN RES, V32, P55; Zheng YQ, 2012, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2012.6247828; Zhong XW, 2015, AAAI CONF ARTIF INTE, P1980; Zhu L., 2020, ACM T MULTIM COMPUT, V16, P1	101	0	0	6	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					8992	9010		10.1109/TPAMI.2021.3122259	http://dx.doi.org/10.1109/TPAMI.2021.3122259			19	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34699349				2022-12-18	WOS:000880661400033
J	Punnappurath, A; Brown, MS				Punnappurath, Abhijith; Brown, Michael S.			A Little Bit More: Bitplane-Wise Bit-Depth Recovery	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Training; Quantization (signal); Deep learning; Image restoration; Standards; Task analysis; Network architecture; Bit depth; bitplane; quantization; image restoration		Imaging sensors digitize incoming scene light at a dynamic range of 10-12 bits (i.e., 1024-4096 tonal values). The sensor image is then processed onboard the camera and finally quantized to only 8 bits (i.e., 256 tonal values) to conform to prevailing encoding standards. There are a number of important applications, such as high-bit-depth displays and photo editing, where it is beneficial to recover the lost bit depth. Deep neural networks are effective at this bit-depth reconstruction task. Given the quantized low-bit-depth image as input, existing deep learning methods employ a single-shot approach that attempts to either (1) directly estimate the high-bit-depth image, or (2) directly estimate the residual between the high- and low-bit-depth images. In contrast, we propose a training and inference strategy that recovers the residual image bitplane-by-bitplane. Our bitplane-wise learning framework has the advantage of allowing for multiple levels of supervision during training and is able to obtain state-of-the-art results using a simple network architecture. We test our proposed method extensively on several image datasets and demonstrate an improvement from 0.5dB to 2.3dB PSNR over prior methods depending on the quantization level.	[Punnappurath, Abhijith; Brown, Michael S.] Samsung AI Ctr, Toronto, ON M5G 1L7, Canada		Punnappurath, A (corresponding author), Samsung AI Ctr, Toronto, ON M5G 1L7, Canada.	abhijith.p@samsung.com; michael.b1@samsung.com						[Anonymous], IPHONE X TECHNICAL S; [Anonymous], GRAPHICS INTERCHANGE; [Anonymous], US; [Anonymous], 2015, TENSORFLOW LARGE SCA; Asuni Nicola, 2014, P SMART TOOLS APPS G; Bychkovsky V, 2011, PROC CVPR IEEE, P97; Byun J, 2019, LECT NOTES COMPUT SC, V11362, P67, DOI 10.1007/978-3-030-20890-5_5; Cheng CH, 2009, IEEE INT SYMP CIRC S, P944, DOI 10.1109/ISCAS.2009.5117913; Eastman Kodak, 1999, KODAK LOSSLESS TRUE; Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816; Guoping Qiu, 2017, Arxiv, DOI arXiv:1707.00116; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Karaimer HC, 2016, LECT NOTES COMPUT SC, V9905, P429, DOI 10.1007/978-3-319-46448-0_26; Kim SY, 2019, IEEE I CONF COMP VIS, P3116, DOI 10.1109/ICCV.2019.00321; Kingma D, 2014, COMPUTER SCI; Kundu D, 2015, IEEE IMAGE PROC, P2374, DOI 10.1109/ICIP.2015.7351227; Li YZ, 2005, ACM T GRAPHIC, V24, P836, DOI 10.1145/1073204.1073271; Liu J., 2018, DIGIT TVWIREL MULTIM, P255; Liu J, 2019, IEEE T IMAGE PROCESS, V28, P4926, DOI 10.1109/TIP.2019.2912294; Liu J, 2018, IEEE T IMAGE PROCESS, V27, P4860, DOI 10.1109/TIP.2018.2803306; Mittal G., 2012, P IEEE VIS COMM IM P, P1, DOI DOI 10.1109/VCIP.2012.6410837; Mizuno A, 2016, INT CONF ACOUST SPEE, P1671, DOI 10.1109/ICASSP.2016.7471961; Pengfei Wan, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P170, DOI 10.1109/ICME.2012.118; Sharma G., 2002, DIGITAL COLOR IMAGIN; Su YT, 2019, NEUROCOMPUTING, V347, P200, DOI 10.1016/j.neucom.2019.04.011; Ulichney R, 1998, P SOC PHOTO-OPT INS, V3300, P232, DOI 10.1117/12.298285; Wan PF, 2016, IEEE T IMAGE PROCESS, V25, P2896, DOI 10.1109/TIP.2016.2553523; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xiao Y, 2019, LECT NOTES COMPUT SC, V11364, P207, DOI 10.1007/978-3-030-20870-7_13; Xiph Foundation, US; Zhao Y, 2019, IEEE T IMAGE PROCESS, V28, P2847, DOI 10.1109/TIP.2019.2891131	32	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9718	9724		10.1109/TPAMI.2021.3125692	http://dx.doi.org/10.1109/TPAMI.2021.3125692			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34748481	Green Submitted			2022-12-18	WOS:000880661400082
J	Rezatofighi, H; Zhu, TY; Kaskman, R; Motlagh, FT; Shi, JQ; Milan, A; Cremers, D; Leal-Taixe, L; Reid, I				Rezatofighi, Hamid; Zhu, Tianyu; Kaskman, Roman; Motlagh, Farbod T.; Shi, Javen Qinfeng; Milan, Anton; Cremers, Daniel; Leal-Taixe, Laura; Reid, Ian			Learn to Predict Sets Using Feed-Forward Neural Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Random finite set; deep learning; unstructured data; permutation; image tagging; object detection; CAPTCHA		This paper addresses the task of set prediction using deep feed-forward neural networks. A set is a collection of elements which is invariant under permutation and the size of a set is not fixed in advance. Many real-world problems, such as image tagging and object detection, have outputs that are naturally expressed as sets of entities. This creates a challenge for traditional deep neural networks which naturally deal with structured outputs such as vectors, matrices or tensors. We present a novel approach for learning to predict sets with unknown permutation and cardinality using deep neural networks. In our formulation we define a likelihood for a set distribution represented by a) two discrete distributions defining the set cardinally and permutation variables, and b) a joint distribution over set elements with a fixed cardinality. Depending on the problem under consideration, we define different training models for set prediction using deep neural networks. We demonstrate the validity of our set formulations on relevant vision problems such as: 1) multi-label image classification where we outperform the other competing methods on the PASCAL VOC and MS COCO datasets, 2) object detection, for which our formulation outperforms popular state-of-the-art detectors, and 3) a complex CAPTCHA test, where we observe that, surprisingly, our set-based network acquired the ability of mimicking arithmetics without any rules being coded.	[Rezatofighi, Hamid] Monash Univ, Fac Informat Technol, Dept Data Sci & AI, Melbourne, Vic 3800, Australia; [Zhu, Tianyu] Monash Univ, Dept Elect & Comp Syst Engn, Melbourne, Vic 3800, Australia; [Kaskman, Roman; Cremers, Daniel; Leal-Taixe, Laura] Tech Univ Munich, D-80333 Munich, Germany; [Motlagh, Farbod T.; Shi, Javen Qinfeng; Reid, Ian] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia; [Milan, Anton] Amazon, Seattle, WA 98109 USA	Monash University; Monash University; Technical University of Munich; University of Adelaide; Amazon.com	Rezatofighi, H (corresponding author), Monash Univ, Fac Informat Technol, Dept Data Sci & AI, Melbourne, Vic 3800, Australia.	hamid.rezatofighi@monash.edu; Tianyu.Zhu@monash.edu; roman.kaskman@gmail.com; farbod.motlagh@student.adelaide.edu.au; javen.shi@adelaide.edu.au; anton.a.milan@gmail.com; cremers@tum.de; leal.taixe@tum.de; ian.reid@adelaide.edu.au	Leal-Taixe, Laura/HFZ-8079-2022	Leal-Taixe, Laura/0000-0001-8709-1133; Zhu, Tianyu/0000-0002-6735-0553; Rezatofighi, Hamid/0000-0002-8659-8773	Australian Research Council through Laureate Fellowship [FL130100102]; Australian Research Council through Centre of Excellence for Robotic Vision [CE140100016]; Sofja Kovalevskaja Award from the Humboldt Foundation	Australian Research Council through Laureate Fellowship(Australian Research Council); Australian Research Council through Centre of Excellence for Robotic Vision(Australian Research Council); Sofja Kovalevskaja Award from the Humboldt Foundation(Alexander von Humboldt Foundation)	Ian Reid and Hamid Rezatofighi gratefully acknowledge the support the Australian Research Council through Laureate Fellowship FL130100102 and Centre of Excellence for Robotic Vision CE140100016. This work was also partially funded by the Sofja Kovalevskaja Award from the Humboldt Foundation.	Adams R. P., 2009, P 26 ANN INT C MACH, P9, DOI DOI 10.1145/1553374.1553376; Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110; Alexander Toshev, 2014, Arxiv, DOI arXiv:1312.4894; Alexey Bochkovskiy, 2020, Arxiv, DOI arXiv:2004.10934; Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Anton Milan, 2016, Arxiv, DOI arXiv:1603.00831; Ba-Ngu Vo, 2017, Arxiv, DOI arXiv:1703.02155; Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13; Dollar P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Hannah LA, 2011, J MACH LEARN RES, V12, P1923; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Hosang J, 2017, PROC CVPR IEEE, P6469, DOI 10.1109/CVPR.2017.685; Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Leal-Taixe L., 2015, ARXIV; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu Y., 2019, ARXIV; Manjunath Kudlur, 2016, Arxiv, DOI arXiv:1511.06391; Mnih V, 2013, PLAYING ATARI DEEP R; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Muandet K., 2012, PROC 25 INT C NEURAL, P10; Murphy R. L., 2019, PROC INT C LEARN REP; Tran NQ, 2016, INT C PATT RECOG, P3174, DOI 10.1109/ICPR.2016.7900123; Oliva J., 2013, PROC 30 INT C MACH L, P1049; Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203; Qi CR, 2017, ADV NEUR IN, V30; Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16; Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075; Rezatofighi SH, 2018, AAAI CONF ARTIF INTE, P3968; Rezatofighi SH, 2017, IEEE I CONF COMP VIS, P5257, DOI 10.1109/ICCV.2017.561; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sadeghian A, 2019, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2019.00144; Skianis K, 2020, PR MACH LEARN RES, V108; Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tchapmi LP, 2019, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2019.00047; Vaswani A, 2017, ADV NEUR IN, V30; Vo BN, 2016, INT C PATT RECOG, P2622, DOI 10.1109/ICPR.2016.7900030; Wang JF, 2021, PROC CVPR IEEE, P15844, DOI 10.1109/CVPR46437.2021.01559; Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251; Zaheer M, 2017, ADV NEUR IN, V30; Zhang Y, 2019, ADV NEUR IN, V32	54	0	0	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9011	9025		10.1109/TPAMI.2021.3122970	http://dx.doi.org/10.1109/TPAMI.2021.3122970			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34705634	Green Submitted			2022-12-18	WOS:000880661400034
J	Shao, HJ; Xiao, ZS; Yao, SC; Sun, DC; Zhang, A; Liu, SZ; Wang, TS; Li, JY; Abdelzaher, T				Shao, Huajie; Xiao, Zhisheng; Yao, Shuochao; Sun, Dachun; Zhang, Aston; Liu, Shengzhong; Wang, Tianshi; Li, Jinyang; Abdelzaher, Tarek			ControlVAE: Tuning, Analytical Properties, and Performance Analysis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image reconstruction; Training; PI control; Optimization; Image synthesis; Task analysis; Tuning; Variational autoencoders (VAEs); ControlVAE; PID controller; disentangled representation learning; language modeling; image generation		This paper reviews the novel concept of a controllable variational autoencoder (ControlVAE), discusses its parameter tuning to meet application needs, derives its key analytic properties, and offers useful extensions and applications. ControlVAE is a new variational autoencoder (VAE) framework that combines automatic control theory with the basic VAE to stabilize the KL-divergence of VAE models to a specified value. It leverages a non-linear PI controller, a variant of the proportional-integral-derivative (PID) controller, to dynamically tune the weight of the KL-divergence term in the evidence lower bound (ELBO) using the output KL-divergence as feedback. This allows us to precisely control the KL-divergence to a desired value (set point) that is effective in avoiding posterior collapse and learning disentangled representations. While prior work developed alternative techniques for controlling the KL divergence, we show that our PI controller has better stability properties and thus better convergence, thereby producing better disentangled representations from finite training data. In order to improve the ELBO of ControlVAE over that of the regular VAE, we provide a simplified theoretical analysis to inform the choice of set point for the KL-divergence of ControlVAE. We evaluate the proposed method on three tasks: image generation, language modeling, and disentangled representation learning. The results show that ControlVAE can achieve much better reconstruction quality than the other methods for comparable disentanglement. On the language modeling task, our method can avoid posterior collapse (KL vanishing) and improve the diversity of generated text. Moreover, it can change the optimization trajectory, improving the ELBO and the reconstruction quality for image generation.	[Shao, Huajie] Coll William & Mary, Dept Comp Sci, Williamsburg, VA 23185 USA; [Xiao, Zhisheng] Univ Chicago, Comm Computat & Appl Math, Chicago, IL 60637 USA; [Yao, Shuochao; Sun, Dachun; Liu, Shengzhong; Wang, Tianshi; Li, Jinyang; Abdelzaher, Tarek] Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA; [Zhang, Aston] Amazon Web Serv, Palo Alto, CA 94301 USA	William & Mary; University of Chicago; University of Illinois System; University of Illinois Urbana-Champaign; Amazon.com	Shao, HJ (corresponding author), Coll William & Mary, Dept Comp Sci, Williamsburg, VA 23185 USA.; Abdelzaher, T (corresponding author), Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.	hshao@wm.edu; zxiao@uchicago.edu; syao9@illinois.edu; dsun18@illinois.edu; astonz@amazon.com; s129@illinois.edu; tianshi3@illinois.edu; jinyang7@illinois.edu; zaher@illinois.edu		Sun, Dachun/0000-0003-4000-2783; Xiao, Zhisheng/0000-0001-7227-8552	DARPA [W911NF-17-C-0099]; DTRA [HDTRA1-18-1-0026]; Army Research Laboratory [W911NF-09-2-0053, W911NF-17-2-0196]; NSF [CPS 20-38817]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DTRA(United States Department of DefenseDefense Threat Reduction Agency); Army Research Laboratory(United States Department of DefenseUS Army Research Laboratory (ARL)); NSF(National Science Foundation (NSF))	This work was supported in part by DARPA Award W911NF-17-C-0099, DTRA Award HDTRA1-18-1-0026, and the Army Research Laboratory under Cooperative Agreements W911NF-09-2-0053 and W911NF-17-2-0196, and in part by NSF under Award CPS 20-38817.	Alexander Lerchner, 2018, Arxiv, DOI arXiv:1804.03599; Andrea Asperti, 2020, Arxiv, DOI arXiv:2002.07514; Azar AT, 2015, STUD FUZZ SOFT COMP, V319, P1, DOI 10.1007/978-3-319-12883-2_1; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Chen T.Q., 2018, NEURIPS, P2610; Czum JM, 2020, J AM COLL RADIOL, V17, P637, DOI 10.1016/j.jacr.2020.02.005; Dai B., 2018, PROC INT C LEARN REP; Dai B., 2018, ARXIV; Danilo Jimenez Rezende, 2018, Arxiv, DOI arXiv:1810.00597; Do K., P INT C LEARN REPR; Fortin M., 1983, Augmented lagrangian methods: Applications to the numerical solution of boundary-value problems; Fu H., 2019, P 2019 C N AM CHAPT, V1, P240, DOI DOI 10.18653/V1/N19-1021; Ghosh P., 2020, 8 INT C LEARN REPR I; Godfrey J.J., 1997, SWITCHBOARD 1 RELEAS; Hellerstein J., 2004, FEEDBACK CONTROL COM; Higgins I, 2016, BETA VAE LEARNING BA; Hu ZT, 2019, PROCEEDINGS OF THE 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, (ACL 2019), P159; Hu ZT, 2017, PR MACH LEARN RES, V70; JELINEK F, 1977, J ACOUST SOC AM, V62, pS63, DOI 10.1121/1.2016299; Kim H, 2018, PR MACH LEARN RES, V80; Kingma D.P., 2013, P 2 INT C LEARN REPR; Rydhmer K, 2021, Arxiv, DOI arXiv:2102.05526; Klushyn A, 2019, PROC INT C NEURAL IN, P2866; Liu MY, 2017, ADV NEUR IN, V30; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lucic M, 2018, ADV NEUR IN, V31; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Peng X. B., 2019, PROC INT C LEARN REP, P1810; Peng YB, 1996, IEEE CONTR SYST MAG, V16, P48, DOI 10.1109/37.526915; Platt J. C., 1987, PROC INT C NEURAL IN, P612; Rezende D. J., 2018, PROC WORKSHOP BAYESI, P2; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Rolinek M, 2019, PROC CVPR IEEE, P12398, DOI 10.1109/CVPR.2019.01269; Shao H, 2020, PROC INT C MACH LEAR, P8655; Stooke A., 2020, PROC INT C MACH LEAR, P9133; str_om K. J. A, 2006, ISA INSTRUM SYST AUT; van Rozendaal T, 2020, PROC IEEE CVF C COMP, P166; Vaswani A, 2017, ADV NEUR IN, V30; Wang Wenlin, 2019, PROC C N AM CHAPTER, P166; WATANABE S, 1960, IBM J RES DEV, V4, P66, DOI 10.1147/rd.41.0066; Xu J., 2018, ARXIV; Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47; Zhao SJ, 2019, AAAI CONF ARTIF INTE, P5885; Zhao TC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P654, DOI 10.18653/v1/P17-1061; Zhu YM, 2018, ACM/SIGIR PROCEEDINGS 2018, P1097, DOI 10.1145/3209978.3210080	51	0	0	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9285	9297		10.1109/TPAMI.2021.3127323	http://dx.doi.org/10.1109/TPAMI.2021.3127323			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34788217	Green Submitted			2022-12-18	WOS:000880661400053
J	Sheinin, M; Schechner, YY; Kutulakos, KN				Sheinin, Mark; Schechner, Yoav Y.; Kutulakos, Kiriakos N.			Computational Imaging on the Electric Grid	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Lighting; Cameras; Light sources; Visualization; Photodiodes; Urban areas; AC illumination; bulb flicker; bulb response function; light transport; light source separation; AC phase recovery; coded exposure; reflection removal	PHOTOGRAPHY; REFLECTANCE; FLICKER; SPACE; FLASH; SHAPE	Night beats with alternating current (AC) illumination. By passively sensing this beat, we reveal new scene information which includes: the type of bulbs in the scene, the phases of the electric grid up to city scale, and the light transport matrix. This information yields unmixing of reflections and semi-reflections, nocturnal high dynamic range, and scene rendering with bulbs not observed during acquisition. The latter is facilitated by a dataset of bulb response functions for a range of sources, which we collected and provide. To do all this, we built a novel coded-exposure high-dynamic-range imaging technique, specifically designed to operate on the grid's AC lighting.	[Sheinin, Mark; Schechner, Yoav Y.] Techn Israel Inst Technol, Viterbi Fac Elect Engn, IL-3200003 Haifa, Israel; [Kutulakos, Kiriakos N.] Univ Toronto, Dept Comp Sci, Toronto, ON M5S IA1, Canada	University of Toronto	Sheinin, M (corresponding author), Techn Israel Inst Technol, Viterbi Fac Elect Engn, IL-3200003 Haifa, Israel.	marksheinin@gmail.com; yoav@ee.technion.ac.il; kyros@cs.toronto.edu		Kutulakos, Kiriakos/0000-0002-5165-902X; Schechner, Yoav/0000-0002-4022-7037	Taub Foundation; Israel Science Foundation [542/16]; BMBF; Mitacs CanadaIsrael Globalink Innovation Initiative; Natural Sciences and Engineering Research Council of Canada; DARPA under the REVEAL program	Taub Foundation; Israel Science Foundation(Israel Science Foundation); BMBF(Federal Ministry of Education & Research (BMBF)); Mitacs CanadaIsrael Globalink Innovation Initiative; Natural Sciences and Engineering Research Council of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)CGIAR); DARPA under the REVEAL program	The authors thank M. O'Toole, Y. Levron, R. Swanson, P. Lehn, Z. Tate, A. Levin and G. Tennenholtz for useful discussions, V. Holodovsky and R. Zamir for help with experiments, K. Lavia and I. Talmon for technical support. Y. Y. Schechner is a Landau Fellow-supported by the Taub Foundation. This research was supported by the Israel Science Foundation (Grant 542/16) and conducted in the Ollendorff Minerva Center. Minerva is funded through the BMBF. M. Sheinin was partly supported by the Mitacs CanadaIsrael Globalink Innovation Initiative. K. N. Kutulakos gratefully acknowledges the support of the Natural Sciences and Engineering Research Council of Canada under the RGPIN and SGP programs, as well as the support of DARPA under the REVEAL program.	Agrawal A, 2005, ACM T GRAPHIC, V24, P828, DOI 10.1145/1073204.1073269; Alterman M., 2010, PROC IEEE INT C COMP, P1; Andersson N., 1994, Lighting Research and Technology, V26, P157; [Anonymous], 2017, COMPUTATIONAL IMAGIN; Batchelor BG, 2012, MACHINE VISION HANDBOOK, VOLS 1-3, P283, DOI 10.1007/978-1-84996-169-1_7; Bianco FB, 2016, BUILDSYS'16: PROCEEDINGS OF THE 3RD ACM CONFERENCE ON SYSTEMS FOR ENERGY-EFFCIENT BUILT ENVIRONMENTS, P61, DOI 10.1145/2993422.2993570; Cula OG, 2005, PROC CVPR IEEE, P1116; Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778; Dana KJ, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P460, DOI 10.1109/ICCV.2001.937661; Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855; Ghosh A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024163; Grainger J. J., 1994, POWER SYSTEM ANAL; Grossberg MD, 2003, PROC CVPR IEEE, P602; Guterman H., 2014, PROC 28 CONV IEEE EL, P1; Hatzitheodorou M, 1998, J COMPLEXITY, V14, P63, DOI 10.1006/jcom.1997.0448; Heide F, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461945; Hitomi Y, 2011, IEEE I CONF COMP VIS, P287, DOI 10.1109/ICCV.2011.6126254; Hsu E, 2008, ACM T GRAPHIC, V27, DOI [10.1145/1399504.1360669, 10.1145/1360612.1360669]; Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5; Jo K, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2896818; Joze HRV, 2014, IEEE T PATTERN ANAL, V36, P860, DOI 10.1109/TPAMI.2013.169; KIM B, 1991, CVGIP-IMAG UNDERSTAN, V54, P416, DOI 10.1016/1049-9660(91)90040-V; Kitsinelis S., 2013, OPT PHOTON J, V3; Kitsinelis S, 2012, LIGHT ENG, V20, P25; Kolaman A, 2016, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2016.402; Kukaka L., 2015, PROC IEEE IND APPL S, P1; Levin A, 2007, IEEE T PATTERN ANAL, V29, P1647, DOI 10.1109/TPAMI.2007.1106; Li B, 2014, IEEE T IMAGE PROCESS, V23, P1194, DOI 10.1109/TIP.2013.2277943; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Masi M. G., 2008, PROC IEEE INT C HARM, P1; Mordechay M, 2014, 2014 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P684, DOI 10.1109/GlobalSIP.2014.7032205; Moreno D, 2015, PROC CVPR IEEE, P2301, DOI 10.1109/CVPR.2015.7298843; Narasimhan SG, 2002, LECT NOTES COMPUT SC, V2352, P148; Narasimhan SG, 2006, ACM T GRAPHIC, V25, P1003, DOI 10.1145/1141911.1141986; O'Toole M, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2016.2545662; O'Toole M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766897; Papadhimitri T., 2014, PROC BRIT MACH VIS C, P1; Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777; Raskar R, 2006, ACM T GRAPHIC, V25, P795, DOI 10.1145/1141911.1141957; Ratner N, 2007, OPT EXPRESS, V15, P17072, DOI 10.1364/OE.15.017072; Reddy D, 2011, PROC CVPR IEEE, P329, DOI 10.1109/CVPR.2011.5995542; Reno V, 2017, IEEE EMBED SYST LETT, V9, P97, DOI 10.1109/LES.2017.2755443; Sakakibara N, 2018, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2018.00670; Salvi J, 2004, PATTERN RECOGN, V37, P827, DOI 10.1016/j.patcog.2003.10.002; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Sen P, 2005, ACM T GRAPHIC, V24, P745, DOI 10.1145/1073204.1073257; Sheinin M, 2018, IEEE INT CONF COMPUT; Sheinin M, 2017, PROC CVPR IEEE, P2363, DOI 10.1109/CVPR.2017.254; Shwartz S, 2006, LECT NOTES COMPUT SC, V3889, P246; Sim T, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P53, DOI 10.1109/AFGR.2002.1004130; Sinha SN, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185596; Su H, 2014, IEEE IMAGE PROC, P5367, DOI 10.1109/ICIP.2014.7026086; Tajbakhsh T, 2007, PROC SPIE, V6502, DOI 10.1117/12.709555; Torralba A, 2014, INT J COMPUT VISION, V110, P92, DOI 10.1007/s11263-014-0697-5; Veeraraghavan A, 2011, IEEE T PATTERN ANAL, V33, P671, DOI 10.1109/TPAMI.2010.87; Vollmer M, 2015, EUR J PHYS, V36, DOI 10.1088/0143-0807/36/3/035027; Wang XM, 2003, INT C POWER ELECT DR, P1529; Weiss Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P68, DOI 10.1109/ICCV.2001.937606; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Wu HY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185561; Yoo Y, 2014, IEEE T CONSUM ELECTR, V60, P294, DOI 10.1109/TCE.2014.6937311; Zongker DE, 1999, COMP GRAPH, P205, DOI 10.1145/311535.311558	63	0	0	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					8728	8739		10.1109/TPAMI.2019.2903035	http://dx.doi.org/10.1109/TPAMI.2019.2903035			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	30843801				2022-12-18	WOS:000880661400016
J	Spurek, P; Zieba, M; Tabor, J; Trzcinski, T				Spurek, Przemyslaw; Zieba, Maciej; Tabor, Jacek; Trzcinski, Tomasz			General Hypernetwork Framework for Creating 3D Point Clouds	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Solid modeling; Shape; Training; Probability distribution; Numerical models; Transforms; Hypernetworks; 3D point cloud processing; generative modeling		In this work, we propose a novel method for generating 3D point clouds that leverages the properties of hypernetworks. Contrary to the existing methods that learn only the representation of a 3D object, our approach simultaneously finds a representation of the object and its 3D surface. The main idea of our HyperCloud method is to build a hypernetwork that returns weights of a particular neural network (target network) trained to map points from prior distribution into a 3D shape. As a consequence, a particular 3D shape can be generated using point-by-point sampling from the prior distribution and transforming the sampled points with the target network. Since the hypernetwork is based on an auto-encoder architecture trained to reconstruct realistic 3D shapes, the target network weights can be considered to be a parametrization of the surface of a 3D shape, and not a standard representation of point cloud usually returned by competitive approaches. We also show that relying on hypernetworks to build 3D point cloud representations offers an elegant and flexible framework. To that point, we further extend our method by incorporating flow-based models, which results in a novel HyperFlow approach.	[Spurek, Przemyslaw; Tabor, Jacek; Trzcinski, Tomasz] Jagiellonian Univ, PL-31007 Krakow, Poland; [Zieba, Maciej] Wroclaw Univ Sci & Technol, PL-50370 Wroclaw, Poland; [Zieba, Maciej; Trzcinski, Tomasz] Tooploox, PL-53601 Wroclaw, Poland; [Trzcinski, Tomasz] Warsaw Univ Technol, PL-00661 Warsaw, Poland	Jagiellonian University; Wroclaw University of Science & Technology; Warsaw University of Technology	Spurek, P (corresponding author), Jagiellonian Univ, PL-31007 Krakow, Poland.	przemyslaw.spurek@gmail.com; maciej.zieba@pwr.edu.pl; jacek.tabor@uj.edu.pl; t.trzcinski@ii.pw.edu.pl			National Centre of Science (Poland) [2020/37/B/ST6/03463, 2019/33/B/ST6/00894, 2017/25/B/ST6/01271, 2020/39/B/ST6/01511]; Foundation for Polish Science - European Union under the European Regional Development Fund [POIR.04.04.00-00-14DE/18-00]	National Centre of Science (Poland)(National Science Centre, Poland); Foundation for Polish Science - European Union under the European Regional Development Fund	The work of Przemyslaw Spurek was supported by the National Centre of Science (Poland) under Grant 2019/33/B/ST6/00894. The work of Jacek Tabor was supported by the National Centre of Science (Poland) under Grant 2017/25/B/ST6/01271. Thework of Tomasz Trzcinski was supported in part by theNational Centre of Science (Poland) under Grant 2020/39/B/ST6/01511 and in part by the Foundation for Polish Science under Grant POIR.04.04.00-00-14DE/18-00 co-financed by the European Union under the European Regional Development Fund. The work of Maciej Zieba was supported by the National Centre of Science (Poland) under Grant 2020/37/B/ST6/03463. The authors have applied a CC BY license to any Author AcceptedManuscript (AAM) version arising from this submission, in accordance with the grants' open access conditions.	Abdul-Saboor Sheikh, 2018, Arxiv, DOI arXiv:1712.01141; Achlioptas P, 2018, PR MACH LEARN RES, V80; Alireza Makhzani, 2016, Arxiv, DOI arXiv:1511.05644; Barnabas Poczos, 2018, Arxiv, DOI arXiv:1810.05795; Bednarik J, 2020, PROC CVPR IEEE, P4715, DOI 10.1109/CVPR42600.2020.00477; Behrmann J, 2019, PR MACH LEARN RES, V97; Chen T.Q., 2018, ADV NEURAL INFORM PR; Deng ZT, 2020, INT CONF 3D VISION, P593, DOI 10.1109/3DV50981.2020.00069; Dill A., 2019, GETTING TOPOLOGY POI; Gadelha M, 2018, LECT NOTES COMPUT SC, V11211, P105, DOI 10.1007/978-3-030-01234-2_7; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grathwohl W., 2018, P INT C LEARN REPR; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Ha David, 2017, ICLR; Han T, 2019, PROC CVPR IEEE, P8662, DOI 10.1109/CVPR.2019.00887; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Kehoe B, 2015, IEEE T AUTOM SCI ENG, V12, P398, DOI 10.1109/TASE.2014.2376492; Kingma D.P, P 3 INT C LEARNING R; Klocek S, 2019, LECT NOTES COMPUT SC, V11731, P496, DOI 10.1007/978-3-030-30493-5_48; Knop S, 2020, J MACH LEARN RES, V21; KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Qi CR, 2017, ADV NEUR IN, V30; Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Spurek P., 2021, ARXIV; Spurek Przemyslaw, 2020, INT C MACH LEARN, P9099; Stypu kowski M., 2019, ARXIV; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Sun YB, 2020, IEEE WINT CONF APPL, P61, DOI 10.1109/WACV45572.2020.9093430; Tomczak JM, 2018, PR MACH LEARN RES, V84; Tran M.-P., 2013, 3D CONTOUR CLOSING A; van den Berg R, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P393; van den Oord A, 2016, ADV NEUR IN, V29; Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4; Wang YF, 2019, PROC CVPR IEEE, P5951, DOI 10.1109/CVPR.2019.00611; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Xie JW, 2022, IEEE T PATTERN ANAL, V44, P2468, DOI 10.1109/TPAMI.2020.3045010; Xie JW, 2021, PROC CVPR IEEE, P14971, DOI 10.1109/CVPR46437.2021.01473; Xie JW, 2021, IEEE T PATTERN ANAL, V43, P516, DOI 10.1109/TPAMI.2019.2934852; Xie JW, 2018, PROC CVPR IEEE, P8629, DOI 10.1109/CVPR.2018.00900; Xie JW, 2016, PR MACH LEARN RES, V48; Xie JW, 2017, PROC CVPR IEEE, P1061, DOI 10.1109/CVPR.2017.119; Yang B, 2018, PROC CVPR IEEE, P7652, DOI 10.1109/CVPR.2018.00798; Yang GD, 2019, IEEE I CONF COMP VIS, P4540, DOI 10.1109/ICCV.2019.00464; Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029; Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295; Zaheer M., 2017, ADV NEURAL INFORM PR, P3391; Zamorski M, 2020, COMPUT VIS IMAGE UND, V193, DOI 10.1016/j.cviu.2020.102921	52	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9995	10008		10.1109/TPAMI.2021.3131131	http://dx.doi.org/10.1109/TPAMI.2021.3131131			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34826294	hybrid			2022-12-18	WOS:000880661400102
J	Valdes, G; Friedman, JH; Jiang, F; Gennatas, ED				Valdes, Gilmer; Friedman, Jerome H.; Jiang, Fei; Gennatas, Efstathios D.			Representational Gradient Boosting: Backpropagation in the Space of Functions	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Boosting; Prediction algorithms; Backpropagation; Convolution; Stacking; Predictive models; Machine learning algorithms; Gradient boosting; representational learning; non parametric models; neural networks	REGRESSION	The estimation of nested functions (i.e., functions of functions) is one of the central reasons for the success and popularity of machine learning. Today, artificial neural networks are the predominant class of algorithms in this area, known as representational learning. Here, we introduce Representational Gradient Boosting (RGB), a nonparametric algorithm that estimates functions with multi-layer architectures obtained using backpropagation in the space of functions. RGB does not need to assume a functional form in the nodes or output (e.g., linear models or rectified linear units), but rather estimates these transformations. RGB can be seen as an optimized stacking procedure where a meta algorithm learns how to combine different classes of functions (e.g., Neural Networks (NN) and Gradient Boosting (GB)), while building and optimizing them jointly in an attempt to compensate each other's weaknesses. This highlights a stark difference with current approaches to meta-learning that combine models only after they have been built independently. We showed that providing optimized stacking is one of the main advantages of RGB over current approaches. Additionally, due to the nested nature of RGB we also showed how it improves over GB in problems that have several high-order interactions. Finally, we investigate both theoretically and in practice the problem of recovering nested functions and the value of prior knowledge.	[Valdes, Gilmer] Univ Calif San Francisco, Dept Radiat Oncol, San Francisco, CA 94143 USA; [Valdes, Gilmer; Jiang, Fei; Gennatas, Efstathios D.] Univ Calif San Francisco, Dept Epidemiol & Biostat, San Francisco, CA 94143 USA; [Friedman, Jerome H.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Gennatas, Efstathios D.] Stanford Univ, Dept Radiat Oncol, Stanford, CA 94305 USA	University of California System; University of California San Francisco; University of California System; University of California San Francisco; Stanford University; Stanford University	Valdes, G (corresponding author), Univ Calif San Francisco, Dept Radiat Oncol, San Francisco, CA 94143 USA.; Valdes, G (corresponding author), Univ Calif San Francisco, Dept Epidemiol & Biostat, San Francisco, CA 94143 USA.	gilmer.valdes@ucsf.edu; jhf@stanford.edu; fei.fiang@ucsf.edu; gennatas@stanford.edu			National Institute of Biomedical Imaging and Bioengineering of the National Institutes of Health [K08EB026500]	National Institute of Biomedical Imaging and Bioengineering of the National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Biomedical Imaging & Bioengineering (NIBIB))	This work was supported in part by the National Institute of Biomedical Imaging and Bioengineering of the National Institutes of Health under Grant K08EB026500. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.	Alain Guillaume, 2016, ARXIV161001644; Alex Krizhevsky, 2012, Arxiv, DOI arXiv:1207.0580; Bengio Y., 2020, PREPRINT; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Breiman L, 1996, MACH LEARN, V24, P49; Caruana R., 2006, P 23 INT C MACH LEAR, P161; Feng J, 2018, ADV NEUR IN, V31; FRIEDMAN JH, 1974, IEEE T COMPUT, VC 23, P881, DOI 10.1109/T-C.1974.224051; FRIEDMAN JH, 1981, J AM STAT ASSOC, V76, P817, DOI 10.2307/2287576; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Larochelle H, 2009, J MACH LEARN RES, V10, P1; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lee DH, 2015, LECT NOTES ARTIF INT, V9284, P498, DOI 10.1007/978-3-319-23528-8_31; Luna JM, 2019, P NATL ACAD SCI USA, V116, P19887, DOI 10.1073/pnas.1816748116; MANI G, 1990, 1990 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS, P242, DOI 10.1109/ICSMC.1990.142101; Seide F, 2011, 12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5, P444; Simonyan K., 2015, VERY DEEP CONVOLUTIO; Valdes G, 2016, SCI REP-UK, V6, DOI 10.1038/srep37854; Vaswani A, 2017, ADV NEUR IN, V30; WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1	24	0	0	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					10186	10195		10.1109/TPAMI.2021.3137715	http://dx.doi.org/10.1109/TPAMI.2021.3137715			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34941500	hybrid			2022-12-18	WOS:000880661400114
J	Valdes, G; Interian, Y; Gennatas, E; Van der Laan, M				Valdes, Gilmer; Interian, Yannet; Gennatas, Efstathios; van der Laan, Mark			The Conditional Super Learner	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cross-validation; meta learning; super learner; interpretability; nonparametric hierarchical models	REGRESSION	Using cross validation to select the best model from a library is standard practice in machine learning. Similarly, meta learning is a widely used technique where models previously developed are combined (mainly linearly) with the expectation of improving performance with respect to individual models. In this article we consider the Conditional Super Learner (CSL), an algorithm that selects the best model candidate from a library of models conditional on the covariates. The CSL expands the idea of using cross validation to select the best model and merges it with meta learning. We propose an optimization algorithm that finds a local minimum to the problem posed and proves that it converges at a rate faster than O-p(n(-1/4)). We offer empirical evidence that: (1) CSL is an excellent candidate to substitute stacking and (2) CLS is suitable for the analysis of Hierarchical problems. Additionally, implications for global interpretability are emphasized.	[Valdes, Gilmer] Univ Calif San Francisco, Dept Radiat Oncol, San Francisco, CA 94143 USA; [Valdes, Gilmer] Univ Calif San Francisco, Dept Epidemiol & Biostat, San Francisco, CA 94143 USA; [Interian, Yannet] Univ San Francisco, Data Sci, San Francisco, CA 94117 USA; [Gennatas, Efstathios] Stanford Univ, Dept Radiat Oncol, Palo Alto, CA 94305 USA; [van der Laan, Mark] Univ Calif Berkeley, Div Biostat, Berkeley, CA 94720 USA	University of California System; University of California San Francisco; University of California System; University of California San Francisco; University of San Francisco; Stanford University; University of California System; University of California Berkeley	Valdes, G (corresponding author), Univ Calif San Francisco, Dept Radiat Oncol, San Francisco, CA 94143 USA.	Gilmer.Valdes@ucsf.edu; yinterian@usfca.edu; gennatas@stanford.edu; laan@berkeley.edu			National Institute Of Biomedical Imaging And Bioengineering of the National Institutes of Health [K08EB026500]; National Institute of Allergy and Infectious Diseases [5R01AI074345-09]	National Institute Of Biomedical Imaging And Bioengineering of the National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Biomedical Imaging & Bioengineering (NIBIB)); National Institute of Allergy and Infectious Diseases(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Allergy & Infectious Diseases (NIAID))	This work was supported in part by the National Institute Of Biomedical Imaging And Bioengineering of the National Institutes of Health under Grant K08EB026500, and in part by the National Institute of Allergy and Infectious Diseases under Grant 5R01AI074345-09.	Been Kim, 2017, Arxiv, DOI arXiv:1702.08608; Benkeser D, 2016, PROCEEDINGS OF 3RD IEEE/ACM INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS, (DSAA 2016), P689, DOI 10.1109/DSAA.2016.93; Breiman L, 1996, MACH LEARN, V24, P49; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655; Breiman L., 2017, CLASSIFICATION REGRE; Cabitza F, 2018, CLIN CHEM LAB MED, V56, P516, DOI 10.1515/cclm-2017-0287; EFRON B, 1983, J AM STAT ASSOC, V78, P316, DOI 10.2307/2288636; Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Gennatas ED, 2020, P NATL ACAD SCI USA, V117, P4571, DOI 10.1073/pnas.1906831117; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; Lipton Z. C., 2017, PROC S INTERPRETABLE, P1; Louzada Francisco, 2016, Surveys in Operations Research and Management Science, V21, P117, DOI 10.1016/j.sorms.2016.10.001; Luna JM, 2019, P NATL ACAD SCI USA, V116, P19887, DOI 10.1073/pnas.1816748116; Lundberg SM, 2017, ADV NEUR IN, V30; Normand SLT, 1997, J AM STAT ASSOC, V92, P803, DOI 10.2307/2965545; Olson RS, 2017, BIODATA MIN, V10, DOI 10.1186/s13040-017-0154-4; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Tan SR, 2018, Arxiv, DOI arXiv:1801.08640; Smyth P, 1998, ADV NEUR IN, V10, P668; Valdes G, 2016, SCI REP-UK, V6, DOI 10.1038/srep37854; van der Laan MJ, 2007, STAT APPL GENET MOL, V6, DOI 10.2202/1544-6115.1309; WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1; Zachary C. Lipton, 2017, Arxiv, DOI arXiv:1606.03490	26	0	0	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					10236	10243		10.1109/TPAMI.2021.3131976	http://dx.doi.org/10.1109/TPAMI.2021.3131976			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34851823	hybrid, Green Submitted			2022-12-18	WOS:000880661400118
J	Wang, JY; Wang, HM; Nie, FP; Li, XL				Wang, Jingyu; Wang, Hongmei; Nie, Feiping; Li, Xuelong			Ratio Sum Versus Sum Ratio for Linear Discriminant Analysis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Kernel; Principal component analysis; Feature extraction; Covariance matrices; Linear discriminant analysis; Eigenvalues and eigenfunctions; Sparse matrices; Supervised learning; dimensionality reduction; linear discriminant analysis; ratio sum problem; kernel technique; subspace learning	FEATURE-EXTRACTION; FEATURE-SELECTION; ROBUST; FRAMEWORK; EFFICIENT; TRACE	Dimension reduction is a critical technology for high-dimensional data processing, where Linear Discriminant Analysis (LDA) and its variants are effective supervised methods. However, LDA prefers to feature with smaller variance, which causes feature with weak discriminative ability retained. In this paper, we propose a novel Ratio Sum for Linear Discriminant Analysis (RSLDA), which aims at maximizing discriminative ability of each feature in subspace. To be specific, it maximizes the sum of ratio of the between-class distance to the within-class distance in each dimension of subspace. Since the original RSLDA problem is difficult to obtain the closed solution, an equivalent problem is developed which can be solved by an alternative optimization algorithm. For solving the equivalent problem, it is transformed into two sub-problems, one of which can be solved directly, the other is changed into a convex optimization problem, where singular value decomposition is employed instead of matrix inversion. Consequently, performance of algorithm cannot be affected by the non-singularity of covariance matrix. Furthermore, Kernel RSLDA (KRSLDA) is presented to improve the robustness of RSLDA. Additionally, time complexity of RSLDA and KRSLDA are analyzed. Extensive experiments show that RSLDA and KRSLDA outperforms other comparison methods on toy datasets and multiple public datasets.	[Wang, Jingyu; Nie, Feiping; Li, Xuelong] Northwestern Polytech Univ, Sch Astronaut, Sch Artificial Intelligence OPt & Elect iOPEN, Xian 710072, Shaanxi, Peoples R China; [Wang, Hongmei] Northwestern Polytech Univ, Sch Astronaut, Xian 710072, Shaanxi, Peoples R China; [Nie, Feiping; Li, Xuelong] Northwestern Polytech Univ, Minist Ind & Informat Technol, Key Lab Intelligent Interact & Applicat, Xian 710072, Shaanxi, Peoples R China	Northwestern Polytechnical University; Northwestern Polytechnical University; Northwestern Polytechnical University	Nie, FP (corresponding author), Northwestern Polytech Univ, Sch Astronaut, Sch Artificial Intelligence OPt & Elect iOPEN, Xian 710072, Shaanxi, Peoples R China.	jywang@nwpu.edu.cn; wanghongmei@mail.nwpu.edu.cn; feipingnie@gmail.com; li@nwpu.edu.cn		Nie, Feiping/0000-0002-0871-6519	National Natural Science Foundation of China [61976179, 61871470, 62176212, 61772427]; Innovation Capability Support Program of Shaanxi [2021KJXX-103]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Innovation Capability Support Program of Shaanxi	This work was supported in part by the National Natural Science Foundation of China under Grants 61976179, 61871470, 62176212, and 61772427 and in part by the Innovation Capability Support Program of Shaanxi under Grant 2021KJXX-103.	Cai J, 2018, IEEE T NEUR NET LEAR, V29, P4957, DOI 10.1109/TNNLS.2017.2785324; Cai X, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1124; Chakraborty R, 2021, IEEE T PATTERN ANAL, V43, P3904, DOI 10.1109/TPAMI.2020.2992392; Ching WK, 2012, PATTERN RECOGN, V45, P2719, DOI 10.1016/j.patcog.2012.01.007; Chu DL, 2012, SIAM J SCI COMPUT, V34, pA2421, DOI 10.1137/110851377; Chu DL, 2010, PATTERN RECOGN, V43, P1373, DOI 10.1016/j.patcog.2009.10.004; Clemmensen L, 2011, TECHNOMETRICS, V53, P406, DOI 10.1198/TECH.2011.08118; Deng WH, 2018, IEEE T PATTERN ANAL, V40, P2513, DOI 10.1109/TPAMI.2017.2757923; Han N, 2020, IEEE T NEUR NET LEAR, V31, P5630, DOI 10.1109/TNNLS.2020.2966746; He L, 2018, IEEE SIGNAL PROC LET, V25, P1575, DOI 10.1109/LSP.2018.2869107; He XF, 2016, IEEE T PATTERN ANAL, V38, P1009, DOI 10.1109/TPAMI.2015.2439252; Jia YQ, 2009, IEEE T NEURAL NETWOR, V20, P729, DOI 10.1109/TNN.2009.2015760; Ju FJ, 2021, INFORM SCIENCES, V561, P196, DOI 10.1016/j.ins.2021.01.054; Ju FJ, 2019, IEEE T NEUR NET LEAR, V30, P2938, DOI 10.1109/TNNLS.2019.2901309; KRZANOWSKI WJ, 1995, J R STAT SOC C-APPL, V44, P101, DOI 10.2307/2986198; Li CN, 2020, IEEE T NEUR NET LEAR, V31, P915, DOI 10.1109/TNNLS.2019.2910991; Li HF, 2006, IEEE T NEURAL NETWOR, V17, P157, DOI 10.1109/TNN.2005.860852; Li HX, 2020, INFORM SCIENCES, V510, P283, DOI 10.1016/j.ins.2019.09.032; Li XL, 2020, IEEE T IMAGE PROCESS, V29, P2139, DOI 10.1109/TIP.2019.2947776; Li XL, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2201; Li XL, 2017, AAAI CONF ARTIF INTE, P4147; Li ZC, 2015, IEEE T PATTERN ANAL, V37, P2085, DOI 10.1109/TPAMI.2015.2400461; Liu Y, 2013, NEUROCOMPUTING, V105, P12, DOI 10.1016/j.neucom.2012.05.031; Luo MN, 2018, IEEE T NEUR NET LEAR, V29, P944, DOI 10.1109/TNNLS.2017.2650978; Nie F., 2011, PROC INT JOINT C ART, P1433; Nie F., 2010, ADV NEURAL INFORM PR, V1, P1813, DOI DOI 10.1007/978-3-319-10690-8_12; Nie FP, 2020, ACM T KNOWL DISCOV D, V14, DOI 10.1145/3369870; Nie FP, 2020, IEEE T CYBERNETICS, V50, P3682, DOI 10.1109/TCYB.2019.2910751; Nie FP, 2014, PR MACH LEARN RES, V32, P1062; Nie FP, 2012, PATTERN RECOGN LETT, V33, P485, DOI 10.1016/j.patrec.2011.11.028; Nie FP, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P993; Pedagadi S, 2013, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2013.426; Saeidi R, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481420; Trigeorgis G, 2018, IEEE T PATTERN ANAL, V40, P1128, DOI 10.1109/TPAMI.2017.2710047; Wan H, 2018, IEEE T PATTERN ANAL, V40, P409, DOI 10.1109/TPAMI.2017.2672557; Wang HH, 2015, INT C INTEL HUM MACH, P411, DOI 10.1109/IHMSC.2015.191; Wang H, 2007, PROC CVPR IEEE, P108; Wang JY, 2022, IEEE T NEUR NET LEAR, V33, P3634, DOI 10.1109/TNNLS.2021.3053840; Wang Q, 2020, IEEE T PATTERN ANAL, V42, P46, DOI 10.1109/TPAMI.2018.2875002; Welling M, 2005, DEP COMPUTER SCI U T, V3, P1; Wen J, 2019, IEEE T CIRC SYST VID, V29, P390, DOI 10.1109/TCSVT.2018.2799214; Xiong HY, 2019, IEEE T NEUR NET LEAR, V30, P707, DOI 10.1109/TNNLS.2018.2846783; Yang A, 2019, IEEE T INSTRUM MEAS, V68, P4629, DOI 10.1109/TIM.2019.2900885; Yang J, 2005, IEEE T PATTERN ANAL, V27, P230, DOI 10.1109/TPAMI.2005.33; Yang L, 2019, IEEE T NEUR NET LEAR, V30, P3205, DOI 10.1109/TNNLS.2018.2890103; Yin W., KNOWL-BASED SYST, V195; Zafeiriou S, 2012, IEEE T NEUR NET LEAR, V23, P526, DOI 10.1109/TNNLS.2011.2182058; Zhang H, 2019, NEUROCOMPUTING, V366, P194, DOI 10.1016/j.neucom.2019.07.020; Zhang WZ, 2017, IEEE T PATTERN ANAL, V39, P1223, DOI 10.1109/TPAMI.2016.2578323; Zhang XW, 2016, IEEE T NEUR NET LEAR, V27, P1469, DOI 10.1109/TNNLS.2015.2448637; Zhao HF, 2019, IEEE T KNOWL DATA EN, V31, P629, DOI 10.1109/TKDE.2018.2842023; Zhao JH, 2015, IEEE T NEUR NET LEAR, V26, P1669, DOI 10.1109/TNNLS.2014.2350993; Zhao XW, 2020, IEEE T NEUR NET LEAR, V31, P433, DOI 10.1109/TNNLS.2019.2904701; Zhou Y, 2017, IEEE T CYBERNETICS, V47, P830, DOI 10.1109/TCYB.2016.2529299	57	0	0	5	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					10171	10185		10.1109/TPAMI.2021.3133351	http://dx.doi.org/10.1109/TPAMI.2021.3133351			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34874851				2022-12-18	WOS:000880661400113
J	You, Y; Lou, YJ; Shi, RX; Liu, Q; Tai, YW; Ma, LZ; Wang, WM; Lu, CW				You, Yang; Lou, Yujing; Shi, Ruoxi; Liu, Qi; Tai, Yu-Wing; Ma, Lizhuang; Wang, Weiming; Lu, Cewu			PRIN/SPRIN: On Extracting Point-Wise Rotation Invariant Features	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature extraction; Three-dimensional displays; Convolution; Shape; Recurrent neural networks; Task analysis; Solid modeling; Point cloud; object analysis; rotation invariance; feature learning	NETWORKS	Point cloud analysis without pose priors is very challenging in real applications, as the orientations of point clouds are often unknown. In this paper, we propose a brand new point-set learning framework PRIN, namely, Point-wise Rotation Invariant Network, focusing on rotation invariant feature extraction in point clouds analysis. We construct spherical signals by Density Aware Adaptive Sampling to deal with distorted point distributions in spherical space. Spherical Voxel Convolution and Point Re-sampling are proposed to extract rotation invariant features for each point. In addition, we extend PRIN to a sparse version called SPRIN, which directly operates on sparse point clouds. Both PRIN and SPRIN can be applied to tasks ranging from object classification, part segmentation, to 3D feature matching and label alignment. Results show that, on the dataset with randomly rotated point clouds, SPRIN demonstrates better performance than state-of-the-art methods without any data augmentation. We also provide thorough theoretical proof and analysis for point-wise rotation invariance achieved by our methods. The code to reproduce our results will be made publicly available.	[You, Yang; Lou, Yujing; Shi, Ruoxi; Liu, Qi; Ma, Lizhuang; Wang, Weiming; Lu, Cewu] Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China; [Tai, Yu-Wing] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China; [Lu, Cewu] Shanghai Jiao Tong Univ, Qing Yuan Res Inst, Shanghai Qizhi Res Inst, Shanghai 200240, Peoples R China; [Lu, Cewu] Shanghai Jiao Tong Univ, MoE Key Lab Artificial Intelligence, AI Inst, Shanghai 200240, Peoples R China	Shanghai Jiao Tong University; Hong Kong University of Science & Technology; Shanghai Jiao Tong University; Shanghai Jiao Tong University	Wang, WM; Lu, CW (corresponding author), Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China.	qq456cvb@sjtu.edu.cn; louyujing@sjtu.edu.cn; eliphat@sjtu.edu.cn; enerald@sjtu.edu.cn; yuwingtai@tencent.com; ma-lz@cs.sjtu.edu.cn; wangweiming@sjtu.edu.cn; lucewu@sjtu.edu.cn		wang, Weiming/0000-0002-1406-3315; You, Yang/0000-0003-0125-0792; Lou, Yujing/0000-0001-6292-8953	National Key R&D Program of China [2017YFA0700800, 2019YFC1521104]; SHEITC [2018-RGZN-02046]; Shanghai Qi Zhi Institute; National Natural Science Foundation of China [51675342, 51975350, 61972157, 61772332]	National Key R&D Program of China; SHEITC; Shanghai Qi Zhi Institute; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by the National Key R&D Program of China, under Grants 2017YFA0700800 and 2019YFC1521104, in part by SHEITC under Grant 2018-RGZN-02046, and in part by Shanghai Qi Zhi Institute. This work was also supported by the National Natural Science Foundation of China under Grants 51675342, 51975350, 61972157, and 61772332.	[Anonymous], HARMONIC ANAL SO 3; Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301; Benjamin Graham, 2017, Arxiv, DOI arXiv:1706.01307; Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319; Cohen T. S., 2018, PROC 6 INT C LEARN R; Cruz-Mota J, 2012, INT J COMPUT VISION, V98, P217, DOI 10.1007/s11263-011-0505-4; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Daniel E. Worrall, 2020, Arxiv, DOI arXiv:2006.10503; Deng HW, 2018, LECT NOTES COMPUT SC, V11209, P620, DOI 10.1007/978-3-030-01228-1_37; Dieleman S, 2015, MON NOT R ASTRON SOC, V450, P1441, DOI 10.1093/mnras/stv632; Esteves C, 2018, LECT NOTES COMPUT SC, V11217, P54, DOI 10.1007/978-3-030-01261-8_4; Gens R, 2014, ADV NEUR IN, V27; Gojcic Z, 2019, PROC CVPR IEEE, P5540, DOI 10.1109/CVPR.2019.00569; Guo YL, 2014, IEEE T PATTERN ANAL, V36, P2270, DOI 10.1109/TPAMI.2014.2316828; Huang QG, 2018, PROC CVPR IEEE, P2626, DOI 10.1109/CVPR.2018.00278; Kai Kohlhoff, 2018, Arxiv, DOI arXiv:1802.08219; Khoury M, 2017, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2017.26; Kingma D.P, P 3 INT C LEARNING R; Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99; Kostelec P. J., 2007, DEP MATH, V3755; Kostelec PJ, 2008, J FOURIER ANAL APPL, V14, P145, DOI 10.1007/s00041-008-9013-5; Lei H, 2021, IEEE T PATTERN ANAL, V43, P3664, DOI 10.1109/TPAMI.2020.2983410; Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979; Li XZ, 2022, IEEE T VIS COMPUT GR, V28, P4503, DOI 10.1109/TVCG.2021.3092570; Liu XH, 2019, AAAI CONF ARTIF INTE, P8778; Malassiotis S, 2007, IEEE T PATTERN ANAL, V29, P1285, DOI 10.1109/TPAMI.2007.1060; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Max Welling, 2016, Arxiv, DOI arXiv:1612.08498; Moon P., 2012, FIELD THEORY HDB INC; Poulenard A, 2019, INT CONF 3D VISION, P47, DOI 10.1109/3DV.2019.00015; Qi CR, 2017, ADV NEUR IN, V30; Kim S, 2021, Arxiv, DOI arXiv:2010.03318; Siciliano B, 2009, ADV TXB CONTR SIG PR, P1; Solomon J., 2017, COMPUTATIONAL OPTIMA; STEIN F, 1992, IEEE T PATTERN ANAL, V14, P125, DOI 10.1109/34.121785; Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Sun X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P980, DOI 10.1145/3343031.3351042; Thomas Jr G. B., 2014, EARLY TRANSCENDENTAL; Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26; Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167; Wang WM, 2021, IMAGE VISION COMPUT, V106, DOI 10.1016/j.imavis.2020.104092; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Li XZ, 2020, Arxiv, DOI arXiv:2003.07238; Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6; Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238; Yongheng Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P1, DOI 10.1007/978-3-030-58452-8_1; You Y, 2020, AAAI CONF ARTIF INTE, V34, P12717; Zaheer M., 2017, ADV NEURAL INFORM PR, P3391; Zhang K., 2019, ARXIV; Zhang YX, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6279; Zhang ZY, 2019, INT CONF 3D VISION, P204, DOI 10.1109/3DV.2019.00031	54	0	0	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9489	9502		10.1109/TPAMI.2021.3130590	http://dx.doi.org/10.1109/TPAMI.2021.3130590			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34822324	Green Submitted			2022-12-18	WOS:000880661400066
J	Yuan, YT; Ma, L; Zhu, WW				Yuan, Yitian; Ma, Lin; Zhu, Wenwu			Syntax Customized Video Captioning by Imitating Exemplar Sentences	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Syntactics; Semantics; Task analysis; Training; Decoding; Encoding; Recurrent neural networks; Video captioning; sentence syntax customization; recurrent neural network		Enhancing the diversity of sentences to describe video contents is an important problem arising in recent video captioning research. In this paper, we explore this problem from a novel perspective of customizing video captions by imitating exemplar sentence syntaxes. Specifically, given a video and any syntax-valid exemplar sentence, we introduce a new task of Syntax Customized Video Captioning (SCVC) aiming to generate one caption which not only semantically describes the video contents but also syntactically imitates the given exemplar sentence. To tackle the SCVC task, we propose a novel video captioning model, where a hierarchical sentence syntax encoder is first designed to extract the syntactic structure of the exemplar sentence, then a syntax conditioned caption decoder is devised to generate the syntactically structured caption expressing video semantics. As there is no available syntax customized groundtruth video captions, we tackle such a challenge by proposing a new training strategy, which leverages the traditional pairwise video captioning data and our collected exemplar sentences to accomplish the model learning. Extensive experiments, in terms of semantic, syntactic, fluency, and diversity evaluations, clearly demonstrate our model capability to generate syntax-varied and semantics-coherent video captions that well imitate different exemplar sentences with enriched diversities. Code is available at https://github.com/yytzsy/Syntax-Customized-Video-Captioning.	[Yuan, Yitian; Ma, Lin] Meituan, Beijing 100102, Peoples R China; [Zhu, Wenwu] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China	Tsinghua University	Zhu, WW (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.	yuanyitian@foxmail.com; forest.linma@gmail.com; wwzhu@tsinghua.edu.cn		Yuan, Yitian/0000-0001-8701-7689	National Key Research and Development Program of China [2020AAA0106300, 2018AAA0102000]; National Natural Science Foundation of China [62050110]; Shenzhen Nanshan District Ling-Hang Team [LHTD20170005]	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shenzhen Nanshan District Ling-Hang Team	This work was supported in part by the National Key Research and Development Program of China under Grants 2020AAA0106300 and 2018AAA0102000, in part by the National Natural Science Foundation of China Key Project 62050110, and in part by Shenzhen Nanshan District Ling-Hang Team under Grant LHTD20170005.	Baraldi L, 2017, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2017.339; Chen SX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P6283; Cheng Y., 2020, PROC C EMPIRICAL MET, P2915; Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323; Deshpande A, 2019, PROC CVPR IEEE, P10687, DOI 10.1109/CVPR.2019.01095; Jin D, 2021, Arxiv, DOI arXiv:2011.00416; dos Santos CN, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P189; Duan X, 2018, ADV NEUR IN, V31; Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754; Feng Y, 2019, PROC CVPR IEEE, P4120, DOI 10.1109/CVPR.2019.00425; Fu ZX, 2018, AAAI CONF ARTIF INTE, P663; Gan C, 2017, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2017.108; Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337; Hall D, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P228; Xiao HH, 2019, Arxiv, DOI arXiv:1910.12019; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83; Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010; Mathews A, 2018, PROC CVPR IEEE, P8591, DOI 10.1109/CVPR.2018.00896; Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497; Pasunuru R., 2017, EMNLP; Paszke A, 2019, ADV NEUR IN, V32; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Rohrbach A, 2014, LECT NOTES COMPUT SC, V8753, P184, DOI 10.1007/978-3-319-11752-2_15; Rohrbach M, 2013, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2013.61; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9959, DOI 10.1109/CVPR42600.2020.00998; Sutskever I, 2014, ADV NEUR IN, V27; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Venugopalan S., 2015, P C N AM CHAPT ASS C, P1494; Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515; Wang BR, 2019, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2019.00273; Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795; Wang QZ, 2019, PROC CVPR IEEE, P4190, DOI 10.1109/CVPR.2019.00432; Wu CH, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10186196; Xu J, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P537, DOI 10.1145/3123266.3123448; Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571; Xu R, 2015, AAAI CONF ARTIF INTE, P2346; Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512; You Q., 2018, ARXIV; ZHANG KZ, 1989, SIAM J COMPUT, V18, P1245, DOI 10.1137/0218082	42	0	0	3	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					10209	10221		10.1109/TPAMI.2021.3131618	http://dx.doi.org/10.1109/TPAMI.2021.3131618			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34847021	Green Submitted			2022-12-18	WOS:000880661400116
J	Zhang, MF; Zheng, YQ; Lu, F				Zhang, Mingfang; Zheng, Yinqiang; Lu, Feng			Optical Flow in the Dark	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Optical flow; Training; Estimation; Videos; Brightness; Image motion analysis; Data models; Low-light; optical flow; noise modeling; GAN; semi-supervised learning		Optical flow estimation in low-light conditions is a challenging task for existing methods and current optical flow datasets lack low-light samples. Even if the dark images are enhanced before estimation, which could achieve great visual perception, it still leads to suboptimal optical flow results because information like motion consistency may be broken during the enhancement. We propose to apply a novel training policy to learn optical flow directly from new synthetic and real low-light images. Specifically, first, we design a method to collect a new optical flow dataset in multiple exposures with shared optical flow pseudo labels. Then we apply a two-step process to create a synthetic low-light optical flow dataset, based on an existing bright one, by simulating low-light raw features from the multi-exposure raw images we collected. To extend the data diversity, we also include published low-light raw videos without optical flow labels. In our training pipeline, with the three datasets, we create two teacher-student pairs to progressively obtain optical flow labels for all data. Finally, we apply a mix-up training policy with our diversified datasets to produce low-light-robust optical flow models for release. The experiments show that our method can relatively maintain the optical flow accuracy as the image exposure descends and the generalization ability of our method is tested with different cameras in multiple practical scenes.	[Zhang, Mingfang] Peng Cheng Lab, Shenzhen 518000, Guangdong, Peoples R China; [Zhang, Mingfang] Univ Tokyo, Inst Ind Sci, Tokyo 1538505, Japan; [Zheng, Yinqiang] Univ Tokyo, Next Generat Artificial Intelligence Res Ctr, Tokyo 1138656, Japan; [Lu, Feng] Beihang Univ, State Key Lab VR Syst & Technol, SCSE, Beijing 100191, Peoples R China; [Lu, Feng] Peng Cheng Lab, Shenzhen 8518000, Guangdong, Peoples R China	Peng Cheng Laboratory; University of Tokyo; University of Tokyo; Beihang University; Peng Cheng Laboratory	Lu, F (corresponding author), Beihang Univ, State Key Lab VR Syst & Technol, SCSE, Beijing 100191, Peoples R China.	mfzhang@iis.u-tokyo.ac.jp; yqzheng@ai.u-tokyo.ac.jp; lufeng@buaa.edu.cn		Zhang, Mingfang/0000-0003-1792-6654; Lu, Feng/0000-0001-9064-7964	National Natural Science Foundation of China (NSFC) [61972012, 20H05951]	National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	This work was supported in part by the National Natural Science Foundation of China (NSFC) under Grant 61972012 and in part by the Grant-in-Aid for Transformative Research Areas 20H05951.	Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182; Brooks T, 2019, PROC CVPR IEEE, P11028, DOI 10.1109/CVPR.2019.01129; Buades A, 2011, IMAGE PROCESS ON LIN, V1, P208, DOI 10.5201/ipol.2011.bcm_nlm; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Chen C, 2019, IEEE I CONF COMP VIS, P3184, DOI 10.1109/ICCV.2019.00328; Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347; Danielyan A, 2012, IEEE T IMAGE PROCESS, V21, P1715, DOI 10.1109/TIP.2011.2176954; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Foi A, 2008, IEEE T IMAGE PROCESS, V17, P1737, DOI 10.1109/TIP.2008.2001399; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jaroensri R, 2019, ARXIV; Lee D.-H., 2013, WORKSHOP CHALLENGES, V3, P896; Li RT, 2018, LECT NOTES COMPUT SC, V11219, P299, DOI 10.1007/978-3-030-01267-0_18; Li RT, 2019, IEEE I CONF COMP VIS, P7303, DOI 10.1109/ICCV.2019.00740; Liang-Chieh Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P695, DOI 10.1007/978-3-030-58545-7_40; Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008; Lv FF, 2019, PROC CVPR IEEE, P5980, DOI 10.1109/CVPR.2019.00614; Lv FF, 2020, AAAI CONF ARTIF INTE, V34, P11725; Lv J. W. Feifan, 2018, BR MACH VIS C; Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925; Mildenhall B, 2018, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2018.00265; Plotz T, 2017, PROC CVPR IEEE, P2750, DOI 10.1109/CVPR.2017.294; Qizhe Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10684, DOI 10.1109/CVPR42600.2020.01070; Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24; Triantafyllidou Danai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P103, DOI 10.1007/978-3-030-58601-0_7; Wei KX, 2020, PROC CVPR IEEE, P2755, DOI 10.1109/CVPR42600.2020.00283; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zheng YQ, 2020, PROC CVPR IEEE, P6748, DOI 10.1109/CVPR42600.2020.00678	33	0	0	4	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9464	9476		10.1109/TPAMI.2021.3130302	http://dx.doi.org/10.1109/TPAMI.2021.3130302			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34818188				2022-12-18	WOS:000880661400064
J	Zhang, Q; Zhou, J; Zhu, L; Sun, W; Xiao, CX; Zheng, WS				Zhang, Qing; Zhou, Jin; Zhu, Lei; Sun, Wei; Xiao, Chunxia; Zheng, Wei-Shi			Unsupervised Intrinsic Image Decomposition Using Internal Self-Similarity Cues	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Training; Lighting; Image reconstruction; Image decomposition; Surface acoustic waves; Image sequences; Annotations; Intrinsic images; reflectance; shading	SINGLE-IMAGE; ILLUMINATION; SEPARATION; RETINEX	Recent learning-based intrinsic image decomposition methods have achieved remarkable progress. However, they usually require massive ground truth intrinsic images for supervised learning, which limits their applicability on real-world images since obtaining ground truth intrinsic decomposition for natural images is very challenging. In this paper, we present an unsupervised framework that is able to learn the decomposition effectively from a single natural image by training solely with the image itself. Our approach is built upon the observations that the reflectance of a natural image typically has high internal self-similarity of patches, and a convolutional generation network tends to boost the self-similarity of an image when trained for image reconstruction. Based on the observations, an unsupervised intrinsic decomposition network (UIDNet) consisting of two fully convolutional encoder-decoder sub-networks, i.e., reflectance prediction network (RPN) and shading prediction network (SPN), is devised to decompose an image into reflectance and shading by promoting the internal self-similarity of the reflectance component, in a way that jointly trains RPN and SPN to reproduce the given image. A novel loss function is also designed to make effective the training for intrinsic decomposition. Experimental results on three benchmark real-world datasets demonstrate the superiority of the proposed method.	[Zhang, Qing; Zheng, Wei-Shi] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510085, Guangdong, Peoples R China; [Zhou, Jin; Sun, Wei] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510275, Peoples R China; [Zhu, Lei] Hong Kong Univ Sci & Technol Guangzhou, Hong Kong, Peoples R China; [Xiao, Chunxia] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China; [Zheng, Wei-Shi] Peng Cheng Lab, Shenzhen 518066, Guangdong, Peoples R China; [Zheng, Wei-Shi] Minist Educ, Key Lab Machine Intelligence & Adv Comp, Beijing 100000, Peoples R China	Sun Yat Sen University; Sun Yat Sen University; Wuhan University; Peng Cheng Laboratory	Zheng, WS (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510085, Guangdong, Peoples R China.	zhangqing.whu.cs@gmail.com; zhouj289@mail2.sysu.edu.cn; lzhu@cse.cuhk.edu.hk; sunwei@mail.sysu.edu.cn; cxxiao@whu.edu.cn; wszheng@ieee.org		Zhu, Lei/0000-0003-3871-663X	NSFC [U21A20471, U1911401, U1811461, 61802453]; Guangdong NSF Project [2020B1515120085, 2018B030312002]; Guangzhou Research Project [201902010037]; Research Projects of Zhejiang Lab [2019KD0AB03]; Key-Area Research and Development Program of Guangzhou [202007030004]	NSFC(National Natural Science Foundation of China (NSFC)); Guangdong NSF Project; Guangzhou Research Project; Research Projects of Zhejiang Lab; Key-Area Research and Development Program of Guangzhou	This work was supported in part by the NSFC under Grants U21A20471, U1911401, U1811461, and 61802453, in part by the Guangdong NSF Project under Grants 2020B1515120085 and 2018B030312002, in part by the Guangzhou Research Project under Grant 201902010037, in part by the Research Projects of Zhejiang Lab under Grant 2019KD0AB03, and in part by the Key-Area Research and Development Program of Guangzhou under Grant 202007030004.	Angel X. Chang, 2015, Arxiv, DOI arXiv:1512.03012; Barnes C, 2010, LECT NOTES COMPUT SC, V6313, P29; Barron J. T., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2521, DOI 10.1109/CVPR.2011.5995392; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Barron JT, 2013, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2013.10; Barron JT, 2012, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2012.6247693; Barron JT, 2012, LECT NOTES COMPUT SC, V7575, P57, DOI 10.1007/978-3-642-33765-9_5; Barrow Harry, 1978, COMPUTER VISION SYST, V2, P1; Baslamisli AS, 2018, PROC CVPR IEEE, P6674, DOI 10.1109/CVPR.2018.00698; Baslamisli AS, 2018, LECT NOTES COMPUT SC, V11210, P289, DOI 10.1007/978-3-030-01231-1_18; Beigpour S, 2011, IEEE I CONF COMP VIS, P327, DOI 10.1109/ICCV.2011.6126259; Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206; Bi S., 2018, PROC EUROGRAPHICS S, P53; Bi S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766946; Bonneel N, 2017, COMPUT GRAPH FORUM, V36, P593, DOI 10.1111/cgf.13149; Bonneel N, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661253; Bousseau A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618476; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Carroll R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964938; Chang J, 2014, LECT NOTES COMPUT SC, V8692, P704, DOI 10.1007/978-3-319-10593-2_46; Chen QF, 2013, IEEE I CONF COMP VIS, P241, DOI 10.1109/ICCV.2013.37; Cheng LC, 2018, PROC CVPR IEEE, P656, DOI 10.1109/CVPR.2018.00075; Cheng Z, 2019, IEEE I CONF COMP VIS, P2521, DOI 10.1109/ICCV.2019.00261; Dollar P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715; Fan QN, 2018, PROC CVPR IEEE, P8944, DOI 10.1109/CVPR.2018.00932; Finlayson GD, 2004, LECT NOTES COMPUT SC, V3023, P582; Freeman William T, 2006, P C COMP VIS PATT RE; Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304; Gandelsman Y, 2019, PROC CVPR IEEE, P11018, DOI 10.1109/CVPR.2019.01128; Garces E, 2012, COMPUT GRAPH FORUM, V31, P1415, DOI 10.1111/j.1467-8659.2012.03137.x; Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428; Hachama M, 2015, IEEE I CONF COMP VIS, P810, DOI 10.1109/ICCV.2015.99; Hauagge D, 2016, IEEE T PATTERN ANAL, V38, P639, DOI 10.1109/TPAMI.2015.2453959; Huang Q, 2018, PROC CVPR IEEE, P6430, DOI 10.1109/CVPR.2018.00673; Janner M, 2017, ADV NEUR IN, V30; Jeon J, 2014, LECT NOTES COMPUT SC, V8695, P218, DOI 10.1007/978-3-319-10584-0_15; Jianbing Shen, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3481, DOI 10.1109/CVPR.2011.5995507; Jiang X, 2010, LECT NOTES COMPUT SC, V6314, P58, DOI 10.1007/978-3-642-15561-1_5; Kim S, 2016, LECT NOTES COMPUT SC, V9912, P143, DOI 10.1007/978-3-319-46484-8_9; Kong N, 2014, LECT NOTES COMPUT SC, V8690, P360; Kovacs B, 2017, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2017.97; Laffont PY, 2015, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2015.57; Laffont PY, 2013, IEEE T VIS COMPUT GR, V19, P210, DOI 10.1109/TVCG.2012.112; Laffont PY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366221; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; Lee KJ, 2012, LECT NOTES COMPUT SC, V7577, P327, DOI 10.1007/978-3-642-33783-3_24; Lettry L, 2018, COMPUT GRAPH FORUM, V37, P409, DOI 10.1111/cgf.13578; Lettry L, 2018, IEEE WINT CONF APPL, P1359, DOI 10.1109/WACV.2018.00153; Li C, 2019, IEEE T PATTERN ANAL, V41, P1455, DOI 10.1109/TPAMI.2018.2832059; Li Y, 2014, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2014.346; Li ZM, 2018, LECT NOTES COMPUT SC, V11213, P339, DOI 10.1007/978-3-030-01240-3_21; Li ZQ, 2018, PROC CVPR IEEE, P9039, DOI 10.1109/CVPR.2018.00942; Li ZQ, 2020, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR42600.2020.00255; Liu Aishan, 2020, ECCV; Liu YF, 2020, PROC CVPR IEEE, P3245, DOI 10.1109/CVPR42600.2020.00331; Lombardi S, 2016, IEEE T PATTERN ANAL, V38, P129, DOI 10.1109/TPAMI.2015.2430318; Louizos C., 2018, PROC INT C LEARN REP; Ma WC, 2018, LECT NOTES COMPUT SC, V11218, P211, DOI 10.1007/978-3-030-01264-9_13; Maddison Chris J, 2017, ICLR; Matsushita Y, 2004, LECT NOTES COMPUT SC, V3022, P274; Meka A, 2018, PROC CVPR IEEE, P6315, DOI 10.1109/CVPR.2018.00661; Meka A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925907; Narihira T, 2015, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2015.342; Narihira T, 2015, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2015.7298915; Nestmeyer T, 2017, PROC CVPR IEEE, P1771, DOI 10.1109/CVPR.2017.192; Rother C., 2011, ADV NEURAL INFORM PR, P765; Sengupta S, 2019, IEEE I CONF COMP VIS, P8597, DOI 10.1109/ICCV.2019.00869; Sengupta S, 2018, PROC CVPR IEEE, P6296, DOI 10.1109/CVPR.2018.00659; Serra M, 2012, PROC CVPR IEEE, P278, DOI 10.1109/CVPR.2012.6247686; Shaham TR, 2019, IEEE I CONF COMP VIS, P4569, DOI 10.1109/ICCV.2019.00467; Shahar O., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3353, DOI 10.1109/CVPR.2011.5995360; Shechtman E, 2007, PROC CVPR IEEE, P1744; Shen JB, 2013, IEEE T CYBERNETICS, V43, P425, DOI 10.1109/TSMCB.2012.2208744; Shen L, 2008, PROC CVPR IEEE, P2479; Shen L, 2011, PROC CVPR IEEE, P697, DOI 10.1109/CVPR.2011.5995738; Sheng B, 2020, IEEE T VIS COMPUT GR, V26, P1332, DOI 10.1109/TVCG.2018.2869326; Shi J, 2017, PROC CVPR IEEE, P5844, DOI 10.1109/CVPR.2017.619; Shocher A, 2019, IEEE I CONF COMP VIS, P4491, DOI 10.1109/ICCV.2019.00459; Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329; Shu ZX, 2017, PROC CVPR IEEE, P5444, DOI 10.1109/CVPR.2017.578; Sunkavalli K, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239552, 10.1145/1276377.1276504]; Tappen MF, 2005, IEEE T PATTERN ANAL, V27, P1459, DOI 10.1109/TPAMI.2005.185; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Weiss Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P68, DOI 10.1109/ICCV.2001.937606; Williams T., 2018, PROC INT C LEARN REP; Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208; Ye GZ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601135; Yi RJ, 2020, AAAI CONF ARTIF INTE, V34, P12685; Yu Y, 2019, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2019.00327; Zhao Q, 2012, IEEE T PATTERN ANAL, V34, P1437, DOI 10.1109/TPAMI.2012.77; Zhou F, 2020, IEEE T IMAGE PROCESS, V29, P3458, DOI 10.1109/TIP.2019.2961232; Zhou H, 2019, IEEE I CONF COMP VIS, P7819, DOI 10.1109/ICCV.2019.00791; Zhou TH, 2015, IEEE I CONF COMP VIS, P3469, DOI 10.1109/ICCV.2015.396; Zontak M, 2011, PROC CVPR IEEE, P977, DOI 10.1109/CVPR.2011.5995401; Zoran D, 2015, IEEE I CONF COMP VIS, P388, DOI 10.1109/ICCV.2015.52	96	0	0	3	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9669	9686		10.1109/TPAMI.2021.3129795	http://dx.doi.org/10.1109/TPAMI.2021.3129795			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34813466				2022-12-18	WOS:000880661400079
J	Zhang, SY; Peng, HW; Fu, JL; Lu, YJ; Luo, JB				Zhang, Songyang; Peng, Houwen; Fu, Jianlong; Lu, Yijuan; Luo, Jiebo			Multi-Scale 2D Temporal Adjacency Networks for Moment Localization With Natural Language	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Location awareness; Context modeling; Task analysis; Natural languages; Feature extraction; Rats; Semantics		We address the problem of retrieving a specific moment from an untrimmed video by natural language. It is a challenging problem because a target moment may take place in the context of other temporal moments in the untrimmed video. Existing methods cannot tackle this challenge well since they do not fully consider the temporal contexts between temporal moments. In this paper, we model the temporal context between video moments by a set of predefined two-dimensional maps under different temporal scales. For each map, one dimension indicates the starting time of a moment and the other indicates the duration. These 2D temporal maps can cover diverse video moments with different lengths, while representing their adjacent contexts at different temporal scales. Based on the 2D temporal maps, we propose a Multi-Scale Temporal Adjacency Network (MS-2D-TAN), a single-shot framework for moment localization. It is capable of encoding the adjacent temporal contexts at each scale, while learning discriminative features for matching video moments with referring expressions. We evaluate the proposed MS-2D-TAN on three challenging benchmarks, i.e., Charades-STA, ActivityNet Captions, and TACoS, where our MS-2D-TAN outperforms the state of the art.	[Zhang, Songyang; Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA; [Peng, Houwen; Fu, Jianlong; Lu, Yijuan] Microsoft, Redmond, WA 98052 USA	University of Rochester; Microsoft	Peng, HW (corresponding author), Microsoft, Redmond, WA 98052 USA.	szhang83@cs.rochester.edu; houwen.peng@microsoft.com; jianf@microsoft.com; yijlu@microsoft.com; jluo@cs.rochester.edu		Luo, Jiebo/0000-0002-4516-9729	NSF [IIS-1704337, IIS1722847, IIS-1813709]	NSF(National Science Foundation (NSF))	This work was supported in part by NSF under Awards IIS-1704337, IIS1722847, and IIS-1813709 and in part by the our corporate sponsors. (Corresponding author: Houwen Peng.)	Arman Cohan, 2020, Arxiv, DOI arXiv:2004.05150; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen Jingyuan, 2018, P C EMP METH NAT LAN, P162; Chen L, 2020, AAAI CONF ARTIF INTE, V34, P10551; Chen PH, 2020, IEEE T MULTIMEDIA, V22, P2723, DOI 10.1109/TMM.2019.2959977; Chen SX, 2019, AAAI CONF ARTIF INTE, P8199; Chu WS, 2015, PROC CVPR IEEE, P3584, DOI 10.1109/CVPR.2015.7298981; Dehmamy N, 2019, ADV NEUR IN, V32; Deng J., 2009, 2009 IEEE C COMP VIS, P248, DOI [DOI 10.1109/CVPR.2009.5206848, 10.1109/CVPR.2009.5206848]; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Duan X, 2018, ADV NEUR IN, V31; Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563; Ge RZ, 2019, IEEE WINT CONF APPL, P245, DOI 10.1109/WACV.2019.00032; Gella Spandana, 2018, P 2018 C EMP METH NA, P968, DOI DOI 10.18653/V1/D18-1117; Ghosh Soham, 2019, ARXIV190402755, P1984; Hahn M., 2019, PROC IEEECVF C COMPU; Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86; He DL, 2019, AAAI CONF ARTIF INTE, P8393; Hendricks L. A., 2018, PROC C EMPIRICAL MET, P1380; Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jiang B, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P217, DOI 10.1145/3323873.3325019; Jonghwan Mun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10807, DOI 10.1109/CVPR42600.2020.01082; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83; Lei J., 2018, PROC C EMPIRICAL MET; Lin CM, 2021, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR46437.2021.00333; Lin CRN, 2020, AAAI CONF ARTIF INTE, V34, P11499; Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399; Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343; Lin ZJ, 2020, IEEE T IMAGE PROCESS, V29, P3750, DOI 10.1109/TIP.2020.2965987; Liu BB, 2018, LECT NOTES COMPUT SC, V11207, P569, DOI 10.1007/978-3-030-01219-9_34; Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549; Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003; Lu CJ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5144; Mengmeng Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10153, DOI 10.1109/CVPR42600.2020.01017; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Regneri M., 2013, TACL, V1, P25, DOI DOI 10.1162/TACL_A_00207; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rodriguez-Opazo C, 2020, IEEE WINT CONF APPL, P2453, DOI 10.1109/WACV45572.2020.9093328; Rohrbach M, 2012, LECT NOTES COMPUT SC, V7572, P144, DOI 10.1007/978-3-642-33718-5_11; Shao D, 2018, LECT NOTES COMPUT SC, V11213, P202, DOI 10.1007/978-3-030-01240-3_13; Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31; Simonyan K., 2015, VERY DEEP CONVOLUTIO; Song XM, 2018, LECT NOTES COMPUT SC, V11165, P340, DOI 10.1007/978-3-030-00767-6_32; Song YL, 2015, PROC CVPR IEEE, P5179, DOI 10.1109/CVPR.2015.7299154; Escorcia V, 2019, Arxiv, DOI arXiv:1907.12763; Wang JW, 2020, AAAI CONF ARTIF INTE, V34, P12168; Wang WN, 2019, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2019.00042; Wu AM, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1029; Wu J, 2020, AAAI CONF ARTIF INTE, V34, P12386; Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062; Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457; Yuan YT, 2019, ADV NEUR IN, V32; Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159; Zeng RH, 2019, IEEE T IMAGE PROCESS, V28, P5797, DOI 10.1109/TIP.2019.2922108; Zeng Runhao, 2020, P IEEE CVF C COMP VI; Zhang D, 2019, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2019.00134; Zhang Hao, 2020, ARXIV200413931, P6; Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870; Zhang SY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1230, DOI 10.1145/3343031.3350879; Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235; Zhao Y, 2021, PROC CVPR IEEE, P4195, DOI 10.1109/CVPR46437.2021.00418; Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317	66	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					9073	9087		10.1109/TPAMI.2021.3120745	http://dx.doi.org/10.1109/TPAMI.2021.3120745			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34665720				2022-12-18	WOS:000880661400038
J	Zhou, YS; He, Y; Zhu, HZ; Wang, C; Li, HY; Jiang, QH				Zhou, Yunsong; He, Yuan; Zhu, Hongzi; Wang, Cheng; Li, Hongyang; Jiang, Qinhong			MonoEF: Extrinsic Parameter Free Monocular 3D Object Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Cameras; Detectors; Perturbation methods; Object detection; Solid modeling; Roads; Monocular 3D object detection; camera extrinsic parameter; autonomous driving	END-TO-END; DEPTH ESTIMATION; VISUAL ODOMETRY	Monocular 3D object detection is an important task in autonomous driving. It can be easily intractable where there exists ego-car pose change w.r.t. ground plane. This is common due to the slight fluctuation of road smoothness and slope. Due to the lack of insight in industrial application, existing methods on open datasets neglect the camera pose information, which inevitably results in the detector being susceptible to camera extrinsic parameters. The perturbation of objects is very popular in most autonomous driving cases for industrial products. To this end, we propose a novel method to capture camera pose to formulate the detector free from extrinsic perturbation. Specifically, the proposed framework predicts camera extrinsic parameters by detecting vanishing point and horizon change. A converter is designed to rectify perturbative features in the latent space. By doing so, our 3D detector works independent of the extrinsic parameter variations and produces accurate results in realistic cases, e.g., potholed and uneven roads, where almost all existing monocular detectors fail to handle. Experiments demonstrate our method yields the best performance compared with the other state-of-the-arts by a large margin on both KITTI 3D and nuScenes datasets.	[Zhou, Yunsong; Zhu, Hongzi] Shanghai Jiao Tong Univ, Dept Comp Sci & Technol, Shanghai 200240, Peoples R China; [He, Yuan; Wang, Cheng; Li, Hongyang; Jiang, Qinhong] SenseTime Res, Shanghai 200233, Peoples R China	Shanghai Jiao Tong University	Zhou, YS (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Technol, Shanghai 200240, Peoples R China.	zhouyunsong@sjtu.edu.cn; heyuan@senseauto.com; hongzi@cs.sjtu.edu.cn; 458464954@qq.com; lihongyang@senseauto.com; jiangqinhong@senseauto.com		Zhou, Yunsong/0000-0001-5101-331X	National Key R&D Program of China [2018YFC1900700]; National Natural Science Foundation of China [61772340]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by the National Key R&D Program of China under Grant 2018YFC1900700 and in part by the National Natural Science Foundation of China under Grant 61772340.	Alexey Artemov, 2019, Arxiv, DOI arXiv:1905.05618; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bao WT, 2020, IEEE T IMAGE PROCESS, V29, P2753, DOI 10.1109/TIP.2019.2952201; Bertoni L, 2019, IEEE I CONF COMP VIS, P6860, DOI 10.1109/ICCV.2019.00696; Bin Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P496, DOI 10.1007/978-3-030-58523-5_29; Bingbing Liu, 2019, Arxiv, DOI arXiv:1911.09712; Bloesch M, 2018, PROC CVPR IEEE, P2560, DOI 10.1109/CVPR.2018.00271; Brazil Garrick, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P135, DOI 10.1007/978-3-030-58592-1_9; Brazil G, 2019, IEEE I CONF COMP VIS, P9286, DOI 10.1109/ICCV.2019.00938; Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164; Chabot F, 2017, PROC CVPR IEEE, P1827, DOI 10.1109/CVPR.2017.198; Chang CK, 2018, IEEE INT CONF ROBOT, P4496; Chen XZ, 2018, IEEE T PATTERN ANAL, V40, P1259, DOI 10.1109/TPAMI.2017.2706685; Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691; Chen XZ, 2016, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2016.236; Chen XZ, 2015, ADV NEUR IN, V28; Eigen D, 2014, ADV NEUR IN, V27; Gatys L. A., 2015, ADV NEURAL INFORM PR, V28, P262, DOI DOI 10.1016/0014-5793(76)80724-7; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393; Jia Deng, 2020, Arxiv, DOI arXiv:1812.04605; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kim J., 2020, P AS C COMP VIS, P388; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Ku J, 2019, PROC CVPR IEEE, P11859, DOI 10.1109/CVPR.2019.01214; Li PL, 2019, PROC CVPR IEEE, P7636, DOI 10.1109/CVPR.2019.00783; Li Peixuan, 2020, ECCV; Li RH, 2018, IEEE INT CONF ROBOT, P7286; Liang M, 2018, LECT NOTES COMPUT SC, V11220, P663, DOI 10.1007/978-3-030-01270-0_39; Liu ZC, 2020, IEEE COMPUT SOC CONF, P4289, DOI 10.1109/CVPRW50498.2020.00506; Ma XZ, 2019, IEEE I CONF COMP VIS, P6850, DOI 10.1109/ICCV.2019.00695; Major B, 2019, IEEE INT CONF COMP V, P924, DOI 10.1109/ICCVW.2019.00121; Manhardt F, 2019, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2019.00217; Mousavian A, 2017, PROC CVPR IEEE, P5632, DOI 10.1109/CVPR.2017.597; Murthy J. Krishna, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P724, DOI 10.1109/ICRA.2017.7989089; Ng MY, 2020, IEEE COMPUT SOC CONF, P4306, DOI 10.1109/CVPRW50498.2020.00508; Pham CC, 2017, SIGNAL PROCESS-IMAGE, V53, P110, DOI 10.1016/j.image.2017.02.007; Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102; Qin ZY, 2019, PROC CVPR IEEE, P7607, DOI 10.1109/CVPR.2019.00780; Qin ZY, 2019, AAAI CONF ARTIF INTE, P8851; Ranjan A., 2019, CVPR, P12240, DOI DOI 10.1109/CVPR.2019.01252; Seungjun Lee, 2020, Arxiv, DOI arXiv:2003.00851; Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086; Shin K, 2019, IEEE INT VEH SYM, P2510, DOI 10.1109/IVS.2019.8813895; Simonelli A, 2019, IEEE I CONF COMP VIS, P1991, DOI 10.1109/ICCV.2019.00208; Tang C., 2018, ARXIV; Tang Y, 2020, DAGM GERMAN C PATTER, P289; Ummenhofer B, 2017, PROC CVPR IEEE, P5622, DOI 10.1109/CVPR.2017.596; van Dijk T, 2019, IEEE I CONF COMP VIS, P2183, DOI 10.1109/ICCV.2019.00227; Wang R, 2019, PROC CVPR IEEE, P5647, DOI 10.1109/CVPR.2019.00570; Wang S, 2018, INT J ROBOT RES, V37, P513, DOI 10.1177/0278364917734298; Wang Sen, 2017, 2017 IEEE INT C ROB, P2043, DOI 10.1109/ICRA.2017.7989236; Wang Y, 2019, PROC CVPR IEEE, P8063, DOI 10.1109/CVPR.2019.00826; Xiang Y, 2017, IEEE WINT CONF APPL, P924, DOI 10.1109/WACV.2017.108; Xinzhu Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P311, DOI 10.1007/978-3-030-58601-0_19; Xu B, 2018, PROC CVPR IEEE, P2345, DOI 10.1109/CVPR.2018.00249; Xue F., 2018, PROC ASIAN C COMPUT, P293; Xue F, 2019, PROC CVPR IEEE, P8567, DOI 10.1109/CVPR.2019.00877; Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255; Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105; Zhan HY, 2018, PROC CVPR IEEE, P340, DOI 10.1109/CVPR.2018.00043; Zhou HZ, 2018, LECT NOTES COMPUT SC, V11220, P851, DOI 10.1007/978-3-030-01270-0_50; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700; Zhou X., 2019, ARXIV, DOI DOI 10.48550/ARXIV.1904.07850; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472; Zou Y., 2020, P 16 EUR C COMP VIS, P710	67	0	0	2	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					10114	10128		10.1109/TPAMI.2021.3136899	http://dx.doi.org/10.1109/TPAMI.2021.3136899			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34932471				2022-12-18	WOS:000880661400109
J	Zhu, JJ; Liu, YG; Zhang, Y; Chen, Z; Wu, XD				Zhu, Jiajing; Liu, Yongguo; Zhang, Yun; Chen, Zhi; Wu, Xindong			Multi-Attribute Discriminative Representation Learning for Prediction of Adverse Drug-Drug Interaction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Drugs; Biochemistry; Predictive models; Feature extraction; Representation learning; Convolutional neural networks; Syntactics; Adverse drug-drug interaction; multi-attribute; discriminative feature selection; representative drug selection; representation learning	LARGE-SCALE PREDICTION; INTERACTION EXTRACTION; MULTIVIEW; FRAMEWORK; MACHINE	Adverse drug-drug interaction (ADDI) is a significant life-threatening issue, posing a leading cause of hospitalizations and deaths in healthcare systems. This paper proposes a unified Multi-Attribute Discriminative Representation Learning (MADRL) model for ADDI prediction. Unlike the existing works that equally treat features of each attribute without discrimination and do not consider the underlying relationship among drugs, we first develop a regularized optimization problem based on CUR matrix decomposition for joint representative drug and discriminative feature selection such that the selected drugs and features can well approximate the original feature spaces and the critical factors discriminative to ADDIs can be properly explored. Different from the existing models that ignore the consistent and unique properties among attributes, a Generative Adversarial Network (GAN) framework is then designed to capture the inter-attribute shared and intra-attribute specific representations of adverse drug pairs for exploiting their consensus and complementary information in ADDI prediction. Meanwhile, MADRL is compatible with any kind of attributes and capable of exploring their respective effects on ADDI prediction. An iterative algorithm based on the alternating direction method of multipliers is developed for optimization. Experiments on publicly available dataset demonstrate the effectiveness of MADRL when compared with eleven baselines and its six variants.	[Zhu, Jiajing; Liu, Yongguo; Zhang, Yun; Chen, Zhi] Univ Elect Sci & Technol China, Sch Informat & Software Engn, Knowledge & Data Engn Lab Chinese Med, Chengdu 610054, Peoples R China; [Wu, Xindong] Hefei Univ Technol, Minist Educ China, Key Lab Knowledge Engn Big Data, Hefei 230009, Peoples R China; [Wu, Xindong] Mininglamp Acad Sci, Mininglamp Technol, Beijing 100190, Peoples R China	University of Electronic Science & Technology of China; Hefei University of Technology; Ministry of Education, China	Liu, YG (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Software Engn, Knowledge & Data Engn Lab Chinese Med, Chengdu 610054, Peoples R China.	jjzhu_cn@163.com; liuyg@uestc.edu.cn; yunzhangwww@163.com; zhic_cn@163.com; xwu@hfut.edu.cn		Wu, Xindong/0000-0003-2396-1704; zhang, yun/0000-0001-8716-4179	National Key R&D Program of China [2019YFC1710300, 2017YFC1703905]; Sichuan Science and Technology Program [2020YFS0372]; Fundamental Research Funds for the Central Universities [ZYGX2021YGLH012, ZYGX2021J020]; China Postdoctoral Science Foundation [2021M690028]; National Natural Science Foundation of China (NSFC) [62120106008]	National Key R&D Program of China; Sichuan Science and Technology Program; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); China Postdoctoral Science Foundation(China Postdoctoral Science Foundation); National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	This work was supported in part by the National Key R&D Program of China under Grants 2019YFC1710300 and 2017YFC1703905, in part by Sichuan Science and Technology Program under Grant 2020YFS0372, in part by the Fundamental Research Funds for the Central Universities under Grants ZYGX2021YGLH012 and ZYGX2021J020, in part by Project funded by China Postdoctoral Science Foundation under Grant 2021M690028, and in part by the National Natural Science Foundation of China (NSFC) under Grant 62120106008.	Baytas IM, 2018, IEEE DATA MINING, P875, DOI 10.1109/ICDM.2018.00104; Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231; Chen N, 2012, IEEE T PATTERN ANAL, V34, P2365, DOI 10.1109/TPAMI.2012.64; Chen XH, 2012, PATTERN RECOGN, V45, P2005, DOI 10.1016/j.patcog.2011.11.008; Cheng FX, 2014, J AM MED INFORM ASSN, V21, pE278, DOI 10.1136/amiajnl-2013-002512; Chu X, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4518; Covas A, 2007, CEPHALALGIA, V27, P676; Davis AP, 2013, NUCLEIC ACIDS RES, V41, pD1104, DOI 10.1093/nar/gks994; Delafoy C, 2019, FUND CLIN PHARMACOL, V33, P70; Hou CP, 2014, IEEE T CYBERNETICS, V44, P793, DOI 10.1109/TCYB.2013.2272642; Hu JL, 2018, IEEE T PATTERN ANAL, V40, P2281, DOI 10.1109/TPAMI.2017.2749576; Huang DG, 2017, INFORM SCIENCES, V415, P100, DOI 10.1016/j.ins.2017.06.021; Huang KX, 2020, AAAI CONF ARTIF INTE, V34, P702; Jia XD, 2021, IEEE T PATTERN ANAL, V43, P2496, DOI 10.1109/TPAMI.2020.2973634; Jin B, 2017, AAAI CONF ARTIF INTE, P1367; Jing XY, 2014, AAAI CONF ARTIF INTE, P1882; Kanehisa M, 2017, NUCLEIC ACIDS RES, V45, pD353, DOI 10.1093/nar/gkw1092; Klekota J, 2008, BIOINFORMATICS, V24, P2518, DOI 10.1093/bioinformatics/btn479; Kuhn M, 2016, NUCLEIC ACIDS RES, V44, pD1075, DOI 10.1093/nar/gkv1075; Lee CY, 2019, DRUG DISCOV TODAY, V24, P1332, DOI 10.1016/j.drudis.2019.03.003; Li CS, 2019, IEEE T PATTERN ANAL, V41, P1382, DOI 10.1109/TPAMI.2018.2840980; Li JX, 2019, INFORM FUSION, V45, P215, DOI 10.1016/j.inffus.2018.02.005; Li YM, 2019, IEEE T KNOWL DATA EN, V31, P1863, DOI 10.1109/TKDE.2018.2872063; Lin X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2739; Lin Z., 2011, PROC INT 25 C NEURAL, P612, DOI DOI 10.1007/S11263-013-0611-6; Liu M, 2012, J AM MED INFORM ASSN, V19, pE28, DOI 10.1136/amiajnl-2011-000699; Liu YT, 2017, PR IEEE COMP DESIGN, P45, DOI 10.1109/ICCD.2017.16; Ma TF, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3477; Mahoney MW, 2009, P NATL ACAD SCI USA, V106, P697, DOI [10.1073/pnas.0803205105, 10.1073/pnas.0803205106]; Masoumi HT, 2019, J ONCOL PHARM PRACT, V25, P1239, DOI 10.1177/1078155218783248; Ryu JY, 2018, P NATL ACAD SCI USA, V115, pE4304, DOI 10.1073/pnas.1803294115; Seong SJ, 2019, ADV THER, V36, P1642, DOI 10.1007/s12325-019-00976-9; Shi JY, 2019, J CHEMINFORMATICS, V11, DOI 10.1186/s13321-019-0352-9; Takigawa I, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0016999; Tatonetti NP, 2012, SCI TRANSL MED, V4, DOI 10.1126/scitranslmed.3003377; Tatonetti NP, 2012, J AM MED INFORM ASSN, V19, P79, DOI 10.1136/amiajnl-2011-000214; Tomasic T, 2020, CHEMMEDCHEM, V15, P726, DOI 10.1002/cmdc.202000095; Vilar S, 2014, NAT PROTOC, V9, P2147, DOI 10.1038/nprot.2014.151; Vilar S, 2012, J AM MED INFORM ASSN, V19, P1066, DOI 10.1136/amiajnl-2012-000935; Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311; Wang WR, 2015, PR MACH LEARN RES, V37, P1083; Wishart DS, 2008, NUCLEIC ACIDS RES, V36, pD901, DOI 10.1093/nar/gkm958; Xu JL, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3161; Yi ZB, 2017, LECT NOTES ARTIF INT, V10604, P554, DOI 10.1007/978-3-319-69179-4_39; Zhang MH, 2016, ONCOL LETT, V12, P3285, DOI 10.3892/ol.2016.5039; Zhang PW, 2015, SCI REP-UK, V5, DOI 10.1038/srep13501; Zhang W, 2019, INFORM SCIENCES, V497, P189, DOI 10.1016/j.ins.2019.05.017; Zhao ZH, 2016, BIOINFORMATICS, V32, P3444, DOI 10.1093/bioinformatics/btw486; Zheng W, 2017, BMC BIOINFORMATICS, V18, DOI 10.1186/s12859-017-1855-x; Zhu JJ, 2022, IEEE T KNOWL DATA EN, V34, P271, DOI 10.1109/TKDE.2020.2978055; Zhu JJ, 2020, KNOWL-BASED SYST, V199, DOI 10.1016/j.knosys.2020.105978; Zitnik M, 2018, BIOINFORMATICS, V34, P457, DOI 10.1093/bioinformatics/bty294	53	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC 1	2022	44	12					10129	10144		10.1109/TPAMI.2021.3135841	http://dx.doi.org/10.1109/TPAMI.2021.3135841			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	6A4XX	34914581				2022-12-18	WOS:000880661400110
J	Badias, A; Alfaro, I; Gonzalez, D; Chinesta, F; Cueto, E				Badias, Alberto; Alfaro, Iciar; Gonzalez, David; Chinesta, Francisco; Cueto, Elias			MORPH-DSLAM: Model Order Reduction for Physics-Based Deformable SLAM	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Cameras; Shape; Physics; Strain; Solids; Simultaneous localization and mapping; Computer vision; augmented reality; deformable solids; computational mechanics; model order reduction; machine learning	STRUCTURE-FROM-MOTION; SHAPE; DECOMPOSITION; EFFICIENT; TRACKING	We propose a new methodology to estimate the 3D displacement field of deformable objects from video sequences using standard monocular cameras. We solve in real time the complete (possibly visco-)hyperelasticity problem to properly describe the strain and stress fields that are consistent with the displacements captured by the images, constrained by real physics. We do not impose any ad-hoc prior or energy minimization in the external surface, since the real and complete mechanics problem is solved. This means that we can also estimate the internal state of the objects, even in occluded areas, just by observing the external surface and the knowledge of material properties and geometry. Solving this problem in real time using a realistic constitutive law, usually non-linear, is out of reach for current systems. To overcome this difficulty, we solve off-line a parametrized problem that considers each source of variability in the problem as a new parameter and, consequently, as a new dimension in the formulation. Model Order Reduction methods allow us to reduce the dimensionality of the problem, and therefore, its computational cost, while preserving the visualization of the solution in the high-dimensionality space. This allows an accurate estimation of the object deformations, improving also the robustness in the 3D points estimation.	[Badias, Alberto; Alfaro, Iciar; Gonzalez, David; Cueto, Elias] Univ Zaragoza, Inst Invest Ingn Aragon, Zaragoza 50009, Spain; [Chinesta, Francisco] Arts & Metiers Inst Technol, PIMM Lab, F-75013 Paris, France	University of Zaragoza	Badias, A (corresponding author), Univ Zaragoza, Inst Invest Ingn Aragon, Zaragoza 50009, Spain.	abadias@unizar.es; iciar@unizar.es; gonzal@unizar.es; francisco.chinesta@ensam.eu; ecueto@unizar.es	Gonzalez Ibanez, David/G-1536-2011; Cueto, Elias/A-2452-2010; /K-7500-2015	Gonzalez Ibanez, David/0000-0003-3003-5856; Cueto, Elias/0000-0003-1017-4381; Badias, Alberto/0000-0001-7639-6767; /0000-0002-9135-866X	ESI Group through ESI Chair at ENSAMArts et Metiers Institute of Technology [2019-0060]; Spanish Ministry of Economy and Competitiveness [CICYT-DPI2017-85139-C2-1-R]; Regional Government of Aragon [T24-20R]; European Social Fund [T24-20R]	ESI Group through ESI Chair at ENSAMArts et Metiers Institute of Technology; Spanish Ministry of Economy and Competitiveness(Spanish Government); Regional Government of Aragon(Gobierno de Aragon); European Social Fund(European Social Fund (ESF))	This work was supported in part by the ESI Group through ESI Chair at ENSAMArts et Metiers Institute of Technology, and the Project No. 2019-0060 "Simulated Reality" at the University of Zaragoza. Authors also thank the support of the Spanish Ministry of Economy and Competitiveness, under Grant CICYT-DPI2017-85139-C2-1-R and the Regional Government of Aragon and the European Social Fund, Project No. T24-20R.	Agudo A, 2018, COMPUT VIS IMAGE UND, V167, P121, DOI 10.1016/j.cviu.2018.01.002; Agudo A, 2016, IEEE T PATTERN ANAL, V38, P979, DOI 10.1109/TPAMI.2015.2469293; Akhter Ijaz, 2009, P NIPS; Alfaro I, 2015, ADV MODELING SIMULAT, V2, P30, DOI DOI 10.1186/s40323-015-0050-8; Ammar A, 2010, COMPUT METHOD APPL M, V199, P1872, DOI 10.1016/j.cma.2010.02.012; Bartoli A, 2015, IEEE T PATTERN ANAL, V37, P2099, DOI 10.1109/TPAMI.2015.2392759; Benner P, 2015, SIAM REV, V57, P483, DOI 10.1137/130932715; BERKOOZ G, 1993, ANNU REV FLUID MECH, V25, P539, DOI 10.1146/annurev.fl.25.010193.002543; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Bonet J, 2008, NONLINEAR CONTINUUM MECHANICS FOR FINITE ELEMENT ANALYSIS, 2ND EDITION, P1, DOI 10.1017/CBO9780511755446; Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chinesta F, 2013, ARCH COMPUT METHOD E, V20, P31, DOI 10.1007/s11831-013-9080-x; Chinesta F, 2011, ARCH COMPUT METHOD E, V18, P395, DOI 10.1007/s11831-011-9064-7; Cole DM, 2006, IEEE INT CONF ROBOT, P1556, DOI 10.1109/ROBOT.2006.1641929; Cueto E., 2016, PROPER GEN DECOMPOSI, DOI DOI 10.1007/978-3-319-29994-5; Rosen DM, 2021, Arxiv, DOI arXiv:2103.05041; Davison A. J, 2019, PROC 2 INT WORKSHOP, P0162; Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049; Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fish J., 2007, 1 COURSE FINITE ELEM; Gallardo M, 2020, INT J COMPUT VISION, V128, P121, DOI 10.1007/s11263-019-01214-z; Garg R, 2013, PROC CVPR IEEE, P1272, DOI 10.1109/CVPR.2013.168; Gotardo PFU, 2011, IEEE I CONF COMP VIS, P802, DOI 10.1109/ICCV.2011.6126319; Gutierrez-Gomez D, 2016, ROBOT AUTON SYST, V75, P571, DOI 10.1016/j.robot.2015.09.026; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Ibanez R, 2018, COMPLEXITY, DOI 10.1155/2018/5608286; Ilic S, 2007, IEEE I CONF COMP VIS, P936; Karhunen K, 1947, LINEARE METHODEN WAH, V37; Kennedy M, 2019, IEEE ROBOT AUTOM LET, V4, P2317, DOI 10.1109/LRA.2019.2902075; Klein G., 2007, 2007 6 IEEE ACM INT, P225; Kong C, 2016, PROC CVPR IEEE, P4123, DOI 10.1109/CVPR.2016.447; Lamarca J, 2021, IEEE T ROBOT, V37, P291, DOI 10.1109/TRO.2020.3020739; Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6; Levenberg K., 1944, Q APPL MATH, V2, P164, DOI 10.1090/qam/10666; Li Yunzhu, 2019, ICLR; Loeve M., 2017, PROBABILITY THEORY; MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030; McInerney T., 1993, [1993] Proceedings Fourth International Conference on Computer Vision, P518, DOI 10.1109/ICCV.1993.378169; Metaxas D., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P337, DOI 10.1109/CVPR.1991.139712; de Almeida JPM, 2013, INT J NUMER METH ENG, V94, P961, DOI 10.1002/nme.4490; Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103; Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671; Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513; Novoselac S, 2012, TEH VJESN, V19, P695; Parashar S, 2022, IEEE T PATTERN ANAL, V44, P6409, DOI 10.1109/TPAMI.2021.3089923; Paz LM, 2008, IEEE T ROBOT, V24, P946, DOI 10.1109/TRO.2008.2004637; Quarteroni Alfio., 2015, UNITEXT MATEMATICA 3, V92, DOI DOI 10.1007/978-3-319-15431-2; SCHULGASSER K, 1981, FIBRE SCI TECHNOL, V15, P257, DOI 10.1016/0015-0568(81)90051-8; Smith R., 1990, ESTIMATING UNCERTAIN; Stoica I, 2017, UCBEECS2017159; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; Yokoyama T., 2007, DIMENSIONS-NBS, V60; Zhu YY, 2014, PROC CVPR IEEE, P1542, DOI 10.1109/CVPR.2014.200	57	0	0	5	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7764	7777		10.1109/TPAMI.2021.3118802	http://dx.doi.org/10.1109/TPAMI.2021.3118802			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34623262	Green Submitted			2022-12-18	WOS:000864325900037
J	Baik, S; Oh, J; Hong, S; Lee, KM				Baik, Sungyong; Oh, Junghoon; Hong, Seokil; Lee, Kyoung Mu			Learning to Forget for Meta-Learning via Task-and-Layer-Wise Attenuation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Optimization; Adaptation models; Attenuation; Knowledge engineering; Visualization; Neural networks; Meta-learning; few-shot learning; MAML; reinforcement learning; visual tracking		Few-shot learning is an emerging yet challenging problem in which the goal is to achieve generalization from only few examples. Meta-learning tackles few-shot learning via the learning of prior knowledge shared across tasks and using it to learn new tasks. One of the most representative meta-learning algorithms is the model-agnostic meta-learning (MAML), which formulates prior knowledge as a common initialization, a shared starting point from where a learner can quickly adapt to unseen tasks. However, forcibly sharing an initialization can lead to conflicts among tasks and the compromised (undesired by tasks) location on optimization landscape, thereby hindering task adaptation. Furthermore, the degree of conflict is observed to vary not only among the tasks but also among the layers of a neural network. Thus, we propose task-and-layer-wise attenuation on the compromised initialization to reduce its adverse influence on task adaptation. As attenuation dynamically controls (or selectively forgets) the influence of the compromised prior knowledge for a given task and each layer, we name our method Learn to Forget (L2F). Experimental results demonstrate that the proposed method greatly improves the performance of the state-of-the-art MAML-based frameworks across diverse domains: few-shot classification, cross-domain few-shot classification, regression, reinforcement learning, and visual tracking.	[Baik, Sungyong; Oh, Junghoon; Hong, Seokil; Lee, Kyoung Mu] Seoul Natl Univ, Automat & Syst Res Inst ASRI, Dept Elect & Comp Engn, Seoul 08826, South Korea	Seoul National University (SNU)	Lee, KM (corresponding author), Seoul Natl Univ, Automat & Syst Res Inst ASRI, Dept Elect & Comp Engn, Seoul 08826, South Korea.	dsybaik@snu.ac.kr; dh6dh@snu.ac.kr; hongceo96@snu.ac.kr; kyoungmu@snu.ac.kr			IITP through the Korea government [2021-0-01343]; AIRS Company in Hyundai Motor and Kia through HMC/KIA-SNU AI Consortium Fund	IITP through the Korea government; AIRS Company in Hyundai Motor and Kia through HMC/KIA-SNU AI Consortium Fund	This work was supported in part by IITP through the Korea government under Grant 2021-0-01343 [Artificial Intelligence Graduate School Program (Seoul National University)] and in part by AIRS Company in Hyundai Motor and Kia through HMC/KIA-SNU AI Consortium Fund.	Alex Nichol, 2018, Arxiv, DOI arXiv:1803.02999; Antoniou A., 2019, PROC INT C LEARN REP; Baik S, 2020, PROC CVPR IEEE, P2376, DOI 10.1109/CVPR42600.2020.00245; Bengio Samy, 1992, OPTIMIZATION SYNAPTI; Bertinetto Luca, 2019, INT C LEARN REPR, P2; Chen Wei-Yu, 2019, INT C LEARN REPR, P12; Chen Z., 2020, ADV NEURAL INF PROCE, V33, P2039; Duan Y, 2016, PR MACH LEARN RES, V48; Fei Chen, 2017, Arxiv, DOI arXiv:1707.09835; Finn C, 2017, PR MACH LEARN RES, V70; Flennerhag Sebastian, 2020, INT C LEARN REPR; Granet A, 2018, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS (ICPRAM 2018), P432, DOI 10.5220/0006598804320439; Hochreiter Sepp, 2001, INT C ART NEUR NETW, P87, DOI [10.1007/3-540-44668-0, DOI 10.1007/3-540-44668-0]; Hung-Yu Tseng, 2020, Arxiv, DOI arXiv:2004.05859; Jamal MA, 2019, PROC CVPR IEEE, P11711, DOI 10.1109/CVPR.2019.01199; James S, 2020, IEEE ROBOT AUTOM LET, V5, P3019, DOI 10.1109/LRA.2020.2974707; Jiang X., 2019, P INT C LEARN REPR; Koch G., 2015, ICML DEEP LEARN WORK; Kristan M., 2014, P INT C COMP VIS, P191; Kristan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P564, DOI 10.1109/ICCVW.2015.79; Kristan M, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P98, DOI 10.1109/ICCVW.2013.20; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091; Lee Y, 2018, PR MACH LEARN RES, V80; Li H, 2018, ADV NEUR IN, V31; Mishra N., 2018, INT C LEARN REPR; Munkhdalai T, 2018, PR MACH LEARN RES, V80; Munkhdalai T, 2017, PR MACH LEARN RES, V70; Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465; Oreshkin BN, 2018, ADV NEUR IN, V31; Park E, 2018, LECT NOTES COMPUT SC, V11207, P587, DOI 10.1007/978-3-030-01219-9_35; Raghu Aniruddh, 2020, ICLR; Ravi S., 2017, INT C LEARN REPR, P12; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Rusu Andrei A, 2019, ICLR; Santoro A, 2016, PR MACH LEARN RES, V48; Santurkar S, 2018, ADV NEUR IN, V31; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131; Schmidhuber J, 1987, THESIS; Simonyan K., 2015, INT C LEARN REPR ICL; Snell J, 2017, ADV NEUR IN, V30; Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Thrun S., 2012, LEARNING LEARN; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Triantafillou E., 2020, PROC INT C LEARN REP; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Vuorio R, 2019, ADV NEUR IN, V32; Wah C., 2011, TECH REP; Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226; Yao HX, 2019, PR MACH LEARN RES, V97; Yin M., 2020, P ICLR, P1; Yu Tang, 2020, arXiv; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang RX, 2018, ADV NEUR IN, V31; Zintgraf L, 2019, PR MACH LEARN RES, V97	57	0	0	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7718	7730		10.1109/TPAMI.2021.3102098	http://dx.doi.org/10.1109/TPAMI.2021.3102098			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34347593				2022-12-18	WOS:000864325900034
J	Cao, TW; Xu, QQ; Yang, ZY; Huang, QM				Cao, Tianwei; Xu, Qianqian; Yang, Zhiyong; Huang, Qingming			Meta-Wrapper: Differentiable Wrapping Operator for User Interest Selection in CTR Prediction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Feature extraction; Predictive models; Wrapping; Frequency modulation; Computational modeling; Training; Recommender systems; Click-through rate prediction; recommender system; bilevel optimization; meta-learning		Click-through rate (CTR) prediction, whose goal is to predict the probability of the user to click on an item, has become increasingly significant in the recommender systems. Recently, some deep learning models with the ability to automatically extract the user interest from his/her behaviors have achieved great success. In these work, the attention mechanism is used to select the user interested items in historical behaviors, improving the performance of the CTR predictor. Normally, these attentive modules can be jointly trained with the base predictor by using gradient descents. In this paper, we regard user interest modeling as a feature selection problem, which we call user interest selection. For such a problem, we propose a novel approach under the framework of the wrapper method, which is named Meta-Wrapper. More specifically, we use a differentiable module as our wrapping operator and then recast its learning problem as a continuous bilevel optimization. Moreover, we use a meta-learning algorithm to solve the optimization and theoretically prove its convergence. Meanwhile, we also provide theoretical analysis to show that our proposed method 1) efficiencies the wrapper-based feature selection, and 2) achieves better resistance to overfitting. Finally, extensive experiments on three public datasets manifest the superiority of our method in boosting the performance of CTR prediction.	[Cao, Tianwei; Yang, Zhiyong] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China; [Xu, Qianqian; Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China; [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Key Lab Big Data Min & Knowledge Management BDKM, Beijing 101408, Peoples R China; [Huang, Qingming] Peng Cheng Lab, Shenzhen 518055, Peoples R China	Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Institute of Computing Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Peng Cheng Laboratory	Xu, QQ; Huang, QM (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.; Huang, QM (corresponding author), Univ Chinese Acad Sci, Sch Comp Sci & Technol, Key Lab Big Data Min & Knowledge Management BDKM, Beijing 101408, Peoples R China.	caotianwei19@mails.ucas.ac.cn; xuqianqian@ict.ac.cn; yangzhiyong@iie.ac.cn; qmhuang@ucas.ac.cn			National Key R&D Program of China [2018AAA0102003]; National Natural Science Foundation of China [61931008, 61620106009, 61836002, 61976202]; Fundamental Research Funds for the Central Universities; Youth Innovation Promotion Association CAS; Strategic Priority Research Program of Chinese Academy of Sciences [XDB28000000]; National Postdoctoral Program for Innovative Talents [BX2021298]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Youth Innovation Promotion Association CAS; Strategic Priority Research Program of Chinese Academy of Sciences(Chinese Academy of Sciences); National Postdoctoral Program for Innovative Talents	This work was supported in part by the National Key R&D Program of China under Grant 2018AAA0102003, in part by the National Natural Science Foundation of China under Grants 61931008, 61620106009, 61836002, and 61976202, in part by the Fundamental Research Funds for the Central Universities, in part by Youth Innovation Promotion Association CAS, in part by the Strategic Priority Research Program of Chinese Academy of Sciences under Grant XDB28000000, and in part by the National Postdoctoral Program for Innovative Talents under Grant BX2021298.	Abadi M., TENSORFLOW LARGE SCA; Bahdanau D., 2015, P 3 INT C LEARNING R; Bengio Y., FEATURE EXTRACTION F; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Chapelle O, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1097, DOI 10.1145/2623330.2623634; Cheng H., 2016, DLRS 2016PROCEEDINGS, P7, DOI [10.1145/2988450.2988454, DOI 10.1145/2988450.2988454]; Duda R. O., 2012, PATTERN CLASSIFICATI; Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276; Feng FL, 2021, IEEE T KNOWL DATA EN, V33, P2493, DOI 10.1109/TKDE.2019.2957786; Feng YF, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2301; Finn C, 2017, PR MACH LEARN RES, V70; Franceschi L, 2018, PR MACH LEARN RES, V80; Girshick R., 2014, PROC IEEE C COMPUT V; Griewank A, 2008, OTHER TITL APPL MATH, V105, P1, DOI 10.1137/1.9780898717761; Gui J, 2017, IEEE T NEUR NET LEAR, V28, P1490, DOI 10.1109/TNNLS.2016.2551724; Guo HF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1725; Ha David, 2017, ICLR; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He X, 2014, DMOA, P1; He XN, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2227; He XN, 2018, IEEE T KNOWL DATA EN, V30, P2354, DOI 10.1109/TKDE.2018.2831682; He XN, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P355, DOI 10.1145/3077136.3080777; He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569; Hong FX, 2019, AAAI CONF ARTIF INTE, P3804; Jaderberg M, 2015, ADV NEUR IN, V28; Jahrer M., 2012, PROC KDDCUP WORKSHOP; Juan YC, 2017, WWW'17 COMPANION: PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P680, DOI 10.1145/3041021.3054185; Kabir MM, 2010, NEUROCOMPUTING, V73, P3273, DOI 10.1016/j.neucom.2010.04.003; Koren Y., 2008, P 14 ACM SIGKDD INT, P426, DOI DOI 10.1145/1401890.1401944; Lan L, 2019, AAAI CONF ARTIF INTE, P4139; Laskar Z, 2017, LECT NOTES COMPUT SC, V10270, P88, DOI 10.1007/978-3-319-59129-2_8; Li S., 2021, ACMTRANS INF SYST, V1; Li YF, 2015, LECT N BIOINFORMAT, V9029, P205, DOI 10.1007/978-3-319-16706-0_20; Lian JX, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1754, DOI 10.1145/3219819.3220023; Lin M, 2019, AAAI CONF ARTIF INTE, P4312; Lin T., 2020, INT C MACHINE LEARNI, P6083; Liu B, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2636, DOI 10.1145/3394486.3403314; Liu B, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P199, DOI 10.1145/3397271.3401082; Liu B, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P1119, DOI 10.1145/3308558.3313497; Liu HF, 2016, IEEE DATA MINING, P281, DOI [10.1109/ICDM.2016.37, 10.1109/ICDM.2016.0039]; Liu J., 2009, P 25 C UNCERTAINTY A, P339, DOI DOI 10.5555/1795114.1795154; Liu WW, 2018, 12TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS), P412, DOI 10.1145/3240323.3240396; Liu Y, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P419, DOI 10.1145/3397271.3401087; Ma FL, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1903, DOI 10.1145/3097983.3098088; McMahan HB, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1222; Ming D, 2019, AAAI CONF ARTIF INTE, P4586; Pan JW, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1349, DOI 10.1145/3178876.3186040; Paszke A, 2017, PROC C NEURAL INF PR; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159; Pi Q, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2671, DOI 10.1145/3292500.3330666; Qu YR, 2016, IEEE DATA MINING, P1149, DOI [10.1109/ICDM.2016.57, 10.1109/ICDM.2016.0151]; Ren K, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P565, DOI 10.1145/3331184.3331230; Rendle S, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P635; Roy D, 2015, IEEE IJCNN; Shan Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P255, DOI 10.1145/2939672.2939704; Shumway R., 1987, TECHNOMETRICS, V29; Song WP, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P1161, DOI 10.1145/3357384.3357925; Tang J., 2014, DATA CLASSIFICATION, P37, DOI DOI 10.1201/B17320; Vaswani A, 2017, ADV NEUR IN, V30; Vilalta R, 2002, ARTIF INTELL REV, V18, P77, DOI 10.1023/A:1019956318069; Wang RX, 2017, ADKDD'17: 23RD ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD 2017), DOI 10.1145/3124749.3124754; Xiao J, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3119; Xiao ZB, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P2265, DOI 10.1145/3340531.3412092; Xin X, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3926; Yan L, 2014, PR MACH LEARN RES, V32, P802; Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512; Zhang W.N., 2016, P EUR C INF RETR ECI, P45; Zhou GR, 2019, AAAI CONF ARTIF INTE, P5941; Zhou GR, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1059, DOI 10.1145/3219819.3219823	70	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8449	8464		10.1109/TPAMI.2021.3103741	http://dx.doi.org/10.1109/TPAMI.2021.3103741			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34375282	Green Submitted			2022-12-18	WOS:000864325900082
J	Chen, S; Jiang, M; Yang, JH; Zhao, Q				Chen, Shi; Jiang, Ming; Yang, Jinhui; Zhao, Qi			Attention in Reasoning: Dataset, Analysis, and Modeling	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cognition; Visualization; Task analysis; Computational modeling; Analytical models; Atmospheric modeling; Atomic measurements; Attention; reasoning; eye-tracking dataset	SALIENCY	While attention has been an increasingly popular component in deep neural networks to both interpret and boost the performance of models, little work has examined how attention progresses to accomplish a task and whether it is reasonable. In this work, we propose an Attention with Reasoning capability (AiR) framework that uses attention to understand and improve the process leading to task outcomes. We first define an evaluation metric based on a sequence of atomic reasoning operations, enabling a quantitative measurement of attention that considers the reasoning process. We then collect human eye-tracking and answer correctness data, and analyze various machine and human attention mechanisms on their reasoning capability and how they impact task performance. To improve the attention and reasoning ability of visual question answering models, we propose to supervise the learning of attention progressively along the reasoning process and to differentiate the correct and incorrect attention patterns. We demonstrate the effectiveness of the proposed framework in analyzing and modeling attention with better reasoning capability and task performance. The code and data are available at https://github.com/szzexpoi/AiR.	[Chen, Shi; Jiang, Ming; Yang, Jinhui; Zhao, Qi] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities	Zhao, Q (corresponding author), Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.	chen4595@umn.edu; mjiang@umn.edu; yang7004@umn.edu; qzhao@cs.umn.edu		Chen, Shi/0000-0002-3749-4767; Yang, Jinhui/0000-0001-8322-1121	NSF [1908711, 1849107]	NSF(National Science Foundation (NSF))	This work was supported by NSF Grants 1908711 and 1849107.	Ali Borji, 2015, Arxiv, DOI arXiv:1505.03581; Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285; Borji A, 2021, IEEE T PATTERN ANAL, V43, P679, DOI 10.1109/TPAMI.2019.2935715; Borji A, 2013, IEEE I CONF COMP VIS, P921, DOI 10.1109/ICCV.2013.118; Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89; Borji A, 2011, IEEE INT CONF ROBOT, P1902; Bruce N., 2005, P 18 INT C NEUR INF, P155; Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601; Cerf M., 2008, ADV NEURAL INFORM PR, V20, P241, DOI DOI 10.1145/2185520.2185525; Chen L., 2005, P 2005 ACM SIGMOD IN, P491, DOI DOI 10.1145/1066157.1066213; Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667; Cho K., 2014, P 2014 C EMP METH NA, P1724; Chou SH, 2020, IEEE WINT CONF APPL, P1596, DOI 10.1109/WACV45572.2020.9093452; Christopher D. Manning, 2018, Arxiv, DOI arXiv:1803.03067; Das A, 2017, COMPUT VIS IMAGE UND, V163, P90, DOI 10.1016/j.cviu.2017.10.001; Ehinger KA, 2009, VIS COGN, V17, P945, DOI 10.1080/13506280902834720; Fan SJ, 2018, PROC CVPR IEEE, P7521, DOI 10.1109/CVPR.2018.00785; Fukui Akira, 2016, ARXIV160601847; Fukui H, 2019, PROC CVPR IEEE, P10697, DOI 10.1109/CVPR.2019.01096; Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139; Gao P, 2019, PROC CVPR IEEE, P6632, DOI 10.1109/CVPR.2019.00680; Garcia N, 2020, AAAI CONF ARTIF INTE, V34, P10826; Gilani SO, 2015, IEEE INT CON MULTI; Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670; Han S, 2010, VISION RES, V50, P2295, DOI 10.1016/j.visres.2010.05.034; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He S, 2019, IEEE I CONF COMP VIS, P8528, DOI 10.1109/ICCV.2019.00862; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93; Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473; Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069; Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686; Ilievski I, 2017, ADV NEUR IN, V30; Jiang M, 2020, PROC CVPR IEEE, P2977, DOI 10.1109/CVPR42600.2020.00305; Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710; Jiang M, 2014, LECT NOTES COMPUT SC, V8695, P17, DOI 10.1007/978-3-319-10584-0_2; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462; Kembhavi A, 2017, PROC CVPR IEEE, P5376, DOI 10.1109/CVPR.2017.571; Kim JH, 2018, ADV NEUR IN, V31; Konig SD, 2014, J NEUROSCI METH, V227, P121, DOI 10.1016/j.jneumeth.2014.01.032; Kootstra G, 2011, COGN COMPUT, V3, P223, DOI 10.1007/s12559-010-9089-5; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Lang CY, 2012, LECT NOTES COMPUT SC, V7573, P101, DOI 10.1007/978-3-642-33709-3_8; Lei Jie, 2018, EMNLP, P1369, DOI DOI 10.18653/V1/D18-1167; Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041; Li W, 2019, LECT NOTES COMPUT SC, V11132, P145, DOI 10.1007/978-3-030-11018-5_13; Li Y, 2018, LECT NOTES COMPUT SC, V11209, P639, DOI 10.1007/978-3-030-01228-1_38; Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43; Long Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10797, DOI 10.1109/CVPR42600.2020.01081; Lu ML, 2019, IEEE T IMAGE PROCESS, V28, P3703, DOI 10.1109/TIP.2019.2901707; Marino K, 2019, PROC CVPR IEEE, P3190, DOI 10.1109/CVPR.2019.00331; Mascharka D, 2018, PROC CVPR IEEE, P4942, DOI 10.1109/CVPR.2018.00519; Mathe S, 2015, IEEE T PATTERN ANAL, V37, P1408, DOI 10.1109/TPAMI.2014.2366154; Nguyen T. V., 2013, P 21 ACM INT C MULTI, P987; Palazzi A, 2019, IEEE T PATTERN ANAL, V41, P1720, DOI 10.1109/TPAMI.2018.2845370; Patro BN, 2020, AAAI CONF ARTIF INTE, V34, P11848; Qiao TT, 2018, AAAI CONF ARTIF INTE, P7300; Ramanathan S, 2010, LECT NOTES COMPUT SC, V6314, P30, DOI 10.1007/978-3-642-15561-1_3; Salah AA, 2002, IEEE T PATTERN ANAL, V24, P420, DOI 10.1109/34.990146; Selvaraju RR, 2019, IEEE I CONF COMP VIS, P2591, DOI 10.1109/ICCV.2019.00268; Shen CY, 2014, LECT NOTES COMPUT SC, V8695, P33, DOI 10.1007/978-3-319-10584-0_3; Shi Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P91, DOI 10.1007/978-3-030-58452-8_6; Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129; Tapaswi M, 2016, PROC CVPR IEEE, P4631, DOI 10.1109/CVPR.2016.501; Tatler BW, 2007, J VISION, V7, DOI 10.1167/7.14.4; Tavakoli HR, 2017, IEEE I CONF COMP VIS, P2506, DOI 10.1109/ICCV.2017.272; Tavakoli HR, 2017, PROC CVPR IEEE, P6354, DOI 10.1109/CVPR.2017.673; Do T, 2019, IEEE I CONF COMP VIS, P392, DOI 10.1109/ICCV.2019.00048; van der Linde I, 2009, SPATIAL VISION, V22, P161, DOI 10.1163/156856809787465636; Vaswani A, 2017, ADV NEUR IN, V30; Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607; Wang WG, 2021, IEEE T PATTERN ANAL, V43, P220, DOI 10.1109/TPAMI.2019.2924417; Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154; Wang WG, 2021, IEEE T PATTERN ANAL, V43, P2413, DOI 10.1109/TPAMI.2020.2966453; Wang Xun, 2020, CVPR; Wu JL, 2019, ADV NEUR IN, V32; Xu J, 2015, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2015.7298836; Xu J, 2014, J VISION, V14, DOI 10.1167/14.1.28; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang C.-J., 2018, PROC HCOMP, P184; Yang ZB, 2020, PROC CVPR IEEE, P190, DOI [10.1109/cvpr42600.2020.00027, 10.1109/CVPR42600.2020.00027]; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Yi KX, 2018, ADV NEUR IN, V31; Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644; Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202; Zelinsky G, 2019, IEEE COMPUT SOC CONF, P828, DOI 10.1109/CVPRW.2019.00111; Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688; Zhang RH, 2020, AAAI CONF ARTIF INTE, V34, P6811, DOI 10.1609/aaai.v34i04.6161; Zhang YD, 2019, IEEE WINT CONF APPL, P349, DOI 10.1109/WACV.2019.00043; Zheng QL, 2018, LECT NOTES COMPUT SC, V11218, P300, DOI 10.1007/978-3-030-01264-9_18	95	0	0	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7310	7326		10.1109/TPAMI.2021.3114582	http://dx.doi.org/10.1109/TPAMI.2021.3114582			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34550881	Green Submitted			2022-12-18	WOS:000864325900009
J	De Jong, DB; Paredes-Valles, F; de Croon, GCHE				De Jong, David Benjamin; Paredes-Valles, Federico; de Croon, Guido C. H. E.			How Do Neural Networks Estimate Optical Flow? A Neuropsychology-Inspired Study	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Optical imaging; Optical fiber networks; Optical sensors; Optical computing; Estimation; Biomedical optical imaging; Visualization; Optical flow; convolutional neural networks; Gabor filters; neuropsychology	SIMPLE RECEPTIVE-FIELDS; CATS STRIATE CORTEX; SPATIOTEMPORAL ORGANIZATION; CELLS; SELECTIVITY; FILTERS; FLY	End-to-end trained convolutional neural networks have led to a breakthrough in optical flow estimation. The most recent advances focus on improving the optical flow estimation by improving the architecture and setting a new benchmark on the publicly available MPI-Sintel dataset. Instead, in this article, we investigate how deep neural networks estimate optical flow. A better understanding of how these networks function is important for (i) assessing their generalization capabilities to unseen inputs, and (ii) suggesting changes to improve their performance. For our investigation, we focus on FlowNetS, as it is the prototype of an encoder-decoder neural network for optical flow estimation. Furthermore, we use a filter identification method that has played a major role in uncovering the motion filters present in animal brains in neuropsychological research. The method shows that the filters in the deepest layer of FlowNetS are sensitive to a variety of motion patterns. Not only do we find translation filters, as demonstrated in animal brains, but thanks to the easier measurements in artificial neural networks, we even unveil dilation, rotation, and occlusion filters. Furthermore, we find similarities in the refinement part of the network and the perceptual filling-in process which occurs in the mammal primary visual cortex.	[De Jong, David Benjamin; Paredes-Valles, Federico; de Croon, Guido C. H. E.] Delft Univ Technol, Fac Aerosp Engn, Dept Control & Simulat, Micro Air Vehicle Lab MAVIab, NL-2629 HS Delft, Netherlands	Delft University of Technology	Paredes-Valles, F (corresponding author), Delft Univ Technol, Fac Aerosp Engn, Dept Control & Simulat, Micro Air Vehicle Lab MAVIab, NL-2629 HS Delft, Netherlands.	daviddejong@me.com; fedeparedesv@gmail.com; g.c.h.e.decroon@tudelft.nl		de Jong, David/0000-0003-2088-2146; Paredes-Valles, Federico/0000-0002-9478-7195				ALBRECHT DG, 1980, SCIENCE, V207, P88, DOI 10.1126/science.6765993; Alexey Dosovitskiy, 2015, Arxiv, DOI arXiv:1412.6806; ANANDAN P, 1989, INT J COMPUT VISION, V2, P283, DOI 10.1007/BF00158167; Nguyen A, 2016, ADV NEUR IN, V29; [Anonymous], 2000, ICIAM; Antonio Torrabla, 2015, Arxiv, DOI arXiv:1507.02379; Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2; Beauchemin SS, 1995, ACM COMPUT SURV, V27, P433, DOI 10.1145/212094.212141; Beauchemin SS, 2000, IEEE T PATTERN ANAL, V22, P200, DOI 10.1109/34.825758; Borst A, 2015, NAT NEUROSCI, V18, P1067, DOI 10.1038/nn.4050; Borst A, 2010, ANNU REV NEUROSCI, V33, P49, DOI 10.1146/annurev-neuro-060909-153155; Bracewell R. N., 1986, FOURIER TRANSFORM IT; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Chen YH, 2019, IEEE I CONF COMP VIS, P7062, DOI 10.1109/ICCV.2019.00716; de Croon GCHE, 2016, BIOINSPIR BIOMIM, V11, DOI 10.1088/1748-3190/11/1/016004; DEANGELIS GC, 1993, J NEUROPHYSIOL, V69, P1118, DOI 10.1152/jn.1993.69.4.1118; DEANGELIS GC, 1993, J NEUROPHYSIOL, V69, P1091, DOI 10.1152/jn.1993.69.4.1091; DEANGELIS GC, 1995, TRENDS NEUROSCI, V18, P451, DOI 10.1016/0166-2236(95)94496-R; DEVALOIS RL, 1982, VISION RES, V22, P531, DOI 10.1016/0042-6989(82)90112-2; DEVALOIS RL, 1982, VISION RES, V22, P545, DOI 10.1016/0042-6989(82)90113-4; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Erhan D, 2009, 1341 U MONTR, V1341, P1, DOI DOI 10.2464/JILM.23.425; Feng J., 2003, COMPUTATIONAL NEUROS; Fleet D. J., 1989, Proceedings CVPR '89 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.89CH2752-4), P379, DOI 10.1109/CVPR.1989.37875; Gabor D., 1945, J I ELECT ENG PART G, V94, P58; Gibson James J., 1950, PERCEPTION VISUAL WO, P3; Godet P., 2020, ARXIV; Guan SS, 2019, IEEE INT CON MULTI, P181, DOI 10.1109/ICME.2019.00039; HEEGER DJ, 1987, INT J COMPUT VISION, V1, P279, DOI 10.1007/BF00133568; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Hui T.-W., 2019, LIGHTWEIGHT OPTICAL, P1; Ilg E, 2018, LECT NOTES COMPUT SC, V11211, P677, DOI 10.1007/978-3-030-01234-2_40; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; JONES JP, 1987, J NEUROPHYSIOL, V58, P1187, DOI 10.1152/jn.1987.58.6.1187; JONES JP, 1987, J NEUROPHYSIOL, V58, P1233, DOI 10.1152/jn.1987.58.6.1233; JONES JP, 1987, J NEUROPHYSIOL, V58, P1212, DOI 10.1152/jn.1987.58.6.1212; Kajo I., 2017, PROC INT C INTELL AD; Kendall Alex, 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295309; Komatsu H, 2006, NAT REV NEUROSCI, V7, P220, DOI 10.1038/nrn1869; Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958; Liu PP, 2019, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2019.00470; LONGUETHIGGINS HC, 1980, PROC R SOC SER B-BIO, V208, P385, DOI 10.1098/rspb.1980.0057; Lucas B.D., 1981, P INT JOINT C ART IN, P121, DOI DOI 10.5334/JORS.BL; Mayer N, 2018, INT J COMPUT VISION, V126, P942, DOI 10.1007/s11263-018-1082-6; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Mordvintsev A, 2015, INCEPTIONISM GOING D; Neoral M, 2019, LECT NOTES COMPUT SC, V11364, P159, DOI 10.1007/978-3-030-20870-7_10; Olah C., 2017, DISTILL, DOI [10.23915/distill.00007, DOI 10.23915/DISTILL.00007]; Olshausen BA, 2003, IEEE IMAGE PROC, P41; PALMER LA, 1981, J NEUROPHYSIOL, V46, P260, DOI 10.1152/jn.1981.46.2.260; Petkov N, 2007, BIOL CYBERN, V97, P423, DOI 10.1007/s00422-007-0182-0; Poort J, 2012, NEURON, V75, P143, DOI 10.1016/j.neuron.2012.04.032; Ranjan A, 2019, IEEE I CONF COMP VIS, P2404, DOI 10.1109/ICCV.2019.00249; Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291; REGAN D, 1978, VISION RES, V18, P415, DOI 10.1016/0042-6989(78)90051-2; Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720; Ronneberger O., 2015, P INT C MED IM COMP; Singh A., 1991, OPTIC FLOW COMPUTATI; Sun DQ, 2020, IEEE T PATTERN ANAL, V42, P1408, DOI 10.1109/TPAMI.2019.2894353; Teney D, 2017, LECT NOTES COMPUT SC, V10115, P412, DOI 10.1007/978-3-319-54193-8_26; Touryan J. O. V., 2004, NONLINEAR ANAL COMPL; Tu ZG, 2019, SIGNAL PROCESS-IMAGE, V72, P9, DOI 10.1016/j.image.2018.12.002; Ullman S., 1979, PROC R SOC SER B-BIO, DOI 10.7551/mitpress/3877.003.0009; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P2315, DOI 10.1098/rspb.1998.0577; von der Heydt Rudiger., 2003, FILLING IN PERCEPTUA, P106, DOI DOI 10.1093/ACPROF:OSO/9780195140132.003.0006; Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700; Zimmer H, 2011, INT J COMPUT VISION, V93, P368, DOI 10.1007/s11263-011-0422-6; Zweig S, 2017, PROC CVPR IEEE, P6363, DOI 10.1109/CVPR.2017.674	72	0	0	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8290	8305		10.1109/TPAMI.2021.3083538	http://dx.doi.org/10.1109/TPAMI.2021.3083538			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34033535	Green Submitted			2022-12-18	WOS:000864325900072
J	Du, L; Ye, XQ; Tan, X; Johns, E; Chen, B; Ding, ER; Xue, XY; Feng, JF				Du, Liang; Ye, Xiaoqing; Tan, Xiao; Johns, Edward; Chen, Bo; Ding, Errui; Xue, Xiangyang; Feng, Jianfeng			AGO-Net: Association-Guided 3D Point Cloud Object Detection Network	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Feature extraction; Object detection; Proposals; Transfer learning; Task analysis; Brain modeling; 3D object detection; domain adaptation; associative recognition; point cloud; neural network; autonomous driving		The human brain can effortlessly recognize and localize objects, whereas current 3D object detection methods based on LiDAR point clouds still report inferior performance for detecting occluded and distant objects: The point cloud appearance varies greatly due to occlusion, and has inherent variance in point densities along the distance to sensors. Therefore, designing feature representations robust to such point clouds is critical. Inspired by human associative recognition, we propose a novel 3D detection framework that associates intact features for objects via domain adaptation. We bridge the gap between the perceptual domain, where features are derived from real scenes with sub-optimal representations, and the conceptual domain, where features are extracted from augmented scenes that consist of non-occlusion objects with rich detailed information. A feasible method is investigated to construct conceptual scenes without external datasets. We further introduce an attention-based re-weighting module that adaptively strengthens the feature adaptation of more informative regions. The network's feature enhancement ability is exploited without introducing extra cost during inference, which is plug-and-play in various 3D detection frameworks. We achieve new state-of-the-art performance on the KITTI 3D detection benchmark in both accuracy and speed. Experiments on nuScenes and Waymo datasets also validate the versatility of our method.	[Du, Liang; Feng, Jianfeng] Fudan Univ, Inst Sci & Technol Brain Inspired Intelligence, Shanghai 200433, Peoples R China; [Du, Liang; Feng, Jianfeng] Fudan Univ, Key Lab Computat Neurosci & Brain Inspired Intell, Minist Educ, Shanghai 200433, Peoples R China; [Du, Liang; Feng, Jianfeng] Fudan Univ, MOE Frontiers Ctr Brain Sci, Shanghai 200433, Peoples R China; [Du, Liang] Zhangjiang Fudan Int Innovat Ctr, Shanghai 200433, Peoples R China; [Ye, Xiaoqing; Tan, Xiao; Ding, Errui] Baidu Inc, Beijing 100085, Peoples R China; [Johns, Edward] Imperial Coll London, Robot Learning Lab, London SW7 2BX, England; [Chen, Bo] FAW Nanjing Technol Dev Co Ltd, Nanjing 211102, Peoples R China; [Xue, Xiangyang] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China; [Feng, Jianfeng] Zhejiang Normal Univ, Fudan ISTRI ZTNU Algorithm Ctr Brain Inspired Int, Jinhua 321004, Zhejiang, Peoples R China	Fudan University; Fudan University; Fudan University; Baidu; Imperial College London; Fudan University; Zhejiang Normal University	Du, L; Feng, JF (corresponding author), Fudan Univ, Inst Sci & Technol Brain Inspired Intelligence, Shanghai 200433, Peoples R China.; Du, L; Feng, JF (corresponding author), Fudan Univ, Key Lab Computat Neurosci & Brain Inspired Intell, Minist Educ, Shanghai 200433, Peoples R China.; Du, L; Feng, JF (corresponding author), Fudan Univ, MOE Frontiers Ctr Brain Sci, Shanghai 200433, Peoples R China.	duliang@mail.ustc.edu.cn; yxq@whu.edu.cn; tanxchong@gmail.com; e.johns@imperial.ac.uk; chenbo2@faw.com.cn; dingerrui@baidu.com; xyxue@fudan.edu.cn; jffeng@fudan.edu.cn		Johns, Edward/0000-0002-8914-8786	National Key R&D Program of China [2019YFA0709502, 2018YFC1312904]; Shanghai Municipal Science and Technology Major Project [2018SHZDZX01]; Shanghai Center for Brain Science and Brain-Inspired Technology, the 111 Project [B18015]; ZJ Lab	National Key R&D Program of China; Shanghai Municipal Science and Technology Major Project; Shanghai Center for Brain Science and Brain-Inspired Technology, the 111 Project; ZJ Lab	This work was supported by National Key R&D Program of China under Grants 2019YFA0709502, 2018YFC1312904, Shanghai Municipal Science and Technology Major Project under Grant 2018SHZDZX01, ZJ Lab, and Shanghai Center for Brain Science and Brain-Inspired Technology, the 111 Project B18015.	Andreas Mitterecker, 2019, Arxiv, DOI arXiv:1910.04093; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Benjamin Graham, 2017, Arxiv, DOI arXiv:1706.01307; Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164; Carlesimo GA, 1998, CORTEX, V34, P563, DOI 10.1016/S0010-9452(08)70514-8; Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691; Chen YL, 2019, IEEE I CONF COMP VIS, P9774, DOI 10.1109/ICCV.2019.00987; Chenhang He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11870, DOI 10.1109/CVPR42600.2020.01189; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Dai JF, 2016, ADV NEUR IN, V29; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Du L, 2020, IEEE INT CONF ROBOT, P6868, DOI 10.1109/ICRA40945.2020.9197242; Du L, 2019, IEEE I CONF COMP VIS, P982, DOI 10.1109/ICCV.2019.00107; Engelcke Martin, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1355, DOI 10.1109/ICRA.2017.7989161; Fidler S., 2012, P ADV NEURAL INFORM, P611; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011; Hoffman Judy, 2014, NIPS; Hoffman P, 2018, CORTEX, V101, P107, DOI 10.1016/j.cortex.2018.01.015; Hu Peiyun, 2020, P IEEE CVF C COMP VI, P11001, DOI DOI 10.1109/CVPR42600.2020.01101; Jun Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P405, DOI 10.1007/978-3-030-58607-2_24; Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049; Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298; Li B, 2017, IEEE INT C INT ROBOT, P1513; Liang Du, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13326, DOI 10.1109/CVPR42600.2020.01334; Liang M, 2019, PROC CVPR IEEE, P7337, DOI 10.1109/CVPR.2019.00752; Liang M, 2018, LECT NOTES COMPUT SC, V11220, P663, DOI 10.1007/978-3-030-01270-0_39; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Liu YF, 2019, PROC CVPR IEEE, P2599, DOI 10.1109/CVPR.2019.00271; Liu Z, 2020, AAAI CONF ARTIF INTE, V34, P11677; OpenPCDet Development Team, 2020, OPENPCDET OP SOURC T; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102; Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Sergey Z., 2017, P INT C LEARN REPR I; Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054; Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086; Shi SS, 2021, IEEE T PATTERN ANAL, V43, P2647, DOI 10.1109/TPAMI.2020.2977026; Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178; Shuyang Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P279, DOI 10.1007/978-3-030-58589-1_17; Smola, 2007, ADV NEURAL INFORM PR, P513, DOI DOI 10.5555/2188385.2188410; Sun P, 2020, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR42600.2020.00252; Vora S, 2020, PROC CVPR IEEE, P4603, DOI 10.1109/CVPR42600.2020.00466; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wang Z, 2019, IEEE INT CONF COMP V, P2320, DOI 10.1109/ICCVW.2019.00285; Wang ZX, 2019, IEEE INT C INT ROBOT, P1742, DOI 10.1109/IROS40897.2019.8968513; Xu DF, 2018, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2018.00033; Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337; Yang B., 2018, C ROBOT LEARNING; Yang B, 2018, PROC CVPR IEEE, P7652, DOI 10.1109/CVPR.2018.00798; Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204; Ye Xiaoqing, 2020, ECCV, P17; Yosinski J, 2014, ADV NEUR IN, V27; Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105; Zhou X., 2019, ARXIV, DOI DOI 10.48550/ARXIV.1904.07850; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472; Zhou Yin, 2019, C ROB LEARN, P923	59	0	0	7	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8097	8109		10.1109/TPAMI.2021.3104172	http://dx.doi.org/10.1109/TPAMI.2021.3104172			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34379590	Green Submitted			2022-12-18	WOS:000864325900059
J	Forsyth, D; Rock, JJ				Forsyth, David; Rock, Jason J.			Intrinsic Image Decomposition Using Paradigms	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Standards; Computational modeling; Training; Image decomposition; Data models; Training data; Licenses; Computer vision; reflectance; image models; image representation; unsupervised learning	RETINEX THEORY; COLOR; REFLECTIONS	Intrinsic image decomposition is the task of mapping image to albedo and shading. Classical approaches derive methods from spatial models. The modern literature stresses evaluation, by comparing predictions to human judgements ("lighter", "same as", "darker"). The best modern intrinsic image methods train a map from image to albedo using images rendered from computer graphics models and example human judgements. This approach yields practical methods, but obtaining rendered images can be inconvenient. Furthermore, the approach cannot explain how a one could learn to recover intrinsic images without geometric, surface and illumination models, as people and animals appear to do. This paper describes a method that learns intrinsic image decomposition without seeing human annotations, rendered data, or ground truth data. Instead, the method relies on paradigms - spatial models of albedo and of shading. Rather than finding the "best" albedo and shading for an image via optimization, our approach trains a neural network on synthetic images. The synthetic images are constructed by multiplying albedos and shading fields sampled from our models. The network is subject to a novel smoothing procedure that ensures good behavior at short scales on real images. An averaging procedure ensures that reported albedo and shading are largely equivariant - different crops and scalings of an image will report the same albedo and shading at shared points. This averaging procedure controls long scale error. The standard evaluation for an intrinsic image method is a WHDR score. Our method achieves WHDR scores competitive with those of strong recent methods allowed to see training WHDR annotations, rendered data, and ground truth data. Our method produces albedo and shading maps with attractive qualitative properties - for example, albedo fields do not suppress wood grain and represent narrow grooves in surfaces well. Because our method is unsupervised, we can compute estimates of the test/train variance of WHDR scores; these are quite large, and suggest is unsafe to rely small differences in reported WHDR.	[Forsyth, David; Rock, Jason J.] Univ Illinois, Comp Sci, Champaign, IL 61820 USA	University of Illinois System; University of Illinois Urbana-Champaign	Forsyth, D (corresponding author), Univ Illinois, Comp Sci, Champaign, IL 61820 USA.	daf@illinois.edu; jjrock2@illinois.edu			National Science Foundation [1718221]; ONR MURI Award [N00014-16-1-2007]; Office of Naval Research [N00014-19-1-2400]	National Science Foundation(National Science Foundation (NSF)); ONR MURI Award(MURI); Office of Naval Research(Office of Naval Research)	This work supported in part by the National Science Foundation under Grant No. 1718221 in part by ONR MURI Award N00014-16-1-2007, in part by a grant from the Office of Naval Research N00014-19-1-2400, and in part by a gift from Boeing.	Barron J. T., 2013, UCBEECS2013117; Barron JT, 2016, LECT NOTES COMPUT SC, V9907, P617, DOI 10.1007/978-3-319-46487-9_38; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Barrow H., 1978, COMPUTER VISION SYST; Beck J., 1972, SURFACE COLOR PERCEP; Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206; Bi S., 2018, PROC EUROGRAPHICS S, P53; Bi S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766946; BLAKE A, 1985, COMPUT VISION GRAPH, V32, P314, DOI 10.1016/0734-189X(85)90054-4; Bousseau A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618476; BRAINARD DH, 1986, J OPT SOC AM A, V3, P1651, DOI 10.1364/JOSAA.3.001651; Brainard DH, 1997, J OPT SOC AM A, V14, P1393, DOI 10.1364/JOSAA.14.001393; BRELSTAFF G, 1987, PATTERN RECOGN LETT, V5, P129, DOI 10.1016/0167-8655(87)90034-1; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Chang J, 2014, LECT NOTES COMPUT SC, V8692, P704, DOI 10.1007/978-3-319-10593-2_46; Chen QF, 2013, IEEE I CONF COMP VIS, P241, DOI 10.1109/ICCV.2013.37; Cheng LC, 2018, PROC CVPR IEEE, P656, DOI 10.1109/CVPR.2018.00075; Cohen TS, 2016, PR MACH LEARN RES, V48; Elad M, 2003, J VIS COMMUN IMAGE R, V14, P369, DOI 10.1016/S1047-3203(03)00045-2; Fan QN, 2018, PROC CVPR IEEE, P8944, DOI 10.1109/CVPR.2018.00932; Farenzena M., 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, pIII; Freeman William T, 2006, P C COMP VIS PATT RE; Gilchrist A., 2006, SEEING BLACK WHITE; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428; Haddon J, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P236, DOI 10.1109/ICCV.1998.710724; Hering E., 1964, OUTLINES THEORY LIGH; Horn B. K, 1973, MEMO, V295; Horn B. K., 1974, COMPUT VISION GRAPH, V3, P277, DOI DOI 10.1016/0146-664X(74)90022-7; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jae Hyun Lim, 2017, Arxiv, DOI arXiv:1705.02894; Janner M, 2017, ADV NEUR IN, V30; Jianbing Shen, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3481, DOI 10.1109/CVPR.2011.5995507; Kenneth Vanhoey, 2018, Arxiv, DOI arXiv:1803.00805; Kimmel R, 2003, INT J COMPUT VISION, V52, P7, DOI 10.1023/A:1022314423998; KOENDERINK JJ, 1983, J OPT SOC AM, V73, P843, DOI 10.1364/JOSA.73.000843; Laffont PY, 2015, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2015.57; LAND EH, 1959, P NATL ACAD SCI USA, V45, P115, DOI 10.1073/pnas.45.1.115; Lenc K, 2019, INT J COMPUT VISION, V127, P456, DOI 10.1007/s11263-018-1098-y; Levin A, 2004, PROC CVPR IEEE, P306; Levin A, 2007, IEEE T PATTERN ANAL, V29, P1647, DOI 10.1109/TPAMI.2007.1106; Li ZM, 2018, LECT NOTES COMPUT SC, V11213, P339, DOI 10.1007/978-3-030-01240-3_21; Li ZQ, 2018, PROC CVPR IEEE, P9039, DOI 10.1109/CVPR.2018.00942; Li ZQ, 2018, LECT NOTES COMPUT SC, V11207, P381, DOI 10.1007/978-3-030-01219-9_23; Liu YF, 2020, PROC CVPR IEEE, P3245, DOI 10.1109/CVPR42600.2020.00331; Ma WC, 2018, LECT NOTES COMPUT SC, V11218, P211, DOI 10.1007/978-3-030-01264-9_13; MCCANN JJ, 1976, VISION RES, V16, P445, DOI 10.1016/0042-6989(76)90020-1; Narihira T, 2015, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2015.342; Narihira T, 2015, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2015.7298915; Nestmeyer T, 2017, PROC CVPR IEEE, P1771, DOI 10.1109/CVPR.2017.192; Perlin K., 1985, Computer Graphics, V19, P287, DOI 10.1145/325165.325247; Radford A., 2015, P COMP C; Rother C., 2011, ADV NEURAL INFORM PR, P765; Shen L, 2011, PROC CVPR IEEE, P697, DOI 10.1109/CVPR.2011.5995738; Sheng B, 2020, IEEE T VIS COMPUT GR, V26, P1332, DOI 10.1109/TVCG.2018.2869326; Shi J, 2017, PROC CVPR IEEE, P5844, DOI 10.1109/CVPR.2017.619; Tappen MF, 2005, IEEE T PATTERN ANAL, V27, P1459, DOI 10.1109/TPAMI.2005.185; von Helmholtz H., 1924, HELMHOLTZS TREATISE; Weiss Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P68, DOI 10.1109/ICCV.2001.937606; Yazici Y., 2021, PROC INT C LEARN REP; Yu Y, 2019, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2019.00327; Zhao Q, 2012, IEEE T PATTERN ANAL, V34, P1437, DOI 10.1109/TPAMI.2012.77; Zhou TH, 2015, IEEE I CONF COMP VIS, P3469, DOI 10.1109/ICCV.2015.396	64	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7624	7637		10.1109/TPAMI.2021.3119551	http://dx.doi.org/10.1109/TPAMI.2021.3119551			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34648429	hybrid, Green Submitted			2022-12-18	WOS:000864325900028
J	Guo, H; Mo, ZP; Shi, BX; Lu, F; Yeung, SK; Tan, P; Matsushita, Y				Guo, Heng; Mo, Zhipeng; Shi, Boxin; Lu, Feng; Yeung, Sai-Kit; Tan, Ping; Matsushita, Yasuyuki			Patch-Based Uncalibrated Photometric Stereo Under Natural Illumination	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Lighting; Shape; Merging; Optimization; Estimation; Markov random fields; Geometry; Uncalibrated photometric stereo; natural lighting; patch-based method; rotation averaging; intensity profile	REFLECTANCE	This paper presents a photometric stereo method that works with unknown natural illumination without any calibration objects or initial guess of the target shape. To solve this challenging problem, we propose the use of an equivalent directional lighting model for small surface patches consisting of slowly varying normals, and solve each patch up to an arbitrary orthogonal ambiguity. We further build the patch connections by extracting consistent surface normal pairs via spatial overlaps among patches and intensity profiles. Guided by these connections, the local ambiguities are unified to a global orthogonal one through Markov Random Field optimization and rotation averaging. After applying the integrability constraint, our solution contains only a binary ambiguity, which could be easily removed. Experiments using both synthetic and real-world datasets show our method provides even comparable results to calibrated methods.	[Guo, Heng; Matsushita, Yasuyuki] Osaka Univ, Grad Sch Informat Sci & Technol, Dept Multimedia Engn, Suita, Osaka 5650871, Japan; [Mo, Zhipeng; Tan, Ping] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada; [Shi, Boxin] Peking Univ, Natl Engn Lab Video Technol, Dept Comp Sci & Technol, Inst Artificial Intelligence,Beijing Acad Artific, Beijing, Peoples R China; [Lu, Feng] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China; [Yeung, Sai-Kit] Hong Kong Univ Sci & Technol, Div Integrat Syst & Design, Hong Kong, Peoples R China; [Yeung, Sai-Kit] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China	Osaka University; Simon Fraser University; Peking University; Beihang University; Hong Kong University of Science & Technology; Hong Kong University of Science & Technology	Shi, BX (corresponding author), Peking Univ, Natl Engn Lab Video Technol, Dept Comp Sci & Technol, Inst Artificial Intelligence,Beijing Acad Artific, Beijing, Peoples R China.	heng.guo@ist.osaka-u.ac.jp; zhipeng_mo@sfu.ca; shiboxin@pku.edu.cn; lufeng@buaa.edu.cn; saikit@ust.hk; pingtan@sfu.ca; yasumat@ist.osaka-u.ac.jp		Matsushita, Yasuyui/0000-0002-1935-4752; Lu, Feng/0000-0001-9064-7964; Mo, Zhipeng/0000-0001-8749-9489; Guo, Heng/0000-0003-0047-3927	JSPS KAKENHI [JP19H01123]; National Natural Science Foundation of China [62136001, 62088102, 61872012]; Internal Grant from HKUST [R9429]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Internal Grant from HKUST	This work was supported by JSPS KAKENHI under Grant JP19H01123, National Natural Science Foundation of China under Grant 62136001, 62088102, 61872012. Sai-Kit Yeung was partially supported by an Internal Grant from HKUST (R9429). (Heng Guo and Zhipeng Mo contributed equally to this work.)	Abrams A, 2012, LECT NOTES COMPUT SC, V7573, P357, DOI 10.1007/978-3-642-33709-3_26; Ackermann J., 2010, PROC EUR C TRENDS TO, P197; Ackermann J, 2012, PROC CVPR IEEE, P262, DOI 10.1109/CVPR.2012.6247684; Alldrin NG, 2007, IEEE I CONF COMP VIS, P417; Alldrin NG, 2007, PROC CVPR IEEE, P1822; B. O. Community, 2018, BLEND 3D MOD REND PA; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Basri R, 2007, INT J COMPUT VISION, V72, P239, DOI 10.1007/s11263-006-8815-7; Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611; Boxin Shi, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P361, DOI 10.1109/3DV.2014.9; Brahimi Mohammed, 2020, ADV PHOTOMETRIC 3D R, P147; Carmigniani J, 2011, MULTIMED TOOLS APPL, V51, P341, DOI 10.1007/s11042-010-0660-6; Chatterjee A, 2018, IEEE T PATTERN ANAL, V40, P958, DOI 10.1109/TPAMI.2017.2693984; Chen L, 2017, IEEE I CONF COMP VIS, P3181, DOI 10.1109/ICCV.2017.343; Cho D, 2016, LECT NOTES COMPUT SC, V9906, P170, DOI 10.1007/978-3-319-46475-6_11; David Li Yifan, 2013, IEEE Int Conf Rehabil Robot, V2013, P6650373, DOI 10.1109/ICORR.2013.6650373; Debevec P., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P189, DOI 10.1145/280814.280864; Farooq AR, 2005, COMPUT IND, V56, P918, DOI 10.1016/j.compind.2005.05.017; GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478; Haefner B, 2019, IEEE I CONF COMP VIS, P8538, DOI 10.1109/ICCV.2019.00863; Haefner B, 2020, IEEE T PATTERN ANAL, V42, P2453, DOI 10.1109/TPAMI.2019.2923621; Hartley R, 2013, INT J COMPUT VISION, V103, P267, DOI 10.1007/s11263-012-0601-0; HAYAKAWA H, 1994, J OPT SOC AM A, V11, P3079, DOI 10.1364/JOSAA.11.003079; Hernandez C, 2008, IEEE T PATTERN ANAL, V30, P548, DOI 10.1109/TPAMI.2007.70820; Higo T, 2009, IEEE I CONF COMP VIS, P1234, DOI 10.1109/ICCV.2009.5459331; Hold-Geoffroy Y, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P28, DOI 10.1109/3DV.2015.11; Jung J, 2019, INT J COMPUT VISION, V127, P1126, DOI 10.1007/s11263-018-01145-1; Jung J, 2015, PROC CVPR IEEE, P4521, DOI 10.1109/CVPR.2015.7299082; Kemelmacher-Shlizerman I, 2011, IEEE T PATTERN ANAL, V33, P394, DOI 10.1109/TPAMI.2010.63; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Koppal S. J., 2006, PROC IEEE C COMPUT V, P1323; Li S. Z., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P361, DOI 10.1007/BFb0028368; Lu F, 2013, PROC CVPR IEEE, P1490, DOI 10.1109/CVPR.2013.196; Maier R, 2017, IEEE I CONF COMP VIS, P3133, DOI 10.1109/ICCV.2017.338; Miyazaki D, 2010, INT J COMPUT VISION, V86, P229, DOI 10.1007/s11263-009-0262-9; Mo ZP, 2018, PROC CVPR IEEE, P2936, DOI 10.1109/CVPR.2018.00310; Mukaigawa Y, 2007, J OPT SOC AM A, V24, P3326, DOI 10.1364/JOSAA.24.003326; Okabe T, 2009, IEEE I CONF COMP VIS, P1693, DOI 10.1109/ICCV.2009.5459381; Oswald MR, 2012, PROC CVPR IEEE, P534, DOI 10.1109/CVPR.2012.6247718; Papadhimitri T, 2014, INT J COMPUT VISION, V107, P139, DOI 10.1007/s11263-013-0665-5; Papadhimitri T, 2013, PROC CVPR IEEE, P1474, DOI 10.1109/CVPR.2013.194; Peng SY, 2017, IEEE INT CONF COMP V, P2961, DOI 10.1109/ICCVW.2017.349; Queau Y., 2015, PROC INT C SCALE SPA, P498; Queau Y, 2018, J MATH IMAGING VIS, V60, P609, DOI 10.1007/s10851-017-0777-6; Santo H, 2018, LECT NOTES COMPUT SC, V11207, P3, DOI 10.1007/978-3-030-01219-9_1; Shen FY, 2014, COMPUT GRAPH FORUM, V33, P359, DOI 10.1111/cgf.12504; Shen L, 2009, PROC CVPR IEEE, P1850, DOI 10.1109/CVPRW.2009.5206732; Shi BX, 2019, IEEE T PATTERN ANAL, V41, P271, DOI 10.1109/TPAMI.2018.2799222; Shi BX, 2014, IEEE T PATTERN ANAL, V36, P1078, DOI 10.1109/TPAMI.2013.196; Shi BX, 2010, PROC CVPR IEEE, P1118, DOI 10.1109/CVPR.2010.5540091; Silver W.M., 1980, THESIS MIT; Sunkavalli K, 2010, LECT NOTES COMPUT SC, V6312, P251, DOI 10.1007/978-3-642-15552-9_19; Szeliski R, 2008, IEEE T PATTERN ANAL, V30, P1068, DOI 10.1109/TPAMI.2007.70844; Tan P, 2007, PROC CVPR IEEE, P1814; ULLMAN S, 1979, PROC R SOC SER B-BIO, V203, P405, DOI 10.1098/rspb.1979.0006; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Wu L, 2011, LECT NOTES COMPUT SC, V6494, P703, DOI 10.1007/978-3-642-19318-7_55; Wu TP, 2006, IEEE T PATTERN ANAL, V28, P1830, DOI 10.1109/TPAMI.2006.224; Wu TP, 2010, IEEE T PATTERN ANAL, V32, P546, DOI 10.1109/TPAMI.2009.15; Wu Z, 2013, PROC CVPR IEEE, P1498, DOI 10.1109/CVPR.2013.197; Xiong Y, 2015, IEEE T PATTERN ANAL, V37, P67, DOI 10.1109/TPAMI.2014.2343211; Yuille AL, 1999, INT J COMPUT VISION, V35, P203, DOI 10.1023/A:1008180726317	62	0	0	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7809	7823		10.1109/TPAMI.2021.3115229	http://dx.doi.org/10.1109/TPAMI.2021.3115229			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34559637				2022-12-18	WOS:000864325900040
J	Haller, E; Florea, AM; Leordeanu, M				Haller, Emanuela; Florea, Adina Magda; Leordeanu, Marius			Iterative Knowledge Exchange Between Deep Learning and Space-Time Spectral Clustering for Unsupervised Segmentation in Videos	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Videos; Clustering algorithms; Task analysis; Object segmentation; Mathematical models; Deep learning; Convergence; Unsupervised video object segmentation; spectral space-time clustering; principal eigenvector; power iteration; space-time graph; Feature-Motion matrix; object discovery; unsupervised learning; deep learning	OBJECT; OPTIMIZATION	We propose a dual system for unsupervised object segmentation in video, which brings together two modules with complementary properties: a space-time graph that discovers objects in videos and a deep network that learns powerful object features. The system uses an iterative knowledge exchange policy. A novel spectral space-time clustering process on the graph produces unsupervised segmentation masks passed to the network as pseudo-labels. The net learns to segment in single frames what the graph discovers in video and passes back to the graph strong image-level features that improve its node-level features in the next iteration. Knowledge is exchanged for several cycles until convergence. The graph has one node per each video pixel, but the object discovery is fast. It uses a novel power iteration algorithm computing the main space-time cluster as the principal eigenvector of a special Feature-Motion matrix without actually computing the matrix. The thorough experimental analysis validates our theoretical claims and proves the effectiveness of the cyclical knowledge exchange. We also perform experiments on the supervised scenario, incorporating features pretrained with human supervision. We achieve state-of-the-art level on unsupervised and supervised scenarios on four challenging datasets: DAVIS, SegTrack, YouTube-Objects, and DAVSOD. We will make our code publicly available.	[Haller, Emanuela; Florea, Adina Magda; Leordeanu, Marius] Univ Politehn Bucuresti, Bucharest 060042, Romania; [Haller, Emanuela] Bitdefender, Bucharest 024102, Romania; [Leordeanu, Marius] Romanian Acad, Inst Math, Bucharest 010702, Romania	Polytechnic University of Bucharest; Bitdefender; Institute of Mathematics of the Romanian Academy; Romanian Academy of Sciences; University of Bucharest	Leordeanu, M (corresponding author), Univ Politehn Bucuresti, Bucharest 060042, Romania.	haller.emanuela@gmail.com; adina.florea@upb.ro; marius.leordeanu@upb.ro			UEFISCDI [EEA-RO-2018-0496, PN-III-P1-1.2-PCCDI-2017-0734, PN-III-P4-ID-PCE-2020-2819]	UEFISCDI(Consiliul National al Cercetarii Stiintifice (CNCS)Unitatea Executiva pentru Finantarea Invatamantului Superior, a Cercetarii, Dezvoltarii si Inovarii (UEFISCDI))	This work was supported by UEFISCDI under Grants EEA-RO-2018-0496, PN-III-P1-1.2-PCCDI-2017-0734, and PN-III-P4-ID-PCE-2020-2819.	Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596; Bao LC, 2018, PROC CVPR IEEE, P5977, DOI 10.1109/CVPR.2018.00626; Bertasius Gedas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9736, DOI 10.1109/CVPR42600.2020.00976; Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen CZ, 2017, IEEE T IMAGE PROCESS, V26, P3156, DOI 10.1109/TIP.2017.2670143; Chen YH, 2018, PROC CVPR IEEE, P1189, DOI 10.1109/CVPR.2018.00130; Chen YH, 2018, IEEE T IMAGE PROCESS, V27, P3345, DOI 10.1109/TIP.2018.2813165; Cheng JC, 2018, PROC CVPR IEEE, P7415, DOI 10.1109/CVPR.2018.00774; Cheng JC, 2017, IEEE I CONF COMP VIS, P686, DOI 10.1109/ICCV.2017.81; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Faisal M, 2020, IEEE WINT CONF APPL, P1873, DOI 10.1109/WACV45572.2020.9093589; Faktor A., 2014, BMVC; Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875; Florian Schroff, 2017, Arxiv, DOI arXiv:1706.05587; Garg S., 2021, PROC IEEECVF WINTER, P1680; Haller E, 2017, IEEE I CONF COMP VIS, P5095, DOI 10.1109/ICCV.2017.544; Hamilton WL, 2017, ADV NEUR IN, V30; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu YT, 2018, LECT NOTES COMPUT SC, V11205, P813, DOI 10.1007/978-3-030-01246-5_48; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Jabri Allan, 2020, ARXIV200614613, P3; Jain SD, 2017, PROC CVPR IEEE, P2117, DOI 10.1109/CVPR.2017.228; Jain SD, 2014, LECT NOTES COMPUT SC, V8692, P656, DOI 10.1007/978-3-319-10593-2_43; Keuper M, 2015, IEEE I CONF COMP VIS, P3271, DOI 10.1109/ICCV.2015.374; Koffka K., 1935, PRINCIPLES GESTALT P; Koh YJ, 2017, PROC CVPR IEEE, P7417, DOI 10.1109/CVPR.2017.784; Lao D, 2018, LECT NOTES COMPUT SC, V11214, P441, DOI 10.1007/978-3-030-01249-6_27; Lee YJ, 2011, IEEE I CONF COMP VIS, P1995, DOI 10.1109/ICCV.2011.6126471; Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482; Leordeanu M, 2012, INT J COMPUT VISION, V96, P28, DOI 10.1007/s11263-011-0442-2; Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273; Li GB, 2018, PROC CVPR IEEE, P3243, DOI 10.1109/CVPR.2018.00342; Li J, 2017, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2017.158; Li SY, 2018, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2018.00683; Li SY, 2018, LECT NOTES COMPUT SC, V11207, P215, DOI 10.1007/978-3-030-01219-9_13; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu Z, 2017, IEEE T CIRC SYST VID, V27, P2527, DOI 10.1109/TCSVT.2016.2595324; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374; Lu Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P490, DOI 10.1007/978-3-030-58568-6_29; Luiten J, 2019, LECT NOTES COMPUT SC, V11364, P565, DOI 10.1007/978-3-030-20870-7_35; Mahadevan Sabarinath, 2020, P 31 BRIT MACH VIS C; Maninis KK, 2019, IEEE T PATTERN ANAL, V41, P1515, DOI 10.1109/TPAMI.2018.2838670; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Meila M., 2001, ARTIFICIAL INTELLIGE; Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925; Ng A.Y., 2001, P INT JOINT C ARTIFI, V17, P903; Nicolicioiu A, 2019, ADV NEUR IN, V32; Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242; Oh SW, 2018, PROC CVPR IEEE, P7376, DOI 10.1109/CVPR.2018.00770; Panagiotakis C, 2018, PATTERN RECOGN, V79, P1, DOI 10.1016/j.patcog.2018.02.001; Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223; Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85; Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372; Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Siam M, 2019, IEEE INT CONF ROBOT, P50, DOI 10.1109/ICRA.2019.8794254; Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44; Stewart G., 1990, MATRIX PERTURBATION; Tang Y, 2019, IEEE T CIRC SYST VID, V29, P1973, DOI 10.1109/TCSVT.2018.2859773; Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24; Tokmakov P, 2019, INT J COMPUT VISION, V127, P282, DOI 10.1007/s11263-018-1122-2; Tokmakov P, 2017, IEEE I CONF COMP VIS, P4491, DOI 10.1109/ICCV.2017.480; Tokmakov P, 2017, PROC CVPR IEEE, P531, DOI 10.1109/CVPR.2017.64; Tsai YH, 2016, LECT NOTES COMPUT SC, V9908, P760, DOI 10.1007/978-3-319-46493-0_46; Tsai YH, 2016, PROC CVPR IEEE, P3899, DOI 10.1109/CVPR.2016.423; Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256; Voigtlaender P., 2017, P DAVIS CHALL VID OB, V5; Wang L, 2021, IEEE T MULTIMEDIA, V23, P1287, DOI 10.1109/TMM.2020.2995266; Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933; Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941; Wang WG, 2019, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2019.00318; Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013; Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961; Wood JN, 2016, COGNITION, V153, P140, DOI 10.1016/j.cognition.2016.04.013; Wulff J, 2012, LECT NOTES COMPUT SC, V7584, P168, DOI 10.1007/978-3-642-33868-7_17; Xi T, 2017, IEEE T IMAGE PROCESS, V26, P3425, DOI 10.1109/TIP.2016.2631900; Xiankai Lu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P661, DOI 10.1007/978-3-030-58580-8_39; Xiankai Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8957, DOI 10.1109/CVPR42600.2020.00898; Yang LJ, 2018, PROC CVPR IEEE, P6499, DOI 10.1109/CVPR.2018.00680; Yang YC, 2019, PROC CVPR IEEE, P879, DOI 10.1109/CVPR.2019.00097; Yang Z, 2019, IEEE I CONF COMP VIS, P931, DOI 10.1109/ICCV.2019.00102; Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165; Zhou J., 2020, OPEN, V1, DOI [10.1016/j.aiopen.2021.01.001, DOI 10.1016/J.AIOPEN.2021.01.001]; Zhou TF, 2020, IEEE T IMAGE PROCESS, V29, P8326, DOI 10.1109/TIP.2020.3013162; Zhuo T, 2020, IEEE T IMAGE PROCESS, V29, P237, DOI 10.1109/TIP.2019.2930152	90	0	0	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7638	7656		10.1109/TPAMI.2021.3120228	http://dx.doi.org/10.1109/TPAMI.2021.3120228			19	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34648435	Green Submitted, hybrid			2022-12-18	WOS:000864325900029
J	Han, AD; Gao, JB				Han, Andi; Gao, Junbin			Improved Variance Reduction Methods for Riemannian Non-Convex Optimization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Complexity theory; Optimization; Manifolds; Convergence; Convex functions; Training; Principal component analysis; Riemannian optimization; non-convex optimization; online optimization; variance reduction; batch size adaptation		Variance reduction is popular in accelerating gradient descent and stochastic gradient descent for optimization problems defined on both euclidean space and Riemannian manifold. This paper further improves on existing variance reduction methods for non-convex Riemannian optimization, including R-SVRG and R-SRG/R-SPIDER by providing a unified framework for batch size adaptation. Such framework is more general than the existing works by considering retraction and vector transport and mini-batch stochastic gradients. We show that the adaptive-batch variance reduction methods require lower gradient complexities for both general non-convex and gradient dominated functions, under both finite-sum and online optimization settings. Moreover, under the new framework, we complete the analysis of R-SVRG and R-SRG, which is currently missing in the literature. We prove convergence of R-SVRG with much simpler analysis, which leads to curvature-free complexity bounds. We also show improved results for R-SRG under double-loop convergence, which match the optimal complexities as the R-SPIDER. In addition, we prove the first online complexity results for R-SVRG and R-SRG. Lastly, we discuss the potential of adapting batch size for non-smooth, constrained and second-order Riemannian optimizers. Extensive experiments on a variety of applications support the analysis and claims in the paper.	[Han, Andi; Gao, Junbin] Univ Sydney, Business Sch, Discipline Business Analyt, Sydney, NSW 2006, Australia	University of Sydney	Gao, JB (corresponding author), Univ Sydney, Business Sch, Discipline Business Analyt, Sydney, NSW 2006, Australia.	andi.han@sydney.edu.au; junbin.gao@sydney.edu.au	; Gao, Junbin/A-1766-2009	Han, Andi/0000-0003-4655-655X; Gao, Junbin/0000-0001-9803-0256	Australian Research Council (ARC) [DP200103015]	Australian Research Council (ARC)(Australian Research Council)	This work was supported in part by the Australian Research Council (ARC) Discovery Project under Grant DP200103015.	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Agarwal A, 2015, PR MACH LEARN RES, V37, P78; Ahn K., 2020, ARXIV200108876, V125, P84; Alimisis F., 2021, PROC INT C ARTIF INT, P1351; Babanezhad Harikandeh R., 2015, ADV NEURAL INFORM PR, V28, P2251; Babanezhad R., 2018, PROC JOINT EUR C MAC, P344; Balles L, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Becigneul G., 2019, PROC INT C LEARN REP, P1; Bennett James, 2007, P KDD CUP WORKSH, V2007, P35; Bini DA, 2013, LINEAR ALGEBRA APPL, V438, P1700, DOI 10.1016/j.laa.2011.08.052; Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619; Boumal N, 2019, IMA J NUMER ANAL, V39, P1, DOI 10.1093/imanum/drx080; Boumal N, 2014, J MACH LEARN RES, V15, P1455; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chris Criscitiello, 2019, Arxiv, DOI arXiv:1906.04321; Chris Ying, 2018, Arxiv, DOI arXiv:1711.00489; De S, 2017, PR MACH LEARN RES, V54, P1504; Defazio A, 2014, ADV NEUR IN, V27; Friedlander MP, 2012, SIAM J SCI COMPUT, V34, pA1380, DOI 10.1137/110830629; Goyal P., 2017, ARXIV, DOI DOI 10.48550/ARXIV.1706.02677; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Hongyi Zhang, 2018, Arxiv, DOI arXiv:1811.04194; Hongyi Zhang, 2018, Arxiv, DOI arXiv:1806.02812; Horv ~ath S., 2020, PROC 12 ANN WORKSHOP; Hosseini R, 2020, MATH PROGRAM, V181, P187, DOI 10.1007/s10107-019-01381-4; Huang W., 2013, OPTIMIZATION ALGORIT; Huang W, 2015, SIAM J OPTIMIZ, V25, P1660, DOI 10.1137/140955483; Huang W, 2015, MATH PROGRAM, V150, P179, DOI 10.1007/s10107-014-0765-1; Jeuris B, 2012, ELECTRON T NUMER ANA, V39, P379; Ji KY, 2020, PR MACH LEARN RES, V119; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Kaiyi Ji, 2019, Arxiv, DOI arXiv:1910.12166; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Kasai H., 2018, PROC 21 INT C ARTIF, V84, P269; Kasai H., 2020, ARXIV; Kasai H, 2019, PR MACH LEARN RES, V97; Kasai H, 2018, PR MACH LEARN RES, V80; Keskar N.S., 2017, ICLR; Kylberg G., 2011, EXTERNAL REPORT BLUE, V35; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lei LH, 2017, ADV NEUR IN, V30; Li Z., 2020, PROC 12 ANN WORKSHOP; Lian XR, 2017, PR MACH LEARN RES, V54, P1159; Liu SJ, 2018, ADV NEUR IN, V31; Weber M, 2021, Arxiv, DOI arXiv:1910.04194; Moritz P, 2016, JMLR WORKSH CONF PRO, V51, P249; Pang YW, 2008, IEEE T CIRC SYST VID, V18, P989, DOI 10.1109/TCSVT.2008.924108; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; Pham NH, 2020, J MACH LEARN RES, V21; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Reddi S. J., 2016, ADV NEURAL INFORM PR, P1145; Reddi SJ, 2016, PR MACH LEARN RES, V48; Reddi SJ, 2016, ANN ALLERTON CONF, P1244, DOI 10.1109/ALLERTON.2016.7852377; Ring W, 2012, SIAM J OPTIMIZ, V22, P596, DOI 10.1137/11082885X; Roy SK, 2018, PROC CVPR IEEE, P4460, DOI 10.1109/CVPR.2018.00469; Sato H, 2019, SIAM J OPTIMIZ, V29, P1444, DOI 10.1137/17M1116787; Sievert S, 2019, Arxiv, DOI arXiv:1910.08222; Theis FJ, 2009, LECT NOTES COMPUT SC, V5441, P354, DOI 10.1007/978-3-642-00599-2_45; Tripuraneni N., 2018, C LEARN THEOR, V75, P650; Udriste C, 1994, CONVEX FUNCTIONS OPT, DOI [10.1007/978-94-015-8390-9, DOI 10.1007/978-94-015-8390-9]; Wang X, 2017, SIAM J OPTIMIZ, V27, P927, DOI 10.1137/15M1053141; Wang Z., 2019, PROC INT C ARTIF INT, P2731; Wang Zhe, 2019, ADV NEURAL INFORM PR, P2403; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Arjevani Y, 2022, Arxiv, DOI arXiv:1912.02365; Yu Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3364; Yuan XR, 2016, PROCEDIA COMPUT SCI, V80, P2147, DOI 10.1016/j.procs.2016.05.534; Yurtsever A, 2019, PR MACH LEARN RES, V97; Zhang D., 2020, ARXIV; Zhang H., 2016, P C LEARN THEOR, P1617; Zhang Hongyi, 2016, ADV NEURAL INFORM PR, V29, P4592; Zhou DR, 2020, J MACH LEARN RES, V21; Zhou P, 2021, IEEE T PATTERN ANAL, V43, P459, DOI 10.1109/TPAMI.2019.2933841; Zhou P, 2018, ADV NEUR IN, V31	81	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7610	7623		10.1109/TPAMI.2021.3112139	http://dx.doi.org/10.1109/TPAMI.2021.3112139			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34516373				2022-12-18	WOS:000864325900027
J	Kim, DJ; Oh, TH; Choi, J; Kweon, IS				Kim, Dong-Jin; Oh, Tae-Hyun; Choi, Jinsoo; Kweon, In So			Dense Relational Image Captioning via Multi-Task Triple-Stream Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Visualization; Proposals; Dogs; Motorcycles; Natural languages; Genomics; Dense captioning; image captioning; visual relationship; relational analysis; scene graph		We introduce dense relational captioning, a novel image captioning task which aims to generate multiple captions with respect to relational information between objects in a visual scene. Relational captioning provides explicit descriptions for each relationship between object combinations. This framework is advantageous in both diversity and amount of information, leading to a comprehensive image understanding based on relationships, e.g., relational proposal generation. For relational understanding between objects, the part-of-speech (POS; i.e., subject-object-predicate categories) can be a valuable prior information to guide the causal sequence of words in a caption. We enforce our framework to learn not only to generate captions but also to understand the POS of each word. To this end, we propose the multi-task triple-stream network (MTTSNet) which consists of three recurrent units responsible for each POS which is trained by jointly predicting the correct captions and POS for each word. In addition, we found that the performance of MTTSNet can be improved by modulating the object embeddings with an explicit relational module. We demonstrate that our proposed model can generate more diverse and richer captions, via extensive experimental analysis on large scale datasets and several metrics. Then, we present applications of our framework to holistic image captioning, scene graph generation, and retrieval tasks.	[Kim, Dong-Jin; Choi, Jinsoo; Kweon, In So] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea; [Oh, Tae-Hyun] Pohang Univ Sci & Technol, Dept Elect Engn, Pohang 37673, South Korea; [Oh, Tae-Hyun] Pohang Univ Sci & Technol, Grad Sch AI GSAI, Pohang 37673, South Korea	Korea Advanced Institute of Science & Technology (KAIST); Pohang University of Science & Technology (POSTECH); Pohang University of Science & Technology (POSTECH)	Kweon, IS (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.; Oh, TH (corresponding author), Pohang Univ Sci & Technol, Dept Elect Engn, Pohang 37673, South Korea.; Oh, TH (corresponding author), Pohang Univ Sci & Technol, Grad Sch AI GSAI, Pohang 37673, South Korea.	djnjusa@kaist.ac.kr; taehyun@postech.ac.kr; jinsc37@kaist.ac.kr; iskweon77@kaist.ac.kr	Oh, Tae-Hyun/D-7854-2016	Oh, Tae-Hyun/0000-0003-0468-1571; Kim, Dong-Jin/0000-0001-7231-7494	Institute for Information & Communications Technology Promotion (IITP) - Korea Government [2017-0-01772]; IITP grants - Korea Government [2019-0-01906]; Artificial Intelligence Graduate School Program [2021-0-02068]; Artificial Intelligence Innovation Hub	Institute for Information & Communications Technology Promotion (IITP) - Korea Government(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of Korea); IITP grants - Korea Government; Artificial Intelligence Graduate School Program; Artificial Intelligence Innovation Hub	This work was supported by the Institute for Information & Communications Technology Promotion (IITP) (2017-0-01772) grant funded by the Korea Government. T.-H. Oh was supported in part by the IITP grants funded by the Korea Government under Grant 2019-0-01906, Artificial Intelligence Graduate School Program under Grant 2021-0-02068, Artificial Intelligence Innovation Hub.	Amir Roshan Zamir, 2012, Arxiv, DOI arXiv:1212.0402; Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Bird Steven, 2004, ACL, DOI DOI 10.3115/1118108.1118117; Byoung-Tak Zhang, 2017, Arxiv, DOI arXiv:1610.04325; Chao YW, 2018, IEEE WINT CONF APPL, P381, DOI 10.1109/WACV.2018.00048; Chen FH, 2018, PROC CVPR IEEE, P1345, DOI 10.1109/CVPR.2018.00146; Chen Gao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P696, DOI 10.1007/978-3-030-58610-2_41; Cho JW, 2021, IEEE COMPUT SOC CONF, P1592, DOI 10.1109/CVPRW53098.2021.00175; Collobert R., 2011, BIGLEARN NIPS WORKSH; Dai B, 2017, ADV NEUR IN, V30; Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323; Dai B, 2017, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR.2017.352; Denkowski Michael, 2014, P 9 WORKSH STAT MACH, P376, DOI DOI 10.3115/V1/W14-3348; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Dong-Jin Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P718, DOI 10.1007/978-3-030-58589-1_43; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Gkioxari G, 2018, PROC CVPR IEEE, P8359, DOI 10.1109/CVPR.2018.00872; Gu JX, 2019, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2019.00207; Gupta Nitish, 2020, INT C LEARN REPR; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378; Hu RH, 2018, LECT NOTES COMPUT SC, V11211, P55, DOI 10.1007/978-3-030-01234-2_4; Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93; Jiang WH, 2018, LECT NOTES COMPUT SC, V11206, P510, DOI 10.1007/978-3-030-01216-8_31; Johnson J, 2017, IEEE I CONF COMP VIS, P3008, DOI 10.1109/ICCV.2017.325; Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kim DJ, 2021, IEEE T IMAGE PROCESS, V30, P9150, DOI 10.1109/TIP.2021.3113563; Kim DJ, 2019, PROC CVPR IEEE, P6264, DOI 10.1109/CVPR.2019.00643; Kim DJ, 2018, IEEE WINT CONF APPL, P1699, DOI 10.1109/WACV.2018.00189; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Kottur S, 2018, LECT NOTES COMPUT SC, V11219, P160, DOI 10.1007/978-3-030-01267-0_10; Krause J, 2017, PROC CVPR IEEE, P3337, DOI 10.1109/CVPR.2017.356; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Land MF, 2002, NEUROCASE, V8, P80, DOI 10.1093/neucas/8.1.80; Li Y., 2017, PROC IEEE C COMPUT V; Li YK, 2018, LECT NOTES COMPUT SC, V11205, P346, DOI 10.1007/978-3-030-01246-5_21; Li YK, 2017, IEEE I CONF COMP VIS, P1270, DOI 10.1109/ICCV.2017.142; Li YL, 2019, PROC CVPR IEEE, P3580, DOI 10.1109/CVPR.2019.00370; Liang XD, 2017, PROC CVPR IEEE, P4408, DOI 10.1109/CVPR.2017.469; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu DQ, 2019, IEEE I CONF COMP VIS, P4672, DOI 10.1109/ICCV.2019.00477; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345; Nair V., 2010, ICML, P807; Oh T., 2019, P C EMPIRICAL METHOD, P2012; Oliva A, 2007, TRENDS COGN SCI, V11, P520, DOI 10.1016/j.tics.2007.09.009; Ordonez Vicente, 2011, ADV NEURAL INFORM PR, P1143; Peyre J, 2017, IEEE I CONF COMP VIS, P5189, DOI 10.1109/ICCV.2017.554; Plummer BA, 2017, IEEE I CONF COMP VIS, P1946, DOI 10.1109/ICCV.2017.213; Qi MS, 2019, PROC CVPR IEEE, P3952, DOI 10.1109/CVPR.2019.00408; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sadeghi MA, 2011, PROC CVPR IEEE, P1745, DOI 10.1109/CVPR.2011.5995711; Shetty R, 2017, IEEE I CONF COMP VIS, P4155, DOI 10.1109/ICCV.2017.445; Simonyan K., 2015, INT C LEARN REPR ICL; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Tan GC, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P745; Tian JJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3576; Venugopalan S, 2017, PROC CVPR IEEE, P1170, DOI 10.1109/CVPR.2017.130; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wang LW, 2017, ADV NEUR IN, V30; Wang WB, 2019, PROC CVPR IEEE, P8180, DOI 10.1109/CVPR.2019.00838; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Woo S, 2018, ADV NEUR IN, V31; Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41; Yang LJ, 2017, PROC CVPR IEEE, P1978, DOI 10.1109/CVPR.2017.214; Yang X, 2018, LECT NOTES COMPUT SC, V11216, P38, DOI 10.1007/978-3-030-01258-8_3; Yang X, 2019, IEEE I CONF COMP VIS, P4249, DOI 10.1109/ICCV.2019.00435; Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42; Yin GJ, 2018, LECT NOTES COMPUT SC, V11207, P330, DOI 10.1007/978-3-030-01219-9_20; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Yu RC, 2017, IEEE I CONF COMP VIS, P1068, DOI 10.1109/ICCV.2017.121; Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611; Zhang HW, 2017, IEEE I CONF COMP VIS, P4243, DOI 10.1109/ICCV.2017.454; Zhang HW, 2017, PROC CVPR IEEE, P3107, DOI 10.1109/CVPR.2017.331; Zhang J, 2017, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2017.555; Zhuang BH, 2017, IEEE I CONF COMP VIS, P589, DOI 10.1109/ICCV.2017.71	84	0	0	2	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7348	7362		10.1109/TPAMI.2021.3119754	http://dx.doi.org/10.1109/TPAMI.2021.3119754			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34648432	Green Submitted			2022-12-18	WOS:000864325900011
J	Lahouti, F; Kostina, V; Hassibi, B				Lahouti, Farshad; Kostina, Victoria; Hassibi, Babak			How to Query an Oracle? Efficient Strategies to Label Data	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Machine learning; labeling datasets; clustering; classification; entity resolution		We consider the basic problem of querying an expert oracle for labeling a dataset in machine learning. This is typically an expensive and time consuming process and therefore, we seek ways to do so efficiently. The conventional approach involves comparing each sample with (the representative of) each class to find a match. In a setting with N equally likely classes, this involves N=2 pairwise comparisons (queries per sample) on average. We consider a k-ary query scheme with k >= 2 samples in a query that identifies (dis)similar items in the set while effectively exploiting the associated transitive relations. We present a randomized batch algorithm that operates on a round-by-round basis to label the samples and achieves a query rate of O(N/k(2)). In addition, we present an adaptive greedy query scheme, which achieves an average rate of approximate to 0:2N queries per sample with triplet queries. For the proposed algorithms, we investigate the query rate performance analytically and with simulations. Empirical studies suggest that each triplet query takes an expert at most 50% more time compared with a pairwise query, indicating the effectiveness of the proposed k-ary query schemes. We generalize the analyses to nonuniform class distributions when possible.	[Lahouti, Farshad; Kostina, Victoria; Hassibi, Babak] CALTECH, Elect Engn Dept, Pasadena, CA 91125 USA	California Institute of Technology	Lahouti, F (corresponding author), CALTECH, Elect Engn Dept, Pasadena, CA 91125 USA.	flahouti@ieee.org; vkostina@caltech.edu; hassibi@caltech.edu						Chen X., 2018, OJBD, V4, P30; Davidson S, 2014, ACM T DATABASE SYST, V39, DOI 10.1145/2684066; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Elmagarmid AK, 2007, IEEE T KNOWL DATA EN, V19, P1, DOI 10.1109/TKDE.2007.250581; Karger DR, 2014, OPER RES, V62, P1, DOI 10.1287/opre.2013.1235; Kopcke H, 2010, DATA KNOWL ENG, V69, P197, DOI 10.1016/j.datak.2009.10.003; Lahouti F., 2016, PROC INT C NEURAL IN, P5065; Li F.-F., 2011, PROC CVPR WORKSHOP F, V2; Marcus A, 2011, PROC VLDB ENDOW, V5, P13; Massey J. L., 1994, Proceedings. 1994 IEEE International Symposium on Information Theory (Cat. No.94CH3467-8), DOI 10.1109/ISIT.1994.394764; Mazumdar A., 2017, PROC INT C NEURAL IN, P4685; PeterWelinder Steve Branson, 2010, P NIPS, V23, P1; Shi F, 2013, 2013 INTERNATIONAL CONFERENCE ON MANAGEMENT AND INFORMATION TECHNOLOGY, P299; Verroios V, 2015, PROC INT CONF DATA, P219, DOI 10.1109/ICDE.2015.7113286; Vesdapunt N, 2014, PROC VLDB ENDOW, V7, P1071, DOI 10.14778/2732977.2732982; Vinayak R. K., 2017, THESIS CALTECH PASAD; Vinayak R. Korlakai, 2014, ADV NEURAL INFORM PR, P2996; Wang J, 2012, PROC VLDB ENDOW, V5, P1483, DOI 10.14778/2350229.2350263; Whang SE, 2013, PROC VLDB ENDOW, V6, P349, DOI 10.14778/2536336.2536337; Zhang Y, 2014, PROC 27 INT C NEURAL, P1260	20	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7597	7609		10.1109/TPAMI.2021.3118644	http://dx.doi.org/10.1109/TPAMI.2021.3118644			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34618669	Green Submitted, Green Accepted			2022-12-18	WOS:000864325900026
J	Li, J; Wang, P; Han, K; Liu, Y				Li, Jie; Wang, Peng; Han, Kai; Liu, Yu			Anisotropic Convolutional Neural Networks for RGB-D Based Semantic Scene Completion	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Convolution; Semantics; Task analysis; Kernel; Solid modeling; Context modeling; Semantic scene completion; anisotropic convolution; dimensional decomposition convolution; 3D scene understanding		Semantic scene completion (SSC) is a computer vision task aiming to simultaneously infer the occupancy and semantic labels for each voxel in a scene from partial information consisting of a depth image and/or a RGB image. As a voxel-wise labeling task, the key for SSC is how to effectively model the visual and geometrical variations to complete the scene. To this end, we propose the Anisotropic Network (AIC-Net), with novel convolutional modules that can model varying anisotropic receptive fields voxel-wisely in a computationally efficient manner. The basic idea to achieve such anisotropy is to decompose 3D convolution into three consecutive dimensional convolutions, and determine the dimension-wise kernels on the fly. One module, termed kernel-selection anisotropic (KSA) convolution, adaptively selects the optimal kernel sizes for each dimensional convolution from a set of candidate kernels, and the other module, termed kernel-modulation anisotropic (KMA) convolution, directly modulates a single convolutional kernel for each dimension to derive more flexible receptive field. By stacking multiple such anisotropic modules, the 3D context modeling capability and flexibility can be further enhanced. Moreover, we present a new end-to-end trainable framework to approach the SSC task avoiding the expensive TSDF pre-processing as in many existing methods. Extensive experiments on SSC benchmarks show the advantage of the proposed methods.	[Li, Jie] Nanjing Univ Sci & Technol, Nanjing 210094, Peoples R China; [Wang, Peng] Univ Wollongong, Keiraville, NSW 2522, Australia; [Han, Kai] Univ Bristol, Bristol BS8 1TH, Avon, England; [Liu, Yu] Univ Adelaide, Adelaide, SA 5005, Australia	Nanjing University of Science & Technology; University of Wollongong; University of Bristol; University of Adelaide	Wang, P (corresponding author), Univ Wollongong, Keiraville, NSW 2522, Australia.	jieli_cn@163.com; pengw@uow.edu.au; kai.han@bristol.ac.uk; yu.liu04@adelaide.edu.au		Wang, Peng/0000-0002-5397-9115; Li, Jie/0000-0002-8064-947X				Dourado A, 2020, Arxiv, DOI arXiv:1908.02893; Andrew G. Howard, 2017, Arxiv, DOI arXiv:1704.04861; Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257; Cesar Cadena, 2020, Arxiv, DOI arXiv:2002.07269; Changqing Zou, 2020, Arxiv, DOI arXiv:2003.13910; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen XK, 2020, PROC CVPR IEEE, P4192, DOI 10.1109/CVPR42600.2020.00425; Cheng R., 2021, ARXIV; Cho K., 2014, P 2014 C EMP METH NA, P1724; Choy C., 2020, P IEEE CVF C COMP VI, P11227; Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319; Dai A., 2017, ARXIV; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Firman M, 2016, PROC CVPR IEEE, P5431, DOI 10.1109/CVPR.2016.586; Fisher Yu, 2016, Arxiv, DOI arXiv:1511.07122; Florian Schroff, 2018, Arxiv, DOI arXiv:1802.02611; Florian Schroff, 2017, Arxiv, DOI arXiv:1706.05587; Geiger A, 2015, LECT NOTES COMPUT SC, V9358, P183, DOI 10.1007/978-3-319-24947-6_15; Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961; Gupta S, 2017, PROC CVPR IEEE, P7272, DOI 10.1109/CVPR.2017.769; Gupta S, 2015, INT J COMPUT VISION, V112, P133, DOI 10.1007/s11263-014-0777-6; Han XG, 2017, IEEE I CONF COMP VIS, P85, DOI 10.1109/ICCV.2017.19; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jaderberg M, 2015, ADV NEUR IN, V28; Johann Sawatzky, 2019, Arxiv, DOI arXiv:1804.03550; Li J, 2020, PROC CVPR IEEE, P3348, DOI 10.1109/CVPR42600.2020.00341; Li J, 2019, PROC CVPR IEEE, P7685, DOI 10.1109/CVPR.2019.00788; Li J, 2020, IEEE ROBOT AUTOM LET, V5, P219, DOI 10.1109/LRA.2019.2953639; Lin DH, 2013, IEEE I CONF COMP VIS, P1417, DOI 10.1109/ICCV.2013.179; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Liu SC, 2018, ADV NEUR IN, V31; Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Park SJ, 2017, IEEE I CONF COMP VIS, P4990, DOI 10.1109/ICCV.2017.533; Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189; Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Rock J, 2015, PROC CVPR IEEE, P2484, DOI 10.1109/CVPR.2015.7298863; Ronneberger O., 2015, P INT C MED IM COMP; Seif G, 2018, IEEE COMPUT SOC CONF, P876, DOI 10.1109/CVPRW.2018.00120; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Sohn K, 2015, ADV NEUR IN, V28; Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Varley J, 2017, IEEE INT C INT ROBOT, P2442; Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686; Wang JH, 2016, LECT NOTES COMPUT SC, V9909, P664, DOI 10.1007/978-3-319-46454-1_40; Wang YD, 2018, INT CONF 3D VISION, P426, DOI 10.1109/3DV.2018.00056; Wei Z, 2017, PROC CVPR IEEE, P3947, DOI 10.1109/CVPR.2017.420; Xin Tong, 2018, Arxiv, DOI arXiv:1806.05361; Zhang JH, 2018, LECT NOTES COMPUT SC, V11216, P749, DOI 10.1007/978-3-030-01258-8_45; Zhang L, 2020, AAAI CONF ARTIF INTE, V34, P12821; Zhang L, 2020, INT J COMPUT VISION, V128, P479, DOI 10.1007/s11263-019-01253-6; Zhang L, 2018, NEUROCOMPUTING, V318, P182, DOI 10.1016/j.neucom.2018.08.052; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zheng B, 2013, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2013.402; Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953; Zou CH, 2018, PROC CVPR IEEE, P2051, DOI 10.1109/CVPR.2018.00219	62	0	0	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8125	8138		10.1109/TPAMI.2021.3081499	http://dx.doi.org/10.1109/TPAMI.2021.3081499			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34003745				2022-12-18	WOS:000864325900061
J	Li, WH; Wang, XF; Jin, B; Luo, DJ; Zha, HY				Li, Wenhao; Wang, Xiangfeng; Jin, Bo; Luo, Dijun; Zha, Hongyuan			Structured Cooperative Reinforcement Learning With Time-Varying Composite Action Space	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Agriculture; Aerospace electronics; Task analysis; Reinforcement learning; Games; Carbon dioxide; Robustness; Cooperative multi-agent reinforcement learning; composite action space; time-varying action space		In recent years, reinforcement learning has achieved excellent results in low-dimensional static action spaces such as games and simple robotics. However, the action space is usually composite, composed of multiple sub-action with different functions, and time-varying for practical tasks. The existing sub-actions might be temporarily invalid due to the external environment, while unseen sub-actions can be added to the current system. To solve the robustness and transferability problems in time-varying composite action spaces, we propose a structured cooperative reinforcement learning algorithm based on the centralized critic and decentralized actor framework, called SCORE. We model the single-agent problem with composite action space as a fully cooperative partially observable stochastic game and further employ a graph attention network to capture the dependencies between heterogeneous sub-actions. To promote tighter cooperation between the decomposed heterogeneous agents, SCORE introduces a hierarchical variational autoencoder, which maps the heterogeneous sub-action space into a common latent action space. We also incorporate an implicit credit assignment structure into the SCORE to overcome the multi-agent credit assignment problem in the fully cooperative partially observable stochastic game. Performance experiments on the proof-of-concept task and precision agriculture task show that SCORE has significant advantages in robustness and transferability for time-varying composite action space.	[Li, Wenhao; Wang, Xiangfeng; Jin, Bo] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China; [Li, Wenhao; Wang, Xiangfeng; Jin, Bo] East China Normal Univ, MOE Key Lab Adv Theory & Applicat Stat & Data Sci, Shanghai 200062, Peoples R China; [Luo, Dijun] Tencent Inc, Tencent AI Lab, Shenzhen 518000, Peoples R China; [Zha, Hongyuan] Chinese Univ Hong Kong, Sch Data Sci, Shenzhen 518172, Peoples R China; [Zha, Hongyuan] Chinese Univ Hong Kong, AIRS, Shenzhen 518172, Peoples R China	East China Normal University; East China Normal University; Tencent; Chinese University of Hong Kong, Shenzhen; Chinese University of Hong Kong, Shenzhen	Wang, XF; Jin, B (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.; Wang, XF; Jin, B (corresponding author), East China Normal Univ, MOE Key Lab Adv Theory & Applicat Stat & Data Sci, Shanghai 200062, Peoples R China.	52194501026@stu.ecnu.edu.cn; xfwang@cs.ecnu.edu.cn; bjin@cs.ecnu.edu.cn; dijunluo@tencent.com; zhahy@cuhk.edu.cn			National Key Research and Development Program of China [2020AAA0107400]; NSFC [12071145]; Shanghai Municipal Science and Technology Major Project [2021SHZDZX0102]; STCSM [18DZ2270700, 20511101100]; Open Research Projects of Zhejiang Lab [2021KE0AB03]; Shenzhen Institute of Artificial Intelligence and Robotics for Society	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Shanghai Municipal Science and Technology Major Project; STCSM(Science & Technology Commission of Shanghai Municipality (STCSM)); Open Research Projects of Zhejiang Lab; Shenzhen Institute of Artificial Intelligence and Robotics for Society	This work was supported in part by the National Key Research and Development Program of China under Grant 2020AAA0107400, in part by NSFC under Grant 12071145, in part by Shanghai Municipal Science and Technology Major Project under Grant 2021SHZDZX0102, in part by STCSM under Grants 18DZ2270700 and 20511101100, in part by the Open Research Projects of Zhejiang Lab under Grant 2021KE0AB03, and a grant from Shenzhen Institute of Artificial Intelligence and Robotics for Society.	Adam Santoro, 2018, Arxiv, DOI arXiv:1806.01261; Aleksandra Malysheva, 2018, Arxiv, DOI arXiv:1811.12557; Amini A, 2020, IEEE ROBOT AUTOM LET, V5, P1143, DOI 10.1109/LRA.2020.2966414; Audrunas Gruslys, 2017, Arxiv, DOI arXiv:1706.05296; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Ben Coppin, 2016, Arxiv, DOI arXiv:1512.07679; Bo Jin, 2020, Arxiv, DOI arXiv:2004.11145; Chandak Y, 2020, AAAI CONF ARTIF INTE, V34, P3373; Chang YH, 2004, ADV NEUR IN, V16, P807; Claire Wang, 2020, Arxiv, DOI arXiv:2001.01818; Drew Wicke, 2018, Arxiv, DOI arXiv:1810.09656; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Edwards Harrison, 2016, ICLR; Fan Z, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2279; Furuta R, 2019, AAAI CONF ARTIF INTE, P3598; Gabriel Synnaeve, 2019, Arxiv, DOI arXiv:1906.12266; Gregory Farquhar, 2017, Arxiv, DOI arXiv:1705.08926; Ha David, 2017, ICLR; Haarnoja T, 2018, PR MACH LEARN RES, V80; Han Liu, 2018, Arxiv, DOI arXiv:1810.06394; Hansen EA, 2004, PROCEEDING OF THE NINETEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE SIXTEENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE, P709; Hausknecht M. J., 2016, P INT C LEARNING REP, P1; He J, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1621; Holman D, 2013, PROC INT JOINT C ART, P2819; ICRISAT, 2017, MICR ICR INT CLOUD P; Iqbal S, 2019, PR MACH LEARN RES, V97; Jain A., 2020, PROC INT C MACH LEAR; Jain A, 2019, COMPASS '19 - PROCEEDINGS OF THE CONFERENCE ON COMPUTING & SUSTAINABLE SOCIETIES, P41, DOI 10.1145/3314344.3332485; Jiang JF, 2020, UK EU CHINA MILLIMET; Keith W. Ross, 2018, Arxiv, DOI arXiv:1806.00589; Kim H., 2019, PROC INT C MACH LEAR; Kuan F, 2020, INT J ROBOT RES, V39, P202, DOI 10.1177/0278364919872545; L_ucken C. V., 2008, PROC AAAI C ARTIF IN, P1751; Li Y., 2016, PROC INT C LEARN REP; Liu I.-J., 2020, PROC 3 C ROBOT LEARN; Liu Y, 2020, AAAI CONF ARTIF INTE, V34, P7211; Lowe R, 2017, ADV NEUR IN, V30; Ma W., 2019, P 11 INT C WIR COMM, P1; mCrops, 2016, US; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Pazis J., 2011, PROC IEEE S ADAPTIVE, P97; Pazis J., 2011, P 28 INT C MACHINE L, P1185; Pong V. H., 2020, PROC INT C MACH LEAR; Quinn J. A., 2011, PROC AAAI C ARTIF IN, P1390; Rashid T, 2018, PR MACH LEARN RES, V80; Rong Yu, 2019, INT C LEARN REPR; Ryu H, 2020, AAAI CONF ARTIF INTE, V34, P7236; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P81, DOI 10.1109/TNN.2008.2005141; Sherstov AA, 2005, LECT NOTES ARTIF INT, V3607, P194; Song YH, 2020, AAAI CONF ARTIF INTE, V34, P7253; Tacchetti A, 2019, PROC INT C LEARN REP; Tennenholtz G, 2019, PR MACH LEARN RES, V97; United Nations UN. Department of Economic and Social Affairs Population Division, 2015, WORLD POP PROSP 2015; Velickovi P., 2018, P INT C LEARN REPR I, P1, DOI DOI 10.48550/ARXIV.1710.10903; Wang H, 2016, LECT NOTES COMPUT SC, V9810, P574, DOI 10.1007/978-3-319-42911-3_48; Wang J., 2020, ARXIV; Xuan Liao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9391, DOI 10.1109/CVPR42600.2020.00941; You JX, 2017, AAAI CONF ARTIF INTE, P4559; Chen Y, 2021, Arxiv, DOI arXiv:1909.02291; Hua Y, 2020, Arxiv, DOI arXiv:2002.04238; Zheng Ding, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7917, DOI 10.1109/CVPR42600.2020.00794; Zhou M., 2020, PROC C NEURAL INF PR; Zhu H., 2020, PROC INT C LEARN REP	66	0	0	6	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8618	8634		10.1109/TPAMI.2021.3102140	http://dx.doi.org/10.1109/TPAMI.2021.3102140			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34347595				2022-12-18	WOS:000864325900093
J	Li, Z; Yu, T; Zheng, ZR; Liu, YB				Li, Zhe; Yu, Tao; Zheng, Zerong; Liu, Yebin			Robust and Accurate 3D Self-Portraits in Seconds	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image reconstruction; Bundle adjustment; Surface reconstruction; Solid modeling; Shape; Strain; Parametric statistics; 3D human reconstruction; RGBD sensor; volumetric fusion; bundle adjustment; texture optimization		In this paper, we propose an efficient method for robust and accurate 3D self-portraits using a single RGBD camera. Our method can generate detailed and realistic 3D self-portraits in seconds and shows the ability to handle subjects wearing extremely loose clothes. To achieve highly efficient and robust reconstruction, we propose PIFusion, which combines learning-based 3D recovery with volumetric non-rigid fusion to generate accurate sparse partial scans of the subject. Meanwhile, a non-rigid volumetric deformation method is proposed to continuously refine the learned shape prior. Moreover, a lightweight bundle adjustment algorithm is proposed to guarantee that all the partial scans can not only "loop" with each other but also remain consistent with the selected live key observations. Finally, to further generate realistic portraits, we propose non-rigid texture optimization to improve the texture quality. Additionally, we also contribute a benchmark for single-view 3D self-portrait reconstruction, an evaluation dataset that contains 10 single-view RGBD sequences of a self-rotating performer wearing various clothes and the corresponding ground-truth 3D models in the first frame of each sequence. The results and experiments based on this dataset show that the proposed method outperforms state-of-the-art methods on accuracy, efficiency, and generality.	[Li, Zhe; Yu, Tao; Zheng, Zerong; Liu, Yebin] Tsinghua Univ, Inst Brain & Cognit Sci, Beijing 100084, Peoples R China	Tsinghua University	Liu, YB (corresponding author), Tsinghua Univ, Inst Brain & Cognit Sci, Beijing 100084, Peoples R China.	liz19@mails.tsinghua.edu.cn; ytrock@mail.tsinghua.edu.cn; zzr18@mails.tsinghua.edu.cn; liuyebin@mail.tsinghua.edu.cn		Li, Zhe/0000-0003-4703-0875; Zheng, Zerong/0000-0003-1339-2480	National Key Research and Development Program of China [2018YFB2100500]; NSFC [61827805, 61861166002, 62171255]; China Postdoctoral Science Foundation [2020M670340]; Shuimu Tsinghua Scholarship	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); China Postdoctoral Science Foundation(China Postdoctoral Science Foundation); Shuimu Tsinghua Scholarship	This work was supported in part by the National Key Research and Development Program of China under Grant 2018YFB2100500, in part by the NSFC under Grants 61827805, 61861166002, and 62171255, in part by the China Postdoctoral Science Foundation under Grant 2020M670340, and in part by Shuimu Tsinghua Scholarship. Zhe Li and Tao Yu are equal contribution to this work.	Alldieck T, 2019, IEEE I CONF COMP VIS, P2293, DOI 10.1109/ICCV.2019.00238; Alldieck T, 2019, PROC CVPR IEEE, P1175, DOI 10.1109/CVPR.2019.00127; Alldieck T, 2018, PROC CVPR IEEE, P8387, DOI 10.1109/CVPR.2018.00875; Alldieck T, 2018, INT CONF 3D VISION, P98, DOI 10.1109/3DV.2018.00022; Bhatnagar Bharat Lal, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P311, DOI 10.1007/978-3-030-58536-5_19; Chibane J, 2020, PROC CVPR IEEE, P6968, DOI 10.1109/CVPR42600.2020.00700; Dou MS, 2015, PROC CVPR IEEE, P493, DOI 10.1109/CVPR.2015.7298647; Gabeur V, 2019, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2019.00232; Guo KW, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3083722; Habermann M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3311970; Huang Z, 2020, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR42600.2020.00316; Innmann M, 2016, LECT NOTES COMPUT SC, V9912, P362, DOI 10.1007/978-3-319-46484-8_22; Kazhdan Michael, 2006, P EUR S GEOM PROC, V7, P2; Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463; Lee JH, 2020, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR42600.2020.00135; Li C, 2018, LECT NOTES COMPUT SC, V11212, P324, DOI 10.1007/978-3-030-01237-3_20; Li H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508407; Li Ruilong, 2020, ARXIV200713988, P2; Li Z, 2020, PROC CVPR IEEE, P1341, DOI 10.1109/CVPR42600.2020.00142; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; Natsume R, 2019, PROC CVPR IEEE, P4475, DOI 10.1109/CVPR.2019.00461; Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062; Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123; Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016; Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239; Slavcheva M, 2018, PROC CVPR IEEE, P2646, DOI 10.1109/CVPR.2018.00280; Slavcheva M, 2017, PROC CVPR IEEE, P5474, DOI 10.1109/CVPR.2017.581; Soucy M, 1996, VISUAL COMPUT, V12, P503, DOI 10.1007/s003710050082; Sumner RW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239531; Tong J, 2012, IEEE T VIS COMPUT GR, V18, P643, DOI 10.1109/TVCG.2012.56; Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2; Wang KK, 2017, IEEE T IMAGE PROCESS, V26, P5966, DOI 10.1109/TIP.2017.2740624; Wang S, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18030886; Yan Cui, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P133, DOI 10.1007/978-3-642-37484-5_12; Yu T, 2019, PROC CVPR IEEE, P5499, DOI 10.1109/CVPR.2019.00565; Yu T, 2018, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR.2018.00761; Yu T, 2017, IEEE I CONF COMP VIS, P910, DOI 10.1109/ICCV.2017.104; Zeng M, 2013, PROC CVPR IEEE, P145, DOI 10.1109/CVPR.2013.26; Zheng ZR, 2022, IEEE T PATTERN ANAL, V44, P3170, DOI 10.1109/TPAMI.2021.3050505; Zheng ZR, 2018, LECT NOTES COMPUT SC, V11213, P389, DOI 10.1007/978-3-030-01240-3_24; Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783; Zhou QY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601134; Zhu H, 2019, PROC CVPR IEEE, P4486, DOI 10.1109/CVPR.2019.00462; Zhu H, 2017, IEEE T CIRC SYST VID, V27, P760, DOI 10.1109/TCSVT.2016.2596118	49	0	0	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7854	7870		10.1109/TPAMI.2021.3113164	http://dx.doi.org/10.1109/TPAMI.2021.3113164			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34529563				2022-12-18	WOS:000864325900043
J	Liu, FH; Huang, XL; Chen, YD; Suykens, JAK				Liu, Fanghui; Huang, Xiaolin; Chen, Yudong; Suykens, Johan A. K.			Towards a Unified Quadrature Framework for Large-Scale Kernel Machines	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Kernel; Monte Carlo methods; Convergence; Costs; Weight measurement; Transforms; Standards; Random features; quadrature methods; fully symmetric interpolatory rule; kernel approximation	INTEGRATION; RULES	In this paper, we develop a quadrature framework for large-scale kernel machines via a numerical integration representation. Considering that the integration domain and measure of typical kernels, e.g., Gaussian kernels, arc-cosine kernels, are fully symmetric, we leverage a numerical integration technique, deterministic fully symmetric interpolatory rules, to efficiently compute quadrature nodes and associated weights for kernel approximation. Thanks to the full symmetric property, the applied interpolatory rules are able to reduce the number of needed nodes while retaining a high approximation accuracy. Further, we randomize the above deterministic rules by the classical Monte-Carlo sampling and control variates techniques with two merits: 1) The proposed stochastic rules make the dimension of the feature mapping flexibly varying, such that we can control the discrepancy between the original and approximate kernels by tuning the dimnension. 2) Our stochastic rules have nice statistical properties of unbiasedness and variance reduction. In addition, we elucidate the relationship between our deterministic/stochastic interpolatory rules and current typical quadrature based rules for kernel approximation, thereby unifying these methods under our framework. Experimental results on several benchmark datasets show that our methods compare favorably with other representative kernel approximation based methods.	[Liu, Fanghui; Suykens, Johan A. K.] Katholieke Univ Leuven, Dept Elect Engn ESAT STADIUS, B-3001 Leuven, Belgium; [Huang, Xiaolin] Shanghai Jiao Tong Univ, Dept Automat, MOE Key Lab Syst Control & Informat Proc, Shanghai 200240, Peoples R China; [Chen, Yudong] Cornell Univ, Sch Operat Res & Informat Engn, Ithaca, NY 14850 USA	KU Leuven; Shanghai Jiao Tong University; Cornell University	Liu, FH (corresponding author), Katholieke Univ Leuven, Dept Elect Engn ESAT STADIUS, B-3001 Leuven, Belgium.; Huang, XL (corresponding author), Shanghai Jiao Tong Univ, Dept Automat, MOE Key Lab Syst Control & Informat Proc, Shanghai 200240, Peoples R China.	fanghui.liu@esat.kuleuven.be; xiaolinhuang@sjtu.edu.cn; yudong.chen@cornell.edu; johan.suykens@esat.kuleuven.be	Suykens, Johan/C-9781-2014	Suykens, Johan/0000-0002-8846-6352	European Research Council under the European Union's Horizon 2020 research and innovation program/ERC Advanced Grant [E-DUALITY 787960]; Research Council KU Leuven [C14/18/068]; Flemish Government: FWO [GOA4917N]; Flemish Government (AI Research Program); Ford KU Leuven Research Alliance Project [KUL0076]; EU H2020 ICT-48 Network TAILOR (Foundations of Trustworthy AI - Integrating Reasoning, Learning and Optimization), Leuven. AI Institute; National Natural Science Foundation of China [61977046]; National Science Foundation [CCF-1704828, CCF-2047910]; SJTU Global Strategic Partnership Fund under Grant 2020 SJTU-CORNELL; Shanghai Municipal Science and Technology Major Project [2021SHZDZX0102]	European Research Council under the European Union's Horizon 2020 research and innovation program/ERC Advanced Grant; Research Council KU Leuven(KU Leuven); Flemish Government: FWO(FWO); Flemish Government (AI Research Program); Ford KU Leuven Research Alliance Project; EU H2020 ICT-48 Network TAILOR (Foundations of Trustworthy AI - Integrating Reasoning, Learning and Optimization), Leuven. AI Institute; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Science Foundation(National Science Foundation (NSF)); SJTU Global Strategic Partnership Fund under Grant 2020 SJTU-CORNELL; Shanghai Municipal Science and Technology Major Project	The research leading to these results has received funding from the European Research Council under the European Union's Horizon 2020 research and innovation program/ERC Advanced Grant E-DUALITY 787960. This paper reflects only the authors' views and the Union is not liable for any use that may be made of the contained information. This work was supported in part by Research Council KU Leuven: Optimization frameworks for deep kernel machines C14/18/068; Flemish Government: FWO projects: GOA4917N (Deep Restricted Kernel Machines: Methods and Foundations), PhD/Postdoc grant. This research received funding from the Flemish Government (AI Research Program). This work was supported in part by Ford KU Leuven Research Alliance Project No. KUL0076 (Stability analysis and performance improvement of deep reinforcement learning algorithms), EU H2020 ICT-48 Network TAILOR (Foundations of Trustworthy AI - Integrating Reasoning, Learning and Optimization), Leuven. AI Institute; and in part by the National Natural Science Foundation of China under Grant 61977046, in part by National Science Foundation under Grants CCF-1704828 and CAREER Award CCF-2047910, and in part by SJTU Global Strategic Partnership Fund under Grant 2020 SJTU-CORNELL and Shanghai Municipal Science and Technology Major Project No. 2021SHZDZX0102.	Arasaratnam I, 2009, IEEE T AUTOMAT CONTR, V54, P1254, DOI 10.1109/TAC.2009.2019800; Arora S, 2019, ADV NEUR IN, V32; Avron H, 2017, PR MACH LEARN RES, V70; Avron H, 2016, J MACH LEARN RES, V17; Belhadji A, 2019, ADV NEUR IN, V32; Bochner S., 2005, HARMONIC ANAL THEORY; Caflisch R. E., 1998, Acta Numerica, V7, P1, DOI 10.1017/S0962492900002804; Chang CC, 2001, IEEE IJCNN, P1031, DOI 10.1109/IJCNN.2001.939502; Cho Youngmin, 2009, P ADV NEUR INF PROC, V22, P342; Choromanski K, 2021, PROC INT C LEARN REP, P1; Choromanski K. M., 2017, PROC 31 INT C NEURAL, P219; Cools R., 1997, Acta Numerica, V6, P1, DOI 10.1017/S0962492900002701; Dang ZY, 2022, IEEE T PATTERN ANAL, V44, P1385, DOI 10.1109/TPAMI.2020.3024987; Dao Tri, 2017, Adv Neural Inf Process Syst, V30, P6109; Davis P.J., 2007, METHODS NUMERICAL IN; Dick J, 2011, ANN STAT, V39, P1372, DOI 10.1214/11-AOS880; El Karoui N, 2010, ANN STAT, V38, P1, DOI 10.1214/08-AOS648; Erd~elyi T., 2020, PROC 34 INT C NEURAL, P102; Evans G., 1993, PRACTICAL NUMERICAL; Gauthier B, 2018, SIAM J SCI COMPUT, V40, pA3636, DOI 10.1137/17M1123614; Genz A, 1998, SIAM J SCI COMPUT, V19, P426, DOI 10.1137/S1064827595286803; Genz A, 1996, J COMPUT APPL MATH, V71, P299, DOI 10.1016/0377-0427(95)00232-4; Genz A, 2000, MONTE CARLO AND QUASI-MONTE CARLO METHODS 1998, P199; Gerace F, 2020, PR MACH LEARN RES, V119; Heiss F, 2008, J ECONOMETRICS, V144, P62, DOI 10.1016/j.jeconom.2007.12.004; Jia B, 2015, IEEE T AUTOMAT CONTR, V60, P199, DOI 10.1109/TAC.2014.2322478; Kafai M, 2019, IEEE T PATTERN ANAL, V41, P34, DOI 10.1109/TPAMI.2017.2785313; Karvonen T, 2018, SIAM J SCI COMPUT, V40, pA697, DOI 10.1137/17M1121779; Leobacher G., 2014, INTRO QUASIMONTE CAR; Liang TY, 2020, ANN STAT, V48, P1329, DOI 10.1214/19-AOS1849; Liu F., 2021, J MACH LEARN RES, V22, P1; Liu F, 2021, INT J MACH LEARN CYB, V12, P2693, DOI [10.1007/s13042-021-01357-x, 10.1109/TPAMI.2021.3097011]; Liu FH, 2020, AAAI CONF ARTIF INTE, V34, P4844; Lopez-Paz D, 2014, PR MACH LEARN RES, V32, P1359; Lyu YM, 2017, PR MACH LEARN RES, V70; Munkhoeva M, 2018, ADV NEUR IN, V31; Niederreiter H., 1992, RANDOM NUMBER GENERA, V63; Novak E, 1999, CONSTR APPROX, V15, P499, DOI 10.1007/s003659900119; PAUWELS E., 2018, ADV NEURAL INFORMATI; Peng H, 2022, INT J ADV MANUF TECH, V119, P99, DOI 10.1007/s00170-021-08284-9; Peters J, 2017, ADAPT COMPUT MACH LE; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; RUBINSTEIN RY, 1985, OPER RES, V33, P661, DOI 10.1287/opre.33.3.661; Rudi A, 2018, ADV NEUR IN, V31; Sun YT, 2018, ADV NEUR IN, V31; Suykens JAK, 2002, LEAST SQUARES SUPPOR; Yehudai G, 2019, ADV NEUR IN, V32; Yu Felix X, 2016, ADV NEURAL INFORM PR, V29, P1975; Zandieh A., 2021, ARXIV; Zhang Jian, 2019, Proc Mach Learn Res, V89, P1264	53	0	0	3	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7975	7988		10.1109/TPAMI.2021.3120183	http://dx.doi.org/10.1109/TPAMI.2021.3120183			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34648434	Green Submitted, Green Accepted			2022-12-18	WOS:000864325900051
J	Liu, RS; Li, Z; Fan, X; Zhao, CY; Huang, H; Luo, ZX				Liu, Risheng; Li, Zi; Fan, Xin; Zhao, Chenying; Huang, Hao; Luo, Zhongxuan			Learning Deformable Image Registration From Optimization: Perspective, Modules, Bilevel Training and Beyond	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Training; Optimization; Strain; Computational modeling; Task analysis; Deformable models; Image registration; Medical image analysis; diffeomorphic deformable registration; deep propagative network; bilevel self-tuned training	FLUID REGISTRATION; FRAMEWORK	Conventional deformable registration methods aim at solving an optimization model carefully designed on image pairs and their computational costs are exceptionally high. In contrast, recent deep learning-based approaches can provide fast deformation estimation. These heuristic network architectures are fully data-driven and thus lack explicit geometric constraints which are indispensable to generate plausible deformations, e.g., topology-preserving. Moreover, these learning-based approaches typically pose hyper-parameter learning as a black-box problem and require considerable computational and human effort to perform many training runs. To tackle the aforementioned problems, we propose a new learning-based framework to optimize a diffeomorphic model via multi-scale propagation. Specifically, we introduce a generic optimization model to formulate diffeomorphic registration and develop a series of learnable architectures to obtain propagative updating in the coarse-to-fine feature space. Further, we propose a new bilevel self-tuned training strategy, allowing efficient search of task-specific hyper-parameters. This training strategy increases the flexibility to various types of data while reduces computational and human burdens. We conduct two groups of image registration experiments on 3D volume datasets including image-to-atlas registration on brain MRI data and image-to-image registration on liver CT data. Extensive results demonstrate the state-of-the-art performance of the proposed method with diffeomorphic guarantee and extreme efficiency. We also apply our framework to challenging multi-modal image registration, and investigate how our registration to support the down-streaming tasks for medical image analysis including multi-modal fusion and image segmentation.	[Liu, Risheng; Li, Zi; Fan, Xin; Luo, Zhongxuan] Dalian Univ Technol, DUT RU Int Sch Informat Sci & Engn, Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian 116024, Liaoning, Peoples R China; [Zhao, Chenying; Huang, Hao] Childrens Hosp Philadelphia, Dept Radiol, Philadelphia, PA 19104 USA; [Zhao, Chenying] Univ Penn, Dept Bioengn, Sch Engn & Appl Sci, Philadelphia, PA 19104 USA; [Huang, Hao] Univ Penn, Perelman Sch Med, Dept Radiol, Philadelphia, PA 19104 USA; [Luo, Zhongxuan] Guilin Univ Elect Technol, Inst Artificial Intelligence, Guilin 541004, Peoples R China	Dalian University of Technology; University of Pennsylvania; Pennsylvania Medicine; Childrens Hospital of Philadelphia; University of Pennsylvania; University of Pennsylvania; Pennsylvania Medicine; Guilin University of Electronic Technology	Fan, X (corresponding author), Dalian Univ Technol, DUT RU Int Sch Informat Sci & Engn, Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian 116024, Liaoning, Peoples R China.	rsliu@dlut.edu.cn; alisoribrielee@gmail.com; xin.fan@ieee.org; chenyzh@seas.upenn.edu; huangh6@email.chop.edu; zxluo@dlut.edu.cn		Huang, Hao/0000-0002-9103-4382; Liu, Risheng/0000-0002-9554-0565; Li, Zi/0000-0001-7952-7021	National Key R&D Program of China [2020YFB1313503]; National Natural Science Foundation of China [61922019, 61733002, 61672125]; LiaoNing Revitalization Talents Program [XLYC1807088]; Fundamental Research Funds for the Central Universities	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); LiaoNing Revitalization Talents Program; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	This work was supported in part by the National Key R&D Program of China under Grant 2020YFB1313503, in part by the National Natural Science Foundation of China under Grants 61922019, 61733002, and 61672125, in part by the LiaoNing Revitalization Talents Program under Grant XLYC1807088, and in part by the Fundamental Research Funds for the Central Universities.	[Anonymous], MED SEGMENTATION DEC; Ashburner J, 2007, NEUROIMAGE, V38, P95, DOI 10.1016/j.neuroimage.2007.07.007; Avants BB, 2008, MED IMAGE ANAL, V12, P26, DOI 10.1016/j.media.2007.06.004; Avants BB, 2011, NEUROIMAGE, V54, P2033, DOI 10.1016/j.neuroimage.2010.09.025; Balakrishnan G, 2019, IEEE T MED IMAGING, V38, P1788, DOI 10.1109/TMI.2019.2897538; Beg MF, 2005, INT J COMPUT VISION, V61, P139, DOI 10.1023/B:VISI.0000043755.93987.aa; Benthien B, 2010, LECT NOTES COMPUT SC, V6362, P546; Brun CC, 2011, IEEE T MED IMAGING, V30, P184, DOI 10.1109/TMI.2010.2067451; Buerger C, 2011, MED IMAGE ANAL, V15, P551, DOI 10.1016/j.media.2011.02.009; Chiang MC, 2008, IEEE T MED IMAGING, V27, P442, DOI 10.1109/TMI.2007.907326; Dalca AV, 2019, MED IMAGE ANAL, V57, P226, DOI 10.1016/j.media.2019.07.006; Di Martino A, 2014, MOL PSYCHIATR, V19, P659, DOI 10.1038/mp.2013.78; DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409; Fischl B, 2012, NEUROIMAGE, V62, P774, DOI 10.1016/j.neuroimage.2012.01.021; Franceschi L, 2017, PR MACH LEARN RES, V70; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Heimann T, 2009, IEEE T MED IMAGING, V28, P1251, DOI 10.1109/TMI.2009.2013851; Heinrich MP, 2012, MED IMAGE ANAL, V16, P1423, DOI 10.1016/j.media.2012.05.008; Hering A, 2019, LECT NOTES COMPUT SC, V11769, P257, DOI 10.1007/978-3-030-32226-7_29; Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936; Hur J, 2019, PROC CVPR IEEE, P5747, DOI 10.1109/CVPR.2019.00590; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Jaderberg M, 2015, ADV NEUR IN, V28; Jiaao Zhang, 2020, Arxiv, DOI arXiv:2012.05609; Jirong Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P653, DOI 10.1007/978-3-030-58452-8_38; Kim M, 2012, IEEE T IMAGE PROCESS, V21, P1823, DOI 10.1109/TIP.2011.2170698; Kingma D.P., 2013, P 2 INT C LEARN REPR; Klein S, 2010, IEEE T MED IMAGING, V29, P196, DOI 10.1109/TMI.2009.2035616; Konukoglu E, 2010, IEEE T MED IMAGING, V29, P77, DOI 10.1109/TMI.2009.2026413; Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342; Liu H., 2019, PROC INT C LEARN REP; Liu R., 2020, PROC INT C MACH LEAR, V786, P6305; Liu R., 2021, ARXIV; Liu RS, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P723; Liu RS, 2021, IEEE T IMAGE PROCESS, V30, P1261, DOI 10.1109/TIP.2020.3043125; Liu RS, 2019, AAAI CONF ARTIF INTE, P4368; Liu RS, 2020, IEEE T PATTERN ANAL, V42, P3027, DOI 10.1109/TPAMI.2019.2920591; Luo K., 2020, ARXIV; MacKay M., 2019, PROC INT C LEARN REP; Maintz J B, 1998, Med Image Anal, V2, P1, DOI 10.1016/S1361-8415(01)80026-8; Mansi T, 2011, INT J COMPUT VISION, V92, P92, DOI 10.1007/s11263-010-0405-z; Marcus DS, 2010, J COGNITIVE NEUROSCI, V22, P2677, DOI 10.1162/jocn.2009.21407; Marek K, 2011, PROG NEUROBIOL, V95, P629, DOI 10.1016/j.pneurobio.2011.09.005; Meister S, 2018, AAAI CONF ARTIF INTE, P7251; Mok TCW, 2020, PROC CVPR IEEE, P4643, DOI 10.1109/CVPR42600.2020.00470; Mueller Susanne G, 2005, Alzheimers Dement, V1, P55, DOI 10.1016/j.jalz.2005.06.003; Niethammer M, 2019, PROC CVPR IEEE, P8455, DOI [10.1109/CVPR.2019.00866, 10.1109/cvpr.2019.00866]; Pai A, 2016, IEEE T MED IMAGING, V35, P1369, DOI 10.1109/TMI.2015.2511062; Paszke A, 2019, ADV NEUR IN, V32; Pennec X, 2005, LECT NOTES COMPUT SC, V3750, P943, DOI 10.1007/11566489_116; Phatak NS, 2009, MED IMAGE ANAL, V13, P354, DOI 10.1016/j.media.2008.07.004; Shen ZY, 2019, PROC CVPR IEEE, P4219, DOI [10.1109/CVPR.2019.00435, 10.1109/cvpr.2019.00435]; Sotiras A, 2013, IEEE T MED IMAGING, V32, P1153, DOI 10.1109/TMI.2013.2265603; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Sun W, 2014, LECT NOTES COMPUT SC, V8673, P194, DOI 10.1007/978-3-319-10404-1_25; Tang SY, 2009, NEUROIMAGE, V47, P1277, DOI 10.1016/j.neuroimage.2009.02.043; Van Essen DC, 2013, NEUROIMAGE, V80, P62, DOI 10.1016/j.neuroimage.2013.05.041; Wang J, 2020, PROC CVPR IEEE, P4443, DOI 10.1109/CVPR42600.2020.00450; Wang ZY, 2020, AAAI CONF ARTIF INTE, V34, P6315; Woolrich MW, 2009, NEUROIMAGE, V45, pS173, DOI 10.1016/j.neuroimage.2008.10.055; Yang XF, 2015, INT J COMPUT VISION, V115, P69, DOI 10.1007/s11263-015-0802-4; Yang X, 2017, NEUROIMAGE, V158, P378, DOI 10.1016/j.neuroimage.2017.07.008; Zhang YX, 2020, I S BIOMED IMAGING, P86, DOI 10.1109/ISBI45749.2020.9098526; Zhao SY, 2019, IEEE I CONF COMP VIS, P10599, DOI 10.1109/ICCV.2019.01070; Zhao SY, 2020, IEEE J BIOMED HEALTH, V24, P1394, DOI 10.1109/JBHI.2019.2951024	65	0	0	9	13	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7688	7704		10.1109/TPAMI.2021.3115825	http://dx.doi.org/10.1109/TPAMI.2021.3115825			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34582346	Green Submitted			2022-12-18	WOS:000864325900032
J	Liu, S; Jiang, WT; Gao, C; He, R; Feng, JS; Li, B; Yan, SC				Liu, Si; Jiang, Wentao; Gao, Chen; He, Ran; Feng, Jiashi; Li, Bo; Yan, Shuicheng			PSGAN plus plus : Robust Detail-Preserving Makeup Transfer and Removal	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Faces; Generative adversarial networks; Task analysis; Visualization; Nose; Image resolution; Skin; Makeup transfer; makeup removal; generative adversarial networks	NETWORK	In this paper, we address the makeup transfer and removal tasks simultaneously, which aim to transfer the makeup from a reference image to a source image and remove the makeup from the with-makeup image respectively. Existing methods have achieved much advancement in constrained scenarios, but it is still very challenging for them to transfer makeup between images with large pose and expression differences, or handle makeup details like blush on cheeks or highlight on the nose. In addition, they are hardly able to control the degree of makeup during transferring or to transfer a specified part in the input face. These defects limit the application of previous makeup transfer methods to real-world scenarios. In this work, we propose a Pose and expression robust Spatial-aware GAN (abbreviated as PSGAN++). PSGAN++ is capable of performing both detail-preserving makeup transfer and effective makeup removal. For makeup transfer, PSGAN++ uses a Makeup Distill Network (MDNet) to extract makeup information, which is embedded into spatial-aware makeup matrices. We also devise an Attentive Makeup Morphing (AMM) module that specifies how the makeup in the source image is morphed from the reference image, and a makeup detail loss to supervise the model within the selected makeup detail area. On the other hand, for makeup removal, PSGAN++ applies an Identity Distill Network (IDNet) to embed the identity information from with-makeup images into identity matrices. Finally, the obtained makeup/identity matrices are fed to a Style Transfer Network (STNet) that is able to edit the feature maps to achieve makeup transfer or removal. To evaluate the effectiveness of our PSGAN++, we collect a Makeup Transfer In the Wild (MT-Wild) dataset that contains images with diverse poses and expressions and a Makeup Transfer High-Resolution (MT-HR) dataset that contains high-resolution images. Experiments demonstrate that PSGAN++ not only achieves state-of-the-art results with fine makeup details even in cases of large pose/expression differences but also can perform partial or degree-controllable makeup transfer. Both the code and the newly collected datasets will be released at https://github.com/wtjiang98/PSGAN.	[Liu, Si; Jiang, Wentao; Gao, Chen; Li, Bo] Beihang Univ, Inst Artificial Intelligence, Beijing 100083, Peoples R China; [He, Ran] Chinese Acad Sci, Inst Automat, Beijing 100049, Peoples R China; [Feng, Jiashi] Natl Univ Singapore, Singapore 119077, Singapore; [Yan, Shuicheng] Sea AI Lab SAIL, Singapore 117576, Singapore	Beihang University; Chinese Academy of Sciences; Institute of Automation, CAS; National University of Singapore	He, R (corresponding author), Chinese Acad Sci, Inst Automat, Beijing 100049, Peoples R China.	liusi@buaa.edu.cn; jiangwentao@buaa.edu.cn; gaochen.ai@gmail.com; ran.he@ia.ac.cn; elefjia@nus.edu.sg; boli@buaa.edu.cn; shuicheng.yan@gmail.com	Yan, Shuicheng/HCI-1431-2022	liu, si/0000-0002-9180-2935; Jiang, Wentao/0000-0002-4894-5703	National Natural Science Foundation of China [61876177]; Beijing Natural Science Foundation [4202034, JQ18017]; Zhejiang Laboratory [2019KD0AB04]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation); Zhejiang Laboratory	The authors would like to thank Jiayun Wang for his efforts and Wenyan Wu, Chengyao Zheng for his useful comments. This work was supported in part by the National Natural Science Foundation of China under Grant 61876177, in part by Beijing Natural Science Foundation under Grants 4202034 and JQ18017, and in part by Zhejiang Laboratory under Grant 2019KD0AB04.	Aaron Hertzmann, 2016, Arxiv, DOI arXiv:1606.05897; Alashkar T, 2017, AAAI CONF ARTIF INTE, P941; Alexander S. Ecker, 2015, Arxiv, DOI arXiv:1508.06576; Chang HW, 2018, PROC CVPR IEEE, P40, DOI 10.1109/CVPR.2018.00012; Chen HJ, 2019, PROC CVPR IEEE, P10034, DOI 10.1109/CVPR.2019.01028; Chen Y.-C., 2017, P IEEE INT C COMPUTE, P4501; Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482; Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33; Gao C, 2020, PROC CVPR IEEE, P5679, DOI 10.1109/CVPR42600.2020.00572; Gu Q, 2019, IEEE I CONF COMP VIS, P10480, DOI 10.1109/ICCV.2019.01058; Guo D, 2009, PROC CVPR IEEE, P73, DOI 10.1109/CVPRW.2009.5206833; Hsin-Ying Lee, 2019, Arxiv, DOI arXiv:1905.01270; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jiang WT, 2020, PROC CVPR IEEE, P5193, DOI 10.1109/CVPR42600.2020.00524; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Jonathon Shlens, 2017, Arxiv, DOI arXiv:1610.07629; Kingma D.P, P 3 INT C LEARNING R; Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3; Li C, 2015, PROC CVPR IEEE, P4621, DOI 10.1109/CVPR.2015.7299093; Li TT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P645, DOI 10.1145/3240508.3240618; Li Y, 2020, INT J COMPUT VISION, V128, P2166, DOI 10.1007/s11263-019-01267-0; Li Y, 2019, PATTERN RECOGN, V90, P99, DOI 10.1016/j.patcog.2019.01.013; Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683; Lingxiao Song, 2017, Arxiv, DOI arXiv:1709.03654; Liu L., 2013, P 21 ACM INT C MULTI, P3, DOI [10.1145/2502081.2502126, DOI 10.1145/2502081.2502126]; Liu Si, 2016, PROC INT JOINT C ART; Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740; Taigman Yaniv, 2017, 5 INT C LEARN REPR I; Tong WS, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P211, DOI 10.1109/PG.2007.31; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032; Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245	40	0	0	8	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8538	8551		10.1109/TPAMI.2021.3083484	http://dx.doi.org/10.1109/TPAMI.2021.3083484			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34033534	Green Submitted			2022-12-18	WOS:000864325900088
J	Liu, YL; Lai, WS; Yang, MH; Chuang, YY; Huang, JB				Liu, Yu-Lun; Lai, Wei-Sheng; Yang, Ming-Hsuan; Chuang, Yung-Yu; Huang, Jia-Bin			Learning to See Through Obstructions With Layered Decomposition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image reconstruction; Optical imaging; Feature extraction; Task analysis; Optimization; Estimation; Cameras; Reflection removal; fence removal; optical flow; layer decomposition; computational photography	BLIND SEPARATION; IMAGES; ENHANCEMENT; REFLECTION	We present a learning-based approach for removing unwanted obstructions, such as window reflections, fence occlusions, or adherent raindrops, from a short sequence of images captured by a moving camera. Our method leverages motion differences between the background and obstructing elements to recover both layers. Specifically, we alternate between estimating dense optical flow fields of the two layers and reconstructing each layer from the flow-warped images via a deep convolutional neural network. This learning-based layer reconstruction module facilitates accommodating potential errors in the flow estimation and brittle assumptions, such as brightness consistency. We show that the proposed approach learned from synthetically generated data performs well to real images. Experimental results on numerous challenging scenarios of reflection and fence removal demonstrate the effectiveness of the proposed method.	[Liu, Yu-Lun; Chuang, Yung-Yu] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan; [Lai, Wei-Sheng; Yang, Ming-Hsuan] Univ Calif Merced, Sch Engn, Merced, CA 95343 USA; [Huang, Jia-Bin] Virginia Tech, Dept Elect & Comp Engn, Blacksburg, VA 24060 USA	National Taiwan University; University of California System; University of California Merced; Virginia Polytechnic Institute & State University	Huang, JB (corresponding author), Virginia Tech, Dept Elect & Comp Engn, Blacksburg, VA 24060 USA.	yulunliu@cmlab.csie.ntu.edu.tw; wlai24@ucmerced.edu; mhyang@ucmerced.edu; cyy@csie.ntu.edu.tw; jbhuang@vt.edu	Yang, Ming-Hsuan/T-9533-2019	Yang, Ming-Hsuan/0000-0003-4848-2304				Alayrac JB, 2019, PROC CVPR IEEE, P2452, DOI 10.1109/CVPR.2019.00256; Alex Nichol, 2018, Arxiv, DOI arXiv:1803.02999; Arvanitopoulos N, 2017, PROC CVPR IEEE, P1752, DOI 10.1109/CVPR.2017.190; Be'ery E, 2008, IEEE T IMAGE PROCESS, V17, P340, DOI 10.1109/TIP.2007.915548; Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206; Chelsea Finn, 2017, Arxiv, DOI arXiv:1703.03400; Chen Gao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P713, DOI 10.1007/978-3-030-58610-2_42; Chen YH, 2019, IEEE I CONF COMP VIS, P7062, DOI 10.1109/ICCV.2019.00716; Du C., 2018, 2018 IEEE INT C MULT, P1; Eigen D, 2014, ADV NEUR IN, V27; Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778; Fan QN, 2017, IEEE I CONF COMP VIS, P3258, DOI 10.1109/ICCV.2017.351; Gai K, 2012, IEEE T PATTERN ANAL, V34, P19, DOI 10.1109/TPAMI.2011.87; Gai K, 2009, PROC CVPR IEEE, P1881, DOI 10.1109/CVPRW.2009.5206825; Gandelsman Y, 2019, PROC CVPR IEEE, P11018, DOI 10.1109/CVPR.2019.01128; Garcia Victor, 2017, ARXIV171104043; Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428; Guo XJ, 2014, PROC CVPR IEEE, P2195, DOI 10.1109/CVPR.2014.281; Hochreiter Sepp, 2001, INT C ART NEUR NETW, P87, DOI [10.1007/3-540-44668-0, DOI 10.1007/3-540-44668-0]; Huang JB, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982398; Ilan S, 2015, COMPUT GRAPH FORUM, V34, P60, DOI 10.1111/cgf.12518; Jeon J, 2014, LECT NOTES COMPUT SC, V8695, P218, DOI 10.1007/978-3-319-10584-0_15; Jin M., 2018, PROC IEEE INT C COMP, P1; Jonna S, 2017, INT CONF ACOUST SPEE, P1792, DOI 10.1109/ICASSP.2017.7952465; Jonna S, 2016, LECT NOTES COMPUT SC, V9915, P836, DOI 10.1007/978-3-319-49409-8_68; Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Li Y, 2014, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2014.346; Li Y, 2013, IEEE I CONF COMP VIS, P2432, DOI 10.1109/ICCV.2013.302; Li ZQ, 2020, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR42600.2020.00255; Liu C, 2008, LECT NOTES COMPUT SC, V5304, P28, DOI 10.1007/978-3-540-88690-7_3; Luo X, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392377; Mu YD, 2014, IEEE T CIRC SYST VID, V24, P1111, DOI 10.1109/TCSVT.2013.2241351; Nandoriya A, 2017, IEEE I CONF COMP VIS, P2430, DOI 10.1109/ICCV.2017.264; Nestmeyer T, 2020, PROC CVPR IEEE, P5123, DOI 10.1109/CVPR42600.2020.00517; Ning Xu, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P311, DOI 10.1109/CVPR.2017.41; Park M., 2010, PROC ASIA C COMPUT V, P422; Punnappurath A, 2019, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2019.00165; Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263; Ravi Sachin, 2017, INT C LEARN REPR, V2, P5; Sengupta S, 2019, IEEE I CONF COMP VIS, P8597, DOI 10.1109/ICCV.2019.00869; Shih YC, 2015, PROC CVPR IEEE, P3193, DOI 10.1109/CVPR.2015.7298939; Sinha SN, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185596; Snell J, 2017, ADV NEUR IN, V30; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Sun Yu, 2020, ICML, P9229; Szeliski R, 2000, PROC CVPR IEEE, P246, DOI 10.1109/CVPR.2000.855826; Tseng H.-Y., 2020, PROC INT C LEARN REP; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wan RJ, 2017, IEEE I CONF COMP VIS, P3942, DOI 10.1109/ICCV.2017.423; Wan RJ, 2016, IEEE IMAGE PROC, P21, DOI 10.1109/ICIP.2016.7532311; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wei KX, 2019, PROC CVPR IEEE, P8170, DOI 10.1109/CVPR.2019.00837; Xu R, 2019, PROC CVPR IEEE, P3718, DOI 10.1109/CVPR.2019.00384; Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2; Xue TF, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766940; Yang JL, 2016, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2016.157; Yang J, 2018, LECT NOTES COMPUT SC, V11207, P675, DOI 10.1007/978-3-030-01219-9_40; Yi RJ, 2016, PROC CVPR IEEE, P705, DOI 10.1109/CVPR.2016.83; Yu-Lun Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14203, DOI 10.1109/CVPR42600.2020.01422; Yun-Chun Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P442, DOI 10.1007/978-3-030-58523-5_26; Zhang X, 2018, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2018.00503; Zhou TH, 2015, IEEE I CONF COMP VIS, P3469, DOI 10.1109/ICCV.2015.396; Zintgraf L, 2019, PR MACH LEARN RES, V97	65	0	0	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8387	8402		10.1109/TPAMI.2021.3111847	http://dx.doi.org/10.1109/TPAMI.2021.3111847			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34506277	Green Submitted			2022-12-18	WOS:000864325900078
J	Liu, YL; Shen, CH; Jin, LW; He, T; Chen, P; Liu, CY; Chen, H				Liu, Yuliang; Shen, Chunhua; Jin, Lianwen; He, Tong; Chen, Peng; Liu, Chongyu; Chen, Hao			ABCNet v2: Adaptive Bezier-Curve Network for Real-Time End-to-End Text Spotting	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Text recognition; Feature extraction; Training; Task analysis; Shape; Real-time systems; Annotations; Bezier curve; scene text spotting; text detection and recognition	NEURAL-NETWORK; SCENE; RECOGNITION	End-to-end text-spotting, which aims to integrate detection and recognition in a unified framework, has attracted increasing attention due to its simplicity of the two complimentary tasks. It remains an open problem especially when processing arbitrarily-shaped text instances. Previous methods can be roughly categorized into two groups: character-based and segmentation-based, which often require character-level annotations and/or complex post-processing due to the unstructured output. Here, we tackle end-to-end text spotting by presenting Adaptive Bezier Curve Network v2 (ABCNet v2). Our main contributions are four-fold: 1) For the first time, we adaptively fit arbitrarily-shaped text by a parameterized Bezier curve, which, compared with segmentation-based methods, can not only provide structured output but also controllable representation. 2) We design a novel BezierAlign layer for extracting accurate convolution features of a text instance of arbitrary shapes, significantly improving the precision of recognition over previous methods. 3) Different from previous methods, which often suffer from complex post-processing and sensitive hyper-parameters, our ABCNet v2 maintains a simple pipeline with the only post-processing non-maximum suppression (NMS). 4) As the performance of text recognition closely depends on feature alignment, ABCNet v2 further adopts a simple yet effective coordinate convolution to encode the position of the convolutional filters, which leads to a considerable improvement with negligible computation overhead. Comprehensive experiments conducted on various bilingual (English and Chinese) benchmark datasets demonstrate that ABCNet v2 can achieve state-of-the-art performance while maintaining very high efficiency. More importantly, as there is little work on quantization of text spotting models, we quantize our models to improve the inference time of the proposed ABCNet v2. This can be valuable for real-time applications. Code and model are available at: https://git.io/AdelaiDet.	[Liu, Yuliang; Jin, Lianwen; Liu, Chongyu] South China Univ Technol, Huizhou 516057, Peoples R China; [Liu, Yuliang; Shen, Chunhua; He, Tong; Chen, Peng; Chen, Hao] Univ Adelaide, Adelaide, SA 5005, Australia; [Jin, Lianwen] Guangdong Artificial Intelligence & Digital Econ, Guangzhou 510335, Guangdong, Peoples R China	South China University of Technology; University of Adelaide	Jin, LW (corresponding author), South China Univ Technol, Huizhou 516057, Peoples R China.; Shen, CH (corresponding author), Univ Adelaide, Adelaide, SA 5005, Australia.	liu.yuliang@mail.scut.edu.cn; chhshen@gmail.com; eelwjin@scut.edu.cn; tong.he@adelaide.edu.au; peng.chen@adelaide.edu.au; liuchongyu1996@gmail.com; stanzju@gmail.com			National Natural Science Foundation of China (NSFC) [61936003]; Natural Science Foundation of Guangdong Province (GD-NSF) [2017A030312006]	National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Guangdong Province (GD-NSF)(National Natural Science Foundation of Guangdong Province)	This work was done when Yuliang Liu, Chunhua Shen, Tong He, Peng Chen, and Hao Chen were with The University of Adelaide, Australia. The work of Lianwen Jin and Congyu Liu were supported by National Natural Science Foundation of China (NSFC) under Grant 61936003 and Natural Science Foundation of Guangdong Province (GD-NSF) under Grant 2017A030312006.	Andreas Veit, 2016, Arxiv, DOI arXiv:1601.07140; Baek Y., 2020, PROC EUR C COMPUT VI, P504; Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959; Bahdanau D., 2015, P 3 INT C LEARNING R; BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792; Busta M, 2017, IEEE I CONF COMP VIS, P2223, DOI 10.1109/ICCV.2017.242; Ch'ng CK, 2020, INT J DOC ANAL RECOG, V23, P31, DOI 10.1007/s10032-019-00334-z; Chee Kheng Chng, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1571, DOI 10.1109/ICDAR.2019.00252; Cheng ZZ, 2018, PROC CVPR IEEE, P5571, DOI 10.1109/CVPR.2018.00584; Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174; Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041; Esser S. K., 2020, P INT C LERN REPR AD; Feng W, 2021, INT J COMPUT VISION, V129, P619, DOI 10.1007/s11263-020-01388-x; Feng W, 2019, IEEE I CONF COMP VIS, P9075, DOI 10.1109/ICCV.2019.00917; Graves A., 2006, P 23 INT C MACH LEAR, P369; Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He T, 2018, PROC CVPR IEEE, P5020, DOI 10.1109/CVPR.2018.00527; He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87; Hu H, 2017, IEEE I CONF COMP VIS, P4950, DOI 10.1109/ICCV.2017.529; Huang WL, 2013, IEEE I CONF COMP VIS, P1241, DOI 10.1109/ICCV.2013.157; Huang WL, 2014, LECT NOTES COMPUT SC, V8692, P497, DOI 10.1007/978-3-319-10593-2_33; Jaderberg M, 2015, ADV NEUR IN, V28; Jungwook Choi, 2018, Arxiv, DOI arXiv:1805.06085; Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942; Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221; Li H, 2019, AAAI CONF ARTIF INTE, P8610; Li H, 2017, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2017.560; Li RD, 2019, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR.2019.00292; Liang GZ, 2015, IEEE T IMAGE PROCESS, V24, P4488, DOI 10.1109/TIP.2015.2465169; Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474; Liao MH, 2019, AAAI CONF ARTIF INTE, P8714; Liao MH, 2021, IEEE T PATTERN ANAL, V43, P532, DOI 10.1109/TPAMI.2019.2937086; Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107; Liao MH, 2017, AAAI CONF ARTIF INTE, P4161; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Litman Ron, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11959, DOI 10.1109/CVPR42600.2020.01198; Liu R, 2018, ADV NEUR IN, V31; Liu W, 2018, AAAI CONF ARTIF INTE, P7154; Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595; Liu YL, 2020, IEEE T IMAGE PROCESS, V29, P2918, DOI 10.1109/TIP.2019.2954218; Liu YL, 2019, PATTERN RECOGN, V90, P337, DOI 10.1016/j.patcog.2019.02.002; Liu YL, 2017, PROC CVPR IEEE, P3454, DOI 10.1109/CVPR.2017.368; Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2; Lucas SM, 2003, PROC INT CONF DOC, P682; Luo CJ, 2019, PATTERN RECOGN, V90, P109, DOI 10.1016/j.patcog.2019.01.020; Lyu PY, 2018, LECT NOTES COMPUT SC, V11218, P71, DOI 10.1007/978-3-030-01264-9_5; Minghui Liao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P706, DOI 10.1007/978-3-030-58621-8_41; Nagy Robert, 2012, Camera-Based Document Analysis and Recognition. 4th International Workshop, CBDAR 2011. Revised Selected Papers, P150, DOI 10.1007/978-3-642-29364-1_12; Nayef Nibal, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1582, DOI 10.1109/ICDAR.2019.00254; Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097; Qin SY, 2019, IEEE I CONF COMP VIS, P4703, DOI 10.1109/ICCV.2019.00480; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Risnumawan A, 2014, EXPERT SYST APPL, V41, P8027, DOI 10.1016/j.eswa.2014.07.008; Rui Zhang, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1577, DOI 10.1109/ICDAR.2019.00253; Shahab A, 2011, PROC INT CONF DOC, P1491, DOI 10.1109/ICDAR.2011.296; Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939; Shi BG, 2017, PROC INT CONF DOC, P1429, DOI 10.1109/ICDAR.2017.233; Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371; Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371; Shi BG, 2016, PROC CVPR IEEE, P4168, DOI 10.1109/CVPR.2016.452; Shi-Xue Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9696, DOI 10.1109/CVPR42600.2020.00972; Shivakumara P, 2011, IEEE T PATTERN ANAL, V33, P412, DOI 10.1109/TPAMI.2010.166; Su BL, 2017, PATTERN RECOGN, V63, P397, DOI 10.1016/j.patcog.2016.10.016; Su BL, 2015, LECT NOTES COMPUT SC, V9003, P35, DOI 10.1007/978-3-319-16865-4_3; Sun YP, 2019, LECT NOTES COMPUT SC, V11363, P83, DOI 10.1007/978-3-030-20893-6_6; Tang J, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.020; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4; Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436; Wan ZY, 2020, AAAI CONF ARTIF INTE, V34, P12120; Wang H, 2020, AAAI CONF ARTIF INTE, V34, P12160; Wang P, 2022, IEEE T PATTERN ANAL, V44, P7266, DOI 10.1109/TPAMI.2021.3095916; Wang TW, 2020, AAAI CONF ARTIF INTE, V34, P12216; Wang WCV, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20133661; Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853; Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956; Wang X., 2020, ADV NEURAL INF PROCE, V33, P17721; Wang XB, 2019, PROC CVPR IEEE, P6442, DOI 10.1109/CVPR.2019.00661; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270; Xing LJ, 2019, IEEE I CONF COMP VIS, P9125, DOI 10.1109/ICCV.2019.00922; Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589; Xue CH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P989; Yao C, 2014, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2014.515; Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787; Yin XC, 2015, IEEE T PATTERN ANAL, V37, P1930, DOI 10.1109/TPAMI.2014.2388210; Yin XC, 2014, IEEE T PATTERN ANAL, V36, P970, DOI 10.1109/TPAMI.2013.182; Yipeng Sun, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1557, DOI 10.1109/ICDAR.2019.00250; Yue Xiaoyu, 2020, LNCS, V2364, P135, DOI [DOI 10.1007/978-3, 10.1007/978-3-030-58529-7_9]; Yuliang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9806, DOI 10.1109/CVPR42600.2020.00983; Yuxin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11750, DOI 10.1109/CVPR42600.2020.01177; Zhan FN, 2019, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2019.00216; Zhang CQ, 2019, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR.2019.01080; Zhang Z, 2016, PROC CVPR IEEE, P4159, DOI 10.1109/CVPR.2016.451; Zhong ZY, 2019, INT J DOC ANAL RECOG, V22, P315, DOI 10.1007/s10032-019-00335-y; Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283	97	0	0	7	14	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8048	8064		10.1109/TPAMI.2021.3107437	http://dx.doi.org/10.1109/TPAMI.2021.3107437			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34460364	Green Submitted			2022-12-18	WOS:000864325900056
J	Luo, WX; Liu, W; Lian, DZ; Gao, SH				Luo, Weixin; Liu, Wen; Lian, Dongze; Gao, Shenghua			Future Frame Prediction Network for Video Anomaly Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Optical losses; Adaptation models; Visualization; Sensitivity; Uncertainty; Toy manufacturing industry; Training data; Video anomaly detection; prediction network; graph neural networks; meta learning	EVENT DETECTION; HISTOGRAMS; FLOW	Video Anomaly detection in videos refers to the identification of events that do not conform to expected behavior. However, almost all existing methods cast this problem as the minimization of reconstruction errors of training data including only normal events, which may lead to self-reconstruction and cannot guarantee a larger reconstruction error for an abnormal event. In this paper, we propose to formulate the video anomaly detection problem within a regime of video prediction. We advocate that not all video prediction networks are suitable for video anomaly detection. Then, we introduce two principles for the design of a video prediction network for video anomaly detection. Based on them, we elaborately design a video prediction network with appearance and motion constraints for video anomaly detection. Further, to promote the generalization of the prediction-based video anomaly detection for novel scenes, we carefully investigate the usage of a meta learning within our framework, where our model can be fast adapted to a new testing scene with only a few starting frames. Extensive experiments on both a toy dataset and three real datasets validate the effectiveness of our method in terms of robustness to the uncertainty in normal events and the sensitivity to abnormal events.	[Luo, Weixin; Liu, Wen; Lian, Dongze] ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai, Peoples R China; [Luo, Weixin; Liu, Wen; Lian, Dongze] Chinese Acad Sci, Shanghai Inst Microsyst & Informat Technol, Shanghai 200050, Peoples R China; [Luo, Weixin; Liu, Wen; Lian, Dongze] Univ Chinese Acad Sci, Beijing 100049, Peoples R China; [Gao, Shenghua] ShanghaiTech Univ, Shanghai Engn Res Ctr Intelligent Vis & Imaging, Shanghai 201210, Peoples R China	ShanghaiTech University; Chinese Academy of Sciences; Shanghai Institute of Microsystem & Information Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; ShanghaiTech University	Gao, SH (corresponding author), ShanghaiTech Univ, Shanghai Engn Res Ctr Intelligent Vis & Imaging, Shanghai 201210, Peoples R China.	luowx@shanghaitech.edu.cn; liuwen@shanghaitech.edu.cn; liandz@shanghaitech.edu.cn; gaoshh@shanghaitech.edu.cn	liu, wen/HGE-3071-2022	, Weixin/0000-0002-0754-6458	National Key R&D Program of China [2018AAA0100704]; NSFC [61932020]; Science and Technology Commission of Shanghai Municipality [20ZR1436000]; "Shuguang Program" - Shanghai Education Development Foundation; Shanghai Municipal Education Commission	National Key R&D Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Science and Technology Commission of Shanghai Municipality(Science & Technology Commission of Shanghai Municipality (STCSM)); "Shuguang Program" - Shanghai Education Development Foundation; Shanghai Municipal Education Commission(Shanghai Municipal Education Commission (SHMEC))	This work was supported by the National Key R&D Program of China under Grant 2018AAA0100704, NSFC #61932020, Science and Technology Commission of Shanghai Municipality under Grant 20ZR1436000, and "Shuguang Program" supported by Shanghai Education Development Foundation and Shanghai Municipal Education Commission.	Abadi M., 2016, ARXIV, DOI DOI 10.48550/ARXIV.1603.04467; Abati D, 2019, PROC CVPR IEEE, P481, DOI 10.1109/CVPR.2019.00057; Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825; Alex Nichol, 2018, Arxiv, DOI arXiv:1803.02999; Amersfoort J.V., 2017, ABS170108435 CORR; Andrychowicz M, 2016, ADV NEUR IN, V29; Bin Zhao, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3313, DOI 10.1109/CVPR.2011.5995524; Cewu Lu, 2020, Arxiv, DOI arXiv:1911.10676; Chao YW, 2018, IEEE WINT CONF APPL, P381, DOI 10.1109/WACV.2018.00048; Chen BY, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P358, DOI 10.1145/3126686.3126737; Chen YT, 2017, PR MACH LEARN RES, V70; Chong YS, 2017, LECT NOTES COMPUT SC, V10262, P189, DOI 10.1007/978-3-319-59081-3_23; Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Dai B, 2017, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR.2017.352; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33; Daoyuan Jia, 2018, Arxiv, DOI arXiv:1801.01726; Del Giorno A, 2016, LECT NOTES COMPUT SC, V9909, P334, DOI 10.1007/978-3-319-46454-1_21; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Fabbri M, 2018, LECT NOTES COMPUT SC, V11208, P450, DOI 10.1007/978-3-030-01225-0_27; Finn C, 2017, PR MACH LEARN RES, V70; Gauthier J., 2014, CLASS PROJECT STANFO, V2014, P5; Gkioxari G, 2015, IEEE I CONF COMP VIS, P2470, DOI 10.1109/ICCV.2015.284; Gong D, 2019, ARXIV; Goodfellow I. J., 2014, CORR; Gupta A, 2009, IEEE T PATTERN ANAL, V31, P1775, DOI 10.1109/TPAMI.2009.83; Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86; He K., 2017, IEEE INT C COMP VIS, P2961; Hinami R, 2017, IEEE I CONF COMP VIS, P3639, DOI 10.1109/ICCV.2017.391; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter Sepp, 2001, INT C ART NEUR NETW, P87, DOI [10.1007/3-540-44668-0, DOI 10.1007/3-540-44668-0]; Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378; Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470; ILGUN K, 1995, IEEE T SOFTWARE ENG, V21, P181, DOI 10.1109/32.372146; Ionescu RT, 2017, IEEE I CONF COMP VIS, P2914, DOI 10.1109/ICCV.2017.315; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990; Khac-Tuan Nguyen, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P457, DOI 10.1145/3372278.3390701; Kim J, 2009, PROC CVPR IEEE, P2913; Kingma DP, 2015, INT C LEARN REPR ICL; Kwon YH, 2019, PROC CVPR IEEE, P1811, DOI 10.1109/CVPR.2019.00191; Lee S, 2020, IEEE T IMAGE PROCESS, V29, P2395, DOI 10.1109/TIP.2019.2948286; Li YK, 2018, LECT NOTES COMPUT SC, V11205, P346, DOI 10.1007/978-3-030-01246-5_21; Li YK, 2017, IEEE I CONF COMP VIS, P1270, DOI 10.1109/ICCV.2017.142; Lian D., 2020, PROC INT C LEARN REP; Lian DZ, 2019, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2019.00192; Liang XD, 2017, PROC CVPR IEEE, P4408, DOI 10.1109/CVPR.2017.469; Liu W, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3023; Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684; Lotter W., 2017, ICLR, DOI [DOI 10.48550/ARXIV.1605.08104, 10.48550/arXiv.1605.08104]; Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338; Luo WX, 2021, IEEE T PATTERN ANAL, V43, P1070, DOI 10.1109/TPAMI.2019.2944377; Luo WX, 2017, IEEE I CONF COMP VIS, P341, DOI 10.1109/ICCV.2017.45; Luo WX, 2017, IEEE INT CON MULTI, P439, DOI 10.1109/ICME.2017.8019325; Ma CY, 2018, PROC CVPR IEEE, P6790, DOI 10.1109/CVPR.2018.00710; Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872; Mao Xudong, 2016, ARXIV; Mathieu M., 2016, INT C LEARN REPR ICL; Max Welling, 2017, Arxiv, DOI arXiv:1609.02907; Morais R, 2019, PROC CVPR IEEE, P11988, DOI 10.1109/CVPR.2019.01227; Munkhdalai T, 2017, PR MACH LEARN RES, V70; Nachum O., 2017, ARXIV; Ni BB, 2016, PROC CVPR IEEE, P1020, DOI 10.1109/CVPR.2016.116; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25; Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7; Ronneberger O., 2015, P INT C MED IM COMP; Salvador S, 2004, FLAIRS C, P306; Santoro A, 2017, ADV NEUR IN, V30; Santoro A, 2016, PR MACH LEARN RES, V48; Shi XJ, 2015, ADV NEUR IN, V28; Simonyan K, 2014, ADV NEUR IN, V27; SMYTH P, 1994, IEEE J SEL AREA COMM, V12, P1600, DOI 10.1109/49.339929; Snell J, 2017, ADV NEUR IN, V30; Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678; Sun C, 2018, LECT NOTES COMPUT SC, V11215, P335, DOI 10.1007/978-3-030-01252-6_20; Nguyen TN, 2019, IEEE I CONF COMP VIS, P1273, DOI 10.1109/ICCV.2019.00136; Tung F, 2011, IMAGE VISION COMPUT, V29, P230, DOI 10.1016/j.imavis.2010.11.003; Vilalta R, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P474, DOI 10.1109/ICDM.2002.1183991; Villegas R, 2017, PROC INT C LEARN REP; Vinyals Oriol, 2016, ARXIV160604080, P3630; Wang LM, 2016, INT J COMPUT VISION, V119, P254, DOI 10.1007/s11263-015-0859-0; Wang T, 2013, IEEE INT W PERFORM, P45, DOI 10.1109/PETS.2013.6523794; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Weiss G. M., 1998, Proceedings Fourth International Conference on Knowledge Discovery and Data Mining, P359; Woo S, 2018, ADV NEUR IN, V31; Wu SD, 2010, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2010.5539882; Xu Dan, 2015, ARXIV151001553; Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330; Xu YY, 2018, PROC CVPR IEEE, P5275, DOI 10.1109/CVPR.2018.00553; Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41; Yao BP, 2012, IEEE T PATTERN ANAL, V34, P1691, DOI 10.1109/TPAMI.2012.67; Yao BP, 2010, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2010.5540235; Ye MC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1805, DOI 10.1145/3343031.3350899; Zhang D, 2005, PROC CVPR IEEE, P611; Zhang HW, 2017, PROC CVPR IEEE, P3107, DOI 10.1109/CVPR.2017.331; Zhang RX, 2018, ADV NEUR IN, V31; Zhou JT, 2020, IEEE T CIRC SYST VID, V30, P4639, DOI 10.1109/TCSVT.2019.2962229	100	0	0	19	23	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7505	7520		10.1109/TPAMI.2021.3129349	http://dx.doi.org/10.1109/TPAMI.2021.3129349			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34797762				2022-12-18	WOS:000864325900020
J	Maziarka, L; Smieja, M; Sendera, M; Struski, L; Tabor, J; Spurek, P				Maziarka, Lukasz; Smieja, Marek; Sendera, Marcin; Struski, Lukasz; Tabor, Jacek; Spurek, Przemyslaw			OneFlow: One-Class Flow for Anomaly Detection Based on a Minimal Volume Region	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Anomaly detection; Support vector machines; Solid modeling; Neural networks; Three-dimensional displays; Data models; Training; Anomaly detection; outlier detection; normalizing flows	SUPPORT	We propose OneFlow - a flow-based one-class classifier for anomaly (outlier) detection that finds a minimal volume bounding region. Contrary to density-based methods, OneFlow is constructed in such a way that its result typically does not depend on the structure of outliers. This is caused by the fact that during training the gradient of the cost function is propagated only over the points located near to the decision boundary (behavior similar to the support vectors in SVM). The combination of flow models and a Bernstein quantile estimator allows OneFlow to find a parametric form of bounding region, which can be useful in various applications including describing shapes from 3D point clouds. Experiments show that the proposed model outperforms related methods on real-world anomaly detection problems.	[Maziarka, Lukasz; Smieja, Marek; Sendera, Marcin; Struski, Lukasz; Tabor, Jacek; Spurek, Przemyslaw] Jagiellonian Univ, Fac Math & Comp Sci, PL-630348 Krakow, Poland	Jagiellonian University	Struski, L (corresponding author), Jagiellonian Univ, Fac Math & Comp Sci, PL-630348 Krakow, Poland.	l.maziarka@gmail.com; marek.smieja@ii.uj.edu.pl; marcin.sendera@gmail.com; lukasz.struski@uj.edu.pl; jacek.tabor@uj.edu.pl; przemyslaw.spurek@ii.uj.edu.pl			National Science Centre (Poland) [2018/31/B/ST6/00993]; Foundation for Polish Science; European Union under the European Regional Development Fund through Team-Net programme [POIR.04.04.00-00-14DE/18-00]; National Science Centre, Poland [2019/33/B/ST6/00894, 2017/25/B/ST6/01271]	National Science Centre (Poland)(National Science Centre, Poland); Foundation for Polish Science(Foundation for Polish ScienceEuropean Commission); European Union under the European Regional Development Fund through Team-Net programme; National Science Centre, Poland(National Science Centre, Poland)	The work of degrees ukasz Maziarka was supported by the National Science Centre (Poland) under Grant 2018/31/B/ST6/00993. The work of Marek ~ Smieja and degrees ukasz Struski was supported by the Foundation for Polish Science, cofinanced by the European Union under the European Regional Development Fund through Team-Net programme under Grant POIR.04.04.00-00-14DE/18-00. The work of Jacek Tabor was supported by the National Science Centre, Poland, under Grant 2017/25/B/ST6/01271. The work of Przemys >> aw Spurek was supported by the National Science Centre, Poland, under Grant 2019/33/B/ST6/00894.	Abati D, 2019, PROC CVPR IEEE, P481, DOI 10.1109/CVPR.2019.00057; Akihiro Matsukawa, 2019, Arxiv, DOI arXiv:1810.09136; Anton van den Hengel, 2020, Arxiv, DOI arXiv:2007.02500; Asuncion A, 2007, UCI MACHINE LEARNING; Bing Liu, 2018, Arxiv, DOI arXiv:1801.05609; Breunig MM, 2000, SIGMOD REC, V29, P93, DOI 10.1145/335191.335388; Chen YT, 2013, INT CONF ACOUST SPEE, P3567, DOI 10.1109/ICASSP.2013.6638322; CHENG C, 1995, STAT PROBABIL LETT, V24, P321, DOI 10.1016/0167-7152(94)00190-J; Dacheng Chen, 2019, Arxiv, DOI arXiv:1809.04758; Dasgupta S, 2018, P NATL ACAD SCI USA, V115, P13093, DOI 10.1073/pnas.1814448115; David Krueger, 2015, Arxiv, DOI arXiv:1410.8516; Deecke L, 2019, LECT NOTES ARTIF INT, V11051, P3, DOI 10.1007/978-3-030-10925-7_1; Diederik P Kingma, 2014, Arxiv, DOI arXiv:1312.6114; Garcia-Teodoro P, 2009, COMPUT SECUR, V28, P18, DOI 10.1016/j.cose.2008.08.003; Goh J, 2017, IEEE HI ASS SYS ENGR, P140, DOI 10.1109/HASE.2017.36; Golan I, 2018, ADV NEUR IN, V31; Gopalan P, 2019, ADV NEUR IN, V32; Graham W. Taylor, 2018, Arxiv, DOI arXiv:1802.04865; Guha S, 2016, PR MACH LEARN RES, V48; Jascha Sohl-Dickstein, 2017, Arxiv, DOI arXiv:1605.08803; Kevin Roth, 2019, Arxiv, DOI arXiv:1902.04818; Kim S, 2015, NEUROCOMPUTING, V165, P111, DOI 10.1016/j.neucom.2014.09.086; Kingma DP, 2018, ADV NEUR IN, V31; Lavin A, 2015, 2015 IEEE 14TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P38, DOI 10.1109/ICMLA.2015.141; Leblanc A, 2012, ANN I STAT MATH, V64, P919, DOI 10.1007/s10463-011-0339-4; Liu FT, 2012, ACM T KNOWL DISCOV D, V6, DOI 10.1145/2133360.2133363; Miljkovic Dubravko, 2010, 2010 33rd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), P593; Nachman B, 2020, PHYS REV D, V101, DOI 10.1103/PhysRevD.101.075042; Chong PY, 2020, Arxiv, DOI arXiv:2001.08873; Perera P, 2019, PROC CVPR IEEE, P2893, DOI 10.1109/CVPR.2019.00301; Pidhorskyi S, 2018, ADV NEUR IN, V31; Ruff L, 2021, P IEEE, V109, P756, DOI 10.1109/JPROC.2021.3052449; Ruff L, 2018, PR MACH LEARN RES, V80; Sabokrou M, 2018, PROC CVPR IEEE, P3379, DOI 10.1109/CVPR.2018.00356; Schlegl T, 2019, MED IMAGE ANAL, V54, P30, DOI 10.1016/j.media.2019.01.010; Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; Scott CD, 2006, J MACH LEARN RES, V7, P665; Shone N, 2018, IEEE TETCI, V2, P41, DOI 10.1109/TETCI.2017.2772792; Shuangfei Zhai, 2016, Arxiv, DOI arXiv:1605.07717; Spurek Przemyslaw, 2020, INT C MACH LEARN, P9099; Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49; Vanschoren J., 2013, ACM SIGKDD EXPLOR NE, V15, P49, DOI [10.1145/2641190.2641198, DOI 10.1145/2641190.2641198]; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Wang JJ, 2019, ADV NEUR IN, V32; Wang SQ, 2019, ADV NEUR IN, V32; Wellhausen L, 2020, IEEE ROBOT AUTOM LET, V5, P1326, DOI 10.1109/LRA.2020.2967706; WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9; Yang GD, 2019, IEEE I CONF COMP VIS, P4540, DOI 10.1109/ICCV.2019.00464; Zhao M., 2009, ADV NEURAL INFORM PR, P2250; Zielinski R, 2004, OPTIMAL QUANTILE EST; Zong B, 2018, INT C LEARN REPR	57	0	0	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8508	8519		10.1109/TPAMI.2021.3108223	http://dx.doi.org/10.1109/TPAMI.2021.3108223			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34460365	Green Submitted			2022-12-18	WOS:000864325900086
J	Mirbauer, M; Krabec, M; Krivanek, J; Sikudova, E				Mirbauer, Martin; Krabec, Miroslav; Krivanek, Jaroslav; Sikudova, Elena			Survey and Evaluation of Neural 3D Shape Classification Approaches	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Shape; Feature extraction; Solid modeling; Convolution; Neural networks; Training; 3D shape analysis; classification algorithms; computer graphics; convolutional neural network; deep learning; image processing; machine learning; multi-layer neural network; neural networks; object recognition	OBJECT RECOGNITION; NETWORKS	Classification of 3D objects - the selection of a category in which each object belongs - is of great interest in the field of machine learning. Numerous researchers use deep neural networks to address this problem, altering the network architecture and representation of the 3D shape used as an input. To investigate the effectiveness of their approaches, we conduct an extensive survey of existing methods and identify common ideas by which we categorize them into a taxonomy. Second, we evaluate 11 selected classification networks on two 3D object datasets, extending the evaluation to a larger dataset on which most of the selected approaches have not been tested yet. For this, we provide a framework for converting shapes from common 3D mesh formats into formats native to each network, and for training and evaluating different classification approaches on this data. Despite being partially unable to reach the accuracies reported in the original papers, we compare the relative performance of the approaches as well as their performance when changing datasets as the only variable to provide valuable insights into performance on different kinds of data. We make our code available to simplify running training experiments with multiple neural networks with different prerequisites.	[Mirbauer, Martin; Krabec, Miroslav; Krivanek, Jaroslav; Sikudova, Elena] Charles Univ Prague, Fac Math & Phys, Prague 11000, Czech Republic; [Krivanek, Jaroslav] Chaos Czech As, Prague 12000, Czech Republic	Charles University Prague	Mirbauer, M (corresponding author), Charles Univ Prague, Fac Math & Phys, Prague 11000, Czech Republic.	martinm@cgg.mff.cuni.cz; miroslavkrabec@seznam.cz; krivanek@cgg.mff; sikudova@cgg.mff.cuni.cz	Šikudová, Elena/T-4763-2017	Šikudová, Elena/0000-0003-4572-4064	Charles University Grant Agency project [GAUK 966119]; Charles University [SVV-260588]; Grant Schemes at CU [CZ.02.2.69/0.0/0.0/19_073/0016935]	Charles University Grant Agency project; Charles University; Grant Schemes at CU	The work was supported by the Charles University Grant Agency project GAUK 966119. This work was supported by the Charles University under Grant SVV-260588. This work was supported by the Grant Schemes at CU, reg. no. CZ.02.2.69/0.0/0.0/19_073/0016935.	Abd El Rahman Shabayek, 2019, Arxiv, DOI arXiv:1808.01462; Achlioptas P., 2017, AUTOENCODING GENERAT; Achlioptas P, 2018, PR MACH LEARN RES, V80; Aditya Khosla, 2015, Arxiv, DOI arXiv:1406.5670; Andrew Brock, 2016, Arxiv, DOI arXiv:1608.04236; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Angel X. Chang, 2015, Arxiv, DOI arXiv:1512.03012; Anthony Costa, 2017, Arxiv, DOI arXiv:1710.01217; Arnold E, 2019, IEEE T INTELL TRANSP, V20, P3782, DOI 10.1109/TITS.2019.2892405; Arthur Szlam, 2014, Arxiv, DOI arXiv:1312.6203; Asako Kanezaki, 2018, Arxiv, DOI arXiv:1603.06208; Atwood J., 2017, IMPLEMENTATION DIFFU; Atwood J, 2016, ADV NEUR IN, V29; Atzmon M., 2018, PCNN TENSORFLOW IMPL; Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301; Bart van Merrienboer, 2014, Arxiv, DOI arXiv:1406.1078; Ben M. Chen, 2018, Arxiv, DOI arXiv:1803.04249; BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007; Hua BS, 2018, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2018.00109; Boscaini D., 2017, ANISOTROPIC CNN MATL; Boscaini Davide, 2016, P 30 INT C NEUR INF, P2; Brock A., 2016, VRNIN THEANOWITH LAS; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Cao Z., 2017, PROC INT C 3DVIS, P4321; Carvalho LE, 2019, PATTERN ANAL APPL, V22, P1243, DOI 10.1007/s10044-019-00804-4; Charles R. Qi, 2017, Arxiv, DOI arXiv:1706.02413; Chen Ch, 2015, HDB PATTERN RECOGNIT; Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319; Christian Szegedy, 2015, Arxiv, DOI arXiv:1502.03167; Chung F.R.K., 1997, AM MATH SOC, DOI DOI 10.1090/CBMS/092; Daniel Sedra, 2016, Arxiv, DOI arXiv:1603.09382; Daniilidis Group, 2017, DEM COD PAP LEARN SO; Defferrard M, 2016, ADV NEUR IN, V29; Didier Stricker, 2019, Arxiv, DOI arXiv:1903.10360; Dominguez M., 2018, G3DNET TENSORFLOW; Dominguez M, 2018, IEEE WINT CONF APPL, P1972, DOI 10.1109/WACV.2018.00218; Dominguez M, 2017, IEEE IMAGE PROC, P3929; Ehsan Amiri, 2017, Arxiv, DOI arXiv:1604.03351; Erik Learned-Miller, 2015, Arxiv, DOI arXiv:1505.00880; Esteves C, 2020, INT J COMPUT VISION, V128, P588, DOI 10.1007/s11263-019-01220-1; Feng C., 2018, KCNET IMPLEMENTATION; Feng Y., 2018, MESHNET PYTORCH IMPL; Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035; Feng YT, 2019, AAAI CONF ARTIF INTE, P8279; Fey M., 2019, PROC INT C LEARN REP; Gadelha M., 2018, MRTNET PYTORCH IMPLE; Gadelha M, 2018, LECT NOTES COMPUT SC, V11211, P105, DOI 10.1007/978-3-030-01234-2_7; Garcia-Garcia A, 2016, IEEE IJCNN, P1578, DOI 10.1109/IJCNN.2016.7727386; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Griffiths D, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11121499; Groh F, 2019, LECT NOTES COMPUT SC, V11361, P105, DOI 10.1007/978-3-030-20887-5_7; Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P658, DOI 10.1109/TIP.2018.2868426; Hang S., 2014, MULTIVIEW CNN MVCNN; Hanocka R., 2019, MESHCNN PYTORCH IMPL; Hanocka R, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322959; Hegde V., 2016, ARXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu DC, 2020, ADV INTELL SYST COMP, V1038, P432, DOI 10.1007/978-3-030-29513-4_31; Hua B. -S., 2018, CODE POINTWISE CONVO; Ioannidou A, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3042064; Ji Q, 1997, ACM COMPUT SURV, V29, P264, DOI 10.1145/262009.262012; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Jian Sun, 2015, Arxiv, DOI arXiv:1512.03385; Jiang Z., 2018, SURFACE NETWORKS PYT; Johns E, 2016, PROC CVPR IEEE, P3813, DOI 10.1109/CVPR.2016.414; Kanezaki A., 2018, ROTATIONNET PYTORCH; Kanezaki A., 2018, ROTATIONNET CAFFE; Kim S., 2018, GVCNN TENSORFLOW; Kipf T. N., 2016, IMPLEMENTATION GRAPH; Klokov R., 2017, KD NET THEANO LASAGN; Koguciuk D., 2018, POINT CLOUD ENSEMBLE; Koguciuk D, 2019, LECT NOTES COMPUT SC, V11845, P100, DOI 10.1007/978-3-030-33723-0_9; Komarichev A., 2019, A CNN TENSORFLOW IMP; Komarichev A, 2019, PROC CVPR IEEE, P7413, DOI 10.1109/CVPR.2019.00760; Kostrikov I, 2018, PROC CVPR IEEE, P2540, DOI 10.1109/CVPR.2018.00269; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Le T., 2018, POINTGRID IMPLEMENTA; Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959; LeCun Y, 2004, PROC CVPR IEEE, P97; Lee J, 2020, HEAT TRANSFER ENG, V41, P209, DOI 10.1080/01457632.2018.1528043; Lee J, 2019, PR MACH LEARN RES, V97; Lei H., 2020, SPH3D GCN TENSORFLOW; Lei H, 2021, IEEE T PATTERN ANAL, V43, P3664, DOI 10.1109/TPAMI.2020.2983410; Li J., 2018, SO NET TENSORFLOW; Li Y., 2016, FPNN IMPLEMENTATION; Li Y., 2017, POINTCNN TENSORFLOW; Li YY, 2018, ADV NEUR IN, V31; Li YY, 2016, ADV NEUR IN, V29; Lian Z., 2011, P EUR WORKSH 3D OBJ; Liu S., 2017, VARIATIONAL SHAPE LE; Liu SK, 2018, INT CONF 3D VISION, P542, DOI 10.1109/3DV.2018.00068; Liu X., 2019, POINT2SEQUENCE TENSO; Liu XH, 2019, AAAI CONF ARTIF INTE, P8778; Liu Y., 2019, RS CNN IMPLEMENTATIO; Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Ma C, 2019, IEEE T INSTRUM MEAS, V68, P38, DOI 10.1109/TIM.2018.2840598; Ma Y, 2012, ENSEMBLE MACHINE LEARNING: METHODS AND APPLICATIONS, P1, DOI [10.1007/978-1-4419-9326-7, 10.1007/978-1-4419-9326-7_1]; Masci J., 2015, SHAPENET DATA PREPAR; Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112; Matthias Zwicker, 2019, Arxiv, DOI arXiv:1905.07506; Maturana D., 2015, 3D VOLUMETRIC CONVOL; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Max Welling, 2017, Arxiv, DOI arXiv:1609.02907; Meagher D, 1980, IPLTR80111 RENSS POL; Merkel D., 2014, LINUX J 2014, V2014, P2, DOI DOI 10.5555/2600239.2600241; Meyer-Baese A., 2014, PATTERN RECOGN; Minto L, 2018, PROCEEDINGS OF THE 13TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2018), VOL 5: VISAPP, P317, DOI 10.5220/0006619103170324; Monti F., 2017, MONET THEONO LASAGNE; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Mou S., 2018, KERAS IMPLEMENTATION; Museth K., 2013, ACM SIGGRAPH 2013 CO, P1, DOI [10.1145/2504435.2504454, DOI 10.1145/2504435.2504454]; Person M, 2019, J DYN SYST-T ASME, V141, DOI 10.1115/1.4043222; Qi C. R., 2016, POINTNET TENSORFLOW; Qi C. R., 2017, POINTNET TENSORFLOW; Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Riegler G., 2017, OCTNET IMPLEMENTATIO; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Roman Klokov, 2017, Arxiv, DOI arXiv:1704.01222; Rostami R, 2019, COMPUT GRAPH FORUM, V38, P356, DOI 10.1111/cgf.13536; Saining Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P574, DOI 10.1007/978-3-030-58580-8_34; Sarkar K., 2018, MVCNN MULTILAYERED H; Sarkar K, 2018, LECT NOTES COMPUT SC, V11220, P74, DOI 10.1007/978-3-030-01270-0_5; Sedaghat N., 2016, ORION CAFFE; Sfikas K., 2017, P 3DOR, V8, P1, DOI DOI 10.2312/3DOR.20171045; Sfikas K, 2018, COMPUT GRAPH-UK, V71, P208, DOI 10.1016/j.cag.2017.12.001; Sharma A., 2017, SOURCE CODE 3D VOLUM; Sharma A, 2016, LECT NOTES COMPUT SC, V9915, P236, DOI 10.1007/978-3-319-49409-8_20; Shen YR, 2018, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2018.00478; Shi B., 2017, DEEPPANO PANORAMA PR; Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802; Singh RD, 2019, MULTIMED TOOLS APPL, V78, P15951, DOI 10.1007/s11042-018-6912-6; Sinha A., 2016, LEARNING GEOMETRY IM; Sinha A, 2016, LECT NOTES COMPUT SC, V9910, P223, DOI 10.1007/978-3-319-46466-4_14; Soltani AA, 2017, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR.2017.269; Su J.-C., 2018, MULTIVIEW CNN PYTORC; Su J.-C., 2018, ARXIV; Szegedy C, 2016, ARXIV; Thomas H., 2019, KPCONV IMPLEMENTATIO; Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651; Ucar A, 2017, SIMUL-T SOC MOD SIM, V93, P759, DOI 10.1177/0037549717709932; Verma N., 2018, FEASTNET TENSORFLOW; Verma N, 2018, PROC CVPR IEEE, P2598, DOI 10.1109/CVPR.2018.00275; Vukasinovic N., 2019, INTRO FREEFORM SURFA; Wang C, 2019, NEUROCOMPUTING, V323, P139, DOI 10.1016/j.neucom.2018.09.075; Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4; Wang P.-S., 2018, O CNN CAFFE; Wang PS, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275050; Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608; Wang Weiyue, 2017, P IEEE INT C COMP VI, P2298; Wang Y., 2018, DGCNN IMPLEMENTATION; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; waxnkw, 2019, GVCNN PYTORCH VERS; Wieschollek P., 2018, FLEX CONVOLUTION TEN; Williams F., 2019, POINT CLOUD UTILS A; Wu W., 2019, PYTORCH VERSION POIN; Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985; Wu Z., 2005, 3D SHAPENETS IMPLEME; Shen XK, 2019, Arxiv, DOI arXiv:1905.12683; Xie S., 2020, POINTCONTRAST IMPLEM; Xie SN, 2018, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR.2018.00484; Xu HT, 2017, IEEE I CONF COMP VIS, P2717, DOI 10.1109/ICCV.2017.294; Xu X, 2016, INT C PATT RECOG, P3506, DOI 10.1109/ICPR.2016.7900177; Xu Y., 2018, SPIDERCNN TENSORFLOW; Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6; Yang Y., 2018, FOLDINGNET CODE REQU; Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029; Yavartanoo M, 2019, LECT NOTES COMPUT SC, V11365, P691, DOI 10.1007/978-3-030-20873-8_44; Yeung C., 2017, VGG19 VGG16 TENSORFL; You H., 2018, PVRNET PVNET IMPLEME; You HX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1310, DOI 10.1145/3240508.3240702; Zaheer M., 2017, ADV NEURAL INFORM PR, P3391; Zaheer M, 2017, ADV NEUR IN, V30; Zanuttigh P, 2017, IEEE IMAGE PROC, P3615; Zelener A., 2015, SURVEY OBJECT CLASSI; Zhang C., 2016, 3D GENERATIVE ADVERS; Zhi S., 2017, PROC EUROGRAPH WORKS, P9; Zhizhong H., 2018, SEQVIEWS2SEQLABELS T	179	0	0	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8635	8656		10.1109/TPAMI.2021.3102676	http://dx.doi.org/10.1109/TPAMI.2021.3102676			22	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34406936	Green Submitted			2022-12-18	WOS:000864325900094
J	Priebe, CE; Shen, C; Huang, N; Chen, T				Priebe, Carey E.; Shen, Cencheng; Huang, Ningyuan; Chen, Tianyi			A Simple Spectral Failure Mode for Graph Convolutional Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Convolution; Task analysis; Symmetric matrices; Graph neural networks; Geometry; Training; Training data; Graph embedding; spectral embedding; node classification; convolutional neural network		Neural networks have achieved remarkable successes in machine learning tasks. This has recently been extended to graph learning using neural networks. However, there is limited theoretical work in understanding how and when they perform well, especially relative to established statistical learning techniques such as spectral embedding. In this short paper, we present a simple generative model where unsupervised graph convolutional network fails, while the adjacency spectral embedding succeeds. Specifically, unsupervised graph convolutional network is unable to look beyond the first eigenvector in certain approximately regular graphs, thus missing inference signals in non-leading eigenvectors. The phenomenon is demonstrated by visual illustrations and comprehensive simulations.	[Priebe, Carey E.] Johns Hopkins Univ, CIS, Dept Appl Math & Stat AMS, Baltimore, MD 21218 USA; [Priebe, Carey E.] Johns Hopkins Univ, Math Inst Data Sci MINDS, Baltimore, MD 21218 USA; [Shen, Cencheng] Univ Delaware, Dept Appl Econ & Stat, Newark, DE 19716 USA; [Huang, Ningyuan; Chen, Tianyi] Johns Hopkins Univ, Dept Appl Math & Stat, Baltimore, MD 21218 USA	Johns Hopkins University; Johns Hopkins University; University of Delaware; Johns Hopkins University	Shen, C (corresponding author), Univ Delaware, Dept Appl Econ & Stat, Newark, DE 19716 USA.	cep@jhu.edu; shenc@udel.edu; nhuang19@jhu.edu; tchen94@jhu.edu			Defense Advanced Research Projects Agency under the D3M program [FA8750-17-2-0112]; National Science Foundation [DMS-2113099, HDR TRIPODS 1934979]; University of Delaware Data Science Institute Seed Funding Grant; Microsoft Research	Defense Advanced Research Projects Agency under the D3M program; National Science Foundation(National Science Foundation (NSF)); University of Delaware Data Science Institute Seed Funding Grant; Microsoft Research(Microsoft)	This work was supported in part by the Defense Advanced Research Projects Agency under the D3M program administered through Contract FA8750-17-2-0112, in part by the National Science Foundation under HDR TRIPODS 1934979, in part by the National Science Foundation under DMS-2113099, in part by the University of Delaware Data Science Institute Seed Funding Grant, and in part by funding from Microsoft Research. The authors would like to thank Wade Shen for providing the motivation for this investigation, and also thank the editor and reviewers for valuable comments and suggestions that significantly improved the exposition of the article.	Athreya A, 2018, J MACH LEARN RES, V18; Cape J, 2019, ANN STAT, V47, P2405, DOI 10.1214/18-AOS1752; Carey E. Priebe, 2017, Arxiv, DOI arXiv:1705.04518; Chen ZD, 2019, ADV NEUR IN, V32; Devroye L., 1997, PROBABILISTIC THEORY; Diederik P Kingma, 2014, Arxiv, DOI arXiv:1312.6114; Du SS, 2019, ADV NEUR IN, V32; Gama F, 2020, IEEE T SIGNAL PROCES, V68, P5680, DOI 10.1109/TSP.2020.3026980; Goyal P, 2018, KNOWL-BASED SYST, V151, P78, DOI 10.1016/j.knosys.2018.03.022; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Hamilton W.L., 2020, SYNTH LECT ARTIF INT, V14, P1, DOI [1457.68005, DOI 10.1007/978-3-031-01588-5]; Hao ZK, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P731, DOI 10.1145/3394486.3403117; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Joan Bruna, 2020, Arxiv, DOI arXiv:2002.04025; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Levie R., 2020, PROC 13 INT C SAMPLI, P1; Li QM, 2018, AAAI CONF ARTIF INTE, P3538; Loukas A, 2020, PROC INT C LEARN REP; Max Welling, 2016, Arxiv, DOI arXiv:1611.07308; Max Welling, 2017, Arxiv, DOI arXiv:1609.02907; Morris C, 2019, ASS ADV ARTIF INTELL, P1; Oono K, 2019, PROC INT C LEARN REP, P1; Priebe CE, 2019, P NATL ACAD SCI USA, V116, P5995, DOI 10.1073/pnas.1814462116; Rahimi A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2009; Ruiz L., 2020, PROC 34 C NEURAL INF, P1; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P81, DOI 10.1109/TNN.2008.2005141; Sussman DL, 2012, J AM STAT ASSOC, V107, P1119, DOI 10.1080/01621459.2012.699795; Tang M, 2013, ANN STAT, V41, P1406, DOI 10.1214/13-AOS1112; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wang Q, 2022, IEEE T PATTERN ANAL, V44, P390, DOI 10.1109/TPAMI.2020.3007673; Wu F, 2019, PR MACH LEARN RES, V97; Xu K., 2020, PROC INT C LEARN REP; Xu K., 2019, ICLR, P1, DOI DOI 10.1109/VTCFALL.2019.8891597; Yuan Y, 2021, NEUROCOMPUTING, V426, P162, DOI 10.1016/j.neucom.2020.09.069; Zhao YP, 2012, ANN STAT, V40, P2266, DOI 10.1214/12-AOS1036; Zhu X. J, 2005, TR1530 U WISC MAD	37	0	0	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8689	8693		10.1109/TPAMI.2021.3104733	http://dx.doi.org/10.1109/TPAMI.2021.3104733			5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34388090	Green Submitted			2022-12-18	WOS:000864325900097
J	Styles, O; Guha, T; Sanchez, V				Styles, Olly; Guha, Tanaya; Sanchez, Victor			Multi-Camera Trajectory Forecasting With Trajectory Tensors	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Trajectory; Cameras; Forecasting; Tensors; Task analysis; Surveillance; Uncertainty; Trajectory forecasting; multi-camera tracking; person re-identification; multi-camera trajectory forecasting	PERFORMANCE	We introduce the problem of multi-camera trajectory forecasting (MCTF), which involves predicting the trajectory of a moving object across a network of cameras. While multi-camera setups are widespread for applications such as surveillance and traffic monitoring, existing trajectory forecasting methods typically focus on single-camera trajectory forecasting (SCTF), limiting their use for such applications. Furthermore, using a single camera limits the field-of-view available, making long-term trajectory forecasting impossible. We address these shortcomings of SCTF by developing an MCTF framework that simultaneously uses all estimated relative object locations from several viewpoints and predicts the object's future location in all possible viewpoints. Our framework follows a Which-When-Where approach that predicts in which camera(s) the objects appear and when and where within the camera views they appear. To this end, we propose the concept of trajectory tensors: a new technique to encode trajectories across multiple camera views and the associated uncertainties. We develop several encoder-decoder MCTF models for trajectory tensors and present extensive experiments on our own database (comprising 600 hours of video data from 15 camera views) created particularly for the MCTF task. Results show that our trajectory tensor models outperform coordinate trajectory-based MCTF models and existing SCTF methods adapted for MCTF. Code is available from: https://github.com/olly-styles/Trajectory-Tensors.	[Styles, Olly; Guha, Tanaya; Sanchez, Victor] Univ Warwick, Dept Comp Sci, Coventry CV4 7AL, W Midlands, England	University of Warwick	Styles, O (corresponding author), Univ Warwick, Dept Comp Sci, Coventry CV4 7AL, W Midlands, England.	o.c.styles@warwick.ac.uk; tanaya.guha@warwick.ac.uk; V.F.Sanchez-Silva@warwick.ac.uk		Guha, Tanaya/0000-0003-2167-4891	UK EPSRC [E/L016400/1]	UK EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The authors would like to thank NVIDIA for their generous hardware donation and the ROSE Lab, Nanyang Technological University, Singapore, for providing the data used in this work. This work was supported by the UK EPSRC under Grant E/L016400/1.	Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110; Alahi A, 2014, PROC CVPR IEEE, P2211, DOI 10.1109/CVPR.2014.283; Alexander G. Hauptmann, 2016, Arxiv, DOI arXiv:1610.02984; Camps O, 2017, IEEE T CIRC SYST VID, V27, P540, DOI 10.1109/TCSVT.2016.2556538; Fawaz HI, 2019, DATA MIN KNOWL DISC, V33, P917, DOI 10.1007/s10618-019-00619-1; Gilitschenski I, 2020, IEEE ROBOT AUTOM LET, V5, P5097, DOI 10.1109/LRA.2020.3004800; Gupta A, 2018, PROC CVPR IEEE, P2255, DOI 10.1109/CVPR.2018.00240; Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hiroaki Minoura, 2019, Arxiv, DOI arXiv:1911.09814; Hsu H., 2019, PROC IEEE C COMPUT V, P416; Jain S, 2019, HOTMOBILE '19 - PROCEEDINGS OF THE 20TH INTERNATIONAL WORKSHOP ON MOBILE COMPUTING SYSTEMS AND APPLICATIONS, P9, DOI 10.1145/3301293.3302366; Junwei Liang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10505, DOI 10.1109/CVPR42600.2020.01052; Lee N, 2017, PROC CVPR IEEE, P2165, DOI 10.1109/CVPR.2017.233; Lerner A, 2007, COMPUT GRAPH FORUM, V26, P655, DOI 10.1111/j.1467-8659.2007.01089.x; Liang JW, 2019, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR.2019.00587; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Luc P, 2018, LECT NOTES COMPUT SC, V11213, P593, DOI 10.1007/978-3-030-01240-3_36; Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190; Mangalam Karttikeya, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P759, DOI 10.1007/978-3-030-58536-5_45; Marchetti F, 2020, PROC CVPR IEEE, P7141, DOI 10.1109/CVPR42600.2020.00717; Martinel N, 2017, IEEE T CYBERNETICS, V47, P3530, DOI 10.1109/TCYB.2016.2568264; Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260; Quan RJ, 2019, IEEE I CONF COMP VIS, P3749, DOI 10.1109/ICCV.2019.00385; Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Sadeghian A, 2018, LECT NOTES COMPUT SC, V11215, P162, DOI 10.1007/978-3-030-01252-6_10; Sadeghian A, 2019, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2019.00144; Satisky J, 2019, DUKE STUDY RECORDED; Scholler C, 2020, IEEE ROBOT AUTOM LET, V5, P1696, DOI 10.1109/LRA.2020.2969925; Styles O, 2020, IEEE COMPUT SOC CONF, P4379, DOI 10.1109/CVPRW50498.2020.00516; Styles O, 2020, IEEE WINT CONF APPL, P679, DOI 10.1109/WACV45572.2020.9093446; Tang Z, 2019, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2019.00900; Wang GC, 2019, AAAI CONF ARTIF INTE, P8933; Wang XG, 2013, PATTERN RECOGN LETT, V34, P3, DOI 10.1016/j.patrec.2012.07.005; Wojke N, 2017, IEEE IMAGE PROC, P3645; Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19; Yagi T, 2018, PROC CVPR IEEE, P7593, DOI 10.1109/CVPR.2018.00792; Yao Y, 2019, IEEE INT CONF ROBOT, P9711, DOI 10.1109/ICRA.2019.8794474; Yi S, 2016, LECT NOTES COMPUT SC, V9905, P263, DOI 10.1007/978-3-319-46448-0_16; Zhang SS, 2018, IEEE T PATTERN ANAL, V40, P973, DOI 10.1109/TPAMI.2017.2700460; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133	43	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8482	8491		10.1109/TPAMI.2021.3107958	http://dx.doi.org/10.1109/TPAMI.2021.3107958			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34437059	Green Accepted, Green Submitted			2022-12-18	WOS:000864325900084
J	Su, YC; Grauman, K				Su, Yu-Chuan; Grauman, Kristen			Learning Spherical Convolution for 360 degrees Recognition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						360 degrees video analysis; omnidirectional video; object detection		While 360 degrees cameras offer tremendous new possibilities in vision, graphics, and augmented reality, the spherical images they produce make visual recognition non-trivial. Ideally, 360 degrees imagery could inherit the deep convolutional neural networks (CNNs) already trained with great success on perspective projection images. However, spherical images cannot be projected to a single plane without significant distortion, and existing methods to transfer CNNs from perspective to spherical images introduce significant computational costs and/or degradations in accuracy. We propose to learn a Spherical Convolution Network (SphConv) that translates a planar CNN to the equirectangular projection of 360 degrees images. Given a source CNN for perspective images as input, SphConv learns to reproduce the flat filter outputs on 360 degrees data, sensitive to the varying distortion effects across the viewing sphere. The key benefits are 1) efficient and accurate recognition for 360 degrees images, and 2) the ability to leverage powerful pre-trained networks for perspective images. We further proposes two instantiation of SphConv-Spherical Kernel learns location dependent kernels on the sphere for SphConv, and Kernel Transformer Network learns a functional transformation that generates SphConv kernels from the source CNN. Among the two variants, Kernel Transformer Network has a much lower memory footprint at the cost of higher computational overhead. Validating our approach with multiple source CNNs and datasets, we show that SphConv using KTN successfully preserves the source CNN's accuracy, while offering efficiency, transferability, and scalability to typical image resolutions. We further introduce a spherical Faster RCNN model based on SphConv and show that we can learn a spherical object detector without any object annotation in 360 degrees images.	[Su, Yu-Chuan; Grauman, Kristen] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA; [Grauman, Kristen] Facebook AI Res, Menlo Pk, CA 94025 USA	University of Texas System; University of Texas Austin; Facebook Inc	Su, YC (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	ycsu@cs.utexas.edu; grauman@cs.utexas.edu			NSF [IIS-1514118]	NSF(National Science Foundation (NSF))	UT Austin was supported in part by NSF IIS-1514118.	Abbas D. N. Adeel, 2017, PROC SPIE, P172; Andrew G. Howard, 2017, Arxiv, DOI arXiv:1704.04861; Asim Kadav, 2017, Arxiv, DOI arXiv:1608.08710; Ba LJ, 2014, ADV NEUR IN, V27; Boomsma W., 2017, ADV NEURAL INFORM PR, V30, P3433; Brown C., 2017, BRINGING PIXELS FRON; Bucilua Cristian, 2006, P 12 ACM SIGKDD INT, P535, DOI [10.1145/1150402.1150464, DOI 10.1145/1150402.1150464]; Chang CH, 2013, IEEE I CONF COMP VIS, P2824, DOI 10.1109/ICCV.2013.351; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Chou S.-H., 2018, P AAAI C ART INT, P6748; Cohen Taco S, 2018, ICLR; Coors B, 2018, LECT NOTES COMPUT SC, V11213, P525, DOI 10.1007/978-3-030-01240-3_32; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Esteves C, 2018, LECT NOTES COMPUT SC, V11217, P54, DOI 10.1007/978-3-030-01261-8_4; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Francesco Cricri, 2018, Arxiv, DOI arXiv:1805.08009; Geoffrey Hinton, 2015, Arxiv, DOI arXiv:1503.02531; Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309; Han S., 2016, P 4 INT C LEARN REPR, P1; Han S, 2015, ADV NEUR IN, V28; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Hu HN, 2017, PROC CVPR IEEE, P1396, DOI 10.1109/CVPR.2017.153; Jaderberg M, 2015, ADV NEUR IN, V28; Jeon YH, 2017, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2017.200; Kamali M., 2011, PROC IAPRMACH VIS AP, P177; Kasahara S., 2015, PROC ACM INT C INTER, P33; Khasanova R, 2017, IEEE INT CONF COMP V, P860, DOI 10.1109/ICCVW.2017.106; Kim YW, 2017, IEEE I CONF COMP VIS, P4753, DOI 10.1109/ICCV.2017.508; Kopf J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982405; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kuzyakov E., 2015, HOOD BUILDING 360 VI; Kuzyakov E., 2016, NEXT GENERATION VIDE; Lai WS, 2018, IEEE T VIS COMPUT GR, V24, P2610, DOI 10.1109/TVCG.2017.2750671; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin YC, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P2535, DOI 10.1145/3025453.3025757; Lin YT, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P255, DOI 10.1145/3126594.3126656; Parisotto Emilio, 2016, 4 INT C LEARN REPR I; Pavel A, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P289, DOI 10.1145/3126594.3126636; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Romero Adriana, 2015, ICLR; Simonyan Karen, 2015, INT C LEARN REPR; Su YC, 2019, PROC CVPR IEEE, P9434, DOI 10.1109/CVPR.2019.00967; Su YC, 2018, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2018.00816; Su YC, 2017, ADV NEUR IN, V30; Su YC, 2017, LECT NOTES COMPUT SC, V10114, P154, DOI 10.1007/978-3-319-54190-7_10; Su YC, 2017, PROC CVPR IEEE, P1368, DOI 10.1109/CVPR.2017.150; Wang YX, 2016, LECT NOTES COMPUT SC, V9910, P616, DOI 10.1007/978-3-319-46466-4_37; Xiong B, 2018, LECT NOTES COMPUT SC, V11209, P3, DOI 10.1007/978-3-030-01228-1_1; Yu Y, 2018, AAAI CONF ARTIF INTE, P7525; Zelnik-Manor L, 2005, IEEE I CONF COMP VIS, P1292; Zhang YD, 2014, LECT NOTES COMPUT SC, V8694, P668, DOI 10.1007/978-3-319-10599-4_43; Zhang ZH, 2018, LECT NOTES COMPUT SC, V11211, P504, DOI 10.1007/978-3-030-01234-2_30	57	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8371	8386		10.1109/TPAMI.2021.3113612	http://dx.doi.org/10.1109/TPAMI.2021.3113612			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34543192				2022-12-18	WOS:000864325900077
J	Trisedya, BD; Qi, JZ; Wang, W; Zhang, R				Trisedya, Bayu Distiawan; Qi, Jianzhong; Wang, Wei; Zhang, Rui			GCP: Graph Encoder With Content-Planning for Sentence Generation From Knowledge Bases	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Natural language processing; triple-to-text generation; knowledge base		A knowledge base is a large repository of facts usually represented as triples, each consisting of a subject, a predicate, and an object. The triples together form a graph, i.e., a knowledge graph. The triple representation in a knowledge graph offers a simple interface for applications to access the facts. However, this representation is not in a natural language form, which is difficult for humans to understand. We address this problem by proposing a system to translate a set of triples (i.e., a graph) into natural sentences. We take an encoder-decoder based approach. Specifically, we propose a Graph encoder with Content-Planning capability (GCP) to encode an input graph. GCP not only works as an encoder but also serves as a content-planner by using an entity-order aware topological traversal to encode a graph. This way, GCP can capture the relationships between entities in a knowledge graph as well as providing information regarding the proper entity order for the decoder. Hence, the decoder can generate sentences with a proper entity mention ordering. Experimental results show that GCP achieves improvements over state-of-the-art models by up to 3:6%, 4:1%, and 3:8% in three common metrics BLEU, METEOR, and TER, respectively. The code is available at (https://github.com/ruizhang-ai/GCP/)	[Trisedya, Bayu Distiawan] Univ Indonesia, Kota Depok 16424, JB, Indonesia; [Trisedya, Bayu Distiawan; Qi, Jianzhong] Univ Melbourne, Parkville, Vic, Australia; [Wang, Wei] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China; [Zhang, Rui] Tsinghua Univ, Grad Sch Shengzhen, Dept Comp Sci, Shenzhen 518055, Peoples R China	University of Indonesia; University of Melbourne; Hong Kong University of Science & Technology; Tsinghua University	Zhang, R (corresponding author), Tsinghua Univ, Grad Sch Shengzhen, Dept Comp Sci, Shenzhen 518055, Peoples R China.	bayu.trisedya@unimelb.edu.au; jianzhong.qi@unimelb.edu.au; weiwcs@ust.hk; rayteam@yeah.net	QI, JIANZHONG/P-7112-2015	QI, JIANZHONG/0000-0001-6501-9050; Trisedya, Bayu Distiawan/0000-0002-1672-9483	Indonesian Endowment Fund for Education (LPDP); Australian Research Council (ARC) [DP180102050]	Indonesian Endowment Fund for Education (LPDP); Australian Research Council (ARC)(Australian Research Council)	The work of Bayu Distiawan Trisedya was supported by the Indonesian Endowment Fund for Education (LPDP). This work was supported by Australian Research Council (ARC) Discovery Project DP180102050.	Ba J., 2015, ICLR; Bengio Y., 2014, ARXIV14061078; Bontcheva K, 2004, LECT NOTES COMPUT SC, V3136, P324; Bordes A., 2013, ADV NEURAL INFORM PR, P2787, DOI DOI 10.5555/2999792.2999923; Cao YX, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1623, DOI 10.18653/v1/P17-1149; Cimiano P., 2013, ENLG 2013 P 14 EUROP, P10; Clark J. H., 2011, P 49 ANN M ASS COMP, P176; Denkowski Michael, 2011, P 6 WORKSH STAT MACH, P85; Duboue PA, 2001, 39TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P172; Duma D., 2013, IWCS, P83; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fellbaum C, 1998, LANG SPEECH & COMMUN, P1; FLEISS JL, 1971, PSYCHOL BULL, V76, P378, DOI 10.1037/h0031619; Gardent C, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P179, DOI 10.18653/v1/P17-1017; Gardent Claire, 2017, P 10 INT C NAT LANG, P124, DOI DOI 10.18653/V1/W17-3518; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Ji GL, 2016, AAAI CONF ARTIF INTE, P985; Kipf TN, 2016, P INT C LEARN REPR; Koncel-Kedziorski Rik, 2019, P 2019 C N AM CHAPT, V1, P2284, DOI DOI 10.18653/V1/N19-1238; Lebret Remi, 2016, P 2016 C EMP METH NA, P1203, DOI DOI 10.18653/V1/D16-1128; Lehmann J, 2015, SEMANT WEB, V6, P167, DOI 10.3233/SW-140134; Liang XD, 2016, LECT NOTES COMPUT SC, V9905, P125, DOI 10.1007/978-3-319-46448-0_8; Lin YK, 2015, AAAI CONF ARTIF INTE, P2181; Liu TY, 2018, AAAI CONF ARTIF INTE, P4881; Lu W., 2009, P C EMP METH NAT LAN, P400; Lu W., 2011, PROC C EMPIR METHODS, P1611; Marcheggiani Diego, 2018, P 11 INT C NAT LANG, P1, DOI DOI 10.18653/V1/W18-6501; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Petar Velickovic, 2018, ARXIV, DOI DOI 10.48550/ARXIV.1710.10903; Puduppully R, 2019, AAAI CONF ARTIF INTE, P6908; REITER EHUD, 2000, BUILDING NATURAL LAN; Schwartz Ariel S, 2003, Pac Symp Biocomput, P451; Sha L, 2018, AAAI CONF ARTIF INTE, P5414; Snover Matthew, 2006, P ASS MACH TRANSL AM, P223; Tai KS, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1556; Trisedya BD, 2020, AAAI CONF ARTIF INTE, V34, P9057; Trisedya BD, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P229; Trisedya BD, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1627; Trisedya BD, 2019, AAAI CONF ARTIF INTE, P297; Vaswani A, 2017, ADV NEUR IN, V30; Vinyals O., 2015, ADV NEURAL INFORM PR, P2692; Vougiouklis P, 2018, J WEB SEMANT, V52-53, P1, DOI 10.1016/j.websem.2018.07.002; Wang P, 2018, IEEE T PATTERN ANAL, V40, P2413, DOI 10.1109/TPAMI.2017.2754246; Wang Q, 2017, IEEE T KNOWL DATA EN, V29, P2724, DOI 10.1109/TKDE.2017.2754499; Wang Z, 2014, AAAI CONF ARTIF INTE, P1112; Wang Zhen, 2014, EMNLP, P1591, DOI DOI 10.3115/V1/D14-1167; Wiseman Sam, 2017, P 2017 C EMP METH NA, P2253, DOI DOI 10.18653/V1/D17-1239; Yamada I., 2016, P 20 SIGNLL CONLL, P250; Zhang R., 2021, ARXIV; Zhong Huaping, 2015, EMNLP, P267	53	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7521	7533		10.1109/TPAMI.2021.3118703	http://dx.doi.org/10.1109/TPAMI.2021.3118703			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34623261				2022-12-18	WOS:000864325900021
J	Wang, GH; Ge, YF; Wu, JX				Wang, Guo-Hua; Ge, Yifan; Wu, Jianxin			Distilling Knowledge by Mimicking Features	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Hash functions; Training; Standards; Residual neural networks; Radio frequency; Numerical models; Convolutional neural networks; deep learning; knowledge distillation; image classification; object detection		Knowledge distillation (KD) is a popular method to train efficient networks ("student") with the help of high-capacity networks ("teacher"). Traditional methods use the teacher's soft logits as extra supervision to train the student network. In this paper, we argue that it is more advantageous to make the student mimic the teacher's features in the penultimate layer. Not only the student can directly learn more effective information from the teacher feature, feature mimicking can also be applied for teachers trained without a softmax layer. Experiments show that it can achieve higher accuracy than traditional KD. To further facilitate feature mimicking, we decompose a feature vector into the magnitude and the direction. We argue that the teacher should give more freedom to the student feature's magnitude, and let the student pay more attention on mimicking the feature direction. To meet this requirement, we propose a loss term based on locality-sensitive hashing (LSH). With the help of this new loss, our method indeed mimics feature directions more accurately, relaxes constraints on feature magnitudes, and achieves state-of-the-art distillation accuracy. We provide theoretical analyses of how LSH facilitates feature direction mimicking, and further extend feature mimicking to multi-label recognition and object detection.	[Wang, Guo-Hua; Ge, Yifan; Wu, Jianxin] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China	Nanjing University	Wu, JX (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.	wangguohua@lamda.nju.edu.cn; geyf@lamda.nju.edu.cn; wujx@lamda.nju.edu.cn			National Natural Science Foundation of China [61772256, 61921006]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by the National Natural Science Foundation of China under Grant 61772256 and Grant 61921006.	Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134; Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598; Chen T, 2020, PR MACH LEARN RES, V119; Datar M., 2004, P ACM 20 ANN S COMP, P253; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Gao B., 2020, ARXIV; Geoffrey Hinton, 2015, Arxiv, DOI arXiv:1503.02531; Gidaris Spyros, 2018, ARXIV180307728; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; Guodong Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P588, DOI 10.1007/978-3-030-58545-7_34; Haoqi Fan, 2020, Arxiv, DOI arXiv:2003.04297; Heo B, 2019, AAAI CONF ARTIF INTE, P3779; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Kunran Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P664, DOI 10.1007/978-3-030-58595-2_40; Lan X, 2018, ADV NEUR IN, V31; Li QQ, 2017, PROC CVPR IEEE, P7341, DOI 10.1109/CVPR.2017.776; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; Peng BY, 2019, IEEE I CONF COMP VIS, P5006, DOI 10.1109/ICCV.2019.00511; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Romero Adriana, 2015, ICLR; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Shenqi Lai, 2020, Arxiv, DOI arXiv:1911.07471; Tian Yonglong, 2020, INT C LEARN REPR, V1; Tianhong Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14627, DOI 10.1109/CVPR42600.2020.01465; Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145; Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359; Wang T, 2019, PROC CVPR IEEE, P4928, DOI 10.1109/CVPR.2019.00507; Xia RK, 2014, AAAI CONF ARTIF INTE, P2156; Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754; Youcai Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P658, DOI 10.1007/978-3-030-58529-7_39; Zagoruyko S., 2017, P INT C LEARN REPR, P1	39	0	0	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8183	8195		10.1109/TPAMI.2021.3103973	http://dx.doi.org/10.1109/TPAMI.2021.3103973			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34379588	Green Submitted			2022-12-18	WOS:000864325900065
J	Wang, Q; Guo, NH; Xiong, ZT; Yin, ZP; Li, XL				Wang, Qi; Guo, Nianhui; Xiong, Zhitong; Yin, Zeping; Li, Xuelong			Gradient Matters: Designing Binarized Neural Networks via Enhanced Information-Flow	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Convolution; Training; Neural networks; Optimization; Robustness; Kernel; Feature extraction; Neural network accelerating; 1-bit convolution; gradient approximation		Binarized neural networks (BNNs) have drawn significant attention in recent years, owing to great potential in reducing computation and storage consumption. While it is attractive, traditional BNNs usually suffer from slow convergence speed and dramatical accuracy-degradation on large-scale classification datasets. To minimize the gap between BNNs and deep neural networks (DNNs), we propose a new framework of designing BNNs, dubbed Hyper-BinaryNet, from the aspect of enhanced information-flow. Our contributions are threefold: 1) Considering the capacity-limitation in the backward pass, we propose an 1-bit convolution module named HyperConv. By exploiting the capacity of auxiliary neural networks, BNNs gain better performance on large-scale image classification task. 2) Considering the slow convergence speed in BNNs, we rethink the gradient accumulation mechanism and propose a hyper accumulation technique. By accumulating gradients in multiple variables rather than one as before, the gradient paths for each weight increase, which escapes BNNs from the gradient bottleneck problem during training. 3) Considering the ill-posed optimization problem, a novel gradient estimation warmup strategy, dubbed STE-Warmup, is developed. This strategy prevents BNNs from the unstable optimization process by progressively transferring neural networks from 32-bit to 1-bit. We conduct evaluations with variant architectures on three public datasets: CIFAR-10/100 and ImageNet. Compared with state-of-the-art BNNs, Hyper-BinaryNet shows faster convergence speed and outperforms existing BNNs by a large margin.	[Wang, Qi; Guo, Nianhui; Xiong, Zhitong; Yin, Zeping; Li, Xuelong] Northwestern Polytech Univ, Sch Artificial Intelligence Opt & Elect iOPEN, Xian 710072, Shaanxi, Peoples R China	Northwestern Polytechnical University	Li, XL (corresponding author), Northwestern Polytech Univ, Sch Artificial Intelligence Opt & Elect iOPEN, Xian 710072, Shaanxi, Peoples R China.	crabwq@gmail.com; guonianhui199512@gmail.com; xiongzhitong@gmail.com; yinzeping@mail.nwpu.edu.cn; li@nwpu.edu.cn		Wang, Qi/0000-0002-7028-4956	National Natural Science Foundation of China [61773316, 61871470]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Natural Science Foundation of China under Grants 61773316 and 61871470.	Adrian Bulat, 2020, Arxiv, DOI arXiv:2003.11535; Alexander Heinecke, 2018, Arxiv, DOI arXiv:1802.00930; Andrew Brock, 2017, Arxiv, DOI arXiv:1708.05344; Andrew Brock, 2019, Arxiv, DOI arXiv:1809.11096; Andrew Dai, 2016, Arxiv, DOI arXiv:1609.09106; Antonio Polino, 2018, Arxiv, DOI arXiv:1802.05668; Asit Mishra, 2017, Arxiv, DOI arXiv:1709.01134; Barret Zoph, 2017, Arxiv, DOI arXiv:1611.01578; Ben Poole, 2017, Arxiv, DOI arXiv:1611.01144; Bengio Y., 2013, ARXIV; Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598; Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chenzhuo Zhu, 2017, Arxiv, DOI arXiv:1612.01064; Chiu CC, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4774; Chris Zhang, 2020, Arxiv, DOI arXiv:1810.05749; Courbariaux M, 2015, ADV NEUR IN, V28; Daniel Soudry, 2016, Arxiv, DOI arXiv:1602.02830; Darabi S., 2019, BNN IMPROVED BINARY; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Gu JX, 2019, AAAI CONF ARTIF INTE, P8344; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hubara I, 2018, J MACH LEARN RES, V18; Jian Sun, 2017, Arxiv, DOI arXiv:1702.00953; Lahoud F., 2019, ARXIV; Lin XF, 2017, ADV NEUR IN, V30; Liu C, 2019, PROC CVPR IEEE, P4445, DOI 10.1109/CVPR.2019.00458; Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44; Mark D. McDonnell, 2018, Arxiv, DOI arXiv:1802.08530; Micikevicius P., 2017, ARXIV; Mnih V, 2016, PR MACH LEARN RES, V48; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Sakr C, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2346; Soudry D., 2014, PROC 27 INT C NEURAL, P963, DOI DOI 10.5555/2968826.2968934; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; Wang ZW, 2019, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.2019.00066; Zhang DQ, 2018, LECT NOTES COMPUT SC, V11212, P373, DOI 10.1007/978-3-030-01237-3_23; Zhou S., 2016, ARXIV; Zhu SL, 2019, PROC CVPR IEEE, P4918, DOI 10.1109/CVPR.2019.00506; Zhuang BH, 2019, PROC CVPR IEEE, P413, DOI 10.1109/CVPR.2019.00050	40	0	0	1	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7551	7562		10.1109/TPAMI.2021.3117908	http://dx.doi.org/10.1109/TPAMI.2021.3117908			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34613908				2022-12-18	WOS:000864325900023
J	Wei, KX; Fu, Y; Zheng, YQ; Yang, JL				Wei, Kaixuan; Fu, Ying; Zheng, Yinqiang; Yang, Jiaolong			Physics-Based Noise Modeling for Extreme Low-Light Photography	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Imaging; Noise reduction; Computational modeling; Photonics; Data models; Cameras; Training data; Extreme low-light imaging; physics-based noise modeling; deep low-light image denosing; low-light denoising dataset	IMAGE; CCD; SENSOR; SPARSE; RANGE; SPACE	Enhancing the visibility in extreme low-light environments is a challenging task. Under nearly lightless condition, existing image denoising methods could easily break down due to significantly low SNR. In this paper, we systematically study the noise statistics in the imaging pipeline of CMOS photosensors, and formulate a comprehensive noise model that can accurately characterize the real noise structures. Our novel model considers the noise sources caused by digital camera electronics which are largely overlooked by existing methods yet have significant influence on raw measurement in the dark. It provides a way to decouple the intricate noise structure into different statistical distributions with physical interpretations. Moreover, our noise model can be used to synthesize realistic training data for learning-based low-light denoising algorithms. In this regard, although promising results have been shown recently with deep convolutional neural networks, the success heavily depends on abundant noisy-clean image pairs for training, which are tremendously difficult to obtain in practice. Generalizing their trained models to images from new devices is also problematic. Extensive experiments on multiple low-light denoising datasets - including a newly collected one in this work covering various devices - show that a deep neural network trained with our proposed noise formation model can reach surprisingly-high accuracy. The results are on par with or sometimes even outperform training with paired real data, opening a new door to real-world extreme low-light photography.	[Wei, Kaixuan; Fu, Ying] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China; [Zheng, Yinqiang] Univ Tokyo, Sch Informat Sci & Technol, Tokyo 1138654, Japan; [Yang, Jiaolong] Microsoft Res Asia, Beijing 100080, Peoples R China	Beijing Institute of Technology; University of Tokyo; Microsoft; Microsoft Research Asia	Fu, Y (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.	kaixuan_wei@bit.edu.cn; fuying@bit.edu.cn; yqzheng@ai.u-tokyo.ac.jp; jiaoyan@microsoft.com			National Natural Science Foundation of China [62171038, 61827901, 61936011, 62088101]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Natural Science Foundation of China under Grants 62171038, 61827901, 61936011, and 62088101.	Abdelhamed A, 2019, IEEE I CONF COMP VIS, P3165, DOI 10.1109/ICCV.2019.00326; Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Anwar S, 2019, IEEE I CONF COMP VIS, P3155, DOI 10.1109/ICCV.2019.00325; Baer R. L., 2006, PROC SPIE THE INT SO, V6068, P37; BOIE RA, 1992, IEEE T PATTERN ANAL, V14, P671, DOI 10.1109/34.141557; Brooks T, 2019, PROC CVPR IEEE, P11028, DOI 10.1109/CVPR.2019.01129; Bruschini C, 2019, LIGHT-SCI APPL, V8, DOI 10.1038/s41377-019-0191-5; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Bychkovsky V, 2011, PROC CVPR IEEE, P97; Chen C, 2018, LECT NOTES COMPUT SC, V11215, P3, DOI 10.1007/978-3-030-01252-6_1; Chen C, 2019, IEEE I CONF COMP VIS, P3184, DOI 10.1109/ICCV.2019.00328; Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347; Chen GY, 2015, IEEE I CONF COMP VIS, P477, DOI 10.1109/ICCV.2015.62; Chen YJ, 2015, PROC CVPR IEEE, P5261, DOI 10.1109/CVPR.2015.7299163; Clark R. N., 2012, EXPOSURE DIGITAL C 1; Clark R. N., 2016, SENSOR DARK CURRENT; Costantini R, 2004, PROC SPIE, V5301, P408, DOI 10.1117/12.525704; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Dong WS, 2011, PROC CVPR IEEE, P457, DOI 10.1109/CVPR.2011.5995478; Dussault D, 2004, P SOC PHOTO-OPT INS, V5563, P195, DOI 10.1117/12.561839; El Gamal A, 2005, IEEE CIRCUITS DEVICE, V21, P6, DOI 10.1109/MCD.2005.1438751; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Farrell J, 2008, PROC SPIE, V6817, DOI 10.1117/12.767901; FILLIBEN JJ, 1975, TECHNOMETRICS, V17, P111, DOI 10.2307/1268008; Foi A, 2008, IEEE T IMAGE PROCESS, V17, P1737, DOI 10.1109/TIP.2008.2001399; Fossum ER, 2014, IEEE J ELECTRON DEVI, V2, P33, DOI 10.1109/JEDS.2014.2306412; G. V. Research, 2016, IM SENS MARK AN; Gariepy G, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7021; Gharbi M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982399; Gilblom DL, 2004, P SOC PHOTO-OPT INS, V5210, P105, DOI 10.1117/12.506206; Gow RD, 2007, IEEE T ELECTRON DEV, V54, P1321, DOI 10.1109/TED.2007.896718; Grossberg MD, 2004, IEEE T PATTERN ANAL, V26, P1272, DOI 10.1109/TPAMI.2004.88; Gu SH, 2019, IEEE I CONF COMP VIS, P2511, DOI 10.1109/ICCV.2019.00260; Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366; Guerrieri Fabrizio, 2009, Proceedings of the SPIE - The International Society for Optical Engineering, V7249, DOI 10.1117/12.807426; Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181; Hasinoff SW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980254; HEALEY GE, 1994, IEEE T PATTERN ANAL, V16, P267, DOI 10.1109/34.276126; Heide F, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-35212-x; Holst GC, 1998, CCD ARRAYS CAMERAS D; Irie K, 2008, IEEE T CIRC SYST VID, V18, P280, DOI 10.1109/TCSVT.2007.913972; James Welsh, 2014, Arxiv, DOI arXiv:1412.4031; Jiang HY, 2019, IEEE I CONF COMP VIS, P7323, DOI 10.1109/ICCV.2019.00742; Jocher G., 2021, YOLOV5, DOI [10.5281/zenodo.4418161, DOI 10.5281/ZENODO.4418161]; Joens MS, 2013, SCI REP-UK, V3, DOI 10.1038/srep03514; JOINER BL, 1971, J AM STAT ASSOC, V66, P394, DOI 10.2307/2283943; Ke-Chi Chang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P343, DOI 10.1007/978-3-030-58586-0_21; Kim SJ, 2012, IEEE T PATTERN ANAL, V34, P2289, DOI 10.1109/TPAMI.2012.58; Kingma D.P, 2015, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1412.6980; Kirmani A, 2014, SCIENCE, V343, P58, DOI 10.1126/science.1246775; Krull A, 2019, PROC CVPR IEEE, P2124, DOI 10.1109/CVPR.2019.00223; Lehtinen J, 2018, PR MACH LEARN RES, V80; Levin N, 2020, REMOTE SENS ENVIRON, V237, DOI 10.1016/j.rse.2019.111443; Leyris C, 2005, PROC SPIE, V5844, P41, DOI 10.1117/12.609375; Liba O, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356508; Lin WS, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17010015; Liu ZW, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661277; Llull P, 2013, OPT EXPRESS, V21, P10526, DOI 10.1364/OE.21.010526; Maggioni M, 2012, IEEE T IMAGE PROCESS, V21, P3952, DOI 10.1109/TIP.2012.2199324; Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828; Makitalo M, 2011, IEEE T IMAGE PROCESS, V20, P99, DOI 10.1109/TIP.2010.2056693; Mao XJ, 2016, ADV NEUR IN, V29; McLean IS, 2008, ELECTRONIC IMAGING IN ASTRONOMY: DETECTORS AND INSTRUMENTATION, P1, DOI 10.1007/978-3-540-76583-7; Mildenhall B, 2018, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2018.00265; Morgan EC, 2011, ENERG CONVERS MANAGE, V52, P15, DOI 10.1016/j.enconman.2010.06.015; Morris PA, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms6913; O'Toole M, 2017, PROC CVPR IEEE, P2289, DOI 10.1109/CVPR.2017.246; Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412; Plotz T, 2017, PROC CVPR IEEE, P2750, DOI 10.1109/CVPR.2017.294; Ranftl R, 2022, IEEE T PATTERN ANAL, V44, P1623, DOI 10.1109/TPAMI.2020.3019967; Ronneberger O., 2015, P INT C MED IM COMP; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349; Schwartz E, 2019, IEEE T IMAGE PROCESS, V28, P912, DOI 10.1109/TIP.2018.2872858; SHAPIRO SS, 1972, J AM STAT ASSOC, V67, P215, DOI 10.2307/2284728; Sheinin M, 2017, PROC CVPR IEEE, P2363, DOI 10.1109/CVPR.2017.254; Shin D, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12046; Soundararajan R, 2013, IEEE T CIRC SYST VID, V23, P684, DOI 10.1109/TCSVT.2012.2214933; Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486; Tassano M, 2020, PROC CVPR IEEE, P1351, DOI 10.1109/CVPR42600.2020.00143; Teed Zachary, 2020, ECCV, DOI DOI 10.1007/978-3-030-58536-5_24; TORR MR, 1986, APPL OPTICS, V25, P2768, DOI 10.1364/AO.25.002768; Wach HB, 2004, P SOC PHOTO-OPT INS, V5438, P159, DOI 10.1117/12.542258; Wang W, 2019, IEEE I CONF COMP VIS, P4110, DOI 10.1109/ICCV.2019.00421; Wei KX, 2020, PROC CVPR IEEE, P2755, DOI 10.1109/CVPR42600.2020.00283; WILK MB, 1968, BIOMETRIKA, V55, P1; Xu J, 2018, LECT NOTES COMPUT SC, V11212, P21, DOI 10.1007/978-3-030-01237-3_2; Xu J, 2017, IEEE I CONF COMP VIS, P1105, DOI 10.1109/ICCV.2017.125; Zhang JC, 2017, IEEE T IMAGE PROCESS, V26, P1565, DOI 10.1109/TIP.2017.2651365; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zhihao Xia, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11841, DOI 10.1109/CVPR42600.2020.01186	92	0	0	7	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8520	8537		10.1109/TPAMI.2021.3103114	http://dx.doi.org/10.1109/TPAMI.2021.3103114			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34375279	Green Submitted			2022-12-18	WOS:000864325900087
J	Xu, HH; Xiong, HK; Qi, GJ				Xu, Haohang; Xiong, Hongkai; Qi, Guo-Jun			K-Shot Contrastive Learning of Visual Features With Multiple Instance Augmentations	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Unsupervised learning; self-supervised learning; contrastive learning		In this paper, we propose the K-Shot Contrastive Learning (KSCL) of visual features by applying multiple augmentations to investigate the sample variations within individual instances. It aims to combine the advantages of interinstance discrimination by learning discriminative features to distinguish between different instances, as well as intra-instance variations by matching queries against the variants of augmented samples over instances. Particularly, for each instance, it constructs an instance subspace to model the configuration of how the significant factors of variations in K-shot augmentations can be combined to form the variants of augmentations. Given a query, the most relevant variant of instances is then retrieved by projecting the query onto their subspaces to predict the positive instance class. This generalizes the existing contrastive learning that can be viewed as a special one-shot case. An eigenvalue decomposition is performed to configure instance subspaces, and the embedding network can be trained end-to-end through the differentiable subspace configuration. Experiment results demonstrate the proposed K-shot contrastive learning achieves superior performances to the state-of-the-art unsupervised methods.	[Xu, Haohang; Xiong, Hongkai] Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China; [Qi, Guo-Jun] OPPO Res USA, Seattle Res Ctr, Bellevue, WA 98004 USA	Shanghai Jiao Tong University	Qi, GJ (corresponding author), OPPO Res USA, Seattle Res Ctr, Bellevue, WA 98004 USA.	xuhaohang@sjtu.edu.cn; xionghongkai@sjtu.edu.cn; guojunq@gmail.com		Xiong, Hongkai/0000-0003-4552-0029; Xu, Haohang/0000-0002-4715-1338	National Natural Science Foundation of China [61932022, 61720106001, 61971285, 61931023, 61831018, 61871267, 61972256, 61838303]; Program of Shanghai Science and Technology Innovation Project [20511100100]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Program of Shanghai Science and Technology Innovation Project	Guo-Jun Qi conceived the idea and led the project. This work was supported in part by the National Natural Science Foundation of China under Grants 61932022, 61720106001, 61971285, 61931023, 61831018, 61871267, 61972256, and 61838303 and in part by the Program of Shanghai Science and Technology Innovation Project under Grant 20511100100.	Aaron van den Oord, 2019, Arxiv, DOI arXiv:1807.03748; Bachman P, 2019, ADV NEUR IN, V32; Chang SY, 2014, IEEE DATA MINING, P60, DOI 10.1109/ICDM.2014.115; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Gao X, 2020, PROC CVPR IEEE, P7161, DOI 10.1109/CVPR42600.2020.00719; Geoffrey Hinton, 2020, Arxiv, DOI arXiv:2002.05709; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo-Jun Qi, 2019, Arxiv, DOI arXiv:1901.04596; Guojun Qi, 2019, Arxiv, DOI arXiv:1912.12674; Haoqi Fan, 2020, Arxiv, DOI arXiv:1911.05722; Haoqi Fan, 2020, Arxiv, DOI arXiv:2003.04297; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Henaff OJ, 2020, PR MACH LEARN RES, V119; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Hjelm R.D., 2018, P INT C LEARN REPR; Hua Xian-Sheng, 2008, P 16 ACM INT C MULTI, P141; Jamal MA, 2019, PROC CVPR IEEE, P11711, DOI 10.1109/CVPR.2019.01199; Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7; Nikos Komodakis, 2018, Arxiv, DOI arXiv:1803.07728; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053; Qi GJ, 2022, IEEE T PATTERN ANAL, V44, P2168, DOI 10.1109/TPAMI.2020.3031898; Qi GJ, 2022, IEEE T PATTERN ANAL, V44, P2045, DOI 10.1109/TPAMI.2020.3029801; Qi GJ, 2019, IEEE I CONF COMP VIS, P8129, DOI 10.1109/ICCV.2019.00822; Qi GJ, 2020, INT J COMPUT VISION, V128, P1118, DOI 10.1007/s11263-019-01265-2; Qi GJ, 2018, PROC CVPR IEEE, P1517, DOI 10.1109/CVPR.2018.00164; Qi GJ, 2016, PROC CVPR IEEE, P2267, DOI 10.1109/CVPR.2016.249; Qi H, 2018, PROC CVPR IEEE, P5822, DOI 10.1109/CVPR.2018.00610; Qin YX, 2021, KNOWL-BASED SYST, V213, DOI 10.1016/j.knosys.2020.106609; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Sabour S, 2017, ADV NEUR IN, V30; Shu XB, 2018, IEEE T CIRC SYST VID, V28, P454, DOI 10.1109/TCSVT.2016.2607345; Tian Y., 2020, ARXIV200510243, V33, P6827; Tian Y., 2020, P EUR C COMP VIS ECC; Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393; Zhang LH, 2018, ADV NEUR IN, V31; Zhao YR, 2018, LECT NOTES COMPUT SC, V11213, P508, DOI 10.1007/978-3-030-01240-3_31; Fang ZY, 2021, Arxiv, DOI arXiv:2101.04731	42	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8694	8700		10.1109/TPAMI.2021.3082567	http://dx.doi.org/10.1109/TPAMI.2021.3082567			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34018928	Green Submitted			2022-12-18	WOS:000864325900098
J	Xu, J; Zhang, W; Wang, F				Xu, Jie; Zhang, Wei; Wang, Fei			A(DP)(2)SGD: Asynchronous Decentralized Parallel Stochastic Gradient Descent With Differential Privacy	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Distributed learning; asynchronous; decentralized; differential privacy		As deep learning models are usually massive and complex, distributed learning is essential for increasing training efficiency. Moreover, in many real-world application scenarios like healthcare, distributed learning can also keep the data local and protect privacy. Recently, the asynchronous decentralized parallel stochastic gradient descent (ADPSGD) algorithm has been proposed and demonstrated to be an efficient and practical strategy where there is no central server, so that each computing node only communicates with its neighbors. Although no raw data will be transmitted across different local nodes, there is still a risk of information leak during the communication process for malicious participants to make attacks. In this paper, we present a differentially private version of asynchronous decentralized parallel SGD framework, or A(DP)(2)SGD for short, which maintains communication efficiency of ADPSGD and prevents the inference from malicious participants. Specifically, Renyi differential privacy is used to provide tighter privacy analysis for our composite Gaussian mechanisms while the convergence rate is consistent with the non-private version. Theoretical analysis shows A(DP)(2)SGD also converges at the optimal O(1/root T) rate as SGD. Empirically, A(DP)(2)SGD achieves comparable model accuracy as the differentially private version of Synchronous SGD (SSGD) but runs much faster than SSGD in heterogeneous computing environments.	[Xu, Jie; Wang, Fei] Weill Cornell Med, Dept Populat Hlth Sci, New York, NY 10065 USA; [Zhang, Wei] IBM Res, Armonk, NY 10504 USA	Cornell University; International Business Machines (IBM)	Xu, J (corresponding author), Weill Cornell Med, Dept Populat Hlth Sci, New York, NY 10065 USA.	jix4002@med.cornell.edu; weiz@us.ibm.com; few2001@med.cornell.edu			ONR [N00014-18-1-2585]; NSF [1750326]; Amazon AWS Machine Learning Research Award; Google Faculty Research Award	ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); Amazon AWS Machine Learning Research Award; Google Faculty Research Award(Google Incorporated)	The work of Jie Xu and Fei Wang was supported in part by the ONR under Grant N00014-18-1-2585 and in part by the NSF under Grant 1750326, an Amazon AWS Machine Learning Research Award and a Google Faculty Research Award.	Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Abhijit G.R., 2019, ARXIV; Ali Anwar, 2019, Arxiv, DOI arXiv:1812.03224; Andrew G. Howard, 2017, Arxiv, DOI arXiv:1704.04861; Andrew Howard, 2019, Arxiv, DOI arXiv:1801.04381; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Anusha Lalitha, 2019, Arxiv, DOI arXiv:1905.10466; Anusha Lalitha, 2019, Arxiv, DOI arXiv:1901.11173; Bargav Jayaraman, 2020, Arxiv, DOI arXiv:1910.13659; Bellet A., 2017, ARXIV; Bellet A., 2017, DISS; Borja Balle, 2018, Arxiv, DOI arXiv:1808.00087; Chaudhuri K, 2013, J MACH LEARN RES, V14, P2905; Cheng HP, 2019, LECT NOTES COMPUT SC, V11513, P130, DOI 10.1007/978-3-030-23502-4_10; Chris J.M. Yoon, 2019, Arxiv, DOI arXiv:1811.09904; Cong Zhao, 2019, Arxiv, DOI arXiv:1912.07902; Daniel Ramage, 2018, Arxiv, DOI arXiv:1710.06963; Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486; Feng Yan, 2018, Arxiv, DOI arXiv:1811.11124; Goyal P., 2017, ARXIV, DOI DOI 10.48550/ARXIV.1706.02677; Gupta S, 2016, IEEE DATA MINING, P171, DOI [10.1109/ICDM.2016.122, 10.1109/ICDM.2016.0028]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Jianmin Chen, 2017, Arxiv, DOI arXiv:1604.00981; Keisuke Fukuda, 2017, Arxiv, DOI arXiv:1710.11351; Konecny J., 2016, FEDERATED LEARNING S, DOI DOI 10.48550/ARXIV.1610.05492; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Lian XR, 2018, PR MACH LEARN RES, V80; Liu K., TRAIN CIFAR10 PYTORC; Lu YL, 2020, IEEE T IND INFORM, V16, P2134, DOI 10.1109/TII.2019.2942179; Mang W, 2020, INT CONF ACOUST SPEE, P3022, DOI 10.1109/ICASSP40776.2020.9054065; Mingxing Tan, 2020, Arxiv, DOI arXiv:1905.11946; Mironov I, 2017, P IEEE CSFW, P263, DOI 10.1109/CSF.2017.11; Nvidia, NCCL OPT PRIM COLL M; Rubinstein B. I., 2009, ARXIV; Sattler F., 2019, ARXIV; Shokri R, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1310, DOI 10.1145/2810103.2813687; Smith V, 2017, ADV NEUR IN, V30; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Zhang W., 2016, PROC 25 INT JOINT C, P2350; Zhang W, 2019, INTERSPEECH, P2628, DOI 10.21437/Interspeech.2019-2700; Zhang W, 2019, INT CONF ACOUST SPEE, P5706, DOI 10.1109/ICASSP.2019.8682888; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zinkevich M., 2010, P ADV NEUR INF PROC, P2595	45	0	0	12	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8036	8047		10.1109/TPAMI.2021.3107796	http://dx.doi.org/10.1109/TPAMI.2021.3107796			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34449356	Green Submitted			2022-12-18	WOS:000864325900055
J	Yang, DF; Chen, JZ; Yan, CG; Kim, M; Laurienti, PJ; Styner, M; Wu, GR				Yang, Defu; Chen, Jiazhou; Yan, Chenggang; Kim, Minjeong; Laurienti, Paul J.; Styner, Martin; Wu, Guorong			Group-Wise Hub Identification by Learning Common Graph Embeddings on Grassmannian Manifold	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Connectors; Manifolds; Neuroscience; Sociology; Diseases; Replicability; Psychiatry; Graph embedding; optimization; grassmannian manifold; hub identification; connector hub	FUNCTIONAL CONNECTIVITY; HUMAN CONNECTOME; ALZHEIMERS-DISEASE; CORTICAL HUBS; NETWORK HUBS; BRAIN; PARCELLATION; ORGANIZATION; DISRUPTION	Human brain is a complex yet economically organized system, where a small portion of critical hub regions support the majority of brain functions. The identification of common hub nodes in a population of networks is often simplified as a voting procedure on the set of identified hub nodes across individual brain networks, which ignores the intrinsic data geometry and partially lacks the reproducible findings in neuroscience. Hence, we propose a first-ever group-wise hub identification method to identify hub nodes that are common across a population of individual brain networks. Specifically, the backbone of our method is to learn common graph embedding that can represent the majority of local topological profiles. By requiring orthogonality among the graph embedding vectors, each graph embedding as a data element is residing on the Grassmannian manifold. We present a novel Grassmannian manifold optimization scheme that allows us to find the common graph embeddings, which not only identify the most reliable hub nodes in each network but also yield a population-based common hub node map. Results of the accuracy and replicability on both synthetic and real network data show that the proposed manifold learning approach outperforms all hub identification methods employed in this evaluation.	[Yang, Defu; Yan, Chenggang] Hangzhou Dianzi Univ, Intelligent Informat Proc Lab, Hangzhou 310018, Zhejiang, Peoples R China; [Yang, Defu; Chen, Jiazhou; Styner, Martin; Wu, Guorong] Univ N Carolina, Dept Psychiat, Chapel Hill, NC 27599 USA; [Kim, Minjeong] Univ North Carolina Greensboro, Dept Comp Sci, Greensboro, NC 27412 USA; [Laurienti, Paul J.] Wake Forest Sch Med, Dept Radiol, Winston Salem, NC 27157 USA; [Styner, Martin; Wu, Guorong] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27599 USA	Hangzhou Dianzi University; University of North Carolina; University of North Carolina Chapel Hill; University of North Carolina; University of North Carolina Greensboro; Wake Forest University; University of North Carolina; University of North Carolina Chapel Hill	Yang, DF (corresponding author), Hangzhou Dianzi Univ, Intelligent Informat Proc Lab, Hangzhou 310018, Zhejiang, Peoples R China.	dfyang@hdu.edu.cn; 09chenjiazhou@gmail.com; cgyan@hdu.edu.cn; mkim@uncg.edu; plaurien@wakehealth.edu; martin_styner@mac.com; grwu@med.unc.edu			National Natural Science Foundation of China [61801157]; National Institutes of Health [AG068399, AG059065, AG070701]; Foundation of Hope	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Foundation of Hope	This work was supported in part by the National Natural Science Foundation of China under Grant 61801157, in part by the National Institutes of Health under Grants AG068399, AG059065, and AG070701, and in part by the Foundation of Hope.	Aerts H, 2016, BRAIN, V139, P3063, DOI 10.1093/brain/aww194; Bassett DS, 2017, NEUROSCIENTIST, V23, P499, DOI 10.1177/1073858416667720; Bassett DS, 2006, NEUROSCIENTIST, V12, P512, DOI 10.1177/1073858406293182; Buckner RL, 2009, J NEUROSCI, V29, P1860, DOI 10.1523/JNEUROSCI.5062-08.2009; Bullmore ET, 2012, NAT REV NEUROSCI, V13, P336, DOI 10.1038/nrn3214; Cetingul HE, 2009, PROC CVPR IEEE, P1896, DOI 10.1109/CVPRW.2009.5206806; Cope TE, 2018, BRAIN, V141, P550, DOI 10.1093/brain/awx347; Crossley NA, 2014, BRAIN, V137, P2382, DOI 10.1093/brain/awu132; Dai ZJ, 2015, CEREB CORTEX, V25, P3723, DOI 10.1093/cercor/bhu246; Daianu M, 2015, HUM BRAIN MAPP, V36, P3087, DOI 10.1002/hbm.22830; de Haan W, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002582; Destrieux C, 2010, NEUROIMAGE, V53, P1, DOI 10.1016/j.neuroimage.2010.06.010; Dipasquale O, 2016, FUNCT NEUROL, V31, P191; Dong XW, 2014, IEEE T SIGNAL PROCES, V62, P905, DOI 10.1109/TSP.2013.2295553; Drzezga A, 2011, BRAIN, V134, P1635, DOI 10.1093/brain/awr066; FAN K, 1949, P NATL ACAD SCI USA, V35, P652, DOI 10.1073/pnas.35.11.652; Ferreira D, 2019, FRONT NEUROL, V10, DOI 10.3389/fneur.2019.00524; Fornito A, 2015, NAT REV NEUROSCI, V16, P159, DOI 10.1038/nrn3901; Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002; Glasser MF, 2013, NEUROIMAGE, V80, P105, DOI 10.1016/j.neuroimage.2013.04.127; Gratton C, 2012, J COGNITIVE NEUROSCI, V24, P1275, DOI 10.1162/jocn_a_00222; Greicius M, 2008, CURR OPIN NEUROL, V21, P424, DOI 10.1097/WCO.0b013e328306f2c5; Guimera R, 2005, NATURE, V433, P895, DOI 10.1038/nature03288; Hedden T, 2009, J NEUROSCI, V29, P12686, DOI 10.1523/JNEUROSCI.3189-09.2009; Jiao ZQ, 2018, COMPUT ELECTR ENG, V69, P740, DOI 10.1016/j.compeleceng.2018.01.010; Mattila P, 1995, GEOMETRY SETS MEASUR; Mattson MP, 2006, NAT REV NEUROSCI, V7, P278, DOI 10.1038/nrn1886; Menon AK, 2011, ACM T KNOWL DISCOV D, V5, DOI 10.1145/1921632.1921639; Mishra B, 2019, MACH LEARN, V108, P1783, DOI 10.1007/s10994-018-05775-x; Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103; Ng AY, 2002, ADV NEUR IN, V14, P849; Palmqvist S, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01150-x; Papo D, 2014, PHILOS T R SOC B, V369, DOI 10.1098/rstb.2013.0520; Pereira JB, 2018, CEREB CORTEX, V28, P3638, DOI 10.1093/cercor/bhx236; Power JD, 2011, NEURON, V72, P665, DOI 10.1016/j.neuron.2011.09.006; Preti MG, 2017, NEUROIMAGE, V160, P41, DOI 10.1016/j.neuroimage.2016.12.061; Pruim RHR, 2015, NEUROIMAGE, V112, P267, DOI 10.1016/j.neuroimage.2015.02.064; Rubinov M, 2010, NEUROIMAGE, V52, P1059, DOI 10.1016/j.neuroimage.2009.10.003; SHALIKA JA, 1974, ANN MATH, V100, P171, DOI 10.2307/1971071; Sheline YI, 2013, BIOL PSYCHIAT, V74, P340, DOI 10.1016/j.biopsych.2012.11.028; Shen X, 2013, NEUROIMAGE, V82, P403, DOI 10.1016/j.neuroimage.2013.05.081; Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192; Smart C. M., 2014, ALZHEIMERS DEMENT, V10; Sporns O, 2005, PLOS COMPUT BIOL, V1, P245, DOI 10.1371/journal.pcbi.0010042; Sporns O., 2009, NETWORKS BRAIN; Sporns O, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0001049; Turaga P., 2008, PROC IEEE C COMPUT V, P1; van den Heuvel MP, 2013, TRENDS COGN SCI, V17, P683, DOI 10.1016/j.tics.2013.09.012; van den Heuvel MP, 2010, EUR NEUROPSYCHOPHARM, V20, P519, DOI 10.1016/j.euroneuro.2010.03.008; Wang YT, 2020, I S BIOMED IMAGING, P292, DOI [10.1109/ISBI45749.2020.9098513, 10.1109/isbi45749.2020.9098513]; Wu K, 2013, FRONT HUM NEUROSCI, V7, DOI 10.3389/fnhum.2013.00113; Yang DF, 2019, LECT NOTES COMPUT SC, V11766, P590, DOI 10.1007/978-3-030-32248-9_66; Zhou D., 2006, ADV NEURAL INF PROCE, V19, P1601	53	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8249	8260		10.1109/TPAMI.2021.3081744	http://dx.doi.org/10.1109/TPAMI.2021.3081744			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34010126				2022-12-18	WOS:000864325900069
J	Yang, ZY; Xu, QQ; Bao, SL; Cao, XC; Huang, QM				Yang, Zhiyong; Xu, Qianqian; Bao, Shilong; Cao, Xiaochun; Huang, Qingming			Learning With Multiclass AUC: Theory and Algorithms	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						AUC optimization; machine learning	AREA; OPTIMIZATION; RANKING; PROBABILITY; BOUNDS; CURVE	The Area under the ROC curve (AUC) is a well-known ranking metric for problems such as imbalanced learning and recommender systems. The vast majority of existing AUC-optimization-based machine learning methods only focus on binary-class cases, while leaving the multiclass cases unconsidered. In this paper, we start an early trial to consider the problem of learning multiclass scoring functions via optimizing multiclass AUC metrics. Our foundation is based on the M metric, which is a well-known multiclass extension of AUC. We first pay a revisit to this metric, showing that it could eliminate the imbalance issue from the minority class pairs. Motivated by this, we propose an empirical surrogate risk minimization framework to approximately optimize the M metric. Theoretically, we show that: (i) optimizing most of the popular differentiable surrogate losses suffices to reach the Bayes optimal scoring function asymptotically; (ii) the training framework enjoys an imbalance-aware generalization error bound, which pays more attention to the bottleneck samples of minority classes compared with the traditional O(root 1/N) THORN result. Practically, to deal with the low scalability of the computational operations, we propose acceleration methods for three popular surrogate loss functions, including the exponential loss, squared loss, and hinge loss, to speed up loss and gradient evaluations. Finally, experimental results on 11 real-world datasets demonstrate the effectiveness of our proposed framework. The code is now available at https://github.com/joshuaas/ Learning-with-Multiclass-AUC-Theory-and-Algorithms.	[Yang, Zhiyong] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China; [Xu, Qianqian; Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China; [Bao, Shilong; Cao, Xiaochun] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing 100093, Peoples R China; [Bao, Shilong] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China; [Cao, Xiaochun] Sun Yat Sen Univ, Sch Cyber Sci & Technol, Shenzhen 518100, Peoples R China; [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Key Lab Big Data Min & Knowledge Management BDKM, Beijing 101408, Peoples R China; [Huang, Qingming] Peng Cheng Lab, Shenzhen 518055, Peoples R China	Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Institute of Computing Technology, CAS; Chinese Academy of Sciences; Institute of Information Engineering, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Sun Yat Sen University; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Peng Cheng Laboratory	Xu, QQ; Huang, QM (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.; Huang, QM (corresponding author), Univ Chinese Acad Sci, Sch Comp Sci & Technol, Key Lab Big Data Min & Knowledge Management BDKM, Beijing 101408, Peoples R China.	joshua.zhiyongyang@gmail.com; xuqianqian@ict.ac.cn; baoshilong@iie.ac.cn; caoxiaochun@iie.ac.cn; qmhuang@ucas.ac.cn		Yang, Zhiyong/0000-0002-4409-4999	National Key R&D Program of China [2018AAA0102003]; National Natural Science Foundation of China [61620106009, 62025604, 61861166002, 61931008, 61836002, 61976202]; Fundamental Research Funds for the Central Universities; National Postdoctoral Program for Innovative Talents [BX2021298]; Youth Innovation Promotion Association CAS; Strategic Priority Research Program of Chinese Academy of Sciences [XDB28000000]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); National Postdoctoral Program for Innovative Talents; Youth Innovation Promotion Association CAS; Strategic Priority Research Program of Chinese Academy of Sciences(Chinese Academy of Sciences)	This work was supported in part by the National Key R&D Program of China under Grant 2018AAA0102003, in part by the National Natural Science Foundation of China under Grants 61620106009, 62025604, 61861166002, 61931008, 61836002, and 61976202, in part by the Fundamental Research Funds for the Central Universities, in part by the National Postdoctoral Program for Innovative Talents under Grant BX2021298, in part by Youth Innovation Promotion Association CAS, and in part by the Strategic Priority Research Program of Chinese Academy of Sciences under Grant XDB28000000.	Agarwal S, 2005, J MACH LEARN RES, V6, P393; Agarwal S, 2014, J MACH LEARN RES, V15, P1653; Ata Kaban, 2020, Arxiv, DOI arXiv:2002.09769; Bai ZX, 2020, INT CONF ACOUST SPEE, P6819, DOI 10.1109/ICASSP40776.2020.9053674; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Barua S, 2014, IEEE T KNOWL DATA EN, V26, P405, DOI 10.1109/TKDE.2012.232; Boucheron S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Calders T, 2007, LECT NOTES ARTIF INT, V4702, P42; Cao KD, 2019, ADV NEUR IN, V32; Chen YH, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P978, DOI 10.1145/3292500.3330880; Clemencon S., 2017, P ADV NEUR INF PROC, P4600; Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910; Clemencon S, 2013, MACH LEARN, V91, P67, DOI 10.1007/s10994-012-5325-4; Cortes C, 2004, ADV NEUR IN, V16, P313; Cortes Corinna, 2016, ADV NEURAL INFORM PR, P2514; Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949; Dai L, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P774, DOI 10.1145/3394486.3403120; Dang ZY, 2022, IEEE T PATTERN ANAL, V44, P1385, DOI 10.1109/TPAMI.2020.3024987; Ding JT, 2018, COMPANION PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2018 (WWW 2018), P13, DOI 10.1145/3184558.3186905; Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010; Ferri C, 2003, LECT NOTES ARTIF INT, V2837, P108; Freund Y, 2004, J MACH LEARN RES, V4, P933, DOI 10.1162/1532443041827916; Gao W., 2013, P 30 INT C MACHINE L, P906; Gao W, 2018, INT J COMPUT MATH, V95, P1527, DOI 10.1080/00207160.2017.1322688; Gao W, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P939; Gardy JL, 2003, NUCLEIC ACIDS RES, V31, P3613, DOI 10.1093/nar/gkg602; Golowich N, 2020, INF INFERENCE, V9, P473, DOI 10.1093/imaiai/iaz007; Gu B, 2019, AAAI CONF ARTIF INTE, P3697; Han H, 2005, LECT NOTES COMPUT SC, V3644, P878, DOI 10.1007/11538059_91; Hand DJ, 2001, MACH LEARN, V45, P171, DOI 10.1023/A:1010920819831; HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747; Hao H., 2020, PROC INT C MED IMAGE; Herschtal A., 2004, P 21 INT C MACH LEAR, P49; Honzik P., 2009, PROC INT C SOFT COMP, P1; Hsu C.-W., 2003, CITESEER; Joachims T., 2006, P 12 ACM SIGKDD INT, V06, P217, DOI DOI 10.1145/1150402.1150429; Joachims Thorsten, 2005, ICML, DOI DOI 10.1145/1102351.1102399; Kingma DP, 2015, INT C LEARN REPR ICL; Lane T., 2000, PROC ICML WORKSHOP C; Ledoux M, 2011, CLASS MATH, P1; Lemaitre G, 2017, J MACH LEARN RES, V18; Li H., 2015, PROC 28 INT C NEURAL, P379; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Liu C, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P3035, DOI 10.1145/3394486.3403354; Liu M., 2020, PROC INT C LEARN REP; Liu RS, 2019, AAAI CONF ARTIF INTE, P4368; Liu RS, 2019, IEEE T IMAGE PROCESS, V28, P5013, DOI 10.1109/TIP.2019.2913536; Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684; Long P. M., 2020, PROC INT C LEARN REP; Mani I., 2003, P WORKSH LEARN IMB D; Maurer A, 2016, LECT NOTES ARTIF INT, V9925, P3, DOI 10.1007/978-3-319-46379-7_1; McDiarmid C., 1998, CONCENTRATION, P195, DOI DOI 10.1007/978-3-662-12788-9_6; Mohri M., 2018, FDN MACHINE LEARNING; Mossman D, 1999, MED DECIS MAKING, V19, P78, DOI 10.1177/0272989X9901900110; Narasimhan H., 2013, PROC INT C MACH LEAR, P516; Narasimhan H, 2017, NEURAL COMPUT, V29, P1919, DOI 10.1162/NECO_a_00972; Narasimhan H, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P167; Natole M. A., 2019, FRONT APPL MATH STAT, V5, DOI [10.3389/fams.2019.00030, DOI 10.3389/FAMS.2019.00030]; Natole M, 2018, PR MACH LEARN RES, V80; Pan FY, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P695, DOI 10.1145/3331184.3331268; Provost F, 2003, MACH LEARN, V52, P199, DOI 10.1023/A:1024099825458; Rajaram S., 2005, PROC NIPS WORKSHOP L, P18; Ralaivola L, 2010, J MACH LEARN RES, V11, P1927; Shi WL, 2020, AAAI CONF ARTIF INTE, V34, P5734; Smith MR, 2014, MACH LEARN, V95, P225, DOI 10.1007/s10994-013-5422-z; TOMEK I, 1976, IEEE T SYST MAN CYB, V6, P769, DOI 10.1109/tsmc.1976.4309452; Uematsu K, 2015, IEEE T PATTERN ANAL, V37, P1080, DOI 10.1109/TPAMI.2014.2360397; Usunier N., 2005, PROC ICML WORKSHOP R; Usunier Nicolas, 2006, PROC 18 INT C ADV NE, P1369; Wang S., 2020, PROC INT JOINT C NEU, P1, DOI 10.1109/IJCNN48605.2020.9207377; Wu HZ, 2020, AAAI CONF ARTIF INTE, V34, P254; Yang B, 2009, 2009 ISECS INTERNATIONAL COLLOQUIUM ON COMPUTING, COMMUNICATION, CONTROL, AND MANAGEMENT, VOL II, P463, DOI 10.1109/CCCM.2009.5267469; Yang ZH, 2020, COMMUN PUR APPL ANAL, V19, P4191, DOI 10.3934/cpaa.2020188; Ying Yiming, 2016, ADV NEURAL INFORM PR, P451; Zhang XH, 2012, J MACH LEARN RES, V13, P3623; Zhao P., 2011, P 28 INT C MACHINE L, P233; Zhou K, 2020, I S BIOMED IMAGING, P1227, DOI 10.1109/ISBI45749.2020.9098374; Zou FY, 2019, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2019.01138	78	0	0	6	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7747	7763		10.1109/TPAMI.2021.3101125	http://dx.doi.org/10.1109/TPAMI.2021.3101125			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34329155	Green Submitted			2022-12-18	WOS:000864325900036
J	Zhang, JN; Zhang, JZ; Mao, S; Ji, MQ; Wang, GY; Chen, ZQ; Zhang, T; Yuan, XY; Dai, QH; Fang, L				Zhang, Jianing; Zhang, Jinzhi; Mao, Shi; Ji, Mengqi; Wang, Guangyu; Chen, Zequn; Zhang, Tian; Yuan, Xiaoyun; Dai, Qionghai; Fang, Lu			GigaMVS: A Benchmark for Ultra-Large-Scale Gigapixel-Level 3D Reconstruction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Multiview stereopsis benchmark; gigapixel-image dataset; large-scale scene reconstruction	MULTIVIEW STEREO; REGISTRATION	Multiview stereopsis (MVS) methods, which can reconstruct both the 3D geometry and texture from multiple images, have been rapidly developed and extensively investigated fromthe feature engineering methods to the data-driven ones. However, there is no dataset containing both the 3D geometry of large-scale scenes and high-resolution observations of small details to benchmark the algorithms. To this end, we present GigaMVS, the first gigapixel-image-based 3D reconstruction benchmark for ultra-large-scale scenes. The gigapixel images, with both wide field-of-view and high-resolution details, can clearly observe both the Palace-scale scene structure and Relievo-scale local details. The ground-truth geometry is captured by the laser scanner, which covers ultra-large-scale scenes with an average area of 8667m(2) and a maximumarea of 32007m(2). Owing to the extremely large scale, complex occlusion, and gigapixel-level images, GigaMVSexposes problems that emerge from the poor scalability and efficiency of the existing MVSalgorithms. We thoroughly investigate the state-of-the-art methods in terms of geometric and textural measurements, which point to the weakness of the existing methods and promising opportunities for future works. We believe that GigaMVS can benefit the community of 3D reconstruction and support the development of novel algorithms balancing robustness, scalability and accuracy.	[Zhang, Jianing; Zhang, Jinzhi; Mao, Shi; Wang, Guangyu; Chen, Zequn; Zhang, Tian; Yuan, Xiaoyun] Tsinghua Univ, Dept Elect Engn, Shenzhen Int Grad Sch, Beijing 100084, Peoples R China; [Ji, Mengqi] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China; [Dai, Qionghai] Tsinghua Univ, Dept Automat, Beijing Natl Res Ctr Informat Sci & Technol, Inst Brain & Cognit Sci, Beijing 100084, Peoples R China; [Fang, Lu] Tsinghua Univ, Dept Elect Engn, Beijing Natl Res Ctr Informat Sci & Technol, Inst Brain & Cognit Sci, Beijing 100084, Peoples R China	Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University	Fang, L (corresponding author), Tsinghua Univ, Dept Elect Engn, Beijing Natl Res Ctr Informat Sci & Technol, Inst Brain & Cognit Sci, Beijing 100084, Peoples R China.	jn-zhang19@mails.tsinghua.edu.cn; zhang-jz19@mails.tsinghua.edu.cn; maos19@mails.tsinghua.edu.cn; mji@tsinghua.edu.cn; wanggy21@mails.tsinghua.edu.cn; czq21@mails.tsinghua.edu.cn; zhangtia20@mails.tsinghua.edu.cn; xiaoyunyuan@tsinghua.edu.cn; daiqionghai@tsinghua.edu.cn; funglu@tsinghua.edu.cn		Mao, Shi/0000-0001-9275-4632; Yuan, Xiaoyun/0000-0002-7914-3658	Natural Science Foundation of China (NSFC) [61860206003, 62088102]; Shenzhen Science and Technology Research and Development Funds [JCYJ20180507183706645]; Beijing National Research Center for Information Science and Technology (BNRist) [BNR2020RC01002]; China Postdoctoral Science Foundation [2020TQ0172, 2020M670338]	Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Shenzhen Science and Technology Research and Development Funds; Beijing National Research Center for Information Science and Technology (BNRist); China Postdoctoral Science Foundation(China Postdoctoral Science Foundation)	This work was supported in part by Natural Science Foundation of China (NSFC) under Grants 61860206003 and 62088102, in part by Shenzhen Science and Technology Research and Development Funds under Grant JCYJ20180507183706645, in part by Beijing National Research Center for Information Science and Technology (BNRist) under Grant No. BNR2020RC01002, and in part by China Postdoctoral Science Foundation under Grants 2020TQ0172 and 2020M670338.	Aanaes H, 2016, INT J COMPUT VISION, V120, P153, DOI 10.1007/s11263-016-0902-9; Bane C, 2017, INT CONF 3D VISION, P412, DOI 10.1109/3DV.2017.00054; Brady DJ, 2012, NATURE, V486, P386, DOI 10.1038/nature11150; Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3; Cernea Dan, 2020, OPENMVS MULTIVIEW ST; Chen R, 2019, IEEE I CONF COMP VIS, P1538, DOI 10.1109/ICCV.2019.00162; CHEN Y, 1992, IMAGE VISION COMPUT, V10, P145, DOI 10.1016/0262-8856(92)90066-C; Chen YB, 2021, PROC CVPR IEEE, P8624, DOI 10.1109/CVPR46437.2021.00852; Cheng S, 2020, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR42600.2020.00260; Choi S, 2015, PROC CVPR IEEE, P5556, DOI 10.1109/CVPR.2015.7299195; Christoph Mertz, 2019, Arxiv, DOI arXiv:1905.02706; Corsini M, 2009, COMPUT GRAPH FORUM, V28, P1755, DOI 10.1111/j.1467-8659.2009.01552.x; Dai YC, 2019, INT CONF 3D VISION, P1, DOI 10.1109/3DV.2019.00010; Duan LY, 2016, LECT NOTES COMPUT SC, V9909, P89, DOI 10.1007/978-3-319-46454-1_6; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Furukawa Y, 2013, FOUND TRENDS COMPUT, V9, P1, DOI 10.1561/0600000052; Galliani S, 2016, PROC CVPR IEEE, P5479, DOI 10.1109/CVPR.2016.591; Galliani S, 2015, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2015.106; Gu XD, 2020, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR42600.2020.00257; Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948; Hongwei Yi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P766, DOI 10.1007/978-3-030-58545-7_44; Huang BC, 2021, IEEE IMAGE PROC, P3163, DOI 10.1109/ICIP42928.2021.9506469; Jancosek M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3121, DOI 10.1109/CVPR.2011.5995693; Ji MQ, 2021, IEEE T PATTERN ANAL, V43, P4078, DOI 10.1109/TPAMI.2020.2996798; Ji MQ, 2017, IEEE I CONF COMP VIS, P2326, DOI 10.1109/ICCV.2017.253; Kazhdan Michael, 2006, P EUR S GEOM PROC, V7, P2; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599; Knobelreiter P, 2017, PROC CVPR IEEE, P1456, DOI 10.1109/CVPR.2017.159; Kopf J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239544; Lempitsky V., 2007, PROC IEEE C COMPUT V, P1, DOI [10.1109/CVPR.2007.383293, DOI 10.1109/CVPR.2007.383293]; Lhuillier M, 2005, IEEE T PATTERN ANAL, V27, P418, DOI 10.1109/TPAMI.2005.44; Lhuillier M, 2002, LECT NOTES COMPUT SC, V2351, P125; Liu C, 2019, PROC CVPR IEEE, P4445, DOI 10.1109/CVPR.2019.00458; Liu C, 2018, PROC CVPR IEEE, P2579, DOI 10.1109/CVPR.2018.00273; Martel JNP, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459785; Merrell P, 2007, IEEE I CONF COMP VIS, P1221; Merrell P, 2007, IEEE I CONF COMP VIS, P3012, DOI 10.1109/iccv.2007.4408984; Riegler G, 2017, INT CONF 3D VISION, P57, DOI 10.1109/3DV.2017.00017; Rusu RB, 2008, ROBOT AUTON SYST, V56, P927, DOI 10.1016/j.robot.2008.08.005; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Schops T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272; Seitz S.M., 2006, P IEEE COMPUTER SOC, P519; Seki A, 2017, PROC CVPR IEEE, P6640, DOI 10.1109/CVPR.2017.703; Strecha C, 2008, PROC CVPR IEEE, P2838; Takikawa T, 2021, PROC CVPR IEEE, P11353, DOI 10.1109/CVPR46437.2021.01120; Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230; Tola E, 2012, MACH VISION APPL, V23, P903, DOI 10.1007/s00138-011-0346-8; Waechter M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2999533; Wang XY, 2020, PROC CVPR IEEE, P3265, DOI 10.1109/CVPR42600.2020.00333; Wu TP, 2010, PROC CVPR IEEE, P1482, DOI 10.1109/CVPR.2010.5539796; Xiaoxiao Long, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P640, DOI 10.1007/978-3-030-58545-7_37; Yang GS, 2019, PROC CVPR IEEE, P5510, DOI 10.1109/CVPR.2019.00566; Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47; Yao Y, 2020, PROC CVPR IEEE, P1787, DOI 10.1109/CVPR42600.2020.00186; Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567; Yuan XY, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON SMART COMPUTING (SMARTCOMP), P9, DOI 10.1109/smartcomp.2017.7946998; Yuan XY, 2021, LIGHT-SCI APPL, V10, DOI 10.1038/s41377-021-00485-x; Zach C., 2008, P INT S 3D DAT PROC, V1; Zhang JN, 2020, IEEE INT CONF COMPUT; Zhou YC, 2019, IEEE I CONF COMP VIS, P7697, DOI 10.1109/ICCV.2019.00779; Zienkiewicz J, 2016, INT CONF 3D VISION, P37, DOI 10.1109/3DV.2016.82	63	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7534	7550		10.1109/TPAMI.2021.3115028	http://dx.doi.org/10.1109/TPAMI.2021.3115028			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34559635				2022-12-18	WOS:000864325900022
J	Zhang, JZ; Ji, MQ; Wang, GY; Xue, ZW; Wang, SJ; Fang, L				Zhang, Jinzhi; Ji, Mengqi; Wang, Guangyu; Xue, Zhiwei; Wang, Shengjin; Fang, Lu			SurRF: Unsupervised Multi-View Stereopsis by Learning Surface Radiance Field	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Surface reconstruction; Surface texture; Geometry; Image reconstruction; Shape; Rendering (computer graphics); Multi-view stereopsis; unsupervised learning; neural rendering		The recent success in supervised multi-view stereopsis (MVS) relies on the onerously collected real-world 3D data. While the latest differentiable rendering techniques enable unsupervised MVS, they are restricted to discretized (e.g., point cloud) or implicit geometric representation, suffering from either low integrity for a textureless region or less geometric details for complex scenes. In this paper, we propose SurRF, an unsupervised MVS pipeline by learning Surface Radiance Field, i.e., a radiance field defined on a continuous and explicit 2D surface. Our key insight is that, in a local region, the explicit surface can be gradually deformed from a continuous initialization along view-dependent camera rays by differentiable rendering. That enables us to define the radiance field only on a 2D deformable surface rather than in a dense volume of 3D space, leading to compact representation while maintaining complete shape and realistic texture for large-scale complex scenes. We experimentally demonstrate that the proposed SurRF produces competitive results over the-state-of-the-art on various real-world challenging scenes, without any 3D supervision. Moreover, SurRF shows great potential in owning the joint advantages of mesh (scene manipulation), continuous surface (high geometric resolution), and radiance field (realistic rendering).	[Zhang, Jinzhi; Wang, Guangyu; Xue, Zhiwei; Wang, Shengjin; Fang, Lu] Tsinghua Univ, Dept Elect Engn, Beijing, Peoples R China; [Zhang, Jinzhi; Wang, Guangyu; Xue, Zhiwei] Tsinghua Univ, Shenzhen Int Grad Sch, Beijing 100084, Peoples R China; [Ji, Mengqi] Tsinghua Univ, Dept Automat, Beijing, Peoples R China; [Fang, Lu] Beijing Natl Res Ctr Informat Sci & Technol, Beijing 100084, Peoples R China; [Fang, Lu] Tsinghua Univ THUIBCS, Inst Brain & Cognit Sci, Beijing 100084, Peoples R China	Tsinghua University; Tsinghua University; Tsinghua University	Fang, L (corresponding author), Tsinghua Univ, Dept Elect Engn, Beijing, Peoples R China.	zhang-jz19@tsinghua.org.cn; mji@tsinghua.edu.cn; 2017020907011@std.uestc.edu.cn; xuezw17@mails.tsinghua.edu.cn; wgsgj@tsinghua.edu.cn; fanglu@tsinghua.edu.cn			Natural Science Foundation of China (NSFC) [61860206003, 62088102]; Shenzhen Science and Technology Research and Development Funds [JCYJ20180507183706645]; Beijing National Research Center for Information Science and Technology (BNRist) [BNR2019TD01022, BNR2020RC01002]; China Postdoctoral Science Foundation [2020TQ0172, 2020M670338]	Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Shenzhen Science and Technology Research and Development Funds; Beijing National Research Center for Information Science and Technology (BNRist); China Postdoctoral Science Foundation(China Postdoctoral Science Foundation)	This work was supported in part by Natural Science Foundation of China (NSFC) under Grants 61860206003 and 62088102, in part by Shenzhen Science and Technology Research and Development Funds under Grant JCYJ20180507183706645, in part by Beijing National Research Center for Information Science and Technology (BNRist) under Grants BNR2019TD01022 and BNR2020RC01002, in part by China Postdoctoral Science Foundation under Grants 2020TQ0172 and 2020M670338.	Aanaes H, 2016, INT J COMPUT VISION, V120, P153, DOI 10.1007/s11263-016-0902-9; Bane C, 2017, INT CONF 3D VISION, P412, DOI 10.1109/3DV.2017.00054; Bernardini F, 1999, IEEE T VIS COMPUT GR, V5, P349, DOI 10.1109/2945.817351; Campbell NDF, 2008, LECT NOTES COMPUT SC, V5302, P766, DOI 10.1007/978-3-540-88682-2_58; Chabra Rohan, 2020, P EUR C COMP VIS ECC, P608, DOI DOI 10.1007/978-3-030-58526-6_36; Chen AP, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203192; Chen R, 2021, IEEE T PATTERN ANAL, V43, P3695, DOI 10.1109/TPAMI.2020.2988729; Chen R, 2019, IEEE I CONF COMP VIS, P1538, DOI 10.1109/ICCV.2019.00162; Cheng S, 2020, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR42600.2020.00260; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Christoph Mertz, 2019, Arxiv, DOI arXiv:1905.02706; Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269; Dai YC, 2019, INT CONF 3D VISION, P1, DOI 10.1109/3DV.2019.00010; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Galliani S, 2016, PROC CVPR IEEE, P5479, DOI 10.1109/CVPR.2016.591; Galliani S, 2015, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2015.106; Genova K, 2020, PROC CVPR IEEE, P4856, DOI 10.1109/CVPR42600.2020.00491; Goesele M, 2007, IEEE I CONF COMP VIS, P825, DOI 10.1109/iccv.2007.4408933; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Gu XD, 2020, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR42600.2020.00257; Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948; Henderson P, 2020, INT J COMPUT VISION, V128, P835, DOI 10.1007/s11263-019-01219-8; Hongwei Yi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P766, DOI 10.1007/978-3-030-58545-7_44; Huang BC, 2021, IEEE IMAGE PROC, P3163, DOI 10.1109/ICIP42928.2021.9506469; Immel D. S., 1986, Computer Graphics, V20, P133, DOI 10.1145/15886.15901; Jancosek M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3121, DOI 10.1109/CVPR.2011.5995693; Ji MQ, 2021, IEEE T PATTERN ANAL, V43, P4078, DOI 10.1109/TPAMI.2020.2996798; Ji MQ, 2017, IEEE I CONF COMP VIS, P2326, DOI 10.1109/ICCV.2017.253; Jianfeng Yan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P674, DOI 10.1007/978-3-030-58548-8_39; Jiang CY, 2020, PROC CVPR IEEE, P6000, DOI 10.1109/CVPR42600.2020.00604; Kajiya J.T., 1986, SIGGRAPH, P143, DOI [DOI 10.1145/15922.15902, 10.1145/15886.15902, DOI 10.1145/15886.15902]; Kanazawa A, 2018, LECT NOTES COMPUT SC, V11219, P386, DOI 10.1007/978-3-030-01267-0_23; Kar A, 2017, ADV NEUR IN, V30; Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411; Kazhdan Michael, 2006, P EUR S GEOM PROC, V7, P2; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kingma D. P, 2015, CORR; Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599; Knobelreiter P, 2017, PROC CVPR IEEE, P1456, DOI 10.1109/CVPR.2017.159; Kutulakos K. N., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P307, DOI 10.1109/ICCV.1999.791235; Ladicky L, 2017, IEEE I CONF COMP VIS, P3913, DOI 10.1109/ICCV.2017.420; Lempitsky V., 2007, PROC IEEE C COMPUT V, P1, DOI [10.1109/CVPR.2007.383293, DOI 10.1109/CVPR.2007.383293]; Lhuillier M, 2005, IEEE T PATTERN ANAL, V27, P418, DOI 10.1109/TPAMI.2005.44; Lin CH, 2019, PROC CVPR IEEE, P969, DOI 10.1109/CVPR.2019.00106; Liu Aishan, 2020, ECCV; Liu L., 2020, ADV NEURAL INF PROCE, V33; Liu SC, 2019, ADV NEUR IN, V32; Liu SC, 2019, IEEE I CONF COMP VIS, P7707, DOI 10.1109/ICCV.2019.00780; Martin-Brualla R, 2021, PROC CVPR IEEE, P7206, DOI 10.1109/CVPR46437.2021.00713; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Merrell P, 2007, IEEE I CONF COMP VIS, P1221; Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459; Mildenhall Ben, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P405, DOI 10.1007/978-3-030-58452-8_24; Niemeyer M, 2020, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR42600.2020.00356; Oechsle M, 2019, IEEE I CONF COMP VIS, P4530, DOI 10.1109/ICCV.2019.00463; Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Riegler G, 2017, INT CONF 3D VISION, P57, DOI 10.1109/3DV.2017.00017; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Schops T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272; Seki A, 2017, PROC CVPR IEEE, P6640, DOI 10.1109/CVPR.2017.703; Sinha A, 2017, PROC CVPR IEEE, P791, DOI 10.1109/CVPR.2017.91; Sinha A, 2016, LECT NOTES COMPUT SC, V9910, P223, DOI 10.1007/978-3-319-46466-4_14; Sitzmann V, 2019, ADV NEUR IN, V32; Sitzmann Vincent, 2020, ARXIV200609661, V33, P7462; Song SR, 2016, PROC CVPR IEEE, P808, DOI 10.1109/CVPR.2016.94; Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230; Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20; Tola E, 2012, MACH VISION APPL, V23, P903, DOI 10.1007/s00138-011-0346-8; Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4; Wood DN, 2000, COMP GRAPH, P287, DOI 10.1145/344779.344925; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47; Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567; Yariv L., 2020, NEURIPS; Zach C., 2008, P INT S 3D DAT PROC, V1; Zhang J, 2020, BRIT MACH VIS C	79	0	0	4	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7912	7927		10.1109/TPAMI.2021.3116695	http://dx.doi.org/10.1109/TPAMI.2021.3116695			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34591757				2022-12-18	WOS:000864325900047
J	Zhang, ZW; Su, C; Zheng, L; Xie, XD; Li, Y				Zhang, Ziwei; Su, Chi; Zheng, Liang; Xie, Xiaodong; Li, Yuan			On the Correlation Among Edge, Pose and Parsing	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Image edge detection; Correlation; Pose estimation; Feature extraction; Semantics; Fuses; Correlation machine; heterogeneous non-local; human parsing; pose estimation; human body edge detection	TEXTURE; COLOR	Semantic parsing, edge detection, and pose estimation of human are three closely-related tasks. They present human characteristics from three complementary aspects. Compared to learning them individually, solving these tasks jointly can explore the interaction of their contextual cues. However, prior works usually study the fusion of two of them, e.g., parsing and pose, parsing and edge. In this paper, we explore how pixel-level semantics, human boundaries and joint locations can be effectively learned in a unified model. Specifically, we propose an end-to-end trainable Human Task Correlation Machine (HTCorrM) to implement the three tasks. It is asymmetric in that it supports a main task using the other two as auxiliary tasks. We also introduce a Heterogeneous Non-Local module (HNL) to discover the correlations of the three heterogeneous domains. HNL fully explores the global dependency among tasks between any two positions in the feature map. Experimental results on human parsing, pose estimation and body edge detection demonstrate that HTCorrM achieves competitive performance. We show that when designated as the main task, the accuracy of each of the three tasks is improved. Importantly, comparative studies confirm the advantages of our proposed feature correlation strategy over feature concatenation or post processing.	[Zhang, Ziwei; Xie, Xiaodong; Li, Yuan] Peking Univ, Dept EECS, Beijing 100871, Peoples R China; [Su, Chi] Kingsoft Cloud, Beijing 100085, Peoples R China; [Zheng, Liang] Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT 0200, Australia	Peking University; Australian National University	Li, Y (corresponding author), Peking Univ, Dept EECS, Beijing 100871, Peoples R China.	ziwei.zh@pku.edu.cn; suchi@kingsoft.com; liangzheng06@gmail.com; donxie@pku.edu.cn; yuanli@pku.edu.cn			BeijingMajor Science and Technology Project [Z191100010618003]; PKU-Baidu Fund [2019BD004]; Beijing Nova Program [Z201100006820023]; ARC Discovery Early Career Researcher Award [DE200101283]; ARC Discovery Project [DP210102801]	BeijingMajor Science and Technology Project; PKU-Baidu Fund; Beijing Nova Program(Beijing Municipal Science & Technology Commission); ARC Discovery Early Career Researcher Award(Australian Research Council); ARC Discovery Project(Australian Research Council)	This work was supported in part by the BeijingMajor Science and Technology Project under Grant Z191100010618003, in part by PKU-Baidu Fund under Grant 2019BD004, and in part by the Beijing Nova Program under Grant Z201100006820023. The work of Liang Zheng was supported in part by an ARC Discovery Early Career Researcher Award under Grant DE200101283 and in part by an ARC Discovery Project under Grant DP210102801.	Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Augustus Odena, 2019, Arxiv, DOI arXiv:1804.09170; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Cewu Lu, 2018, Arxiv, DOI arXiv:1805.04310; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LC, 2016, PROC CVPR IEEE, P4545, DOI 10.1109/CVPR.2016.492; Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396; Chiew-lan Tai, 2020, Arxiv, DOI arXiv:2007.06888; Congyan Lang, 2018, Arxiv, DOI arXiv:1705.07206; Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343; Daniil Osokin, 2019, Arxiv, DOI arXiv:1906.04104; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Dong J, 2014, PROC CVPR IEEE, P843, DOI 10.1109/CVPR.2014.113; Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47; Gong K, 2019, PROC CVPR IEEE, P7442, DOI 10.1109/CVPR.2019.00763; Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715; He JZ, 2022, IEEE T PATTERN ANAL, V44, P100, DOI 10.1109/TPAMI.2020.3007074; He K., 2017, IEEE INT C COMP VIS, P2961; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069; Kittler J, 1983, IMAGE VISION COMPUT, V1, P37, DOI DOI 10.1016/0262-8856(83)90006-9; Ladicky L, 2013, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2013.459; Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063; Liang XD, 2015, IEEE I CONF COMP VIS, P1386, DOI 10.1109/ICCV.2015.163; Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu XC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P338, DOI 10.1145/3343031.3350857; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Yang L, 2021, Arxiv, DOI arXiv:2103.05997; Lu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P421, DOI 10.1007/978-3-030-58610-2_25; Lu YY, 2014, NEUROCOMPUTING, V126, P132, DOI 10.1016/j.neucom.2012.08.071; Luo XH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P654, DOI 10.1145/3240508.3240634; Luo YW, 2018, LECT NOTES COMPUT SC, V11213, P424, DOI 10.1007/978-3-030-01240-3_26; Ma C, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P253, DOI 10.1145/3323873.3325010; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Newell A, 2017, ADV NEUR IN, V30; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Nie XC, 2018, LECT NOTES COMPUT SC, V11209, P519, DOI 10.1007/978-3-030-01228-1_31; Nie XC, 2018, PROC CVPR IEEE, P2100, DOI 10.1109/CVPR.2018.00224; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395; Peng X, 2016, LECT NOTES COMPUT SC, V9905, P38, DOI 10.1007/978-3-319-46448-0_3; Ronneberger O., 2015, P INT C MED IM COMP; Ruan T, 2019, AAAI CONF ARTIF INTE, P4814; Ruyi Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P205, DOI 10.1007/978-3-030-58601-0_13; Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584; Tang W, 2018, LECT NOTES COMPUT SC, V11207, P197, DOI 10.1007/978-3-030-01219-9_12; Tao Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9260, DOI 10.1109/CVPR42600.2020.00928; Vaswani A, 2017, ADV NEUR IN, V30; Wang CY, 2013, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2013.123; Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686; Wang WG, 2019, IEEE I CONF COMP VIS, P5702, DOI 10.1109/ICCV.2019.00580; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wang Y, 2012, J MACH LEARN RES, V13, P3075; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Wenguan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8926, DOI 10.1109/CVPR42600.2020.00895; Xia FT, 2017, PROC CVPR IEEE, P6080, DOI 10.1109/CVPR.2017.644; Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29; Xiaomei Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8968, DOI 10.1109/CVPR42600.2020.00899; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Yamaguchi K, 2013, IEEE I CONF COMP VIS, P3519, DOI 10.1109/ICCV.2013.437; Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101; Yang L, 2019, PROC CVPR IEEE, P364, DOI 10.1109/CVPR.2019.00045; Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144; Yoon JH, 2016, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR.2016.155; Yu ZD, 2017, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2017.191; Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhao J, 2017, IEEE COMPUT SOC CONF, P1595, DOI 10.1109/CVPRW.2017.204; Zheng Z., 2019, AAAI CONF ARTIF INTE; Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068; Zhuang YQ, 2018, IEEE IMAGE PROC, P3698, DOI 10.1109/ICIP.2018.8451830; Ziwei Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8897, DOI 10.1109/CVPR42600.2020.00892	78	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					8492	8507		10.1109/TPAMI.2021.3108771	http://dx.doi.org/10.1109/TPAMI.2021.3108771			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34469290				2022-12-18	WOS:000864325900085
J	Zhao, W; Xu, C; Guan, ZY; Wu, XL; Zhao, WQ; Miao, QG; He, XF; Wang, Q				Zhao, Wei; Xu, Cai; Guan, Ziyu; Wu, Xunlian; Zhao, Wanqing; Miao, Qiguang; He, Xiaofei; Wang, Quan			TelecomNet: Tag-Based Weakly-Supervised Modally Cooperative Hashing Network for Image Retrieval	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Semantics; Training; Tagging; Image retrieval; Correlation; Training data; Binary codes; Image retrieval; multimedia retrieval; multi-modal learning; weakly-supervised learning	COMPLETION	We are concerned with using user-tagged images to learn proper hashing functions for image retrieval. The benefits are two-fold: (1) we could obtain abundant training data for deep hashing models; (2) tagging data possesses richer semantic information which could help better characterize similarity relationships between images. However, tagging data suffers from noises, vagueness and incompleteness. Different from previous unsupervised or supervised hashing learning, we propose a novel weakly-supervised deep hashing framework which consists of two stages: weakly-supervised pre-training and supervised fine-tuning. The second stage is as usual. In the first stage, we propose two formulations Tag-basEd weakLy-supErvised Modally COoperative hashing Network (TelecomNet) and Generalized TelecomNet (GTelecomNet). Rather than performing supervision on tags, TelecomNet first learns an observed semantic embedding vector for each image from attached tags and then uses it to guide hashing learning. GTelecomNet introduces a novel semantic network to exploit more precise semantic information. By carefully designing the optimization problem, they can well leverage tagging information and image content for hashing learning. The framework is general and does not depend on specific deep hashing methods. Empirical results on real world datasets show that they significantly increase the performance of state-of-the-art deep hashing methods.	[Zhao, Wei; Xu, Cai; Guan, Ziyu; Wu, Xunlian] Xidian Univ, Sch Comp Sci & Technol, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China; [Zhao, Wanqing] Northwestern Univ, Sch Informat & Technol, Xian 710127, Shaanxi, Peoples R China; [Miao, Qiguang; Wang, Quan] Xidian Univ, Sch Comp Sci & Technol, Xian 710071, Shaanxi, Peoples R China; [He, Xiaofei] Zhejiang Univ, Coll Comp Sci, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China	Xidian University; Northwest University Xi'an; Xidian University; Zhejiang University	Guan, ZY (corresponding author), Xidian Univ, Sch Comp Sci & Technol, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China.	ywzhao@mail.xidian.edu.cn; cxu@xidian.edu.cn; zyguan@xidian.edu.cn; xunlianwu@stu.xidian.edu.cn; zhaowq@nwu.edu.cn; qgmiao@xidian.edu.cn; xiaofeihe@cad.zju.edu.cn; qwang@xidian.edu.cn	Zhao, Wanqing/ADL-9932-2022	xu, cai/0000-0002-7191-7348; Wang, Quan/0000-0001-6913-8604; Zhao, Wanqing/0000-0001-7622-0665	National Natural Science Foundation of China [62133012, 61936006, 62103314, 61876144, 61876145, 62073255]; Key Research and Development Program of Shaanxi Program [2020ZDLGY04-07]; Innovation Capability Support Program of Shaanxi Program [2021TD-05]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Key Research and Development Program of Shaanxi Program; Innovation Capability Support Program of Shaanxi Program	This work was supported in part by the National Natural Science Foundation of China under Grants 62133012, 61936006, 62103314, 61876144, 61876145, and 62073255, in part by the Key Research and Development Program of Shaanxi Programunder Grant 2020ZDLGY04-07, and in part by the Innovation Capability Support Program of Shaanxi Program underGrant 2021TD-05.	Aizawa A, 2003, INFORM PROCESS MANAG, V39, P45, DOI 10.1016/S0306-4573(02)00021-3; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134; Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598; Chen YD, 2019, IEEE I CONF COMP VIS, P9795, DOI 10.1109/ICCV.2019.00989; Chua T.-S., 2009, P ACM INT C IM VID R, P1, DOI 10.1145/1646396.1646452; Donoho DL, 2008, IEEE T INFORM THEORY, V54, P4789, DOI 10.1109/TIT.2008.929958; Frenay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894; Gattupalli V, 2019, PROC CVPR IEEE, P10367, DOI 10.1109/CVPR.2019.01062; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; Gu SM, 2013, INT CONF MACH LEARN, P108, DOI 10.1109/ICMLC.2013.6890453; Guan ZY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3776; Han B., 2020, ARXIV; Huiskes M. J., 2010, P INT C MULT INF RET, P527, DOI DOI 10.1145/1743384.1743475; Irie G, 2014, PROC CVPR IEEE, P2123, DOI 10.1109/CVPR.2014.272; Jiang QY, 2018, AAAI CONF ARTIF INTE, P3342; Jin ZM, 2014, IEEE T CYBERNETICS, V44, P2167, DOI 10.1109/TCYB.2014.2302018; Jin ZM, 2014, IEEE T CYBERNETICS, V44, P1362, DOI 10.1109/TCYB.2013.2283497; Kulis Brian, 2009, ADV NEURAL INFORM PR, P1042; Li W., 2016, INT JOINT C ARTIFICI, P1711; Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0; Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750; Li ZC, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2624140; Lin CJ, 2007, NEURAL COMPUT, V19, P2756, DOI 10.1162/neco.2007.19.10.2756; Lin KV, 2016, PROC CVPR IEEE, P1183, DOI 10.1109/CVPR.2016.133; Liu D., 2009, P 18 INT C WORLD WID, P351; Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227; Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899; Liu W., 2012, ARXIV; Liu W., 2014, P ADV NEUR INF PROC, P3419; Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2; Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912; Liu W, 2011, SER INF MANAGE SCI, V10, P1; Lu J, 2019, IEEE I CONF COMP VIS, P7960, DOI 10.1109/ICCV.2019.00805; Mikolov T., 2013, WORKSHOP TRACK P; Niu YL, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3749; Qian XM, 2014, IEEE T CYBERNETICS, V44, P2493, DOI 10.1109/TCYB.2014.2309593; Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598; Shen YM, 2020, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR42600.2020.00289; Song D., 2016, PROC INT JOINT C ART, P2018; Song DJ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2229, DOI 10.1145/3219819.3220108; Song DJ, 2015, IEEE I CONF COMP VIS, P1922, DOI 10.1109/ICCV.2015.223; Sun A., 2011, PROC ACM INT C MULTI, P1181; Tang J., 2009, PROC ACM INT C MULTI, P223, DOI DOI 10.1145/1631272.1631305; Tang JH, 2019, IEEE T PATTERN ANAL, V41, P2027, DOI 10.1109/TPAMI.2019.2906603; Tang JH, 2018, IEEE T CIRC SYST VID, V28, P2730, DOI 10.1109/TCSVT.2017.2715227; Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976; Weiss Y, 2009, ADV NEURAL INFORM PR, P1753; Wu L, 2013, IEEE T PATTERN ANAL, V35, P716, DOI 10.1109/TPAMI.2012.124; Xia X., 2020, ADV NEURAL INFORM PR, V33, P7597; Xia ZQ, 2015, NEUROCOMPUTING, V147, P500, DOI 10.1016/j.neucom.2014.06.028; Yan XY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3238; Yang EK, 2019, PROC CVPR IEEE, P2941, DOI 10.1109/CVPR.2019.00306; Yang EK, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1064; Yuan L, 2020, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR42600.2020.00315; Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315; Zhao WQ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3504; Zhuang J., 2011, PROC INT C WEB SEARC, P625	58	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7940	7954		10.1109/TPAMI.2021.3114089	http://dx.doi.org/10.1109/TPAMI.2021.3114089			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34546917				2022-12-18	WOS:000864325900049
J	Zhu, H; Zuo, XX; Yang, HT; Wang, S; Cao, X; Yang, RG				Zhu, Hao; Zuo, Xinxin; Yang, Haotian; Wang, Sen; Cao, Xun; Yang, Ruigang			Detailed Avatar Recovery From Single Image	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Shape; Three-dimensional displays; Predictive models; Strain; Biological system modeling; Avatars; Solid modeling; Human avatar; 3D reconstruction; texture completion; deep neural network	SHAPE	This paper presents a novel framework to recover detailed avatar from a single image. It is a challenging task due to factors such as variations in human shapes, body poses, texture, and viewpoints. Prior methods typically attempt to recover the human body shape using a parametric-based template that lacks the surface details. As such resulting body shape appears to be without clothing. In this paper, we propose a novel learning-based framework that combines the robustness of the parametric model with the flexibility of free-form 3D deformation. We use the deep neural networks to refine the 3D shape in a Hierarchical Mesh Deformation (HMD) framework, utilizing the constraints from body joints, silhouettes, and per-pixel shading information. Our method can restore detailed human body shapes with complete textures beyond skinned models. Experiments demonstrate that our method has outperformed previous state-of-the-art approaches, achieving better accuracy in terms of both 2D IoU number and 3D metric distance.	[Zhu, Hao; Yang, Haotian; Cao, Xun] Nanjing Univ, Nanjing 210023, Jiangsu, Peoples R China; [Zhu, Hao; Zuo, Xinxin; Wang, Sen; Yang, Ruigang] Univ Kentucky, Lexington, KY 40508 USA; [Zuo, Xinxin] Northwestern Polytech Univ, Xian 710072, Peoples R China; [Zuo, Xinxin] Univ Alberta, Edmonton, AB T6G 2R3, Canada; [Yang, Ruigang] Inceptio Technol, Fremont, CA 94538 USA	Nanjing University; University of Kentucky; Northwestern Polytechnical University; University of Alberta	Cao, X (corresponding author), Nanjing Univ, Nanjing 210023, Jiangsu, Peoples R China.	zhuhaoese@nju.edu.cn; xinxinzuo2353@gmail.com; yanght321@gmail.com; wangsen1312@gmail.com; caoxun@nju.edu.cn; ryang2@uky.edu		Wang, Sen/0000-0002-1808-5239; Zuo, Xinxin/0000-0002-7116-9634; Zhu, Hao/0000-0003-1596-4366	NSFC [62025108, 62001213, 61627804]; USDA [2018-67021-27416]; NSERC Discovery Grant	NSFC(National Natural Science Foundation of China (NSFC)); USDA(United States Department of Agriculture (USDA)); NSERC Discovery Grant(Natural Sciences and Engineering Research Council of Canada (NSERC))	This work was supported in part by the NSFC under Grants 62025108, 62001213, 61627804, in part by USDA under Grant 2018-67021-27416, and in part by NSERC Discovery Grant.	Alldieck T, 2019, IEEE I CONF COMP VIS, P2293, DOI 10.1109/ICCV.2019.00238; Alldieck T, 2019, PROC CVPR IEEE, P1175, DOI 10.1109/CVPR.2019.00127; Alldieck T, 2018, PROC CVPR IEEE, P8387, DOI 10.1109/CVPR.2018.00875; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207; [Anonymous], 2021, RENDERPEOPLE DATASET; [Anonymous], 2021, TWINDOM DATASET; Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206; Bhatnagar BL, 2019, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2019.00552; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Chen Change Loy, 2019, Arxiv, DOI arXiv:1903.11326; Dibra E, 2017, PROC CVPR IEEE, P5504, DOI 10.1109/CVPR.2017.584; Dibra E, 2016, INT CONF 3D VISION, P108, DOI 10.1109/3DV.2016.19; Dosovitskiy Alexey, 2016, NEURIPS; Furukawa Y, 2013, FOUND TRENDS COMPUT, V9, P1, DOI 10.1561/0600000052; Guan P, 2009, IEEE I CONF COMP VIS, P1381, DOI 10.1109/iccv.2009.5459300; Guler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762; Guler RA, 2017, PROC CVPR IEEE, P2614, DOI 10.1109/CVPR.2017.280; Habermann M, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3311970; Hasler N, 2010, PROC CVPR IEEE, P1823, DOI 10.1109/CVPR.2010.5539853; Huang Z, 2020, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR42600.2020.00316; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Ionescu C, 2011, IEEE I CONF COMP VIS, P2220, DOI 10.1109/ICCV.2011.6126500; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jaderberg M, 2015, ADV NEUR IN, V28; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Johnson S, 2011, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR.2011.5995318; Johnson Sam, 2010, BMVC, DOI [10.5244/C.24.12, DOI 10.5244/C.24.12]; Kanazawa A, 2019, PROC CVPR IEEE, P5597, DOI 10.1109/CVPR.2019.00576; Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463; Kulkarni TD, 2015, PROC CVPR IEEE, P4390, DOI 10.1109/CVPR.2015.7299068; Lassner C, 2017, PROC CVPR IEEE, P4704, DOI 10.1109/CVPR.2017.500; Lazova V, 2019, INT CONF 3D VISION, P643, DOI 10.1109/3DV.2019.00076; Li Ruilong, 2020, ARXIV200713988, P2; Liao M, 2009, IEEE I CONF COMP VIS, P167, DOI 10.1109/ICCV.2009.5459161; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Masanori Koyama, 2018, Arxiv, DOI arXiv:1802.05957; Mir A, 2020, PROC CVPR IEEE, P7021, DOI 10.1109/CVPR42600.2020.00705; Natsume R, 2019, PROC CVPR IEEE, P4475, DOI 10.1109/CVPR.2019.00461; Neverova N, 2018, LECT NOTES COMPUT SC, V11207, P128, DOI 10.1007/978-3-030-01219-9_8; Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062; Or-El R, 2015, PROC CVPR IEEE, P5407, DOI 10.1109/CVPR.2015.7299179; Park E, 2017, PROC CVPR IEEE, P702, DOI 10.1109/CVPR.2017.82; Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055; Plankers R, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P394, DOI 10.1109/ICCV.2001.937545; Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317; Rhodin H, 2016, LECT NOTES COMPUT SC, V9909, P509, DOI 10.1007/978-3-319-46454-1_31; Robertini N, 2016, INT CONF 3D VISION, P166, DOI 10.1109/3DV.2016.25; Ronneberger O., 2015, P INT C MED IM COMP; Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016; Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239; Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481; Sengupta S, 2018, PROC CVPR IEEE, P6296, DOI 10.1109/CVPR.2018.00659; Smith D, 2019, IEEE I CONF COMP VIS, P5329, DOI 10.1109/ICCV.2019.00543; Tan FT, 2020, PROC CVPR IEEE, P647, DOI 10.1109/CVPR42600.2020.00073; Tan Jun Kai Vince, 2017, BRIT MACH VIS C; Tang SC, 2019, IEEE I CONF COMP VIS, P7749, DOI 10.1109/ICCV.2019.00784; Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2; Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492; Vaswani A, 2017, ADV NEUR IN, V30; Venkat A., 2018, PROC BRIT MACH VIS C; Yan S, 2018, LECT NOTES COMPUT SC, V11214, P155, DOI 10.1007/978-3-030-01249-6_10; Yan ZY, 2018, LECT NOTES COMPUT SC, V11218, P3, DOI 10.1007/978-3-030-01264-9_1; Yu T, 2019, PROC CVPR IEEE, P5499, DOI 10.1109/CVPR.2019.00565; Yu T, 2018, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR.2018.00761; Zhang C, 2017, PROC CVPR IEEE, P5484, DOI 10.1109/CVPR.2017.582; Zheng ZR, 2022, IEEE T PATTERN ANAL, V44, P3170, DOI 10.1109/TPAMI.2021.3050505; Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009; Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18; Zhu H, 2019, PROC CVPR IEEE, P4486, DOI 10.1109/CVPR.2019.00462; Zhu H, 2018, PROC CVPR IEEE, P4450, DOI 10.1109/CVPR.2018.00468; Zhu H, 2017, IEEE T CIRC SYST VID, V27, P760, DOI 10.1109/TCSVT.2016.2596118; Zhu H, 2017, FRONT COMPUT SCI-CHI, V11, P175, DOI 10.1007/s11704-016-5520-8; Zollhofer M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601165; Zuo XX, 2017, IEEE I CONF COMP VIS, P3152, DOI 10.1109/ICCV.2017.340	81	0	0	6	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	NOV 1	2022	44	11					7363	7379		10.1109/TPAMI.2021.3102128	http://dx.doi.org/10.1109/TPAMI.2021.3102128			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	5C5UY	34347594	Green Submitted			2022-12-18	WOS:000864325900012
J	Azizzadenesheli, K				Azizzadenesheli, Kamyar			Importance Weight Estimation and Generalization in Domain Adaptation Under Label Shift	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Diseases; Task analysis; Hilbert space; Statistics; Sociology; Predictive models; Medical diagnostic imaging; Artificial intelligence; machine learning		We study generalization under labeled shift for categorical and general normed label spaces. We propose a series of methods to estimate the importance weights from labeled source to unlabeled target domain and provide confidence bounds for these estimators. We deploy these estimators and provide generalization bounds in the unlabeled target domain.	[Azizzadenesheli, Kamyar] Purdue Univ, Comp Sci Dept, W Lafayette, IN 47907 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus	Azizzadenesheli, K (corresponding author), Purdue Univ, Comp Sci Dept, W Lafayette, IN 47907 USA.	kaazizzad@gmail.com						A.Salman Avestimehr, 2020, Arxiv, DOI arXiv:2006.10581; Alex Smola, 2018, Arxiv, DOI arXiv:1802.03916; Amr Alexandari, 2020, Arxiv, DOI arXiv:1901.06852; Andrew Stuart, 2020, Arxiv, DOI arXiv:2003.03485; Animashree Anandkumar, 2019, Arxiv, DOI arXiv:1903.09734; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Blanchard G, 2010, J MACH LEARN RES, V11, P2973; Cao KD, 2019, ADV NEUR IN, V32; Chan YS, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1010; Chen XL, 2016, JMLR WORKSH CONF PRO, V51, P1270; Cortes C., 2010, PROC ADV NEURAL INF, V10, P442; Cortes C, 2014, THEOR COMPUT SCI, V519, P103, DOI 10.1016/j.tcs.2013.09.027; Crammer K, 2008, J MACH LEARN RES, V9, P1757; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Esfahani PM, 2018, MATH PROGRAM, V171, P115, DOI 10.1007/s10107-017-1172-1; Forman G, 2008, DATA MIN KNOWL DISC, V17, P164, DOI 10.1007/s10618-008-0097-y; GART JJ, 1966, AM J EPIDEMIOL, V83, P593, DOI 10.1093/oxfordjournals.aje.a120610; Gretton A, 2012, J MACH LEARN RES, V13, P723; Gretton A, 2009, NEURAL INF PROCESS S, P131; Hofer V, 2015, EUR J OPER RES, V243, P177, DOI 10.1016/j.ejor.2014.11.022; Iyer A, 2014, PR MACH LEARN RES, V32; Kress R., 1989, LINEAR INTEGRAL EQUA, V17; Kull M., 2014, PROC 1 INT WORKSHOP; Lang S., 2012, REAL FUNCTIONAL ANAL, Vvol 142; Li Z., 2020, ADV NEURAL INF PROCE, V33, P6755; Liu AQ, 2014, ADV NEUR IN, V27; Moreno-Torres JG, 2012, PATTERN RECOGN, V45, P521, DOI 10.1016/j.patcog.2011.06.019; Namkoong H, 2016, ADV NEUR IN, V29; Nystrom EJ, 1930, ACTA MATH-DJURSHOLM, V54, P185, DOI 10.1007/BF02547521; Pinelis I., 1992, PROBABILITY BANACH S, V30, P128; Pires B. A., 2012, ARXIV; Ramaswamy HG, 2016, PR MACH LEARN RES, V48; Rosasco L, 2010, J MACH LEARN RES, V11, P905; Saerens M, 2002, NEURAL COMPUT, V14, P21, DOI 10.1162/089976602753284446; Sanderson T, 2014, JMLR WORKSH CONF PRO, V33, P850; Scholkopf B., 2012, ARXIV; Scott C., 2013, P 26 ANN C LEARN THE, P489; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Shui C., 2020, ARXIV; Storkey A, 2009, NEURAL INF PROCESS S, P3; Tasche D, 2017, J MACH LEARN RES, V18; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; Vapnik VN, 1999, IEEE T NEURAL NETWOR, V10, P988, DOI 10.1109/72.788640; Zadrozny  B., 2004, INT C MACH LEARN ICM, DOI 10.1145/1015330.1015425; Zaremba W., 2013, ADV NEURAL INF PROCE, V26, P1; Zhang L, 2013, PROCEEDINGS OF 2013 CHINA INTERNATIONAL CONFERENCE ON INSURANCE AND RISK MANAGEMENT, P819; Li ZY, 2020, Arxiv, DOI arXiv:2010.08895	50	0	0	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6578	6584		10.1109/TPAMI.2021.3086060	http://dx.doi.org/10.1109/TPAMI.2021.3086060			7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34077355	Green Submitted			2022-12-18	WOS:000853875300052
J	Cao, XF; Tsang, IW				Cao, Xiaofeng; Tsang, Ivor W.			Distribution Disagreement via Lorentzian Focal Representation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Annotations; Neural networks; Training; Geometry; Deep learning; Data models; Complexity theory; Error disagreement; active learning; deep neural network; distribution disagreement; focal points; hyperbolic geometry		Error disagreement-based active learning (AL) selects the data that maximally update the error of a classification hypothesis. However, poor human supervision (e.g., few labels, improper classifier parameters) may weaken or clutter this update; moreover, the computational cost of performing a greedy search to estimate the errors using a deep neural network is intolerable. In this paper, a novel disagreement coefficient based on distribution, not error, provides a tighter bound on label complexity, which further guarantees its generalization in hyperbolic space. The focal points derived from the squared Lorentzian distance, present more effective hyperbolic representations on aspherical distribution from geometry, replacing the typical euclidean, kernelized, and Poincare centroids. Experiments on different deep AL tasks show that, the focal representation adopted in a tree-likeliness splitting, significantly performs better than typical baselines of centroid representations and error disagreement, and state-of-the-art neural network architectures-based AL, dramatically accelerating the learning process.	[Cao, Xiaofeng] Jilin Univ, Sch Artificial Intelligence, Changchun 130012, Jilin, Peoples R China; [Cao, Xiaofeng; Tsang, Ivor W.] Univ Technol Sydney, Australian Artificial Intelligence Inst, Sydney, NSW 2008, Australia	Jilin University; University of Technology Sydney	Tsang, IW (corresponding author), Univ Technol Sydney, Australian Artificial Intelligence Inst, Sydney, NSW 2008, Australia.	xiaofeng.cao.uts@gmail.com; ivor.tsang@uts.edu.au			Australian Research Council [DP180100106, DP200101328]	Australian Research Council(Australian Research Council)	This work was supported by Australian Research Council under Grants DP180100106 and DP200101328.	Agarwal Sharat, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P137, DOI 10.1007/978-3-030-58517-4_9; Balcan M.-F.F., 2013, ADV NEURAL INFORM PR, V26, P1295; Behpour S., 2019, PROC INT C MACH LEAR, P563; Ben-David S., 2007, ADV NEURAL INFORM PR, V19, P137; Beygelzimer A., 2009, P 26 ANN INT C MACH, P49; Blum A., 2001, P INT C MACH LEARN I, P19, DOI DOI 10.1184/R1/6606860.V1; BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371; Bratieres S, 2015, IEEE T PATTERN ANAL, V37, P1514, DOI 10.1109/TPAMI.2014.2366151; BUNTINE W, 1992, MACH LEARN, V8, P75, DOI 10.1023/A:1022686419106; Cao XF, 2022, IEEE T NEUR NET LEAR, V33, P215, DOI 10.1109/TNNLS.2020.3027605; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Cortes C, 2019, PR MACH LEARN RES, V97; Cranko Z, 2019, PR MACH LEARN RES, V97; Dasgupta S., 2005, ADV NEURAL INFORM PR, V18, P235; Dasgupta S, 2005, ADV NEURAL INFORM PR, P337; Dasgupta S, 2019, PR MACH LEARN RES, V97; Del Pero L, 2012, PROC CVPR IEEE, P2719, DOI 10.1109/CVPR.2012.6247994; Ertoz L, 2003, SIAM PROC S, P47; FAYYAD UM, 1992, MACH LEARN, V8, P87, DOI 10.1023/A:1022638503176; Ganea OE, 2018, ADV NEUR IN, V31; Geifman Y, 2019, ADV NEUR IN, V32; Guo Yu, 2020, Arxiv, DOI arXiv:1910.07153; Guo Yuhong, 2008, ADV NEURAL INFORM PR, V20, P593; Hanneke S, 2014, FOUND TRENDS MACH LE, V7, DOI 10.1561/2200000037; Haussmann M, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2470; Hossain HMS, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1862, DOI 10.1145/3292500.3330688; Huang SJ, 2014, IEEE T PATTERN ANAL, V36, P1936, DOI 10.1109/TPAMI.2014.2307881; JIN C., 2018, ADV NEURAL INFORM PR, V31, P4901; Kanai S, 2017, ADV NEUR IN, V30; Langford J, 2005, J MACH LEARN RES, V6, P273; Law MT, 2019, PR MACH LEARN RES, V97; Leskovec J, 2014, MINING OF MASSIVE DATASETS, 2ND EDITION, P1; Liu WY, 2018, PR MACH LEARN RES, V80; Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713; Lou A., 2020, PROC 37 INT C MACH L, P6393; Monath N, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P714, DOI 10.1145/3292500.3330997; Nickel M, 2018, PR MACH LEARN RES, V80; Nickel M, 2017, ADV NEUR IN, V30; Ozan Sener, 2018, Arxiv, DOI arXiv:1708.00489; Pham DT, 2005, P I MECH ENG C-J MEC, V219, P103, DOI 10.1243/095440605X8298; Pinsler R, 2019, ADV NEUR IN, V32; Quadrianto N, 2017, ADV NEUR IN, V30; Quang M. H., 2014, ADV NEURAL INFORM PR, P388; Riegler G, 2019, PROC CVPR IEEE, P7616, DOI 10.1109/CVPR.2019.00781; Sala F, 2018, PR MACH LEARN RES, V80; Sinha S, 2019, IEEE I CONF COMP VIS, P5971, DOI 10.1109/ICCV.2019.00607; Tran T, 2019, PR MACH LEARN RES, V97; Hoang TN, 2014, PR MACH LEARN RES, V32, P739; Le T, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2355; Tsang IW, 2005, J MACH LEARN RES, V6, P363; Wang KZ, 2017, IEEE T CIRC SYST VID, V27, P2591, DOI 10.1109/TCSVT.2016.2589879; Welling M., 2014, ADV NEURAL INFORM PR, V27; Yang L., 2017, INT C MEDICAL IMAGE, P399; Young N., 2012, INTRO HILBERT SPACE; Zhang N., 2019, P 22 INT C ART INT S, V89, P2801; Zhu XJ, 2015, AAAI CONF ARTIF INTE, P4083	58	0	0	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6872	6889		10.1109/TPAMI.2021.3093590	http://dx.doi.org/10.1109/TPAMI.2021.3093590			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34191723				2022-12-18	WOS:000853875300073
J	Cherian, A; Wang, J				Cherian, Anoop; Wang, Jue			Generalized One-Class Learning Using Pairs of Complementary Classifiers	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Anomaly detection; Data models; Task analysis; Kernel; Computer vision; Support vector machines; Manifolds; One-class classification; subspace learning; kernelized subspaces; Riemannian optimization	NOVELTY DETECTION; ANOMALY DETECTION; SUPPORT; OPTIMIZATION; RECOGNITION; ALGORITHMS; VIDEO; PCA	One-class learning is the classic problem of fitting a model to the data for which annotations are available only for a single class. In this paper, we explore novel objectives for one-class learning, which we collectively refer to as Generalized One-class Discriminative Subspaces (GODS). Our key idea is to learn a pair of complementary classifiers to flexibly bound the one-class data distribution, where the data belongs to the positive half-space of one of the classifiers in the complementary pair and to the negative half-space of the other. To avoid redundancy while allowing non-linearity in the classifier decision surfaces, we propose to design each classifier as an orthonormal frame and seek to learn these frames via jointly optimizing for two conflicting objectives, namely: i) to minimize the distance between the two frames, and ii) to maximize the margin between the frames and the data. The learned orthonormal frames will thus characterize a piecewise linear decision surface that allows for efficient inference, while our objectives seek to bound the data within a minimal volume that maximizes the decision margin, thereby robustly capturing the data distribution. We explore several variants of our formulation under different constraints on the constituent classifiers, including kernelized feature maps. We demonstrate the empirical benefits of our approach via experiments on data from several applications in computer vision, such as anomaly detection in video sequences, human poses, and human activities. We also explore the generality and effectiveness of GODS for non-vision tasks via experiments on several UCI datasets, demonstrating state-of-the-art results.	[Cherian, Anoop] Mitsubishi Elect Res Labs MERL, Cambridge, MA 02139 USA; [Wang, Jue] Australian Natl Univ, Res Sch Engn, Canberra, ACT 2601, Australia	Australian National University	Cherian, A (corresponding author), Mitsubishi Elect Res Labs MERL, Cambridge, MA 02139 USA.	cherian@merl.com; jue.wang@anu.edu.au						Abati D, 2019, PROC CVPR IEEE, P481, DOI 10.1109/CVPR.2019.00057; Absil P.-A., 2006, 2006 IEEE INT C AC S, pV; Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Aditya Krishna Menon, 2019, Arxiv, DOI arXiv:1802.06360; Anoop Cherian, 2016, Arxiv, DOI arXiv:1607.05447; Anton van den Hengel, 2020, Arxiv, DOI arXiv:2007.02500; Barla A, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P513; Bin Zhao, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3313, DOI 10.1109/CVPR.2011.5995524; BISHOP CM, 1994, IEE P-VIS IMAGE SIGN, V141, P217, DOI 10.1049/ip-vis:19941330; Bodesheim P, 2013, PROC CVPR IEEE, P3374, DOI 10.1109/CVPR.2013.433; Boothby W. M., 1986, INTRO DIFFERENTIABLE, V120; Boumal N, 2014, J MACH LEARN RES, V15, P1455; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Calderara S, 2011, COMPUT VIS IMAGE UND, V115, P1099, DOI 10.1016/j.cviu.2011.03.003; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Carlos D. Castillo, 2017, Arxiv, DOI arXiv:1703.09507; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Cherian A, 2018, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2018.00234; Choi MJ, 2012, PATTERN RECOGN LETT, V33, P853, DOI 10.1016/j.patrec.2011.12.004; Choi YS, 2009, PATTERN RECOGN LETT, V30, P1236, DOI 10.1016/j.patrec.2009.05.007; Chong YS, 2017, LECT NOTES COMPUT SC, V10262, P189, DOI 10.1007/978-3-319-59081-3_23; Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33; De la Torre F, 2003, INT J COMPUT VISION, V54, P117, DOI 10.1023/A:1023709501986; Del Giorno A, 2016, LECT NOTES COMPUT SC, V9909, P334, DOI 10.1007/978-3-319-46454-1_21; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Duma SM, 1996, J TRAUMA, V41, P114, DOI 10.1097/00005373-199607000-00018; Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Fragoso V, 2016, INT C PATT RECOG, P420, DOI 10.1109/ICPR.2016.7899670; Gardner AB, 2006, J MACH LEARN RES, V7, P1025; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Graf ABA, 2003, IEEE T NEURAL NETWOR, V14, P597, DOI 10.1109/TNN.2003.811708; Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86; Hautamaki V, 2004, INT C PATT RECOG, P430, DOI 10.1109/ICPR.2004.1334558; Heller K., 2003, P WORKSHOP DATA MINI, P2; Herbrich R, 2001, ADV NEUR IN, V13, P224; Higham NJ, 2010, SIAM J MATRIX ANAL A, V31, P2163, DOI 10.1137/090765018; Hoffmann H, 2007, PATTERN RECOGN, V40, P863, DOI 10.1016/j.patcog.2006.07.009; Itti L, 2000, VISION RES, V40, P1489, DOI 10.1016/S0042-6989(99)00163-7; J.Zico Kolter, 2018, Arxiv, DOI arXiv:1803.01271; Jegou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462; Khan SS, 2010, LECT NOTES ARTIF INT, V6206, P188; Kim J, 2009, PROC CVPR IEEE, P2913; Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207; Koniusz P, 2017, IEEE T PATTERN ANAL, V39, P313, DOI 10.1109/TPAMI.2016.2545667; Krawczyk B, 2015, NEUROCOMPUTING, V150, P490, DOI 10.1016/j.neucom.2014.07.068; Lazarevic A, 2003, SIAM PROC S, P25; Lee K, 2007, IEEE T NEURAL NETWOR, V18, P284, DOI 10.1109/TNN.2006.884673; Lee YJ, 2001, COMPUT OPTIM APPL, V20, P5, DOI 10.1023/A:1011215321374; Li C, 2013, NEUROCOMPUTING, V119, P94, DOI 10.1016/j.neucom.2012.03.040; Li Junnan., 2020, P INT C LEARNING REP; Li WX, 2014, IEEE T PATTERN ANAL, V36, P18, DOI 10.1109/TPAMI.2013.111; Liu W, 2014, PROC CVPR IEEE, P3826, DOI 10.1109/CVPR.2014.483; Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684; Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338; Luo WX, 2017, IEEE I CONF COMP VIS, P341, DOI 10.1109/ICCV.2017.45; Luo WX, 2017, IEEE INT CON MULTI, P439, DOI 10.1109/ICME.2017.8019325; Mahmood Fathy, 2017, Arxiv, DOI arXiv:1609.00866; Mangasaian OL, 2001, J MACH LEARN RES, V1, P161, DOI 10.1162/15324430152748218; Matteoli S, 2014, IEEE J-STARS, V7, P2317, DOI 10.1109/JSTARS.2014.2315772; Moh'd Abdelwadood, 2007, Journal of Computer Sciences, V3, P430, DOI 10.3844/jcssp.2007.430.435; Mohseni S, 2020, AAAI CONF ARTIF INTE, V34, P5216; Muirhead R.J., 1982, ASPECTS MULTIVARIATE; Nguyen M. H., 2009, P ADV NEUR INF PROC, P1185; NOLAN D, 1991, J MULTIVARIATE ANAL, V39, P348, DOI 10.1016/0047-259X(91)90106-C; Nordhoff L. S., 2005, MOTOR VEHICLE COLLIS; Park H., 2020, PROC IEEECVF C COMPU; Park S, 2012, LECT NOTES COMPUT SC, V7574, P651, DOI 10.1007/978-3-642-33712-3_47; Pimentel MAF, 2014, SIGNAL PROCESS, V99, P215, DOI 10.1016/j.sigpro.2013.12.026; Popoola OP, 2012, IEEE T SYST MAN CY C, V42, P865, DOI 10.1109/TSMCC.2011.2178594; Pramuditha Perera, 2019, Arxiv, DOI arXiv:1801.05365; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Ravanbakhsh M, 2019, IEEE WINT CONF APPL, P1896, DOI 10.1109/WACV.2019.00206; Ravanbakhsh M, 2018, IEEE WINT CONF APPL, P1689, DOI 10.1109/WACV.2018.00188; Ravanbakhsh M, 2017, IEEE IMAGE PROC, P1577; Ren H., 2016, ARXIV; Ritter G, 1997, PATTERN RECOGN LETT, V18, P525, DOI 10.1016/S0167-8655(97)00049-4; Ruff L, 2018, PR MACH LEARN RES, V80; Sabokrou M, 2018, PROC CVPR IEEE, P3379, DOI 10.1109/CVPR.2018.00356; Saleh B, 2013, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2013.107; Saligrama V, 2012, PROC CVPR IEEE, P2112, DOI 10.1109/CVPR.2012.6247917; Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115; Shih-En Wei, 2017, Arxiv, DOI arXiv:1611.08050; Smola Alex J, 1998, LEARNING KERNELS, V4; Sohrab F, 2018, INT C PATT RECOG, P722, DOI 10.1109/ICPR.2018.8545819; Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678; Tax, 2001, ONE CLASS CLASSIFICA; Tax D. M. J., 2000, Learning from Imbalanced Data Sets. Papers from the AAAI Workshop (Technical Report WS-00-05), P25; Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49; Townsend J, 2016, J MACH LEARN RES, V17; Trivedi MM, 2004, IEEE T VEH TECHNOL, V53, P1698, DOI 10.1109/TVT.2004.835526; Trivedi MM, 2007, IEEE T INTELL TRANSP, V8, P108, DOI 10.1109/TITS.2006.889442; Tsybakov AB, 1997, ANN STAT, V25, P948, DOI 10.1214/aos/1069362732; Tung F, 2011, IMAGE VISION COMPUT, V29, P230, DOI 10.1016/j.imavis.2010.11.003; Lai V, 2015, PROCEEDINGS OF 2015 2ND NATIONAL FOUNDATION FOR SCIENCE AND TECHNOLOGY DEVELOPMENT CONFERENCE ON INFORMATION AND COMPUTER SCIENCE NICS 2015, P135, DOI 10.1109/NICS.2015.7302178; Wang J, 2019, IEEE I CONF COMP VIS, P8200, DOI 10.1109/ICCV.2019.00829; Wang J, 2018, PROC CVPR IEEE, P1149, DOI 10.1109/CVPR.2018.00126; Wang Jue, 2018, P EUR C COMP VIS, P685, DOI DOI 10.1007/978-3-030-01225-0_42; Wang T, 2013, SENSORS-BASEL, V13, P17130, DOI 10.3390/s131217130; Wen ZW, 2013, MATH PROGRAM, V142, P397, DOI 10.1007/s10107-012-0584-1; Wu SD, 2010, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2010.5539882; Xia Y, 2015, IEEE I CONF COMP VIS, P1511, DOI 10.1109/ICCV.2015.177; Xu D, 2017, COMPUT VIS IMAGE UND, V156, P117, DOI 10.1016/j.cviu.2016.10.010; Xu Dan, 2015, ARXIV151001553; Xu H, 2013, IEEE T INFORM THEORY, V59, P546, DOI 10.1109/TIT.2012.2212415; Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156; You C, 2017, PROC CVPR IEEE, P4323, DOI 10.1109/CVPR.2017.460; Zhang D, 2005, PROC CVPR IEEE, P611; Zisselman Ev, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13991, DOI 10.1109/CVPR42600.2020.01401	117	0	0	6	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6993	7009		10.1109/TPAMI.2021.3092999	http://dx.doi.org/10.1109/TPAMI.2021.3092999			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34181535	Green Submitted			2022-12-18	WOS:000853875300081
J	Grieggs, S; Shen, BY; Rauch, G; Li, P; Ma, JQ; Chiang, D; Price, B; Scheirer, WJ				Grieggs, Samuel; Shen, Bingyu; Rauch, Greta; Li, Pei; Ma, Jiaqi; Chiang, David; Price, Brian; Scheirer, Walter J.			Measuring Human Perception to Improve Handwritten Document Transcription	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Loss measurement; Training; Tools; Task analysis; Annotations; Visualization; Computer vision; Document transcription; visual recognition; visual psychophysics; deep learning; digital humanities		In this paper, we consider how to incorporate psychophysical measurements of human visual perception into the loss function of a deep neural network being trained for a recognition task, under the assumption that such information can reduce errors. As a case study to assess the viability of this approach, we look at the problem of handwritten document transcription. While good progress has been made towards automatically transcribing modern handwriting, significant challenges remain in transcribing historical documents. Here we describe a general enhancement strategy, underpinned by the new loss formulation, which can be applied to the training regime of any deep learning-based document transcription system. Through experimentation, reliable performance improvement is demonstrated for the standard IAM and RIMES datasets for three different network architectures. Further, we go on to show feasibility for our approach on a new dataset of digitized Latin manuscripts, originally produced by scribes in the Cloister of St. Gall in the the 9th century.	[Grieggs, Samuel; Shen, Bingyu; Rauch, Greta; Chiang, David; Scheirer, Walter J.] Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA; [Li, Pei] Zoom Video Commun, San Jose, CA 95113 USA; [Ma, Jiaqi] Yale Univ, Dept Class, New Haven, CT 06520 USA; [Price, Brian] Adobe Res, Salt Lake City, UT 84112 USA	University of Notre Dame; Yale University	Scheirer, WJ (corresponding author), Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.	sgrieggs@nd.edu; bshen@nd.edu; mrauch2@nd.edu; Pei.Li.237@nd.edu; jiaqi.ma@yak.edu; dchiang@nd.edu; bprice@adobe.com; walter.scheirer@nd.edu			Notre Dame Research, University of Notre Dame; College of Arts and Letters, University of Notre Dame; Office of Mission Engagement, University of Notre Dame; Office of Digital Learning, University of Notre Dame; Adobe Systems; NVIDIA Corporation; Medieval Institute, University of Notre Dame	Notre Dame Research, University of Notre Dame; College of Arts and Letters, University of Notre Dame; Office of Mission Engagement, University of Notre Dame; Office of Digital Learning, University of Notre Dame; Adobe Systems; NVIDIA Corporation; Medieval Institute, University of Notre Dame	The Authors would like to thank Cana Short, Mihow McKenny, Melody Wauke, Curtis Wigington, and Christopher Tensmeyer for their help preparing data and invaluable advice. This work was supported in part by the Notre Dame Research, University of Notre Dame, in part by the College of Arts and Letters, University of Notre Dame, in part by the Medieval Institute, University of Notre Dame, in part by the Office of Mission Engagement, University of Notre Dame, and the Office of Digital Learning, University of Notre Dame, in part by Adobe Systems, and in part by the NVIDIA Corporation.	Abbott A, 2017, NATURE, V546, P341, DOI 10.1038/546341a; Sanchez JA, 2016, INT CONF FRONT HAND, P630, DOI [10.1109/ICFHR.2016.0120, 10.1109/ICFHR.2016.112]; [Anonymous], 2020, LATIN OCR TESSERACT; Augustin E., 2006, INT WORKSHOP FRONTIE, P231; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Berg-Kirkpatrick T., 2017, OCULAR; Berg-Kirkpatrick Taylor, 2013, P 51 ANN M ASS COMP, V1, P207; Blackwell C. W., 2014, PROC I STUDY ANCIENT, P5; Bluche T, 2014, LECT NOTES COMPUT SC, V8791, P199, DOI 10.1007/978-3-319-11397-5_15; Boschetti F, 2009, LECT NOTES COMPUT SC, V5714, P156, DOI 10.1007/978-3-642-04346-8_17; Breuel TM, 2008, PROC SPIE, V6815, DOI 10.1117/12.783598; Brown MS, 2007, IEEE T PATTERN ANAL, V29, P1904, DOI 10.1109/TPAMI.2007.1118; Brown MS, 2004, IEEE T PATTERN ANAL, V26, P1295, DOI 10.1109/TPAMI.2004.87; Bukhari SS, 2017, PROC INT CONF DOC, P305, DOI 10.1109/ICDAR.2017.58; Cloppet F, 2016, INT CONF FRONT HAND, P590, DOI [10.1109/ICFHR.2016.0113, 10.1109/ICFHR.2016.106]; codices, E COD VIRT MAN LIB S; codices, ST GALLEN STIFTSBIBL; Crane G., 2012, P 12 ACM IEEE CS JOI, P213; Doetseh P, 2014, INT CONF FRONT HAND, P279, DOI 10.1109/ICFHR.2014.54; Eberhardt S, 2016, ADV NEUR IN, V29; Edwards J., 2004, P ADV NEUR INF PROC, P385; Firmani D, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P263, DOI 10.1145/3219819.3219879; Fischer A., 2011, P 2011 WORKSH HIST D, P29, DOI DOI 10.1145/2037342.2037348; Fischer A, 2009, 2009 15TH INTERNATIONAL CONFERENCE ON VIRTUAL SYSTEMS AND MULTIMEDIA PROCEEDINGS (VSMM 2009), P137, DOI 10.1109/VSMM.2009.26; Geirhos Robert, 2017, ARXIV PREPRINT ARXIV, DOI [10.48550/arxiv.1706.06969, DOI 10.48550/ARXIV.1706.06969]; Gerhard HE, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002873; Graves A., 2006, P INT C MACH LEARN I; Hofland K, 1978, LANCASTER OSLO BERGE; Kahle P, 2017, PROC INT CONF DOC, P19, DOI 10.1109/ICDAR.2017.307; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Lu Z, 2009, PROC CVPR IEEE, P88, DOI 10.1109/CVPRW.2009.5206789; Marti U.-V., 2002, International Journal on Document Analysis and Recognition, V5, P39, DOI 10.1007/s100320200071; Poznanski A, 2016, PROC CVPR IEEE, P2305, DOI 10.1109/CVPR.2016.253; Puigcerver J., 2016, LAIA DEEP LEARNING T; Puigcerver J, 2017, PROC INT CONF DOC, P67, DOI 10.1109/ICDAR.2017.20; Rajalingham R, 2018, J NEUROSCI, V38, P7255, DOI 10.1523/JNEUROSCI.0388-18.2018; RichardWebster B, 2019, IEEE T PATTERN ANAL, V41, P2280, DOI 10.1109/TPAMI.2018.2849989; Ronneberger O., 2015, P MEDICAL IMAGE COMP, P234; Schapire RE, 2003, LECT NOTES STAT, V171, P149, DOI 10.1007/978-0-387-21579-2_9; Scheidl H, 2018, INT CONF FRONT HAND, P253, DOI 10.1109/ICFHR-2018.2018.00052; Scheirer WJ, 2014, IEEE T PATTERN ANAL, V36, P1679, DOI 10.1109/TPAMI.2013.2297711; Shanyu Xiao, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P9, DOI 10.1109/ICDAR.2019.00012; Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Sim S.-C., 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040563; Smith DavidA, 2018, RES AGENDA HIST MULT; Smith R, 2007, PROC INT CONF DOC, P629, DOI 10.1109/icdar.2007.4376991; SPRINGMANN U, 2014, P 1 INT C DIG ACC TE, P71, DOI DOI 10.1145/2595188.2595205; Stutzmann D., 2018, P DIGIT HUMANITIES, P298; Taylor E, 2020, IEEE COMPUT SOC CONF, P1555, DOI 10.1109/CVPRW50498.2020.00199; Terras M., 2012, DIGITAL HUMANITIES P, P47; Vaswani A, 2017, ADV NEUR IN, V30; Voigtlaender P, 2016, INT CONF FRONT HAND, P228, DOI [10.1109/ICFHR.2016.48, 10.1109/ICFHR.2016.0052]; Wigington C, 2018, LECT NOTES COMPUT SC, V11210, P372, DOI 10.1007/978-3-030-01231-1_23; Wigington C, 2017, PROC INT CONF DOC, P639, DOI 10.1109/ICDAR.2017.110; Wolf Lior, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3545, DOI 10.1109/ICIP.2011.6116481; Wuthrich Markus, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P211, DOI 10.1109/ICDAR.2009.17	57	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6594	6601		10.1109/TPAMI.2021.3092688	http://dx.doi.org/10.1109/TPAMI.2021.3092688			8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34170823	Green Submitted			2022-12-18	WOS:000853875300054
J	Guo, Y; Zheng, Y; Tan, MK; Chen, Q; Li, ZP; Chen, J; Zhao, PL; Huang, JZ				Guo, Yong; Zheng, Yin; Tan, Mingkui; Chen, Qi; Li, Zhipeng; Chen, Jian; Zhao, Peilin; Huang, Junzhou			Towards Accurate and Compact Architectures via Neural Architecture Transformer	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Architecture optimization; neural architecture search; compact architecture design; operation transition	ALGORITHMS	Designing effective architectures is one of the key factors behind the success of deep neural networks. Existing deep architectures are either manually designed or automatically searched by some Neural Architecture Search (NAS) methods. However, even a well-designed/searched architecture may still contain many nonsignificant or redundant modules/operations (e.g., some intermediate convolution or pooling layers). Such redundancy may not only incur substantial memory consumption and computational cost but also deteriorate the performance. Thus, it is necessary to optimize the operations inside an architecture to improve the performance without introducing extra computational cost. To this end, we have proposed a Neural Architecture Transformer (NAT) method which casts the optimization problem into a Markov Decision Process (MDP) and seeks to replace the redundant operations with more efficient operations, such as skip or null connection. Note that NAT only considers a small number of possible replacements/transitions and thus comes with a limited search space. As a result, such a small search space may hamper the performance of architecture optimization. To address this issue, we propose a Neural Architecture Transformer++ (NAT++) method which further enlarges the set of candidate transitions to improve the performance of architecture optimization. Specifically, we present a two-level transition rule to obtain valid transitions, i.e., allowing operations to have more efficient types (e.g., convolution -> separable convolution) or smaller kernel sizes (e.g., 5x5 -> 3x3). Note that different operations may have different valid transitions. We further propose a Binary-Masked Softmax (BMSoftmax) layer to omit the possible invalid transitions. Last, based on the MDP formulation, we apply policy gradient to learn an optimal policy, which will be used to infer the optimized architectures. Extensive experiments show that the transformed architectures significantly outperform both their original counterparts and the architectures optimized by existing methods.	[Guo, Yong; Tan, Mingkui; Chen, Qi; Li, Zhipeng; Chen, Jian] South China Univ Technol, Sch Software Engn, Guangzhou 510006, Peoples R China; [Guo, Yong] Peng Cheng Lab, Shenzhen 518066, Peoples R China; [Zheng, Yin] Tencent, Weixin Grp, Shenzhen 518054, Peoples R China; [Tan, Mingkui] South China Univ Technol, Key Lab Big Data & Intelligent Robot, Minist Educ, Guangzhou 510006, Peoples R China; [Zhao, Peilin; Huang, Junzhou] Tencent, Tencent AI Lab, Shenzhen 518054, Peoples R China	South China University of Technology; Peng Cheng Laboratory; Tencent; South China University of Technology; Tencent	Tan, MK (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou 510006, Peoples R China.	guo.yong@mail.scut.edu.cn; yzheng3xg@gmail.com; mingkuitan@scut.edu.cn; sechenqi@scut.edu.cn; sezhipeng@mail.scut.edu.cn; ellachen@scut.edu.cn; masonzhao@tencent.com; joehhuang@tencent.com						ANANTHARAM V, 1987, IEEE T AUTOMAT CONTR, V32, P968, DOI 10.1109/TAC.1987.1104491; Andrew G. Howard, 2017, Arxiv, DOI arXiv:1704.04861; Baker Bowen, 2017, ICLR; Bo Du, 2019, Arxiv, DOI arXiv:1911.04978; Brock A., 2018, ICLR, P1; Cai Han, 2019, INT C LEARN REPR; Cao S., 2019, P INT C LEARN REPR, P1; Chen S, 2018, LECT NOTES COMPUT SC, V10996, P428, DOI 10.1007/978-3-319-97909-0_46; Chen Tianqi, 2016, ICLR; Chen XN, 2020, PR MACH LEARN RES, V119; Chu X., 2019, ARXIV; Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482; Ding M, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2694; Dong XT, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3023706; Guo Y., 2020, INTERNA TIONAL C MAC, P3822; Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6; Guo Y, 2019, ADV NEUR IN, V32; Guo Y, 2018, AAAI CONF ARTIF INTE, P3134; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Huang G.B., 2008, WORKSHOP FACESREAL L; Jiang ZX, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1965; Kipf TN, 2016, P INT C LEARN REPR; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lemaire C, 2019, PROC CVPR IEEE, P9100, DOI 10.1109/CVPR.2019.00932; Li H., 2017, P INT C LEARN REPR I, P1; Lin X. V., 2018, EMNLP, P3243, DOI 10.18653/v1/D18-1362; Liu CX, 2018, LECT NOTES COMPUT SC, V11205, P19, DOI 10.1007/978-3-030-01246-5_2; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Luo JH, 2019, IEEE T PATTERN ANAL, V41, P2525, DOI 10.1109/TPAMI.2018.2858232; Luo RQ, 2018, ADV NEUR IN, V31; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250; Nair V., 2010, ICML, P807; Pham H, 2018, PR MACH LEARN RES, V80; Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233; Rao YM, 2019, IEEE T PATTERN ANAL, V41, P2291, DOI 10.1109/TPAMI.2018.2878258; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1476, DOI 10.1109/TPAMI.2016.2601099; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Sengupta S, 2016, IEEE WINT CONF APPL; Shu Y., 2019, PROC INT C LEARN REP, P1; Simonyan K., 2015, ARXIV PREPRINT ARXIV; So DR, 2019, PR MACH LEARN RES, V97; Srivastava RK, 2015, ADV NEUR IN, V28; Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Vaswani A, 2017, ADV NEUR IN, V30; Vermorel J, 2005, LECT NOTES ARTIF INT, V3720, P437, DOI 10.1007/11564096_42; Wang J., 2020, P ADV NEUR INF PROC, P5991; Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386; Xie Sirui, 2019, ICLR, V1, P13; Xu Y, 2020, PLANT SOIL, V449, P133, DOI 10.1007/s11104-020-04435-1; Yang TJ, 2018, LECT NOTES COMPUT SC, V11214, P289, DOI 10.1007/978-3-030-01249-6_18; Zela A., 2019, P INT C LEARN REPR, P1; Zhang C, 2020, PEER PEER NETW APPL, V13, P16, DOI [10.1007/s12083-018-0715-4, 10.1109/IRMMW-THz.2019.8874219]; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhang XY, 2016, IEEE T PATTERN ANAL, V38, P1943, DOI 10.1109/TPAMI.2015.2502579; Zhou P., 2020, ADV NEURAL INFORM PR, P8296; Zhuang ZW, 2018, ADV NEUR IN, V31; Zoph B., 2017, P1; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	72	0	0	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6501	6516		10.1109/TPAMI.2021.3086914	http://dx.doi.org/10.1109/TPAMI.2021.3086914			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34097606	Green Submitted			2022-12-18	WOS:000853875300047
J	Han, K; Rebuffi, SA; Ehrhardt, S; Vedaldi, A; Zisserman, A				Han, Kai; Rebuffi, Sylvestre-Alvise; Ehrhardt, Sebastien; Vedaldi, Andrea; Zisserman, Andrew			AutoNovel: Automatically Discovering and Learning Novel Visual Categories	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Ranking (statistics); Data models; Annotations; Benchmark testing; Visualization; Transfer learning; Novel category discovery; deep transfer clustering; clustering; classification; incremental learning	MEAN SHIFT; CLUSTER	We tackle the problem of discovering novel classes in an image collection given labelled examples of other classes. We present a new approach called AutoNovel to address this problem by combining three ideas: (1) we suggest that the common approach of bootstrapping an image representation using the labelled data only introduces an unwanted bias, and that this can be avoided by using self-supervised learning to train the representation from scratch on the union of labelled and unlabelled data; (2) we use ranking statistics to transfer the model's knowledge of the labelled classes to the problem of clustering the unlabelled images; and, (3) we train the data representation by optimizing a joint objective function on the labelled and unlabelled subsets of the data, improving both the supervised classification of the labelled data, and the clustering of the unlabelled data. Moreover, we propose a method to estimate the number of classes for the case where the number of new categories is not known a priori. We evaluate AutoNovel on standard classification benchmarks and substantially outperform current methods for novel category discovery. In addition, we also show that AutoNovel can be used for fully unsupervised image clustering, achieving promising results.	[Han, Kai; Rebuffi, Sylvestre-Alvise; Ehrhardt, Sebastien; Vedaldi, Andrea; Zisserman, Andrew] Univ Oxford, Dept Engn Sci, Visual Geometry Grp, Oxford OX1 3PJ, England	University of Oxford	Han, K (corresponding author), Univ Oxford, Dept Engn Sci, Visual Geometry Grp, Oxford OX1 3PJ, England.	khan@robots.ox.ac.uk; srebuffi@robots.ox.ac.uk; hyenal@robots.ox.ac.uk; vedaldi@robots.ox.ac.uk; az@robots.ox.ac.uk	Vedaldi, Andrea/B-9071-2015	Vedaldi, Andrea/0000-0003-1374-2858	EPSRC [Seebibyte EP/M013774/1, Mathworks/DTA DFR02620, ERC IDIU-638009]; Nielsen	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Nielsen	This work was supported by EPSRC Programme under Grants Seebibyte EP/M013774/1, Mathworks/DTA DFR02620, and ERC IDIU-638009. We also gratefully acknowledge the support of Nielsen.	Aggarwal CC, 2014, CH CRC DATA MIN KNOW, P1; Aljundi R, 2018, LECT NOTES COMPUT SC, V11207, P144, DOI 10.1007/978-3-030-01219-9_9; Anand S, 2014, IEEE T PATTERN ANAL, V36, P1201, DOI 10.1109/TPAMI.2013.190; Andrea Vedaldi, 2020, Arxiv, DOI arXiv:1905.08845; Andrea Vedaldi, 2020, Arxiv, DOI arXiv:2006.10039; Arbelaitz O, 2013, PATTERN RECOGN, V46, P243, DOI 10.1016/j.patcog.2012.07.021; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Bezdek JC, 1998, IEEE T SYST MAN CY B, V28, P301, DOI 10.1109/3477.678624; Cai D, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1010; Calinski T., 1974, COMMUN STAT-THEOR M, V3, P1, DOI [DOI 10.1080/03610927408827101, 10.1080/03610927408827101]; Chang JL, 2020, IEEE T PATTERN ANAL, V42, P809, DOI 10.1109/TPAMI.2018.2889949; Chang JL, 2017, IEEE I CONF COMP VIS, P5880, DOI 10.1109/ICCV.2017.626; Chapelle Olivier, 2010, SEMISUPERVISED LEARN, V2, P5; Chen T, 2020, PR MACH LEARN RES, V119; Chen X., 2011, P 21 INT JOINT C ART, P1010; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; DAVIES DL, 1979, IEEE T PATTERN ANAL, V1, P224, DOI 10.1109/TPAMI.1979.4766909; Dean T, 2013, PROC CVPR IEEE, P1814, DOI 10.1109/CVPR.2013.237; Deng J., 2009, P 2009 IEEE C COMP V, P248, DOI DOI 10.1109/CVPR.2009.5206848; Dizaji KG, 2017, IEEE I CONF COMP VIS, P5747, DOI 10.1109/ICCV.2017.612; Dunn J. C., 1974, J CYBERNETICS, V4, P95, DOI 10.1080/01969727408546059; Fu YW, 2018, IEEE SIGNAL PROC MAG, V35, P112, DOI 10.1109/MSP.2017.2763441; Gidaris Spyros, 2018, ARXIV180307728; Han K., 2020, INT C LEARN REPR ICL; Han K, 2019, IEEE I CONF COMP VIS, P8400, DOI 10.1109/ICCV.2019.00849; Haoqi Fan, 2020, Arxiv, DOI arXiv:2003.04297; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hsu Y. -C., 2016, ARXIV; Hsu Yen-chang, 2019, ICLR; Hsu Yen-Chang, 2018, ICLR; Ji X, 2019, IEEE I CONF COMP VIS, P9864, DOI 10.1109/ICCV.2019.00996; Kaban A, 2011, PATTERN RECOGN, V44, P265, DOI 10.1016/j.patcog.2010.08.018; Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975; Kingma D. P., 2015, 3 INT C LEARN REPR I, P1; Kolesnikov A, 2019, PROC CVPR IEEE, P1920, DOI 10.1109/CVPR.2019.00202; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Laine Samuli, 2017, P INT C LEARN REPR I, P3; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Lopez-Paz D, 2017, ADV NEUR IN, V30; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; Ng A.Y., 2011, NIPS WORKSH DEEP LEA; Ng AY, 2002, ADV NEUR IN, V14, P849; Oliver Avital, 2018, ARXIV180409170; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587; ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7; Sarfraz MS, 2019, PROC CVPR IEEE, P8926, DOI 10.1109/CVPR.2019.00914; Shmelkov K, 2017, IEEE I CONF COMP VIS, P3420, DOI 10.1109/ICCV.2017.368; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Sohn Kihyuk, 2020, ARXIV200107685; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Tan CQ, 2018, LECT NOTES COMPUT SC, V11141, P270, DOI 10.1007/978-3-030-01424-7_27; Tarvainen Antti, 2017, CORR, Vabs/1703; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Van Gansbeke Wouter, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P268, DOI 10.1007/978-3-030-58607-2_16; Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6; Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768; Xie JY, 2016, PR MACH LEARN RES, V48; Yagnik J, 2011, IEEE I CONF COMP VIS, P2431, DOI 10.1109/ICCV.2011.6126527; Yang B, 2017, PR MACH LEARN RES, V70; Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556; Zhai XH, 2019, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2019.00156	66	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6767	6781		10.1109/TPAMI.2021.3091944	http://dx.doi.org/10.1109/TPAMI.2021.3091944			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34166184	Green Submitted			2022-12-18	WOS:000853875300067
J	Hu, WM; Liu, HW; Du, Y; Yuan, CF; Li, B; Maybank, SJ				Hu, Weiming; Liu, Haowei; Du, Yang; Yuan, Chunfeng; Li, Bing; Maybank, Stephen John			Interaction-Aware Spatio-Temporal Pyramid Attention Networks for Action Classification	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Videos; Feature extraction; Training; Recurrent neural networks; Semantics; Principal component analysis; Deep learning; Action recognition; Attention networks; Interaction-aware; spatio-temporal pyramid	HISTOGRAMS	For CNN-based visual action recognition, the accuracy may be increased if local key action regions are focused on. The task of self-attention is to focus on key features and ignore irrelevant information. So, self-attention is useful for action recognition. However, current self-attention methods usually ignore correlations among local feature vectors at spatial positions in CNN feature maps. In this paper, we propose an effective interaction-aware self-attention model which can extract information about the interactions between feature vectors to learn attention maps. Since the different layers in a network capture feature maps at different scales, we introduce a spatial pyramid with the feature maps at different layers for attention modeling. The multi-scale information is utilized to obtain more accurate attention scores. These attention scores are used to weight the local feature vectors of the feature maps and then calculate attentional feature maps. Since the number of feature maps input to the spatial pyramid attention layer is unrestricted, we easily extend this attention layer to a spatio-temporal version. Our model can be embedded in any general CNN to form a video-level end-to-end attention network for action recognition. Several methods are investigated to combine the RGB and flow streams to obtain accurate predictions of human actions. Experimental results show that our method achieves state-of-the-art results on the datasets UCF101, HMDB51, Kinetics-400, and untrimmed Charades.	[Hu, Weiming; Liu, Haowei; Du, Yang; Yuan, Chunfeng; Li, Bing] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China; [Hu, Weiming; Liu, Haowei; Du, Yang; Yuan, Chunfeng; Li, Bing] Univ Chinese Acad Sci, CAS Ctr Excellence Brain Sci & Intelligence Techn, Sch Artificial Intelligenceee, Beijing 100190, Peoples R China; [Maybank, Stephen John] Univ London, Birkbeck Coll, Dept Comp Sci & Informat Syst, London WC1E 7HX, England	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; University of London; Birkbeck University London	Hu, WM (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.	wmhu@nlpr.ia.ac.an; haowei.liu@nlpr.ia.ac.an; yang.du@nlpr.ia.ac.an; cfyuan@nlpr.ia.ac.an; bli@nlpr.ia.ac.an; sjmaybank@dcs.bbk.ac.uk			National Key R&D Program of China [2018AAA0102802]; Natural Science Foundation of China [62036011, 61721004]; Key Research Program of Frontier Sciences, CAS [QYZDJ-SSW-JSC040]	National Key R&D Program of China; Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Key Research Program of Frontier Sciences, CAS	This work was supported in part by the National Key R&D Program of China under Grant 2018AAA0102802, in part by the Natural Science Foundation of China under Grants 62036011 and 61721004, and in part by the Key Research Program of Frontier Sciences, CAS, under Grant QYZDJ-SSW-JSC040.	Abu-El-Haija S., 2016, ARXIV; Amir Roshan Zamir, 2012, Arxiv, DOI arXiv:1212.0402; Andrew Zisserman, 2017, Arxiv, DOI arXiv:1705.06950; Awad, 2016, TRECVID 2016 EVALUAT; Baradel F, 2018, LECT NOTES COMPUT SC, V11217, P106, DOI 10.1007/978-3-030-01261-8_7; Bouwmans Thierry, 2011, Recent Patents on Computer Science, V4, P147, DOI 10.2174/1874479611104030147; Cai ZW, 2014, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2014.83; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen JJ, 2019, IEEE INT CON MULTI, P1054, DOI 10.1109/ICME.2019.00185; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33; Diba A, 2017, PROC CVPR IEEE, P1541, DOI 10.1109/CVPR.2017.168; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Dong WK, 2019, AAAI CONF ARTIF INTE, P8247; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Du Y, 2018, LECT NOTES COMPUT SC, V11220, P388, DOI 10.1007/978-3-030-01270-0_23; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Feichtenhofer C, 2016, ADV NEUR IN, V29; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Gao RH, 2020, PROC CVPR IEEE, P10454, DOI 10.1109/CVPR42600.2020.01047; Ghadiyaram D, 2019, PROC CVPR IEEE, P12038, DOI 10.1109/CVPR.2019.01232; Girdhar R, 2017, ADV NEUR IN, V30; Girdhar R, 2019, IEEE I CONF COMP VIS, P852, DOI 10.1109/ICCV.2019.00094; Girdhar R, 2017, PROC CVPR IEEE, P3165, DOI 10.1109/CVPR.2017.337; github.com, OPEN MMLABMMACTION2; github.com, GSIGPYVIDEORESEARCHB; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang GX, 2020, INT CONF ACOUST SPEE, P2103; Huang HY, 2021, IEEE T AFFECT COMPUT, V12, P832, DOI [10.1109/TAFFC.2019.2901456, 10.1145/3290605.3300851]; Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Ji J., 2020, CVPR, P10236, DOI 10.1109/cvpr42600.2020.01025; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209; Jimmy Ba, 2015, Arxiv, DOI arXiv:1412.7755; Kar A, 2017, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR.2017.604; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Klaser Alexander, 2008, P BRIT MACH VIS C; Kopuklu O, 2019, IEEE INT CONF COMP V, P1910, DOI 10.1109/ICCVW.2019.00240; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Kumawat S., 2021, T PATTERN ANAL MACH; Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756; Lei JJ, 2019, IEEE INT CON MULTI, P562, DOI 10.1109/ICME.2019.00103; Lev G, 2016, LECT NOTES COMPUT SC, V9910, P833, DOI 10.1007/978-3-319-46466-4_50; Li J, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107037; Li JN, 2020, IEEE WINT CONF APPL, P497, DOI 10.1109/WACV45572.2020.9093283; Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099; Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Lin Z, 2017, PROC INT C LEARN REP, P1; Long X, 2018, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2018.00817; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Ma CY, 2018, PROC CVPR IEEE, P6790, DOI 10.1109/CVPR.2018.00710; Mallya A, 2016, LECT NOTES COMPUT SC, V9905, P414, DOI 10.1007/978-3-319-46448-0_25; Mnih V, 2014, ADV NEUR IN, V27; Ruslan Salakhutdinov, 2016, Arxiv, DOI arXiv:1511.04119; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sigurdsson GA, 2017, PROC CVPR IEEE, P5650, DOI 10.1109/CVPR.2017.599; Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31; Simonyan K, 2014, ADV NEUR IN, V27; Simonyan Karen, 2015, INT C LEARN REPR; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Vaswani A, 2017, ADV NEUR IN, V30; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang JL, 2019, PROC CVPR IEEE, P4001, DOI 10.1109/CVPR.2019.00413; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668; Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678; Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059; Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48; Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19; Yang CY, 2020, PROC CVPR IEEE, P588, DOI 10.1109/CVPR42600.2020.00067; Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22; Zhang JR, 2020, IEEE T IMAGE PROCESS, V29, P5491, DOI 10.1109/TIP.2020.2985219; Zhang SW, 2020, AAAI CONF ARTIF INTE, V34, P12862; Zhao JJ, 2019, PROC CVPR IEEE, P9927, DOI 10.1109/CVPR.2019.01017; Zhao Y, 2020, INT J COMPUT VISION, V128, P74, DOI 10.1007/s11263-019-01211-2; Zheng ZX, 2021, IEEE T NEUR NET LEAR, V32, P334, DOI 10.1109/TNNLS.2020.2978613; Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49; Zhuang C, 2020, P IEEECVF C COMPUTER, P9563	95	0	0	9	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					7010	7028		10.1109/TPAMI.2021.3100277	http://dx.doi.org/10.1109/TPAMI.2021.3100277			19	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34314355	Green Submitted, Green Accepted			2022-12-18	WOS:000853875300082
J	Hu, YQ; Liu, XH; Li, SQ; Yu, Y				Hu, Yi-Qi; Liu, Xu-Hui; Li, Shu-Qiao; Yu, Yang			Cascaded Algorithm Selection With Extreme-Region UCB Bandit	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Algorithm selection; hyper-parameter tuning; AutoML; machine learning		AutoML aims at best configuring learning systems automatically. It contains core subtasks of algorithm selection and hyper-parameter tuning. Previous approaches considered searching in the joint hyper-parameter space of all algorithms, which forms a huge but redundant space and causes an inefficient search. We tackle this issue in a cascaded algorithm selection way, which contains an upper-level process of algorithm selection and a lower-level process of hyper-parameter tuning for algorithms. While the lower-level process employs an anytime tuning approach, the upper-level process is naturally formulated as a multi-armed bandit, deciding which algorithm should be allocated one more piece of time for the lower-level tuning. To achieve the goal of finding the best configuration, we propose the Extreme-Region Upper Confidence Bound (ER-UCB) strategy. Unlike UCB bandits that maximize the mean of feedback distribution, ER-UCB maximizes the extreme-region of feedback distribution. We first consider stationary distributions and propose the ER-UCB-S algorithm that has O(K ln n) regret upper bound with K arms and n trials. We then extend to non-stationary settings and propose the ER-UCB-N algorithm that has O(Kn(nu)) regret upper bound, where 2/3 < nu < 1. Finally, empirical studies on synthetic and AutoML tasks verify the effectiveness of ER-UCB-S/N by their outperformance in corresponding settings.	[Hu, Yi-Qi; Liu, Xu-Hui; Li, Shu-Qiao; Yu, Yang] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China; [Yu, Yang] Pazhou Lab, Guangzhou 510330, Guangdong, Peoples R China; [Yu, Yang] Polixir Ai, Nanjing 210046, Jiangsu, Peoples R China	Nanjing University; Pazhou Lab	Yu, Y (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.; Yu, Y (corresponding author), Pazhou Lab, Guangzhou 510330, Guangdong, Peoples R China.	huyq@lamda.nju.edu.cn; liuxh@lamda.nju.edu.cn; gavinlee.cn@outlook.com; yuy@nju.edu.cn			National Key R&D Program of China [2020AAA0107200, NSFC 61876077]; Collaborative Innovation Center of Novel Software Technology and Industrialization	National Key R&D Program of China; Collaborative Innovation Center of Novel Software Technology and Industrialization	This work was supported by the National Key R&D Program of China under Grants 2020AAA0107200, NSFC 61876077, and the Collaborative Innovation Center of Novel Software Technology and Industrialization.	Audibert Jean-Yves, 2010, P 23 ANN C LEARN THE, p2010a; Bellman R., 1961, ADAPTIVE CONTROL PRO; Bergstra J, 2012, J MACH LEARN RES, V13, P281; Bergstra James S, 2011, ADV NEURAL INFORM PR, P2546, DOI [10.5555/2986459.2986743, DOI 10.5555/2986459.2986743]; Biem A, 2003, PROC INT CONF DOC, P104; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Bugata P, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2633-y; Carpentier A, 2014, ADV NEUR IN, V27; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chistyakov G. P., 1999, J MATH SCI-U TOKYO, V93, P480; Cicirello V. A., 2005, PROC AAAI C ARTIF IN, P1355; Elsken T, 2019, J MACH LEARN RES, V20; Feurer M, 2015, ADV NEUR IN, V28; Guo XC, 2008, NEUROCOMPUTING, V71, P3211, DOI 10.1016/j.neucom.2008.04.027; Hu Y.-Q., 2019, PROC 28 INT JOINT C; Hu Y.-Q., 2020, PROC 24 EUR C ARTIF; Hu YQ, 2020, INT J MACH LEARN CYB, V11, P795, DOI 10.1007/s13042-020-01062-1; Hu YQ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2276; Hu YQ, 2019, AAAI CONF ARTIF INTE, P3846; Hu YQ, 2017, AAAI CONF ARTIF INTE, P2029; Jamieson K, 2016, JMLR WORKSH CONF PRO, V51, P240; Kleinbaum DG., 2002, LOGISTIC REGRESSION; Lindauer M, 2018, AAAI CONF ARTIF INTE, P1355; Liu H., 2019, PROC INT C LEARN REP; Ma B., 2021, FRONT COMPUT SCI-CHI, V15, P152; Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1023/A:1022643204877; Sen R, 2018, PR MACH LEARN RES, V80; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Wolpert D. H., 1995, NO FREE LUNCH THEORE; Yao X, 1999, P IEEE, V87, P1423, DOI 10.1109/5.784219; Ye HJ, 2021, INT J COMPUT VISION, V129, P1930, DOI 10.1007/s11263-020-01381-4; Yu Y, 2016, AAAI CONF ARTIF INTE, P2286; Zoph B., 2017, P1	35	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6782	6794		10.1109/TPAMI.2021.3094844	http://dx.doi.org/10.1109/TPAMI.2021.3094844			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34232866				2022-12-18	WOS:000853875300068
J	Lin, BQ; Zhu, Y; Long, YX; Liang, XD; Ye, QX; Lin, L				Lin, Bingqian; Zhu, Yi; Long, Yanxin; Liang, Xiaodan; Ye, Qixiang; Lin, Liang			Adversarial Reinforced Instruction Attacker for Robust Vision-Language Navigation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Navigation; Task analysis; Visualization; Robustness; Perturbation methods; Stairs; Natural languages; Vision-and-language navigation; adversarial attack; reinforcement learning; self-supervised learning		Language instruction plays an essential role in the natural language grounded navigation tasks. However, navigators trained with limited human-annotated instructions may have difficulties in accurately capturing key information from the complicated instruction at different timesteps, leading to poor navigation performance. In this paper, we exploit to train a more robust navigator which is capable of dynamically extracting crucial factors from the long instruction, by using an adversarial attacking paradigm. Specifically, we propose a Dynamic Reinforced Instruction Attacker (DR-Attacker), which learns to mislead the navigator to move to the wrong target by destroying the most instructive information in instructions at different timesteps. By formulating the perturbation generation as a Markov Decision Process, DR-Attacker is optimized by the reinforcement learning algorithm to generate perturbed instructions sequentially during the navigation, according to a learnable attack score. Then, the perturbed instructions, which serve as hard samples, are used for improving the robustness of the navigator with an effective adversarial training strategy and an auxiliary self-supervised reasoning task. Experimental results on both Vision-and-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks show the superiority of our proposed method over state-of-the-art methods. Moreover, the visualization analysis shows the effectiveness of the proposed DR-Attacker, which can successfully attack crucial information in the instructions at different timesteps. Code is available at https://github.com/expectorlin/DR-Attacker.	[Lin, Bingqian; Long, Yanxin; Liang, Xiaodan] Sun Yat Sen Univ, Shenzhen Campus, Shenzhen 510275, Peoples R China; [Zhu, Yi; Ye, Qixiang] Univ Chinese Acad Sci UCAS, Beijing 100049, Peoples R China; [Lin, Liang] Sun Yat Sen Univ, Guangzhou 510275, Peoples R China	Sun Yat Sen University; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Sun Yat Sen University	Liang, XD (corresponding author), Sun Yat Sen Univ, Shenzhen Campus, Shenzhen 510275, Peoples R China.	linbq6@mail2.sysu.edu.cn; zhu.yee@outlook.com; longyx9@mail2.sysu.edu.cn; liangxd9@mail.sysu.edu.cn; qxye@ucas.ac.cn; linliang@ieee.org			National Key R&D Program of China [2020AAA0109700]; National Natural Science Foundation of China (NSFC) [U19A2073, 61976233]; Guangdong Province Basic and Applied Basic Research (Regional Joint Fund-Key) [2019B1515120039]; Guangdong Outstanding Youth Fund [2021B1515020061]; Shenzhen Fundamental Research Program [RCYX20200714114642083, JCYJ20190807154211365]; Zhejiang Lab's Open Fund [2020AA3AB14]; CSIG Young Fellow Support Fund	National Key R&D Program of China; National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Guangdong Province Basic and Applied Basic Research (Regional Joint Fund-Key); Guangdong Outstanding Youth Fund; Shenzhen Fundamental Research Program; Zhejiang Lab's Open Fund; CSIG Young Fellow Support Fund	This work was supported in part by National Key R&D Program of China under Grant 2020AAA0109700, in part by the National Natural Science Foundation of China (NSFC) under Grants U19A2073 and 61976233, in part by the Guangdong Province Basic and Applied Basic Research (Regional Joint Fund-Key) under Grant 2019B1515120039, in part by the Guangdong Outstanding Youth Fund under Grant 2021B1515020061, in part by the Shenzhen Fundamental Research Program under Projects RCYX20200714114642083 and JCYJ20190807154211365, in part by the Zhejiang Lab's Open Fund under Grant 2020AA3AB14, and in part by the CSIG Young Fellow Support Fund.	Aishan Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P122, DOI 10.1007/978-3-030-58520-4_8; Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Andres Carvallo, 2020, Arxiv, DOI arXiv:2004.11157; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Cemgil T., 2020, PROC ICLR 8 INT C LE; Chen H, 2019, PROC CVPR IEEE, P12530, DOI 10.1109/CVPR.2019.01282; Cheng Y, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4324; Conneau A, 2017, PROC 2017 C EMPIR ME, P670, DOI [10.18653/v1/d17-1070, DOI 10.18653/V1/D17-1070]; Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359; Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020; Das A, 2018, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2018.00008; Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121; de Vries H, 2017, PROC CVPR IEEE, P4466, DOI 10.1109/CVPR.2017.475; Devlin J., 2018, P 2019 C N AM CHAPT, DOI [DOI 10.18653/V1/N19-1423, 10.18653/v1/N19-1423]; Ebrahimi J., 2018, P 27 INT C COMP LING, P653; Ebrahimi J, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P31; Eger S., 2019, TEXT PROCESSING HUMA, P1634; Feng J, 2019, ADV NEUR IN, V32; Fried D, 2018, ADV NEUR IN, V31; Hao Weituo, 2020, P IEEE CVF C COMP VI, P13134; Hataya Ryuichiro, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P1, DOI 10.1007/978-3-030-58595-2_1; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Ho D, 2019, PR MACH LEARN RES, V97; Jones E, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2752; Nguyen K, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P684; Li L., 2020, P 2020 C EMPIRICAL M, P6193; Li X., 2019, PROC C EMPIRICAL MET, P1494; Lim S, 2019, ADV NEUR IN, V32; Liu Xiaodong, 2020, ARXIV; Ma C.-Y., 2019, PROC ICLR 7 INT C LE; Ma CY, 2019, PROC CVPR IEEE, P6725, DOI 10.1109/CVPR.2019.00689; Mnih V, 2016, PR MACH LEARN RES, V48; Nguyen K, 2019, PROC CVPR IEEE, P12519, DOI 10.1109/CVPR.2019.01281; Pinto L, 2017, PR MACH LEARN RES, V70; Ren SH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1085; Salman Hadi, 2019, ADV NEURAL INFORM PR; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Tan Hao, 2019, P 2019 C N AM ASS CO, V1, P2610, DOI DOI 10.18653/V1/N19-1268; Thomason Jesse, 2020, C ROB LEARN, P394; Tsu-Jui Fu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P71, DOI 10.1007/978-3-030-58539-6_5; Wang B., 2021, PROC ICLR 9 INT C LE; Wang DL, 2019, PR MACH LEARN RES, V97; Wang Xin Eric, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P413, DOI 10.1007/978-3-030-58586-0_25; Wu Y., 2018, PROC ICLR INT C LEAR; Xin Wang, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6622, DOI 10.1109/CVPR.2019.00679; Yi Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10727, DOI 10.1109/CVPR42600.2020.01074; Yin F, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3386; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Yuankai Qi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9979, DOI 10.1109/CVPR42600.2020.01000; Zang Y., 2020, P 58 ANN M ASS COMP, P6066; Zhang HC, 2019, ADV NEUR IN, V32; Zhu C., 2019, PROC INT C LEARN REP	54	0	0	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					7175	7189		10.1109/TPAMI.2021.3097435	http://dx.doi.org/10.1109/TPAMI.2021.3097435			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34270414	Green Submitted			2022-12-18	WOS:000853875300092
J	Mostafavi, M; Nam, Y; Choi, J; Yoon, KJ				Mostafavi, Mohammad; Nam, Yeongwoo; Choi, Jonghyun; Yoon, Kuk-Jin			E2SRI: Learning to Super-Resolve Intensity Images From Events	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image reconstruction; Cameras; Videos; Spatial resolution; Streaming media; Stacking; Optical imaging; Color events; event-based vision; high dynamic range; image reconstruction; neuromorphic camera; super-resolution	RECONSTRUCTION	An event camera reports per-pixel intensity differences as an asynchronous stream of events with low latency, high dynamic range (HDR), and low power consumption. This stream of sparse/dense events limits the direct use of well-known computer vision applications for event cameras. Further applications of event streams to vision tasks that are sensitive to image quality issues, such as spatial resolution and blur, e.g., object detection, would benefit from a higher resolution of image reconstruction. Moreover, despite the recent advances in spatial resolution in event camera hardware, the majority of commercially available event cameras still have relatively low spatial resolutions when compared to conventional cameras. We propose an end-to-end recurrent network to reconstruct high-resolution, HDR, and temporally consistent grayscale or color frames directly from the event stream, and extend it to generate temporally consistent videos. We evaluate our algorithm on real-world and simulated sequences and verify that it reconstructs fine details of the scene, outperforming previous methods in quantitative quality measures. We further investigate how to (1) incorporate active pixel sensor frames (produced by an event camera) and events together in a complementary setting and (2) reconstruct images iteratively to create an even higher quality and resolution in the images.	[Mostafavi, Mohammad; Nam, Yeongwoo] Gwangju Inst Sci & Technol, Dept Elect Engn & Comp Sci, Gwangju 61005, South Korea; [Choi, Jonghyun] Gwangju Inst Sci & Technol, Artificial Intelligence Grad Sch, Gwangju 61005, South Korea; [Yoon, Kuk-Jin] Korea Adv Inst Sci & Technol, Dept Mech Engn, Daejeon 34141, South Korea	Gwangju Institute of Science & Technology (GIST); Gwangju Institute of Science & Technology (GIST); Korea Advanced Institute of Science & Technology (KAIST)	Yoon, KJ (corresponding author), Korea Adv Inst Sci & Technol, Dept Mech Engn, Daejeon 34141, South Korea.	mostafavi.isfahani@gmail.com; yeong-oo@gist.ac.kr; jhc@gist.ac.kr; kjyoon@kaist.ac.kr		Mostafavi Isfahani, Sayed Mohammad/0000-0002-5883-3844	National Research Foundation of Korea(NRF) - Korea Government (MSIT) [NRF-2018R1A2B3008640]; Institute of Information and Communications Technology Planning & Evaluation(IITP) - Korea Government (MSIT) [2020-0-00440]; NRF - Korea government (MSIT) [2019R1C1C1009283]; IITP - Korea government (MSIT) [2019-0-01842]	National Research Foundation of Korea(NRF) - Korea Government (MSIT)(National Research Foundation of KoreaMinistry of Science, ICT & Future Planning, Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); Institute of Information and Communications Technology Planning & Evaluation(IITP) - Korea Government (MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); NRF - Korea government (MSIT)(National Research Foundation of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); IITP - Korea government (MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea)	This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea Government (MSIT) under Grant NRF-2018R1A2B3008640 and the Institute of Information and Communications Technology Planning & Evaluation(IITP) grant funded by Korea Government (MSIT) under Grant 2020-0-00440, Development of Artificial Intelligence Technology that Continuously Improves Itself as the Situation Changes in the Real World. This work was also supported in part by NRF grant funded by the Korea government (MSIT) under Grant 2019R1C1C1009283 and IITP grant funded by the Korea government (MSIT) under Grant 2019-0-01842, Artificial Intelligence Graduate School Program (GIST).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bardow P, 2016, PROC CVPR IEEE, P884, DOI 10.1109/CVPR.2016.102; Barua S., 2016, P IEEE REAL TIM SYST, P1; Cook M, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P770, DOI 10.1109/IJCNN.2011.6033299; Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132; Forrest N. Iandola, 2016, Arxiv, DOI arXiv:1602.07360; Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413; Gehrig D, 2019, IEEE I CONF COMP VIS, P5632, DOI 10.1109/ICCV.2019.00573; Haris M, 2019, PROC CVPR IEEE, P3892, DOI 10.1109/CVPR.2019.00402; Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Irani M., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P115, DOI 10.1109/ICPR.1990.119340; Kim H., 2014, P BRIT MACH VIS C; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lai WS, 2018, LECT NOTES COMPUT SC, V11219, P179, DOI 10.1007/978-3-030-01267-0_11; Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8312, DOI 10.1109/CVPR42600.2020.00834; Mostafavi M, 2021, INT J COMPUT VISION, V129, P900, DOI 10.1007/s11263-020-01410-2; Mostafavi ISM, 2020, PROC CVPR IEEE, P2765, DOI 10.1109/CVPR42600.2020.00284; Mueggler E, 2017, INT J ROBOT RES, V36, P142, DOI 10.1177/0278364917691115; Munda G, 2018, INT J COMPUT VISION, V126, P1381, DOI 10.1007/s11263-018-1106-2; Rebecq H, 2019, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2019.00398; Rebecq H, 2021, IEEE T PATTERN ANAL, V43, P1964, DOI 10.1109/TPAMI.2019.2963386; Rebecq Henri, 2018, C ROB LEARN, P2; Ronneberger O., 2015, P C MED IMAGE COMPUT, P234, DOI DOI 10.1007/978-3-319-24574-4_28; Ruder M, 2016, LECT NOTES COMPUT SC, V9796, P26, DOI 10.1007/978-3-319-45886-1_3; Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693; Scheerlinck C, 2019, IEEE COMPUT SOC CONF, P1684, DOI 10.1109/CVPRW.2019.00215; Scheerlinck C, 2019, LECT NOTES COMPUT SC, V11365, P308, DOI 10.1007/978-3-030-20873-8_20; Stoffregen Timo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P534, DOI 10.1007/978-3-030-58583-9_32; Suzuki K, 2003, IEEE T PATTERN ANAL, V25, P1582, DOI 10.1109/TPAMI.2003.1251151; Taverni G, 2018, IEEE T CIRCUITS-II, V65, P677, DOI 10.1109/TCSII.2018.2824899; Tulyakov S, 2019, IEEE I CONF COMP VIS, P1527, DOI 10.1109/ICCV.2019.00161; Wang L, 2019, PROC CVPR IEEE, P10073, DOI 10.1109/CVPR.2019.01032; Wang Z, 2003, CONF REC ASILOMAR C, P1398; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhao Y, 2017, NEUROCOMPUTING, V226, P200, DOI 10.1016/j.neucom.2016.11.049; Zhu A. Z., 2018, IEEE ROBOT AUTOM LET, V3, P2032, DOI [DOI 10.1109/LRA.2018.2800793, 10.1109/lra.2018.2800793]; Zhu A. Z., 2018, P EUR C COMP VIS, P711	42	0	0	6	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6890	6909		10.1109/TPAMI.2021.3096985	http://dx.doi.org/10.1109/TPAMI.2021.3096985			20	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34260349				2022-12-18	WOS:000853875300074
J	Niu, W; Li, ZG; Ma, XL; Dong, PY; Zhou, G; Qian, XH; Lin, X; Wang, YZ; Ren, B				Niu, Wei; Li, Zhengang; Ma, Xiaolong; Dong, Peiyan; Zhou, Gang; Qian, Xuehai; Lin, Xue; Wang, Yanzhi; Ren, Bin			GRIM: A General, Real-Time Deep Learning Inference Framework for Mobile Devices Based on Fine-Grained Structured Weight Sparsity	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Compiler optimization; model compression; real-time inference; deep neural networks; mobile computing		It is appealing but challenging to achieve real-time deep neural network (DNN) inference on mobile devices, because even the powerful modern mobile devices are considered as "resource-constrained" when executing large-scale DNNs. It necessitates the sparse model inference via weight pruning, i.e., DNN weight sparsity, and it is desirable to design a new DNN weight sparsity scheme that can facilitate real-time inference onmobile devices while preserving a high sparse model accuracy. This paper designs a novel mobile inference acceleration framework GRIM that is General to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) and that achieves Real-time execution and high accuracy, leveraging fine-grained structured sparse model Inference and compiler optimizations for Mobiles. We start by proposing a new fine-grained structured sparsity scheme through the Block-based Column-Row (BCR) pruning. Based on this new fine-grained structured sparsity, our GRIM framework consists of two parts: (a) the compiler optimization and code generation for real-time mobile inference; and (b) the BCR pruning optimizations for determining pruning hyperparameters and performing weight pruning. We compare GRIM with Alibaba MNN, TVM, TensorFlow-Lite, a sparse implementation based on CSR, PatDNN, and ESE (a representative FPGA inference acceleration framework for RNNs), and achieve up to 14:08x speedup.	[Niu, Wei; Zhou, Gang; Ren, Bin] William & Mary, Dept Comp Sci, Williamsburg, VA 23185 USA; [Li, Zhengang; Ma, Xiaolong; Dong, Peiyan; Lin, Xue; Wang, Yanzhi] Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA; [Qian, Xuehai] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA	Northeastern University; University of Southern California	Niu, W (corresponding author), William & Mary, Dept Comp Sci, Williamsburg, VA 23185 USA.	wniu@email.wm.edu; li.zhen@northeastern.edu; ma.xiaol@husky.neu.edu; dong.pe@northeastern.edu; gzhou@cs.wm.edu; xuehai.qian@usc.edu; xue.lin@northeastern.edu; yanz.wang@northeastern.edu; bren@cs.wm.edu			National Science Foundation [CCF-2047516, CCF-1901378, CCF-1937500]; Army Research Office/Army Research Laboratory [W911NF-20-1-0167]; Jeffress Trust	National Science Foundation(National Science Foundation (NSF)); Army Research Office/Army Research Laboratory; Jeffress Trust	This work is partially funded by National Science Foundation under Grants CCF-2047516 (CAREER), CCF-1901378 and CCF-1937500, Army Research Office/Army Research Laboratory via grant W911NF-20-1-0167 (YIP) to Northeastern University, and Jeffress Trust awards in Interdisciplinary Research to William & Mary. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of NSF, Army Research Office, or Thomas F. and Kate Miller Jeffress Memorial Trust. Wei Niu and Zhengang Li contributed equally.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Aosen Wang, 2018, Arxiv, DOI arXiv:1809.02220; Bhattacharya S, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING AND COMMUNICATION WORKSHOPS (PERCOM WORKSHOPS); Bin Ren, 2019, Arxiv, DOI arXiv:1905.00571; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Bryan Catanzaro, 2014, Arxiv, DOI arXiv:1410.0759; Chen TQ, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P579; Chi-Keung Tang, 2016, Arxiv, DOI arXiv:1607.03250; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123, DOI DOI 10.1109/TWC.2016.2633262; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; developer nvidia, EXPLOITING AMPERE ST; Dong PY, 2020, DES AUT CON; Dong X, 2017, ADV NEUR IN, V30; Garofolo J. S., 1993, LINGUISTIC DATA CONS; Greathouse Joseph L., 2016, P 4 INT WORKSHOP OPE, DOI [10.1145/2909437.2909442, DOI 10.1145/2909437.2909442]; Han S, 2016, Harvard Yenching Ins, V101, P123; Han S, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P75, DOI 10.1145/3020078.3021745; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Hill P, 2017, 50TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P786, DOI 10.1145/3123939.3123970; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hong MY, 2016, SIAM J OPTIMIZ, V26, P337, DOI 10.1137/140990309; Hubara I, 2016, ADV NEUR IN, V29; Huynh LN, 2017, MOBISYS'17: PROCEEDINGS OF THE 15TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P82, DOI 10.1145/3081333.3081360; Jian Tang, 2019, Arxiv, DOI arXiv:1907.03141; Jiang X., 2020, P MACHINE LEARNING S, V2, P1, DOI 10.48550/arXiv.2002.12418; Kingma D.P, P 3 INT C LEARNING R; Lane, 2016, 2016 15 ACM IEEE INT, P1; Lane ND, 2017, IEEE PERVAS COMPUT, V16, P82, DOI 10.1109/MPRV.2017.2940968; Lane ND, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP 2015), P283, DOI 10.1145/2750858.2804262; Lavin A, 2016, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2016.435; Leng C., 2018, PROC 32 AAAI C ARTIF; Li H., 2017, INT C LEARN REPR; Li Z, 2019, INT S HIGH PERF COMP, P69, DOI 10.1109/HPCA.2019.00028; Lin DD, 2016, PR MACH LEARN RES, V48; Liu SC, 2018, MOBISYS'18: PROCEEDINGS OF THE 16TH ACM INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P389, DOI 10.1145/3210240.3210337; Liu SJ, 2018, PR MACH LEARN RES, V84; Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298; Liu Zhuang, 2018, ARXIV181005270; Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681; Ma XL, 2020, AAAI CONF ARTIF INTE, V34, P5117; Niu W, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P907, DOI 10.1145/3373376.3378534; Ota K, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3092831; Parashar A, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P27, DOI 10.1145/3079856.3080254; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren A, 2019, TWENTY-FOURTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXIV), P925, DOI 10.1145/3297858.3304076; tensorflow, US; Tian, 2019, P IEEE C COMP VIS PA, P2780, DOI DOI 10.1109/CVPR.2019.00289; Torrey L., 2010, HDB RES MACHINE LEAR, P242, DOI DOI 10.4018/978-1-60566-766-9.CH011; Wang S, 2018, PROCEEDINGS OF THE 2018 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS (FPGA'18), P11, DOI 10.1145/3174243.3174253; Wen W, 2016, ADV NEUR IN, V29; Xu MW, 2018, MOBICOM'18: PROCEEDINGS OF THE 24TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P129, DOI 10.1145/3241539.3241563; Yang M, 2018, INT SYM MVL, P180, DOI 10.1109/ISMVL.2018.00039; Yao SC, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P351, DOI 10.1145/3038912.3052577; Yunbin Deng, 2019, Proceedings of the SPIE, V10993, DOI 10.1117/12.2518469; Zhang CY, 2019, IEEE COMMUN SURV TUT, V21, P2224, DOI 10.1109/COMST.2019.2904897; Zhang T., ARXIV; Zhu XT, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3264; Zhuang ZW, 2018, ADV NEUR IN, V31	64	0	0	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6224	6239		10.1109/TPAMI.2021.3089687	http://dx.doi.org/10.1109/TPAMI.2021.3089687			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34133272	Green Submitted			2022-12-18	WOS:000853875300029
J	Qu, Y; Shao, ZZ; Qi, HR				Qu, Ying; Shao, Zhenzhou; Qi, Hairong			Non-Local Representation Based Mutual Affine-Transfer Network for Photorealistic Stylization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image color analysis; Feature extraction; Context modeling; Semantics; Image segmentation; Distortion; Data mining; Photorealistic stylization; non-local representation; mutual information; affine-transfer	PROBABILITY DENSITY-FUNCTION; COLOR; REGISTRATION	Photorealistic stylization aims to transfer the style of a reference photo onto a content photo in a natural fashion, such that the stylized image looks like a real photo taken by a camera. State-of-the-art methods stylize the image locally within each matched semantic region and are prone to global color inconsistency across semantic objects/parts, making the stylized image less photorealistic. To tackle the challenging issues, we propose a non-local representation scheme, constrained with a mutual affine-transfer network (NL-MAT). Through a dictionary-based decomposition, NL-MAT is able to successfully decouple matched non-local representations and color information of the image pair, such that the context correspondence between the image pair is incorporated naturally, which largely facilitates local style transfer in a global-consistent fashion. To the best of our knowledge, this is the first attempt to address the photorealistic stylization problem with a non-local representation scheme, such that no additional models or steps for semantic matching are required during stylization. Experimental results demonstrate that, the proposed method is able to generate photorealistic results with local style transfer while preserving both the spatial structure and global color consistency of the content image.	[Qu, Ying; Qi, Hairong] Univ Tennessee, Dept Elect Engn & Comp Sci, Adv Imaging & Collaborat Informat Proc Grp, Knoxville, TN 37996 USA; [Shao, Zhenzhou] Capital Normal Univ, Coll Informat Engn, Beijing Key Lab Light Weight Ind Robot & Safety V, Beijing 100048, Peoples R China	University of Tennessee System; University of Tennessee Knoxville; Capital Normal University	Shao, ZZ (corresponding author), Capital Normal Univ, Coll Informat Engn, Beijing Key Lab Light Weight Ind Robot & Safety V, Beijing 100048, Peoples R China.	yqu3@vols.utk.edu; zshao@cnu.edu.cn; hqi@utk.edu			Gonzalez Family Professorship; Beijing Nova Program of Science and Technology [191100001119075]	Gonzalez Family Professorship; Beijing Nova Program of Science and Technology	The authors would like to thank all the developers of the evaluated methods who kindly offer their codes or results, and the anonymous reviewers who have helped them greatly in improving the quality of this paper. This work was supported in part by Gonzalez Family Professorship, and in part by the Beijing Nova Program of Science and Technology under Grant 191100001119075.	An J, 2020, AAAI CONF ARTIF INTE, V34, P10443; Bae SM, 2006, ACM T GRAPHIC, V25, P637, DOI 10.1145/1141911.1141935; Belghazi MI, 2018, PR MACH LEARN RES, V80; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; DONSKER MD, 1983, COMMUN PUR APPL MATH, V36, P183, DOI 10.1002/cpa.3160360204; Dugas C, 2001, ADV NEUR IN, V13, P472; Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666; Freedman D, 2010, PROC CVPR IEEE, P287, DOI 10.1109/CVPR.2010.5540201; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Han J, 1995, LECT NOTES COMPUT SC, V930, P195; He MM, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3292482; Hjelm R.D., 2018, P INT C LEARN REPR; Huang S, 2019, IEEE T SIGNAL PROCES, V67, P1322, DOI 10.1109/TSP.2018.2889951; KUMARASWAMY P, 1980, J HYDROL, V46, P79, DOI 10.1016/0022-1694(80)90036-0; Laffont PY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366221; Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060; Li YJ, 2018, LECT NOTES COMPUT SC, V11207, P468, DOI 10.1007/978-3-030-01219-9_28; Li YJ, 2017, ADV NEUR IN, V30; Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683; Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740; Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452; Mechrez R., 2017, PROC BRIT MACH VIS C; Nalisnick E., 2017, PROC 5 INT C LEARN R; Nie F., 2010, ADV NEURAL INFORM PR, V1, P1813, DOI DOI 10.1007/978-3-319-10690-8_12; Omer I, 2004, PROC CVPR IEEE, P946; Pitie F, 2005, IEEE I CONF COMP VIS, P1434; Pitie F, 2007, COMPUT VIS IMAGE UND, V107, P123, DOI 10.1016/j.cviu.2006.11.011; Qu Y, 2018, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR.2018.00266; Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629; SETHURAMAN J, 1994, STAT SINICA, V4, P639; Woo J, 2015, IEEE T IMAGE PROCESS, V24, P757, DOI 10.1109/TIP.2014.2387019; Yoo J, 2019, IEEE I CONF COMP VIS, P9035, DOI 10.1109/ICCV.2019.00913; Zitova B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9	34	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					7046	7061		10.1109/TPAMI.2021.3095948	http://dx.doi.org/10.1109/TPAMI.2021.3095948			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34260345	Green Submitted			2022-12-18	WOS:000853875300084
J	Rasheed, AH; Romero, V; Bertails-Descoubes, F; Wuhrer, S; Franco, JS; Lazarus, A				Rasheed, Abdullah Haroon; Romero, Victor; Bertails-Descoubes, Florence; Wuhrer, Stefanie; Franco, Jean-Sebastien; Lazarus, Arnaud			A Visual Approach to Measure Cloth-Body and Cloth-Cloth Friction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Friction; Visualization; Protocols; Estimation; Data models; Physics; Force; Friction estimation; cloth simulation; deep learning; material estimation; inverse problem	COEFFICIENT; COLLISIONS	Measuring contact friction in soft-bodies usually requires a specialised physics bench and a tedious acquisition protocol. This makes the prospect of a purely non-invasive, video-based measurement technique particularly attractive. Previous works have shown that such a video-based estimation is feasible for material parameters using deep learning, but this has never been applied to the friction estimation problem which results in even more subtle visual variations. Because acquiring a large dataset for this problem is impractical, generating it from simulation is the obvious alternative. However, this requires the use of a frictional contact simulator whose results are not only visually plausible, but physically-correct enough to match observations made at the macroscopic scale. In this paper, which is an extended version of our former work A. H. Rasheed, V. Romero, F. Bertails-Descoubes, S. Wuhrer, J.-S. Franco, and A Lazarus, "Learning to measure the static friction coefficient in cloth contact," in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020, pp. 9909-9918, we propose to our knowledge the first non-invasive measurement network and adjoining synthetic training dataset for estimating cloth friction at contact, for both cloth-hard body and cloth-cloth contacts. To this end we build a protocol for validating and calibrating a state-of-the-art frictional contact simulator, in order to produce a reliable dataset. We furthermore show that without our careful calibration procedure, the training fails to provide accurate estimation results on real data. We present extensive results on a large acquired test set of several hundred real video sequences of cloth in friction, which validates the proposed protocol and its accuracy.	[Rasheed, Abdullah Haroon; Romero, Victor; Bertails-Descoubes, Florence; Wuhrer, Stefanie; Franco, Jean-Sebastien] Univ Grenoble Alpes, INRIA, Grenoble INP, CNRS,LJK, F-38400 Grenoble, France; [Lazarus, Arnaud] Sorbonne Univ, CNRS, Inst Jean Rond Alembert, UMR 7190, F-75005 Paris, France	Centre National de la Recherche Scientifique (CNRS); Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); Inria; Centre National de la Recherche Scientifique (CNRS); CNRS - Institute for Engineering & Systems Sciences (INSIS); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite	Rasheed, AH (corresponding author), Univ Grenoble Alpes, INRIA, Grenoble INP, CNRS,LJK, F-38400 Grenoble, France.	abdullah-haroon.rasheed@inria.fr; victor.romero-gramegna@inria.fr; florence.descoubes@inria.fr; stefanie.wuhrer@inria.fr; jean-sebastien.franco@inria.fr; Arnaud.Lazarus@upmc.fr	Lazarus, Arnaud/A-6645-2013	Lazarus, Arnaud/0000-0002-4985-1127; ROMERO, Victor/0000-0001-6697-9918	ERC GEM [StG-2014-639139]	ERC GEM	This work was supported in part by the ERC GEM under Grant StG-2014-639139. A special thanks to Mickael Ly and Laurence Boissieux for their help with final animations and renderings.	Abadi M., 2015, TENSORFLOW LARGE SCA; Amontons G, 1699, MEMOIRES LACADEMIE R, P257; Bhat K. S., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P37; Bouman KL, 2013, IEEE I CONF COMP VIS, P1984, DOI 10.1109/ICCV.2013.455; Brandao M, 2016, IEEE-RAS INT C HUMAN, P428, DOI 10.1109/HUMANOIDS.2016.7803311; Bridson R, 2002, ACM T GRAPHIC, V21, P594, DOI 10.1145/566570.566623; Candelier R, 2011, SENSORS-BASEL, V11, P7934, DOI 10.3390/s110807934; Chen Z., 2013, ACM T GRAPHIC, V32, P1; Chevalier NR, 2017, COLLOID SURFACE B, V159, P924, DOI 10.1016/j.colsurfb.2017.08.048; Chollet F., 2015, KERAS; Clyde D, 2017, ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION (SCA 2017), DOI 10.1145/3099564.3099577; Courtel R, 1964, NAVAL ENG J, V76, P451; Dahl P. R, 1968, TOR0158310718 AER CO; Davis A, 2017, IEEE T PATTERN ANAL, V39, P732, DOI 10.1109/TPAMI.2016.2622271; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Dreby EC, 1943, J RES NAT BUR STAND, V31, P237, DOI 10.6028/jres.031.012; Erleben K, 2020, COMPUT GRAPH FORUM, V39, P450, DOI 10.1111/cgf.13885; Gondal M. W, 2019, PROC INT C NEURAL IN, P15714; Haroon Rasheed Abdullah, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9909, DOI 10.1109/CVPR42600.2020.00993; James S, 2019, PROC CVPR IEEE, P12819, DOI 10.1109/CVPR.2019.01291; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kunitomo S., 2010, P ACM SIGGRAPH POST; Li J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201308; Liang JB, 2019, ADV NEUR IN, V32; Liu F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899408; Liu X, 2008, MEAS SCI TECHNOL, V19, DOI 10.1088/0957-0233/19/8/084007; Mao N., 2016, PROC 90 TEXTILE I WO, P125; Miguel E, 2012, COMPUT GRAPH FORUM, V31, P519, DOI 10.1111/j.1467-8659.2012.03031.x; Miguel E, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508389; Moorthy R. R., 2015, J TEXTILE APPAREL TE, V9, P1; Narain R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366171; Pabst S., 2009, P ACM SIGGRAPH EUR S, P149; Sano TG, 2017, PHYS REV LETT, V118, DOI 10.1103/PhysRevLett.118.178001; Selle A, 2009, IEEE T VIS COMPUT GR, V15, P339, DOI 10.1109/TVCG.2008.79; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Syllebranque C, 2008, VISUAL COMPUT, V24, P963, DOI 10.1007/s00371-008-0273-5; Tobin J, 2017, IEEE INT C INT ROBOT, P23; Varley L., 1961, J TEXT I, V52, pP255, DOI [https://doi.org/10.1080/19447016108688509, DOI 10.1080/19447016108688509]; Wang B, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766911; Wu J., 2015, ADV NEURAL INFORM PR; Wu J., 2016, PROC BRIT MACH VIS C, P391, DOI [10.5244/C.30.39, DOI 10.5244/C.30.39]; Yang S, 2017, IEEE I CONF COMP VIS, P4393, DOI 10.1109/ICCV.2017.470; Yang S, 2016, IEEE T VIS COMPUT GR, V22, P2122, DOI 10.1109/TVCG.2015.2505285; Yuan WZ, 2017, PROC CVPR IEEE, P4494, DOI 10.1109/CVPR.2017.478; Zeiler Matthew D, 2012, ARXIV12125701; Zhang H, 2016, LECT NOTES COMPUT SC, V9908, P808, DOI 10.1007/978-3-319-46493-0_49	46	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6683	6694		10.1109/TPAMI.2021.3097547	http://dx.doi.org/10.1109/TPAMI.2021.3097547			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34270415	Green Submitted			2022-12-18	WOS:000853875300062
J	Souri, Y; Fayyaz, M; Minciullo, L; Francesca, G; Gall, J				Souri, Yaser; Fayyaz, Mohsen; Minciullo, Luca; Francesca, Gianpiero; Gall, Juergen			Fast Weakly Supervised Action Segmentation Using Mutual Consistency	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Training; Viterbi algorithm; Streaming media; Artificial neural networks; Task analysis; Predictive models; Markov processes; Video understanding; action segmentation; weakly supervised learning		Action segmentation is the task of predicting the actions for each frame of a video. As obtaining the full annotation of videos for action segmentation is expensive, weakly supervised approaches that can learn only from transcripts are appealing. In this paper, we propose a novel end-to-end approach for weakly supervised action segmentation based on a two-branch neural network. The two branches of our network predict two redundant but different representations for action segmentation and we propose a novel mutual consistency (MuCon) loss that enforces the consistency of the two redundant representations. Using the MuCon loss together with a loss for transcript prediction, our proposed approach achieves the accuracy of state-of-the-art approaches while being 14 times faster to train and 20 times faster during inference. The MuCon loss proves beneficial even in the fully supervised setting.	[Souri, Yaser; Fayyaz, Mohsen; Gall, Juergen] Univ Bonn, D-53113 Bonn, Germany; [Minciullo, Luca; Francesca, Gianpiero] Toyota Motor Europe, B-1140 Brussels, Belgium	University of Bonn; Toyota Motor Corporation	Souri, Y (corresponding author), Univ Bonn, D-53113 Bonn, Germany.	souri@iai.uni-bonn.de; fayyaz@iai.uni-bonn.de; Luca.Minciullo@toyota-europe.com; gianpiero.francesca@toyota-europe.com; gall@iai.uni-bonn.de			Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) [GA 1927/4-2]; ERC [ARCA 677650]	Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)(German Research Foundation (DFG)); ERC(European Research Council (ERC)European Commission)	This work was supported in part by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Grant GA 1927/4-2 (FOR 2535 Anticipating Human Behavior) and in part by ERC under Grant ARCA 677650. Yaser Souri and Mohsen Fayyaz contributed equally to this article.	Abu Farha Y, 2019, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2019.00369; Andrew Zisserman, 2017, Arxiv, DOI arXiv:1705.06950; Bojanowski P, 2014, LECT NOTES COMPUT SC, V8693, P628, DOI 10.1007/978-3-319-10602-1_41; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chang CY, 2019, PROC CVPR IEEE, P3541, DOI 10.1109/CVPR.2019.00366; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Diba Ali, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P593, DOI 10.1007/978-3-030-58558-7_35; Diba A., 2018, CVPR WORKSH; Ding L, 2018, PROC CVPR IEEE, P6508, DOI 10.1109/CVPR.2018.00681; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Friedman M, 1937, J AM STAT ASSOC, V32, P675, DOI 10.2307/2279372; Furnari A, 2018, J VIS COMMUN IMAGE R, V52, P1, DOI 10.1016/j.jvcir.2018.01.019; Gaidon A, 2013, IEEE T PATTERN ANAL, V35, P2782, DOI 10.1109/TPAMI.2013.65; Gehring J, 2017, PR MACH LEARN RES, V70; Huang DA, 2016, LECT NOTES COMPUT SC, V9908, P137, DOI 10.1007/978-3-319-46493-0_9; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Karaman S., 2014, ECCV THUMOS WORKSH; Kuehne H, 2020, IEEE T PATTERN ANAL, V42, P765, DOI 10.1109/TPAMI.2018.2884469; Kuehne H, 2017, COMPUT VIS IMAGE UND, V163, P78, DOI 10.1016/j.cviu.2017.06.004; Kuehne H, 2016, IEEE WINT CONF APPL; Kuehne H, 2014, PROC CVPR IEEE, P780, DOI 10.1109/CVPR.2014.105; Lambert B, 2019, EDIT DISTANCE; Lea C, 2017, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2017.113; Lea C, 2016, LECT NOTES COMPUT SC, V9907, P36, DOI 10.1007/978-3-319-46487-9_3; Li J, 2019, IEEE I CONF COMP VIS, P6252, DOI 10.1109/ICCV.2019.00634; Li Shi-Jie, 2020, IEEE Trans Pattern Anal Mach Intell, VPP, DOI 10.1109/TPAMI.2020.3021756; Oord A.V.D., 2016, SSW; Poleg Y, 2014, PROC CVPR IEEE, P2537, DOI 10.1109/CVPR.2014.325; Richard A, 2018, PROC CVPR IEEE, P7386, DOI 10.1109/CVPR.2018.00771; Richard A, 2017, PROC CVPR IEEE, P1273, DOI 10.1109/CVPR.2017.140; Richard A, 2016, PROC CVPR IEEE, P3131, DOI 10.1109/CVPR.2016.341; Rohrbach M, 2012, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2012.6247801; Simonyan K., 2014, PROC 27 INT C NEURAL, P568; Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216; Spriggs EH, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2009.5204354; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Vaswani A, 2017, ADV NEUR IN, V30; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wei Z, 2018, PROC 32 INT C NEURAL, P3511; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Souri Y, 2020, Arxiv, DOI arXiv:2005.09743; Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317	45	0	0	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6196	6208		10.1109/TPAMI.2021.3089127	http://dx.doi.org/10.1109/TPAMI.2021.3089127			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34125671	Green Submitted			2022-12-18	WOS:000853875300027
J	Tan, QY; Zhang, LX; Yang, J; Lai, YK; Gao, L				Tan, Qingyang; Zhang, Ling-Xiao; Yang, Jie; Lai, Yu-Kun; Gao, Lin			Variational Autoencoders for Localized Mesh Deformation Component Analysis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Strain; Shape; Three-dimensional displays; Principal component analysis; Geometry; Convolution; Solid modeling; 3D meshes; variational autoencoder; graph convolution; sparsity regularization		Spatially localized deformation components are very useful for shape analysis and synthesis in 3D geometry processing. Several methods have recently been developed, with an aim to extract intuitive and interpretable deformation components. However, these techniques suffer from fundamental limitations especially for meshes with noise or large-scale nonlinear deformations, and may not always be able to identify important deformation components. In this paper we propose a mesh-based variational autoencoder architecture that is able to cope with meshes with irregular connectivity and nonlinear deformations, assuming that the analyzed dataset contains meshes with the same vertex connectivity, which is common for deformation analysis. To help localize deformations, we introduce sparse regularization in this framework, along with spectral graph convolutional operations. Through modifying the regularization formulation and allowing dynamic change of sparsity ranges, we improve the visual quality and reconstruction ability of the extracted deformation components. Our system also provides a nonlinear approach to reconstruction of meshes using the extracted basis, which is more effective than the current linear combination approach. As an important application of localized deformation components and a novel approach on its own, we further develop a neural shape editing method, achieving shape editing and deformation component extraction in a unified framework, and ensuring plausibility of the edited shapes. Extensive experiments show that our method outperforms state-of-the-art methods in both qualitative and quantitative evaluations. We also demonstrate the effectiveness of our method for neural shape editing.	[Tan, Qingyang; Zhang, Ling-Xiao; Yang, Jie; Gao, Lin] Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100864, Peoples R China; [Tan, Qingyang] Univ Maryland, College Pk, MD 20742 USA; [Yang, Jie; Gao, Lin] Univ Chinese Acad Sci, Beijing 100049, Peoples R China; [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF10 3AT, Wales	Chinese Academy of Sciences; Institute of Computing Technology, CAS; University System of Maryland; University of Maryland College Park; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Cardiff University	Gao, L (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100864, Peoples R China.	qytan@cs.umd.edu; zhanglingxiao@ict.ac.cn; yangjie01@ict.ac.cn; LaiY4@cardiff.ac.uk; gaolin@ict.ac.cn			Newton Advanced Fellowship from the Royal Society [NAFnR2n192151]; Youth Innovation Promotion Association, CAS [2019108]; National Natural Science Foundation of China [62061136007, 61872440]; Beijing Municipal Natural Science Foundation [L182016]; Tencent AI Lab Rhino-Bird Focused Research Program [JR202024]	Newton Advanced Fellowship from the Royal Society; Youth Innovation Promotion Association, CAS; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Municipal Natural Science Foundation(Beijing Natural Science Foundation); Tencent AI Lab Rhino-Bird Focused Research Program	This work was supported in part by the Newton Advanced Fellowship from the Royal Society under Grant NAFnR2n192151, in part by the Youth Innovation Promotion Association, CAS under Grant 2019108, in part by the National Natural Science Foundation of China under Grants 62061136007 and 61872440, in part by the Beijing Municipal Natural Science Foundation under Grant L182016, and in part by the Tencent AI Lab Rhino-Bird Focused Research Program under Grant JR202024. Qingyang Tan and LingXiao Zhang contributed equally to this work.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Alexa M, 2000, COMPUT GRAPH FORUM, V19, pC411, DOI 10.1111/1467-8659.00433; Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207; Ba J., 2017, P 3 INT C LEARN REPR; Bengio Y., 2013, ARXIV; Bernard Florian, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13823, DOI 10.1109/CVPR42600.2020.01384; Bernard F, 2019, IEEE I CONF COMP VIS, P10283, DOI 10.1109/ICCV.2019.01038; Bernard F, 2016, PROC CVPR IEEE, P5629, DOI 10.1109/CVPR.2016.607; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Boscaini D, 2016, COMPUT GRAPH FORUM, V35, P431, DOI 10.1111/cgf.12844; Bruna J, 2013, PROC INT C LEARN REP; Cao C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766943; CATTELL RB, 1966, MULTIVAR BEHAV RES, V1, P245, DOI 10.1207/s15327906mbr0102_10; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Crane K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516977; Defferrard M, 2016, ADV NEUR IN, V29; Duvenaud David K, 2015, P NIPS; Gao L, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356488; Gao L, 2021, IEEE T VIS COMPUT GR, V27, P2085, DOI 10.1109/TVCG.2019.2941200; Gao L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2908736; Gao L, 2012, SCI CHINA INFORM SCI, V55, P983, DOI 10.1007/s11432-012-4574-y; Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29; Guo K, 2015, ACM T GRAPHIC, V35, DOI 10.1145/2835487; Huang ZC, 2014, COMPUT GRAPH FORUM, V33, P239, DOI 10.1111/cgf.12492; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Kavan L, 2010, COMPUT GRAPH FORUM, V29, P327, DOI 10.1111/j.1467-8659.2009.01602.x; Kolen JF, 2001, FIELD GUIDE DYNAMICA, P237, DOI [10.1109/9780470544037.ch14, DOI 10.1109/9780470544037.CH14]; Li J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073637; Li YY, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818071; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Mo KC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356527; Nash C, 2017, COMPUT GRAPH FORUM, V36, P1, DOI 10.1111/cgf.13240; Neumann T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508417; Niepert M, 2016, PR MACH LEARN RES, V48; Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025; Sharma A, 2016, LECT NOTES COMPUT SC, V9915, P236, DOI 10.1007/978-3-319-49409-8_20; Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802; Sinha A, 2017, PROC CVPR IEEE, P791, DOI 10.1109/CVPR.2017.91; Sorkine Olga, 2007, P EUROGRAPHICS ACM S, V4, P109, DOI 10.1145/1281991.1282006; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Sumner RW, 2005, ACM T GRAPHIC, V24, P488, DOI 10.1145/1073204.1073218; Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736; Tan QY, 2018, AAAI CONF ARTIF INTE, P2452; Tena JR, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964971; Tretschk Edgar, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P293, DOI 10.1007/978-3-030-58517-4_18; Tulsiani S, 2017, PROC CVPR IEEE, P1466, DOI 10.1109/CVPR.2017.160; Vasa L, 2011, IEEE T VIS COMPUT GR, V17, P220, DOI 10.1109/TVCG.2010.38; Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696; Wang YP, 2017, COMPUT GRAPH FORUM, V36, P247, DOI 10.1111/cgf.13076; Williams L., 1990, Computer Graphics, V24, P235, DOI 10.1145/97880.97906; Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801; Yang YP, 2014, 2014 2ND INTERNATIONAL CONFERENCE ON 3D VISION, VOL. 2, P41, DOI 10.1109/3DV.2014.47; Yi L, 2017, PROC CVPR IEEE, P6584, DOI 10.1109/CVPR.2017.697; Zhang L, 2004, ACM T GRAPHIC, V23, P548, DOI 10.1145/1015706.1015759; Zollhofer M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601165; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	58	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6297	6310		10.1109/TPAMI.2021.3085887	http://dx.doi.org/10.1109/TPAMI.2021.3085887			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34061742	Green Accepted			2022-12-18	WOS:000853875300034
J	Tang, H; Zhu, XT; Chen, K; Jia, K; Chen, CLP				Tang, Hui; Zhu, Xiatian; Chen, Ke; Jia, Kui; Chen, C. L. Philip			Towards Uncovering the Intrinsic Data Structures for Unsupervised Domain Adaptation Using Structurally Regularized Deep Clustering	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Semantics; Image segmentation; Task analysis; Benchmark testing; Training; Data models; Adaptation models; Domain adaptation; deep clustering; inductive learning; image classification; semantic segmentation		Unsupervised domain adaptation (UDA) is to learn classification models that make predictions for unlabeled data on a target domain, given labeled data on a source domain whose distribution diverges from the target one. Mainstream UDA methods strive to learn domain-aligned features such that classifiers trained on the source features can be readily applied to the target ones. Although impressive results have been achieved, these methods have a potential risk of damaging the intrinsic data structures of target discrimination, raising an issue of generalization particularly for UDA tasks in an inductive setting. To address this issue, we are motivated by a UDA assumption of structural similarity across domains, and propose to directly uncover the intrinsic target discrimination via constrained clustering, where we constrain the clustering solutions using structural source regularization that hinges on the very same assumption. Technically, we propose a hybrid model of Structurally Regularized Deep Clustering, which integrates the regularized discriminative clustering of target data with a generative one, and we thus term our method as H-SRDC. Our hybrid model is based on a deep clustering framework that minimizes the Kullback-Leibler divergence between the distribution of network prediction and an auxiliary one, where we impose structural regularization by learning domain-shared classifier and cluster centroids. By enriching the structural similarity assumption, we are able to extend H-SRDC for a pixel-level UDA task of semantic segmentation. We conduct extensive experiments on seven UDA benchmarks of image classification and semantic segmentation. With no explicit feature alignment, our proposed H-SRDC outperforms all the existing methods under both the inductive and transductive settings. We make our implementation codes publicly available at https://github.com/huitangtang/H-SRDC.	[Tang, Hui; Chen, Ke; Jia, Kui] South China Univ Technol, Sch Elect & Inforat Engn, Guangzhou 510641, Peoples R China; [Zhu, Xiatian] Univ Surrey, Ctr Vis Speech & Signal Proc CVSSP, Guildford GU2 7XH, Surrey, England; [Chen, C. L. Philip] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Peoples R China	South China University of Technology; University of Surrey; South China University of Technology	Jia, K (corresponding author), South China Univ Technol, Sch Elect & Inforat Engn, Guangzhou 510641, Peoples R China.	eehuitang@mail.scut.edu.cn; zhuxt@gmail.com; chenk@scut.edu.cn; kuijia@scut.edu.cn; philip.chen@ieee.org			National Natural Science Foundation of China [61771201]; Program for Guangdong Introducing Innovative and Enterpreneurial Teams [2017ZT07X183]; Guangdong R&D Key Project of China [2019B010155001]; Microsoft Research Asia	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Program for Guangdong Introducing Innovative and Enterpreneurial Teams; Guangdong R&D Key Project of China; Microsoft Research Asia(Microsoft)	This work was supported in part by the National Natural Science Foundation of China under Grant 61771201, in part by the Program for Guangdong Introducing Innovative and Enterpreneurial Teams under Grant 2017ZT07X183, in part by the Guangdong R&D Key Project of China under Grant 2019B010155001, and in part by Microsoft Research Asia.	[Anonymous], IMAGECLEF DA DATASET; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Ben-David Shai, 2007, NEURIPS, P7; Chang WL, 2019, PROC CVPR IEEE, P1900, DOI 10.1109/CVPR.2019.00200; Chang WG, 2019, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2019.00753; Chapelle O, 2005, P INT WORKSH ART INT, V2005, P57; Chen CQ, 2019, PROC CVPR IEEE, P627, DOI 10.1109/CVPR.2019.00072; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen QC, 2018, PROC CVPR IEEE, P7976, DOI 10.1109/CVPR.2018.00832; Chen X, 2016, ADV NEUR IN, V29; Chen XY, 2019, PR MACH LEARN RES, V97; Cicek S, 2019, IEEE I CONF COMP VIS, P1416, DOI 10.1109/ICCV.2019.00150; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Cui SH, 2020, PROC CVPR IEEE, P3940, DOI 10.1109/CVPR42600.2020.00400; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng ZJ, 2019, IEEE I CONF COMP VIS, P9943, DOI 10.1109/ICCV.2019.01004; Dizaji KG, 2017, IEEE I CONF COMP VIS, P5747, DOI 10.1109/ICCV.2017.612; Ganin Y, 2016, J MACH LEARN RES, V17; Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36; Ghifary M, 2014, LECT NOTES ARTIF INT, V8862, P898, DOI 10.1007/978-3-319-13560-1_76; Grandvalet Yves, 2004, NIPS, P529; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hoffman J, 2018, PR MACH LEARN RES, V80; Hui Tang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8722, DOI 10.1109/CVPR42600.2020.00875; HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440; Jabi M., 2018, ARXIV; Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503; Kim M, 2019, PROC CVPR IEEE, P4375, DOI 10.1109/CVPR.2019.00451; Kingma D.P., 2015, INT C LEARN REPR ICL; Krause Andreas, 2010, ADV NEURAL INFORM PR, V23, P5; Kurmi VK, 2019, PROC CVPR IEEE, P491, DOI 10.1109/CVPR.2019.00058; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee J, 2019, PR MACH LEARN RES, V97; Li HF, 2004, 2004 IEEE COMPUTATIONAL SYSTEMS BIOINFORMATICS CONFERENCE, PROCEEDINGS, P142; Long MS, 2018, ADV NEUR IN, V31; Long MS, 2019, IEEE T PATTERN ANAL, V41, P3071, DOI 10.1109/TPAMI.2018.2868685; Long MS, 2016, ADV NEUR IN, V29; Luo YW, 2019, IEEE I CONF COMP VIS, P6777, DOI 10.1109/ICCV.2019.00688; Luo YW, 2019, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2019.00261; Luo YC, 2018, PROC CVPR IEEE, P8896, DOI 10.1109/CVPR.2018.00927; Mansour Yishay, 2009, PROC 22 ANN C LEARN; Mengxue Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13933, DOI 10.1109/CVPR42600.2020.01395; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pan YW, 2019, PROC CVPR IEEE, P2234, DOI 10.1109/CVPR.2019.00234; Pei ZY, 2018, AAAI CONF ARTIF INTE, P3934; Peng XC, 2018, IEEE COMPUT SOC CONF, P2102, DOI 10.1109/CVPRW.2018.00271; Rastrow A., 2010, PROC HUM LANG TECHNO, P190; Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7; Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352; Roy S, 2019, PROC CVPR IEEE, P9463, DOI 10.1109/CVPR.2019.00970; Rozantsev A, 2019, IEEE T PATTERN ANAL, V41, P801, DOI 10.1109/TPAMI.2018.2814042; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392; Saminger-Platz S., 2017, P INT C LEARN REPR I; Shi Y., 2012, P 29 INT C INT C MAC, P1275; Shu R., 2018, PROC INT C LEARN REP; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vaswani A, 2017, ADV NEUR IN, V30; Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572; Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083; Xie JY, 2016, PR MACH LEARN RES, V48; Xu RJ, 2019, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2019.00151; Yang LX, 2019, IEEE I CONF COMP VIS, P6449, DOI 10.1109/ICCV.2019.00654; Zadrozny B., 2004, P 21 INT C MACH LEAR, DOI 10.1145/1015330.1015425; Zhang WC, 2018, PROC CVPR IEEE, P3801, DOI 10.1109/CVPR.2018.00400; Zhang YB, 2019, PROC CVPR IEEE, P5026, DOI 10.1109/CVPR.2019.00517; Zhang Y, 2020, IEEE T PATTERN ANAL, V42, P1823, DOI 10.1109/TPAMI.2019.2903401; Zhang YC, 2019, PR MACH LEARN RES, V97; Zhao H, 2019, PR MACH LEARN RES, V97; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	75	0	0	8	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6517	6533		10.1109/TPAMI.2021.3087830	http://dx.doi.org/10.1109/TPAMI.2021.3087830			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34106846	Green Submitted			2022-12-18	WOS:000853875300048
J	Tang, JP; Han, XG; Tan, MK; Tong, X; Jia, K				Tang, Jiapeng; Han, Xiaoguang; Tan, Mingkui; Tong, Xin; Jia, Kui			SkeletonNet: A Topology-Preserving Solution for Learning Mesh Reconstruction of Object Surfaces From RGB Images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Surface reconstruction; Shape; Image reconstruction; Topology; Skeleton; Task analysis; Three-dimensional displays; Surface reconstruction learning from RGB images; skeleton; mesh deformation; implicit surface field	STEREO; ALGORITHM	This paper focuses on the challenging task of learning 3D object surface reconstructions from RGB images. Existing methods achieve varying degrees of success by using different surface representations. However, they all have their own drawbacks, and cannot properly reconstruct the surface shapes of complex topologies, arguably due to a lack of constraints on the topological structures in their learning frameworks. To this end, we propose to learn and use the topology-preserved, skeletal shape representation to assist the downstream task of object surface reconstruction from RGB images. Technically, we propose the novel SkeletonNet design that learns a volumetric representation of a skeleton via a bridged learning of a skeletal point set, where we use parallel decoders each responsible for the learning of points on 1D skeletal curves and 2D skeletal sheets, as well as an efficient module of globally guided subvolume synthesis for a refined, high-resolution skeletal volume; we present a differentiable Point2Voxel layer to make SkeletonNet end-to-end and trainable. With the learned skeletal volumes, we propose two models, the Skeleton-Based Graph Convolutional Neural Network (SkeGCNN) and the Skeleton-Regularized Deep Implicit Surface Network (SkeDISN), which respectively build upon and improve over the existing frameworks of explicit mesh deformation and implicit field learning for the downstream surface reconstruction task. We conduct thorough experiments that verify the efficacy of our proposed SkeletonNet. SkeGCNN and SkeDISN outperform existing methods as well, and they have their own merits when measured by different metrics. Additional results in generalized task settings further demonstrate the usefulness of our proposed methods. We have made our implementation code publicly available at https://github.com/tangjiapeng/SkeletonNet.	[Tang, Jiapeng; Jia, Kui] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Guangdong, Peoples R China; [Tang, Jiapeng; Jia, Kui] Pazhou Lab, Guangzhou 510335, Peoples R China; [Tang, Jiapeng; Jia, Kui] Peng Cheng Lab, Shenzhen 518066, Guangdong, Peoples R China; [Han, Xiaoguang] Chinese Univ Hong Kong, Shenzhen 518172, Peoples R China; [Han, Xiaoguang] Shenzhen Res Inst Big Data, Shenzhen 518172, Peoples R China; [Tan, Mingkui] South China Univ Technol, Sch Software Engn, Guangzhou 510006, Guangdong, Peoples R China; [Tong, Xin] Microsoft Res Asia, Beijing 100080, Peoples R China	South China University of Technology; Pazhou Lab; Peng Cheng Laboratory; Chinese University of Hong Kong, Shenzhen; South China University of Technology; Microsoft; Microsoft Research Asia	Jia, K (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Guangdong, Peoples R China.	msjptang@mail.scut.edu.cn; hanxiaoguang@cuhk.edu.cn; mingkuitan@scut.edu.cn; xtong@microsoft.com; kuijia@scut.edu.cn			National Natural Science Foundation of China [61771201, 61629101, 61902334]; Program for Guangdong Introducing Innovative and Enterpreneurial Teams [2017ZT07X183]; Guangdong R&D Key Project of China [2019B010155001]; Key Area R&D Program of Guangdong Province [2018B030338001]; National Key R&D Program of China [2018YFB1800800]; Guangdong Research Project [2017ZT07X152]; Shenzhen Key Laboratory Fund [ZDSYS201707251409055]; Microsoft Research Asia	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Program for Guangdong Introducing Innovative and Enterpreneurial Teams; Guangdong R&D Key Project of China; Key Area R&D Program of Guangdong Province; National Key R&D Program of China; Guangdong Research Project; Shenzhen Key Laboratory Fund; Microsoft Research Asia(Microsoft)	This work was supported in part by the National Natural Science Foundation of China under Grants 61771201, 61629101, and 61902334, in part by the Program for Guangdong Introducing Innovative and Enterpreneurial Teams under Grant 2017ZT07X183, in part by the Guangdong R&D Key Project of China under Grant 2019B010155001, in part by the Key Area R&D Program of Guangdong Province under Grant 2018B030338001, in part by the National Key R&D Program of China under Grant 2018YFB1800800, in part by Guangdong Research Project under Grant 2017ZT07X152, in part by Shenzhen Key Laboratory Fund under Grant ZDSYS201707251409055, and in part by Microsoft Research Asia. Jiapeng Tang, Xiaoguang Han contributed equally to this work.	Achlioptas P, 2018, PR MACH LEARN RES, V80; Alec Jacobson, 2020, Arxiv, DOI arXiv:2011.01437; Ali Mahdavi-Amiri, 2020, Arxiv, DOI arXiv:2007.04883; Amos Gropp, 2020, Arxiv, DOI arXiv:2002.10099; Anders Eriksson, 2019, Arxiv, DOI arXiv:1901.06802; Angel X. Chang, 2015, Arxiv, DOI arXiv:1512.03012; Angjoo Kanazawa, 2019, Arxiv, DOI arXiv:1905.05172; Attali D, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P13, DOI 10.1109/ICIP.1996.560357; Atzmon M, 2020, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR42600.2020.00264; Bane C, 2017, INT CONF 3D VISION, P412, DOI 10.1109/3DV.2017.00054; Blum H., 1967, S MODELS PERCEPTION; Bucksch A, 2010, VISUAL COMPUT, V26, P1283, DOI 10.1007/s00371-010-0520-4; Campbell NDF, 2008, LECT NOTES COMPUT SC, V5302, P766, DOI 10.1007/978-3-540-88682-2_58; Chazal F, 2005, GRAPH MODELS, V67, P304, DOI 10.1016/j.gmod.2005.01.002; Chen ZQ, 2020, PROC CVPR IEEE, P42, DOI 10.1109/CVPR42600.2020.00012; Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609; Lin C, 2021, Arxiv, DOI arXiv:2012.00230; Chibane J, 2020, PROC CVPR IEEE, P6968, DOI 10.1109/CVPR42600.2020.00700; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Cornea ND, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P95; Daniel Cohen-Or, 2020, Arxiv, DOI arXiv:2005.11084; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; FIELD DA, 1988, COMMUN APPL NUMER M, V4, P709, DOI 10.1002/cnm.1630040603; Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Han XG, 2017, IEEE I CONF COMP VIS, P85, DOI 10.1109/ICCV.2017.19; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; Hu JW, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P774; Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461913; Jack Dominic, 2018, P AS C COMP VIS, P317; Jiabao Lei, 2020, Arxiv, DOI arXiv:2002.06597; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Jian Chang, 2020, Arxiv, DOI arXiv:2002.12212; Jian Chang, 2020, Arxiv, DOI arXiv:2010.07428; Jiang L, 2018, LECT NOTES COMPUT SC, V11212, P820, DOI 10.1007/978-3-030-01237-3_49; Kar A, 2017, ADV NEUR IN, V30; Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li P, 2015, ACM T GRAPHIC, V35, DOI 10.1145/2753755; Lin CH, 2018, AAAI CONF ARTIF INTE, P7114; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029; Merrell P, 2007, IEEE I CONF COMP VIS, P1221; Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459; Miklos B, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778838; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Oechsle M, 2020, INT CONF 3D VISION, P452, DOI 10.1109/3DV50981.2020.00055; Oechsle M, 2019, IEEE I CONF COMP VIS, P4530, DOI 10.1109/ICCV.2019.00463; Pan JY, 2019, IEEE I CONF COMP VIS, P9963, DOI 10.1109/ICCV.2019.01006; Pan JY, 2018, INT CONF 3D VISION, P719, DOI 10.1109/3DV.2018.00087; Pan YL, 2019, COMPUT AIDED GEOM D, V71, P16, DOI 10.1016/j.cagd.2019.04.007; Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025; Peng Songyou, 2020, ARXIV; Pinheiro PO, 2019, IEEE I CONF COMP VIS, P7637, DOI 10.1109/ICCV.2019.00773; Pontes JK, 2019, LECT NOTES COMPUT SC, V11361, P365, DOI 10.1007/978-3-030-20887-5_23; Riegler G, 2017, INT CONF 3D VISION, P57, DOI 10.1109/3DV.2017.00017; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Ronneberger O., 2015, P MEDICAL IMAGE COMP, P234; Sharf A, 2007, COMPUT GRAPH FORUM, V26, P323, DOI 10.1111/j.1467-8659.2007.01054.x; Siddiqi K, 2008, COMPUT IMAGING VIS, V37, P1, DOI 10.1007/978-1-4020-8658-8; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Soltani AA, 2017, PROC CVPR IEEE, P2511, DOI 10.1109/CVPR.2017.269; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tagliasacchi A, 2012, COMPUT GRAPH FORUM, V31, P1735, DOI 10.1111/j.1467-8659.2012.03178.x; Tagliasacchi A, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531377; Tang JP, 2019, PROC CVPR IEEE, P4536, DOI 10.1109/CVPR.2019.00467; Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230; Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20; Tola E, 2012, MACH VISION APPL, V23, P903, DOI 10.1007/s00138-011-0346-8; Wang PS, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275050; Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608; Wang WY, 2019, PROC CVPR IEEE, P1038, DOI 10.1109/CVPR.2019.00113; Wang WY, 2019, Arxiv, DOI arXiv:1905.10711; Williams F, 2019, PROC CVPR IEEE, P10122, DOI 10.1109/CVPR.2019.01037; Wu SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818073; Yang BR, 2020, COMPUT AIDED GEOM D, V80, DOI 10.1016/j.cagd.2020.101874; Yariv L., 2020, ADV NEURAL INF PROCE, V33; Yin KX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201288; Zach C, 2007, IEEE I CONF COMP VIS, P1213; Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783	86	0	0	9	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6454	6471		10.1109/TPAMI.2021.3087358	http://dx.doi.org/10.1109/TPAMI.2021.3087358			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34101584	Green Submitted			2022-12-18	WOS:000853875300044
J	Tosatto, S; Carvalho, J; Peters, J				Tosatto, Samuele; Carvalho, Joao; Peters, Jan			Batch Reinforcement Learning With a Nonparametric Off-Policy Policy Gradient	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Mathematical model; Estimation; Kernel; Reinforcement learning; Monte Carlo methods; Task analysis; Closed-form solutions; Reinforcement learning; policy gradient; nonparametric estimation	ITERATION	Off-policy reinforcement learning (RL) holds the promise of better data efficiency as it allows sample reuse and potentially enables safe interaction with the environment. Current off-policy policy gradient methods either suffer from high bias or high variance, delivering often unreliable estimates. The price of inefficiency becomes evident in real-world scenarios such as interaction-driven robot learning, where the success of RL has been rather limited, and a very high sample cost hinders straightforward application. In this paper, we propose a nonparametric Bellman equation, which can be solved in closed form. The solution is differentiable w.r.t the policy parameters and gives access to an estimation of the policy gradient. In this way, we avoid the high variance of importance sampling approaches, and the high bias of semi-gradient methods. We empirically analyze the quality of our gradient estimate against state-of-the-art methods, and show that it outperforms the baselines in terms of sample efficiency on classical control tasks.	[Tosatto, Samuele] Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2R3, Canada; [Carvalho, Joao; Peters, Jan] Tech Univ Darmstadt, FG Intelligent Autonomous Syst, D-64289 Darmstadt, Germany	University of Alberta; Technical University of Darmstadt	Tosatto, S (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2R3, Canada.	tosatto@ualberta.ca; joao.carvalho@tu-darmstadt.de; mail@jan-peters.net	Peters, Jan/D-5068-2009	Peters, Jan/0000-0002-5266-8091	Bosch-Forschungsstiftung program; European Union [640554]; German Federal Ministry of Education and Research (BMBF) [16SV798 (KoBo34)]	Bosch-Forschungsstiftung program; European Union(European Commission); German Federal Ministry of Education and Research (BMBF)(Federal Ministry of Education & Research (BMBF))	This work was supported by Bosch-Forschungsstiftung program, European Union's Horizon 2020 research and innovation program under Grant 640554 (SKILLS4ROBOTS), and the German Federal Ministry of Education and Research (BMBF) project 16SV798 (KoBo34).	Adith Swaminathan, 2019, Arxiv, DOI arXiv:1904.08473; Alec Radford, 2017, Arxiv, DOI arXiv:1707.06347; Andrea Bonarini, 2020, Arxiv, DOI arXiv:2001.01102; Antoniou A., 2007, PROC NEURAL INF PROC, P1, DOI 10.1007/978-0-387-71107-2_1; Aviral Kumar, 2019, Arxiv, DOI arXiv:1906.00949; Baird L., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P30; Bo Dai, 2019, Arxiv, DOI arXiv:1912.02074; Borrelli F., 2017, P 40 IEEE C DECISION; Chelsea Finn, 2020, Arxiv, DOI arXiv:2005.13239; Christian R. Shelton, 2013, Arxiv, DOI arXiv:1301.2310; Chua K, 2018, ADV NEUR IN, V31; Degris T., 2012, ARXIV; Dulac-Arnold G, 2020, PROC 9 INT C LEARN R; Engel Y., 2005, P 22 INT C MACH LEAR, P201; Ernst D, 2005, J MACH LEARN RES, V6, P503; FAN JQ, 1992, J AM STAT ASSOC, V87, P998, DOI 10.2307/2290637; Fu J., 2020, PREPRINT; George Tucker, 2019, Arxiv, DOI arXiv:1911.11361; Greg Brockman, 2016, Arxiv, DOI arXiv:1606.01540; Haarnoja T, 2018, PR MACH LEARN RES, V80; Imani E, 2018, ADV NEUR IN, V31; Jan Peters, 2020, Arxiv, DOI arXiv:2001.10972; Jie T., 2010, ADV NEURAL INFORM PR, P1000; Kakade S, 2002, ADV NEUR IN, V14, P1531; Kim K.-E, 2001, TRAIM2001003 MIT; Kober J., 2009, PROC NEURAL INF PROC, P849; Kroemer O, 2012, IEEE INT CONF ROBOT, P2605, DOI 10.1109/ICRA.2012.6224957; Kroemer O. B., 2011, P ADV NEUR INF PROC, P1719; Lillicrap T. P, 2015, ARXIV, DOI DOI 10.48550/ARXIV.1509.02971; Liu Q., 2018, ADV NEURAL INFORM PR, P5356; Lowrey K., 2018, ARXIV; Lu T, 2018, ADV NEUR IN, V31; Metelli AM, 2018, ADV NEUR IN, V31; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nadaraya E.A., 1964, THEOR PROBAB APPL+, V9, P141, DOI [DOI 10.1137/1109020, 10.1137/1109020]; Nota C., 2020, P 19 INT C AUT AG MU, P939; Ormoneit D, 2002, MACH LEARN, V49, P161, DOI 10.1023/A:1017928328829; Peshkin L., 2002, ARXIV; Peters J, 2010, AAAI CONF ARTIF INTE, P1607; Riedmiller M, 2005, LECT NOTES ARTIF INT, V3720, P317, DOI 10.1007/11564096_32; Rubinstein R.Y., 2016, SIMULATION MONTE CAR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Shelton C. R., 2001, P 17 C UNC ART INT, P496; Silver D, 2014, PR MACH LEARN RES, V32; Sutton RS, 2016, J MACH LEARN RES, V17; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Taylor Gavin, 2009, P 26 ANN INT C MACHI, P1017; Thomas PS, 2014, PR MACH LEARN RES, V32; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Tosatto S., 2020, PROC 23 INT ARTIF IN, P167; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Watson G.S., 1964, SANKHYA SER A, V26, P359, DOI DOI 10.2307/25049340; White M, 2017, PR MACH LEARN RES, V70; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xu X, 2007, IEEE T NEURAL NETWOR, V18, P973, DOI 10.1109/TNN.2007.899161	62	0	0	3	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					5996	6010		10.1109/TPAMI.2021.3088063	http://dx.doi.org/10.1109/TPAMI.2021.3088063			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34106848	Green Submitted			2022-12-18	WOS:000853875300014
J	Wan, YY; Zhang, LJ				Wan, Yuanyu; Zhang, Lijun			Efficient Adaptive Online Learning via Frequent Directions	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Online learning; frequent directions; adaptive subgradient methods	ANOMALY DETECTION; ALGORITHMS	By employing time-varying proximal functions, adaptive subgradient methods (ADAGRAD) have improved the regret bound and been widely used in online learning and optimization. However, ADAGRAD with full matrix proximal functions (ADA-FULL) cannot handle large-scale problems due to the impractical O(d(3)) time and O(d(2)) space complexities, though it has better performance when gradients are correlated. In this paper, we propose two efficient variants of ADA-FULL via a matrix sketching technique called frequent directions (FD). The first variant named as ADA-FD directly utilizes FD to maintain and manipulate low-rank matrices, which reduces the space and time complexities to O(tau d) and O(tau(2)d) respectively, where d is the dimensionality and tau << d is the sketching size. The second variant named as ADA-FFD further adopts a doubling trick to accelerate FD used in ADA-FD, which reduces the average time complexity to O(tau d) while only doubles the space complexity of ADA-FD. Theoretical analysis reveals that the regret of ADA-FD and ADA-FFD is close to that of ADA-FULL as long as the outer product matrix of gradients is approximately low-rank. Experimental results demonstrate the efficiency and effectiveness of our algorithms.	[Wan, Yuanyu; Zhang, Lijun] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China	Nanjing University	Zhang, LJ (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.	wanyy@lamda.nju.edu.cn; zhanglj@lamda.nju.edu.cn			NSFC [61976112]; NSFC-NRF Joint Research Project [61861146001]; JiangsuSF [BK20200064]	NSFC(National Natural Science Foundation of China (NSFC)); NSFC-NRF Joint Research Project; JiangsuSF	This work was supported in part by the NSFC under Grant 61976112, in part by the NSFC-NRF Joint Research Project under Grant 61861146001, and JiangsuSF under Grant BK20200064. A preliminary version of this paper was presented at the 27th International Joint Conference on Artificial Intelligence in 2018 [1].	Agarwal D, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P173, DOI 10.1145/2556195.2556252; Ba J., 2017, P 3 INT C LEARN REPR; Brand M, 2006, LINEAR ALGEBRA APPL, V415, P20, DOI 10.1016/j.laa.2005.07.021; Busa-Fekete R., 2015, PROC INT C NEURAL IN, P595; Cao LL, 2017, FRONT COMPUT SCI-CHI, V11, P276, DOI 10.1007/s11704-016-5171-9; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Charikar M, 2004, THEOR COMPUT SCI, V312, P3, DOI 10.1016/S0304-3975(03)00400-6; Chechik G, 2010, J MACH LEARN RES, V11, P1109; Dhillon IS, 2008, NIPS, P761, DOI 10.5555/2981780.2981875; Drineas P, 2006, SIAM J COMPUT, V36, P132, DOI 10.1137/S0097539704442684; Duchi J. C., 2010, COLT, P14; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Freund Y, 1999, MACH LEARN, V37, P277, DOI 10.1023/A:1007662407062; Gao W., 2013, P 30 INT C MACHINE L, P906; Gao XY, 2014, AAAI CONF ARTIF INTE, P1206; Ghashami M., 2014, P 25 ANN ACM SIAM S, P707, DOI [DOI 10.1137/1.9781611973402.53, 10.1137/1.9781611973402.53]; Ghashami M, 2016, SIAM J COMPUT, V45, P1762, DOI 10.1137/15M1009718; HAGER WW, 1989, SIAM REV, V31, P221, DOI 10.1137/1031049; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Jin C., 2016, ADV NEURAL INFORM PR, P4520; Kakade S. M., 2008, ICML 2008, P440; Kaski S, 1998, IEEE WORLD CONGRESS ON COMPUTATIONAL INTELLIGENCE, P413, DOI 10.1109/IJCNN.1998.682302; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Krummenacher G., 2016, PROC INT C NEURAL IN, P1750; Laxhammar R, 2014, IEEE T PATTERN ANAL, V36, P1158, DOI 10.1109/TPAMI.2013.172; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liberty E, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P581, DOI 10.1145/2487575.2487623; Lois B, 2015, IEEE INT SYMP INFO, P1826, DOI 10.1109/ISIT.2015.7282771; LUO H, 2016, ADV NEURAL INFORM PR, P902; Mairal J, 2010, J MACH LEARN RES, V11, P19; McMahan HB, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1222; MISRA J, 1982, SCI COMPUT PROGRAM, V2, P143, DOI 10.1016/0167-6423(82)90012-0; Netzer Yuval, 2011, NEURIPS WORKSH, V2, P6; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Tsagkatakis G, 2011, IEEE T CIRC SYST VID, V21, P1810, DOI 10.1109/TCSVT.2011.2133970; Wan Y Y, 2020, P 37 INT C MACHINE L, P9818; Wan YY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2748; Wan YY, 2018, LECT NOTES ARTIF INT, V10938, P403, DOI 10.1007/978-3-319-93037-4_32; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060; Xiao L., 2009, ADV NEURAL INFORM PR, Vvol 22, P2116; Ying Yiming, 2016, ADV NEURAL INFORM PR, P451; Yuan Y, 2015, IEEE T CYBERNETICS, V45, P562, DOI 10.1109/TCYB.2014.2330853; Zhang L J, 2013, P 30 INT C MACHINE L, P621; Zhang L J, 2012, P 26 AAAI C ARTIFICI, P1219; Zhang LJ, 2018, ADV NEUR IN, V31; Zhang LJ, 2016, PR MACH LEARN RES, V48; Zhao RB, 2016, INT CONF ACOUST SPEE, P2662, DOI 10.1109/ICASSP.2016.7472160; Zinkevich M., 2003, P 20 INT C MACH LEAR, P928, DOI 10.5555/3041838	51	0	0	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6910	6923		10.1109/TPAMI.2021.3096880	http://dx.doi.org/10.1109/TPAMI.2021.3096880			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34260347				2022-12-18	WOS:000853875300075
J	Wang, C; Liu, YL; Wang, YR; Li, XC; Wang, MN				Wang, Chen; Liu, Yinlong; Wang, Yiru; Li, Xuechen; Wang, Manning			Efficient and Outlier-Robust Simultaneous Pose and Correspondence Determination by Branch-and-Bound and Transformation Decomposition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Cameras; Pose estimation; Search problems; Computer vision; Complexity theory; Time complexity; Simultaneous pose and correspondence determination; global optimization; transformation decomposition; branch-and-bound	GLOBAL OPTIMIZATION; CAMERA PARAMETERS; PERSPECTIVE; ALGORITHM; ACCURATE	Estimating the pose of a calibrated camera relative to a 3D point set from one image is an important task in computer vision. Perspective-n-Point algorithms are often used if perfect 2D-3D correspondences are known. However, it is difficult to determine 2D-3D correspondences perfectly, and then the simultaneous pose and correspondence determination problem is needed to be solved. Early methods aimed to solve this problem by local optimization. Recently, several new methods are proposed to globally solve this problem by using branch-and-bound (BnB) method, but they tend to be slow because the time complexity of the BnB-based methods is exponential to the dimensionality of the parameter space, and they directly search the 6D parameter space. In this paper, we propose to decompose the joint searching into two separate searching processes by introducing a rotation invariant feature (RIF). Specifically, we construct RIFs from the original 3D and 2D point sets and search for the globally optimal translation to match these two RIFs first. Then, the original 3D point set is translated and matched with the 2D point set to find a globally optimal rotation. Experiments on challenging data show that the proposed method outperforms state-of-the-art methods in terms of both speed and accuracy.	[Wang, Chen; Liu, Yinlong; Wang, Yiru; Li, Xuechen; Wang, Manning] Fudan Univ, Sch Basic Med Sci, Digital Med Res Ctr, Shanghai 200032, Peoples R China; [Liu, Yinlong] Tech Univ Munich, Dept Informat, D-85748 Munich, Germany	Fudan University; Technical University of Munich	Wang, MN (corresponding author), Fudan Univ, Sch Basic Med Sci, Digital Med Res Ctr, Shanghai 200032, Peoples R China.	wangchenl7@fudan.edu.cn; yinlong.liu@tum.de; yiruwang18@fudan.edu.cn; lixuechen@fudan.edu.cn; mnwang@fudan.edu.cn		Manning, Wang/0000-0002-9255-3897				Aanaes H, 2012, INT J COMPUT VISION, V97, P18, DOI 10.1007/s11263-011-0473-8; ABIDI MA, 1995, IEEE T PATTERN ANAL, V17, P534, DOI 10.1109/34.391388; Aftab K, 2015, IEEE WINT CONF APPL, P480, DOI 10.1109/WACV.2015.70; Aiger D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360684; Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487; Brown M, 2015, IEEE I CONF COMP VIS, P2111, DOI 10.1109/ICCV.2015.244; Bustos AP, 2018, IEEE T PATTERN ANAL, V40, P2868, DOI 10.1109/TPAMI.2017.2773482; Bustos AP, 2016, IEEE T PATTERN ANAL, V38, P2227, DOI 10.1109/TPAMI.2016.2517636; Cai ZP, 2018, LECT NOTES COMPUT SC, V11216, P699, DOI 10.1007/978-3-030-01258-8_42; Campbell D, 2019, PROC CVPR IEEE, P11788, DOI 10.1109/CVPR.2019.01207; Campbell D, 2020, IEEE T PATTERN ANAL, V42, P328, DOI 10.1109/TPAMI.2018.2848650; Cao Y., 2018, PROC IEEE WINTER C A, P947; Chen Wang, 2019, Arxiv, DOI arXiv:1812.11307; Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236; Chum O, 2008, IEEE T PATTERN ANAL, V30, P1472, DOI 10.1109/TPAMI.2007.70787; Civera J, 2010, J FIELD ROBOT, V27, P609, DOI 10.1002/rob.20345; David P, 2004, INT J COMPUT VISION, V59, P259, DOI 10.1023/B:VISI.0000025800.10423.1f; DEMENTHON DF, 1995, INT J COMPUT VISION, V15, P123, DOI 10.1007/BF01450852; Enqvist O., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P264, DOI 10.1109/ICCVW.2011.6130252; Ferraz L, 2014, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2014.71; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Gold S, 1996, IEEE T PATTERN ANAL, V18, P377, DOI 10.1109/34.491619; Gold S, 1998, PATTERN RECOGN, V31, P1019, DOI 10.1016/S0031-3203(98)80010-1; Guo F, 2017, IEEE GLOB CONF SIG, P408; HARALICK RM, 1994, INT J COMPUT VISION, V13, P331, DOI 10.1007/BF02028352; Hartley RI, 2009, INT J COMPUT VISION, V82, P64, DOI 10.1007/s11263-008-0186-9; Hesch JA, 2011, IEEE I CONF COMP VIS, P383, DOI 10.1109/ICCV.2011.6126266; HORAUD R, 1989, COMPUT VISION GRAPH, V47, P33, DOI 10.1016/0734-189X(89)90052-2; Hu ZY, 2002, IEEE T PATTERN ANAL, V24, P550, DOI 10.1109/34.993561; Kahl F, 2008, INT J COMPUT VISION, V79, P271, DOI 10.1007/s11263-007-0117-1; Kendall A, 2017, PROC CVPR IEEE, P6555, DOI 10.1109/CVPR.2017.694; Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336; Kneip L., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2969, DOI 10.1109/CVPR.2011.5995464; Kneip L, 2014, LECT NOTES COMPUT SC, V8689, P127, DOI 10.1007/978-3-319-10590-1_9; Kneip L, 2013, IEEE INT CONF ROBOT, P3770, DOI 10.1109/ICRA.2013.6631107; Kuramachi R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (ROBIO), P176, DOI 10.1109/ROBIO.2015.7418763; LAND AH, 1960, ECONOMETRICA, V28, P497, DOI 10.2307/1910129; Larsson V., 2016, PROC BRIT MACH VIS C; Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6; Liu YL, 2019, IEEE T IMAGE PROCESS, V28, P2599, DOI 10.1109/TIP.2018.2887207; Liu YL, 2018, LECT NOTES COMPUT SC, V11216, P460, DOI 10.1007/978-3-030-01258-8_28; Marchand E, 2016, IEEE T VIS COMPUT GR, V22, P2633, DOI 10.1109/TVCG.2015.2513408; Melekhov I, 2017, LECT NOTES COMPUT SC, V10617, P675, DOI 10.1007/978-3-319-70353-4_57; Moore R.E., 2009, INTRO INTERVAL ANAL; Moore R.E., 1979, METHODS APPL INTERVA; Moreno-Noguer F, 2008, LECT NOTES COMPUT SC, V5303, P405, DOI 10.1007/978-3-540-88688-4_30; Namin ST, 2015, IEEE WINT CONF APPL, P1006, DOI 10.1109/WACV.2015.139; Olsson C, 2006, INT C PATT RECOG, P5; Olsson C, 2011, LECT NOTES COMPUT SC, V6688, P524, DOI 10.1007/978-3-642-21227-7_49; PENNA MA, 1991, PATTERN RECOGN, V24, P533, DOI 10.1016/0031-3203(91)90019-2; Shotton J, 2013, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2013.377; Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405; Yu L, 2007, IEEE INT C BIOINFORM, P9, DOI 10.1109/BIBM.2007.19; Zeisl B, 2013, IEEE I CONF COMP VIS, P2808, DOI 10.1109/ICCV.2013.349; Zheng YQ, 2013, IEEE I CONF COMP VIS, P2344, DOI 10.1109/ICCV.2013.291; Zhou FQ, 2013, OPT LASER ENG, V51, P197, DOI 10.1016/j.optlaseng.2012.10.012; Zhou HY, 2019, IEEE T PATTERN ANAL, V41, P3022, DOI 10.1109/TPAMI.2018.2871832	57	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6924	6938		10.1109/TPAMI.2021.3096842	http://dx.doi.org/10.1109/TPAMI.2021.3096842			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34260346				2022-12-18	WOS:000853875300076
J	Xu, J; Ma, J; Gao, XS; Zhu, ZX				Xu, Ju; Ma, Jin; Gao, Xuesong; Zhu, Zhanxing			Adaptive Progressive Continual Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Optimization; Bayes methods; Training; Reinforcement learning; Knowledge engineering; Complexity theory; Machine learning; adaptive progressive network framework; continual learning; Bayesian optimization; reinforcement learning; neural networks		Continual learning paradigm learns from a continuous stream of tasks in an incremental manner and aims to overcome the notorious issue: the catastrophic forgetting. In this work, we propose a new adaptive progressive network framework including two models for continual learning: Reinforced Continual Learning (RCL) and Bayesian Optimized Continual Learning with Attention mechanism (BOCL) to solve this fundamental issue. The core idea of this framework is to dynamically and adaptively expand the neural network structure upon the arrival of new tasks. RCL and BOCL employ reinforcement learning and Bayesian optimization to achieve it, respectively. An outstanding advantage of our proposed framework is that it will not forget the knowledge that has been learned through adaptively controlling the architecture. We propose effective ways of employing the learned knowledge in the two methods to control the size of the network. RCL employs previous knowledge directly while BOCL selectively utilizes previous knowledge (e.g., feature maps of previous tasks) via attention mechanism. The experiments on variants of MNIST, CIFAR-100 and Sequence of 5-Datasets demonstrate that our methods outperform the state-of-the-art in preventing catastrophic forgetting and fitting new tasks better under the same or less computing resource.	[Xu, Ju; Ma, Jin] Peking Univ, Ctr Data Sci, Beijing 100871, Peoples R China; [Gao, Xuesong] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China; [Gao, Xuesong] Hisense Co Ltd, State Key Lab Digital Multimedia Technol, Qingdao 266071, Shandong, Peoples R China; [Gao, Xuesong] Shandong Univ, Sch Informat Sci & Engn, Qingdao 266510, Shandong, Peoples R China; [Zhu, Zhanxing] Beijing Inst Big Data Res, Beijing 100124, Peoples R China	Peking University; Tianjin University; Hisense; Shandong University	Zhu, ZX (corresponding author), Beijing Inst Big Data Res, Beijing 100124, Peoples R China.	xuju@pku.edu.cn; ma_jin@pku.edu.cn; gaoxuesong@tju.edu.cn; zhanxing.zhu@pku.edu.cn			National Defense Basic Scientific Research Project, Ministry of Industry and Information Technology, China [JCKY2018204C004]; Beijing Nova Program [202072]; National Natural Science Foundation of China [61806009, 61932001, 2019BD005]	National Defense Basic Scientific Research Project, Ministry of Industry and Information Technology, China; Beijing Nova Program(Beijing Municipal Science & Technology Commission); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Defense Basic Scientific Research Project, Ministry of Industry and Information Technology, China under Grant JCKY2018204C004, Beijing Nova Program under Grant 202072, National Natural Science Foundation of China under Grants 61806009, 61932001 and PKUBaidu Funding 2019BD005. Ju Xu and Jin Ma contributed equally to this work.	Aljundi R, 2017, PROC CVPR IEEE, P7120, DOI 10.1109/CVPR.2017.753; Rusu AA, 2016, Arxiv, DOI arXiv:1606.04671; Bello I, 2017, PR MACH LEARN RES, V70; Bergstra James S, 2011, ADV NEURAL INFORM PR, P2546, DOI [10.5555/2986459.2986743, DOI 10.5555/2986459.2986743]; Chaudhry Arslan, 2019, P INT C LEARN REPR I, P2; Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667; Chen Z, 2018, SYNTHESIS LECT ARTIF, V12, P1, DOI [10.2200/S00737ED1V01Y201610AIM033, DOI 10.2200/S00737ED1V01Y201610AIM033]; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Ebrahimi S., 2020, ARXIV200309553; Fernando Chrisantha, 2017, ARXIV; Han Xiao, 2017, Arxiv, DOI arXiv:1708.07747; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Jaderberg M, 2015, ADV NEUR IN, V28; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081; Lopez-Paz D, 2017, ADV NEUR IN, V30; Mallya A, 2018, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2018.00810; Mnih V, 2014, ADV NEUR IN, V27; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Nguyen C. V., 2017, ARXIV171010628; Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587; Serra J, 2018, PR MACH LEARN RES, V80; Silver Daniel L, 2002, C CAN SOC COMP STUD, P90, DOI DOI 10.1007/3-540-47922-8_8; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Snoek J., 2012, ADV NEURAL INF PROCE; Sutton R. S., 1999, REINFORCEMENT LEARNI; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Thrun S., 1995, INTELLIGENT ROBOTS S, P201, DOI DOI 10.1016/B978-044482250-5/50015-3; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xu J, 2018, ADV NEUR IN, V31; Yang Z, 2016, P 2016 C N AM CHAPTE, P1480; Yoon Jaehong, 2018, INT C LEARN REPR ICL; Yu LT, 2017, AAAI CONF ARTIF INTE, P2852; Zoph B., 2017, P1	41	0	0	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6715	6728		10.1109/TPAMI.2021.3095064	http://dx.doi.org/10.1109/TPAMI.2021.3095064			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34232867				2022-12-18	WOS:000853875300064
J	Xu, XG; Chen, YC; Tao, X; Jia, JY				Xu, Xiaogang; Chen, Ying-Cong; Tao, Xin; Jia, Jiaya			Text-Guided Human Image Manipulation via Image-Text Shared Space	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Aerospace electronics; Task analysis; Faces; Training; Natural languages; Image reconstruction; Feature extraction; Human image manipulation; adversarial generative networks; image and text		Text is a new way to guide human image manipulation. Albeit natural and flexible, text usually suffers from inaccuracy in spatial description, ambiguity in the description of appearance, and incompleteness. We in this paper address these issues. To overcome inaccuracy, we use structured information (e.g., poses) to help identify correct location to manipulate, by disentangling the control of appearance and spatial structure. Moreover, we learn the image-text shared space with derived disentanglement to improve accuracy and quality of manipulation, by separating relevant and irrelevant editing directions for the textual instructions in this space. Our model generates a series of manipulation results by moving source images in this space with different degrees of editing strength. Thus, to reduce the ambiguity in text, our model generates sequential output for manual selection. In addition, we propose an efficient pseudo-label loss to enhance editing performance when the text is incomplete. We evaluate our method on various datasets and show its precision and interactiveness to manipulate human images.	[Xu, Xiaogang; Jia, Jiaya] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China; [Chen, Ying-Cong] MIT, Comp Sci & Artif Intelligence Lab, Cambridge, MA 02139 USA; [Tao, Xin] Kuaishou Technol, Beijing 100085, Peoples R China	Chinese University of Hong Kong; Massachusetts Institute of Technology (MIT)	Chen, YC (corresponding author), MIT, Comp Sci & Artif Intelligence Lab, Cambridge, MA 02139 USA.	xgxu@cse.cuhk.edu.hk; yingcong.ian.chen@gmail.com; jiangsutx@gmail.com; leojia@cse.cuhk.edu.hk	Chen, Ying-Cong/ABE-3123-2020	Chen, Ying-Cong/0000-0002-9565-8205; Tao, Xin/0000-0001-9126-4746				Ak KE, 2019, IEEE I CONF COMP VIS, P10540, DOI 10.1109/ICCV.2019.01064; Andrew Tao, 2019, Arxiv, DOI arXiv:1910.12713; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bernt Schiele, 2016, Arxiv, DOI arXiv:1605.05396; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603; Chen JB, 2018, PROC CVPR IEEE, P8721, DOI 10.1109/CVPR.2018.00909; Chen YC, 2019, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2019.00251; Chen YC, 2018, PROC CVPR IEEE, P3541, DOI 10.1109/CVPR.2018.00373; Christopher D. Manning, 2015, Arxiv, DOI arXiv:1508.04025; Dong H, 2017, IEEE I CONF COMP VIS, pCP1, DOI 10.1109/ICCV.2017.608; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; El-Nouby A, 2019, IEEE I CONF COMP VIS, P10303, DOI 10.1109/ICCV.2019.01040; Gao LL, 2019, AAAI CONF ARTIF INTE, P8312; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gray D., 2007, PROC IEEE INT WORKSH; Gunel M., 2018, ARXIV; Hai Wang, 2018, Arxiv, DOI arXiv:1810.05786; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hensel M, 2017, ADV NEUR IN, V30; Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833; Hou XX, 2017, IEEE WINT CONF APPL, P1133, DOI 10.1109/WACV.2017.131; Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Jiahao Bu, 2015, Arxiv, DOI arXiv:1502.02171; Jianfeng Gao, 2020, Arxiv, DOI arXiv:1812.08352; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241; Kingma D.P, P 3 INT C LEARNING R; Koichiro Yoshino, 2018, Arxiv, DOI arXiv:1802.08645; Laina I, 2019, IEEE I CONF COMP VIS, P7413, DOI 10.1109/ICCV.2019.00751; Larry S. Davis, 2019, Arxiv, DOI arXiv:1902.01096; Li B., 2020, P IEEECVF C COMPUTER, P7880; Li B., 2019, PROC C NEURAL INF PR; Li S, 2017, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2017.551; Li W., 2013, LNCS, V7724, P31, DOI [10.1007/978-3-642-37331-2, DOI 10.1007/978-3-642-37331-2]; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Li YN, 2019, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2019.00381; Liu Ming-Yu, 2019, ARXIV; Liu X., 2019, PROC C NEURAL INF PR; Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Manuvinakurike R, 2018, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2018), P4322; Manuvinakurike R, 2018, 19TH ANNUAL MEETING OF THE SPECIAL INTEREST GROUP ON DISCOURSE AND DIALOGUE (SIGDIAL 2018), P284; Mao XF, 2019, INT CONF ACOUST SPEE, P2047, DOI 10.1109/ICASSP.2019.8683008; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Men YF, 2020, PROC CVPR IEEE, P5083, DOI 10.1109/CVPR42600.2020.00513; Nam S, 2018, ADV NEUR IN, V31; Neverova N, 2018, LECT NOTES COMPUT SC, V11207, P128, DOI 10.1007/978-3-030-01219-9_8; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160; Reed S., 2016, NEURAL INFORM PROCES; Richard S. Zemel, 2014, Arxiv, DOI arXiv:1411.2539; Singh A., 2018, PROC SYSML WORKSHOP; Sundermeyer M, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P194; Upchurch P, 2017, PROC CVPR IEEE, P6090, DOI 10.1109/CVPR.2017.645; Wang M, 2019, PROC CVPR IEEE, P1495, DOI 10.1109/CVPR.2019.00159; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Xiao T., 2016, ARXIV; Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143; Yang L., 2020, INT J ENVIRON AN CH, P1, DOI DOI 10.1142/S021800142059003X; Yujun Shen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9240, DOI 10.1109/CVPR42600.2020.00926; Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821; Yurui Ren, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7687, DOI 10.1109/CVPR42600.2020.00771; Zakharov E., 2018, IEEE; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649; Zhou XR, 2019, PROC CVPR IEEE, P3658, DOI 10.1109/CVPR.2019.00378; Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186; Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245	70	0	0	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6486	6500		10.1109/TPAMI.2021.3085339	http://dx.doi.org/10.1109/TPAMI.2021.3085339			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34061734				2022-12-18	WOS:000853875300046
J	Xu, ZB; Yang, W; Zhang, W; Tan, X; Huang, H; Huang, LS				Xu, Zhenbo; Yang, Wei; Zhang, Wei; Tan, Xiao; Huang, Huan; Huang, Liusheng			Segment as Points for Efficient and Effective Online Multi-Object Tracking and Segmentation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Image color analysis; Feature extraction; Image segmentation; Automobiles; Motion segmentation; Annotations; Object segmentation; tracking		Current multi-object tracking and segmentation (MOTS) methods follow the tracking-by-detection paradigm and adopt 2D or 3D convolutions to extract instance embeddings for instance association. However, due to the large receptive field of deep convolutional neural networks, the foreground areas of the current instance and the surrounding areas containing the nearby instances or environments are usually mixed up in the learned instance embeddings, resulting in ambiguities in tracking. In this paper, we propose a highly effective method for learning instance embeddings based on segments by converting the compact image representation to un-ordered 2D point cloud representation. In this way, the non-overlapping nature of instance segments can be fully exploited by strictly separating the foreground point cloud and the background point cloud. Moreover, multiple informative data modalities are formulated as point-wise representations to enrich point-wise features. For each instance, the embedding is learned on the foreground 2D point cloud, the environment 2D point cloud, and the smallest circumscribed bounding box. Then, similarities between instance embeddings are measured for the inter-frame association. In addition, to enable the practical utility of MOTS, we modify the one-stage instance segmentation method SpatialEmbedding for instance segmentation. The resulting efficient and effective framework, named PointTrackV2, outperforms all the state-of-the-art methods including 3D tracking methods by large margins (4.8 percent higher sMOTSA for pedestrians over MOTSFusion) with the near real-time speed (20 FPS evaluated on a single 2080Ti). Extensive evaluations on three datasets demonstrate both the effectiveness and efficiency of our method. Furthermore, as crowded scenes for cars are insufficient in current MOTS datasets, we provide a more challenging dataset named APOLLO MOTS with a much higher instance density.	[Xu, Zhenbo; Yang, Wei; Huang, Huan; Huang, Liusheng] Univ Sci & Technol China USTC, Hefei 230052, Peoples R China; [Zhang, Wei; Tan, Xiao] Baidu Inc, Dept Comupter Vis, Beijing 713702, Peoples R China	Chinese Academy of Sciences; University of Science & Technology of China, CAS; Baidu	Yang, W (corresponding author), Univ Sci & Technol China USTC, Hefei 230052, Peoples R China.	xuzhenbo@mail.ustc.edu.cn; qubit@ustc.edu.cn; zhangwei99@baidu.com; tanxiao01@baidu.com; huanghuan@xtkg.com; lshuang@ustc.edu.cn			Anhui Initiative in Quantum Information Technologies [AHY150300]	Anhui Initiative in Quantum Information Technologies	The authors would like to thank the anonymous reviewers for their constructive comments and the editor for his review of the manuscript. This work was supported by the Anhui Initiative in Quantum Information Technologies under Grant AHY150300.	Alex Kendall, 2019, Arxiv, DOI arXiv:1912.08969; Anton Milan, 2016, Arxiv, DOI arXiv:1603.00831; Baser E, 2019, IEEE INT VEH SYM, P1426, DOI 10.1109/IVS.2019.8813779; Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103; Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628; Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479; Geiger A, 2014, IEEE T PATTERN ANAL, V36, P1012, DOI 10.1109/TPAMI.2013.185; Held D, 2013, IEEE INT CONF ROBOT, P1138, DOI 10.1109/ICRA.2013.6630715; Huang XY, 2018, IEEE COMPUT SOC CONF, P1067, DOI 10.1109/CVPRW.2018.00141; Karunasekera H, 2019, IEEE ACCESS, V7, P104423, DOI 10.1109/ACCESS.2019.2932301; Lin C.C., 2020, P IEEE CVF C COMP VI, P13147; Luiten J, 2020, IEEE ROBOT AUTOM LET, V5, P1803, DOI 10.1109/LRA.2020.2969183; Luiten J, 2019, LECT NOTES COMPUT SC, V11364, P565, DOI 10.1007/978-3-030-20870-7_35; Mitzel D, 2012, LECT NOTES COMPUT SC, V7576, P566, DOI 10.1007/978-3-642-33715-4_41; Neven D, 2019, PROC CVPR IEEE, P8829, DOI 10.1109/CVPR.2019.00904; Osep A., 2017, P 2017 IEEE INT C RO, P1995, DOI [DOI 10.1109/ICRA.2017.7989230, 10.1109/icra.2017.7989230]; Osep A, 2018, IEEE INT CONF ROBOT, P3494; Payer C, 2018, LECT NOTES COMPUT SC, V11071, P3, DOI 10.1007/978-3-030-00934-2_1; Ruiz I., 2019, ARXIV; Sharma S, 2018, IEEE INT CONF ROBOT, P3508; Tian W, 2020, IEEE T INTELL TRANSP, V21, P374, DOI 10.1109/TITS.2019.2892413; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Voigtlaender P., 2019, P IEEECVF C COMPUTER, P7942; Xu JR, 2019, IEEE I CONF COMP VIS, P3987, DOI 10.1109/ICCV.2019.00409; Xu Z, 2020, PROC EUR C COMPUT VI; Zhang WW, 2019, IEEE I CONF COMP VIS, P2365, DOI 10.1109/ICCV.2019.00245	28	0	0	7	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6424	6437		10.1109/TPAMI.2021.3087898	http://dx.doi.org/10.1109/TPAMI.2021.3087898			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34106847				2022-12-18	WOS:000853875300042
J	Yang, C; Cheung, G; Hu, W				Yang, Cheng; Cheung, Gene; Hu, Wei			Signed Graph Metric Learning via Gershgorin Disc Perfect Alignment	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Graph signal processing; metric learning; Gershgorin circle theorem; convex optimization	DISTANCE	Given a convex and differentiable objective Q(M) for a real symmetric matrix M in the positive definite (PD) cone-used to compute Mahalanobis distances-we propose a fast general metric learning framework that is entirely projection-free. We first assume that M resides in a space S of generalized graph Laplacian matrices corresponding to balanced signed graphs. M is an element of S that is also PD is called a graph metric matrix. Unlike low-rank metric matrices common in the literature, S includes the important diagonal-only matrices as a special case. The key theorem to circumvent full eigen-decomposition and enable fast metric matrix optimization is Gershgorin disc perfect alignment (GDPA): given M is an element of S and diagonal matrix S, where S-ii = 1/(v)(i) and v is the first eigenvector of M, we prove that Gershgorin disc left-ends of similarity transform B = SMS-1 are perfectly aligned at the smallest eigenvalue lambda(min). Using this theorem, we replace the PD cone constraint in the metric learning problem with tightest possible linear constraints per iteration, so that the alternating optimization of the diagonal / off-diagonal terms in M can be solved efficiently as linear programs via the Frank-Wolfe method. We update v using Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) with warm start as entries in M are optimized successively. Experiments show that our graph metric optimization is significantly faster than cone-projection schemes, and produces competitive binary classification performance.	[Yang, Cheng] Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China; [Cheung, Gene] York Univ, Toronto, ON M3J 1P3, Canada; [Hu, Wei] Peking Univ, Beijing 100871, Peoples R China	Shanghai Jiao Tong University; York University - Canada; Peking University	Cheung, G (corresponding author), York Univ, Toronto, ON M3J 1P3, Canada.; Hu, W (corresponding author), Peking Univ, Beijing 100871, Peoples R China.	cheng.yang@ieee.org; genec@yorku.ca; forhuwei@pku.edu.cn			China Postdoctoral Science Foundation [2020TQ0194]; NSERC [RGPIN-2019-06271, RGPAS-2019-00110]; National Key R&D project of China [2019YFF0302903]; National Natural Science Foundation of China [61972009]	China Postdoctoral Science Foundation(China Postdoctoral Science Foundation); NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); National Key R&D project of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	The work of Cheng Yang was supported by the China Postdoctoral Science Foundation under Grant 2020TQ0194. The work of Gene Cheung was supported by the NSERC Grants RGPIN-2019-06271, and RGPAS-2019-00110. The work of Wei Hu was supported by the National Key R&D project of China under contract No. 2019YFF0302903 and the National Natural Science Foundation of China under Grant 61972009.	Bai YC, 2020, IEEE T SIGNAL PROCES, V68, P2419, DOI 10.1109/TSP.2020.2981202; Bai YC, 2019, INT CONF ACOUST SPEE, P5396, DOI 10.1109/ICASSP.2019.8682200; Bertsekas D.P., 2016, NONLINEAR PROGRAMMIN, V3; Biyikoglu T, 2005, ELECTRON J LINEAR AL, V13, P344; Boyd S, 2004, CONVEX OPTIMIZATION; Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343; CARTWRIGHT D, 1956, PSYCHOL REV, V63, P277, DOI 10.1037/h0046049; Cheung G., 2021, GRAPH SPECTRAL IMAGE; Cheung G, 2018, P IEEE, V106, P907, DOI 10.1109/JPROC.2018.2799702; Dong MZ, 2020, IEEE T PATTERN ANAL, V42, P1522, DOI 10.1109/TPAMI.2019.2914899; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Globerson A., 2005, NIPS; Golub G. H., 1996, MATRIX COMPUTATIONS; Goyal V.K., 2014, FDN SIGNAL PROCESSIN; Hadsell R., 2006, 2006 IEEE COMPUTER S, P1735, DOI DOI 10.1109/CVPR.2006.100; Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7; Horn R.A., 2013, MATRIX ANAL, VSecond; Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242; Hu W, 2020, IEEE T SIGNAL PROCES, V68, P2841, DOI 10.1109/TSP.2020.2978617; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Knyazev AV, 2001, SIAM J SCI COMPUT, V23, P517, DOI 10.1137/S1064827500366124; Leskovec J, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1361; Liu EY, 2012, IEEE DATA MINING, P978, DOI 10.1109/ICDM.2012.38; Liu W, 2015, AAAI CONF ARTIF INTE, P2792; Lu JW, 2017, IEEE SIGNAL PROC MAG, V34, P76, DOI 10.1109/MSP.2017.2732900; Mahalanobis PC, 2018, SANKHYA SER A, V80, P1, DOI 10.1007/s13171-019-00164-5; McFee B, 2013, P 30 INT C MACH LEAR, P615; Mika S., 1999, Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468), P41, DOI 10.1109/NNSP.1999.788121; Milgram M., 1972, J COMBINATORIAL THEO, V12, P6; Mu YD, 2016, AAAI CONF ARTIF INTE, P1941; Ortega A., 2021, INTRO GRAPH SIGNAL P; Pang JH, 2017, IEEE T IMAGE PROCESS, V26, P1770, DOI 10.1109/TIP.2017.2651400; Papadimitriou C. H., 1982, COMBINATORIAL OPTIMI; Parikh N., 2014, FDN TRENDS OPTIM, V1, P127, DOI DOI 10.1561/2400000003; Qi G.-J., 2009, P INT C MACH LEARN J, P841; Raphson J., 1690, ANAL AEQUATIONUM UNI; Russell S., 2009, ARTIF INTELL; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Schultz M, 2004, ADV NEUR IN, V16, P41; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Su WT, 2017, IEEE IMAGE PROC, P1682; Sun Y., 2014, ADV NEURAL INFORM PR, P1988; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Torresani L., 2007, ADV NEURAL INFORM PR, V19, P1385; Varga RS., 2004, GERSGORIN HIS CIRCLE; Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Wright J., 2021, HIGHDIMENSIONAL DATA; Xing E., 2002, ADV NEURAL INFORM PR, V15, P505, DOI DOI 10.5555/2968618.2968683; Yang C, 2020, INT CONF ACOUST SPEE, P5530, DOI 10.1109/ICASSP40776.2020.9053552; Yang C, 2018, ASIAPAC SIGN INFO PR, P1137, DOI 10.23919/APSIPA.2018.8659448; Ying YM, 2012, J MACH LEARN RES, V13, P1; Zadeh PH, 2016, PR MACH LEARN RES, V48; Zhang J, 2017, AAAI CONF ARTIF INTE, P933	55	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					7219	7234		10.1109/TPAMI.2021.3091682	http://dx.doi.org/10.1109/TPAMI.2021.3091682			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4O3KZ	34161237	Green Submitted			2022-12-18	WOS:000854602700003
J	Yao, QM; Yang, HS; Hu, EL; Kwok, JT				Yao, Quanming; Yang, Hansi; Hu, En-Liang; Kwok, James T.			Efficient Low-Rank Semidefinite Programming With Robust Loss Functions	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Semi-definite programming; robustness; majorization-minimization; alternating direction method of multipliers	MATRIX FACTORIZATION; OPTIMIZATION; CONVERGENCE; COMPLETION; ALGORITHM	In real-world applications, it is important for machine learning algorithms to be robust against data outliers or corruptions. In this paper, we focus on improving the robustness of a large class of learning algorithms that are formulated as low-rank semi-definite programming (SDP) problems. Traditional formulations use the square loss, which is notorious for being sensitive to outliers. We propose to replace this with more robust noise models, including the l(1)-loss and other nonconvex losses. However, the resultant optimization problem becomes difficult as the objective is no longer convex or smooth. To alleviate this problem, we design an efficient algorithm based on majorization-minimization. The crux is on constructing a good optimization surrogate, and we show that this surrogate can be efficiently obtained by the alternating direction method of multipliers (ADMM). By properly monitoring ADMM's convergence, the proposed algorithm is empirically efficient and also theoretically guaranteed to converge to a critical point. Extensive experiments are performed on four machine learning applications using both synthetic and real-world data sets. Results show that the proposed algorithm is not only fast but also has better performance than the state-of-the-arts.	[Yao, Quanming] 4Paradigm Inc, Beijing 100089, Peoples R China; [Yao, Quanming; Yang, Hansi] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China; [Hu, En-Liang] Yunnan Normal Univ, Dept Math, Kunming 650092, Yunnan, Peoples R China; [Kwok, James T.] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China	Tsinghua University; Yunnan Normal University; Hong Kong University of Science & Technology	Yao, QM (corresponding author), 4Paradigm Inc, Beijing 100089, Peoples R China.	qyaoaa@connect.ust.hk; yhs17@mails.tsinghua.edu.cn; ynel.hu@gmail.com; jamesk@cse.ust.hk	Yao, Quanming/Y-6095-2019	Yao, Quanming/0000-0001-8944-8618	National Natural Science Foundation of China [61663049]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by the National Natural Science Foundation of China (No.61663049).	[Anonymous], 2007, P INT C MACH LEARN, DOI DOI 10.1145/1273496.1273542; Basri R, 2007, INT J COMPUT VISION, V72, P239, DOI 10.1007/s11263-006-8815-7; Bhargava A, 2017, PR MACH LEARN RES, V54, P1349; Bhojanapalli Srinadh, 2016, P C LEARN THEOR, P530; Bishop W. E., 2014, ADV NEURAL INF PROCE, P2762; BOUMAL N, 2016, ADV NEURAL INFORM PR, P2757, DOI DOI 10.5555/3157382.3157407; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Boyd S, 2004, CONVEX OPTIMIZATION; Burke R., 2015, RECOMMENDER SYSTEMS, P961, DOI [DOI 10.1007/978-1-4899-7637-6_28, 10.1007/978-1-4899-7637-628, DOI 10.1007/978-1-4899-7637-628]; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x; Clarke Frank H., 1990, OPTIMIZATION NONSMOO, V5; d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506; De la Torre F, 2003, INT J COMPUT VISION, V54, P117, DOI 10.1023/A:1023709501986; Dekel O, 2010, MACH LEARN, V81, P149, DOI 10.1007/s10994-009-5124-8; Eriksson A, 2010, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2010.5540139; GEMAN D, 1995, IEEE T IMAGE PROCESS, V4, P932, DOI 10.1109/83.392335; Gong Pinghua, 2013, JMLR Workshop Conf Proc, V28, P37; Hastie T, 2015, J MACH LEARN RES, V16, P3367; He BS, 2012, SIAM J NUMER ANAL, V50, P700, DOI 10.1137/110836936; He ZS, 2011, IEEE T NEURAL NETWOR, V22, P2117, DOI 10.1109/TNN.2011.2172457; Hein M, 2010, ADV NEURAL INFORM PR, V1, P847; Helmberg C, 1996, SIAM J OPTIMIZ, V6, P342, DOI 10.1137/0806020; Hu E.-L., 2011, P INT C MACH LEARN, P209; Hu EL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2514; Hu EL, 2019, IEEE T NEUR NET LEAR, V30, P3517, DOI 10.1109/TNNLS.2019.2927819; Huang KJ, 2014, IEEE T SIGNAL PROCES, V62, P211, DOI 10.1109/TSP.2013.2285514; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Hunter DR, 2004, AM STAT, V58, P30, DOI 10.1198/0003130042836; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Jiang WH, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3590; Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359; Kim E, 2015, IEEE T NEUR NET LEAR, V26, P237, DOI 10.1109/TNNLS.2014.2312535; Kuang D, 2015, J GLOBAL OPTIM, V62, P545, DOI 10.1007/s10898-014-0247-2; Kulis B., 2007, INT C ART INT STAT, P235; Lange K, 2000, J COMPUT GRAPH STAT, V9, P1, DOI 10.2307/1390605; Laue S., 2012, P INT C MACH LEARN, P177; Laurent M, 2014, LINEAR ALGEBRA APPL, V452, P292, DOI 10.1016/j.laa.2014.03.015; Lemon Alex, 2016, Foundations and Trends in Optimization, V2, P1, DOI 10.1561/2400000009; Li H., 2015, PROC 28 INT C NEURAL, P379; Li Z, 2008, PROC 25 INT C MACH L; Lin ZC, 2018, IEEE T PATTERN ANAL, V40, P208, DOI 10.1109/TPAMI.2017.2651816; Lu CW, 2013, PROC CVPR IEEE, P415, DOI 10.1109/CVPR.2013.60; Mairal J., 2013, P 30 INT C INT C MAC, P783; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Pirinen A, 2019, J MACH LEARN RES, V20; Pumir T., 2018, P INT C NEUR INF PRO, P2287; Raykar VC, 2010, J MACH LEARN RES, V11, P1297; Salakhutdinov R., 2007, ADV NEURAL INF PROCE, V20, P1257; Schmidt M., 2011, ADV NEURAL INFORM PR, P1458; Shi QJ, 2017, IEEE T SIGNAL PROCES, V65, P5995, DOI 10.1109/TSP.2017.2731321; Singh V, 2010, MACH LEARN, V79, P177, DOI 10.1007/s10994-009-5158-y; Sjostrand K, 2018, J STAT SOFTW, V84, P1, DOI 10.18637/jss.v084.i10; Song L., 2008, P 20 INT C NEUR INF, P1385; Srinadh B., 2016, PROC C LEARN THEORY, P530; Trzasko J, 2009, IEEE T MED IMAGING, V28, P106, DOI 10.1109/TMI.2008.927346; Vandenberghe L, 1996, SIAM REV, V38, P49, DOI 10.1137/1038003; Weinberger K. Q., 2004, P 21 INT C MACH LEAR, P839; Yang LQ, 2015, MATH PROGRAM COMPUT, V7, P331, DOI 10.1007/s12532-015-0082-6; Yao Q., 2018, P INT C NEUR INF PRO, P5061; Yao Q., 2017, J MACH LEARN RES, V18, P6574; Yao QM, 2019, IEEE T PATTERN ANAL, V41, P2628, DOI 10.1109/TPAMI.2018.2858249; Yao QM, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3308; Yuan XT, 2013, J MACH LEARN RES, V14, P899; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; Zhang T, 2010, J MACH LEARN RES, V11, P1081; Zheng Q., 2015, ADV NEURAL INFORM PR, P109; Zheng YQ, 2012, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2012.6247828; Zhuang JF, 2011, J MACH LEARN RES, V12, P1313	73	0	0	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6153	6168		10.1109/TPAMI.2021.3085858	http://dx.doi.org/10.1109/TPAMI.2021.3085858			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34061741	Green Submitted			2022-12-18	WOS:000853875300024
J	Zhang, WL; Liu, YH; Dong, C; Qiao, Y				Zhang Wenlong; Liu Yihao; Dong, Chao; Qiao, Yu			RankSRGAN: Super Resolution Generative Adversarial Networks With Learning to Rank	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image super resolution; generative adversarial network; learning to rank	SINGLE-IMAGE SUPERRESOLUTION; ALGORITHM	Generative Adversarial Networks (GAN) have demonstrated the potential to recover realistic details for single image super-resolution (SISR). To further improve the visual quality of super-resolved results, PIRM2018-SR Challenge employed perceptual metrics to assess the perceptual quality, such as PI, NIQE, and Ma. However, existing methods cannot directly optimize these indifferentiable perceptual metrics, which are shown to be highly correlated with human ratings. To address the problem, we propose Super-Resolution Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator in the direction of different perceptual metrics. Specifically, we first train a Ranker which can learn the behaviour of perceptual metrics and then introduce a novel rank-content loss to optimize the perceptual quality. The most appealing part is that the proposed method can combine the strengths of different SR methods to generate better results. Furthermore, we extend our method to multiple Rankers to provide multi-dimension constraints for the generator. Extensive experiments show that RankSRGAN achieves visually pleasing results and reaches state-of-the-art performance in perceptual metrics and quality. Project page: https://wenlongzhang0517.github.io/Projects/RankSRGAN .	[Zhang Wenlong; Liu Yihao; Dong, Chao; Qiao, Yu] Chinese Acad Sci, Shenzhen Inst Adv Technol, ShenZhen Key Lab Comp Vis & Pattern Recognit, SIAT SenseTime Joint Lab, Beijing 100864, Peoples R China; [Qiao, Yu] Shanghai AI Lab, Shanghai 200232, Peoples R China	Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS	Dong, C (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, ShenZhen Key Lab Comp Vis & Pattern Recognit, SIAT SenseTime Joint Lab, Beijing 100864, Peoples R China.	wl.zhang1@siat.ac.cn; yh.liu4@siat.ac.cn; chao.dong@siat.ac.cn; yu.qiao@siat.ac.cn			National Natural Science Foundation of China [61906184]; Science and Technology Service Network Initiative of Chinese Academy of Sciences [KFJ-STS-QYZX-092]; Shanghai Committee of Science and Technology, China [21DZ1100100]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Science and Technology Service Network Initiative of Chinese Academy of Sciences; Shanghai Committee of Science and Technology, China(Shanghai Science & Technology Committee)	This work was partially supported by National Natural Science Foundation of China under Grant 61906184, the Science and Technology Service Network Initiative of Chinese Academy of Sciences under Grant KFJ-STS-QYZX-092, the Shanghai Committee of Science and Technology, China under Grant 21DZ1100100.	Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652; Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518; Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339; Bruna Joan, 2015, ARXIV; Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043; Chao Dong, 2018, Arxiv, DOI arXiv:1804.02815; Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Chris Burges T.S., 2005, P 22 INT MACH LEARN, DOI 10.1145/1102351.1102363; Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Dosovitskiy Alexey, 2016, NEURIPS; Feng RC, 2019, IEEE COMPUT SOC CONF, P1964, DOI 10.1109/CVPRW.2019.00248; Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747; Freund Y, 2004, J MACH LEARN RES, V4, P933, DOI 10.1162/1532443041827916; Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271; Gu JJ, 2019, PROC CVPR IEEE, P1604, DOI 10.1109/CVPR.2019.00170; Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179; He JW, 2019, PROC CVPR IEEE, P11048, DOI 10.1109/CVPR.2019.01131; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Herbrich R, 1999, IEE CONF PUBL, P97, DOI 10.1049/cp:19991091; Jian Chen, 2020, Arxiv, DOI arXiv:2003.07018; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Jong-Seok Lee, 2019, Arxiv, DOI arXiv:1809.04789; Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kingma D.P, P 3 INT C LEARNING R; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li CF, 2011, IEEE T NEURAL NETWOR, V22, P793, DOI 10.1109/TNN.2011.2120620; Lihi Zelnik-Manor, 2019, Arxiv, DOI arXiv:1809.07517; Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151; Liu TY, 2011, LEARNING TO RANK FOR INFORMATION RETRIEVAL, P1, DOI 10.1007/978-3-642-14267-3; Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118; Ma C, 2017, COMPUT VIS IMAGE UND, V158, P1, DOI 10.1016/j.cviu.2016.12.009; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Mehdi Mirza, 2014, Arxiv, DOI arXiv:1411.1784; Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726; Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281; Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009; Rad MS, 2019, IEEE I CONF COMP VIS, P2710, DOI 10.1109/ICCV.2019.00280; Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481; Saquil Y., 2018, PROC BRIT MACH VIS C; Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149; Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514; Vasu S., 2018, ANAL PERCEPTION DIST, P114; Wang LF, 2013, IEEE T CIRC SYST VID, V23, P1289, DOI 10.1109/TCSVT.2013.2240915; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Yang XS, 2016, IEEE T MULTIMEDIA, V18, P1832, DOI 10.1109/TMM.2016.2582379; Ye P, 2011, IEEE IMAGE PROC; Yunbo Cao, 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P186; Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064; Zeyde Roman, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47; Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977; Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407; Zhang WL, 2019, IEEE I CONF COMP VIS, P3096, DOI 10.1109/ICCV.2019.00319; Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI [10.1007/978-3-030-01234-2_18, 10.1007/978-3-030-01240-3_22]; Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262; Zhou F, 2012, IEEE T IMAGE PROCESS, V21, P3312, DOI 10.1109/TIP.2012.2189576; Zhou RF, 2019, IEEE I CONF COMP VIS, P2433, DOI 10.1109/ICCV.2019.00252	63	0	0	12	12	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					7149	7166		10.1109/TPAMI.2021.3096327	http://dx.doi.org/10.1109/TPAMI.2021.3096327			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34310284	Green Submitted			2022-12-18	WOS:000853875300090
J	Zhang, XC; Ma, R; Zou, CQ; Zhang, MH; Zhao, XB; Gao, Y				Zhang, Xuancheng; Ma, Rui; Zou, Changqing; Zhang, Minghao; Zhao, Xibin; Gao, Yue			View-Aware Geometry-Structure Joint Learning for Single-View 3D Shape Reconstruction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Three-dimensional displays; Shape; Image reconstruction; Geometry; Periodic structures; Solid modeling; Topology; Single-view 3D reconstruction; structure-aware reconstruction; multimodal learning; representation learning		Reconstructing a 3D shape from a single-view image using deep learning has become increasingly popular recently. Most existing methods only focus on reconstructing the 3D shape geometry based on image constraints. The lack of explicit modeling of structure relations among shape parts yields low-quality reconstruction results for structure-rich man-made shapes. In addition, conventional 2D-3D joint embedding architecture for image-based 3D shape reconstruction often omits the specific view information from the given image, which may lead to degraded geometry and structure reconstruction. We address these problems by introducing VGSNet, an encoder-decoder architecture for view-aware joint geometry and structure learning. The key idea is to jointly learn a multimodal feature representation of 2D image, 3D shape geometry and structure so that both geometry and structure details can be reconstructed from a single-view image. To this end, we explicitly represent 3D shape structures as part relations and employ image supervision to guide the geometry and structure reconstruction. Trained with pairs of view-aligned images and 3D shapes, the VGSNet implicitly encodes the view-aware shape information in the latent feature space. Qualitative and quantitative comparisons with the state-of-the-art baseline methods as well as ablation studies demonstrate the effectiveness of the VGSNet for structure-aware single-view 3D shape reconstruction.	[Zhang, Xuancheng; Zhang, Minghao; Zhao, Xibin; Gao, Yue] Tsinghua Univ, Sch Software, KLISS, THUICBS,BNRist, Beijing 100084, Peoples R China; [Ma, Rui] Jilin Univ, Sch Artificial Intelligence, Changchun 130012, Jilin, Peoples R China; [Zou, Changqing] Huawei Technol Canada Co Ltd, Markham, ON L3R 5A4, Canada	Tsinghua University; Jilin University; Huawei Technologies	Zhao, XB; Gao, Y (corresponding author), Tsinghua Univ, Sch Software, KLISS, THUICBS,BNRist, Beijing 100084, Peoples R China.	zxc19@mails.tsinghua.edu.cn; maruitx@gmail.com; aaronzou1125@gmail.com; mehoozhang@gmail.com; zxb@tsinghua.edu.cn; kevin.gaoy@gmail.com	Ma, Rui/Y-3479-2019	Ma, Rui/0000-0002-3477-1466; Zhang, Minghao/0000-0003-3337-890X	National Key R&D Program of China [2018YFB1703404]; National Natural Science Funds of China [U1701262, U1801263]	National Key R&D Program of China; National Natural Science Funds of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by the National Key R&D Program of China under Grant 2018YFB1703404 and in part by the National Natural Science Funds of China under Grants U1701262 and U1801263. X. Zhang and R. Ma are cofirst authors.	Adrien Gaidon, 2020, Arxiv, DOI arXiv:2006.12057; Anastasia Dubrovina, 2020, Arxiv, DOI arXiv:1811.08988; Angel X. Chang, 2015, Arxiv, DOI arXiv:1512.03012; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Chen ZQ, 2020, PROC CVPR IEEE, P42, DOI 10.1109/CVPR42600.2020.00012; Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Deng BY, 2020, PROC CVPR IEEE, P31, DOI 10.1109/CVPR42600.2020.00011; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Gao Y, 2022, IEEE T PATTERN ANAL, V44, P2548, DOI 10.1109/TPAMI.2020.3039374; Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Guo WZ, 2019, IEEE ACCESS, V7, P63373, DOI 10.1109/ACCESS.2019.2916887; Han XF, 2021, IEEE T PATTERN ANAL, V43, P1578, DOI 10.1109/TPAMI.2019.2954885; Hao Zhang, 2020, Arxiv, DOI arXiv:2012.06650; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Henderson P, 2020, INT J COMPUT VISION, V128, P835, DOI 10.1007/s11263-019-01219-8; Insafutdinov E, 2018, ADV NEUR IN, V31; Jiahui Lei, 2020, Arxiv, DOI arXiv:2008.07760; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Jiang L, 2018, LECT NOTES COMPUT SC, V11212, P820, DOI 10.1007/978-3-030-01237-3_49; Yang J, 2020, Arxiv, DOI arXiv:2008.05440; Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411; Khan SH, 2019, PROC CVPR IEEE, P9731, DOI 10.1109/CVPR.2019.00997; Kingma DP, 2 INT C LEARN REPR I, P1; Li J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073637; Li YY, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818071; Liao YY, 2018, PROC CVPR IEEE, P2916, DOI 10.1109/CVPR.2018.00308; Lin CH, 2018, AAAI CONF ARTIF INTE, P7114; Liu SH, 2020, PROC CVPR IEEE, P2016, DOI 10.1109/CVPR42600.2020.00209; Lorensen W. E., 1987, COMPUT GRAPH, V21, P163, DOI [10.1145/37401.37422, DOI 10.1145/37401.37422]; Mandikal P, 2019, IEEE WINT CONF APPL, P1052, DOI 10.1109/WACV.2019.00117; Mandikal Priyanka, 2018, P BRIT MACH VIS C BM; Manessi F, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107000; Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459; Michalkiewicz M, 2019, IEEE I CONF COMP VIS, P4742, DOI 10.1109/ICCV.2019.00484; Mo KC, 2019, PROC CVPR IEEE, P909, DOI 10.1109/CVPR.2019.00100; Mo KC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356527; Navaneet KL, 2019, AAAI CONF ARTIF INTE, P8819; Niu CJ, 2018, PROC CVPR IEEE, P4521, DOI 10.1109/CVPR.2018.00475; Pan JY, 2019, IEEE I CONF COMP VIS, P9963, DOI 10.1109/ICCV.2019.01006; Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025; Paschalidou D, 2020, PROC CVPR IEEE, P1057, DOI 10.1109/CVPR42600.2020.00114; Paschalidou D, 2019, PROC CVPR IEEE, P10336, DOI 10.1109/CVPR.2019.01059; Peng Songyou, 2020, P EUR C COMP VIS ECC, P523; Qi C.R., 2016, ARXIV; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Sitzmann V, 2019, ADV NEUR IN, V32; Sun CY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356529; Sun XY, 2018, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2018.00314; Tatarchenko M, 2019, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2019.00352; Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20; Tulsiani S, 2017, PROC CVPR IEEE, P1466, DOI 10.1109/CVPR.2017.160; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; van Kaick O, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461924; Wang JL, 2019, AAAI CONF ARTIF INTE, P8949; Wang L, 2022, IEEE T PATTERN ANAL, V44, P3048, DOI 10.1109/TPAMI.2021.3055564; Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4; Wang WY, 2019, ADV NEUR IN, V32; Wang XG, 2020, PROC CVPR IEEE, P787, DOI 10.1109/CVPR42600.2020.00087; Wang Y, 2011, COMPUT GRAPH FORUM, V30, P287, DOI 10.1111/j.1467-8659.2011.01885.x; Wen C, 2019, IEEE I CONF COMP VIS, P1042, DOI 10.1109/ICCV.2019.00113; Wu RD, 2020, PROC CVPR IEEE, P826, DOI 10.1109/CVPR42600.2020.00091; Wu ZJ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322956; Xie HZ, 2019, IEEE I CONF COMP VIS, P2690, DOI 10.1109/ICCV.2019.00278; Xie HZ, 2020, INT J COMPUT VISION, V128, P2919, DOI 10.1007/s11263-020-01347-6; Xu K, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964975; Yichen Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P664, DOI 10.1007/978-3-030-58539-6_40; Yu FG, 2019, PROC CVPR IEEE, P9483, DOI 10.1109/CVPR.2019.00972; Zhang X., 2021, PROC IEEE C COMPUT V, P15890; Zhu C., 2020, PROC IEEE C COMPUT V, P8540; Zhu R, 2017, IEEE I CONF COMP VIS, P57, DOI 10.1109/ICCV.2017.16; Zou CH, 2017, IEEE I CONF COMP VIS, P900, DOI 10.1109/ICCV.2017.103	75	0	0	9	9	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					6546	6561		10.1109/TPAMI.2021.3090917	http://dx.doi.org/10.1109/TPAMI.2021.3090917			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4O3KZ	34156936				2022-12-18	WOS:000854602700001
J	Zhu, Y; Wu, Y; Yang, Y; Yan, Y				Zhu, Ye; Wu, Yu; Yang, Yi; Yan, Yan			Saying the Unseen: Video Descriptions via Dialog Agents	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Task analysis; Visualization; Artificial intelligence; Natural languages; Knowledge transfer; Semantics; Video description; Video description; dialog agents; multi-modal learning	ATTENTION	Current vision and language tasks usually take complete visual data (e.g., raw images or videos) as input, however, practical scenarios may often consist the situations where part of the visual information becomes inaccessible due to various reasons e.g., restricted view with fixed camera or intentional vision block for security concerns. As a step towards the more practical application scenarios, we introduce a novel task that aims to describe a video using the natural language dialog between two agents as a supplementary information source given incomplete visual data. Different from most existing vision-language tasks where AI systems have full access to images or video clips, which may reveal sensitive information such as recognizable human faces or voices, we intentionally limit the visual input for AI systems and seek a more secure and transparent information medium, i.e., the natural language dialog, to supplement the missing visual information. Specifically, one of the intelligent agents - Q-BOT - is given two semantic segmented frames from the beginning and the end of the video, as well as a finite number of opportunities to ask relevant natural language questions before describing the unseen video. A-BOT, the other agent who has access to the entire video, assists Q-BOT to accomplish the goal by answering the asked questions. We introduce two different experimental settings with either a generative (i.e., agents generate questions and answers freely) or a discriminative (i.e., agents select the questions and answers from candidates) internal dialog generation process. With the proposed unified QA-Cooperative networks, we experimentally demonstrate the knowledge transfer process between the two dialog agents and the effectiveness of using the natural language dialog as a supplement for incomplete implicit visions.	[Zhu, Ye; Yan, Yan] IIT, Dept Comp Sci, Chicago, IL 60616 USA; [Wu, Yu; Yang, Yi] Univ Technol Sydney, Ultimo, NSW 2007, Australia	Illinois Institute of Technology; University of Technology Sydney	Yan, Y (corresponding author), IIT, Dept Comp Sci, Chicago, IL 60616 USA.	yzhu96@hawk.iit.edu; yu.wu-3@student.uts.edu.au; yi.yang@uts.edu.au; yyan34@iit.edu	yang, yang/HGT-7999-2022; Yang, Yi/B-9273-2017	Yang, Yi/0000-0002-0512-880X	NSF [NeTS2109982]	NSF(National Science Foundation (NSF))	This work was supported by NSF under Grant NeTS2109982 and the gift donation from Cisco. This article solely reflects the opinions and conclusions of its authors and not the funding agents.	Agarwal S., 2020, P 58 ANN M ASS COMP, P8182; Agarwal Vedika, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9687, DOI 10.1109/CVPR42600.2020.00971; Alamri H, 2019, PROC CVPR IEEE, P7550, DOI 10.1109/CVPR.2019.00774; Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636; Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Arandjelovic R, 2018, LECT NOTES COMPUT SC, V11205, P451, DOI 10.1007/978-3-030-01246-5_27; Banerjee S., 2005, P ACL WORKSH INTR EX, P65; Chen FH, 2019, ADV NEUR IN, V32; Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667; Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059; Dan Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10052, DOI 10.1109/CVPR42600.2020.01007; Das A, 2017, IEEE I CONF COMP VIS, P2970, DOI 10.1109/ICCV.2017.321; Das A, 2017, COMPUT VIS IMAGE UND, V163, P90, DOI 10.1016/j.cviu.2017.10.001; Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121; de Vries H, 2017, PROC CVPR IEEE, P4466, DOI 10.1109/CVPR.2017.475; Duan B, 2021, IEEE WINT CONF APPL, P4012, DOI 10.1109/WACV48630.2021.00406; Gan Z, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6463; Gao RH, 2020, PROC CVPR IEEE, P10454, DOI 10.1109/CVPR42600.2020.01047; Gao RH, 2018, LECT NOTES COMPUT SC, V11207, P36, DOI 10.1007/978-3-030-01219-9_3; Guo XX, 2018, ADV NEUR IN, V31; Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hori C, 2019, INT CONF ACOUST SPEE, P2352, DOI 10.1109/ICASSP.2019.8682583; Hu D, 2019, PROC CVPR IEEE, P9240, DOI 10.1109/CVPR.2019.00947; Jacob Devlin, 2019, Arxiv, DOI arXiv:1810.04805; Jain U, 2018, PROC CVPR IEEE, P5754, DOI 10.1109/CVPR.2018.00603; Jiaxin Qi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10857, DOI 10.1109/CVPR42600.2020.01087; Kastner S, 2000, ANNU REV NEUROSCI, V23, P315, DOI 10.1146/annurev.neuro.23.1.315; Kastner S, 1999, NEURON, V22, P751, DOI 10.1016/S0896-6273(00)80734-5; Lee SW, 2018, ADV NEUR IN, V31; Lin Chin-Yew, 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.2307/3105454; Long Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10797, DOI 10.1109/CVPR42600.2020.01081; Longteng Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10324, DOI 10.1109/CVPR42600.2020.01034; Lu JS, 2016, ADV NEUR IN, V29; Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345; Massiceti D, 2018, PROC CVPR IEEE, P6097, DOI 10.1109/CVPR.2018.00638; Murahari Vishvak, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P336, DOI 10.1007/978-3-030-58523-5_20; Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39; Owens A, 2018, INT J COMPUT VISION, V126, P1120, DOI 10.1007/s11263-018-1083-5; Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI DOI 10.3115/V1/D14-1162; Rajendran Janarthanan, 2018, P 2018 C EMP METH NA, P3834, DOI DOI 10.18653/V1/D18-1418; Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131; Rouditchenko A, 2019, INT CONF ACOUST SPEE, P2357, DOI 10.1109/ICASSP.2019.8682467; Roy N, 2000, 38TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P93; Schwartz I, 2019, PROC CVPR IEEE, P2039, DOI 10.1109/CVPR.2019.00214; Schwartz I, 2019, PROC CVPR IEEE, P12540, DOI 10.1109/CVPR.2019.01283; Senocak A, 2018, PROC CVPR IEEE, P4358, DOI 10.1109/CVPR.2018.00458; Seo PH, 2017, ADV NEUR IN, V30; Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499; Shukla Pushkar, 2019, P 57 ANN M ASS COMP, P6442, DOI [10.18653/v1/P19-1646, DOI 10.18653/V1/P19-1646]; Singh S, 2000, ADV NEUR IN, V12, P956; Song XM, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P239, DOI 10.1145/3240508.3240563; Strub F, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2765; Tian YP, 2018, LECT NOTES COMPUT SC, V11206, P252, DOI 10.1007/978-3-030-01216-8_16; Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087; Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795; Wang JW, 2018, PROC CVPR IEEE, P7190, DOI 10.1109/CVPR.2018.00751; Wang LW, 2017, ADV NEUR IN, V30; Wu Q, 2018, PROC CVPR IEEE, P6106, DOI 10.1109/CVPR.2018.00639; Wu Y, 2021, PROC CVPR IEEE, P1326, DOI 10.1109/CVPR46437.2021.00138; Wu Y, 2019, IEEE I CONF COMP VIS, P6301, DOI 10.1109/ICCV.2019.00639; Wu Y, 2020, IEEE T IMAGE PROCESS, V29, P3984, DOI 10.1109/TIP.2020.2967584; Wu Y, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1029, DOI 10.1145/3240508.3240640; Xiong CM, 2016, PR MACH LEARN RES, V48; Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524; Ye Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P153, DOI 10.1007/978-3-030-58592-1_10; Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Yue Wang, 2020, P 2020 C EMPIRICAL M, P3325, DOI DOI 10.18653/V1/2020.EMNLP-MAIN.269; Zhang JJ, 2018, LECT NOTES COMPUT SC, V11209, P189, DOI 10.1007/978-3-030-01228-1_12; Zhao H., 2018, LECT NOTES COMPUT SC, P570, DOI DOI 10.1007/978-3-030-01246-5_35; Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0; Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544; Zhou LW, 2020, AAAI CONF ARTIF INTE, V34, P13041; Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911; Zhu Y, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P4300, DOI 10.1109/ICASSP39728.2021.9414296	82	0	0	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	OCT 1	2022	44	10					7190	7204		10.1109/TPAMI.2021.3093360	http://dx.doi.org/10.1109/TPAMI.2021.3093360			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	4N2UL	34185637	Green Submitted			2022-12-18	WOS:000853875300093
J	Afifi, M; Abdelhamed, A; Abuolaim, A; Punnappurath, A; Brown, MS				Afifi, Mahmoud; Abdelhamed, Abdelrahman; Abuolaim, Abdullah; Punnappurath, Abhijith; Brown, Michael S.			CIE XYZ Net: Unprocessing Images for Low-Level Computer Vision Tasks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image color analysis; Cameras; Pipelines; Task analysis; Image restoration; Computer vision; Computational modeling; CIE XYZ color space; color linearization; scene-referred image reconstruction; image rendering	COLOR CONSTANCY; NOISE	Cameras currently allow access to two image states: (i) a minimally processed linear raw-RGB image state (i.e., raw sensor data); or (ii) a highly-processed nonlinear image state (e.g., sRGB). There are many computer vision tasks that work best with a linear image state, such as image deblurring and image dehazing. Unfortunately, the vast majority of images are saved in the nonlinear image state. Because of this, a number of methods have been proposed to "unprocess" nonlinear images back to a raw-RGB state. However, existing unprocessing methods have a drawback because raw-RGB images are sensor-specific. As a result, it is necessary to know which camera produced the sRGB output and use a method or network tailored for that sensor to properly unprocess it. This paper addresses this limitation by exploiting another camera image state that is not available as an output, but it is available inside the camera pipeline. In particular, cameras apply a colorimetric conversion step to convert the raw-RGB image to a device-independent space based on the CIE XYZ color space before they apply the nonlinear photo-finishing. Leveraging this canonical image state, we propose a deep learning framework, CIE XYZ Net, that can unprocess a nonlinear image back to the canonical CIE XYZ image. This image can then be processed by any low-level computer vision operator and re-rendered back to the nonlinear image. We demonstrate the usefulness of the CIE XYZ Net on several low-level vision tasks and show significant gains that can be obtained by this processing framework. Code and dataset are publicly available at https://github.com/mahmoudnafifi/CIE_XYZ_NET.	[Afifi, Mahmoud; Abdelhamed, Abdelrahman; Abuolaim, Abdullah; Punnappurath, Abhijith; Brown, Michael S.] York Univ, Lassonde Sch Engn, Dept Elect Engn & Comp Sci, Toronto, ON M3J 1P3, Canada	York University - Canada	Afifi, M (corresponding author), York Univ, Lassonde Sch Engn, Dept Elect Engn & Comp Sci, Toronto, ON M3J 1P3, Canada.	mafifi@eecs.yorku.ca; kamel@eecs.yorku.ca; abuolaim@eecs.yorku.ca; pabhijith@eecs.yorku.ca; mbrown@eecs.yorku.ca	Abdelhamed, Abdelrahman/AGM-9019-2022	Abdelhamed, Abdelrahman/0000-0002-4836-794X	Canada First Research Excellence Fund for the Vision: Science to Applications (VISTA) programme; NSERC	Canada First Research Excellence Fund for the Vision: Science to Applications (VISTA) programme; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC))	This work was supported in part by the Canada First Research Excellence Fund for the Vision: Science to Applications (VISTA) programme and the NSERC Discovery Grant.	Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182; Afifi M, 2019, PROC CVPR IEEE, P1535, DOI 10.1109/CVPR.2019.00163; Anderson M, 1996, FOURTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS AND APPLICATIONS, P238; Brooks T, 2019, PROC CVPR IEEE, P11028, DOI 10.1109/CVPR.2019.01129; Bychkovsky V, 2011, PROC CVPR IEEE, P97; Chakrabarti A, 2014, IEEE T PATTERN ANAL, V36, P2185, DOI 10.1109/TPAMI.2014.2318713; Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660; Cheng DL, 2014, J OPT SOC AM A, V31, P1049, DOI 10.1364/JOSAA.31.001049; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Ebner M., 2007, COMPUTER VISION; Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816; Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956; Fourure D, 2016, IEEE IMAGE PROC, P3997, DOI 10.1109/ICIP.2016.7533110; Gehler PV, 2008, PROC CVPR IEEE, P3291; Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592; Golestaneh SA, 2017, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2017.71; Gow RD, 2007, IEEE T ELECTRON DEV, V54, P1321, DOI 10.1109/TED.2007.896718; Hasinoff SW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980254; Hasinoff SW, 2010, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2010.5540167; He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168; Hu YM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181974; Hu YM, 2017, PROC CVPR IEEE, P330, DOI 10.1109/CVPR.2017.43; Karaimer H. C, 2016, PROC EUR C COMPUT VI; Kim SJ, 2012, IEEE T PATTERN ANAL, V34, P2289, DOI 10.1109/TPAMI.2012.58; Kingma D.P, P 3 INT C LEARNING R; Krishnan D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531402; Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188; Lin HT, 2011, IEEE I CONF COMP VIS, P129, DOI 10.1109/ICCV.2011.6126234; Liu C, 2008, IEEE T PATTERN ANAL, V30, P299, DOI [10.1109/TPAMI.2007.1176, 10.1109/TPAMI.20071176]; Lou Z., 2015, BMVC, V76, P71, DOI [10.5244/C.29.76, DOI 10.5244/C.29.76]; Mahmoud Afifi, 2019, Arxiv, DOI arXiv:1912.06888; Mertens T, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P382, DOI 10.1109/PG.2007.17; Nam S, 2017, IEEE I CONF COMP VIS, P1726, DOI 10.1109/ICCV.2017.190; Nguyen RMH, 2016, PROC CVPR IEEE, P1655, DOI 10.1109/CVPR.2016.183; Paris S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964963; Park J, 2018, PROC CVPR IEEE, P5928, DOI 10.1109/CVPR.2018.00621; Plotz T, 2017, PROC CVPR IEEE, P2750, DOI 10.1109/CVPR.2017.294; Nguyen R, 2014, PROC CVPR IEEE, P3398, DOI 10.1109/CVPR.2014.434; Schewe J., 2015, DIGITAL NEGATIVE RAW; Shi JP, 2014, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2014.379; Diamond S, 2017, Arxiv, DOI arXiv:1701.06487; Tai YW, 2013, IEEE T PATTERN ANAL, V35, P2498, DOI 10.1109/TPAMI.2013.40; Tang C., 2019, PROC IEEECVF C COMPU, P2700; Wah C., 2011, TECH REP; Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Yi X, 2016, IEEE T IMAGE PROCESS, V25, P1626, DOI 10.1109/TIP.2016.2528042; Zamir SW, 2020, PROC CVPR IEEE, P2693, DOI 10.1109/CVPR42600.2020.00277; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zhao WD, 2019, PROC CVPR IEEE, P8897, DOI 10.1109/CVPR.2019.00911; Zhao WD, 2018, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2018.00325	51	0	0	5	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4688	4700		10.1109/TPAMI.2021.3070580	http://dx.doi.org/10.1109/TPAMI.2021.3070580			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33798069	Green Submitted			2022-12-18	WOS:000836666600018
J	Balgi, S; Dukkipati, A				Balgi, Sourabh; Dukkipati, Ambedkar			Contradistinguisher: A Vapnik's Imperative to Unsupervised Domain Adaptation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Graphics processing units; Task analysis; Standards; Adaptation models; Neural networks; Data models; Training; Contrastive feature learning; deep learning; domain adaptation; transfer learning; unsupervised learning		Recent domain adaptation works rely on an indirect way of first aligning the source and target domain distributions and then train a classifier on the labeled source domain to classify the target domain. However, the main drawback of this approach is that obtaining a near-perfect domain alignment in itself might be difficult/impossible (e.g., language domains). To address this, inspired by how humans use supervised-unsupervised learning to perform tasks seamlessly across multiple domains or tasks, we follow Vapnik's imperative of statistical learning that states any desired problem should be solved in the most direct way rather than solving a more general intermediate task and propose a direct approach to domain adaptation that does not require domain alignment. We propose a model referred to as Contradistinguisher that learns contrastive features and whose objective is to jointly learn to contradistinguish the unlabeled target domain in an unsupervised way and classify in a supervised way on the source domain. We achieve the state-of-the-art on Office-31, Digits and VisDA-2017 datasets in both single-source and multi-source settings. We demonstrate that performing data augmentation results in an improvement in the performance over vanilla approach. We also notice that the contradistinguish-loss enhances performance by increasing the shape bias.	[Balgi, Sourabh; Dukkipati, Ambedkar] Indian Inst Sci, Dept Comp Sci & Automat, Bengaluru 560012, Karnataka, India	Indian Institute of Science (IISC) - Bangalore	Balgi, S (corresponding author), Indian Inst Sci, Dept Comp Sci & Automat, Bengaluru 560012, Karnataka, India.	sourabhbalgi@iisc.ac.in; ambedkar@iisc.ac.in		Balgi, Sourabh/0000-0002-3329-5533	Ministry of Human Resource Development (MHRD), Government of India [IISc 001]	Ministry of Human Resource Development (MHRD), Government of India(Ministry of Human Resource Development (MHRD), Government of India)	The authors would like to thank the Ministry of Human Resource Development (MHRD), Government of India, for their generous funding towards this work through the UAY Project: IISc 001. The authors would like to thank Tejas Duseja for helping the authors with setting up some experiments. They also like to thank anonymous reviewers for providing their valuable feedback that helped in improving the manuscript.	Balgi S, 2019, IEEE DATA MINING, P21, DOI 10.1109/ICDM.2019.00012; Bousmalis K, 2016, ADV NEUR IN, V29; Carlucci FM, 2017, LECT NOTES COMPUT SC, V10484, P357, DOI 10.1007/978-3-319-68560-1_32; Carlucci FM, 2017, IEEE I CONF COMP VIS, P5077, DOI 10.1109/ICCV.2017.542; Chen C, 2019, AAAI CONF ARTIF INTE, P3296; Chen CQ, 2019, PROC CVPR IEEE, P627, DOI 10.1109/CVPR.2019.00072; Chen XY, 2019, PR MACH LEARN RES, V97; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Diesendruck G, 2003, CHILD DEV, V74, P168, DOI 10.1111/1467-8624.00528; French Geoffrey, 2018, P INT C LEARN REPR, V6, P6; Ganin Y, 2016, J MACH LEARN RES, V17; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Geirhos R., 2019, PROC 7 INT C LEARN R; Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; Grandvalet Y., 2005, CAP, P529; Gretton A, 2012, J MACH LEARN RES, V13, P723; Haeusser P, 2017, IEEE I CONF COMP VIS, P2784, DOI 10.1109/ICCV.2017.301; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hoffman J, 2018, PR MACH LEARN RES, V80; Hosseini H, 2018, IEEE COMPUT SOC CONF, P2004, DOI 10.1109/CVPRW.2018.00258; Hosseini-Asl Ehsan, 2019, PROC 7 INT C LEARN R; Jacobsen Joern-Henrik, 2019, ICLR; Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503; Kingma D.P, P 3 INT C LEARNING R; Kurmi VK, 2019, PROC CVPR IEEE, P491, DOI 10.1109/CVPR.2019.00058; Laine Samuli, 2017, P INT C LEARN REPR I, P3; LANDAU B, 1992, J MEM LANG, V31, P807, DOI 10.1016/0749-596X(92)90040-5; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee Dong-Hyun, 2013, WORKSH CHALL REPR LE, V3; Li JJ, 2021, IEEE T PATTERN ANAL, V43, P3918, DOI 10.1109/TPAMI.2020.2991050; Liang J, 2019, IEEE T PATTERN ANAL, V41, P1027, DOI 10.1109/TPAMI.2018.2832198; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu MY, 2016, ADV NEUR IN, V29; Liu YC, 2018, PROC CVPR IEEE, P8867, DOI 10.1109/CVPR.2018.00924; Long MS, 2018, ADV NEUR IN, V31; Long MS, 2015, PR MACH LEARN RES, V37, P97; Long MS, 2017, PR MACH LEARN RES, V70; Long MS, 2016, ADV NEUR IN, V29; Louizos C., 2016, 4 INT C LEARN REPR; Mancini M, 2021, IEEE T PATTERN ANAL, V43, P485, DOI 10.1109/TPAMI.2019.2933829; Mancini M, 2018, PROC CVPR IEEE, P3771, DOI 10.1109/CVPR.2018.00397; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Pan YW, 2019, PROC CVPR IEEE, P2234, DOI 10.1109/CVPR.2019.00234; Pandey G, 2017, IEEE DATA MINING, P367, DOI 10.1109/ICDM.2017.46; Paszke A, 2019, ADV NEUR IN, V32; Pedregosa F., 2011, J MACH LEARN RES, V12, P2825; Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149; Peng Xingchao, 2017, VISDA VISUAL DOMAIN; Potrzeba ER, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00446; Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789; Ritter S, 2017, PR MACH LEARN RES, V70; Rozantsev A, 2019, IEEE T PATTERN ANAL, V41, P801, DOI 10.1109/TPAMI.2018.2814042; Ruder S, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1044; Russo P, 2018, PROC CVPR IEEE, P8099, DOI 10.1109/CVPR.2018.00845; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392; Saito K, 2017, PR MACH LEARN RES, V70; Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887; Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140; Shu R., 2018, PROC 6 INT C LEARN R; Simonyan K., 2014, VERY DEEP CONVOLUTIO; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tarvainen A, 2017, ADV NEUR IN, V30; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Vapnik VN, 1999, IEEE T NEURAL NETWOR, V10, P988, DOI 10.1109/72.788640; Wang XM, 2019, ADV NEUR IN, V32; Xie JW, 2015, INT J COMPUT VISION, V114, P91, DOI 10.1007/s11263-014-0757-x; Xie SA, 2018, PR MACH LEARN RES, V80; Xu RJ, 2018, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2018.00417; Zellinger W., 2017, 5 INT C LEARN REPR I; Zhang YS, 2020, IEEE WINT CONF APPL, P173, DOI 10.1109/WACVW50321.2020.9096945	78	0	0	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4730	4747		10.1109/TPAMI.2021.3071225	http://dx.doi.org/10.1109/TPAMI.2021.3071225			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33822721	Green Submitted			2022-12-18	WOS:000836666600021
J	Cherian, A; Stanitsas, P; Wang, J; Harandi, MT; Morellas, V; Papanikolopoulos, N				Cherian, Anoop; Stanitsas, Panagiotis; Wang, Jue; Harandi, Mehrtash T.; Morellas, Vassilios; Papanikolopoulos, Nikos			Learning Log-Determinant Divergences for Positive Definite Matrices	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Region covariance matrices; positive definite matrices; log-det divergence; action recognition; texture recognition	DIMENSIONALITY REDUCTION; ALPHA-BETA; COVARIANCE; ROBUST	Representations in the form of Symmetric Positive Definite (SPD) matrices have been popularized in a variety of visual learning applications due to their demonstrated ability to capture rich second-order statistics of visual data. There exist several similarity measures for comparing SPD matrices with documented benefits. However, selecting an appropriate measure for a given problem remains a challenge and in most cases, is the result of a trial-and-error process. In this paper, we propose to learn similarity measures in a data-driven manner. To this end, we capitalize on the alpha beta-log-det divergence, which is a meta-divergence parametrized by scalars alpha and beta, subsuming a wide family of popular information divergences on SPD matrices for distinct and discrete values of these parameters. Our key idea is to cast these parameters in a continuum and learn them from data. We systematically extend this idea to learn vector-valued parameters, thereby increasing the expressiveness of the underlying non-linear measure. We conjoin the divergence learning problem with several standard tasks in machine learning, including supervised discriminative dictionary learning and unsupervised SPD matrix clustering. We present Riemannian gradient descent schemes for optimizing our formulations efficiently, and show the usefulness of our method on eight standard computer vision tasks.	[Cherian, Anoop] Mitsubishi Elect Res Labs MERL, Cambridge, MA 02139 USA; [Cherian, Anoop] Australian Natl Univ, Australian Ctr Robot Vis, Canberra, ACT 2601, Australia; [Stanitsas, Panagiotis; Morellas, Vassilios; Papanikolopoulos, Nikos] Univ Minnesota, Minneapolis, MN 55455 USA; [Wang, Jue] Australian Natl Univ, Res Sch Engn, Canberra, ACT 2601, Australia; [Harandi, Mehrtash T.] Monash Univ, Dept Elect & Comp Syst Engn, Clayton, Vic 3800, Australia; [Harandi, Mehrtash T.] Data61 CSIRO, Melbourne, Vic 71649, Australia	Australian Centre for Robotic Vision; Australian National University; University of Minnesota System; University of Minnesota Twin Cities; Australian National University; Monash University; Commonwealth Scientific & Industrial Research Organisation (CSIRO)	Papanikolopoulos, N (corresponding author), Univ Minnesota, Minneapolis, MN 55455 USA.	cherian@merl.com; stani078@umn.edu; jue.wang@anu.edu.au; mehrtash.harandi@monash.edu; morellas@cs.umn.edu; npapas@cs.umn.edu	Harandi, Mehrtash/D-6586-2018	Harandi, Mehrtash/0000-0002-6937-6300	National Science Foundation [SMA-1028076, CNS-1338042, CNS-1439728, CNS-1514626, CNS-1939033, CNS-1919631]; National Cancer Institute of the NIH [R01C A225435]; Australian Research Council Centre of Excellence for Robotic Vision [CE140100016]	National Science Foundation(National Science Foundation (NSF)); National Cancer Institute of the NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Cancer Institute (NCI)); Australian Research Council Centre of Excellence for Robotic Vision(Australian Research Council)	This workwas supported by theNational Science Foundation under Grants #SMA-1028076, #CNS-1338042, #CNS-1439728, #CNS-1514626, #CNS-1939033, and #CNS-1919631. Anoop Cherian was funded by the Australian Research Council Centre of Excellence for Robotic Vision (#CE140100016). Research reported in this publication was supported by the National Cancer Institute of the NIH under Award Number R01C A225435. Anoop Cherian and Panagiotis Stanitsas contributed equally to this work.	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Amari S.-I., 2007, METHODS INFORM GEOME; Arsigny V, 2006, MAGN RESON MED, V56, P411, DOI 10.1002/mrm.20965; Basu A, 1998, BIOMETRIKA, V85, P549, DOI 10.1093/biomet/85.3.549; Belkin M, 2002, ADV NEUR IN, V14, P585; Bhattacharyya A., 1943, BULL CALCUTTA MATH S, V35, P99; Bini DA, 2013, LINEAR ALGEBRA APPL, V438, P1700, DOI 10.1016/j.laa.2011.08.052; Birgin EG, 2001, ACM T MATH SOFTWARE, V27, P340, DOI 10.1145/502800.502803; Brox T, 2006, IMAGE VISION COMPUT, V24, P41, DOI 10.1016/j.imavis.2005.09.010; Cherian A, 2017, IEEE I CONF COMP VIS, P4280, DOI 10.1109/ICCV.2017.458; Cherian A, 2019, INT J COMPUT VISION, V127, P340, DOI 10.1007/s11263-018-1111-5; Cherian A, 2017, IEEE T NEUR NET LEAR, V28, P2859, DOI 10.1109/TNNLS.2016.2601307; Cherian A, 2017, IEEE WINT CONF APPL, P130, DOI 10.1109/WACV.2017.22; Cherian A, 2016, IEEE T PATTERN ANAL, V38, P862, DOI 10.1109/TPAMI.2015.2456903; Cherian A, 2013, IEEE T PATTERN ANAL, V35, P2161, DOI 10.1109/TPAMI.2012.259; Cichocki A, 2015, ENTROPY-SWITZ, V17, P2988, DOI 10.3390/e17052988; Cichocki Andrzej, 2009, NONNEGATIVE MATRIX T, P2; Dhillon I., 2005, P 18 INT C NEUR INF, P283; Dikmen O, 2015, IEEE T PATTERN ANAL, V37, P1442, DOI 10.1109/TPAMI.2014.2366144; Fehr D., 2013, THESIS U MINNESOTA M; Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41; Goh A, 2008, PROC CVPR IEEE, P626; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Harandi M, 2018, IEEE T PATTERN ANAL, V40, P48, DOI 10.1109/TPAMI.2017.2655048; Harandi M, 2015, PROC CVPR IEEE, P3926, DOI 10.1109/CVPR.2015.7299018; Harandi M, 2014, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2014.132; Hinton Geoffrey, 2002, ADV NEURAL INFORM PR, V15, P833, DOI DOI 10.1109/TSMCB.2011.2106208; Huang ZW, 2017, AAAI CONF ARTIF INTE, P2036; Huang ZW, 2015, PR MACH LEARN RES, V37, P720; Ionescu C, 2015, IEEE I CONF COMP VIS, P2965, DOI 10.1109/ICCV.2015.339; Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396; Kompass R, 2007, NEURAL COMPUT, V19, P780, DOI 10.1162/neco.2007.19.3.780; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Kulis B., 2006, P 23 INT C MACH LEAR, P505; Kylberg G, 2012, J MICROSC-OXFORD, V245, P140, DOI 10.1111/j.1365-2818.2011.03556.x; Lafferty J., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P125, DOI 10.1145/307400.307422; Lai K, 2011, IEEE INT CONF ROBOT, P1817; Lakshminarayanan B, 2017, ADV NEUR IN, V30; Li PH, 2013, IEEE I CONF COMP VIS, P1601, DOI 10.1109/ICCV.2013.202; Lin SY, 2018, LECT NOTES COMPUT SC, V11207, P639, DOI 10.1007/978-3-030-01219-9_38; Luo SL, 2004, PHYS REV A, V69, DOI 10.1103/PhysRevA.69.032106; Mallikarjuna P., 2006, COMP VIS ACT PERC LA; Mihoko M, 2002, NEURAL COMPUT, V14, P1859, DOI 10.1162/089976602760128045; Minh HQ., 2019, INFORM GEOMETRY, V2, P101; Moakher M, 2006, VISUALIZATION AND PROCESSING OF TENSOR FIELDS, P285, DOI 10.1007/3-540-31272-2_17; Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4; Patel VM, 2014, IEEE IMAGE PROC, P2849, DOI 10.1109/ICIP.2014.7025576; Pelletier B, 2005, STAT PROBABIL LETT, V73, P297, DOI 10.1016/j.spl.2005.04.004; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; Quang M. H., 2014, ADV NEURAL INFORM PR, P388; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Simonyan K, 2014, ADV NEUR IN, V27; Sivalingam R, 2010, LECT NOTES COMPUT SC, V6314, P722, DOI 10.1007/978-3-642-15561-1_52; Sivalingam R, 2009, 2009 THIRD ACM/IEEE INTERNATIONAL CONFERENCE ON DISTRIBUTED SMART CAMERAS, P76; Sra S., 2012, ADV NEURAL INFORM PR, P144; Stanitsas P, 2017, IEEE INT CONF COMP V, P1304, DOI 10.1109/ICCVW.2017.155; Stanitsas P, 2016, INT C PATT RECOG, P1490, DOI 10.1109/ICPR.2016.7899848; Subbarao R, 2009, INT J COMPUT VISION, V84, P1, DOI 10.1007/s11263-008-0195-8; Thiyam DB, 2017, ENTROPY-SWITZ, V19, DOI 10.3390/e19030089; Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589; Wang RP, 2012, PROC CVPR IEEE, P2496, DOI 10.1109/CVPR.2012.6247965; Wang ZZ, 2004, PROC CVPR IEEE, P228; Xie Yuchen, 2013, JMLR Workshop Conf Proc, V28, P1480; Yger F, 2015, EUR SIGNAL PR CONF, P2721, DOI 10.1109/EUSIPCO.2015.7362879; Yin M, 2016, PROC CVPR IEEE, P5157, DOI 10.1109/CVPR.2016.557; Yu KC, 2018, LECT NOTES COMPUT SC, V11211, P621, DOI 10.1007/978-3-030-01234-2_37; Zadeh PH, 2016, PR MACH LEARN RES, V48	72	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5088	5102		10.1109/TPAMI.2021.3073588	http://dx.doi.org/10.1109/TPAMI.2021.3073588			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33856984	Green Submitted			2022-12-18	WOS:000836666600045
J	Douik, A; Hassibi, B				Douik, Ahmed; Hassibi, Babak			Low-Rank Riemannian Optimization for Graph-Based Clustering Applications	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Riemannian manifolds; low-rank factorization; graph-based clustering; convex and non-convex optimization	DOUBLY STOCHASTIC MATRICES	With the abundance of data, machine learning applications engaged increased attention in the last decade. An attractive approach to robustify the statistical analysis is to preprocess the data through clustering. This paper develops a low-complexity Riemannian optimization framework for solving optimization problems on the set of positive semidefinite stochastic matrices. The low-complexity feature of the proposed algorithms stems from the factorization of the optimization variable X = YYT and deriving conditions on the number of columns of Y under which the factorization yields a satisfactory solution. The paper further investigates the embedded and quotient geometries of the resulting Riemannian manifolds. In particular, the paper explicitly derives the tangent space, Riemannian gradients and Hessians, and a retraction operator allowing the design of efficient first and second-order optimization methods for the graph-based clustering applications of interest. The numerical results reveal that the resulting algorithms present a clear complexity advantage as compared with state-of-the-art euclidean and Riemannian approaches for graph clustering applications.	[Douik, Ahmed; Hassibi, Babak] CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA	California Institute of Technology	Douik, A (corresponding author), CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA.	ahmed.douik@caltech.edu; hassibi@caltech.edu						Absil PA, 2007, FOUND COMPUT MATH, V7, P303, DOI 10.1007/s10208-005-0179-9; Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Arora R., 2011, P 28 INT C MACH LEAR, P761; Baker CG, 2008, IMA J NUMER ANAL, V28, P665, DOI 10.1093/imanum/drn029; Berger M., 1988, DIFFERENTIAL GEOMETR, V1st, DOI [10.1007/978-1-4612-1033-7, DOI 10.1007/978-1-4612-1033-7]; Bishop, 1995, NEURAL NETWORKS PATT; Bonnabel S, 2009, SIAM J MATRIX ANAL A, V31, P1055, DOI 10.1137/080731347; Boumal N, 2014, J MACH LEARN RES, V15, P1455; Boyd S, 2004, CONVEX OPTIMIZATION; BRUALDI RA, 1988, LINEAR ALGEBRA APPL, V107, P77, DOI 10.1016/0024-3795(88)90239-X; Buhrmester M, 2011, PERSPECT PSYCHOL SCI, V6, P3, DOI 10.1177/1745691610393980; Datta B., 1972, J COMBINATORIAL THEO, V12, P147; Ding C, 2010, IEEE T PATTERN ANAL, V32, P45, DOI 10.1109/TPAMI.2008.277; Douik A, 2018, PR MACH LEARN RES, V80; Douik A, 2019, IEEE T SIGNAL PROCES, V67, P5761, DOI 10.1109/TSP.2019.2946024; Douik A, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P3959; Douik A, 2018, 2018 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP), P806; GABAY D, 1982, J OPTIMIZ THEORY APP, V37, P177, DOI 10.1007/BF00934767; Grant M., 2014, CVX MATLAB SOFTWARE; Grubisic I, 2007, LINEAR ALGEBRA APPL, V422, P629, DOI 10.1016/j.laa.2006.11.024; HARTFIEL DJ, 1972, P AM MATH SOC, V36, P389, DOI 10.2307/2039166; Helmberg C, 2000, SIAM J OPTIMIZ, V10, P673, DOI 10.1137/S1052623497328987; Huper K, 2004, CONF REC ASILOMAR C, P136; Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359; KYPARISIS J, 1985, MATH PROGRAM, V32, P242, DOI 10.1007/BF01586095; Lee J. M., 2010, INTRO TOPOLOGICAL MA; Li F.-F., 2011, PROC CVPR WORKSHOP F, V2; LUENBERGER DG, 1972, MANAGE SCI, V18, P620, DOI 10.1287/mnsc.18.11.620; Mehlum M, 2012, THESIS U OSLO OSLO; Meila M, 2003, LECT NOTES ARTIF INT, V2777, P173, DOI 10.1007/978-3-540-45167-9_14; Minming Chen, 2013, Arxiv, DOI arXiv:1009.5055; Petersen P., 1998, RIEMANNIAN GEOMETRY; Ring W, 2012, SIAM J OPTIMIZ, V22, P596, DOI 10.1137/11082885X; Sato H, 2016, COMPUT OPTIM APPL, V64, P101, DOI 10.1007/s10589-015-9801-1; Sato H, 2015, OPTIMIZATION, V64, P1011, DOI 10.1080/02331934.2013.836650; Shanmugam K, 2013, IEEE INT SYMP INFO, P1152, DOI 10.1109/ISIT.2013.6620407; SINKHORN R, 1964, ANN MATH STAT, V35, P876, DOI 10.1214/aoms/1177703591; Vandereycken B, 2013, SIAM J OPTIMIZ, V23, P1214, DOI 10.1137/110845768; Vinayak RK, 2016, IEEE INT SYMP INFO, P91, DOI 10.1109/ISIT.2016.7541267; Wang XQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1245, DOI 10.1145/2939672.2939805; Yang Z., 2012, P 29 INT C MACH LEAR, P707; Zass R., 2006, P ADV NEUR INF PROC, P1569	46	0	0	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5133	5148		10.1109/TPAMI.2021.3074467	http://dx.doi.org/10.1109/TPAMI.2021.3074467			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33877969	Green Accepted			2022-12-18	WOS:000836666600048
J	Geng, X; Zheng, RY; Lv, JQ; Zhang, Y				Geng, Xin; Zheng, Renyi; Lv, Jiaqi; Zhang, Yu			Multilabel Ranking With Inconsistent Rankers	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Training; Predictive models; Adaptation models; Task analysis; Machine learning; Machine learning algorithms; Encoding; Multilabel ranking; instance-oriented preference distribution learning; ranker-oriented preference distribution learning; machine learning	LABEL RANKING; ALGORITHM; RULES	While most existing multilabel ranking methods assume the availability of a single objective label ranking for each instance in the training set, this paper deals with a more common case where only subjective inconsistent rankings from multiple rankers are associated with each instance. Two ranking methods are proposed from the perspective of instances and rankers, respectively. The first method, Instance-oriented Preference Distribution Learning (IPDL), is to learn a latent preference distribution for each instance. IPDL generates a common preference distribution that is most compatible to all the personal rankings, and then learns a mapping from the instances to the preference distributions. The second method, Ranker-oriented Preference Distribution Learning (RPDL), is proposed by leveraging interpersonal inconsistency among rankers, to learn a unified model from personal preference distribution models of all rankers. These two methods are applied to natural scene images dataset and 3D facial expression dataset BU_3DFE. Experimental results show that IPDL and RPDL can effectively incorporate the information given by the inconsistent rankers, and perform remarkably better than the compared state-of-the-art multilabel ranking algorithms.	[Geng, Xin; Zheng, Renyi; Lv, Jiaqi; Zhang, Yu] Southeast Univ, Sch Comp Sci & Engn, Minist Educ, Nanjing 211189, Peoples R China; [Geng, Xin; Zheng, Renyi; Lv, Jiaqi; Zhang, Yu] Southeast Univ, Key Isib Comp Network & Informat Integrat, Minist Educ, Nanjing 211189, Peoples R China	Southeast University - China; Southeast University - China	Geng, X (corresponding author), Southeast Univ, Sch Comp Sci & Engn, Minist Educ, Nanjing 211189, Peoples R China.; Geng, X (corresponding author), Southeast Univ, Key Isib Comp Network & Informat Integrat, Minist Educ, Nanjing 211189, Peoples R China.	xgeng@seu.edu.cn; zhengry@seu.edu.cn; lvjiaqi@seu.edu.cn; zhang_yu@seu.edu.cn			National Key Research and Development Plan of China [2017YFB1002801]; National Science Foundation of China [61622203]; Collaborative Innovation Center of Novel Software Technology and Industrialization; Collaborative Innovation Center of Wireless Communications Technology	National Key Research and Development Plan of China; National Science Foundation of China(National Natural Science Foundation of China (NSFC)); Collaborative Innovation Center of Novel Software Technology and Industrialization; Collaborative Innovation Center of Wireless Communications Technology	This work was supported in part by the National Key Research and Development Plan of China under Grant 2017YFB1002801, in part by the National Science Foundation of China under Grant 61622203, in part by the Collaborative Innovation Center of Novel Software Technology and Industrialization, and in part by the Collaborative Innovation Center of Wireless Communications Technology.	Agresti A., 2010, ANAL ORDINAL CATEGOR, DOI [DOI 10.1002/9780470594001, 10.1002/9780470594001]; Aiguzhinov A, 2010, LECT NOTES ARTIF INT, V6332, P16, DOI 10.1007/978-3-642-16184-1_2; Aledo JA, 2017, INFORM FUSION, V35, P38, DOI 10.1016/j.inffus.2016.09.002; Berger AL, 1996, COMPUT LINGUIST, V22, P39; Brinker K., 2019, PROC EUR C MACH LEAR, P204; Brinker K, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P702; Brinker K, 2006, FRONT ARTIF INTEL AP, V141, P489; Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249; Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020; Cao Z., 2007, P 24 INT C MACH LEAR, P129, DOI DOI 10.1145/1273496.1273513; Alfaro JC, 2019, LECT NOTES ARTIF INT, V11726, P351, DOI 10.1007/978-3-030-29765-7_29; Cha S.H, 2007, CITY, V1, P300, DOI DOI 10.1007/S00167-009-0884-Z; Chen WY, 2009, PROCEEDINGS OF THE 2009 INTERNATIONAL CONFERENCE ON PUBLIC ECONOMICS AND MANAGEMENT ICPEM 2009, VOL 6, P21; Cheng W., 2012, ADV NEURAL INFORM PR, P2501; Cheng W., 2012, THESIS PHILIPPS U MA; Cheng W., 2013, PROC MULTIDISCIPLINA, P1; Cheng W., 2010, ICML, P215; Cheng WW, 2010, LECT NOTES ARTIF INT, V6321, P215, DOI 10.1007/978-3-642-15880-3_20; Cohen WW, 1999, J ARTIF INTELL RES, V10, P243, DOI 10.1613/jair.587; Corrente S, 2013, MACH LEARN, V93, P381, DOI 10.1007/s10994-013-5365-4; de Sa CR, 2018, INFORM FUSION, V40, P112, DOI 10.1016/j.inffus.2017.07.001; de Sa CR, 2017, EXPERT SYST, V34, DOI 10.1111/exsy.12166; de Sa CR, 2016, INFORM SCIENCES, V329, P921, DOI 10.1016/j.ins.2015.04.022; de Sa CR, 2015, LECT NOTES ARTIF INT, V9273, P525, DOI 10.1007/978-3-319-23485-4_52; de Sa CR, 2013, LECT NOTES ARTIF INT, V8140, P155, DOI 10.1007/978-3-642-40897-7_11; de Sa CR, 2011, LECT NOTES ARTIF INT, V6635, P432, DOI 10.1007/978-3-642-20847-8_36; Dekel O, 2004, ADV NEUR IN, V16, P497; Destercke Sebastien, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference (ECML PKDD 2013). Proceedings: LNCS 8189, P112, DOI 10.1007/978-3-642-40991-2_8; Destercke S, 2015, EUR J OPER RES, V246, P927, DOI 10.1016/j.ejor.2015.05.005; Djuric N, 2014, AAAI CONF ARTIF INTE, P1788; Fagin R., 2004, P ACM S PRINC DAT SY, P47, DOI DOI 10.1145/1055558.1055568; FAYYAD UM, 1993, IJCAI-93, VOLS 1 AND 2, P1022; Freund Y, 2004, J MACH LEARN RES, V4, P933, DOI 10.1162/1532443041827916; Fu YW, 2016, IEEE T PATTERN ANAL, V38, P563, DOI 10.1109/TPAMI.2015.2456887; Furnkranz J, 2003, LECT NOTES ARTIF INT, V2837, P145; Furnkranz J, 2010, PREFERENCE LEARNING, P1; GEHRLEIN WV, 1983, THEOR DECIS, V15, P161, DOI 10.1007/BF00143070; Geng X, 2016, IEEE T KNOWL DATA EN, V28, P1734, DOI 10.1109/TKDE.2016.2545658; Geng X, 2014, PROC CVPR IEEE, P3742, DOI 10.1109/CVPR.2014.478; Grbovic M., 2013, P 23 INT JOINT C ART, P1358; Gurrieri M, 2012, CCIS, P613, DOI [10.1007/978-3-642-31709-5_62, DOI 10.1007/978-3-642-31709-5-62]; Gurrieri M, 2014, COMM COM INF SC, V443, P464; Habibu R., 2012, PRICAI 2012 TRENDS A, P470; Hall M., 2008, WEKA DATA MINING SOF, V11, P10, DOI [10.1145/1656274.1656278, DOI 10.1145/1656274.1656278]; Hanselle Jonas, 2020, KI 2020: Advances in Artificial Intelligence. 43rd German Conference on AI. Proceedings. Lecture Notes in Artificial Intelligence Subseries of Lecture Notes in Computer Science (LNAI 12325), P59, DOI 10.1007/978-3-030-58285-2_5; Har-Peled S., 2002, P 15 INT C NEUR INF, P809; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hullermeier E, 2008, ARTIF INTELL, V172, P1897, DOI 10.1016/j.artint.2008.08.002; Hullermeier E, 2004, IEEE INT CONF FUZZY, P97; Jiang XY, 2011, MATH PROGRAM, V127, P203, DOI 10.1007/s10107-010-0419-x; Kovashka A, 2012, PROC CVPR IEEE, P2973, DOI 10.1109/CVPR.2012.6248026; MALLOWS CL, 1957, BIOMETRIKA, V44, P114, DOI 10.2307/2333244; Mohri M., 2018, FDN MACHINE LEARNING; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Osting B, 2013, INVERSE PROBL IMAG, V7, P907, DOI 10.3934/ipi.2013.7.907; Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281; Ribeiro Geraldina, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. 22nd International Conference on Artificial Neural Networks, P25, DOI 10.1007/978-3-642-33266-1_4; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schafer D, 2018, MACH LEARN, V107, P903, DOI 10.1007/s10994-017-5694-9; Schafer D, 2015, LECT NOTES ARTIF INT, V9285, P227, DOI 10.1007/978-3-319-23525-7_14; Schafer D., 2016, PROC LERNEN WISSEN D, P323; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Todorovski L, 2002, LECT NOTES ARTIF INT, V2430, P444; Tsoumakas G., 2007, INT J DATA WAREHOUSI, V3, P1; Vitelli V, 2018, J MACH LEARN RES, V18, P1; Waltz RA, 2006, MATH PROGRAM, V107, P391, DOI 10.1007/s10107-004-0560-5; Wang QS, 2011, 2011 FIRST ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P164, DOI 10.1109/ACPR.2011.6166699; Werbin-Ofir H, 2019, EXPERT SYST APPL, V136, P50, DOI 10.1016/j.eswa.2019.06.022; Xu QQ, 2019, PROC CVPR IEEE, P8985, DOI 10.1109/CVPR.2019.00920; Zhang J, 2018, AAAI CONF ARTIF INTE, P4422; Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019; Zhou YM, 2018, EXPERT SYST APPL, V112, P99, DOI 10.1016/j.eswa.2018.06.036; Zhou YM, 2014, KNOWL-BASED SYST, V72, P108, DOI 10.1016/j.knosys.2014.08.029; Zhou YM, 2014, J COMPUT, V9, P557, DOI 10.4304/jcp.9.3.557-565	75	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5211	5224		10.1109/TPAMI.2021.3070709	http://dx.doi.org/10.1109/TPAMI.2021.3070709			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33798071				2022-12-18	WOS:000836666600053
J	Guo, SQ; Yan, Q; Su, X; Hu, XL; Chen, F				Guo, Shangqi; Yan, Qi; Su, Xin; Hu, Xiaolin; Chen, Feng			State-Temporal Compression in Reinforcement Learning With the Reward-Restricted Geodesic Metric	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Measurement; Task analysis; Reinforcement learning; Neural networks; Time-domain analysis; Semiconductor device measurement; Mathematical model; Semi-Markov decision process (SMDP); reward-restricted geodesic (RRG) metric; option; state compression; state-temporal compression; reinforcement learning (RL)	FRAMEWORK; MDPS	It is difficult to solve complex tasks that involve large state spaces and long-term decision processes by reinforcement learning (RL) algorithms. A common and promising method to address this challenge is to compress a large RL problem into a small one. Towards this goal, the compression should be state-temporal and optimality-preserving (i.e., the optimal policy of the compressed problem should correspond to that of the uncompressed problem). In this paper, we propose a reward-restricted geodesic (RRG) metric, which can be learned by a neural network, to perform state-temporal compression in RL. We prove that compression based on the RRG metric is approximately optimality-preserving for the raw RL problem endowed with temporally abstract actions. With this compression, we design an RRG metric-based reinforcement learning (RRG-RL) algorithm to solve complex tasks. Experiments in both discrete (2D Minecraft) and continuous (Doom) environments demonstrated the superiority of our method over existing RL approaches.	[Guo, Shangqi; Yan, Qi; Su, Xin; Chen, Feng] Tsinghua Univ, Dept Automat, Beijing 100086, Peoples R China; [Guo, Shangqi; Yan, Qi; Su, Xin; Chen, Feng] Beijing Innovat Ctr Future Chip, Beijing 100086, Peoples R China; [Guo, Shangqi; Yan, Qi; Su, Xin; Chen, Feng] LSBDPA Beijing Key Lab, Beijing 100084, Peoples R China; [Hu, Xiaolin] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, Inst Artificial Intelligence, Dept Comp Sci & Technol,State Key Lab Intelligent, Beijing 100084, Peoples R China	Tsinghua University; Tsinghua University	Chen, F (corresponding author), Tsinghua Univ, Dept Automat, Beijing 100086, Peoples R China.; Chen, F (corresponding author), Beijing Innovat Ctr Future Chip, Beijing 100086, Peoples R China.; Hu, XL (corresponding author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, Inst Artificial Intelligence, Dept Comp Sci & Technol,State Key Lab Intelligent, Beijing 100084, Peoples R China.	gsq15@mail.tsinghua.edu.cn; q-yan15@mail.tsinghua.edu.cn; suxin16@mail.tsinghua.edu.cn; xlhu@mail.tsinghua.edu.cn; chenfeng@mail.tsinghua.edu.cn		Su, Xin/0000-0002-5979-8702; Hu, Xiaolin/0000-0002-4907-7354	National Natural Science Foundation of China [61671266, 61836004, U19B2034, 61836014, 2019GQG0006]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by the National Natural Science Foundation of China under Grant 61671266, Grant 61836004, Grant U19B2034, and Grant 61836014 and in part by the Tsinghua-Guoqiang research program under Grant 2019GQG0006.	Abel D, 2020, PR MACH LEARN RES, V108, P1639; Abel D, 2018, PR MACH LEARN RES, V80; Abel D, 2016, PR MACH LEARN RES, V48; Andreas J, 2017, PR MACH LEARN RES, V70; Aravind Srinivas, 2020, Arxiv, DOI arXiv:1605.05359; Arjona-Medina JA, 2019, ADV NEUR IN, V32; Barreto A, 2017, ADV NEUR IN, V30; Bhatia S. K., 2004, P INT FLOR ART INT R, P695; Botvinick MM, 2012, CURR OPIN NEUROBIOL, V22, P956, DOI 10.1016/j.conb.2012.05.008; Bouttier J, 2003, NUCL PHYS B, V663, P535, DOI 10.1016/S0550-3213(03)00355-9; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Castro Pablo Samuel, 2012, Recent Advances in Reinforcement Learning. 9th European Workshop (EWRL 2011). Revised Selected Papers, P140, DOI 10.1007/978-3-642-29946-9_16; Castro P. S., 2011, THESIS SCH COMPUT SC; Castro PS, 2020, AAAI CONF ARTIF INTE, V34, P10069; Castro PS, 2010, AAAI CONF ARTIF INTE, P1065; DAYAN P, 1993, NEURAL COMPUT, V5, P613, DOI 10.1162/neco.1993.5.4.613; Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639; Dietterich TG, 2000, ADV NEUR IN, V12, P994; Ferns N, 2004, PROCEEDING OF THE NINETEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE SIXTEENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE, P950; Ferns N., 2005, P 21 C UNC ART INT U, P201; Gibbs AL, 2002, INT STAT REV, V70, P419, DOI 10.2307/1403865; Greg Brockman, 2016, Arxiv, DOI arXiv:1606.01540; Hadsell R., 2006, 2006 IEEE COMPUTER S, P1735, DOI DOI 10.1109/CVPR.2006.100; Jiang N, 2015, PR MACH LEARN RES, V37, P179; Jiang YD, 2019, ADV NEUR IN, V32; Kakade Sham M., 2003, THESIS; Kempka M, 2016, IEEE CONF COMPU INTE; Kingma D.P, P 3 INT C LEARNING R; Klein U., 2004, P 21 INT C MACH LEAR, P71, DOI DOI 10.1145/1015330.1015355; Konidaris G, 2019, CURR OPIN BEHAV SCI, V29, P1, DOI 10.1016/j.cobeha.2018.11.005; Konidaris G, 2018, J ARTIF INTELL RES, V61, P215, DOI 10.1613/jair.5575; Konidaris G, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3619; Kulkarni TD, 2016, ADV NEUR IN, V29; Li L., 2006, ISAIM; Machado MC, 2017, PR MACH LEARN RES, V70; Machado MC, 2018, P INT C LEARN REPR; Mahadevan S, 2007, J MACH LEARN RES, V8, P2169; Mattos TG, 2012, PHYS REV E, V86, DOI 10.1103/PhysRevE.86.031143; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mugan J, 2012, IEEE T AUTON MENT DE, V4, P70, DOI 10.1109/TAMD.2011.2160943; Nachum O., 2018, ARXIV181001257, P1; Nachum O, 2018, ADV NEUR IN, V31; Oh J, 2017, PR MACH LEARN RES, V70; Ortner R, 2007, LECT NOTES ARTIF INT, V4754, P373; Pathak D, 2017, IEEE COMPUT SOC CONF, P488, DOI 10.1109/CVPRW.2017.70; Pong V., 2018, INT C LEARN REPR ICL; Rafati J., 2019, PROC 7 INT C LEARN R; Ravindran B., 2004, P 5 INT C KNOWL BAS; Ravindran B., 2004, THESIS U MASSCHUSETT; Ravindran B., 2003, P 12 INT JOINT C ART, P1011; Savinov N., 2019, INT C LEARNING REPRE; Savinov N., 2018, PROC INT C LEARN REP; Strehl AL, 2009, J MACH LEARN RES, V10, P2413; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Taiga A. A., 2018, ARXIV; Taylor J., 2008, ADV NEURAL INFORM PR, V21, P1649; Taylor ME, 2009, J MACH LEARN RES, V10, P1633; Tiwari S, 2019, AAAI CONF ARTIF INTE, P5175; Verechtchaguina T, 2006, PHYS REV E, V73, DOI 10.1103/PhysRevE.73.031108; Vezhnevets AS, 2017, PR MACH LEARN RES, V70; Yang L., 2006, DISTANCE METRIC LEAR	63	0	0	6	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5572	5589		10.1109/TPAMI.2021.3069005	http://dx.doi.org/10.1109/TPAMI.2021.3069005			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33764874				2022-12-18	WOS:000836666600076
J	Han, RZ; Feng, W; Zhang, YJ; Zhao, JW; Wang, S				Han, Ruize; Feng, Wei; Zhang, Yujun; Zhao, Jiewen; Wang, Song			Multiple Human Association and Tracking From Egocentric and Complementary Top Views	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cameras; Videos; Collaboration; Graphical models; Distribution functions; Trajectory; Optimization; Crowded scene surveillance; top view; horizontal view; complementary view; human association; tracking; wearable cameras; video surveillance; egocentric perception	MULTITARGET; MODEL	Crowded scene surveillance can significantly benefit from combining egocentric-view and its complementary top-view cameras. A typical setting is an egocentric-view camera, e.g., a wearable camera on the ground capturing rich local details, and a top-view camera, e.g., a drone-mounted one from high altitude providing a global picture of the scene. To collaboratively analyze such complementary-view videos, an important task is to associate and track multiple people across views and over time, which is challenging and differs from classical human tracking, since we need to not only track multiple subjects in each video, but also identify the same subjects across the two complementary views. This paper formulates it as a constrained mixed integer programming problem, wherein a major challenge is how to effectively measure subjects similarity over time in each video and across two views. Although appearance and motion consistencies well apply to over-time association, they are not good at connecting two highly different complementary views. To this end, we present a spatial distribution based approach to reliable cross-view subject association. We also build a dataset to benchmark this new challenging task. Extensive experiments verify the effectiveness of our method.	[Han, Ruize; Feng, Wei; Zhang, Yujun; Zhao, Jiewen] Tianjin Univ, Coll Intelligence & Comput, Sch Comp Sci & Technol, Tianjin 300350, Peoples R China; [Han, Ruize; Feng, Wei; Zhang, Yujun; Zhao, Jiewen] SACH, Key Res Ctr Surface Inspect & Anal Cultural Rel, Tianjin 300350, Peoples R China; [Wang, Song] Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29201 USA; [Wang, Song] Tianjin Univ, Coll Intelligence & Comp, Sch Comp Sci & Technol, Tianjin 300350, Peoples R China	Tianjin University; University of South Carolina System; University of South Carolina Columbia; Tianjin University	Feng, W (corresponding author), Tianjin Univ, Coll Intelligence & Comput, Sch Comp Sci & Technol, Tianjin 300350, Peoples R China.; Wang, S (corresponding author), Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29201 USA.	han_ruize@tju.edu.cn; wfeng@tju.edu.cn; yujunzhang@tju.edu.cn; zhaojw@tju.edu.cn; songwang@cec.sc.edu		Wang, Song/0000-0003-4152-5295	NSFC [U1803264, 62072334, 61672376, 61671325]	NSFC(National Natural Science Foundation of China (NSFC))	The authors would like to thank Nan Li, Zicheng Niu, Yiyang Gan, Tingliang Feng, Xiaotao Liu, Haomin Yan, Yibo Shi, and Qian Zhang for the daily academic communication for this paper. They would also like to thank Sibo Wang, Shuai Wang, Chenxing Gong, Xiaoyu Zhang, Yun Wang, and Haibo Li for their kind assistance in the collection and annotation of our dataset. This work was supported by the NSFC under Grants U1803264, 62072334, 61672376, and 61671325.	Ardeshir S, 2018, LECT NOTES COMPUT SC, V11215, P300, DOI 10.1007/978-3-030-01252-6_18; Ardeshir S, 2019, IEEE T PATTERN ANAL, V41, P1353, DOI 10.1109/TPAMI.2018.2832121; Ardeshir S, 2016, LECT NOTES COMPUT SC, V9909, P253, DOI 10.1007/978-3-319-46454-1_16; Ayazoglu M, 2011, IEEE I CONF COMP VIS, P2462, DOI 10.1109/ICCV.2011.6126531; Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103; Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691; Chenxing Gong, 2019, Arxiv, DOI arXiv:1907.11458; Feng W, 2019, IEEE T IMAGE PROCESS, V28, P3232, DOI 10.1109/TIP.2019.2895411; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fleuret F, 2008, IEEE T PATTERN ANAL, V30, P267, DOI 10.1109/TPAMI.2007.1174; Grauman K, 2005, IEEE I CONF COMP VIS, P1458; Guo Q, 2020, IEEE T IMAGE PROCESS, V29, P2999, DOI 10.1109/TIP.2019.2955292; Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196; Hadsell R., 2006, 2006 IEEE COMPUTER S, P1735, DOI DOI 10.1109/CVPR.2006.100; Han RZ, 2020, AAAI CONF ARTIF INTE, V34, P10917; Han RZ, 2020, IEEE T IMAGE PROCESS, V29, P7128, DOI 10.1109/TIP.2020.2998978; Han RZ, 2018, IEEE INT CON MULTI; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hofmann M, 2013, PROC CVPR IEEE, P3650, DOI 10.1109/CVPR.2013.468; Izadinia H, 2012, LECT NOTES COMPUT SC, V7577, P100, DOI 10.1007/978-3-642-33783-3_8; Koch G., 2015, ICML DEEP LEARN WORK; Kuang Hongwu, 2020, ARXIV200203138; Leal-Taixe L., 2015, ARXIV; Liang GQ, 2019, IEEE T IMAGE PROCESS, V28, P3821, DOI 10.1109/TIP.2019.2899782; Liang M, 2018, LECT NOTES COMPUT SC, V11220, P663, DOI 10.1007/978-3-030-01270-0_39; Liu XB, 2016, AAAI CONF ARTIF INTE, P3553; Liu XB, 2018, IEEE T CIRC SYST VID, V28, P2884, DOI 10.1109/TCSVT.2017.2781738; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Milan A, 2014, IEEE T PATTERN ANAL, V36, P58, DOI 10.1109/TPAMI.2013.103; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Ruize Han, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P2746, DOI 10.1145/3394171.3413659; Tang Z, 2018, IEEE INT CON MULTI; Wu Z, 2009, IEEE I CONF COMP VIS, P1546; Xu JR, 2019, IEEE I CONF COMP VIS, P3987, DOI 10.1109/ICCV.2019.00409; Xu YL, 2017, AAAI CONF ARTIF INTE, P4299; Xu YL, 2016, PROC CVPR IEEE, P4256, DOI 10.1109/CVPR.2016.461; Yang B., 2018, P C ROB LEARN, P146; Yang B, 2018, PROC CVPR IEEE, P7652, DOI 10.1109/CVPR.2018.00798; Yang B, 2012, PROC CVPR IEEE, P1918, DOI 10.1109/CVPR.2012.6247892; Zhang WW, 2019, IEEE I CONF COMP VIS, P2365, DOI 10.1109/ICCV.2019.00245; Zhao JW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2608, DOI 10.1145/3394171.3413903; Zheng K, 2017, IEEE I CONF COMP VIS, P2877, DOI 10.1109/ICCV.2017.311; Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541; Zhu J, 2018, LECT NOTES COMPUT SC, V11209, P379, DOI 10.1007/978-3-030-01228-1_23	55	0	0	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5225	5242		10.1109/TPAMI.2021.3070562	http://dx.doi.org/10.1109/TPAMI.2021.3070562			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33798068				2022-12-18	WOS:000836666600054
J	Huang, C; Dang, YJ; Chen, P; Yang, X; Cheng, KT				Huang, Chong; Dang, Yuanjie; Chen, Peng; Yang, Xin; Cheng, Kwang-Ting			One-Shot Imitation Drone Filming of Human Motion Videos	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Videos; Cameras; Drones; Training; Task analysis; Feature extraction; Robot vision systems; Autonomous drone cinematography; imitation learning; one-shot imitation filming; style feature; motion prediction		Imitation learning has recently been applied to mimic the operation of a cameraman in existing autonomous camera systems. To imitate a certain demonstration video, existing methods require users to collect a significant number of training videos with a similar filming style. Because the trained model is style-specific, it is challenging to generalize the model to imitate other videos with a different filming style. To address this problem, we propose a framework that we term "one-shot imitation filming", which can imitate a filming style by "seeing" only a single demonstration video of the target style without style-specific model training. This is achieved by two key enabling techniques: 1) filming style feature extraction, which encodes sequential cinematic characteristics of a variable-length video clip into a fixed-length feature vector; and 2) camera motion prediction, which dynamically plans the camera trajectory to reproduce the filming style of the demo video. We implemented the approach with a deep neural network and deployed it on a 6 degrees of freedom (DOF) drone system by first predicting the future camera motions, and then converting them into the drone's control commands via an odometer. Our experimental results on comprehensive datasets and showcases exhibit that the proposed approach achieves significant improvements over conventional baselines, and our approach can mimic the footage of an unseen style with high fidelity.	[Huang, Chong] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA; [Dang, Yuanjie; Chen, Peng] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310023, Zhejiang, Peoples R China; [Yang, Xin] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China; [Cheng, Kwang-Ting] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Kowloon, Hong Kong, Peoples R China	University of California System; University of California Santa Barbara; Zhejiang University of Technology; Huazhong University of Science & Technology; Hong Kong University of Science & Technology	Yang, X (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.	chonghuang@ucsb.edu; dangyj@zjut.edu.cn; chenpeng@zjut.edu.cn; xinyang2014@hust.edu.cn; timcheng@ust.hk	Chen, Peng/T-7500-2019	Chen, Peng/0000-0001-6122-0574; Cheng, Kwang-Ting Tim/0000-0002-3885-4912	Hong Kong General Research Fund (GRF) [16203319]; Natural Science Foundation of China [U1909203]	Hong Kong General Research Fund (GRF); Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by the Hong Kong General Research Fund (GRF) 16203319 and the Natural Science Foundation of China under Grant U1909203.	Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Ba J., 2017, P 3 INT C LEARN REPR; Berndt D. J., 1994, P 3 INT C KNOWL DISC, P359; Bhattacharya S, 2014, IEEE T MULTIMEDIA, V16, P686, DOI 10.1109/TMM.2014.2300833; Chen JH, 2017, COMPUT VIS IMAGE UND, V159, P59, DOI 10.1016/j.cviu.2016.10.017; Chen JH, 2016, PROC CVPR IEEE, P4688, DOI 10.1109/CVPR.2016.507; Chen JH, 2015, IEEE WINT CONF APPL, P215, DOI 10.1109/WACV.2015.36; Dan B Goldman, 2016, Arxiv, DOI arXiv:1610.01691; dji, 2021, DJI QUICKSHOT MODES; Finn C, 2017, PR MACH LEARN RES, V70; Galvane Q., 2016, P EUR WORKSH INT CIN, P23, DOI DOI 10.2312/WICED.20161097; Galvane Q, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181975; Gschwindt M, 2019, IEEE INT C INT ROBOT, P1107, DOI 10.1109/IROS40897.2019.8967592; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hu HN, 2017, PROC CVPR IEEE, P1396, DOI 10.1109/CVPR.2017.153; Huang C, 2019, PROC CVPR IEEE, P4239, DOI 10.1109/CVPR.2019.00437; Huang C, 2019, IEEE INT CONF ROBOT, P1871, DOI 10.1109/ICRA.2019.8793915; Huang C, 2018, IEEE INT C INT ROBOT, P4692, DOI 10.1109/IROS.2018.8594333; Huang C, 2018, IEEE INT CONF ROBOT, P7039; Kang H, 2018, IEEE ROBOT AUTOM LET, V3, P3717, DOI 10.1109/LRA.2018.2856271; Lewandowski B, 2019, IEEE INT C INT ROBOT, P441, DOI 10.1109/IROS40897.2019.8968506; Li K, 2017, IEEE T IMAGE PROCESS, V26, P2261, DOI 10.1109/TIP.2017.2678800; Lim H, 2015, IEEE INT CONF ROBOT, P2182, DOI 10.1109/ICRA.2015.7139487; Nageli T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073712; Nageli T, 2017, IEEE ROBOT AUTOM LET, V2, P1696, DOI 10.1109/LRA.2017.2665693; OpenSFM, 2017, US; Rath GB, 1999, IEEE T CIRC SYST VID, V9, P1075, DOI 10.1109/76.795060; Sharmin N, 2012, SENSORS-BASEL, V12, P12694, DOI 10.3390/s120912694; Shi XJ, 2015, ADV NEUR IN, V28; Smith C., 2016, PHOTOGRAPHERS GUIDE; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; store.dji, 2017, GUIDES FILM; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684	34	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5335	5348		10.1109/TPAMI.2021.3067359	http://dx.doi.org/10.1109/TPAMI.2021.3067359			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33735075				2022-12-18	WOS:000836666600060
J	Jiang, B; Wang, BB; Tang, J; Luo, B				Jiang, Bo; Wang, Beibei; Tang, Jin; Luo, Bin			GeCNs: Graph Elastic Convolutional Networks for Data Representation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Convolution; Task analysis; Optimization; Supervised learning; Machine learning; Training; Sparse matrices; Graph convolutional networks; elastic net; semi-supervised classification; graph representation	SELECTION	Graph representation and learning is a fundamental problem in machine learning area. Graph Convolutional Networks (GCNs) have been recently studied and demonstrated very powerful for graph representation and learning. Graph convolution (GC) operation in GCNs can be regarded as a composition of feature aggregation and nonlinear transformation step. Existing GCs generally conduct feature aggregation on a full neighborhood set in which each node computes its representation by aggregating the feature information of all its neighbors. However, this full aggregation strategy is not guaranteed to be optimal for GCN learning and also can be affected by some graph structure noises, such as incorrect or undesired edge connections. To address these issues, we propose to integrate elastic net based selection into graph convolution and propose a novel graph elastic convolution (GeC) operation. In GeC, each node can adaptively select the optimal neighbors in its feature aggregation. The key aspect of the proposed GeC operation is that it can be formulated by a regularization framework, based on which we can derive a simple update rule to implement GeC in a self-supervised manner. Using GeC, we then present a novel GeCN for graph learning. Experimental results demonstrate the effectiveness and robustness of GeCN.	[Jiang, Bo; Wang, Beibei; Tang, Jin; Luo, Bin] Anhui Univ, Sch Comp Sci & Technol, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei 230601, Peoples R China	Anhui University	Tang, J (corresponding author), Anhui Univ, Sch Comp Sci & Technol, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei 230601, Peoples R China.	zeyiabc@163.com; wbb_double@163.com; ahu_tj@163.com; ahu_lb@163.com			Major Project for New Generation of AI [2018AAA0100400]; NSFC Key Projects of International (Regional) Cooperation and Exchanges [61860206004]; National Natural Science Foundation of China [62076004, 62076003]; Cooperative Research Project Program of Nanjing Artificial Intelligence Chip Research, Institute of Automation, Chinese Academy of Sciences	Major Project for New Generation of AI; NSFC Key Projects of International (Regional) Cooperation and Exchanges; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Cooperative Research Project Program of Nanjing Artificial Intelligence Chip Research, Institute of Automation, Chinese Academy of Sciences	This work was supported in part by the Major Project for New Generation of AI under Grant 2018AAA0100400, in part by the NSFC Key Projects of International (Regional) Cooperation and Exchanges under Grant 61860206004, in part by the National Natural Science Foundation of China under Grants 62076004 and 62076003, and in part by the Cooperative Research Project Program of Nanjing Artificial Intelligence Chip Research, Institute of Automation, Chinese Academy of Sciences.	Ba J., 2017, P 3 INT C LEARN REPR; Bo Jiang, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P595, DOI 10.1007/978-3-662-44848-9_38; Bruna J., 2014, C TRACK P; Chen JF, 2018, PR MACH LEARN RES, V80; Defferrard M, 2016, ADV NEUR IN, V29; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Hamilton WL, 2017, ADV NEUR IN, V30; Huang J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3569; Jiang B., 2019, ARXIV; Jiang B., 2019, ARXIV; Jiang B, 2019, PROC CVPR IEEE, P10406, DOI 10.1109/CVPR.2019.01066; Joan Bruna, 2015, Arxiv, DOI arXiv:1506.05163; Kipf T. N., 2017, INT C LEARN REPR, DOI [DOI 10.1109/ICDM.2008.17, DOI 10.1109/ICDM.2019.00070]; Klicpera J., 2019, PROC ANN C NEURAL IN, P13333; Klicpera Johannes, 2019, INT C LEARN REPR ICL; Kriege N.M., 2012, INT C MACHINE LEARNI, P291; Li QM, 2019, PROC CVPR IEEE, P9574, DOI 10.1109/CVPR.2019.00981; Li RY, 2018, AAAI CONF ARTIF INTE, P3546; McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755; McCallum AK, 2000, INFORM RETRIEVAL, V3, P127, DOI 10.1023/A:1009953814988; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Nie FP, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P977, DOI 10.1145/2623330.2623726; Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Shahabi C., 2018, C TRACK P; Shchur O., 2018, ARXIV181105868; Ji SH, 2021, Arxiv, DOI arXiv:1912.00552; Tibshirani R, 2011, J R STAT SOC B, V73, P273, DOI 10.1111/j.1467-9868.2011.00771.x; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Velickovic P., 2018, INT C LEARN REPR; Velickovic P., 2018, P INT C LEARN REPR; Wang F, 2008, IEEE T KNOWL DATA EN, V20, P55, DOI 10.1109/TKDE.2007.190672; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Xinyi Z., 2019, P ICLR, V2018, P1; Yanardag P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1365, DOI 10.1145/2783258.2783417; Yang L, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4070; Yang L, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4054; Zass R., 2006, P ADV NEUR INF PROC, P1569; Zhou DY, 2004, ADV NEUR IN, V16, P321; Zhu Xiaojin., 2003, P ICLR, P912; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	42	0	0	5	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4935	4947		10.1109/TPAMI.2021.3070599	http://dx.doi.org/10.1109/TPAMI.2021.3070599			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33798070				2022-12-18	WOS:000836666600034
J	Joo, K; Li, H; Oh, TH; Kweon, IS				Joo, Kyungdon; Li, Hongdong; Oh, Tae-Hyun; Kweon, In So			Robust and Efficient Estimation of Relative Pose for Cameras on Selfie Sticks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cameras; Calibration; Pose estimation; Geometry; Position measurement; Motion measurement; Wrist; Selfie; selfie stick; relative pose estimation; branch-and-bound; global optimization	CONSENSUS; RANSAC	Taking selfies has become one of the major photographic trends of our time. In this study, we focus on the selfie stick, on which a camera is mounted to take selfies. We observe that a camera on a selfie stick typically travels through a particular type of trajectory around a sphere. Based on this finding, we propose a robust, efficient, and optimal estimation method for relative camera pose between two images captured by a camera mounted on a selfie stick. We exploit the special geometric structure of camera motion constrained by a selfie stick and define this motion as spherical joint motion. Utilizing a novel parametrization and calibration scheme, we demonstrate that the pose estimation problem can be reduced to a 3-degrees of freedom (DoF) search problem, instead of a generic 6-DoF problem. This facilitates the derivation of an efficient branch-and-bound optimization method that guarantees a global optimal solution, even in the presence of outliers. Furthermore, as a simplified case of spherical joint motion, we introduce selfie motion, which has a fewer number of DoF than spherical joint motion. We validate the performance and guaranteed optimality of our method on both synthetic and real-world data. Additionally, we demonstrate the applicability of the proposed method for two applications: refocusing and stylization.	[Joo, Kyungdon] UNIST, Dept Comp Sci, Artificial Intelligence Grad Sch, Ulsan 44919, South Korea; [Li, Hongdong] Australian Natl Univ, Comp Vis & Robot Grp, Canberra, ACT 0200, Australia; [Oh, Tae-Hyun] POSTECH, Dept Elect Engn, Pohang 37673, South Korea; [Oh, Tae-Hyun] POSTECH, Grad Sch AI GSAI, Pohang 37673, South Korea; [Kweon, In So] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea	Ulsan National Institute of Science & Technology (UNIST); Australian National University; Pohang University of Science & Technology (POSTECH); Pohang University of Science & Technology (POSTECH); Korea Advanced Institute of Science & Technology (KAIST)	Oh, TH (corresponding author), POSTECH, Dept Elect Engn, Pohang 37673, South Korea.; Oh, TH (corresponding author), POSTECH, Grad Sch AI GSAI, Pohang 37673, South Korea.	kdjoo369@gmail.com; hongdong.li@anu.edu.au; taehyun@postech.ac.kr; iskweon77@kaist.ac.kr			Institute of Information and Communications Technology Planning and Evaluation (IITP) - Korea government (MSIT) [2020-0-01336]; National Research Foundation of Korea (NRF) Grant - Korea government (MSIT) [NRF-2021R1C1C1005723, NRF-2021R1C1C1006799]	Institute of Information and Communications Technology Planning and Evaluation (IITP) - Korea government (MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); National Research Foundation of Korea (NRF) Grant - Korea government (MSIT)(National Research Foundation of KoreaMinistry of Science & ICT (MSIT), Republic of Korea)	The work of Kyungdon Joo was supported by Institute of Information and Communications Technology Planning and Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2020-0-01336, Artificial Intelligence Graduate School Program (UNIST)) and the National Research Foundation of Korea (NRF) Grant funded by the Korea government (MSIT) (No. NRF-2021R1C1C1005723). The work of Tae-Hyun Oh was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2021R1C1C1006799).	Balakrishnan V., 1991, International Journal of Robust and Nonlinear Control, V1, P295, DOI 10.1002/rnc.4590010404; Bazin Jean-Charles, 2012, P AS C COMP VIS, P2; Campbell D, 2017, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2017.10; Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236; Dai YC, 2016, PROC CVPR IEEE, P4132, DOI 10.1109/CVPR.2016.448; Elqursh A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3049, DOI 10.1109/CVPR.2011.5995512; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fitzgibbon A. W., 1998, 3D Structure from Multiple Images of Large-Scale Environments. European Workshop, SMILE'98. Proceedings, P155; Fraundorfer F, 2010, LECT NOTES COMPUT SC, V6314, P269, DOI 10.1007/978-3-642-15561-1_20; Fredriksson J, 2015, PROC CVPR IEEE, P2684, DOI 10.1109/CVPR.2015.7298884; Ha H, 2018, IEEE SIGNAL PROC LET, V25, P393, DOI 10.1109/LSP.2017.2705138; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Hartley RI, 2007, IEEE I CONF COMP VIS, P534; Hartley RI, 2009, INT J COMPUT VISION, V82, P64, DOI 10.1007/s11263-008-0186-9; Hirschmuller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56; HORAUD R, 1995, INT J ROBOT RES, V14, P195, DOI 10.1177/027836499501400301; Horst R., 1990, GLOBAL OPTIMIZATION, DOI DOI 10.1007/978-3-662-02598-7; Jiang G, 2004, IEEE T PATTERN ANAL, V26, P721, DOI 10.1109/TPAMI.2004.4; Joo K, 2020, IEEE INT CONF ROBOT, P4983, DOI 10.1109/ICRA40945.2020.9196921; Joo K, 2020, IEEE T PATTERN ANAL, V42, P2656, DOI 10.1109/TPAMI.2019.2909863; Joo K, 2018, PROC CVPR IEEE, P5726, DOI 10.1109/CVPR.2018.00600; Joo K, 2019, IEEE T PATTERN ANAL, V41, P682, DOI 10.1109/TPAMI.2018.2799944; Joo K, 2016, PROC CVPR IEEE, P1763, DOI 10.1109/CVPR.2016.195; Li HD, 2009, IEEE I CONF COMP VIS, P1074, DOI 10.1109/ICCV.2009.5459398; LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105; Nguyen RMH, 2014, COMPUT GRAPH FORUM, V33, P319, DOI 10.1111/cgf.12500; Nister D, 2005, MACH VISION APPL, V16, P321, DOI 10.1007/s00138-005-0006-y; Nister D, 2004, IEEE T PATTERN ANAL, V26, P756, DOI 10.1109/TPAMI.2004.17; Oh TH, 2017, IEEE I CONF COMP VIS, P5170, DOI 10.1109/ICCV.2017.552; Qiu WC, 2016, LECT NOTES COMPUT SC, V9915, P909, DOI 10.1007/978-3-319-49409-8_75; Raguram R, 2008, LECT NOTES COMPUT SC, V5303, P500, DOI 10.1007/978-3-540-88688-4_37; Scaramuzza D, 2009, IEEE INT CONF ROBOT, P488; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; Ventura J, 2016, LECT NOTES COMPUT SC, V9907, P53, DOI 10.1007/978-3-319-46487-9_4; Yang JL, 2013, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2013.184; Yang JL, 2014, LECT NOTES COMPUT SC, V8689, P111, DOI 10.1007/978-3-319-10590-1_8; Zhang Q, 2014, PROC CVPR IEEE, P2830, DOI 10.1109/CVPR.2014.362; Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718	40	0	0	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5460	5471		10.1109/TPAMI.2021.3085134	http://dx.doi.org/10.1109/TPAMI.2021.3085134			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	34057889				2022-12-18	WOS:000836666600068
J	Kocanaogullari, A; Akcakaya, M; Erdogmus, D				Kocanaogullari, Aziz; Akcakaya, Murat; Erdogmus, D.			Stopping Criterion Design for Recursive Bayesian Classification: Analysis and Decision Geometry	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Uncertainty; Entropy; Bayes methods; Geometry; Radar tracking; Probability distribution; Brain modeling; Active learning; sequential decision making; recursive Bayesian classification; optimal stopping criterion design	STATISTICAL-ANALYSIS	Systems that are based on recursive Bayesian updates for classification limit the cost of evidence collection through certain stopping/termination criteria and accordingly enforce decision making. Conventionally, two termination criteria based on pre-defined thresholds over (i) the maximum of the state posterior distribution; and (ii) the state posterior uncertainty are commonly used. In this paper, we propose a geometric interpretation over the state posterior progression and accordingly we provide a point-by-point analysis over the disadvantages of using such conventional termination criteria. For example, through the proposed geometric interpretation we show that confidence thresholds defined over maximum of the state posteriors suffer from stiffness that results in unnecessary evidence collection whereas uncertainty based thresholding methods are fragile to number of categories and terminate prematurely if some state candidates are already discovered to be unfavorable. Moreover, both types of termination methods neglect the evolution of posterior updates. We then propose a new stopping/termination criterion with a geometrical insight to overcome the limitations of these conventional methods and provide a comparison in terms of decision accuracy and speed. We validate our claims using simulations and using real experimental data obtained through a brain computer interfaced typing system.	[Kocanaogullari, Aziz; Erdogmus, D.] Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA; [Akcakaya, Murat] Pittsburg Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15261 USA	Northeastern University; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Kocanaogullari, A (corresponding author), Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA.	akocanaogullari@ece.neu.edu; akcakaya@pitt.edu; erdogmus@ece.neu.edu			NSF [IIS-1149570, CNS1544895, IIS-1715858, IIS-1717654, IIS-1844885, IIS-1915083]; DHHS [90RE5017-02-01]; NIH [R01DC009834]	NSF(National Science Foundation (NSF)); DHHS; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This work was supported by the NSF (IIS-1149570, CNS1544895, IIS-1715858, IIS-1717654, IIS-1844885, IIS-1915083), DHHS (90RE5017-02-01), and NIH (R01DC009834).	AITCHISON J, 1982, J ROY STAT SOC B MET, V44, P139; Akcakaya Murat, 2014, IEEE Rev Biomed Eng, V7, P31, DOI 10.1109/RBME.2013.2295097; Alaa A. M., 2016, P 30 INT C NEUR INF, V29, P2910; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Barcel~o-Vidal C., 2001, PROC ANN C INT ASS M, V1, P1; Bulatov K, 2019, INT J DOC ANAL RECOG, V22, P303, DOI 10.1007/s10032-019-00333-0; Charlish Alexander, 2017, Novel Radar Techniques and Applications. Volume 2: Waveform Diversity and Cognitive Radar, and Target Tracking and Data Fusion, P157, DOI 10.1049/SBRA512G_ch5; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Duda R.O., 2000, PATTERN CLASSIFICATI; FRIEDMAN JH, 1989, J AM STAT ASSOC, V84, P165, DOI 10.2307/2289860; Geisser S., 1993, PREDICTIVE INFERENCE; Golovin D, 2010, ARON CULOTTA ADV NEU, P766; Higger M, 2017, IEEE T NEUR SYS REH, V25, P704, DOI 10.1109/TNSRE.2016.2590959; Ho SW, 2010, IEEE T INFORM THEORY, V56, P5930, DOI 10.1109/TIT.2010.2080891; Irci A, 2010, IEEE T AERO ELEC SYS, V46, P1848, DOI 10.1109/TAES.2010.5595599; Jha SK, 2009, LECT N BIOINFORMAT, V5688, P218, DOI 10.1007/978-3-642-03845-7_15; Kittler J, 2019, IEEE T CYBERNETICS, V49, P2331, DOI 10.1109/TCYB.2018.2825353; Lai TL, 1997, STAT SINICA, V7, P33; Lai TL, 2001, STAT SINICA, V11, P303; LUGOSI G, 1995, IEEE T INFORM THEORY, V41, P677, DOI 10.1109/18.382014; Marghi YM, 2019, INT CONF ACOUST SPEE, P3362, DOI 10.1109/ICASSP.2019.8683726; Memmott T, 2021, BRAIN-COMPUT INTERFA, V8, P137, DOI 10.1080/2326263X.2021.1878727; Moghadamfalahi M, 2015, IEEE T NEUR SYS REH, V23, P910, DOI 10.1109/TNSRE.2015.2411574; Orhan U, 2016, BRAIN-COMPUT INTERFA, V3, P171, DOI 10.1080/2326263X.2016.1252621; Orhan U, 2012, INT CONF ACOUST SPEE, P645, DOI 10.1109/ICASSP.2012.6287966; Pavlichin DS, 2016, IEEE INT SYMP INFO, P580, DOI 10.1109/ISIT.2016.7541365; Pawlowsky-Glahn V, 2001, STOCH ENV RES RISK A, V15, P384, DOI 10.1007/s004770100077; Renyi A., 1961, P 4 BERK S MATH STAT, P547; Sarkka S., 2013, BAYESIAN FILTERING S; Sason I., 2018, IEEE T INFORM THEORY, V64, P4; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x; Shiryaev A.N., 2007, OPTIMAL STOPPING RUL, V8; Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243; Tsiligkaridis T, 2014, IEEE T INFORM THEORY, V60, P2233, DOI 10.1109/TIT.2014.2304455; Van Trees H. L, 2001, DETECTION ESTIMATI 1; VAPNIK V, 1992, ADV NEUR IN, V4, P831; Weinshall D., 2009, ADV NEURAL INFORM PR, V21, P1745; Wilson A., 2012, ADV NEURAL INFORM PR, V25, P1133; Woodman GF, 2010, ATTEN PERCEPT PSYCHO, V72, P2031, DOI 10.3758/APP.72.8.2031; Xiang YJ, 2019, SIGNAL PROCESS, V155, P157, DOI 10.1016/j.sigpro.2018.09.035	41	0	0	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5590	5601		10.1109/TPAMI.2021.3075915	http://dx.doi.org/10.1109/TPAMI.2021.3075915			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33909559	Green Submitted			2022-12-18	WOS:000836666600077
J	Liu, L; Liu, J; Tao, DC				Liu, Liu; Liu, Ji; Tao, Dacheng			Variance Reduced Methods for Non-Convex Composition Optimization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Complexity theory; Optimization; Radio frequency; Estimation; Convergence; Approximation algorithms; Acceleration; Variance reduction; composition problem; biased estimation		This paper explores the non-convex composition optimization consisting of inner and outer finite-sum functions with a large number of component functions. This problem arises in important applications such as nonlinear embedding and reinforcement learning. Although existing approaches such as stochastic gradient descent (SGD) and stochastic variance reduced gradient (SVRG) descent can be applied to solve this problem, their query complexities tend to be high, especially when the number of inner component functions is large. Therefore, to significantly improve the query complexity of current approaches, we have devised the stochastic composition via variance reduction (SCVR). What's more, we analyze the query complexity under different numbers of inner function and outer function. Based on different kinds of estimation of inner component function, we also present the SCVRII algorithm, though the order of query complexities are the same with SCVR. Additionally, we propose an extension to handle the mini-batch cases, which improve the query complexity under the optimal mini-batch size. The experimental results validate our proposed algorithms and theoretical analyses.	[Liu, Liu; Tao, Dacheng] Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia; [Liu, Ji] Kuaishou Technol, Seattle, WA 98004 USA	University of Sydney	Tao, DC (corresponding author), Univ Sydney, Fac Engn, Sch Comp Sci, Darlington, NSW 2008, Australia.	liu.liu1@sydney.edu.au; ji.liu.uwisc@gmail.com; dacheng.tao@sydney.edu.au			Australian Research Council [FL-170100117, DP-180103424, IH180100002, IC-190100031]	Australian Research Council(Australian Research Council)	This work was supported by the Australian Research Council Projects under Grants FL-170100117, DP-180103424, IH180100002, and IC-190100031.	Allen-Zhu Z., 2018, INT C MACH LEARN; Allen-Zhu Z, 2017, PR MACH LEARN RES, V70; Allen-Zhu ZY, 2016, PR MACH LEARN RES, V48; [Anonymous], 2013, INTRO LECT CONVEX OP; Bo Dai, 2016, Arxiv, DOI arXiv:1607.04579; Carmon Y, 2018, SIAM J OPTIMIZ, V28, P1751, DOI 10.1137/17M1114296; Cheng J, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2441634; Chenyou Fan, 2019, Arxiv, DOI arXiv:1802.02339; Defazio A, 2014, ADV NEUR IN, V27; Dentcheva D, 2017, ANN I STAT MATH, V69, P737, DOI 10.1007/s10463-016-0559-8; Hinton Geoffrey, 2002, ADV NEURAL INFORM PR, V15, P833, DOI DOI 10.1109/TSMCB.2011.2106208; Huo ZY, 2018, AAAI CONF ARTIF INTE, P3287; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Lei L., 2017, ADV NEUR IN; Li H., 2015, PROC 28 INT C NEURAL, P379; Li HL, 2018, IEEE T NEUR NET LEAR, V29, P932, DOI 10.1109/TNNLS.2017.2650943; Li XL, 2018, IEEE T NEUR NET LEAR, V29, P1454, DOI 10.1109/TNNLS.2017.2672978; Lian XR, 2017, PR MACH LEARN RES, V54, P1159; Liu L, 2019, IEEE T NEUR NET LEAR, V30, P1205, DOI 10.1109/TNNLS.2018.2866699; Lunga D, 2013, IEEE T GEOSCI REMOTE, V51, P857, DOI 10.1109/TGRS.2012.2205004; Najafi A, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487981; Reddi S. J., 2016, ADV NEURAL INFORM PR, P1145; Reddi SJ, 2016, PR MACH LEARN RES, V48; Shalev-Shwartz S, 2016, PR MACH LEARN RES, V48; Shalev-Shwartz S, 2014, PR MACH LEARN RES, V32; Shen FM, 2015, IEEE T IMAGE PROCESS, V24, P1839, DOI 10.1109/TIP.2015.2405340; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Tao Q, 2018, IEEE T NEUR NET LEAR, V29, P2782, DOI 10.1109/TNNLS.2017.2705429; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Verbeek J, 2006, IEEE T PATTERN ANAL, V28, P1236, DOI 10.1109/TPAMI.2006.166; Wang M., 2016, P ADV NEUR INF PROC, P1714; Wang MD, 2016, WINT SIMUL C PROC, P702, DOI 10.1109/WSC.2016.7822134; Wang MD, 2017, MATH PROGRAM, V161, P419, DOI 10.1007/s10107-016-1017-3; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783; Yao C, 2017, IEEE T IMAGE PROCESS, V26, P5257, DOI 10.1109/TIP.2017.2733200; Zachevsky I, 2016, IEEE T IMAGE PROCESS, V25, P2130, DOI 10.1109/TIP.2016.2539689; Zeyuan Allen-Zhu, 2018, Arxiv, DOI arXiv:1708.08694; Zhang WZ, 2017, IEEE T PATTERN ANAL, V39, P1223, DOI 10.1109/TPAMI.2016.2578323	42	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5813	5825		10.1109/TPAMI.2021.3071594	http://dx.doi.org/10.1109/TPAMI.2021.3071594			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33826512	Green Submitted			2022-12-18	WOS:000836666600091
J	Liu, PP; Lyu, MR; King, I; Xu, J				Liu, Pengpeng; Lyu, Michael R.; King, Irwin; Xu, Jia			Learning by Distillation: A Self-Supervised Learning Framework for Optical Flow Estimation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Optical imaging; Predictive models; Optical variables control; Estimation; Optical computing; Training; Data models; Optical flow; knowledge distillation; unsupervised learning; self-supervised learning; and stereo matching		We present DistillFlow, a knowledge distillation approach to learning optical flow. DistillFlow trains multiple teacher models and a student model, where challenging transformations are applied to the input of the student model to generate hallucinated occlusions as well as less confident predictions. Then, a self-supervised learning framework is constructed: confident predictions from teacher models are served as annotations to guide the student model to learn optical flow for those less confident predictions. The self-supervised learning framework enables us to effectively learn optical flow from unlabeled data, not only for non-occluded pixels, but also for occluded pixels. DistillFlow achieves state-of-the-art unsupervised learning performance on both KITTI and Sintel datasets. Our self-supervised pre-trained model also provides an excellent initialization for supervised fine-tuning, suggesting an alternate training paradigm in contrast to current supervised learning methods that highly rely on pre-training on synthetic data. At the time of writing, our fine-tuned models ranked 1st among all monocular methods on the KITTI 2015 benchmark, and outperform all published methods on the Sintel Final benchmark. More importantly, we demonstrate the generalization capability of DistillFlow in three aspects: framework generalization, correspondence generalization and cross-dataset generalization. Our code and models will be available on https://github.com/ppliuboy/DistillFlow.	[Liu, Pengpeng; Lyu, Michael R.; King, Irwin] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China; [Xu, Jia] Huya AI, Guangzhou 511442, Guangdong, Peoples R China	Chinese University of Hong Kong	Liu, PP (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.	ppliu@cse.cuhk.edu.hk; lyu@cse.cuhk.edu.hk; king@cse.cuhk.edu.hk; xujia@huya.com			National Key Research and Development Program of China [2018A AA0100204]; Research Grants Council of the Hong Kong Special Administrative Region, China [CUHK 14210717]	National Key Research and Development Program of China; Research Grants Council of the Hong Kong Special Administrative Region, China(Hong Kong Research Grants Council)	This work was supported in part by the National Key Research and Development Program of China (No. 2018A AA0100204) and Research Grants Council of the Hong Kong Special Administrative Region, China (CUHK 14210717 of the General Research Fund). Pengpeng Liu did mainly the work during an internship at Huya AI.	Bailer C, 2017, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2017.290; Bar-Haim Aviram, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7995, DOI 10.1109/CVPR42600.2020.00802; Black M. J., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P296, DOI 10.1109/CVPR.1991.139705; Bonneel N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818107; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Chauhan A.K., 2013, INT J ADV RES COMPUT, V3, P243; Chen DD, 2017, IEEE I CONF COMP VIS, P1114, DOI 10.1109/ICCV.2017.126; Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Guo XY, 2018, LECT NOTES COMPUT SC, V11215, P506, DOI 10.1007/978-3-030-01252-6_30; Hafner D, 2013, LNCS, P210, DOI DOI 10.1007/978-3-642-38267-3; Hinton G., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/TPAMI.2012.59; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Hui TW, 2021, IEEE T PATTERN ANAL, V43, P2555, DOI 10.1109/TPAMI.2020.2976928; Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936; Hur J, 2019, PROC CVPR IEEE, P5747, DOI 10.1109/CVPR.2019.00590; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Jaderberg M, 2015, ADV NEUR IN, V28; Janai J., 2018, P EUR C COMP VIS, P690; Janai J, 2017, PROC CVPR IEEE, P1406, DOI 10.1109/CVPR.2017.154; Jiang HZ, 2019, IEEE I CONF COMP VIS, P3194, DOI 10.1109/ICCV.2019.00329; Jing LL, 2021, IEEE T PATTERN ANAL, V43, P4037, DOI 10.1109/TPAMI.2020.2992393; Joung S, 2020, IEEE T INTELL TRANSP, V21, P2190, DOI 10.1109/TITS.2019.2917538; Kennedy R, 2015, LECT NOTES COMPUT SC, V8932, P364, DOI 10.1007/978-3-319-14612-6_27; Lai HY, 2019, PROC CVPR IEEE, P1890, DOI 10.1109/CVPR.2019.00199; Larsson G, 2017, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2017.96; Li A., 2018, PROC ASIAN C COMPUT, P197; Liu L, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P876; Liu PP, 2020, PROC CVPR IEEE, P6647, DOI 10.1109/CVPR42600.2020.00668; Liu PP, 2019, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2019.00470; Liu PP, 2019, AAAI CONF ARTIF INTE, P8770; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Meister S, 2018, AAAI CONF ARTIF INTE, P7251; Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925; Neoral M, 2019, LECT NOTES COMPUT SC, V11364, P159, DOI 10.1007/978-3-030-20870-7_10; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Pathak D, 2017, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2017.638; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Radosavovic I, 2018, PROC CVPR IEEE, P4119, DOI 10.1109/CVPR.2018.00433; Ranjan A., 2019, CVPR, P12240, DOI DOI 10.1109/CVPR.2019.01252; Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291; Ren Z, 2017, AAAI CONF ARTIF INTE, P1495; Ren ZL, 2019, IEEE WINT CONF APPL, P2077, DOI 10.1109/WACV.2019.00225; Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720; Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693; Simonyan K, 2014, ADV NEUR IN, V27; Sun D., 2010, ADV NEURAL INFORM PR, V23, P2226; Sun DQ, 2020, IEEE T PATTERN ANAL, V42, P1408, DOI 10.1109/TPAMI.2019.2894353; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939; Sundaram N, 2010, LECT NOTES COMPUT SC, V6311, P438, DOI 10.1007/978-3-642-15549-9_32; Tonioni A, 2019, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2019.00028; Tonioni A, 2017, IEEE I CONF COMP VIS, P1614, DOI 10.1109/ICCV.2017.178; Volz S, 2011, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2011.6126359; Wang Y, 2019, PROC CVPR IEEE, P8063, DOI 10.1109/CVPR.2019.00826; Wang Y, 2018, PROC CVPR IEEE, P4884, DOI 10.1109/CVPR.2018.00513; Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175; Xu J., 2017, P C COMP VIS PATT RE, P1289; Yang GQ, 2019, ICVISP 2019: PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON VISION, IMAGE AND SIGNAL PROCESSING, DOI 10.1145/3387168.3387244; Yang GR, 2018, LECT NOTES COMPUT SC, V11211, P660, DOI 10.1007/978-3-030-01234-2_39; Yin ZC, 2019, PROC CVPR IEEE, P6037, DOI 10.1109/CVPR.2019.00620; Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212; Yu JJ, 2016, LECT NOTES COMPUT SC, V9915, P3, DOI 10.1007/978-3-319-49409-8_1; Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151, DOI 10.1007/BFb0028345; Zhao SY, 2020, PROC CVPR IEEE, P6277, DOI 10.1109/CVPR42600.2020.00631; Zhong YR, 2019, PROC CVPR IEEE, P12087, DOI 10.1109/CVPR.2019.01237; Zhong YR, 2018, LECT NOTES COMPUT SC, V11206, P104, DOI 10.1007/978-3-030-01216-8_7; Zhou C, 2017, IEEE I CONF COMP VIS, P1576, DOI 10.1109/ICCV.2017.174; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700; Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI 10.1007/978-3-030-01219-9_	76	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5026	5041		10.1109/TPAMI.2021.3085525	http://dx.doi.org/10.1109/TPAMI.2021.3085525			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	34061735	Green Submitted			2022-12-18	WOS:000836666600041
J	Liu, S; Wang, ZT; Gao, YL; Ren, LJ; Liao, Y; Ren, GH; Li, B; Yan, SC				Liu, Si; Wang, Zitian; Gao, Yulu; Ren, Lejian; Liao, Yue; Ren, Guanghui; Li, Bo; Yan, Shuicheng			Human-Centric Relation Segmentation: Dataset and Solution	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Human-centric relation segmentation; matching; human object interaction; visual relation detection		Vision and language understanding techniques have achieved remarkable progress, but currently it is still difficult to well handle problems involving very fine-grained details. For example, when the robot is told to "bring me the book in the girl's left hand", most existing methods would fail if the girl holds one book respectively in her left and right hand. In this work, we introduce a new task named human-centric relation segmentation (HRS), as a fine-grained case of HOI-det. HRS aims to predict the relations between the human and surrounding entities and identify the relation-correlated human parts, which are represented as pixel-level masks. For the above exemplar case, our HRS task produces results in the form of relation triplets < girl [left hand], hold, book > and exacts segmentation masks of the book, with which the robot can easily accomplish the grabbing task. Correspondingly, we collect a new Person In Context (PIC) dataset for this new task, which contains 17,122 high-resolution images and densely annotated entity segmentation and relations, including 141 object categories, 23 relation categories and 25 semantic human parts. We also propose a Simultaneous Matching and Segmentation (SMS) framework as a solution to the HRS task. It contains three parallel branches for entity segmentation, subject object matching and human parsing respectively. Specifically, the entity segmentation branch obtains entity masks by dynamically-generated conditional convolutions; the subject object matching branch detects the existence of any relations, links the corresponding subjects and objects by displacement estimation and classifies the interacted human parts; and the human parsing branch generates the pixelwise human part labels. Outputs of the three branches are fused to produce the final HRS results. Extensive experiments on PIC and V-COCO datasets show that the proposed SMS method outperforms baselines with the 36 FPS inference speed. Notably, SMS outperforms the best performing baseline m-KERN with only 17.6 percent time cost. The dataset and code will be released at http://picdataset.com/challenge/index/.	[Liu, Si; Wang, Zitian; Gao, Yulu; Liao, Yue; Li, Bo] Beihang Univ, Beijing 100083, Peoples R China; [Ren, Lejian; Ren, Guanghui] Chinese Acad Sci, Beijing 100864, Peoples R China; [Yan, Shuicheng] Sea AI Lab SAIL, Singapore 117576, Singapore	Beihang University; Chinese Academy of Sciences	Liu, S (corresponding author), Beihang Univ, Beijing 100083, Peoples R China.	liusi@buaa.edu.cn; a2394797795@gmail.com; gyl97@buaa.edu.cn; renlejian@iie.ac.cn; liaoyue.ai@gmail.com; renguanghui@iie.ac.cn; boli@buaa.edu.cn; shuicheng.yan@gmail.com	Yan, Shuicheng/HCI-1431-2022	, Zitian/0000-0001-8138-1869	National Natural Science Foundation of China [61876177]; Beijing Natural Science Foundation [4202034]; Fundamental Research Funds for the Central Universities, Zhejiang Lab [2019KD0AB04]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation); Fundamental Research Funds for the Central Universities, Zhejiang Lab	This work was supported in part by the National Natural Science Foundation of China under Grant 61876177, in part by the Beijing Natural Science Foundation under Grant 4202034, and in part by the Fundamental Research Funds for the Central Universities, Zhejiang Lab under Grant 2019KD0AB04.	Aaron Gokaslan, 2020, Arxiv, DOI arXiv:1912.06321; Ahmed El Kholy, 2020, Arxiv, DOI arXiv:1909.11740; Alexander C. Berg, 2019, Arxiv, DOI arXiv:1901.03353; Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636; Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Berman M, 2018, PROC CVPR IEEE, P4413, DOI 10.1109/CVPR.2018.00464; Bin Li, 2020, Arxiv, DOI arXiv:1908.08530; Bowen Li, 2020, Arxiv, DOI arXiv:1912.06203; Chao YW, 2018, IEEE WINT CONF APPL, P381, DOI 10.1109/WACV.2018.00048; Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511; Chen TS, 2019, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2019.00632; Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254; Chunhua Shen, 2020, Arxiv, DOI arXiv:1912.04488; Chunhua Shen, 2020, Arxiv, DOI arXiv:2003.10152; Congyan Lang, 2018, Arxiv, DOI arXiv:1705.07206; Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343; Das A, 2018, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2018.00008; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221; Feng W, 2019, AAAI CONF ARTIF INTE, P898; Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Jitendra Malik, 2015, Arxiv, DOI arXiv:1505.04474; Kingma D.P, P 3 INT C LEARNING R; Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1; Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472; Li YK, 2017, PROC CVPR IEEE, P7244, DOI 10.1109/CVPR.2017.766; Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063; Liang XD, 2015, IEEE I CONF COMP VIS, P1386, DOI 10.1109/ICCV.2015.163; Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; Lu JS, 2019, ADV NEUR IN, V32; Max Welling, 2017, Arxiv, DOI arXiv:1609.02907; Misra I, 2016, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2016.320; Neven D, 2019, PROC CVPR IEEE, P8829, DOI 10.1109/CVPR.2019.00904; Newell A, 2017, ADV NEUR IN, V30; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031; Tang K., 2020, ARXIV, DOI [10.48550/arXiv.2002.11949, DOI 10.48550/ARXIV.2002.11949]; Wan B, 2019, IEEE I CONF COMP VIS, P9468, DOI 10.1109/ICCV.2019.00956; Wang TC, 2020, PROC CVPR IEEE, P4115, DOI 10.1109/CVPR42600.2020.00417; Wang TC, 2019, IEEE I CONF COMP VIS, P5693, DOI 10.1109/ICCV.2019.00579; Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330; Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41; Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255; Yuankai Qi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9979, DOI 10.1109/CVPR42600.2020.01000; Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611; Zhang HW, 2017, PROC CVPR IEEE, P3107, DOI 10.1109/CVPR.2017.331; Zhao J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P792, DOI 10.1145/3240508.3240509; Zhou TF, 2020, PROC CVPR IEEE, P4262, DOI 10.1109/CVPR42600.2020.00432; Zhou X., 2019, ARXIV; Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953	61	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4987	5001		10.1109/TPAMI.2021.3075846	http://dx.doi.org/10.1109/TPAMI.2021.3075846			15	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33905323	Green Submitted			2022-12-18	WOS:000836666600038
J	Liu, W; Piao, ZX; Tu, Z; Luo, WH; Ma, L; Gao, SH				Liu, Wen; Piao, Zhixin; Tu, Zhi; Luo, Wenhan; Ma, Lin; Gao, Shenghua			Liquid Warping GAN With Attention: A Unified Framework for Human Image Synthesis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Human image synthesis; motion imitation; appearance transfer; novel view synthesis; generative adversarial network		We tackle human image synthesis, including human motion imitation, appearance transfer, and novel view synthesis, within a unified framework. It means that the model, once being trained, can be used to handle all these tasks. The existing task-specific methods mainly use 2D keypoints (pose) to estimate the human body structure. However, they only express the position information with no ability to characterize the personalized shape of the person and model the limb rotations. In this paper, we propose to use a 3D body mesh recovery module to disentangle the pose and shape. It can not only model the joint location and rotation but also characterize the personalized body shape. To preserve the source information, such as texture, style, color, and face identity, we propose an Attentional Liquid Warping GAN with Attentional Liquid Warping Block (AttLWB) that propagates the source information in both image and feature spaces to the synthesized reference. Specifically, the source features are extracted by a denoising convolutional auto-encoder for characterizing the source identity well. Furthermore, our proposed method can support a more flexible warping from multiple sources. To further improve the generalization ability of the unseen source images, a one/few-shot adversarial learning is applied. In detail, it first trains a model in an extensive training set. Then, it finetunes the model by one/few-shot unseen image(s) in a self-supervised way to generate high-resolution (512 x 512 and 1024 x 1024) results. Also, we build a new dataset, namely Impersonator (iPER) dataset, for the evaluation of human motion imitation, appearance transfer, and novel view synthesis. Extensive experiments demonstrate the effectiveness of our methods in terms of preserving face identity, shape consistency, and clothes details. All codes and dataset are available on https://impersonator.org/work/impersonator-plus-plus.html.	[Liu, Wen] Shanghai Tech Univ, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China; [Liu, Wen] Chinese Acad Sci, Shanghai Inst Microsyst & Informat Technol, Beijing 100864, Peoples R China; [Liu, Wen] Univ Chinese Acad Sci, Beijing 100049, Peoples R China; [Piao, Zhixin; Tu, Zhi] ShatighaiTech Univ, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China; [Luo, Wenhan] Tencent, Shenzhen 518000, Peoples R China; [Ma, Lin] Meituan, Beijing 100102, Peoples R China; [Gao, Shenghua] ShanghaiTech Univ, Engn Res Ctr Intelligent Vis & Imaging, Shanghai 201210, Peoples R China	ShanghaiTech University; Chinese Academy of Sciences; Shanghai Institute of Microsystem & Information Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Tencent; ShanghaiTech University	Gao, SH (corresponding author), ShanghaiTech Univ, Engn Res Ctr Intelligent Vis & Imaging, Shanghai 201210, Peoples R China.	liuweng@shanghaitech.edu.cn; piaozhx@shanghaitech.edu; tuzhi@shanghaitech.edu.cn; china@gmail.com; linma@gmail.com; gaoshh@shanghaitech.edu.cn	liu, wen/HGE-3071-2022; Luo, Wenhan/GZL-0535-2022		National Key R&D Program of China [2018AAA0100704]; NSFC [61932020]; Science and Technology Commission of Shanghai Municipality [20ZR1436000]; Shanghai Education Development Foundation; Shanghai Municipal Education Commission through Shuguang Program	National Key R&D Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Science and Technology Commission of Shanghai Municipality(Science & Technology Commission of Shanghai Municipality (STCSM)); Shanghai Education Development Foundation; Shanghai Municipal Education Commission through Shuguang Program	This work was supported by the National Key R&D Program of China under Grant 2018AAA0100704, NSFC under Grant 61932020, the Science and Technology Commission of Shanghai Municipality under Grant 20ZR1436000, and the Shanghai Education Development Foundation and Shanghai Municipal Education Commission through Shuguang Program. The authors would like to thank Dr. Weixin Luo for the meaningful discussion in the whole procedure, and appreciate all the help in building the first version of the iPER dataset from Jie Min.	Adobe, 2015, MIXAMO; AlBahar B, 2019, IEEE I CONF COMP VIS, P9015, DOI 10.1109/ICCV.2019.00911; Alex Nichol, 2018, Arxiv, DOI arXiv:1803.02999; Alldieck T, 2018, PROC CVPR IEEE, P8387, DOI 10.1109/CVPR.2018.00875; Ba J., 2017, P 3 INT C LEARN REPR; Balakrishnan G, 2018, PROC CVPR IEEE, P8340, DOI 10.1109/CVPR.2018.00870; Bhatnagar BL, 2019, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2019.00552; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603; Ding ZM, 2018, IEEE INT CONF AUTOMA, P1, DOI 10.1109/FG.2018.00011; Dong H., 2018, ADV NEURAL INFORM PR, P474; Esler T., 2019, FACENET PYTORCH; Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923; Finn C, 2017, PR MACH LEARN RES, V70; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grigorev A, 2019, PROC CVPR IEEE, P12127, DOI 10.1109/CVPR.2019.01241; Guler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hensel M, 2017, ADV NEUR IN, V30; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jaderberg M, 2015, ADV NEUR IN, V28; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411; Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234; Lee JH, 2020, PLANT BIOTECHNOL REP, V14, P363, DOI 10.1007/s11816-020-00610-z; Leroy V, 2017, IEEE I CONF COMP VIS, P3113, DOI 10.1109/ICCV.2017.336; Li YN, 2019, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2019.00381; Liang D, 2020, 2020 OPTICAL FIBER COMMUNICATIONS CONFERENCE AND EXPOSITION (OFC); Liu LJ, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3333002; Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713; Liu W, 2019, IEEE I CONF COMP VIS, P5903, DOI 10.1109/ICCV.2019.00600; Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018; Ma Liqian, 2017, P NEUR INF PROC SYST, P405; Mao XD, 2019, IEEE T PATTERN ANAL, V41, P2947, DOI 10.1109/TPAMI.2018.2872043; Neverova N., 2018, P EUR C COMP VIS, P123; Park E, 2017, PROC CVPR IEEE, P702, DOI 10.1109/CVPR.2017.82; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Pons-Moll G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073711; Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50; Raj A, 2018, LECT NOTES COMPUT SC, V11216, P679, DOI 10.1007/978-3-030-01258-8_41; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ronneberger O., 2015, P INT C MED IM COMP; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239; Sarkar Kripasindhu, 2020, ECCV; Shaham TR, 2019, IEEE I CONF COMP VIS, P4569, DOI 10.1109/ICCV.2019.00467; Shysheya A, 2019, PROC CVPR IEEE, P2382, DOI 10.1109/CVPR.2019.00249; Si CY, 2018, PROC CVPR IEEE, P118, DOI 10.1109/CVPR.2018.00020; Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359; Simonyan Karen, 2015, VERY DEEP CONVOLUTIO; Sun SH, 2018, LECT NOTES COMPUT SC, V11207, P162, DOI 10.1007/978-3-030-01219-9_10; Vaswani A, 2017, ADV NEUR IN, V30; Wang TC, 2019, ADV NEUR IN, V32; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang TC, 2018, ADV NEUR IN, V31; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Zablotskaia P., 2019, PROC 30 BRIT MACH VI, P1; Zakharov E, 2019, IEEE I CONF COMP VIS, P9458, DOI 10.1109/ICCV.2019.00955; Zanfir M, 2018, PROC CVPR IEEE, P5391, DOI 10.1109/CVPR.2018.00565; Zhang C, 2017, PROC CVPR IEEE, P5484, DOI 10.1109/CVPR.2017.582; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360; Zhao B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P383, DOI 10.1145/3240508.3240536; Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380; Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18; Zhu H, 2018, PROC CVPR IEEE, P4450, DOI 10.1109/CVPR.2018.00468; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245	72	0	0	10	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5114	5132		10.1109/TPAMI.2021.3078270	http://dx.doi.org/10.1109/TPAMI.2021.3078270			19	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33961551	Green Submitted			2022-12-18	WOS:000836666600047
J	Liu, ZY; Wang, L; Zhang, QL; Tang, W; Zheng, NN; Hua, G				Liu, Ziyi; Wang, Le; Zhang, Qilin; Tang, Wei; Zheng, Nanning; Hua, Gang			Weakly Supervised Temporal Action Localization Through Contrast Based Evaluation Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Location awareness; Proposals; Videos; Task analysis; Feature extraction; Training; Three-dimensional displays; Action localization; weakly supervised learning; temporal contrast		Given only video-level action categorical labels during training, weakly-supervised temporal action localization (WS-TAL) learns to detect action instances and locates their temporal boundaries in untrimmed videos. Compared to its fully supervised counterpart, WS-TAL is more cost-effective in data labeling and thus favorable in practical applications. However, the coarse video-level supervision inevitably incurs ambiguities in action localization, especially in untrimmed videos containing multiple action instances. To overcome this challenge, we observe that significant temporal contrasts among video snippets, e.g., caused by temporal discontinuities and sudden changes, often occur around true action boundaries. This motivates us to introduce a Contrast-based Localization EvaluAtioN Network (CleanNet), whose core is a new temporal action proposal evaluator, which provides fine-grained pseudo supervision by leveraging the temporal contrasts among snippet-level classification predictions. As a result, the uncertainty in locating action instances can be resolved via evaluating their temporal contrast scores. Moreover, the new action localization module is an integral part of CleanNet which enables end-to-end training. This is in contrast to many existing WS-TAL methods where action localization is merely a post-processing step. Besides, we also explore the usage of temporal contrast on temporal action proposal (TAP) generation task, which we believe is the first attempt with the weak supervision setting. Experiments on the THUMOS14, ActivityNet v1.2 and v1.3 datasets validate the efficacy of our method against existing state-of-the-art WS-TAL algorithms.	[Liu, Ziyi; Wang, Le; Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China; [Zhang, Qilin] ABB Corp Res Ctr, Raleigh, NC 27606 USA; [Tang, Wei] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA; [Hua, Gang] Wormpex AI Res, Bellevue, WA 98004 USA	Xi'an Jiaotong University; ABB; University of Illinois System; University of Illinois Chicago; University of Illinois Chicago Hospital	Wang, L (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.	liuziyi@stu.xjtu.edu.cn; lewang@mail.xjtu.edu.cn; samqzhang@gmail.com; tangw@uic.edu; nnzheng@mail.xjtu.edu.cn; ganghua@gmail.com	Liu, ziyi/HHM-8313-2022		National Key R&D Program of China [2018AAA0101400]; NSFC [62088102, 61773312, 61976171]; Young Elite Scientists Sponsorship Program through CAST [2018QNRC001]; Natural Science Foundation of Shaanxi [2020JQ-069]	National Key R&D Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Young Elite Scientists Sponsorship Program through CAST; Natural Science Foundation of Shaanxi(Natural Science Foundation of Shaanxi Province)	This work was supported in part by the National Key R&D Program of China under Grant 2018AAA0101400, in part by NSFC under Grants 62088102, 61773312, and 61976171, in part by Young Elite Scientists Sponsorship Program through CAST under Grant 2018QNRC001, and in part by the Natural Science Foundation of Shaanxi under Grant 2020JQ-069.	Asadi-Aghbolaghi M, 2017, IEEE INT CONF AUTOMA, P476, DOI 10.1109/FG.2017.150; Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675; Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124; Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610; De Geest R, 2016, LECT NOTES COMPUT SC, V9909, P269, DOI 10.1007/978-3-319-46454-1_17; Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Gao JY, 2018, LECT NOTES COMPUT SC, V11206, P70, DOI 10.1007/978-3-030-01216-8_5; Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392; Gao Jiyang, 2017, ARXIV170501180; Girshick R., 2014, PROC IEEE C COMPUT V; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Huang LJ, 2020, AAAI CONF ARTIF INTE, V34, P11053; Hussein N, 2019, PROC CVPR IEEE, P254, DOI 10.1109/CVPR.2019.00034; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jiang Y.-G., 2014, THUMOS CHALLENGE ACT; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Lee P, 2020, AAAI CONF ARTIF INTE, V34, P11320; Li J, 2020, AAAI CONF ARTIF INTE, V34, P4626; Lin CRN, 2020, AAAI CONF ARTIF INTE, V34, P11499; Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399; Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1; Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139; Liu ZY, 2019, IEEE I CONF COMP VIS, P3898, DOI 10.1109/ICCV.2019.00400; Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043; Min Kyle, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P283, DOI 10.1007/978-3-030-58568-6_17; Hoai M, 2012, PROC CVPR IEEE, P2863, DOI 10.1109/CVPR.2012.6248012; Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228; Oneata Dan, 2014, LEAR SUBMISSION THUM, P2; Paszke A., 2017, AUTOMATIC DIFFERENTI; Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35; Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706; Nguyen PX, 2019, IEEE I CONF COMP VIS, P5501, DOI 10.1109/ICCV.2019.00560; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Richard P. Wildes, 2016, Arxiv, DOI arXiv:1610.06906; Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806; Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10; Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155; Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119; Simonyan K, 2014, ADV NEUR IN, V27; Simonyan Karen, 2015, VERY DEEP CONVOLUTIO; Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381; Sun C, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P371, DOI 10.1145/2733373.2806226; Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang L., 2014, ACTION RECOGNITION D; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678; Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25; Wu CY, 2019, PROC CVPR IEEE, P284, DOI 10.1109/CVPR.2019.00037; Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617; Xu M., 2020, CVPR, P10156; Yu T, 2019, IEEE I CONF COMP VIS, P5521, DOI 10.1109/ICCV.2019.00562; Yuan J, 2016, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR.2016.337; Yuan Y., 2019, P INT C LEARN REP; Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719; Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317	64	0	0	6	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5886	5902		10.1109/TPAMI.2021.3078798	http://dx.doi.org/10.1109/TPAMI.2021.3078798			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33974541				2022-12-18	WOS:000836666600095
J	Maki, A; Kragic, D; Kjellstrom, H; Azizpour, H; Sullivan, J; Bjorkman, M; Jensfelt, P; Carlsson, S; Lindeberg, T; Sundblad, Y				Maki, Atsuto; Kragic, Danica; Kjellstrom, Hedvig; Azizpour, Hossein; Sullivan, Josephine; Bjorkman, Marten; Jensfelt, Patric; Carlsson, Stefan; Lindeberg, Tony; Sundblad, Yngve			In Memoriam: Jan-Olof Eklundh	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Biographical-Item									[Maki, Atsuto; Kragic, Danica; Kjellstrom, Hedvig; Azizpour, Hossein; Sullivan, Josephine; Bjorkman, Marten; Jensfelt, Patric; Carlsson, Stefan; Lindeberg, Tony; Sundblad, Yngve] KTH Royal Inst Technol, Stockholm, Sweden	Royal Institute of Technology	Maki, A (corresponding author), KTH Royal Inst Technol, Stockholm, Sweden.	atsuto@kth.se						Eklundh J.-O., 1982, TECH REP TRITA NA 82; Eklundh JO, 1996, INT J COMPUT VISION, V17, P107, DOI 10.1007/BF00058747	2	0	0	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4488	4489		10.1109/TPAMI.2022.3183266	http://dx.doi.org/10.1109/TPAMI.2022.3183266			2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN		Bronze			2022-12-18	WOS:000836666600005
J	Ngo, TT; Nagahara, H; Taniguchi, R				Ngo, Trung Thanh; Nagahara, Hajime; Taniguchi, Rin-ichiro			Surface Normals and Light Directions From Shading and Polarization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cameras; Light sources; Refractive index; Shape; Three-dimensional displays; Estimation; Faces; Surface normal estimation; light direction estimation; shading-stereoscopic constraint; polarization-stereoscopic constraint	PHOTOMETRIC STEREO; SHAPE; ORIENTATION	We introduce a method of recovering the shape of a smooth dielectric object using diffuse polarization images taken with different directional light sources. We present two constraints on shading and polarization and use both in a single optimization scheme. This integration is motivated by photometric stereo and polarization-based methods having complementary abilities. Polarization gives strong cues for the surface orientation and refractive index, which are independent of the light direction. However, employing polarization leads to ambiguities in selecting between two ambiguous choices of the surface orientation, in the relationship between the refractive index and zenith angle (observing angle). Moreover, polarization-based methods for surface points with small zenith angles perform poorly owing to the weak polarization. In contrast, the photometric stereo method with multiple light sources disambiguates the surface normals and gives a strong relationship between surface normals and light directions. However, the method has limited performance for large zenith angles and refractive index estimation and faces strong ambiguity when light directions are unknown. Taking the advantages of these methods, our proposed method recovers surface normals for small and large zenith angles, light directions, and refractive indexes of the object. The proposed method is positively evaluated in simulations and real-world experiments.	[Ngo, Trung Thanh; Nagahara, Hajime] Osaka Univ, Inst Databil Sci, Suita, Osaka 5650871, Japan; [Taniguchi, Rin-ichiro] Kyushu Univ, Fac Informat Sci & Elect Engn, Fukuoka 8190395, Japan	Osaka University; Kyushu University	Ngo, TT (corresponding author), Osaka Univ, Inst Databil Sci, Suita, Osaka 5650871, Japan.	trung@am.sanken.osaka-u.ac.jp; nagahara@ids.osaka-u.ac.jp; rin@kyudai.jp			JSPS KAKENHI [25240027, 17H06102]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	The authors would like to thank SONY Semiconductor Solutions Corporation for supplying the SONY polarization sensor, and Glenn Pennycook, MSc, from Edanz Group for editing a draft of this manuscript. This work was supported by JSPS KAKENHI under Grants 25240027 and 17H06102.	3Shape, 2021, 3SHAPE E1; Atkinson GA, 2005, IEEE I CONF COMP VIS, P309; Atkinson GA, 2007, IEEE T PATTERN ANAL, V29, P2001, DOI 10.1109/TPAMI.2007.1099; Atkinson GA, 2007, LECT NOTES COMPUT SC, V4673, P466; Atkinson GA, 2006, IEEE T IMAGE PROCESS, V15, P1653, DOI 10.1109/TIP.2006.871114; Atkinson GA, 2017, COMPUT VIS IMAGE UND, V160, P158, DOI 10.1016/j.cviu.2017.04.014; Basri R, 2007, INT J COMPUT VISION, V72, P239, DOI 10.1007/s11263-006-8815-7; Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611; Chandraker M, 2014, PROC CVPR IEEE, P2179, DOI 10.1109/CVPR.2014.279; Chandraker MK, 2005, PROC CVPR IEEE, P788; Chen GY, 2019, PROC CVPR IEEE, P8731, DOI 10.1109/CVPR.2019.00894; Chen LX, 2018, LECT NOTES COMPUT SC, V11220, P21, DOI 10.1007/978-3-030-01270-0_2; Cui ZP, 2017, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2017.47; Dizhong Zhu, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7578, DOI 10.1109/CVPR.2019.00777; Drbohlav O, 2002, LECT NOTES COMPUT SC, V2351, P46; Drbohlav O, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P581, DOI 10.1109/ICCV.2001.937570; Foix S, 2011, IEEE SENS J, V11, P1917, DOI 10.1109/JSEN.2010.2101060; Hartley R., 2003, MULTIPLE VIEW GEOMET; HAYAKAWA H, 1994, J OPT SOC AM A, V11, P3079, DOI 10.1364/JOSAA.11.003079; Hecht E., 2015, OPTICS; Higo T, 2010, PROC CVPR IEEE, P1157, DOI 10.1109/CVPR.2010.5540084; Huynh CP, 2013, INT J COMPUT VISION, V101, P64, DOI 10.1007/s11263-012-0546-3; Inoshita C, 2014, LECT NOTES COMPUT SC, V8690, P346, DOI 10.1007/978-3-319-10605-2_23; Kadambi A, 2015, IEEE I CONF COMP VIS, P3370, DOI 10.1109/ICCV.2015.385; Lu F, 2013, PROC CVPR IEEE, P1490, DOI 10.1109/CVPR.2013.196; Mahmoud AH, 2012, IEEE IMAGE PROC, P1769, DOI 10.1109/ICIP.2012.6467223; Maruyama Y, 2018, IEEE T ELECTRON DEV, V65, P2544, DOI 10.1109/TED.2018.2829190; Miyazaki D, 2002, J OPT SOC AM A, V19, P687, DOI 10.1364/JOSAA.19.000687; Miyazaki D, 2004, IEEE T PATTERN ANAL, V26, P73, DOI 10.1109/TPAMI.2004.1261080; Miyazaki D, 2010, IEEE IMAGE PROC, P4057, DOI 10.1109/ICIP.2010.5650067; Nayar SK, 1997, INT J COMPUT VISION, V21, P163, DOI 10.1023/A:1007937815113; Ngan A, 2005, EUR S REND, V2, P117, DOI DOI 10.2312/EGWR/EGSR05/117-126; Okabe T, 2009, IEEE I CONF COMP VIS, P1693, DOI 10.1109/ICCV.2009.5459381; Papadhimitri T., 2014, PROC BRIT MACH VIS C; Papadhimitri T, 2013, PROC CVPR IEEE, P1474, DOI 10.1109/CVPR.2013.194; Rahmann S, 1999, P SOC PHOTO-OPT INS, V3826, P22, DOI 10.1117/12.364333; Saito M., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P381, DOI 10.1109/CVPR.1999.786967; Saman G., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P1925, DOI 10.1109/ICIP.2011.6115847; Santo H, 2017, IEEE INT CONF COMP V, P501, DOI 10.1109/ICCVW.2017.66; Shi BX, 2010, PROC CVPR IEEE, P1118, DOI 10.1109/CVPR.2010.5540091; Smith WAP, 2019, IEEE T PATTERN ANAL, V41, P2875, DOI 10.1109/TPAMI.2018.2868065; Smith WAP, 2016, LECT NOTES COMPUT SC, V9912, P109, DOI 10.1007/978-3-319-46484-8_7; Sunkavalli K, 2010, LECT NOTES COMPUT SC, V6312, P251, DOI 10.1007/978-3-642-15552-9_19; Ngo TT, 2015, PROC CVPR IEEE, P2310, DOI 10.1109/CVPR.2015.7298844; WOLFF LB, 1991, IEEE T PATTERN ANAL, V13, P635, DOI 10.1109/34.85655; Wolff LB, 1998, INT J COMPUT VISION, V30, P55, DOI 10.1023/A:1008017513536; WOODHAM RJ, 1980, OPT ENG, V19, P139, DOI 10.1117/12.7972479; Yang LW, 2018, PROC CVPR IEEE, P3857, DOI 10.1109/CVPR.2018.00406; Zhang LC, 2013, PATTERN RECOGN LETT, V34, P856, DOI 10.1016/j.patrec.2012.08.010; Zickler TE, 2002, INT J COMPUT VISION, V49, P215, DOI 10.1023/A:1020149707513	50	0	0	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5618	5630		10.1109/TPAMI.2021.3072656	http://dx.doi.org/10.1109/TPAMI.2021.3072656			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33848240				2022-12-18	WOS:000836666600079
J	Oner, D; Kozinski, M; Citraro, L; Dadap, NC; Konings, AG; Fua, P				Oner, Doruk; Kozinski, Mateusz; Citraro, Leonardo; Dadap, Nathan C.; Konings, Alexandra G.; Fua, Pascal			Promoting Connectivity of Network-Like Structures by Enforcing Region Separation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Roads; Irrigation; Training; Image reconstruction; Image segmentation; Annotations; Topology; Road network reconstruction; aerial images; map reconstruction; connectivity	EXTRACTION; ROADS	We propose a novel, connectivity-oriented loss function for training deep convolutional networks to reconstruct network-like structures, like roads and irrigation canals, from aerial images. The main idea behind our loss is to express the connectivity of roads, or canals, in terms of disconnections that they create between background regions of the image. In simple terms, a gap in the predicted road causes two background regions, that lie on the opposite sides of a ground truth road, to touch in prediction. Our loss function is designed to prevent such unwanted connections between background regions, and therefore close the gaps in predicted roads. It also prevents predicting false positive roads and canals by penalizing unwarranted disconnections of background regions. In order to capture even short, dead-ending road segments, we evaluate the loss in small image crops. We show, in experiments on two standard road benchmarks and a new data set of irrigation canals, that convnets trained with our loss function recover road connectivity so well that it suffices to skeletonize their output to produce state of the art maps. A distinct advantage of our approach is that the loss can be plugged in to any existing training setup without further modifications.	[Oner, Doruk; Kozinski, Mateusz; Citraro, Leonardo; Fua, Pascal] Ecole Polytech Fed Lausanne, Fac Informat & Commun, CH-1015 Ecublens, Switzerland; [Dadap, Nathan C.; Konings, Alexandra G.] Stanford Univ, Dept Earth Syst Sci, Stanford, CA 94305 USA	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Stanford University	Kozinski, M (corresponding author), Ecole Polytech Fed Lausanne, Fac Informat & Commun, CH-1015 Ecublens, Switzerland.	doruk.oner@epfl.ch; mateusz.kozinski@epfl.ch; leonardo.citraro@epfl.ch; ndadap@stanford.edu; konings@stanford.edu; pascal.fua@epfl.ch		Kozinski, Mateusz/0000-0002-3187-518X; Dadap, Nathan/0000-0001-7586-6210	Swiss National Science Foundation [177237]; NASA Headquarters, NASA Earth and Space Science Fellowship Program [80NSSC18K1341]; NSF [EAR-1923478]	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission); NASA Headquarters, NASA Earth and Space Science Fellowship Program; NSF(National Science Foundation (NSF))	The authors would like to thank Xiaoling Hu from Stony Brook for providing the results of PersHomo, Zuoyue Li from ETH Zurich for sharing the results of PolyMapper, and Xiaofei Yang from the Harbin Institute of Technology for the results of RCNNU-Net. They would also like to thank Wei Wang from EPFL for advice on deploying his DRU. This work was supported in part by Swiss National Science Foundation under Sinergia Grant 177237, in part by NASA Headquarters, NASA Earth and Space Science Fellowship Program under Grant 80NSSC18K1341, and in part by NSF Award EAR-1923478.	BAJCSY R, 1976, IEEE T SYST MAN CYB, V6, P623, DOI 10.1109/TSMC.1976.4309568; Bastani F., 2018, ROADTRACER WEB PAGE; Bastani F, 2018, PROC CVPR IEEE, P4720, DOI 10.1109/CVPR.2018.00496; Batra A, 2019, PROC CVPR IEEE, P10377, DOI 10.1109/CVPR.2019.01063; Biagioni J, 2012, TRANSPORT RES REC, P61, DOI 10.3141/2291-08; Chaurasia A, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP); Cheng GL, 2017, IEEE T GEOSCI REMOTE, V55, P3322, DOI 10.1109/TGRS.2017.2669341; Chu H, 2019, IEEE I CONF COMP VIS, P4521, DOI 10.1109/ICCV.2019.00462; Clough JR, 2019, LECT NOTES COMPUT SC, V11492, P16, DOI 10.1007/978-3-030-20351-1_2; Demir I, 2018, IEEE COMPUT SOC CONF, P172, DOI 10.1109/CVPRW.2018.00031; FISCHLER MA, 1981, COMPUT VISION GRAPH, V15, P201, DOI 10.1016/0146-664X(81)90056-3; Funke J, 2019, IEEE T PATTERN ANAL, V41, P1669, DOI 10.1109/TPAMI.2018.2835450; Hu XL, 2019, ADV NEUR IN, V32; Ishii Y., 2016, TROPICAL PEATLAND EC, P265, DOI DOI 10.1007/978-4-431-55681-7_17; Kingma D.P, P 3 INT C LEARNING R; Kozinski M, 2020, MED IMAGE ANAL, V60, DOI 10.1016/j.media.2019.101590; Kozinski M, 2018, LECT NOTES COMPUT SC, V11071, P283, DOI 10.1007/978-3-030-00934-2_32; Leifeld J, 2019, NAT CLIM CHANGE, V9, P945, DOI 10.1038/s41558-019-0615-5; Li XG, 2020, IEEE T GEOSCI REMOTE, V58, P8819, DOI 10.1109/TGRS.2020.2991006; Li ZY, 2019, IEEE I CONF COMP VIS, P1715, DOI 10.1109/ICCV.2019.00180; Lindenbaum D., 2017, INTRO SPACENET ROAD; Mattyus G, 2017, IEEE I CONF COMP VIS, P3458, DOI 10.1109/ICCV.2017.372; Mnih V, 2013, MACHINE LEARNING AER; Mosinska A, 2020, IEEE T PATTERN ANAL, V42, P1515, DOI 10.1109/TPAMI.2019.2921327; Mosinska A, 2018, PROC CVPR IEEE, P3136, DOI 10.1109/CVPR.2018.00331; Murdiyarso D, 2010, P NATL ACAD SCI USA, V107, P19655, DOI 10.1073/pnas.0911966107; Planet Team, 2018, SPAC LIF EARTH; Quam L., 1978, PROC DARPA IMAGE UND, P51; Ronneberger O., 2015, P INT C MED IM COMP; Sironi A, 2014, PROC CVPR IEEE, P2697, DOI 10.1109/CVPR.2014.351; Turaga S., 2009, ADV NEURAL INFORM PR, P1865; VANDERBRUG GJ, 1976, IEEE T GEOSCI REMOTE, V14, P37, DOI 10.1109/TGE.1976.294463; Vernimmen R, 2020, WATER-SUI, V12, DOI 10.3390/w12051486; Wang W, 2019, IEEE I CONF COMP VIS, P2142, DOI 10.1109/ICCV.2019.00223; Wegner JD, 2013, PROC CVPR IEEE, P1698, DOI 10.1109/CVPR.2013.222; Wiedemann C., 1998, EMPIRICAL EVALUATION, P172; Yang XF, 2019, IEEE T GEOSCI REMOTE, V57, P7209, DOI 10.1109/TGRS.2019.2912301	37	0	0	8	8	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5401	5413		10.1109/TPAMI.2021.3074366	http://dx.doi.org/10.1109/TPAMI.2021.3074366			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33881988	Green Submitted			2022-12-18	WOS:000836666600064
J	Qiao, XT; Zheng, QL; Cao, Y; Lau, RWH				Qiao, Xiaotian; Zheng, Quanlong; Cao, Ying; Lau, Rynson W. H.			Object-Level Scene Context Prediction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Layout; Shape; Generators; Context modeling; Visualization; Task analysis; Semantics; Scene context; object inference; object properties; scene understanding		Contextual information plays an important role in solving various image and scene understanding tasks. Prior works have focused on the extraction of contextual information from an image and use it to infer the properties of some object(s) in the image or understand the scene behind the image, e.g., context-based object detection, recognition and semantic segmentation. In this paper, we consider an inverse problem, i.e., how to hallucinate the missing contextual information from the properties of standalone objects. We refer to it as object-level scene context prediction. This problem is difficult, as it requires extensive knowledge of the complex and diverse relationships among objects in the scene. We propose a deep neural network, which takes as input the properties (i.e., category, shape, and position) of a few standalone objects to predict an object-level scene layout that compactly encodes the semantics and structure of the scene context where the given objects are. Quantitative experiments and user studies demonstrate that our model can generate more plausible scene contexts than the baselines. Our model also enables the synthesis of realistic scene images from partial scene layouts. Finally, we validate that our model internally learns useful features for scene recognition and fake scene detection.	[Qiao, Xiaotian; Zheng, Quanlong; Cao, Ying; Lau, Rynson W. H.] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China	City University of Hong Kong	Cao, Y; Lau, RWH (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.	qiaoxt1992@gmail.com; xiaolong921001@gmail.com; caoying59@gmail.com; rynson.lau@cityu.edu.hk	Qiao, Xiaotian/ABB-7324-2022	Qiao, Xiaotian/0000-0002-5351-8335; Zheng, Quanlong/0000-0001-5059-0078	Research Grants Council of Hong Kong [11205620]	Research Grants Council of Hong Kong(Hong Kong Research Grants Council)	This work was supported by the Research Grants Council of Hong Kong under Grant 11205620.	Aneja S, ARXIV 210106278, P2021; Bar M, 2004, NAT REV NEUROSCI, V5, P617, DOI 10.1038/nrn1476; Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168; Chien JT, 2017, IEEE INT CONF COMP V, P182, DOI 10.1109/ICCVW.2017.30; Choi MJ, 2012, PATTERN RECOGN LETT, V33, P853, DOI 10.1016/j.patrec.2011.12.004; Choi MJ, 2012, IEEE T PATTERN ANAL, V34, P240, DOI 10.1109/TPAMI.2011.119; Chun MM, 1998, COGNITIVE PSYCHOL, V36, P28, DOI 10.1006/cogp.1998.0681; Desai C, 2011, INT J COMPUT VISION, V95, P1, DOI 10.1007/s11263-011-0439-x; Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Dvornik N, 2018, LECT NOTES COMPUT SC, V11216, P375, DOI 10.1007/978-3-030-01258-8_23; Fu Q, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130805; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He KM, 2010, LECT NOTES COMPUT SC, V6311, P1; Hensel M, 2017, ADV NEUR IN, V30; Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833; Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Izadinia H, 2014, PROC CVPR IEEE, P232, DOI 10.1109/CVPR.2014.37; Jaderberg M, 2015, ADV NEUR IN, V28; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133; Jyothi AA, 2019, IEEE I CONF COMP VIS, P9894, DOI 10.1109/ICCV.2019.00999; Kermani ZS, 2016, COMPUT GRAPH FORUM, V35, P197, DOI 10.1111/cgf.12976; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Li JN, 2021, IEEE T PATTERN ANAL, V43, P2388, DOI 10.1109/TPAMI.2019.2963663; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu TQ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661243; Liu Y, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278091; Mao Q, 2019, PROC CVPR IEEE, P1429, DOI 10.1109/CVPR.2019.00152; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Pang JM, 2019, IEEE T GEOSCI REMOTE, V57, P5512, DOI 10.1109/TGRS.2019.2899955; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Qi SY, 2018, PROC CVPR IEEE, P5899, DOI 10.1109/CVPR.2018.00618; Qiao XT, 2019, PROC CVPR IEEE, P2628, DOI 10.1109/CVPR.2019.00274; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Schwing AG, 2013, IEEE I CONF COMP VIS, P353, DOI 10.1109/ICCV.2013.51; Tan FW, 2018, IEEE WINT CONF APPL, P1519, DOI 10.1109/WACV.2018.00170; Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951; Torralba A, 2010, COMMUN ACM, V53, P107, DOI 10.1145/1666420.1666446; van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154; Vondrick C, 2016, PROC CVPR IEEE, P98, DOI 10.1109/CVPR.2016.18; Wang K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201362; Wang P, 2016, PROC CVPR IEEE, P1573, DOI 10.1109/CVPR.2016.174; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang XL, 2017, PROC CVPR IEEE, P3366, DOI 10.1109/CVPR.2017.359; Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P517, DOI 10.1007/978-3-030-01219-9_31; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	63	0	0	7	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5280	5292		10.1109/TPAMI.2021.3075676	http://dx.doi.org/10.1109/TPAMI.2021.3075676			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33905322				2022-12-18	WOS:000836666600057
J	Riochet, R; Castro, MY; Bernard, M; Lerer, A; Fergus, R; Izard, V; Dupoux, E				Riochet, Ronan; Castro, Mario Ynocente; Bernard, Mathieu; Lerer, Adam; Fergus, Rob; Izard, Veronique; Dupoux, Emmanuel			IntPhys 2019: A Benchmark for Visual Intuitive Physics Understanding	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Physics; Task analysis; Visualization; Shape; Predictive models; Motion pictures; Benchmark testing	OBJECT; PERCEPTION	In order to reach human performance on complex visual tasks, artificial systems need to incorporate a significant amount of understanding of the world in terms of macroscopic objects, movements, forces, etc. Inspired by work on intuitive physics in infants, we propose an evaluation benchmark which diagnoses how much a given system understands about physics by testing whether it can tell apart well matched videos of possible versus impossible events constructed with a game engine. The test requires systems to compute a physical plausibility score over an entire video. To prevent perceptual biases, the dataset is made of pixel matched quadruplets of videos, enforcing systems to focus on high level temporal dependencies between frames rather than pixel-level details. We then describe two Deep Neural Networks systems aimed at learning intuitive physics in an unsupervised way, using only physically possible videos. The systems are trained with a future semantic mask prediction objective and tested on the possible versus impossible discrimination task. The analysis of their results compared to human data gives novel insights in the potentials and limitations of next frame prediction architectures.	[Riochet, Ronan; Castro, Mario Ynocente; Bernard, Mathieu] Ecole Normale Super, INRIA, F-75005 Paris, France; [Lerer, Adam; Fergus, Rob] Facebook AI Res, New York, NY 10003 USA; [Izard, Veronique] Univ Paris 05, CNRS, F-75006 Paris, France; [Dupoux, Emmanuel] PSL Res Univ, CoML, ENS, EHESS,INRIA,CNRS, F-75006 Paris, France	Inria; Facebook Inc; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Cite; Centre National de la Recherche Scientifique (CNRS); Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Riochet, R (corresponding author), Ecole Normale Super, INRIA, F-75005 Paris, France.	ronan.riochet@inria.fr; mario.ynocente-castro@polyteclunique.edu; mathieu.a.bernardj@inria.fr; alerer@fb.com; robfergusl@fb.com; izard.veronique@gmail.com; ennnanuel.dupoux@gmail.com		Izard, Veronique/0000-0001-5120-7165				Alec Radford, 2016, Arxiv, DOI arXiv:1511.06434; Andrea Tacchetti, 2017, Arxiv, DOI arXiv:1706.01433; Andrea Vedaldi, 2017, Arxiv, DOI arXiv:1703.00247; Angel X. Chang, 2015, Arxiv, DOI arXiv:1512.03012; [Anonymous], 1999, 10153 BS EN; Anton Bakhtin, 2019, Arxiv, DOI arXiv:1908.05656; Antonio Torralba, 2017, Arxiv, DOI arXiv:1612.00341; Ari Weinstein, 2018, Arxiv, DOI arXiv:1804.01128; BAILLARGEON R, 1985, COGNITION, V20, P191, DOI 10.1016/0010-0277(85)90008-3; BAILLARGEON R, 1990, COGNITIVE DEV, V5, P29, DOI 10.1016/0885-2014(90)90011-H; Baillargeon R., 1992, EARLY DEV PARENTING, V1, P69, DOI DOI 10.1002/EDP.2430010203; Baillargeon R, 2012, JACOBS FOUND SER ADO, P33; Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110; Battaglia Peter W, 2016, ARXIV161200222; Battaglia PW, 2016, INTERACTION NETWORKS; Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56; Chelsea Finn, 2020, Arxiv, DOI arXiv:1910.12827; Christopher B. Choy, 2016, Arxiv, DOI arXiv:1604.00449; Emily Denton, 2016, Arxiv, DOI arXiv:1611.06430; Emmanuel Dupoux, 2020, Arxiv, DOI arXiv:2005.00069; Emmanuel Dupoux, 2016, Arxiv, DOI arXiv:1611.01368; Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2; FOX R, 1980, SCIENCE, V207, P323, DOI 10.1126/science.7350666; Fragkiadaki K., 2016, 6 INT C LEARN REPR I; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hespos SJ, 2012, WIRES COGN SCI, V3, P19, DOI 10.1002/wcs.157; KELLMAN PJ, 1983, COGNITIVE PSYCHOL, V15, P483, DOI 10.1016/0010-0285(83)90017-8; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Lerer A, 2016, PR MACH LEARN RES, V48; Li W., 2016, ARXIV; Li W., 2016, ARXIV; Mehdi Mirza, 2014, Arxiv, DOI arXiv:1411.1784; Mirza M., 2016, ARXIV161203809; Mottaghi R, 2016, LECT NOTES COMPUT SC, V9908, P269, DOI 10.1007/978-3-319-46493-0_17; Oh J., 2015, P ADV NEUR INF PROC, P2863; Pinheiro P. O. O., 2015, ADV NEURAL INFORM PR, V28, P1990; PYLYSHYN Z W, 1988, Spatial Vision, V3, P179, DOI 10.1163/156856888X00122; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Saxe R, 2006, ACTA PSYCHOL, V123, P144, DOI 10.1016/j.actpsy.2006.05.005; Smith KA, 2019, ADV NEUR IN, V32; SPELKE ES, 1995, BRIT J DEV PSYCHOL, V13, P113, DOI 10.1111/j.2044-835X.1995.tb00669.x; Valenza E, 2006, CHILD DEV, V77, P1810, DOI 10.1111/j.1467-8624.2006.00975.x; Wilcox T, 1999, COGNITION, V72, P125, DOI 10.1016/S0010-0277(99)00035-9; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Wu J., 2016, PROC BRIT MACH VIS C, V2, P7; Xu F, 1996, COGNITIVE PSYCHOL, V30, P111, DOI 10.1006/cogp.1996.0005; Xue Tianfan, 2016, ADV NEURAL INFORM PR, P2; Zhang R., 2016, P ANN M COGN SCI SOC; Zitnick CL, 2013, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR.2013.387	54	0	0	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5016	5025		10.1109/TPAMI.2021.3083839	http://dx.doi.org/10.1109/TPAMI.2021.3083839			10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	34038357	Green Submitted, hybrid			2022-12-18	WOS:000836666600040
J	Sun, W; Wu, TF				Sun, Wei; Wu, Tianfu			Learning Layout and Style Reconfigurable GANs for Controllable Image Synthesis	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Layout; Image synthesis; Generators; Snow; Task analysis; Training; Fasteners; Image synthesis; layout-to-image; layout-to-mask-to-image; deep generative learning; GAN; ISLA-norm		With the remarkable recent progress on learning deep generative models, it becomes increasingly interesting to develop models for controllable image synthesis from reconfigurable structured inputs. This paper focuses on a recently emerged task, layout-to-image, whose goal is to learn generative models for synthesizing photo-realistic images from a spatial layout (i.e., object bounding boxes configured in an image lattice) and its style codes (i.e., structural and appearance variations encoded by latent vectors). This paper first proposes an intuitive paradigm for the task, layout-to-mask-to-image, which learns to unfold object masks in a weakly-supervised way based on an input layout and object style codes. The layout-to-mask component deeply interacts with layers in the generator network to bridge the gap between an input layout and synthesized images. Then, this paper presents a method built on Generative Adversarial Networks (GANs) for the proposed layout-to-mask-to-image synthesis with layout and style control at both image and object levels. The controllability is realized by a proposed novel Instance-Sensitive and Layout-Aware Normalization (ISLA-Norm) scheme. A layout semi-supervised version of the proposed method is further developed without sacrificing performance. In experiments, the proposed method is tested in the COCO-Stuff dataset and the Visual Genome dataset with state-of-the-art performance obtained.	[Sun, Wei; Wu, Tianfu] North Carolina State Univ, Dept Elect & Comp Engn, Raleigh, NC 27695 USA; [Sun, Wei; Wu, Tianfu] North Carolina State Univ, Visual Narrat Initiat, Raleigh, NC 27695 USA	University of North Carolina; North Carolina State University; University of North Carolina; North Carolina State University	Wu, TF (corresponding author), North Carolina State Univ, Dept Elect & Comp Engn, Raleigh, NC 27695 USA.; Wu, TF (corresponding author), North Carolina State Univ, Visual Narrat Initiat, Raleigh, NC 27695 USA.	wsun12@ncsu.edu; tianfu_wu@ncsu.edu		Wu, Tianfu/0000-0001-8911-5506	NSF [IIS-1909644, IIS-1822477, CMMI-2024688, IUSE-2013451]; ARO [W911NF1810295]; DHHS-ACL Grant [90IFDV0017-01-00]	NSF(National Science Foundation (NSF)); ARO; DHHS-ACL Grant	This work was supported in part by NSF IIS-1909644, ARO Grant W911NF1810295, NSF IIS-1822477, NSF CMMI-2024688, NSF IUSE-2013451 and DHHS-ACL Grant 90IFDV0017-01-00. The views presented in this paper are those of the authors and should not be interpreted as representing any funding agencies.	[Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Arjovsky M, 2017, PR MACH LEARN RES, V70; Ashual O, 2019, IEEE I CONF COMP VIS, P4560, DOI 10.1109/ICCV.2019.00466; Bansal A., 2019, P IEEE C COMPUTER VI, P2317; Brock A., 2019, PROC 7 INT C LEARN R; de Vries H, 2017, ADV NEUR IN, V30; DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X; Dumoulin V., 2017, PROC 5 INT C LEARN R; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grathwohl Will, 2019, ICLR; Han T, 2019, PROC CVPR IEEE, P8662, DOI 10.1109/CVPR.2019.00887; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hensel M, 2017, ADV NEUR IN, V30; Hinz T., 2019, ICLR; Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833; Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jae Hyun Lim, 2017, Arxiv, DOI arXiv:1705.02894; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Karras Tero, 2018, INT C LEARN REPR; Kim T, 2017, PR MACH LEARN RES, V70; Kingma D.P., 2013, P 2 INT C LEARN REPR; Kingma D. P., 2015, 3 INT C LEARN REPR I, P1; Kirillov Alexander, 2020, CVPR; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; LeCun Y., 2006, PREDICTING STRUCTURE, V1; Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514; Li YK, 2019, ADV NEUR IN, V32; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065; Mansimov E., 2016, PROC 4 INT C LEARN R; Mehdi Mirza, 2014, Arxiv, DOI arXiv:1411.1784; Miyato Takeru, 2018, 6 INT C LEARNING REP, P8; Odena A, 2017, PR MACH LEARN RES, V70; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Radford A., 2016, INT C LEARN REPR; Ravuri S, 2019, ADV NEUR IN, V32; Reed S, 2016, PR MACH LEARN RES, V48; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Salimans T, 2016, ADV NEUR IN, V29; Saxe A. M., 2014, PROC 2 INT C LEARN R; Simonyan K., 2015, ICLR; Stefan Heinrich, 2020, Arxiv, DOI arXiv:1910.13321; Sun W, 2019, IEEE I CONF COMP VIS, P10530, DOI 10.1109/ICCV.2019.01063; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tran D, 2017, ADV NEUR IN, V30; van den Oord A, 2016, ADV NEUR IN, V29; van den Oord A, 2016, PR MACH LEARN RES, V48; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Xie JW, 2016, PR MACH LEARN RES, V48; Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143; Zhang H, 2019, PR MACH LEARN RES, V97; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhao B., 2020, INT J COMPUT VISION, V24, P1; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	62	0	0	3	3	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5070	5087		10.1109/TPAMI.2021.3078577	http://dx.doi.org/10.1109/TPAMI.2021.3078577			18	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33970857	Green Submitted			2022-12-18	WOS:000836666600044
J	Tilmon, B; Jain, E; Ferrari, S; Koppal, S				Tilmon, Brevin; Jain, Eakta; Ferrari, Silvia; Koppal, Sanjeev			Fast Foveating Cameras for Dense Adaptive Resolution	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cameras; Mirrors; Micromechanical devices; Sensors; Image resolution; Imaging; Optical sensors; Computational photography; computer vision	TRACKING	Traditional cameras field of view (FOV) and resolution predetermine computer vision algorithm performance. These trade-offs decide the range and performance in computer vision algorithms. We present a novel foveating camera whose viewpoint is dynamically modulated by a programmable micro-electromechanical (MEMS) mirror, resulting in a natively high-angular resolution wide-FOV camera capable of densely and simultaneously imaging multiple regions of interest in a scene. We present calibrations, novel MEMS control algorithms, a real-time prototype, and comparisons in remote eye-tracking performance against a traditional smartphone, where high-angular resolution and wide-FOV are necessary, but traditionally unavailable.	[Tilmon, Brevin; Koppal, Sanjeev] Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA; [Jain, Eakta] Univ Florida, Dept Comp & Informat Sci & Engn, Gainesville, FL 32611 USA; [Ferrari, Silvia] Cornell Univ, Sch Mech & Aerosp Engn, Ithaca, NY 14850 USA	State University System of Florida; University of Florida; State University System of Florida; University of Florida; Cornell University	Tilmon, B (corresponding author), Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.	btilmon@ufl.edu; ejain@cise.ufl.edu; sf375@cornell.edu; sjkoppal@ece.ufl.edu			National Science Foundation [NSF IIS: 1909729, NSF: 1566481]; Office of Naval Research [N00014-19-1-2144, ONR N00014-18-1-2663]	National Science Foundation(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research)	The work of Brevin Tilmon and Sanjeev Koppal was supported in part by the National Science Foundation under Grant NSF IIS: 1909729 and in part by the Office of Naval Research under Grant ONR N00014-18-1-2663. The work of Eakta Jain was supported by the National Science Foundation under Grant NSF: 1566481. The work of Silvia Ferrari was supported by the Office of Naval Research under Grant N00014-19-1-2144.	Achar S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073686; Ayan Chakrabarti, 2020, Arxiv, DOI arXiv:2003.09545; Beymer D, 2003, PROC CVPR IEEE, P451; Bruce N., 2010, J VISION, V7, P950, DOI [10.1167/7.9.950, DOI 10.1167/7.9.950]; Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754; Charrow B, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Cho DC, 2012, IEEE T CONSUM ELECTR, V58, P1119, DOI 10.1109/TCE.2012.6414976; Ciocoiu IB, 2015, CIRC SYST SIGNAL PR, V34, P1001, DOI 10.1007/s00034-014-9878-2; Flatley T. P., 2015, SPACECUBE FAMILY REC; Frintrop S, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1658349.1658355; Geisler D, 2018, IEEE INT CONF ROBOT, P7119; Gupta M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2735702; Henderson K, 2020, IEEE T COMPUT IMAG, V6, P529, DOI 10.1109/TCI.2020.2964246; Hennessey C., 2012, P ETRA 12, P249, DOI [DOI 10.1145/2168556.2168608, 10.1145/2168556.2168608]; Hua H, 2008, APPL OPTICS, V47, P317, DOI 10.1364/AO.47.000317; Jones A, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276427, 10.1145/1239451.1239491]; Jones M., 2003, TR2000396314 MITS EL, V3, P2; Kasturi A, 2016, PROC SPIE, V9832, DOI 10.1117/12.2224285; Khamis M, 2017, UIST'17: PROCEEDINGS OF THE 30TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P155, DOI 10.1145/3126594.3126630; Krafka K, 2016, PROC CVPR IEEE, P2176, DOI 10.1109/CVPR.2016.239; Krastev K. T., 2013, U.S. Patent, Patent No. 8526089; Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521; Liu S., 2006, PROC INT OPT C, P869; Milanovic V., 2011, TRANSDUCERS 2011 - 2011 16th International Solid-State Sensors, Actuators and Microsystems Conference, P1895, DOI 10.1109/TRANSDUCERS.2011.5969770; Milanovic V, 2017, PROC SPIE, V10116, DOI 10.1117/12.2253425; Nakao T, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P1045; Nanhu Chen, 2010, 2010 IEEE International Conference on Automation Science and Engineering (CASE 2010), P27, DOI 10.1109/COASE.2010.5584424; Nayar SK, 2006, INT J COMPUT VISION, V70, P7, DOI 10.1007/s11263-005-3102-6; Ng R, 2005, ACM T GRAPHIC, V24, P735, DOI 10.1145/1073204.1073256; O'Toole M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766897; O'Toole M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601103; Okumura Kohei, 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P6186, DOI 10.1109/ICRA.2011.5980080; Paas F, 2003, EDUC PSYCHOL-US, V38, P63, DOI 10.1207/S15326985EP3801_8; Sandini G, 2003, SENSORS AND SENSING IN BIOLOGY AND ENGINEERING, P251; Sandner T, 2015, PROC SPIE, V9375, DOI 10.1117/12.2076440; Stann B. L, 2014, PROC SPIE, P177; Tang T, 2009, OPT ENG, V48, DOI 10.1117/1.3065500; Tasneem Z, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Tasneem Z, 2020, INT J ROBOT RES, V39, P837, DOI 10.1177/0278364920920931; Tilmon B, 2020, IEEE INT CONF COMPUT; Velten A, 2016, COMMUN ACM, V59, P79, DOI 10.1145/2975165; Wakin MB, 2006, IEEE IMAGE PROC, P1273, DOI 10.1109/ICIP.2006.312577; Wang J, 2018, LECT NOTES COMPUT SC, V11207, P20, DOI 10.1007/978-3-030-01219-9_2; Wei HC, 2019, IEEE T AUTOMAT CONTR, V64, P159, DOI 10.1109/TAC.2018.2849584; Wei HC, 2014, IEEE INT C INT ROBOT, P76, DOI 10.1109/IROS.2014.6942543; Weisstein E. W, RADON TRANSFORM GAUS; Zomet A., 2006, COMP VIS PATT REC 20, V1, P339	48	0	0	5	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4867	4878		10.1109/TPAMI.2021.3071588	http://dx.doi.org/10.1109/TPAMI.2021.3071588			12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33856979				2022-12-18	WOS:000836666600030
J	Tozza, S; Zhu, DZ; Smith, WAP; Ramamoorthi, R; Hancock, ER				Tozza, Silvia; Zhu, Dizhong; Smith, William A. P.; Ramamoorthi, Ravi; Hancock, Edwin R.			Uncalibrated, Two Source Photo-Polarimetric Stereo	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Estimation; Lighting; Shape; Refractive index; Light sources; Image color analysis; Cameras; Polarisation; shape-from-x; bas-relief ambiguity; illumination estimation; differential approach; photometric stereo	POLARIZATION; SHAPE; RECONSTRUCTION	In this paper we present methods for estimating shape from polarisation and shading information, i.e. photo-polarimetric shape estimation, under varying, but unknown, illumination, i.e. in an uncalibrated scenario. We propose several alternative photo-polarimetric constraints that depend upon the partial derivatives of the surface and show how to express them in a unified system of partial differential equations of which previous work is a special case. By careful combination and manipulation of the constraints, we show how to eliminate non-linearities such that a discrete version of the problem can be solved using linear least squares. We derive a minimal, combinatorial approach for two source illumination estimation which we use with RANSAC for robust light direction and intensity estimation. We also introduce a new method for estimating a polarisation image from multichannel data and provide methods for estimating albedo and refractive index. We evaluate lighting, shape, albedo and refractive index estimation methods on both synthetic and real-world data showing improvements over existing state-of-the-art.	[Tozza, Silvia] Univ Napoli Federico II, Dept Math & Applicat Renato Caccioppoli, I-80126 Naples, Italy; [Zhu, Dizhong; Smith, William A. P.; Hancock, Edwin R.] Univ York, Dept Comp Sci, York YO10 5DD, N Yorkshire, England; [Ramamoorthi, Ravi] Univ Calif San Diego, CSE Dept, La Jolla, CA 92093 USA	University of Naples Federico II; University of York - UK; University of California System; University of California San Diego	Smith, WAP (corresponding author), Univ York, Dept Comp Sci, York YO10 5DD, N Yorkshire, England.	silvia.tozza@unina.it; dz761@york.ac.uk; william.smith@york.ac.uk; ravir@cs.ucsd.edu; edwin.hancock@york.ac.uk	Hancock, Edwin R/C-6071-2008	Hancock, Edwin R/0000-0003-4496-2028	GNCS -INdAM; ONR [N000141512013, N000142012529]; UC San Diego Center for Visual Computing; EPSRC [EP/N028481/1]; Royal Academy of Engineering/The Leverhulme Trust Senior Research Fellowship; PON R&I under Grant 2014-2020 through AIM: Attraction and International Mobility (Linea 2.1) [AIM1834118 -2, CUP: E61G19000050001]	GNCS -INdAM(Istituto Nazionale di Alta Matematica (INDAM)); ONR(Office of Naval Research); UC San Diego Center for Visual Computing; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Royal Academy of Engineering/The Leverhulme Trust Senior Research Fellowship; PON R&I under Grant 2014-2020 through AIM: Attraction and International Mobility (Linea 2.1)	This work was supported in part by GNCS -INdAM, in part by ONR under Grants N000141512013 and N000142012529, and in part by the UC San Diego Center for Visual Computing. The work of William Smith was supported in part by EPSRC under Grant EP/N028481/1 and in part by the Royal Academy of Engineering/The Leverhulme Trust Senior Research Fellowship. The work of Silvia Tozza was supported by PON R&I under Grant 2014-2020 through AIM: Attraction and International Mobility (Linea 2.1, project AIM1834118 -2, CUP: E61G19000050001).	Achuta Kadambi, 2016, Arxiv, DOI arXiv:1605.02066; [Anonymous], 2020, MAT INDEX REFRACTION; Atkinson GA, 2007, IEEE T PATTERN ANAL, V29, P2001, DOI 10.1109/TPAMI.2007.1099; Atkinson GA, 2007, LECT NOTES COMPUT SC, V4673, P466; Atkinson GA, 2006, IEEE T IMAGE PROCESS, V15, P1653, DOI 10.1109/TIP.2006.871114; Atkinson GA, 2017, COMPUT VIS IMAGE UND, V160, P158, DOI 10.1016/j.cviu.2017.04.014; Barron J. T., 1999, IEEE T PATTERN ANAL, V21, P690; Belhumeur PN, 1999, INT J COMPUT VISION, V35, P33, DOI 10.1023/A:1008154927611; Berger Kai, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1966, DOI 10.1109/ICRA.2017.7989227; Blinn James F., 1977, COMPUT GRAPHICS-US, V11, P192, DOI [DOI 10.1145/965141.563893, 10.1145/965141, DOI 10.1145/965141]; Chen LX, 2018, LECT NOTES COMPUT SC, V11220, P21, DOI 10.1007/978-3-030-01270-0_2; Cui ZP, 2019, IEEE I CONF COMP VIS, P2671, DOI 10.1109/ICCV.2019.00276; Cui ZP, 2017, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2017.47; Dizhong Zhu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P530, DOI 10.1007/978-3-030-58542-6_32; Dizhong Zhu, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7578, DOI 10.1109/CVPR.2019.00777; Drbohlav O, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P581, DOI 10.1109/ICCV.2001.937570; Ecker A, 2010, PROC CVPR IEEE, P145, DOI 10.1109/CVPR.2010.5540219; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Huynh CP, 2013, INT J COMPUT VISION, V101, P64, DOI 10.1007/s11263-012-0546-3; Huynh CP, 2010, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2010.5539828; Kadambi A, 2017, INT J COMPUT VISION, V125, P34, DOI 10.1007/s11263-017-1025-7; Kadambi A, 2015, IEEE I CONF COMP VIS, P3370, DOI 10.1109/ICCV.2015.385; Logothetis F, 2019, INT J COMPUT VISION, V127, P1680, DOI 10.1007/s11263-018-1127-x; Lyu Y., 2019, PROC NEURAL INF PROC, P14559; Mahmoud AH, 2012, IEEE IMAGE PROC, P1769, DOI 10.1109/ICIP.2012.6467223; Mecca R., 2017, PROC BRIT MACH VIS C, P1680; Mecca R, 2013, SIAM J IMAGING SCI, V6, P616, DOI 10.1137/110857258; Mecca Roberto, 2016, P IEEE WINT C APPL C, P1; Miyazaki D, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P982; Miyazaki D, 2004, IEEE T PATTERN ANAL, V26, P73, DOI 10.1109/TPAMI.2004.1261080; Miyazaki D, 2017, OPT ENG, V56, DOI 10.1117/1.OE.56.4.041303; Miyazaki D, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P104, DOI 10.1109/3DIMPVT.2012.14; Morel O, 2005, PROC SPIE, V5679, P178, DOI 10.1117/12.586815; Narasimhan SG, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1387; Nayar SK, 1997, INT J COMPUT VISION, V21, P163, DOI 10.1023/A:1007937815113; Queau Y, 2017, IMAGE VISION COMPUT, V57, P175, DOI 10.1016/j.imavis.2016.11.006; Queau Y, 2016, PROC CVPR IEEE, P4359, DOI 10.1109/CVPR.2016.472; Rahmann S, 2001, PROC CVPR IEEE, P149; Smith W, 2016, COMPUT VIS IMAGE UND, V145, P128, DOI 10.1016/j.cviu.2015.11.019; Smith WAP, 2019, IEEE T PATTERN ANAL, V41, P2875, DOI 10.1109/TPAMI.2018.2868065; Smith WAP, 2016, LECT NOTES COMPUT SC, V9912, P109, DOI 10.1007/978-3-319-46484-8_7; Tickell F. G, 2011, TECHNIQUES SEDIMENTA; Tozza S, 2016, J MATH IMAGING VIS, V56, P57, DOI 10.1007/s10851-016-0633-0; Tozza S, 2017, IEEE I CONF COMP VIS, P2298, DOI 10.1109/ICCV.2017.250; Ngo TT, 2015, PROC CVPR IEEE, P2310, DOI 10.1109/CVPR.2015.7298844; Wieschollek P, 2018, LECT NOTES COMPUT SC, V11217, P90, DOI 10.1007/978-3-030-01261-8_6; WOLFF LB, 1991, IEEE T PATTERN ANAL, V13, P635, DOI 10.1109/34.85655; WOLFF LB, 1990, P SOC PHOTO-OPT INS, V1194, P287, DOI 10.1117/12.969861; Wolff LB, 1997, IMAGE VISION COMPUT, V15, P81, DOI 10.1016/S0262-8856(96)01123-7; Yang LW, 2018, PROC CVPR IEEE, P3857, DOI 10.1109/CVPR.2018.00406; Yu Y, 2017, IEEE INT CONF COMP V, P2969, DOI 10.1109/ICCVW.2017.350	52	0	0	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5747	5760		10.1109/TPAMI.2021.3078101	http://dx.doi.org/10.1109/TPAMI.2021.3078101			14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33956625	Green Accepted			2022-12-18	WOS:000836666600087
J	Wang, W; Dang, Z; Hu, YL; Fua, P; Salzmann, M				Wang, Wei; Dang, Zheng; Hu, Yinlin; Fua, Pascal; Salzmann, Mathieu			Robust Differentiable SVD	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Covariance matrices; Eigenvalues and eigenfunctions; Training; Explosions; Decorrelation; Taylor series; Principal component analysis; Eigendecomposition; differentiable SVD; power iteration; taylor expansion		Eigendecomposition of symmetric matrices is at the heart of many computer vision algorithms. However, the derivatives of the eigenvectors tend to be numerically unstable, whether using the SVD to compute them analytically or using the Power Iteration (PI) method to approximate them. This instability arises in the presence of eigenvalues that are close to each other. This makes integrating eigendecomposition into deep networks difficult and often results in poor convergence, particularly when dealing with large matrices. While this can be mitigated by partitioning the data into small arbitrary groups, doing so has no theoretical basis and makes it impossible to exploit the full power of eigendecomposition. In previous work, we mitigated this using SVD during the forward pass and PI to compute the gradients during the backward pass. However, the iterative deflation procedure required to compute multiple eigenvectors using PI tends to accumulate errors and yield inaccurate gradients. Here, we show that the Taylor expansion of the SVD gradient is theoretically equivalent to the gradient obtained using PI without relying in practice on an iterative process and thus yields more accurate gradients. We demonstrate the benefits of this increased accuracy for image classification and style transfer.	[Wang, Wei; Hu, Yinlin; Fua, Pascal; Salzmann, Mathieu] Ecole Polytech Fed Lausanne, CVLab, Sch Comp & Commun Sci, CH-1015 Lausanne, Switzerland; [Dang, Zheng] Xi An Jiao Tong Univ, Natl Engn Lab Visual Informat Proc & Applicat, Xian 710049, Peoples R China	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Xi'an Jiaotong University	Wang, W (corresponding author), Ecole Polytech Fed Lausanne, CVLab, Sch Comp & Commun Sci, CH-1015 Lausanne, Switzerland.	wangwei1990@gmail.com; dangzheng713@stu.xjtu.edu.cn; huyinlin@gmail.com; pascal.fua@epfl.ch; mathieu.salzmanni@epfl.ch		Hu, Yinlin/0000-0003-2614-5200; WANG, WEI/0000-0002-5477-1017; Salzmann, Mathieu/0000-0002-8347-8637	 [PFES-ES 25939.1]		This work was supported in part under an Innossuisse contract PFES-ES 25939.1.	Alexander S. Ecker, 2015, Arxiv, DOI arXiv:1505.07376; Andrew R, 2014, PARALLEL COMPUT, V40, P161, DOI 10.1016/j.parco.2014.03.003; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Burden R., 1989, NUMERICAL ANAL, V4th; Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32; Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296; Chiu TY, 2019, IEEE I CONF COMP VIS, P4451, DOI 10.1109/ICCV.2019.00455; Cho W, 2019, PROC CVPR IEEE, P10631, DOI 10.1109/CVPR.2019.01089; Dang Z, 2018, LECT NOTES COMPUT SC, V11209, P792, DOI 10.1007/978-3-030-01228-1_47; Ferraz L, 2014, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2014.71; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang L, 2020, PROC CVPR IEEE, P6438, DOI 10.1109/CVPR42600.2020.00647; Huang L, 2019, PROC CVPR IEEE, P4869, DOI 10.1109/CVPR.2019.00501; Huang L, 2018, PROC CVPR IEEE, P791, DOI 10.1109/CVPR.2018.00089; Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Ionescu C, 2015, IEEE I CONF COMP VIS, P2965, DOI 10.1109/ICCV.2015.339; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; kaggle, TINY IMAGENET; Kaicheng Yu, 2017, Arxiv, DOI arXiv:1703.06817; Kessy A, 2018, AM STAT, V72, P309, DOI 10.1080/00031305.2016.1277159; Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3; Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6; Lewis AS, 1996, MATH OPER RES, V21, P576, DOI 10.1287/moor.21.3.576; Li PH, 2018, PROC CVPR IEEE, P947, DOI 10.1109/CVPR.2018.00105; Li PH, 2017, IEEE I CONF COMP VIS, P2089, DOI 10.1109/ICCV.2017.228; Li YJ, 2017, ADV NEUR IN, V30; Li YJ, 2017, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.2017.36; Liang JB, 2019, ADV NEUR IN, V32; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Nakatsukasa Y, 2013, SIAM J SCI COMPUT, V35, pA1325, DOI 10.1137/120876605; Pan XG, 2019, IEEE I CONF COMP VIS, P1863, DOI 10.1109/ICCV.2019.00195; Papadopoulo T, 2000, LECT NOTES COMPUT SC, V1842, P554; Roy R., 1990, MATH MAG, V63, P291; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; RUTISHAUSER H, 1970, NUMER MATH, V16, P205, DOI 10.1007/BF02219773; Siarohin A., 2019, PROC INT C LEARN REP; Subhransu Maji, 2017, Arxiv, DOI arXiv:1707.06772; Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Ulyanov D, 2016, PR MACH LEARN RES, V48; Wang QL, 2017, PROC CVPR IEEE, P6507, DOI 10.1109/CVPR.2017.689; Wang W, 2020, IEEE T MULTIMEDIA, V22, P2808, DOI 10.1109/TMM.2019.2963621; Wang W, 2019, ADV NEUR IN, V32; Wang W, 2018, PROC CVPR IEEE, P7083, DOI 10.1109/CVPR.2018.00740; Ye M, 2019, IEEE T IMAGE PROCESS, V28, P2976, DOI 10.1109/TIP.2019.2893066; Yi KM, 2018, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2018.00282; Zanfir A, 2018, PROC CVPR IEEE, P2684, DOI 10.1109/CVPR.2018.00284; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	56	0	0	4	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					5472	5487		10.1109/TPAMI.2021.3072422	http://dx.doi.org/10.1109/TPAMI.2021.3072422			16	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33844626	Green Submitted			2022-12-18	WOS:000836666600069
J	Xiong, H; Yu, MY; Liu, L; Zhu, F; Qin, J; Shen, FM; Shao, L				Xiong, Huan; Yu, Mengyang; Liu, Li; Zhu, Fan; Qin, Jie; Shen, Fumin; Shao, Ling			A Generalized Method for Binary Optimization: Convergence Analysis and Applications	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Convergence; Optimization methods; Approximation algorithms; Quantization (signal); Image segmentation; Matrix decomposition; Machine learning; Binary optimization; hashing; graph bisection; dense subgraph discovery; constrained image segmentation	ALTERNATING DIRECTION METHOD; MATRIX COMPLETION; MINIMIZATION; ALGORITHMS; SEARCH; REAL	Binary optimization problems (BOPs) arise naturally in many fields, such as information retrieval, computer vision, and machine learning. Most existing binary optimization methods either use continuous relaxation which can cause large quantization errors, or incorporate a highly specific algorithm that can only be used for particular loss functions. To overcome these difficulties, we propose a novel generalized optimization method, named Alternating Binary Matrix Optimization (ABMO), for solving BOPs. ABMO can handle BOPs with/without orthogonality or linear constraints for a large class of loss functions. ABMO involves rewriting the binary, orthogonality and linear constraints for BOPs as an intersection of two closed sets, then iteratively dividing the original problems into several small optimization problems that can be solved as closed forms. To provide a strict theoretical convergence analysis, we add a sufficiently small perturbation and translate the original problem to an approximated problem whose feasible set is continuous. We not only provide rigorous mathematical proof for the convergence to a stationary and feasible point, but also derive the convergence rate of the proposed algorithm. The promising results obtained from four binary optimization tasks validate the superiority and the generality of ABMO compared with the state-of-the-art methods.	[Xiong, Huan] Harbin Inst Technol, Inst Adv Study Math, Harbin 150001, Peoples R China; [Xiong, Huan] Mohamed bin Zayed Univ Artificial Intelligence, Abu Dhabi, U Arab Emirates; [Yu, Mengyang; Zhu, Fan; Qin, Jie; Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates; [Liu, Li] Incept Inst Artificial Intelligence, Res Comp Vis, Abu Dhabi, U Arab Emirates; [Shen, Fumin] Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Peoples R China; [Shen, Fumin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China	Harbin Institute of Technology; Mohamed Bin Zayed University of Artificial Intelligence; University of Electronic Science & Technology of China; University of Electronic Science & Technology of China	Xiong, H (corresponding author), Harbin Inst Technol, Inst Adv Study Math, Harbin 150001, Peoples R China.	huan.xiong.math@gmail.com; mengyang.yu@inceptioniai.org; liuli1213@ieee.org; fan.zhu@inceptioniai.org; qinjiebuaa@gmail.com; fumin.shen@gmail.com; ling.shao@ieee.org						Alavian A, 2017, IEEE C INFORM SCI SY, P1; Alvarez-Hamelin J. I., 2005, ADV NEURAL INFORM PR, P41; Ames BPW, 2015, J OPTIMIZ THEORY APP, V167, P653, DOI 10.1007/s10957-015-0777-x; Ames BPW, 2011, MATH PROGRAM, V129, P69, DOI 10.1007/s10107-011-0459-x; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Angel A, 2012, PROC VLDB ENDOW, V5, P574, DOI 10.14778/2168651.2168658; Attouch H, 2010, MATH OPER RES, V35, P438, DOI 10.1287/moor.1100.0449; Balalau OD, 2015, WSDM'15: PROCEEDINGS OF THE EIGHTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P379, DOI 10.1145/2684822.2685298; Bernard Ghanem, 2018, Arxiv, DOI arXiv:1608.04430; Bernard Ghanem, 2017, Arxiv, DOI arXiv:1608.04425; Bolte J, 2007, SIAM J OPTIMIZ, V17, P1205, DOI 10.1137/050644641; Bolte J, 2007, SIAM J OPTIMIZ, V18, P556, DOI 10.1137/060670080; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505; Burchard A., 2009, LECT NOTES; Chan RH, 2011, SIAM J IMAGING SCI, V4, P807, DOI 10.1137/100807247; Chen CH, 2012, IMA J NUMER ANAL, V32, P227, DOI 10.1093/imanum/drq039; Cheney W., 2009, LINEAR ALGEBRA THEOR, P110; Chua T.-S., 2009, P ACM INT C IM VID R, P1, DOI 10.1145/1646396.1646452; DAI WM, 1987, IEEE T COMPUT AID D, V6, P828; Forsyth DA, 2002, PRENT HALL PROF TECH; Garey M. R., 1979, Computers and intractability. A guide to the theory of NP-completeness; Garey M. R., 1974, P 6 ANN ACM S THEORY, P47, DOI DOI 10.1145/800119.803884; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Gui J, 2018, IEEE T PATTERN ANAL, V40, P490, DOI 10.1109/TPAMI.2017.2678475; Hendrickson B, 2000, SIAM J SCI COMPUT, V21, P2048, DOI 10.1137/S1064827598341475; Hendrickson B, 2000, PARALLEL COMPUT, V26, P1519, DOI 10.1016/S0167-8191(00)00048-X; HORN BKP, 1988, J OPT SOC AM A, V5, P1127, DOI 10.1364/JOSAA.5.001127; Hsieh CJ, 2015, PR MACH LEARN RES, V37, P2445; Hu MQ, 2018, IEEE T IMAGE PROCESS, V27, P545, DOI 10.1109/TIP.2017.2749147; Huang QX, 2014, PR MACH LEARN RES, V32, P64; Jiang B, 2014, OPTIMIZATION, V63, P883, DOI 10.1080/02331934.2014.895901; KARGER DR, 1993, PROCEEDINGS OF THE FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P21; Keuchel J, 2003, IEEE T PATTERN ANAL, V25, P1364, DOI 10.1109/TPAMI.2003.1240111; Komodakis N, 2007, IEEE T PATTERN ANAL, V29, P1436, DOI 10.1109/TPAMI.2007.1061; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Li GY, 2015, SIAM J OPTIMIZ, V25, P2434, DOI 10.1137/140998135; Lin GS, 2014, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2014.253; Lin GS, 2013, IEEE I CONF COMP VIS, P2552, DOI 10.1109/ICCV.2013.317; Liu W., 2014, ADV NEURAL INFORM PR, V4, P3419; Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912; Liu W, 2011, SER INF MANAGE SCI, V10, P1; Loiola EM, 2007, EUR J OPER RES, V176, P657, DOI 10.1016/j.ejor.2005.09.032; Lojasiewicz S., 1993, ANN LINSTITUT FOURIE, V43, P1575; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu ZS, 2013, SIAM J OPTIMIZ, V23, P2448, DOI 10.1137/100808071; Luo YD, 2018, PATTERN RECOGN, V75, P128, DOI 10.1016/j.patcog.2017.02.034; Luo Z.Q., 1996, MATH PROGRAMS EQUILI, DOI DOI 10.1017/CBO9780511983658; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Moreno J, 2009, MECH SYST SIGNAL PR, V23, P1784, DOI 10.1016/j.ymssp.2008.06.011; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Olsson C., 2007, P IEEE C COMP VIS PA, p1{8, DOI DOI 10.1109/CVPR.2007.383202; Park S, 2018, IEEE ACCESS, V6, P58439, DOI 10.1109/ACCESS.2018.2875022; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; RAVI SS, 1994, OPER RES, V42, P299, DOI 10.1287/opre.42.2.299; Rockafellar R. T., 2009, VARIATIONAL ANAL, DOI DOI 10.1007/978-3-030-63416-2_683; Rozenshtein P, 2018, IEEE DATA MINING, P397, DOI 10.1109/ICDM.2018.00055; SANCHIS LA, 1989, IEEE T COMPUT, V38, P62, DOI 10.1109/12.8730; Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887; Shen FM, 2016, IEEE T IMAGE PROCESS, V25, P5610, DOI 10.1109/TIP.2016.2612883; Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598; Shen XB, 2016, NEUROCOMPUTING, V213, P14, DOI 10.1016/j.neucom.2016.01.121; Shrivastava A, 2015, PROC CVPR IEEE, P2282, DOI 10.1109/CVPR.2015.7298841; Sivic J, 2009, IEEE T PATTERN ANAL, V31, P591, DOI 10.1109/TPAMI.2008.111; Sotirov R, 2018, ANN OPER RES, V265, P143, DOI 10.1007/s10479-017-2575-3; Szeliski R, 2011, TEXTS COMPUT SCI, P1, DOI 10.1007/978-1-84882-935-0; Vedaldi Andrea, 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249; Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960; Wang M., 2011, J MACHINE LEARNING R, P2011; Wang P, 2017, IEEE T PATTERN ANAL, V39, P470, DOI 10.1109/TPAMI.2016.2541146; Wang Y, 2019, J SCI COMPUT, V78, P29, DOI 10.1007/s10915-018-0757-z; Weiss Y., 2009, ADV NEURAL INFORM PR, P1753; Wu B., 2015, IEEE T PATTERN ANAL; Xu YY, 2012, FRONT MATH CHINA, V7, P365, DOI 10.1007/s11464-012-0194-5; Yadav AK, 2016, ANN ALLERTON CONF, P1074, DOI 10.1109/ALLERTON.2016.7852354; Yan M, 2012, IEEE T SIGNAL PROCES, V60, P3868, DOI 10.1109/TSP.2012.2193397; Yancheshmeh FS, 2015, PROC CVPR IEEE, P2901, DOI 10.1109/CVPR.2015.7298908; Yang JF, 2011, SIAM J SCI COMPUT, V33, P250, DOI 10.1137/090777761; Yang L, 2017, SIAM J IMAGING SCI, V10, P74, DOI 10.1137/15M1027528; Yu SX, 2004, IEEE T PATTERN ANAL, V26, P173, DOI 10.1109/TPAMI.2004.1262179; Yuan GZ, 2017, AAAI CONF ARTIF INTE, P2867; Yuan XT, 2013, J MACH LEARN RES, V14, P899; Zhang B, 2005, STAT APPL GENET MOL, V4, DOI 10.2202/1544-6115.1128; Zhang ZG, 2007, IEEE DATA MINING, P391, DOI 10.1109/ICDM.2007.99; Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236	87	0	0	2	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4524	4543		10.1109/TPAMI.2021.3070753	http://dx.doi.org/10.1109/TPAMI.2021.3070753			20	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33798072				2022-12-18	WOS:000836666600008
J	Yan, K; Wang, XY; Kim, JM; Zuo, WM; Feng, DG				Yan, Ke; Wang, Xiuying; Kim, Jinman; Zuo, Wangmeng; Feng, Dagan			Deep Cognitive Gate: Resembling Human Cognition for Saliency Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Cognition; Saliency detection; Visualization; Feature extraction; Logic gates; Benchmark testing; Heating systems; Cognition; image processing; neural networks; object detection	OBJECT DETECTION; ACTION RECOGNITION; VISUAL-ATTENTION; NETWORK; MODEL	Saliency detection by human refers to the ability to identify pertinent information using our perceptive and cognitive capabilities. While human perception is attracted by visual stimuli, our cognitive capability is derived from the inspiration of constructing concepts of reasoning. Saliency detection has gained intensive interest with the aim of resembling human 'perceptual' system. However, saliency related to human 'cognition', particularly the analysis of complex salient regions ('cogitating' process), is yet to be fully exploited. We propose to resemble human cognition, coupled with human perception, to improve saliency detection. We recognize saliency in three phases ('Seeing' - 'Perceiving' - 'Cogitating), mimicking human's perceptive and cognitive thinking of an image. In our method, 'Seeing' phase is related to human perception, and we formulate the 'Perceiving' and 'Cogitating' phases related to the human cognition systems via deep neural networks (DNNs) to construct a new module (Cognitive Gate) that enhances the DNN features for saliency detection. To the best of our knowledge, this is the first work that established DNNs to resemble human cognition for saliency detection. In our experiments, our approach outperformed 17 benchmarking DNN methods on six well-recognized datasets, demonstrating that resembling human cognition improves saliency detection.	[Yan, Ke] Univ New South Wales, Sch Elect Engn & Telecommun, Sydney, NSW 2052, Australia; [Yan, Ke] Rudder Technol Pty Ltd, Sydney, NSW 2000, Australia; [Wang, Xiuying; Kim, Jinman; Feng, Dagan] Univ Sydney, Sch Comp Sci, Biomed & Multimedia Informat Technol BMIT Res Grp, Darlington, NSW 2008, Australia; [Zuo, Wangmeng] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China	University of New South Wales Sydney; University of Sydney; Harbin Institute of Technology	Wang, XY (corresponding author), Univ Sydney, Sch Comp Sci, Biomed & Multimedia Informat Technol BMIT Res Grp, Darlington, NSW 2008, Australia.	ke.yan@ruddergroup.com.au; xiu.wang@sydney.edu.au; jinman.kim@sydney.edu.au; cswmzuo@gmail.com; dagan.feng@sydney.edu.au	Guo, Jun/AAB-2448-2020	Guo, Jun/0000-0002-3329-1188; Wang, Xiu Ying/0000-0001-7160-5929				Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596; Ahmad M, 2021, MATH METHOD APPL SCI, DOI 10.1002/mma.7332; [Anonymous], 2011, P ADV NEUR INF PROC; [Anonymous], 2006, NIPS; Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Bauckhage C, 2004, PROC CVPR IEEE, P827; Birmingham E, 2009, VISION RES, V49, P2992, DOI 10.1016/j.visres.2009.09.014; Borji A., 2012, PROC IEEE COMPUT SOC, P23; Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833; Borji A, 2012, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2012.6247706; Cavanagh P, 2011, VISION RES, V51, P1538, DOI 10.1016/j.visres.2011.01.015; Cerf M., 2008, ADV NEURAL INFORM PR, V20, P241, DOI DOI 10.1145/2185520.2185525; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4; Christensen H. I., 2006, COGNITIVE VISION SYS; Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672; Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684; Derry S. J., 2014, INTERDISCIPLINARY CO; Dodge SF, 2018, IEEE T IMAGE PROCESS, V27, P4080, DOI 10.1109/TIP.2018.2834826; Erdem E, 2013, J VISION, V13, DOI 10.1167/13.4.11; Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487; Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698; Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172; Frintrop S, 2014, INT C PATT RECOG, P2329, DOI 10.1109/ICPR.2014.404; Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326; Gajjar V, 2017, IEEE INT CONF COMP V, P2805, DOI 10.1109/ICCVW.2017.330; Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272; Gong C, 2015, PROC CVPR IEEE, P2531, DOI 10.1109/CVPR.2015.7298868; Gopalakrishnan V, 2010, IEEE T IMAGE PROCESS, V19, P3232, DOI 10.1109/TIP.2010.2053940; Harel J., 2006, PAPER PRESENTED INT, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073; He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168; Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563; Hou XD, 2007, PROC CVPR IEEE, P2280; Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Jetley S, 2016, PROC CVPR IEEE, P5753, DOI 10.1109/CVPR.2016.620; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462; Kruthiventi SSS, 2017, IEEE T IMAGE PROCESS, V26, P4446, DOI 10.1109/TIP.2017.2710620; Kummerer M, 2017, IEEE I CONF COMP VIS, P4799, DOI 10.1109/ICCV.2017.513; Lee G, 2018, IEEE T PATTERN ANAL, V40, P1599, DOI 10.1109/TPAMI.2017.2737631; Li GB, 2018, IEEE T NEUR NET LEAR, V29, P6038, DOI 10.1109/TNNLS.2018.2817540; Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34; Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58; Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184; Li X, 2018, LECT NOTES COMPUT SC, V11206, P287, DOI [10.1007/978-3-030-01216-8_18, 10.1007/978-3-030-01267-0_22]; Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370; Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43; Lin L, 2017, IEEE T PATTERN ANAL, V39, P1089, DOI 10.1109/TPAMI.2016.2567386; Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326; Liu N, 2018, IEEE T IMAGE PROCESS, V27, P3264, DOI 10.1109/TIP.2018.2817047; Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80; Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70; Long J., 2015, P IEEE C COMPUTER VI, P3431, DOI DOI 10.1109/CVPR.2015.7298965; Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698; Murray N, 2011, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2011.5995506; Pan J, 2017, PROC IEEE C COMPUT V, P1; Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71; Pirsiavash Hamed, 2014, TECHNICAL REPORT; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rother C, 2006, ACM T GRAPHIC, V25, P847, DOI 10.1145/1141911.1141965; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Simonyan Karen, 2015, VERY DEEP CONVOLUTIO; Sweller J, 2006, LEARN INSTR, V16, P165, DOI 10.1016/j.learninstruc.2006.02.005; Tang Y., 2016, P ACM INT C MULT, P397, DOI [10.1145/2964284.2967250, DOI 10.1145/2964284.2967250]; Tavakoli HR, 2017, NEUROCOMPUTING, V244, P10, DOI 10.1016/j.neucom.2017.03.018; Tong N, 2015, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2015.7298798; TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5; Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358; Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404; Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938; Wang LZ, 2019, IEEE T PATTERN ANAL, V41, P1734, DOI 10.1109/TPAMI.2018.2846598; Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330; Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433; Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607; Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154; Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612; Wu RM, 2019, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR.2019.00834; Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403; Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153; Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407; Yuan YC, 2018, IEEE T IMAGE PROCESS, V27, P1311, DOI 10.1109/TIP.2017.2762422; Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688; Zeng XY, 2016, LECT NOTES COMPUT SC, V9911, P354, DOI 10.1007/978-3-319-46478-7_22; Zeng Y, 2019, IEEE I CONF COMP VIS, P7233, DOI 10.1109/ICCV.2019.00733; Zeng Y, 2019, IEEE I CONF COMP VIS, P7222, DOI 10.1109/ICCV.2019.00732; Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26; Zhang L, 2019, PROC CVPR IEEE, P6017, DOI 10.1109/CVPR.2019.00618; Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187; Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31; Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081; Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887; Zhao K, 2019, IEEE I CONF COMP VIS, P8848, DOI 10.1109/ICCV.2019.00894; Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731; Zhen XT, 2014, INFORM SCIENCES, V281, P295, DOI 10.1016/j.ins.2014.05.021	101	0	0	7	7	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4776	4792		10.1109/TPAMI.2021.3068277	http://dx.doi.org/10.1109/TPAMI.2021.3068277			17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	33755558				2022-12-18	WOS:000836666600024
J	Yang, JY; Mao, W; Alvarez, J; Liu, MM				Yang, Jiayu; Mao, Wei; Alvarez, Jose; Liu, Miaomiao			Cost Volume Pyramid Based Depth Inference for Multi-View Stereo	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Image resolution; Three-dimensional displays; Estimation; Buildings; Memory management; Feature extraction; Image reconstruction; Multi-view stereo; cost volume	RECONSTRUCTION	We propose a cost volume-based neural network for depth inference from multi-view images. We demonstrate that building a cost volume pyramid in a coarse-to-fine manner instead of constructing a cost volume at a fixed resolution leads to a compact, lightweight network and allows us inferring high resolution depth maps to achieve better reconstruction results. To this end, we first build a cost volume based on uniform sampling of fronto-parallel planes across the entire depth range at the coarsest resolution of an image. Then, given current depth estimate, we construct new cost volumes iteratively to perform depth map refinement. We show that working on cost volume pyramid can lead to a more compact, yet efficient network structure compared with existing works. We further show that the (residual) depth sampling can be fully determined by analytical geometric derivation, which serves as a principle for building compact cost volume pyramid. To demonstrate the effectiveness of our proposed framework, we extend our cost volume pyramid structure to handle the unsupervised depth inference scenario. Experimental results on benchmark datasets show that our model can perform 6x faster with similar performance as state-of-the-art methods for supervised scenario and demonstrates superior performance on unsupervised scenario. Code is available at https://github.com/JiayuYANG/CVP-MVSNet.	[Yang, Jiayu; Mao, Wei] Australian Natl Univ, Res Sch Engn, Canberra, ACT 0200, Australia; [Alvarez, Jose] NVIDIA, Santa Clara, CA 95051 USA; [Liu, Miaomiao] Australian Natl Univ, Sch Comp, Canberra, ACT 0200, Australia	Australian National University; Nvidia Corporation; Australian National University	Yang, JY (corresponding author), Australian Natl Univ, Res Sch Engn, Canberra, ACT 0200, Australia.	jiayu.yang@anu.edu.au; wei.mao@anu.edu.au; josea@nvidia.com; miaomiao.liu@anu.edu.au		Mao, Wei/0000-0002-8876-8983; Yang, Jiayu/0000-0001-7475-1286	Australian Research Council [DE180100628, DP200102274]	Australian Research Council(Australian Research Council)	This work was supported by Australian Research Council under Grants DE180100628 and DP200102274.	Aanaes H, 2016, INT J COMPUT VISION, V120, P153, DOI 10.1007/s11263-016-0902-9; Batsos K, 2018, PROC CVPR IEEE, P2060, DOI 10.1109/CVPR.2018.00220; Campbell NDF, 2008, LECT NOTES COMPUT SC, V5302, P766, DOI 10.1007/978-3-540-88682-2_58; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen R, 2019, IEEE I CONF COMP VIS, P1538, DOI 10.1109/ICCV.2019.00162; Cheng S, 2020, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR42600.2020.00260; Collins RT, 1996, PROC CVPR IEEE, P358, DOI 10.1109/CVPR.1996.517097; Dai YC, 2019, INT CONF 3D VISION, P1, DOI 10.1109/3DV.2019.00010; De Bonet J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P418, DOI 10.1109/ICCV.1999.791251; Diebel J., CVPR, V1, P519; Esteban CH, 2004, COMPUT VIS IMAGE UND, V96, P367, DOI 10.1016/j.cviu.2004.03.016; Faugeras O, 1998, IEEE T IMAGE PROCESS, V7, P336, DOI 10.1109/83.661183; FUA P, 1995, INT J COMPUT VISION, V16, P35, DOI 10.1007/BF01428192; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Galliani Silvano, 2016, PUBLIKATIONEN DTSCH, P7; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Gu XD, 2020, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR42600.2020.00257; Hartmann W, 2017, IEEE I CONF COMP VIS, P1595, DOI 10.1109/ICCV.2017.176; He K., 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90; Im S., 2019, P INT C LEARN REPR M, P1; Ji MQ, 2017, IEEE I CONF COMP VIS, P2326, DOI 10.1109/ICCV.2017.253; Jianfeng Yan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P674, DOI 10.1007/978-3-030-58548-8_39; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kang SB, 2001, PROC CVPR IEEE, P103; Kar A, 2017, ADV NEUR IN, V30; Kingma D.P., 2015, INT C LEARN REPR, P1; Knapitsch A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073599; Kolmogorov V, 2002, LECT NOTES COMPUT SC, V2352, P82; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; Leroy V, 2018, LECT NOTES COMPUT SC, V11213, P796, DOI 10.1007/978-3-030-01240-3_48; Long J., 2015, P IEEE C COMPUTER VI, P3431, DOI DOI 10.1109/CVPR.2015.7298965; Luo KY, 2019, IEEE I CONF COMP VIS, P10451, DOI 10.1109/ICCV.2019.01055; Luo WJ, 2016, PROC CVPR IEEE, P5695, DOI 10.1109/CVPR.2016.614; Merrell P, 2007, IEEE I CONF COMP VIS, P1221; Paschalidou D, 2018, PROC CVPR IEEE, P3897, DOI 10.1109/CVPR.2018.00410; Pons JP, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P597; Qingshan Xu, 2020, Arxiv, DOI arXiv:2007.07714; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Schonberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445; Schonberger JL, 2016, LECT NOTES COMPUT SC, V9907, P501, DOI 10.1007/978-3-319-46487-9_31; Schops T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272; Simonyan Karen, 2015, VERY DEEP CONVOLUTIO; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Tang CR, 2020, PHARMACOLOGY, V105, P339, DOI 10.1159/000503865; Tola E, 2012, MACH VISION APPL, V23, P903, DOI 10.1007/s00138-011-0346-8; Tonioni A, 2019, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2019.00028; Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216; Wang Y, 2019, IEEE INT CONF ROBOT, P5893, DOI 10.1109/ICRA.2019.8794003; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xue YZ, 2019, IEEE I CONF COMP VIS, P4311, DOI 10.1109/ICCV.2019.00441; Yang JY, 2020, PROC CVPR IEEE, P4876, DOI 10.1109/CVPR42600.2020.00493; Yang RG, 2003, PROC CVPR IEEE, P211, DOI 10.1109/ISCS.2003.1239980; Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47; Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567; Zbontar J, 2016, J MACH LEARN RES, V17; Zhang J., 2020, IEEE T PATTERN ANAL, DOI [10.1109/TPAMI.2020.2988729, DOI 10.1109/TPAMI.2020.2988729]; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700	58	0	0	6	6	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEPT 1	2022	44	9					4748	4760		10.1109/TPAMI.2021.3082562	http://dx.doi.org/10.1109/TPAMI.2021.3082562			13	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Science Citation Index Expanded (SCI-EXPANDED)	Computer Science; Engineering	3O2KN	34018927	Green Submitted			2022-12-18	WOS:000836666600022
